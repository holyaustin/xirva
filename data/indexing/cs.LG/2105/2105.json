[{"id": "2105.00003", "submitter": "Swapnil Mache", "authors": "Swapnil Mache, Praveen Kumar Pokala, Kusala Rajendran and Chandra\n  Sekhar Seelamantula", "title": "NuSPAN: A Proximal Average Network for Nonuniform Sparse Model --\n  Application to Seismic Reflectivity Inversion", "comments": "15 pages, 10 figures. This article builds on arXiv:2104.04704", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We solve the problem of sparse signal deconvolution in the context of seismic\nreflectivity inversion, which pertains to high-resolution recovery of the\nsubsurface reflection coefficients. Our formulation employs a nonuniform,\nnon-convex synthesis sparse model comprising a combination of convex and\nnon-convex regularizers, which results in accurate approximations of the l0\npseudo-norm. The resulting iterative algorithm requires the proximal average\nstrategy. When unfolded, the iterations give rise to a learnable proximal\naverage network architecture that can be optimized in a data-driven fashion. We\ndemonstrate the efficacy of the proposed approach through numerical experiments\non synthetic 1-D seismic traces and 2-D wedge models in comparison with the\nbenchmark techniques. We also present validations considering the simulated\nMarmousi2 model as well as real 3-D seismic volume data acquired from the\nPenobscot 3D survey off the coast of Nova Scotia, Canada.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 04:33:02 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mache", "Swapnil", ""], ["Pokala", "Praveen Kumar", ""], ["Rajendran", "Kusala", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "2105.00026", "submitter": "Cl\\'ement Chadebec", "authors": "Cl\\'ement Chadebec, Elina Thibeau-Sutre, Ninon Burgos and St\\'ephanie\n  Allassonni\\`ere", "title": "Data Augmentation in High Dimensional Low Sample Size Setting Using a\n  Geometry-Based Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method to perform data augmentation in a\nreliable way in the High Dimensional Low Sample Size (HDLSS) setting using a\ngeometry-based variational autoencoder. Our approach combines a proper latent\nspace modeling of the VAE seen as a Riemannian manifold with a new generation\nscheme which produces more meaningful samples especially in the context of\nsmall data sets. The proposed method is tested through a wide experimental\nstudy where its robustness to data sets, classifiers and training samples size\nis stressed. It is also validated on a medical imaging classification task on\nthe challenging ADNI database where a small number of 3D brain MRIs are\nconsidered and augmented using the proposed VAE framework. In each case, the\nproposed method allows for a significant and reliable gain in the\nclassification metrics. For instance, balanced accuracy jumps from 66.3% to\n74.3% for a state-of-the-art CNN classifier trained with 50 MRIs of cognitively\nnormal (CN) and 50 Alzheimer disease (AD) patients and from 77.7% to 86.3% when\ntrained with 243 CN and 210 AD while improving greatly sensitivity and\nspecificity metrics.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:10:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chadebec", "Cl\u00e9ment", ""], ["Thibeau-Sutre", "Elina", ""], ["Burgos", "Ninon", ""], ["Allassonni\u00e8re", "St\u00e9phanie", ""]]}, {"id": "2105.00043", "submitter": "Suraj Kothawade", "authors": "Suraj Kothawade, Vishal Kaushal, Ganesh Ramakrishnan, Jeff Bilmes,\n  Rishabh Iyer", "title": "Submodular Mutual Information for Targeted Data Subset Selection", "comments": "Accepted to ICLR 2021 S2D-OLAD Workshop; https://s2d-olad.github.io/.\n  arXiv admin note: substantial text overlap with arXiv:2103.00128", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of data, it is becoming increasingly difficult to train\nor improve deep learning models with the right subset of data. We show that\nthis problem can be effectively solved at an additional labeling cost by\ntargeted data subset selection(TSS) where a subset of unlabeled data points\nsimilar to an auxiliary set are added to the training data. We do so by using a\nrich class of Submodular Mutual Information (SMI) functions and demonstrate its\neffectiveness for image classification on CIFAR-10 and MNIST datasets. Lastly,\nwe compare the performance of SMI functions for TSS with other state-of-the-art\nmethods for closely related problems like active learning. Using SMI functions,\nwe observe ~20-30% gain over the model's performance before re-training with\nadded targeted subset; ~12% more than other methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:53:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kothawade", "Suraj", ""], ["Kaushal", "Vishal", ""], ["Ramakrishnan", "Ganesh", ""], ["Bilmes", "Jeff", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2105.00053", "submitter": "Goncalo Raposo", "authors": "Gon\\c{c}alo Raposo and Pedro Tom\\'as and Nuno Roma", "title": "PositNN: Training Deep Neural Networks with Mixed Low-Precision Posit", "comments": null, "journal-ref": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), pp. 7908-7912", "doi": "10.1109/ICASSP39728.2021.9413919", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-precision formats have proven to be an efficient way to reduce not only\nthe memory footprint but also the hardware resources and power consumption of\ndeep learning computations. Under this premise, the posit numerical format\nappears to be a highly viable substitute for the IEEE floating-point, but its\napplication to neural networks training still requires further research. Some\npreliminary results have shown that 8-bit (and even smaller) posits may be used\nfor inference and 16-bit for training, while maintaining the model accuracy.\nThe presented research aims to evaluate the feasibility to train deep\nconvolutional neural networks using posits. For such purpose, a software\nframework was developed to use simulated posits and quires in end-to-end\ntraining and inference. This implementation allows using any bit size,\nconfiguration, and even mixed precision, suitable for different precision\nrequirements in various stages. The obtained results suggest that 8-bit posits\ncan substitute 32-bit floats during training with no negative impact on the\nresulting loss and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 19:30:37 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 09:26:38 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 17:15:46 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Raposo", "Gon\u00e7alo", ""], ["Tom\u00e1s", "Pedro", ""], ["Roma", "Nuno", ""]]}, {"id": "2105.00059", "submitter": "Alexandr Sboev", "authors": "Alexander Sboev, Sanna Sboeva, Ivan Moloshnikov, Artem Gryaznov, Roman\n  Rybka, Alexander Naumov, Anton Selivanov, Gleb Rylkov, Viacheslav Ilyin", "title": "An analysis of full-size Russian complexly NER labelled corpus of\n  Internet user reviews on the drugs based on deep learning and language neural\n  nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the full-size Russian complexly NER-labeled corpus of Internet\nuser reviews, along with an evaluation of accuracy levels reached on this\ncorpus by a set of advanced deep learning neural networks to extract the\npharmacologically meaningful entities from Russian texts. The corpus annotation\nincludes mentions of the following entities: Medication (33005 mentions),\nAdverse Drug Reaction (1778), Disease (17403), and Note (4490). Two of them -\nMedication and Disease - comprise a set of attributes. A part of the corpus has\nthe coreference annotation with 1560 coreference chains in 300 documents.\nSpecial multi-label model based on a language model and the set of features is\ndeveloped, appropriate for presented corpus labeling. The influence of the\nchoice of different modifications of the models: word vector representations,\ntypes of language models pre-trained for Russian, text normalization styles,\nand other preliminary processing are analyzed. The sufficient size of our\ncorpus allows to study the effects of particularities of corpus labeling and\nbalancing entities in the corpus. As a result, the state of the art for the\npharmacological entity extraction problem for Russian is established on a\nfull-size labeled corpus. In case of the adverse drug reaction (ADR)\nrecognition, it is 61.1 by the F1-exact metric that, as our analysis shows, is\non par with the accuracy level for other language corpora with similar\ncharacteristics and the ADR representativnes. The evaluated baseline precision\nof coreference relation extraction on the corpus is 71, that is higher the\nresults reached on other Russian corpora.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 19:46:24 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sboev", "Alexander", ""], ["Sboeva", "Sanna", ""], ["Moloshnikov", "Ivan", ""], ["Gryaznov", "Artem", ""], ["Rybka", "Roman", ""], ["Naumov", "Alexander", ""], ["Selivanov", "Anton", ""], ["Rylkov", "Gleb", ""], ["Ilyin", "Viacheslav", ""]]}, {"id": "2105.00063", "submitter": "Dejan Stepec", "authors": "Tomaz Martincic and Dejan Stepec and Joao Pita Costa and Kristijan\n  Cagran and Athanasios Chaldeakis", "title": "Vessel and Port Efficiency Metrics through Validated AIS data", "comments": "OCEANS 2020", "journal-ref": null, "doi": "10.1109/IEEECONF38699.2020.9389112", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic Identification System (AIS) data represents a rich source of\ninformation about maritime traffic and offers a great potential for data\nanalytics and predictive modeling solutions, which can help optimizing logistic\nchains and to reduce environmental impacts. In this work, we address the main\nlimitations of the validity of AIS navigational data fields, by proposing a\nmachine learning-based data-driven methodology to detect and (to the possible\nextent) also correct erroneous data. Additionally, we propose a metric that can\nbe used by vessel operators and ports to express numerically their business and\nenvironmental efficiency through time and spatial dimensions, enabled with the\nobtained validated AIS data. We also demonstrate Port Area Vessel Movements\n(PARES) tool, which demonstrates the proposed solutions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 19:51:51 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Martincic", "Tomaz", ""], ["Stepec", "Dejan", ""], ["Costa", "Joao Pita", ""], ["Cagran", "Kristijan", ""], ["Chaldeakis", "Athanasios", ""]]}, {"id": "2105.00074", "submitter": "Sayantan Chowdhury", "authors": "Sayantan Chowdhury, Ben Liang, Ali Tizghadam, Ilijc Albanese", "title": "Flow-Packet Hybrid Traffic Classification for Class-Aware Network\n  Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network traffic classification using machine learning techniques has been\nwidely studied. Most existing schemes classify entire traffic flows, but there\nare major limitations to their practicality. At a network router, the packets\nneed to be processed with minimum delay, so the classifier cannot wait until\nthe end of the flow to make a decision. Furthermore, a complicated machine\nlearning algorithm can be too computationally expensive to implement inside the\nrouter. In this paper, we introduce flow-packet hybrid traffic classification\n(FPHTC), where the router makes a decision per packet based on a routing policy\nthat is designed through transferring the learned knowledge from a flow-based\nclassifier residing outside the router. We analyze the generalization bound of\nFPHTC and show its advantage over regular packet-based traffic classification.\nWe present experimental results using a real-world traffic dataset to\nillustrate the classification performance of FPHTC. We show that it is robust\ntoward traffic pattern changes and can be deployed with limited computational\nresource.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 20:30:36 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chowdhury", "Sayantan", ""], ["Liang", "Ben", ""], ["Tizghadam", "Ali", ""], ["Albanese", "Ilijc", ""]]}, {"id": "2105.00075", "submitter": "Samuel Raymond", "authors": "Samuel J. Raymond and David B. Camarillo", "title": "Applying physics-based loss functions to neural networks for improved\n  generalizability in mechanics problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-Informed Machine Learning (PIML) has gained momentum in the last 5\nyears with scientists and researchers aiming to utilize the benefits afforded\nby advances in machine learning, particularly in deep learning. With large\nscientific data sets with rich spatio-temporal data and high-performance\ncomputing providing large amounts of data to be inferred and interpreted, the\ntask of PIML is to ensure that these predictions, categorizations, and\ninferences are enforced by, and conform to the limits imposed by physical laws.\nIn this work a new approach to utilizing PIML is discussed that deals with the\nuse of physics-based loss functions. While typical usage of physical equations\nin the loss function requires complex layers of derivatives and other functions\nto ensure that the known governing equation is satisfied, here we show that a\nsimilar level of enforcement can be found by implementing more simpler loss\nfunctions on specific kinds of output data. The generalizability that this\napproach affords is shown using examples of simple mechanical models that can\nbe thought of as sufficiently simplified surrogate models for a wide class of\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 20:31:09 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 14:00:56 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Raymond", "Samuel J.", ""], ["Camarillo", "David B.", ""]]}, {"id": "2105.00097", "submitter": "Nikita Araslanov", "authors": "Nikita Araslanov and Stefan Roth", "title": "Self-supervised Augmentation Consistency for Adapting Semantic\n  Segmentation", "comments": "To appear at CVPR 2021. Code: https://github.com/visinf/da-sac", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to domain adaptation for semantic segmentation that is\nboth practical and highly accurate. In contrast to previous work, we abandon\nthe use of computationally involved adversarial objectives, network ensembles\nand style transfer. Instead, we employ standard data augmentation techniques\n$-$ photometric noise, flipping and scaling $-$ and ensure consistency of the\nsemantic predictions across these image transformations. We develop this\nprinciple in a lightweight self-supervised framework trained on co-evolving\npseudo labels without the need for cumbersome extra training rounds. Simple in\ntraining from a practitioner's standpoint, our approach is remarkably\neffective. We achieve significant improvements of the state-of-the-art\nsegmentation accuracy after adaptation, consistent both across different\nchoices of the backbone architecture and adaptation scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 21:32:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Araslanov", "Nikita", ""], ["Roth", "Stefan", ""]]}, {"id": "2105.00100", "submitter": "Marcus Saraiva", "authors": "Saraiva Marcus, Forechi Avelino, de Oliveira Neto Jorcy, DelRey\n  Antonio and Rauber Thomas", "title": "Data-driven Full-waveform Inversion Surrogate using Conditional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Oil and Gas industry, estimating a subsurface velocity field is an\nessential step in seismic processing, reservoir characterization, and\nhydrocarbon volume calculation. Full-waveform inversion (FWI) velocity modeling\nis an iterative advanced technique that provides an accurate and detailed\nvelocity field model, although at a very high computational cost due to the\nphysics-based numerical simulations required at each FWI iteration. In this\nstudy, we propose a method of generating velocity field models, as detailed as\nthose obtained through FWI, using a conditional generative adversarial network\n(cGAN) with multiple inputs. The primary motivation of this approach is to\ncircumvent the extremely high cost of full-waveform inversion velocity\nmodeling. Real-world data were used to train and test the proposed network\narchitecture, and three evaluation metrics (percent error, structural\nsimilarity index measure, and visual analysis) were adopted as quality\ncriteria. Based on these metrics, the results evaluated upon the test set\nsuggest that the GAN was able to accurately match real FWI generated outputs,\nenabling it to extract from input data the main geological structures and\nlateral velocity variations. Experimental results indicate that the proposed\nmethod, when deployed, has the potential to increase the speed of geophysical\nreservoir characterization processes, saving on time and computational\nresources.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 21:41:24 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Marcus", "Saraiva", ""], ["Avelino", "Forechi", ""], ["Jorcy", "de Oliveira Neto", ""], ["Antonio", "DelRey", ""], ["Thomas", "Rauber", ""]]}, {"id": "2105.00104", "submitter": "Guangyi Zhang", "authors": "Guangyi Zhang and Ali Etemad", "title": "Distilling EEG Representations via Capsules for Affective Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective computing with Electroencephalogram (EEG) is a challenging task\nthat requires cumbersome models to effectively learn the information contained\nin large-scale EEG signals, causing difficulties for real-time smart-device\ndeployment. In this paper, we propose a novel knowledge distillation pipeline\nto distill EEG representations via capsule-based architectures for both\nclassification and regression tasks. Our goal is to distill information from a\nheavy model to a lightweight model for subject-specific tasks. To this end, we\nfirst pre-train a large model (teacher network) on large number of training\nsamples. Then, we employ the teacher network to learn the discriminative\nfeatures embedded in capsules by adopting a lightweight model (student network)\nto mimic the teacher using the privileged knowledge. Such privileged\ninformation learned by the teacher contain similarities among capsules and are\nonly available during the training stage of the student network. We evaluate\nthe proposed architecture on two large-scale public EEG datasets, showing that\nour framework consistently enables student networks with different compression\nratios to effectively learn from the teacher, even when provided with limited\ntraining samples. Lastly, our method achieves state-of-the-art results on one\nof the two datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:04:35 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Guangyi", ""], ["Etemad", "Ali", ""]]}, {"id": "2105.00105", "submitter": "Madeleine Udell", "authors": "Yiming Sun and Yang Guo and Joel A. Tropp and Madeleine Udell", "title": "Tensor Random Projection for Low Memory Dimension Reduction", "comments": "In NeurIPS Workshop on Relational Representation Learning (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projections reduce the dimension of a set of vectors while preserving\nstructural information, such as distances between vectors in the set. This\npaper proposes a novel use of row-product random matrices in random projection,\nwhere we call it Tensor Random Projection (TRP). It requires substantially less\nmemory than existing dimension reduction maps. The TRP map is formed as the\nKhatri-Rao product of several smaller random projections, and is compatible\nwith any base random projection including sparse maps, which enable dimension\nreduction with very low query cost and no floating point operations. We also\ndevelop a reduced variance extension. We provide a theoretical analysis of the\nbias and variance of the TRP, and a non-asymptotic error analysis for a TRP\ncomposed of two smaller maps. Experiments on both synthetic and MNIST data show\nthat our method performs as well as conventional methods with substantially\nless storage.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:08:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sun", "Yiming", ""], ["Guo", "Yang", ""], ["Tropp", "Joel A.", ""], ["Udell", "Madeleine", ""]]}, {"id": "2105.00108", "submitter": "Hugh Chen", "authors": "Hugh Chen, Scott M. Lundberg, Su-In Lee", "title": "Explaining a Series of Models by Propagating Local Feature Attributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pipelines involving a series of several machine learning models (e.g.,\nstacked generalization ensembles, neural network feature extractors) improve\nperformance in many domains but are difficult to understand. To improve their\ntransparency, we introduce a framework to propagate local feature attributions\nthrough complex pipelines of models based on a connection to the Shapley value.\nOur framework enables us to (1) draw higher-level conclusions based on groups\nof gene expression features for Alzheimer's and breast cancer histologic grade\nprediction, (2) draw important insights about the errors a mortality prediction\nmodel makes by explaining a loss that is a non-linear transformation of the\nmodel's output, (3) explain pipelines of deep feature extractors fed into a\ntree model for MNIST digit classification, and (4) interpret important consumer\nscores and raw features in a stacked generalization setting to predict risk for\nhome equity line of credit applications. Importantly, in the consumer scoring\nexample, DeepSHAP is the only feature attribution technique we are aware of\nthat allows independent entities (e.g., lending institutions, credit bureaus)\nto compute attributions for the original features without having to share their\nproprietary models. Quantitatively comparing our framework to model-agnostic\napproaches, we show that our approach is an order of magnitude faster while\nproviding equally salient explanations. In addition, we describe how to\nincorporate an empirical baseline distribution, which allows us to (1)\ndemonstrate the bias of previous approaches that use a single baseline sample,\nand (2) present a straightforward methodology for choosing meaningful baseline\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:20:58 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Hugh", ""], ["Lundberg", "Scott M.", ""], ["Lee", "Su-In", ""]]}, {"id": "2105.00113", "submitter": "Yisroel Mirsky Dr.", "authors": "Yisroel Mirsky", "title": "IPatch: A Remote Adversarial Patch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Applications such as autonomous vehicles and medical screening use deep\nlearning models to localize and identify hundreds of objects in a single frame.\nIn the past, it has been shown how an attacker can fool these models by placing\nan adversarial patch within a scene. However, these patches must be placed in\nthe target location and do not explicitly alter the semantics elsewhere in the\nimage.\n  In this paper, we introduce a new type of adversarial patch which alters a\nmodel's perception of an image's semantics. These patches can be placed\nanywhere within an image to change the classification or semantics of locations\nfar from the patch. We call this new class of adversarial examples `remote\nadversarial patches' (RAP).\n  We implement our own RAP called IPatch and perform an in-depth analysis on\nimage segmentation RAP attacks using five state-of-the-art architectures with\neight different encoders on the CamVid street view dataset. Moreover, we\ndemonstrate that the attack can be extended to object recognition models with\npreliminary results on the popular YOLOv3 model. We found that the patch can\nchange the classification of a remote target region with a success rate of up\nto 93% on average.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:34:32 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:21:53 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mirsky", "Yisroel", ""]]}, {"id": "2105.00134", "submitter": "Arseny Tolmachev", "authors": "Arseny Tolmachev, Akira Sakai, Masaru Todoriki, Koji Maruhashi", "title": "Bermuda Triangles: GNNs Fail to Detect Simple Topological Structures", "comments": "ICLR 2021 GTRL Poster Presentation:\n  https://openreview.net/forum?id=Vz_Nl9MSQnu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most graph neural network architectures work by message-passing node vector\nembeddings over the adjacency matrix, and it is assumed that they capture graph\ntopology by doing that. We design two synthetic tasks, focusing purely on\ntopological problems -- triangle detection and clique distance -- on which\ngraph neural networks perform surprisingly badly, failing to detect those\n\"bermuda\" triangles. Datasets and their generation scripts are publicly\navailable on github.com/FujitsuLaboratories/bermudatriangles and\ndataset.labs.fujitsu.com.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 00:47:37 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Tolmachev", "Arseny", ""], ["Sakai", "Akira", ""], ["Todoriki", "Masaru", ""], ["Maruhashi", "Koji", ""]]}, {"id": "2105.00137", "submitter": "Erich Merrill", "authors": "Erich Merrill, Stefan Lee, Li Fuxin, Thomas G. Dietterich, Alan Fern", "title": "Deep Convolution for Irregularly Sampled Temporal Point Clouds", "comments": "12 pages, submitted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modeling the dynamics of continuous\nspatial-temporal processes represented by irregular samples through both space\nand time. Such processes occur in sensor networks, citizen science, multi-robot\nsystems, and many others. We propose a new deep model that is able to directly\nlearn and predict over this irregularly sampled data, without voxelization, by\nleveraging a recent convolutional architecture for static point clouds. The\nmodel also easily incorporates the notion of multiple entities in the process.\nIn particular, the model can flexibly answer prediction queries about arbitrary\nspace-time points for different entities regardless of the distribution of the\ntraining or test-time data. We present experiments on real-world weather\nstation data and battles between large armies in StarCraft II. The results\ndemonstrate the model's flexibility in answering a variety of query types and\ndemonstrate improved performance and efficiency compared to state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 00:54:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Merrill", "Erich", ""], ["Lee", "Stefan", ""], ["Fuxin", "Li", ""], ["Dietterich", "Thomas G.", ""], ["Fern", "Alan", ""]]}, {"id": "2105.00153", "submitter": "Kasra Mokhtari", "authors": "Kasra Mokhtari, Alan R. Wagner", "title": "Pedestrian Collision Avoidance for Autonomous Vehicles at Unsignalized\n  Intersection Using Deep Q-Network", "comments": "8 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior research has extensively explored Autonomous Vehicle (AV) navigation in\nthe presence of other vehicles, however, navigation among pedestrians, who are\nthe most vulnerable element in urban environments, has been less examined. This\npaper explores AV navigation in crowded, unsignalized intersections. We compare\nthe performance of different deep reinforcement learning methods trained on our\nreward function and state representation. The performance of these methods and\na standard rule-based approach were evaluated in two ways, first at the\nunsignalized intersection on which the methods were trained, and secondly at an\nunknown unsignalized intersection with a different topology. For both\nscenarios, the rule-based method achieves less than 40\\% collision-free\nepisodes, whereas our methods result in a performance of approximately 100\\%.\nOf the three methods used, DDQN/PER outperforms the other two methods while it\nalso shows the smallest average intersection crossing time, the greatest\naverage speed, and the greatest distance from the closest pedestrian.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 03:02:21 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mokhtari", "Kasra", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2105.00173", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis\n  Tool for Singers", "comments": "26 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CY cs.LG cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current computational-emotion research has focused on applying acoustic\nproperties to analyze how emotions are perceived mathematically or used in\nnatural language processing machine learning models. While recent interest has\nfocused on analyzing emotions from the spoken voice, little experimentation has\nbeen performed to discover how emotions are recognized in the singing voice --\nboth in noiseless and noisy data (i.e., data that is either inaccurate,\ndifficult to interpret, has corrupted/distorted/nonsense information like\nactual noise sounds in this case, or has a low ratio of usable/unusable\ninformation). Not only does this ignore the challenges of training machine\nlearning models on more subjective data and testing them with much noisier\ndata, but there is also a clear disconnect in progress between advancing the\ndevelopment of convolutional neural networks and the goal of emotionally\ncognizant artificial intelligence. By training a new model to include this type\nof information with a rich comprehension of psycho-acoustic properties, not\nonly can models be trained to recognize information within extremely noisy\ndata, but advancement can be made toward more complex biofeedback applications\n-- including creating a model which could recognize emotions given any human\ninformation (language, breath, voice, body, posture) and be used in any\nperformance medium (music, speech, acting) or psychological assistance for\npatients with disorders such as BPD, alexithymia, autism, among others. This\npaper seeks to reflect and expand upon the findings of related research and\npresent a stepping-stone toward this end goal.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 05:47:15 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 07:34:14 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2105.00177", "submitter": "Sagar Shrestha", "authors": "Sagar Shrestha, Xiao Fu and Mingyi Hong", "title": "Deep Spectrum Cartography: Completing Radio Map Tensors Using Learned\n  Neural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectrum cartography (SC) technique constructs multi-domain (e.g.,\nfrequency, space, and time) radio frequency (RF) maps from limited\nmeasurements, which can be viewed as an ill-posed tensor completion problem.\nModel-based cartography techniques often rely on handcrafted priors (e.g.,\nsparsity, smoothness and low-rank structures) for the completion task. Such\npriors may be inadequate to capture the essence of complex wireless\nenvironments -- especially when severe shadowing happens. To circumvent such\nchallenges, offline-trained deep neural models of radio maps were considered\nfor SC, as deep neural networks (DNNs) are able to \"learn\" intricate underlying\nstructures from data. However, such deep learning (DL)-based SC approaches\nencounter serious challenges in both off-line model learning (training) and\ncompletion (generalization), possibly because the latent state space for\ngenerating the radio maps is prohibitively large. In this work, an emitter\nradio map disaggregation-based approach is proposed, under which only\nindividual emitters' radio maps are modeled by DNNs. This way, the learning and\ngeneralization challenges can both be substantially alleviated. Using the\nlearned DNNs, a fast nonnegative matrix factorization-based two-stage SC method\nand a performance-enhanced iterative optimization algorithm are proposed.\nTheoretical aspects -- such as recoverability of the radio tensor, sample\ncomplexity, and noise robustness -- under the proposed framework are\ncharacterized, and such theoretical properties have been elusive in the context\nof DL-based radio tensor completion. Experiments using synthetic and real-data\nfrom indoor and heavily shadowed environments are employed to showcase the\neffectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 07:04:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Shrestha", "Sagar", ""], ["Fu", "Xiao", ""], ["Hong", "Mingyi", ""]]}, {"id": "2105.00191", "submitter": "Ozan \\\"Ozdenizci", "authors": "Ozan Ozdenizci, Deniz Erdogmus", "title": "Stochastic Mutual Information Gradient Estimation for Dimensionality\n  Reduction Networks", "comments": "Accepted for publication at Elsevier - Information Sciences", "journal-ref": null, "doi": "10.1016/j.ins.2021.04.066", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Feature ranking and selection is a widely used approach in various\napplications of supervised dimensionality reduction in discriminative machine\nlearning. Nevertheless there exists significant evidence on feature ranking and\nselection algorithms based on any criterion leading to potentially sub-optimal\nsolutions for class separability. In that regard, we introduce emerging\ninformation theoretic feature transformation protocols as an end-to-end neural\nnetwork training approach. We present a dimensionality reduction network\n(MMINet) training procedure based on the stochastic estimate of the mutual\ninformation gradient. The network projects high-dimensional features onto an\noutput feature space where lower dimensional representations of features carry\nmaximum mutual information with their associated class labels. Furthermore, we\nformulate the training objective to be estimated non-parametrically with no\ndistributional assumptions. We experimentally evaluate our method with\napplications to high-dimensional biological data sets, and relate it to\nconventional feature selection algorithms to form a special case of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 08:20:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ozdenizci", "Ozan", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "2105.00192", "submitter": "Bahar Uddin Mahmud", "authors": "Bahar Uddin Mahmud, Afsana Sharmin", "title": "Deep Insights of Deepfake Technology : A Review", "comments": null, "journal-ref": "DUJASE Vol. 5(1 & 2) 13-23, 2020 (January & July)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under the aegis of computer vision and deep learning technology, a new\nemerging techniques has introduced that anyone can make highly realistic but\nfake videos, images even can manipulates the voices. This technology is widely\nknown as Deepfake Technology. Although it seems interesting techniques to make\nfake videos or image of something or some individuals but it could spread as\nmisinformation via internet. Deepfake contents could be dangerous for\nindividuals as well as for our communities, organizations, countries religions\netc. As Deepfake content creation involve a high level expertise with\ncombination of several algorithms of deep learning, it seems almost real and\ngenuine and difficult to differentiate. In this paper, a wide range of articles\nhave been examined to understand Deepfake technology more extensively. We have\nexamined several articles to find some insights such as what is Deepfake, who\nare responsible for this, is there any benefits of Deepfake and what are the\nchallenges of this technology. We have also examined several creation and\ndetection techniques. Our study revealed that although Deepfake is a threat to\nour societies, proper measures and strict regulations could prevent this.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 08:25:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mahmud", "Bahar Uddin", ""], ["Sharmin", "Afsana", ""]]}, {"id": "2105.00194", "submitter": "Jong Chul Ye", "authors": "Hyungjin Chung and Jong Chul Ye", "title": "Feature Disentanglement in generating three-dimensional structure from\n  two-dimensional slice with sliceGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep generative models are known to be able to model arbitrary probability\ndistributions. Among these, a recent deep generative model, dubbed sliceGAN,\nproposed a new way of using the generative adversarial network (GAN) to capture\nthe micro-structural characteristics of a two-dimensional (2D) slice and\ngenerate three-dimensional (3D) volumes with similar properties. While 3D\nmicrographs are largely beneficial in simulating diverse material behavior,\nthey are often much harder to obtain than their 2D counterparts. Hence,\nsliceGAN opens up many interesting directions of research by learning the\nrepresentative distribution from 2D slices, and transferring the learned\nknowledge to generate arbitrary 3D volumes. However, one limitation of sliceGAN\nis that latent space steering is not possible. Hence, we combine sliceGAN with\nAdaIN to endow the model with the ability to disentangle the features and\ncontrol the synthesis.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 08:29:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chung", "Hyungjin", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2105.00195", "submitter": "Jannik Z\\\"urn", "authors": "Jannik Z\\\"urn, Johan Vertens, Wolfram Burgard", "title": "Lane Graph Estimation for Scene Understanding in Urban Driving", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane-level scene annotations provide invaluable data in autonomous vehicles\nfor trajectory planning in complex environments such as urban areas and cities.\nHowever, obtaining such data is time-consuming and expensive since lane\nannotations have to be annotated manually by humans and are as such hard to\nscale to large areas. In this work, we propose a novel approach for lane\ngeometry estimation from bird's-eye-view images. We formulate the problem of\nlane shape and lane connections estimation as a graph estimation problem where\nlane anchor points are graph nodes and lane segments are graph edges. We train\na graph estimation model on multimodal bird's-eye-view data processed from the\npopular NuScenes dataset and its map expansion pack. We furthermore estimate\nthe direction of the lane connection for each lane segment with a separate\nmodel which results in a directed lane graph. We illustrate the performance of\nour LaneGraphNet model on the challenging NuScenes dataset and provide\nextensive qualitative and quantitative evaluation. Our model shows promising\nperformance for most evaluated urban scenes and can serve as a step towards\nautomated generation of HD lane annotations for autonomous driving.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 08:38:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Z\u00fcrn", "Jannik", ""], ["Vertens", "Johan", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2105.00202", "submitter": "Stavros Ntalampiras", "authors": "Michelangelo Acconcjaioco and Stavros Ntalampiras", "title": "One-shot learning for acoustic identification of bird species in\n  non-stationary environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work introduces the one-shot learning paradigm in the computational\nbioacoustics domain. Even though, most of the related literature assumes\navailability of data characterizing the entire class dictionary of the problem\nat hand, that is rarely true as a habitat's species composition is only known\nup to a certain extent. Thus, the problem needs to be addressed by\nmethodologies able to cope with non-stationarity. To this end, we propose a\nframework able to detect changes in the class dictionary and incorporate new\nclasses on the fly. We design an one-shot learning architecture composed of a\nSiamese Neural Network operating in the logMel spectrogram space. We\nextensively examine the proposed approach on two datasets of various bird\nspecies using suitable figures of merit. Interestingly, such a learning scheme\nexhibits state of the art performance, while taking into account extreme\nnon-stationarity cases.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 09:43:20 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Acconcjaioco", "Michelangelo", ""], ["Ntalampiras", "Stavros", ""]]}, {"id": "2105.00210", "submitter": "Avi Mohan", "authors": "Mohammani Zaki, Avi Mohan, Aditya Gopalan, Shie Mannor", "title": "Better than the Best: Gradient-based Improper Reinforcement Learning for\n  Network Scheduling", "comments": "4 pages, 5 figures, RLNQ workshop at the SIGMETRICS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of scheduling in constrained queueing networks with a\nview to minimizing packet delay. Modern communication systems are becoming\nincreasingly complex, and are required to handle multiple types of traffic with\nwidely varying characteristics such as arrival rates and service times. This,\ncoupled with the need for rapid network deployment, render a bottom up approach\nof first characterizing the traffic and then devising an appropriate scheduling\nprotocol infeasible.\n  In contrast, we formulate a top down approach to scheduling where, given an\nunknown network and a set of scheduling policies, we use a policy gradient\nbased reinforcement learning algorithm that produces a scheduler that performs\nbetter than the available atomic policies. We derive convergence results and\nanalyze finite time performance of the algorithm. Simulation results show that\nthe algorithm performs well even when the arrival rates are nonstationary and\ncan stabilize the system even when the constituent policies are unstable.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 10:18:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zaki", "Mohammani", ""], ["Mohan", "Avi", ""], ["Gopalan", "Aditya", ""], ["Mannor", "Shie", ""]]}, {"id": "2105.00211", "submitter": "Emmanuel Ramasso", "authors": "Pablo Juesas, Emmanuel Ramasso, S\\'ebastien Drujont, Vincent Placet", "title": "Autoregressive Hidden Markov Models with partial knowledge on latent\n  space applied to aero-engines prognostics", "comments": null, "journal-ref": "European Conference of the PHM Society 2016, selected for extended\n  version in IJPHM", "doi": "10.36001/phme.2016.v3i1.1642", "report-no": "hal-02131233", "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  [This paper was initially published in PHME conference in 2016, selected for\nfurther publication in International Journal of Prognostics and Health\nManagement.]\n  This paper describes an Autoregressive Partially-hidden Markov model (ARPHMM)\nfor fault detection and prognostics of equipments based on sensors' data. It is\na particular dynamic Bayesian network that allows to represent the dynamics of\na system by means of a Hidden Markov Model (HMM) and an autoregressive (AR)\nprocess. The Markov chain assumes that the system is switching back and forth\nbetween internal states while the AR process ensures a temporal coherence on\nsensor measurements. A sound learning procedure of standard ARHMM based on\nmaximum likelihood allows to iteratively estimate all parameters\nsimultaneously. This paper suggests a modification of the learning procedure\nconsidering that one may have prior knowledge about the structure which becomes\npartially hidden. The integration of the prior is based on the Theory of\nWeighted Distributions which is compatible with the Expectation-Maximization\nalgorithm in the sense that the convergence properties are still satisfied. We\nshow how to apply this model to estimate the remaining useful life based on\nhealth indicators. The autoregressive parameters can indeed be used for\nprediction while the latent structure can be used to get information about the\ndegradation level. The interest of the proposed method for prognostics and\nhealth assessment is demonstrated on CMAPSS datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 10:23:22 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Juesas", "Pablo", ""], ["Ramasso", "Emmanuel", ""], ["Drujont", "S\u00e9bastien", ""], ["Placet", "Vincent", ""]]}, {"id": "2105.00220", "submitter": "Kensuke Nakamura", "authors": "Kensuke Nakamura and Simon Korman and Byung-Woo Hong", "title": "Stabilization of generative adversarial networks via noisy scale-space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GAN) is a framework for generating fake data\nbased on given reals but is unstable in the optimization. In order to stabilize\nGANs, the noise enlarges the overlap of the real and fake distributions at the\ncost of significant variance. The data smoothing may reduce the dimensionality\nof data but suppresses the capability of GANs to learn high-frequency\ninformation. Based on these observations, we propose a data representation for\nGANs, called noisy scale-space, that recursively applies the smoothing with\nnoise to data in order to preserve the data variance while replacing\nhigh-frequency information by random data, leading to a coarse-to-fine training\nof GANs. We also present a synthetic data-set using the Hadamard bases that\nenables us to visualize the true distribution of data. We experiment with a\nDCGAN with the noise scale-space (NSS-GAN) using major data-sets in which\nNSS-GAN overtook state-of-the-arts in most cases independent of the image\ncontent.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:32:16 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 01:12:19 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Nakamura", "Kensuke", ""], ["Korman", "Simon", ""], ["Hong", "Byung-Woo", ""]]}, {"id": "2105.00227", "submitter": "Cory Merkel", "authors": "Micah Gorsline, James Smith, Cory Merkel", "title": "On the Adversarial Robustness of Quantized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the size of neural network models is a critical step in moving AI\nfrom a cloud-centric to an edge-centric (i.e. on-device) compute paradigm. This\nshift from cloud to edge is motivated by a number of factors including reduced\nlatency, improved security, and higher flexibility of AI algorithms across\nseveral application domains (e.g. transportation, healthcare, defense, etc.).\nHowever, it is currently unclear how model compression techniques may affect\nthe robustness of AI algorithms against adversarial attacks. This paper\nexplores the effect of quantization, one of the most common compression\ntechniques, on the adversarial robustness of neural networks. Specifically, we\ninvestigate and model the accuracy of quantized neural networks on\nadversarially-perturbed images. Results indicate that for simple gradient-based\nattacks, quantization can either improve or degrade adversarial robustness\ndepending on the attack strength.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:46:35 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Gorsline", "Micah", ""], ["Smith", "James", ""], ["Merkel", "Cory", ""]]}, {"id": "2105.00233", "submitter": "Koki Okajima", "authors": "Koki Okajima and Yoshiyuki Kabashima", "title": "Matrix completion based on Gaussian belief propagation", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a message-passing algorithm for noisy matrix completion problems\nbased on matrix factorization. The algorithm is derived by approximating\nmessage distributions of belief propagation with Gaussian distributions that\nshare the same first and second moments. We also derive a memory-friendly\nversion of the proposed algorithm by applying a perturbation treatment commonly\nused in the literature of approximate message passing. In addition, a damping\ntechnique, which is demonstrated to be crucial for optimal performance, is\nintroduced without computational strain, and the relationship to the\nmessage-passing version of alternating least squares, a method reported to be\noptimal in certain settings, is discussed. Experiments on synthetic datasets\nshow that while the proposed algorithm quantitatively exhibits almost the same\nperformance under settings where the earlier algorithm is optimal, it is\nadvantageous when the observed datasets are corrupted by non-Gaussian noise.\nExperiments on real-world datasets also emphasize the performance differences\nbetween the two algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 12:16:49 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Okajima", "Koki", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "2105.00240", "submitter": "Jong Chul Ye", "authors": "Hyungjin Chung, Jaehyun Kim, Jeong Hee Yoon, Jeong Min Lee, and Jong\n  Chul Ye", "title": "Simultaneous super-resolution and motion artifact removal in\n  diffusion-weighted MRI using unsupervised deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Diffusion-weighted MRI is nowadays performed routinely due to its prognostic\nability, yet the quality of the scans are often unsatisfactory which can\nsubsequently hamper the clinical utility. To overcome the limitations, here we\npropose a fully unsupervised quality enhancement scheme, which boosts the\nresolution and removes the motion artifact simultaneously. This process is done\nby first training the network using optimal transport driven cycleGAN with\nstochastic degradation block which learns to remove aliasing artifacts and\nenhance the resolution, then using the trained network in the test stage by\nutilizing bootstrap subsampling and aggregation for motion artifact\nsuppression. We further show that we can control the trade-off between the\namount of artifact correction and resolution by controlling the bootstrap\nsubsampling ratio at the inference stage. To the best of our knowledge, the\nproposed method is the first to tackle super-resolution and motion artifact\ncorrection simultaneously in the context of MRI using unsupervised learning. We\ndemonstrate the efficiency of our method by applying it to both quantitative\nevaluation using simulation study, and to in vivo diffusion-weighted MR scans,\nwhich shows that our method is superior to the current state-of-the-art\nmethods. The proposed method is flexible in that it can be applied to various\nquality enhancement schemes in other types of MR scans, and also directly to\nthe quality enhancement of apparent diffusion coefficient maps.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:13:53 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chung", "Hyungjin", ""], ["Kim", "Jaehyun", ""], ["Yoon", "Jeong Hee", ""], ["Lee", "Jeong Min", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2105.00243", "submitter": "Yue Tan", "authors": "Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang,\n  and Chengqi Zhang", "title": "FedProto: Federated Prototype Learning over Heterogeneous Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The heterogeneity across devices usually hinders the optimization convergence\nand generalization performance of federated learning (FL) when the aggregation\nof devices' knowledge occurs in the gradient space. For example, devices may\ndiffer in terms of data distribution, network latency, input/output space,\nand/or model architecture, which can easily lead to the misalignment of their\nlocal gradients. To improve the tolerance to heterogeneity, we propose a novel\nfederated prototype learning (FedProto) framework in which the devices and\nserver communicate the class prototypes instead of the gradients. FedProto\naggregates the local prototypes collected from different devices, and then\nsends the global prototypes back to all devices to regularize the training of\nlocal models. The training on each device aims to minimize the classification\nerror on the local data while keeping the resulting local prototypes\nsufficiently close to the corresponding global ones. Through experiments, we\npropose a benchmark setting tailored for heterogeneous FL, with FedProto\noutperforming several recent FL approaches on multiple datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:21:56 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 02:05:32 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Tan", "Yue", ""], ["Long", "Guodong", ""], ["Liu", "Lu", ""], ["Zhou", "Tianyi", ""], ["Lu", "Qinghua", ""], ["Jiang", "Jing", ""], ["Zhang", "Chengqi", ""]]}, {"id": "2105.00246", "submitter": "Luca Varotto", "authors": "Luca Varotto, Angelo Cenedese", "title": "Online and Adaptive Parking Availability Mapping: An Uncertainty-Aware\n  Active Sensing Approach for Connected Vehicles", "comments": "8 pages, 3 figures, 1 table, submitted to MAVROC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Research on connected vehicles represents a continuously evolving\ntechnological domain, fostered by the emerging Internet of Things (IoT)\nparadigm and the recent advances in intelligent transportation systems.\nNowadays, vehicles are platforms capable of generating, receiving and\nautomatically act based on large amount of data. In the context of assisted\ndriving, connected vehicle technology provides real-time information about the\nsurrounding traffic conditions. Such information is expected to improve\ndrivers' quality of life, for example, by adopting decision making strategies\naccording to the current parking availability status. In this context, we\npropose an online and adaptive scheme for parking availability mapping.\nSpecifically, we adopt an information-seeking active sensing approach to select\nthe incoming data, thus preserving the onboard storage and processing\nresources; then, we estimate the parking availability through Gaussian Process\nRegression. We compare the proposed algorithm with several baselines, which\nattain inferior performance in terms of mapping convergence speed and\nadaptivity capabilities; moreover, the proposed approach comes at the cost of a\nvery small computational demand.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:35:36 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Varotto", "Luca", ""], ["Cenedese", "Angelo", ""]]}, {"id": "2105.00248", "submitter": "Zhang Chen", "authors": "Chen Zhang, Siwei Wang, Wenxuan Tu, Pei Zhang, Xinwang Liu, Changwang\n  Zhang, Bo Yuan", "title": "Multi-view Clustering with Deep Matrix Factorization and Global Graph\n  Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-view clustering is an important yet challenging task in machine\nlearning and data mining community. One popular strategy for multi-view\nclustering is matrix factorization which could explore useful feature\nrepresentations at lower-dimensional space and therefore alleviate dimension\ncurse. However, there are two major drawbacks in the existing work: i) most\nmatrix factorization methods are limited to shadow depth, which leads to the\ninability to fully discover the rich hidden information of original data. Few\ndeep matrix factorization methods provide a basis for the selection of the new\nrepresentation's dimensions of different layers. ii) the majority of current\napproaches only concentrate on the view-shared information and ignore the\nspecific local features in different views. To tackle the above issues, we\npropose a novel Multi-View Clustering method with Deep semi-NMF and Global\nGraph Refinement (MVC-DMF-GGR) in this paper. Firstly, we capture new\nrepresentation matrices for each view by hierarchical decomposition, then learn\na common graph by approximating a combination of graphs which are reconstructed\nfrom these new representations to refine the new representations in return. An\nalternate algorithm with proved convergence is then developed to solve the\noptimization problem and the results on six multi-view benchmarks demonstrate\nthe effectiveness and superiority of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:40:20 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Chen", ""], ["Wang", "Siwei", ""], ["Tu", "Wenxuan", ""], ["Zhang", "Pei", ""], ["Liu", "Xinwang", ""], ["Zhang", "Changwang", ""], ["Yuan", "Bo", ""]]}, {"id": "2105.00250", "submitter": "Zhuangwei Shi", "authors": "Zhuangwei Shi", "title": "Incorporating Transformer and LSTM to Kalman Filter with EM algorithm\n  for state estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kalman Filter requires the true parameters of the model and solves optimal\nstate estimation recursively. Expectation Maximization (EM) algorithm is\napplicable for estimating the parameters of the model that are not available\nbefore Kalman filtering, which is EM-KF algorithm. To improve the preciseness\nof EM-KF algorithm, the author presents a state estimation method by combining\nthe Long-Short Term Memory network (LSTM), Transformer and EM-KF algorithm in\nthe framework of Encoder-Decoder in Sequence to Sequence (seq2seq). Simulation\non a linear mobile robot model demonstrates that the new method is more\naccurate. Source code of this paper is available at\nhttps://github.com/zshicode/Deep-Learning-Based-State-Estimation.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:52:28 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 05:19:30 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shi", "Zhuangwei", ""]]}, {"id": "2105.00262", "submitter": "Hanjing Zhu", "authors": "Jiaming Xu and Hanjing Zhu", "title": "One-pass Stochastic Gradient Descent in Overparametrized Two-layer\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent surge of interest in understanding the convergence of\ngradient descent (GD) and stochastic gradient descent (SGD) in\noverparameterized neural networks. Most previous works assume that the training\ndata is provided a priori in a batch, while less attention has been paid to the\nimportant setting where the training data arrives in a stream. In this paper,\nwe study the streaming data setup and show that with overparamterization and\nrandom initialization, the prediction error of two-layer neural networks under\none-pass SGD converges in expectation. The convergence rate depends on the\neigen-decomposition of the integral operator associated with the so-called\nneural tangent kernel (NTK). A key step of our analysis is to show a random\nkernel function converges to the NTK with high probability using the VC\ndimension and McDiarmid's inequality.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 14:34:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xu", "Jiaming", ""], ["Zhu", "Hanjing", ""]]}, {"id": "2105.00266", "submitter": "Nicolas Boull\\'e", "authors": "Nicolas Boull\\'e, Christopher J. Earls, Alex Townsend", "title": "Data-driven discovery of physical laws with human-understandable deep\n  learning", "comments": "52 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an opportunity for deep learning to revolutionize science and\ntechnology by revealing its findings in a human interpretable manner. We\ndevelop a novel data-driven approach for creating a human-machine partnership\nto accelerate scientific discovery. By collecting physical system responses,\nunder carefully selected excitations, we train rational neural networks to\nlearn Green's functions of hidden partial differential equation. These\nsolutions reveal human-understandable properties and features, such as linear\nconservation laws, and symmetries, along with shock and singularity locations,\nboundary effects, and dominant modes. We illustrate this technique on several\nexamples and capture a range of physics, including advection-diffusion, viscous\nshocks, and Stokes flow in a lid-driven cavity.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 14:40:16 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Boull\u00e9", "Nicolas", ""], ["Earls", "Christopher J.", ""], ["Townsend", "Alex", ""]]}, {"id": "2105.00267", "submitter": "Tiago Rodrigues", "authors": "Kuan Lee, Ann Yang, Yen-Chu Lin, Daniel Reker, Goncalo J. L. Bernardes\n  and Tiago Rodrigues", "title": "Combating small molecule aggregation with machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Biological screens are plagued by false positive hits resulting from\naggregation. Thus, methods to triage small colloidally aggregating molecules\n(SCAMs) are in high demand. Herein, we disclose a bespoke machine-learning tool\nto confidently and intelligibly flag such entities. Our data demonstrate an\nunprecedented utility of machine learning for predicting SCAMs, achieving 80%\nof correct predictions in a challenging out-of-sample validation. The tool\noutperformed a panel of expert chemists, who correctly predicted 61 +/- 7% of\nthe same test molecules in a Turing-like test. Further, the computational\nroutine provided insight into molecular features governing aggregation that had\nremained hidden to expert intuition. Leveraging our tool, we quantify that up\nto 15-20% of ligands in publicly available chemogenomic databases have the high\npotential to aggregate at typical screening concentrations, imposing caution in\nsystems biology and drug design programs. Our approach provides a means to\naugment human intuition, mitigate attrition and a pathway to accelerate future\nmolecular medicine.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 14:41:01 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lee", "Kuan", ""], ["Yang", "Ann", ""], ["Lin", "Yen-Chu", ""], ["Reker", "Daniel", ""], ["Bernardes", "Goncalo J. L.", ""], ["Rodrigues", "Tiago", ""]]}, {"id": "2105.00277", "submitter": "Zhang Chen", "authors": "Chen Zhang, Siwei Wang, Jiyuan Liu, Sihang Zhou, Pei Zhang, Xinwang\n  Liu, En Zhu, Changwang Zhang", "title": "Multi-view Clustering via Deep Matrix Factorization and Partition\n  Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-view clustering (MVC) has been extensively studied to collect multiple\nsource information in recent years. One typical type of MVC methods is based on\nmatrix factorization to effectively perform dimension reduction and clustering.\nHowever, the existing approaches can be further improved with following\nconsiderations: i) The current one-layer matrix factorization framework cannot\nfully exploit the useful data representations. ii) Most algorithms only focus\non the shared information while ignore the view-specific structure leading to\nsuboptimal solutions. iii) The partition level information has not been\nutilized in existing work. To solve the above issues, we propose a novel\nmulti-view clustering algorithm via deep matrix decomposition and partition\nalignment. To be specific, the partition representations of each view are\nobtained through deep matrix decomposition, and then are jointly utilized with\nthe optimal partition representation for fusing multi-view information.\nFinally, an alternating optimization algorithm is developed to solve the\noptimization problem with proven convergence. The comprehensive experimental\nresults conducted on six benchmark multi-view datasets clearly demonstrates the\neffectiveness of the proposed algorithm against the SOTA methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:06:57 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 12:26:50 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhang", "Chen", ""], ["Wang", "Siwei", ""], ["Liu", "Jiyuan", ""], ["Zhou", "Sihang", ""], ["Zhang", "Pei", ""], ["Liu", "Xinwang", ""], ["Zhu", "En", ""], ["Zhang", "Changwang", ""]]}, {"id": "2105.00278", "submitter": "Rj Yang", "authors": "Ruijie Yang, Yunhong Wang and Yuanfang Guo", "title": "A Perceptual Distortion Reduction Framework for Adversarial Perturbation\n  Generation", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the adversarial attack methods suffer from large perceptual\ndistortions such as visible artifacts, when the attack strength is relatively\nhigh. These perceptual distortions contain a certain portion which contributes\nless to the attack success rate. This portion of distortions, which is induced\nby unnecessary modifications and lack of proper perceptual distortion\nconstraint, is the target of the proposed framework. In this paper, we propose\na perceptual distortion reduction framework to tackle this problem from two\nperspectives. We guide the perturbation addition process to reduce unnecessary\nmodifications by proposing an activated region transfer attention mask, which\nintends to transfer the activated regions of the target model from the correct\nprediction to incorrect ones. Note that an ensemble model is adopted to predict\nthe activated regions of the unseen models in the black-box setting of our\nframework. Besides, we propose a perceptual distortion constraint and add it\ninto the objective function of adversarial attack to jointly optimize the\nperceptual distortions and attack success rate. Extensive experiments have\nverified the effectiveness of our framework on several baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:08:10 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yang", "Ruijie", ""], ["Wang", "Yunhong", ""], ["Guo", "Yuanfang", ""]]}, {"id": "2105.00282", "submitter": "Tien Dung Nguyen", "authors": "Tien-Dung Nguyen, David Jacob Kedziora, Katarzyna Musial, Bogdan\n  Gabrys", "title": "Exploring Opportunistic Meta-knowledge to Reduce Search Spaces for\n  Automated Machine Learning", "comments": null, "journal-ref": "International Joint Conference on Neural Network 2021", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) pipeline composition and optimisation have been studied\nto seek multi-stage ML models, i.e. preprocessor-inclusive, that are both valid\nand well-performing. These processes typically require the design and traversal\nof complex configuration spaces consisting of not just individual ML components\nand their hyperparameters, but also higher-level pipeline structures that link\nthese components together. Optimisation efficiency and resulting ML-model\naccuracy both suffer if this pipeline search space is unwieldy and excessively\nlarge; it becomes an appealing notion to avoid costly evaluations of poorly\nperforming ML components ahead of time. Accordingly, this paper investigates\nwhether, based on previous experience, a pool of available\nclassifiers/regressors can be preemptively culled ahead of initiating a\npipeline composition/optimisation process for a new ML problem, i.e. dataset.\nThe previous experience comes in the form of classifier/regressor accuracy\nrankings derived, with loose assumptions, from a substantial but non-exhaustive\nnumber of pipeline evaluations; this meta-knowledge is considered\n'opportunistic'. Numerous experiments with the AutoWeka4MCPS package, including\nones leveraging similarities between datasets via the relative landmarking\nmethod, show that, despite its seeming unreliability, opportunistic\nmeta-knowledge can improve ML outcomes. However, results also indicate that the\nculling of classifiers/regressors should not be too severe either. In effect,\nit is better to search through a 'top tier' of recommended predictors than to\npin hopes onto one previously supreme performer.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:25:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nguyen", "Tien-Dung", ""], ["Kedziora", "David Jacob", ""], ["Musial", "Katarzyna", ""], ["Gabrys", "Bogdan", ""]]}, {"id": "2105.00290", "submitter": "Yunhao Ge", "authors": "Yunhao Ge, Yao Xiao, Zhi Xu, Meng Zheng, Srikrishna Karanam, Terrence\n  Chen, Laurent Itti, Ziyan Wu", "title": "A Peek Into the Reasoning of Neural Networks: Interpreting with\n  Structural Visual Concepts", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite substantial progress in applying neural networks (NN) to a wide\nvariety of areas, they still largely suffer from a lack of transparency and\ninterpretability. While recent developments in explainable artificial\nintelligence attempt to bridge this gap (e.g., by visualizing the correlation\nbetween input pixels and final outputs), these approaches are limited to\nexplaining low-level relationships, and crucially, do not provide insights on\nerror correction. In this work, we propose a framework (VRX) to interpret\nclassification NNs with intuitive structural visual concepts. Given a trained\nclassification model, the proposed VRX extracts relevant class-specific visual\nconcepts and organizes them using structural concept graphs (SCG) based on\npairwise concept relationships. By means of knowledge distillation, we show VRX\ncan take a step towards mimicking the reasoning process of NNs and provide\nlogical, concept-level explanations for final model decisions. With extensive\nexperiments, we empirically show VRX can meaningfully answer \"why\" and \"why\nnot\" questions about the prediction, providing easy-to-understand insights\nabout the reasoning process. We also show that these insights can potentially\nprovide guidance on improving NN's performance.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:47:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ge", "Yunhao", ""], ["Xiao", "Yao", ""], ["Xu", "Zhi", ""], ["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Chen", "Terrence", ""], ["Itti", "Laurent", ""], ["Wu", "Ziyan", ""]]}, {"id": "2105.00292", "submitter": "Jian Huang", "authors": "Guohao Shen, Yuling Jiao, Yuanyuan Lin and Jian Huang", "title": "Non-asymptotic Excess Risk Bounds for Classification with Deep\n  Convolutional Neural Networks", "comments": "Guohao Shen and Yuling Jiao contributed equally to this work.\n  Co-corresponding authors: Yuanyuan Lin (Email: ylin@sta.cuhk.edu.hk) and Jian\n  Huang (Email: jian-huang@uiowa.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the problem of binary classification with a class\nof general deep convolutional neural networks, which includes fully-connected\nneural networks and fully convolutional neural networks as special cases. We\nestablish non-asymptotic excess risk bounds for a class of convex surrogate\nlosses and target functions with different modulus of continuity. An important\nfeature of our results is that we clearly define the prefactors of the risk\nbounds in terms of the input data dimension and other model parameters and show\nthat they depend polynomially on the dimensionality in some important models.\nWe also show that the classification methods with CNNs can circumvent the curse\nof dimensionality if the input data is supported on an approximate\nlow-dimensional manifold. To establish these results, we derive an upper bound\nfor the covering number for the class of general convolutional neural networks\nwith a bias term in each convolutional layer, and derive new results on the\napproximation power of CNNs for any uniformly-continuous target functions.\nThese results provide further insights into the complexity and the\napproximation power of general convolutional neural networks, which are of\nindependent interest and may have other applications. Finally, we apply our\ngeneral results to analyze the non-asymptotic excess risk bounds for four\nwidely used methods with different loss functions using CNNs, including the\nleast squares, the logistic, the exponential and the SVM hinge losses.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:55:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Shen", "Guohao", ""], ["Jiao", "Yuling", ""], ["Lin", "Yuanyuan", ""], ["Huang", "Jian", ""]]}, {"id": "2105.00303", "submitter": "Saurabh Garg", "authors": "Saurabh Garg, Sivaraman Balakrishnan, J. Zico Kolter, Zachary C.\n  Lipton", "title": "RATT: Leveraging Unlabeled Data to Guarantee Generalization", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assess generalization, machine learning scientists typically either (i)\nbound the generalization gap and then (after training) plug in the empirical\nrisk to obtain a bound on the true risk; or (ii) validate empirically on\nholdout data. However, (i) typically yields vacuous guarantees for\noverparameterized models. Furthermore, (ii) shrinks the training set and its\nguarantee erodes with each re-use of the holdout set. In this paper, we\nintroduce a method that leverages unlabeled data to produce generalization\nbounds. After augmenting our (labeled) training set with randomly labeled fresh\nexamples, we train in the standard fashion. Whenever classifiers achieve low\nerror on clean data and high error on noisy data, our bound provides a tight\nupper bound on the true risk. We prove that our bound is valid for 0-1\nempirical risk minimization and with linear classifiers trained by gradient\ndescent. Our approach is especially useful in conjunction with deep learning\ndue to the early learning phenomenon whereby networks fit true labels before\nnoisy labels but requires one intuitive assumption. Empirically, on canonical\ncomputer vision and NLP tasks, our bound provides non-vacuous generalization\nguarantees that track actual performance closely. This work provides\npractitioners with an option for certifying the generalization of deep nets\neven when unseen labeled data is unavailable and provides theoretical insights\ninto the relationship between random label noise and generalization.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 17:05:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Garg", "Saurabh", ""], ["Balakrishnan", "Sivaraman", ""], ["Kolter", "J. Zico", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "2105.00304", "submitter": "Oliver T. Unke", "authors": "Oliver T. Unke, Stefan Chmiela, Michael Gastegger, Kristof T.\n  Sch\\\"utt, Huziel E. Sauceda, Klaus-Robert M\\\"uller", "title": "SpookyNet: Learning Force Fields with Electronic Degrees of Freedom and\n  Nonlocal Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learned force fields (ML-FFs) combine the accuracy of ab initio\nmethods with the efficiency of conventional force fields. However, current\nML-FFs typically ignore electronic degrees of freedom, such as the total charge\nor spin state, and assume chemical locality, which is problematic when\nmolecules have inconsistent electronic states, or when nonlocal effects play a\nsignificant role. This work introduces SpookyNet, a deep neural network for\nconstructing ML-FFs with explicit treatment of electronic degrees of freedom\nand quantum nonlocality. Chemically meaningful inductive biases and analytical\ncorrections built into the network architecture allow it to properly model\nphysical limits. SpookyNet improves upon the current state-of-the-art (or\nachieves similar performance) on popular quantum chemistry data sets. Notably,\nit is able to generalize across chemical and conformational space and can\nleverage the learned chemical insights, e.g. by predicting unknown spin states,\nthus helping to close a further important remaining gap for today's machine\nlearning models in quantum chemistry.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 17:06:40 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 10:33:01 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Unke", "Oliver T.", ""], ["Chmiela", "Stefan", ""], ["Gastegger", "Michael", ""], ["Sch\u00fctt", "Kristof T.", ""], ["Sauceda", "Huziel E.", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "2105.00312", "submitter": "Hussein Ali Jaafar", "authors": "Alexandru-Iosif Toma, Hussein Ali Jaafar, Hao-Ya Hsueh, Stephen James,\n  Daniel Lenton, Ronald Clark, Sajad Saeedi", "title": "Waypoint Planning Networks", "comments": "The Conference on Robots and Vision (CRV2021) Supplementary Website:\n  https://sites.google.com/view/waypoint-planning-networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances in machine learning, path planning algorithms are\nalso evolving; however, the learned path planning algorithms often have\ndifficulty competing with success rates of classic algorithms. We propose\nwaypoint planning networks (WPN), a hybrid algorithm based on LSTMs with a\nlocal kernel - a classic algorithm such as A*, and a global kernel using a\nlearned algorithm. WPN produces a more computationally efficient and robust\nsolution. We compare WPN against A*, as well as related works including motion\nplanning networks (MPNet) and value iteration networks (VIN). In this paper,\nthe design and experiments have been conducted for 2D environments.\nExperimental results outline the benefits of WPN, both in efficiency and\ngeneralization. It is shown that WPN's search space is considerably less than\nA*, while being able to generate near optimal results. Additionally, WPN works\non partial maps, unlike A* which needs the full map in advance. The code is\navailable online.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 18:02:01 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Toma", "Alexandru-Iosif", ""], ["Jaafar", "Hussein Ali", ""], ["Hsueh", "Hao-Ya", ""], ["James", "Stephen", ""], ["Lenton", "Daniel", ""], ["Clark", "Ronald", ""], ["Saeedi", "Sajad", ""]]}, {"id": "2105.00315", "submitter": "Preethi V", "authors": "Preethi V, Nachiappan Sundaram, Ravindra Babu Tallamraju", "title": "Online Fashion Commerce: Modelling Customer Promise Date", "comments": null, "journal-ref": "SDM Data Science for Retail and E-Commerce Workshop (2021)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the e-commerce space, accurate prediction of delivery dates plays a major\nrole in customer experience as well as in optimizing the supply chain\noperations. Predicting a date later than the actual delivery date might\nsometimes result in the customer not placing the order (lost sales) while\npromising a date earlier than the actual delivery date would lead to a bad\ncustomer experience and consequent customer churn. In this paper, we present a\nmachine learning-based approach for penalizing incorrect predictions\ndifferently using non-conventional loss functions, while working under various\nuncertainties involved in making successful deliveries such as traffic\ndisruptions, weather conditions, supply chain, and logistics. We examine\nstatistical, deep learning, and conventional machine learning approaches, and\nwe propose an approach that outperformed the pre-existing rule-based models.\nThe proposed model is deployed internally for Fashion e-Commerce and is\noperational.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 18:16:12 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["V", "Preethi", ""], ["Sundaram", "Nachiappan", ""], ["Tallamraju", "Ravindra Babu", ""]]}, {"id": "2105.00321", "submitter": "Xinlei Yi", "authors": "Xinlei Yi, Xiuxian Li, Tao Yang, Lihua Xie, Tianyou Chai, and Karl H.\n  Johansson", "title": "Regret and Cumulative Constraint Violation Analysis for Distributed\n  Online Constrained Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the distributed online convex optimization problem with\ntime-varying constraints over a network of agents. This is a sequential\ndecision making problem with two sequences of arbitrarily varying convex loss\nand constraint functions. At each round, each agent selects a decision from the\ndecision set, and then only a portion of the loss function and a coordinate\nblock of the constraint function at this round are privately revealed to this\nagent. The goal of the network is to minimize network regret and constraint\nviolation. Two distributed online algorithms with full-information and bandit\nfeedback are proposed. Both dynamic and static network regret bounds are\nanalyzed for the proposed algorithms, and network cumulative constraint\nviolation is used to measure constraint violation, which excludes the situation\nthat strictly feasible constraints can compensate the effects of violated\nconstraints. In particular, we show that the proposed algorithms achieve\n$\\mathcal{O}(T^{\\max\\{\\kappa,1-\\kappa\\}})$ static network regret and\n$\\mathcal{O}(T^{1-\\kappa/2})$ network cumulative constraint violation, where\n$T$ is the total number of rounds and $\\kappa\\in(0,1)$ is a user-defined\ntrade-off parameter. Moreover, if the loss functions are strongly convex, then\nthe static network regret bound can be reduced to $\\mathcal{O}(T^{\\kappa})$.\nFinally, numerical simulations are provided to illustrate the effectiveness of\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 18:28:53 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yi", "Xinlei", ""], ["Li", "Xiuxian", ""], ["Yang", "Tao", ""], ["Xie", "Lihua", ""], ["Chai", "Tianyou", ""], ["Johansson", "Karl H.", ""]]}, {"id": "2105.00324", "submitter": "Fangfang Xia", "authors": "Zixuan Zhao, Nathan Wycoff, Neil Getty, Rick Stevens, Fangfang Xia", "title": "Neko: a Library for Exploring Neuromorphic Learning Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of neuromorphic computing is in a period of active exploration.\nWhile many tools have been developed to simulate neuronal dynamics or convert\ndeep networks to spiking models, general software libraries for learning rules\nremain underexplored. This is partly due to the diverse, challenging nature of\nefforts to design new learning rules, which range from encoding methods to\ngradient approximations, from population approaches that mimic the Bayesian\nbrain to constrained learning algorithms deployed on memristor crossbars. To\naddress this gap, we present Neko, a modular, extensible library with a focus\non aiding the design of new learning algorithms. We demonstrate the utility of\nNeko in three exemplar cases: online local learning, probabilistic learning,\nand analog on-device learning. Our results show that Neko can replicate the\nstate-of-the-art algorithms and, in one case, lead to significant\noutperformance in accuracy and speed. Further, it offers tools including\ngradient comparison that can help develop new algorithmic variants. Neko is an\nopen source Python library that supports PyTorch and TensorFlow backends.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 18:50:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhao", "Zixuan", ""], ["Wycoff", "Nathan", ""], ["Getty", "Neil", ""], ["Stevens", "Rick", ""], ["Xia", "Fangfang", ""]]}, {"id": "2105.00333", "submitter": "Stefanos Kollias", "authors": "Ilianna Kollia and Jack Stevenson and Stefanos Kollias", "title": "AI-enabled Efficient and Safe Food Supply Chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper provides a review of an emerging field in the food processing\nsector, referring to efficient and safe food supply chains, from farm to fork,\nas enabled by Artificial Intelligence (AI). Recent advances in machine and deep\nlearning are used for effective food production, energy management and food\nlabeling. Appropriate deep neural architectures are adopted and used for this\npurpose, including Fully Convolutional Networks, Long Short-Term Memories and\nRecurrent Neural Networks, Auto-Encoders and Attention mechanisms, Latent\nVariable extraction and clustering, as well as Domain Adaptation. Three\nexperimental studies are presented, illustrating the ability of these AI\nmethodologies to produce state-of-the-art performance in the whole food supply\nchain. In particular, these concern: (i) predicting plant growth and tomato\nyield in greenhouses, thus matching food production to market needs and\nreducing food waste or food unavailability; (ii) optimizing energy consumption\nacross large networks of food retail refrigeration systems, through optimal\nselection of systems that can get shut-down and through prediction of the\nrespective food de-freezing times, during peaks of power demand load; (iii)\noptical recognition and verification of food consumption expiry date in\nautomatic inspection of retail packaged food, thus ensuring safety of food and\npeople's health.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 19:24:53 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kollia", "Ilianna", ""], ["Stevenson", "Jack", ""], ["Kollias", "Stefanos", ""]]}, {"id": "2105.00334", "submitter": "Hanieh Hashemi", "authors": "Hanieh Hashemi, Yongqin Wang, Murali Annavaram", "title": "Privacy and Integrity Preserving Training Using Trusted Hardware", "comments": null, "journal-ref": "Distributed and Private Machine Learning ICLR 2021 Workshop", "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy and security-related concerns are growing as machine learning reaches\ndiverse application domains. The data holders want to train with private data\nwhile exploiting accelerators, such as GPUs, that are hosted in the cloud.\nHowever, Cloud systems are vulnerable to attackers that compromise the privacy\nof data and integrity of computations. This work presents DarKnight, a\nframework for large DNN training while protecting input privacy and computation\nintegrity. DarKnight relies on cooperative execution between trusted execution\nenvironments (TEE) and accelerators, where the TEE provides privacy and\nintegrity verification, while accelerators perform the computation heavy linear\nalgebraic operations.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 19:33:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hashemi", "Hanieh", ""], ["Wang", "Yongqin", ""], ["Annavaram", "Murali", ""]]}, {"id": "2105.00335", "submitter": "Prateek Verma", "authors": "Prateek Verma and Jonathan Berger", "title": "Audio Transformers:Transformer Architectures For Large Scale Audio\n  Understanding. Adieu Convolutions", "comments": "5 pages, 4 figures; Under review WASPAA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, CNN architectures have produced compelling models\nof sound perception and cognition, learning hierarchical organizations of\nfeatures. Analogous to successes in computer vision, audio feature\nclassification can be optimized for a particular task of interest, over a wide\nvariety of datasets and labels. In fact similar architectures designed for\nimage understanding have proven effective for acoustic scene analysis. Here we\npropose applying Transformer based architectures without convolutional layers\nto raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200\ncategories, our model outperforms convolutional models to produce state of the\nart results. This is significant as unlike in natural language processing and\ncomputer vision, we do not perform unsupervised pre-training for outperforming\nconvolutional architectures. On the same training set, with respect mean\naver-age precision benchmarks, we show a significant improvement. We further\nimprove the performance of Transformer architectures by using techniques such\nas pooling inspired from convolutional net-work designed in the past few years.\nIn addition, we also show how multi-rate signal processing ideas inspired from\nwavelets, can be applied to the Transformer embeddings to improve the results.\nWe also show how our models learns a non-linear non constant band-width\nfilter-bank, which shows an adaptable time frequency front end representation\nfor the task of audio understanding, different from other tasks e.g. pitch\nestimation.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 19:38:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Verma", "Prateek", ""], ["Berger", "Jonathan", ""]]}, {"id": "2105.00336", "submitter": "M Tanveer PhD", "authors": "M. Tanveer and T. Rajani and R. Rastogi and Y.H. Shao", "title": "Comprehensive Review On Twin Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twin support vector machine (TSVM) and twin support vector regression (TSVR)\nare newly emerging efficient machine learning techniques which offer promising\nsolutions for classification and regression challenges respectively. TSVM is\nbased upon the idea to identify two nonparallel hyperplanes which classify the\ndata points to their respective classes. It requires to solve two small sized\nquadratic programming problems (QPPs) in lieu of solving single large size QPP\nin support vector machine (SVM) while TSVR is formulated on the lines of TSVM\nand requires to solve two SVM kind problems. Although there has been good\nresearch progress on these techniques; there is limited literature on the\ncomparison of different variants of TSVR. Thus, this review presents a rigorous\nanalysis of recent research in TSVM and TSVR simultaneously mentioning their\nlimitations and advantages. To begin with we first introduce the basic theory\nof TSVM and then focus on the various improvements and applications of TSVM,\nand then we introduce TSVR and its various enhancements. Finally, we suggest\nfuture research and development prospects.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 19:48:45 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Tanveer", "M.", ""], ["Rajani", "T.", ""], ["Rastogi", "R.", ""], ["Shao", "Y. H.", ""]]}, {"id": "2105.00339", "submitter": "Saeed Khorram", "authors": "Saeed Khorram, Xiao Fu, Mohamad H. Danesh, Zhongang Qi, Li Fuxin", "title": "Stochastic Block-ADMM for Training Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose Stochastic Block-ADMM as an approach to train deep\nneural networks in batch and online settings. Our method works by splitting\nneural networks into an arbitrary number of blocks and utilizes auxiliary\nvariables to connect these blocks while optimizing with stochastic gradient\ndescent. This allows training deep networks with non-differentiable constraints\nwhere conventional backpropagation is not applicable. An application of this is\nsupervised feature disentangling, where our proposed DeepFacto inserts a\nnon-negative matrix factorization (NMF) layer into the network. Since\nbackpropagation only needs to be performed within each block, our approach\nalleviates vanishing gradients and provides potentials for parallelization. We\nprove the convergence of our proposed method and justify its capabilities\nthrough experiments in supervised and weakly-supervised settings.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 19:56:13 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Khorram", "Saeed", ""], ["Fu", "Xiao", ""], ["Danesh", "Mohamad H.", ""], ["Qi", "Zhongang", ""], ["Fuxin", "Li", ""]]}, {"id": "2105.00349", "submitter": "Andrea Castellani", "authors": "Andrea Castellani, Sebastian Schmitt, and Barbara Hammer", "title": "Estimating the electrical power output of industrial devices with\n  end-to-end time-series classification in the presence of label noise", "comments": "Accepted in Applied Data Science track at ECML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In complex industrial settings, it is common practice to monitor the\noperation of machines in order to detect undesired states, adjust maintenance\nschedules, optimize system performance or collect usage statistics of\nindividual machines. In this work, we focus on estimating the power output of a\nCombined Heat and Power (CHP) machine of a medium-sized company facility by\nanalyzing the total facility power consumption. We formulate the problem as a\ntime-series classification problem where the class label represents the CHP\npower output. As the facility is fully instrumented and sensor measurements\nfrom the CHP are available, we generate the training labels in an automated\nfashion from the CHP sensor readings. However, sensor failures result in\nmislabeled training data samples which are hard to detect and remove from the\ndataset. Therefore, we propose a novel multi-task deep learning approach that\njointly trains a classifier and an autoencoder with a shared embedding\nrepresentation. The proposed approach targets to gradually correct the\nmislabelled data samples during training in a self-supervised fashion, without\nany prior assumption on the amount of label noise. We benchmark our approach on\nseveral time-series classification datasets and find it to be comparable and\nsometimes better than state-of-the-art methods. On the real-world use-case of\npredicting the CHP power output, we thoroughly evaluate the architectural\ndesign choices and show that the final architecture considerably increases the\nrobustness of the learning process and consistently beats other recent\nstate-of-the-art algorithms in the presence of unstructured as well as\nstructured label noise.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 21:45:42 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:21:07 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Castellani", "Andrea", ""], ["Schmitt", "Sebastian", ""], ["Hammer", "Barbara", ""]]}, {"id": "2105.00351", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Hernando Ombao", "title": "Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus\n  Spike Proteins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.BM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Topological data analysis, including persistent homology, has undergone\nsignificant development in recent years. However, one outstanding challenge is\nto build a coherent statistical inference procedure on persistent diagrams. The\npaired dependent data structure, which are the births and deaths in persistent\ndiagrams, adds complexity to statistical inference. In this paper, we present a\nnew lattice path representation for persistent diagrams. A new exact\nstatistical inference procedure is developed for lattice paths via\ncombinatorial enumerations. The proposed lattice path method is applied to\nstudy the topological characterization of the protein structures of the\nCOVID-19 virus. We demonstrate that there are topological changes during the\nconformational change of spike proteins, a necessary step in infecting host\ncells.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 22:12:57 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 07:45:49 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 12:34:39 GMT"}, {"version": "v4", "created": "Sat, 26 Jun 2021 20:21:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chung", "Moo K.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2105.00357", "submitter": "Vlad Velici", "authors": "Vlad Velici, Adam Pr\\\"ugel-Bennett", "title": "RotLSTM: Rotating Memories in Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Long Short-Term Memory (LSTM) units have the ability to memorise and use\nlong-term dependencies between inputs to generate predictions on time series\ndata. We introduce the concept of modifying the cell state (memory) of LSTMs\nusing rotation matrices parametrised by a new set of trainable weights. This\naddition shows significant increases of performance on some of the tasks from\nthe bAbI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 23:48:58 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Velici", "Vlad", ""], ["Pr\u00fcgel-Bennett", "Adam", ""]]}, {"id": "2105.00371", "submitter": "KangKang Yin", "authors": "Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin", "title": "Discovering Diverse Athletic Jumping Strategies", "comments": "17 pages; SIGGRAPH 2021", "journal-ref": "ACM Trans. Graph. 40, 4, Article 91 (August 2021), 17 pages (2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework that enables the discovery of diverse and\nnatural-looking motion strategies for athletic skills such as the high jump.\nThe strategies are realized as control policies for physics-based characters.\nGiven a task objective and an initial character configuration, the combination\nof physics simulation and deep reinforcement learning (DRL) provides a suitable\nstarting point for automatic control policy training. To facilitate the\nlearning of realistic human motions, we propose a Pose Variational Autoencoder\n(P-VAE) to constrain the actions to a subspace of natural poses. In contrast to\nmotion imitation methods, a rich variety of novel strategies can naturally\nemerge by exploring initial character states through a sample-efficient\nBayesian diversity search (BDS) algorithm. A second stage of optimization that\nencourages novel policies can further enrich the unique strategies discovered.\nOur method allows for the discovery of diverse and novel strategies for\nathletic jumping motions such as high jumps and obstacle jumps with no motion\nexamples and less reward engineering than prior work.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 01:37:16 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yin", "Zhiqi", ""], ["Yang", "Zeshi", ""], ["van de Panne", "Michiel", ""], ["Yin", "KangKang", ""]]}, {"id": "2105.00373", "submitter": "Sharad Chitlangia", "authors": "Sharad Chitlangia, Zuxin Liu, Akhil Agnihotri, Ding Zhao", "title": "Improving Perception via Sensor Placement: Designing Multi-LiDAR Systems\n  for Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed an increasing interest in improving the\nperception performance of LiDARs on autonomous vehicles. While most of the\nexisting works focus on developing novel model architectures to process point\ncloud data, we study the problem from an optimal sensing perspective. To this\nend, together with a fast evaluation function based on ray tracing within the\nperception region of a LiDAR configuration, we propose an easy-to-compute\ninformation-theoretic surrogate cost metric based on Probabilistic Occupancy\nGrids (POG) to optimize LiDAR placement for maximal sensing. We show a\ncorrelation between our surrogate function and common object detection\nperformance metrics. We demonstrate the efficacy of our approach by verifying\nour results in a robust and reproducible data collection and extraction\nframework based on the CARLA simulator. Our results confirm that sensor\nplacement is an important factor in 3D point cloud-based object detection and\ncould lead to a variation of performance by 10% ~ 20% on the state-of-the-art\nperception algorithms. We believe that this is one of the first studies to use\nLiDAR placement to improve the performance of perception.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 01:52:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chitlangia", "Sharad", ""], ["Liu", "Zuxin", ""], ["Agnihotri", "Akhil", ""], ["Zhao", "Ding", ""]]}, {"id": "2105.00374", "submitter": "Kumar Abhishek", "authors": "Mengliu Zhao, Jeremy Kawahara, Sajjad Shamanian, Kumar Abhishek,\n  Priyanka Chandrashekar, Ghassan Hamarneh", "title": "Detection and Longitudinal Tracking of Pigmented Skin Lesions in 3D\n  Total-Body Skin Textured Meshes", "comments": "12 pages, 11 figures; Zhao and Kawahara: joint first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automated approach to detect and longitudinally track skin\nlesions on 3D total-body skin surfaces scans. The acquired 3D mesh of the\nsubject is unwrapped to a 2D texture image, where a trained region\nconvolutional neural network (R-CNN) localizes the lesions within the 2D\ndomain. These detected skin lesions are mapped back to the 3D surface of the\nsubject and, for subjects imaged multiple times, the anatomical correspondences\namong pairs of meshes and the geodesic distances among lesions are leveraged in\nour longitudinal lesion tracking algorithm.\n  We evaluated the proposed approach using three sources of data. Firstly, we\naugmented the 3D meshes of human subjects from the public FAUST dataset with a\nvariety of poses, textures, and images of lesions. Secondly, using a handheld\nstructured light 3D scanner, we imaged a mannequin with multiple synthetic skin\nlesions at selected location and with varying shapes, sizes, and colours.\nFinally, we used 3DBodyTex, a publicly available dataset composed of 3D scans\nimaging the colored (textured) skin of 200 human subjects. We manually\nannotated locations that appeared to the human eye to contain a pigmented skin\nlesion as well as tracked a subset of lesions occurring on the same subject\nimaged in different poses.\n  Our results, on test subjects annotated by three human annotators, suggest\nthat the trained R-CNN detects lesions at a similar performance level as the\nhuman annotators. Our lesion tracking algorithm achieves an average accuracy of\n80% when identifying corresponding pairs of lesions across subjects imaged in\ndifferent poses. As there currently is no other large-scale publicly available\ndataset of 3D total-body skin lesions, we publicly release the 10 mannequin\nmeshes and over 25,000 3DBodyTex manual annotations, which we hope will further\nresearch on total-body skin lesion analysis.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 01:52:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhao", "Mengliu", ""], ["Kawahara", "Jeremy", ""], ["Shamanian", "Sajjad", ""], ["Abhishek", "Kumar", ""], ["Chandrashekar", "Priyanka", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "2105.00376", "submitter": "Lijun Sun Mr", "authors": "Jiawei Wang and Lijun Sun", "title": "Reducing Bus Bunching with Asynchronous Multi-Agent Reinforcement\n  Learning", "comments": "IJCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bus system is a critical component of sustainable urban transportation.\nHowever, due to the significant uncertainties in passenger demand and traffic\nconditions, bus operation is unstable in nature and bus bunching has become a\ncommon phenomenon that undermines the reliability and efficiency of bus\nservices. Despite recent advances in multi-agent reinforcement learning (MARL)\non traffic control, little research has focused on bus fleet control due to the\ntricky asynchronous characteristic -- control actions only happen when a bus\narrives at a bus stop and thus agents do not act simultaneously. In this study,\nwe formulate route-level bus fleet control as an asynchronous multi-agent\nreinforcement learning (ASMR) problem and extend the classical actor-critic\narchitecture to handle the asynchronous issue. Specifically, we design a novel\ncritic network to effectively approximate the marginal contribution for other\nagents, in which graph attention neural network is used to conduct inductive\nlearning for policy evaluation. The critic structure also helps the ego agent\noptimize its policy more efficiently. We evaluate the proposed framework on\nreal-world bus services and actual passenger demand derived from smart card\ndata. Our results show that the proposed model outperforms both traditional\nheadway-based control methods and existing MARL methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 02:08:07 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 13:06:07 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Jiawei", ""], ["Sun", "Lijun", ""]]}, {"id": "2105.00385", "submitter": "Zachary Pardos", "authors": "Anirudhan Badrinath, Frederic Wang, Zachary Pardos", "title": "pyBKT: An Accessible Python Library of Bayesian Knowledge Tracing Models", "comments": "Accepted to the 2021 Conference on Educational Data Mining (EDM '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Knowledge Tracing, a model used for cognitive mastery estimation,\nhas been a hallmark of adaptive learning research and an integral component of\ndeployed intelligent tutoring systems (ITS). In this paper, we provide a brief\nhistory of knowledge tracing model research and introduce pyBKT, an accessible\nand computationally efficient library of model extensions from the literature.\nThe library provides data generation, fitting, prediction, and cross-validation\nroutines, as well as a simple to use data helper interface to ingest typical\ntutor log dataset formats. We evaluate the runtime with various dataset sizes\nand compare to past implementations. Additionally, we conduct sanity checks of\nthe model using experiments with simulated data to evaluate the accuracy of its\nEM parameter learning and use real-world data to validate its predictions,\ncomparing pyBKT's supported model variants with results from the papers in\nwhich they were originally introduced. The library is open source and open\nlicense for the purpose of making knowledge tracing more accessible to\ncommunities of research and practice and to facilitate progress in the field\nthrough easier replication of past approaches.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 03:08:53 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 04:20:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Badrinath", "Anirudhan", ""], ["Wang", "Frederic", ""], ["Pardos", "Zachary", ""]]}, {"id": "2105.00395", "submitter": "Yusuke Koda", "authors": "Yusuke Koda and Jihong Park and Mehdi Bennis and Praneeth Vepakomma\n  and Ramesh Raskar", "title": "AirMixML: Over-the-Air Data Mixup for Inherently Privacy-Preserving Edge\n  Machine Learning", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless channels can be inherently privacy-preserving by distorting the\nreceived signals due to channel noise, and superpositioning multiple signals\nover-the-air. By harnessing these natural distortions and superpositions by\nwireless channels, we propose a novel privacy-preserving machine learning (ML)\nframework at the network edge, coined over-the-air mixup ML (AirMixML). In\nAirMixML, multiple workers transmit analog-modulated signals of their private\ndata samples to an edge server who trains an ML model using the received\nnoisy-and superpositioned samples. AirMixML coincides with model training using\nmixup data augmentation achieving comparable accuracy to that with raw data\nsamples. From a privacy perspective, AirMixML is a differentially private (DP)\nmechanism limiting the disclosure of each worker's private sample information\nat the server, while the worker's transmit power determines the privacy\ndisclosure level. To this end, we develop a fractional channel-inversion power\ncontrol (PC) method, {\\alpha}-Dirichlet mixup PC (DirMix({\\alpha})-PC), wherein\nfor a given global power scaling factor after channel inversion, each worker's\nlocal power contribution to the superpositioned signal is controlled by the\nDirichlet dispersion ratio {\\alpha}. Mathematically, we derive a closed-form\nexpression clarifying the relationship between the local and global PC factors\nto guarantee a target DP level. By simulations, we provide DirMix({\\alpha})-PC\ndesign guidelines to improve accuracy, privacy, and energy-efficiency. Finally,\nAirMixML with DirMix({\\alpha})-PC is shown to achieve reasonable accuracy\ncompared to a privacy-violating baseline with neither superposition nor PC.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 05:45:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Koda", "Yusuke", ""], ["Park", "Jihong", ""], ["Bennis", "Mehdi", ""], ["Vepakomma", "Praneeth", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2105.00397", "submitter": "Qianyu Feng", "authors": "Qianyu Feng, Linchao Zhu, Bang Zhang, Pan Pan, Yi Yang", "title": "OR-Net: Pointwise Relational Inference for Data Completion under Partial\n  Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contemporary data-driven methods are typically fed with full supervision on\nlarge-scale datasets which limits their applicability. However, in the actual\nsystems with limitations such as measurement error and data acquisition\nproblems, people usually obtain incomplete data. Although data completion has\nattracted wide attention, the underlying data pattern and relativity are still\nunder-developed. Currently, the family of latent variable models allows\nlearning deep latent variables over observed variables by fitting the marginal\ndistribution. As far as we know, current methods fail to perceive the data\nrelativity under partial observation. Aiming at modeling incomplete data, this\nwork uses relational inference to fill in the incomplete data. Specifically, we\nexpect to approximate the real joint distribution over the partial observation\nand latent variables, thus infer the unseen targets respectively. To this end,\nwe propose Omni-Relational Network (OR-Net) to model the pointwise relativity\nin two aspects: (i) On one hand, the inner relationship is built among the\ncontext points in the partial observation; (ii) On the other hand, the unseen\ntargets are inferred by learning the cross-relationship with the observed data\npoints. It is further discovered that the proposed method can be generalized to\ndifferent scenarios regardless of whether the physical structure can be\nobserved or not. It is demonstrated that the proposed OR-Net can be well\ngeneralized for data completion tasks of various modalities, including function\nregression, image completion on MNIST and CelebA datasets, and also sequential\nmotion generation conditioned on the observed poses.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 06:05:54 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 09:28:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Feng", "Qianyu", ""], ["Zhu", "Linchao", ""], ["Zhang", "Bang", ""], ["Pan", "Pan", ""], ["Yang", "Yi", ""]]}, {"id": "2105.00408", "submitter": "Arkadiy Skopenkov", "authors": "S. Dzhenzher and A. Skopenkov", "title": "A structured proof of the Kolmogorov superposition theorem", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a well-structured detailed exposition of a well-known proof of the\nfollowing celebrated result solving Hilbert's 13th problem on superpositions.\nFor functions of 2 variables the statement is as follows.\n  Kolmogorov Theorem. There are continuous functions\n$\\varphi_1,\\ldots,\\varphi_5 : [\\,0, 1\\,]\\to [\\,0,1\\,]$ such that for any\ncontinuous function $f: [\\,0,1\\,]^2\\to\\mathbb R$ there is a continuous function\n$h: [\\,0,3\\,]\\to\\mathbb R$ such that for any $x,y\\in [\\,0, 1\\,]$ we have\n$$f(x,y)=\\sum\\limits_{k=1}^5\nh\\left(\\varphi_k(x)+\\sqrt{2}\\,\\varphi_k(y)\\right).$$ The proof is accessible to\nnon-specialists, in particular, to students familiar with only basic properties\nof continuous functions.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 07:35:01 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dzhenzher", "S.", ""], ["Skopenkov", "A.", ""]]}, {"id": "2105.00412", "submitter": "Chenxi Sun", "authors": "Chenxi Sun and Shenda Hong and Moxian Song and Yanxiu Zhou and Yongyue\n  Sun and Derun Cai and Hongyan Li", "title": "TE-ESN: Time Encoding Echo State Network for Prediction Based on\n  Irregularly Sampled Time Series Data", "comments": "7 pages, 4 figures, accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prediction based on Irregularly Sampled Time Series (ISTS) is of wide concern\nin the real-world applications. For more accurate prediction, the methods had\nbetter grasp more data characteristics. Different from ordinary time series,\nISTS is characterised with irregular time intervals of intra-series and\ndifferent sampling rates of inter-series. However, existing methods have\nsuboptimal predictions due to artificially introducing new dependencies in a\ntime series and biasedly learning relations among time series when modeling\nthese two characteristics. In this work, we propose a novel Time Encoding (TE)\nmechanism. TE can embed the time information as time vectors in the complex\ndomain. It has the the properties of absolute distance and relative distance\nunder different sampling rates, which helps to represent both two\nirregularities of ISTS. Meanwhile, we create a new model structure named Time\nEncoding Echo State Network (TE-ESN). It is the first ESNs-based model that can\nprocess ISTS data. Besides, TE-ESN can incorporate long short-term memories and\nseries fusion to grasp horizontal and vertical relations. Experiments on one\nchaos system and three real-world datasets show that TE-ESN performs better\nthan all baselines and has better reservoir property.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 08:00:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sun", "Chenxi", ""], ["Hong", "Shenda", ""], ["Song", "Moxian", ""], ["Zhou", "Yanxiu", ""], ["Sun", "Yongyue", ""], ["Cai", "Derun", ""], ["Li", "Hongyan", ""]]}, {"id": "2105.00421", "submitter": "Yeyun Zou", "authors": "Yeyun Zou, Qiyu Xie", "title": "A survey on VQA_Datasets and Approaches", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/ITCA52113.2020.00069", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual question answering (VQA) is a task that combines both the techniques\nof computer vision and natural language processing. It requires models to\nanswer a text-based question according to the information contained in a\nvisual. In recent years, the research field of VQA has been expanded. Research\nthat focuses on the VQA, examining the reasoning ability and VQA on scientific\ndiagrams, has also been explored more. Meanwhile, more multimodal feature\nfusion mechanisms have been proposed. This paper will review and analyze\nexisting datasets, metrics, and models proposed for the VQA task.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 08:50:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zou", "Yeyun", ""], ["Xie", "Qiyu", ""]]}, {"id": "2105.00425", "submitter": "Islem Rekik", "authors": "Megi Isallari and Islem Rekik", "title": "Brain Graph Super-Resolution Using Adversarial Graph Neural Network with\n  Application to Functional Brain Connectivity", "comments": "arXiv admin note: text overlap with arXiv:2009.11080", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Brain image analysis has advanced substantially in recent years with the\nproliferation of neuroimaging datasets acquired at different resolutions. While\nresearch on brain image super-resolution has undergone a rapid development in\nthe recent years, brain graph super-resolution is still poorly investigated\nbecause of the complex nature of non-Euclidean graph data. In this paper, we\npropose the first-ever deep graph super-resolution (GSR) framework that\nattempts to automatically generate high-resolution (HR) brain graphs with N'\nnodes (i.e., anatomical regions of interest (ROIs)) from low-resolution (LR)\ngraphs with N nodes where N < N'. First, we formalize our GSR problem as a node\nfeature embedding learning task. Once the HR nodes' embeddings are learned, the\npairwise connectivity strength between brain ROIs can be derived through an\naggregation rule based on a novel Graph U-Net architecture. While typically the\nGraph U-Net is a node-focused architecture where graph embedding depends mainly\non node attributes, we propose a graph-focused architecture where the node\nfeature embedding is based on the graph topology. Second, inspired by graph\nspectral theory, we break the symmetry of the U-Net architecture by\nsuper-resolving the low-resolution brain graph structure and node content with\na GSR layer and two graph convolutional network layers to further learn the\nnode embeddings in the HR graph. Third, to handle the domain shift between the\nground-truth and the predicted HR brain graphs, we incorporate adversarial\nregularization to align their respective distributions. Our proposed AGSR-Net\nframework outperformed its variants for predicting high-resolution functional\nbrain graphs from low-resolution ones. Our AGSR-Net code is available on GitHub\nat https://github.com/basiralab/AGSR-Net.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 09:09:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Isallari", "Megi", ""], ["Rekik", "Islem", ""]]}, {"id": "2105.00429", "submitter": "Sarthak Gupta", "authors": "Sarthak Gupta and Vassilis Kekatos and Ming Jin", "title": "Controlling Smart Inverters using Proxies: A Chance-Constrained\n  DNN-based Approach", "comments": "Submitted to IEEE Transactions on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinating inverters at scale under uncertainty is the desideratum for\nintegrating renewables in distribution grids. Unless load demands and solar\ngeneration are telemetered frequently, controlling inverters given approximate\ngrid conditions or proxies thereof becomes a key specification. Although deep\nneural networks (DNNs) can learn optimal inverter schedules, guaranteeing\nfeasibility is largely elusive. Rather than training DNNs to imitate already\ncomputed optimal power flow (OPF) solutions, this work integrates DNN-based\ninverter policies into the OPF. The proposed DNNs are trained through two OPF\nalternatives that confine voltage deviations on the average and as a convex\nrestriction of chance constraints. The trained DNNs can be driven by partial,\nnoisy, or proxy descriptors of the current grid conditions. This is important\nwhen OPF has to be solved for an unobservable feeder. DNN weights are trained\nvia back-propagation and upon differentiating the AC power flow equations\nassuming the network model is known. Otherwise, a gradient-free variant is put\nforth. The latter is relevant when inverters are controlled by an aggregator\nhaving access only to a power flow solver or a digital twin of the feeder.\nNumerical tests compare the DNN-based inverter control schemes with the optimal\ninverter setpoints in terms of optimality and feasibility.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 09:21:41 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Gupta", "Sarthak", ""], ["Kekatos", "Vassilis", ""], ["Jin", "Ming", ""]]}, {"id": "2105.00433", "submitter": "Ziv Katzir", "authors": "Ziv Katzir, Yuval Elovici", "title": "Who's Afraid of Adversarial Transferability?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial transferability, namely the ability of adversarial perturbations\nto simultaneously fool multiple learning models, has long been the \"big bad\nwolf\" of adversarial machine learning. Successful transferability-based attacks\nrequiring no prior knowledge of the attacked model's parameters or training\ndata have been demonstrated numerous times in the past, implying that machine\nlearning models pose an inherent security threat to real-life systems. However,\nall of the research performed in this area regarded transferability as a\nprobabilistic property and attempted to estimate the percentage of adversarial\nexamples that are likely to mislead a target model given some predefined\nevaluation set. As a result, those studies ignored the fact that real-life\nadversaries are often highly sensitive to the cost of a failed attack. We argue\nthat overlooking this sensitivity has led to an exaggerated perception of the\ntransferability threat, when in fact real-life transferability-based attacks\nare quite unlikely. By combining theoretical reasoning with a series of\nempirical results, we show that it is practically impossible to predict whether\na given adversarial example is transferable to a specific target model in a\nblack-box setting, hence questioning the validity of adversarial\ntransferability as a real-life attack tool for adversaries that are sensitive\nto the cost of a failed attack.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 09:44:12 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 11:16:59 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Katzir", "Ziv", ""], ["Elovici", "Yuval", ""]]}, {"id": "2105.00447", "submitter": "Masoud Jalayer", "authors": "Masoud Jalayer, Reza Jalayer, Amin Kaboli, Carlotta Orsenigo, Carlo\n  Vercellis", "title": "Automatic Visual Inspection of Rare Defects: A Framework based on\n  GP-WGAN and Enhanced Faster R-CNN", "comments": "13 pages, submitted for THE IEEE INTERNATIONAL CONFERENCE ON INDUSTRY\n  4.0, ARTIFICIAL INTELLIGENCE, AND COMMUNICATIONS TECHNOLOGY (IAICT2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A current trend in industries such as semiconductors and foundry is to shift\ntheir visual inspection processes to Automatic Visual Inspection (AVI) systems,\nto reduce their costs, mistakes, and dependency on human experts. This paper\nproposes a two-staged fault diagnosis framework for AVI systems. In the first\nstage, a generation model is designed to synthesize new samples based on real\nsamples. The proposed augmentation algorithm extracts objects from the real\nsamples and blends them randomly, to generate new samples and enhance the\nperformance of the image processor. In the second stage, an improved deep\nlearning architecture based on Faster R-CNN, Feature Pyramid Network (FPN), and\na Residual Network is proposed to perform object detection on the enhanced\ndataset. The performance of the algorithm is validated and evaluated on two\nmulti-class datasets. The experimental results performed over a range of\nimbalance severities demonstrate the superiority of the proposed framework\ncompared to other solutions.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 11:34:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jalayer", "Masoud", ""], ["Jalayer", "Reza", ""], ["Kaboli", "Amin", ""], ["Orsenigo", "Carlotta", ""], ["Vercellis", "Carlo", ""]]}, {"id": "2105.00455", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Thomas A. Lasko", "title": "Synthesized Difference in Differences", "comments": "Accepted to ACM BCB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the conditional average treatment effect for everyone\nby eliminating confounding and selection bias. Unfortunately, randomized\nclinical trials (RCTs) eliminate confounding but impose strict exclusion\ncriteria that prevent sampling of the entire clinical population. Observational\ndatasets are more inclusive but suffer from confounding. We therefore analyze\nRCT and observational data simultaneously in order to extract the strengths of\neach. Our solution builds upon Difference in Differences (DD), an algorithm\nthat eliminates confounding from observational data by comparing outcomes\nbefore and after treatment administration. DD requires a parallel slopes\nassumption that may not apply in practice when confounding shifts across time.\nWe instead propose Synthesized Difference in Differences (SDD) that infers the\ncorrect (possibly non-parallel) slopes by linearly adjusting a conditional\nversion of DD using additional RCT data. The algorithm achieves state of the\nart performance across multiple synthetic and real datasets even when the RCT\nexcludes the majority of patients.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 12:19:16 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 02:19:44 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Strobl", "Eric V.", ""], ["Lasko", "Thomas A.", ""]]}, {"id": "2105.00459", "submitter": "Ivana Nikoloska", "authors": "Ivana Nikoloska and Osvaldo Simeone", "title": "Fast Power Control Adaptation via Meta-Learning for Random Edge Graph\n  Neural Networks", "comments": "submitted for a conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power control in decentralized wireless networks poses a complex stochastic\noptimization problem when formulated as the maximization of the average sum\nrate for arbitrary interference graphs. Recent work has introduced data-driven\ndesign methods that leverage graph neural network (GNN) to efficiently\nparametrize the power control policy mapping channel state information (CSI) to\nthe power vector. The specific GNN architecture, known as random edge GNN\n(REGNN), defines a non-linear graph convolutional architecture whose spatial\nweights are tied to the channel coefficients, enabling a direct adaption to\nchannel conditions. This paper studies the higher-level problem of enabling\nfast adaption of the power control policy to time-varying topologies. To this\nend, we apply first-order meta-learning on data from multiple topologies with\nthe aim of optimizing for a few-shot adaptation to new network configurations.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 12:43:10 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nikoloska", "Ivana", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2105.00469", "submitter": "Harry Clifford MSci DPhil", "authors": "Adnan Akbar, Andrey Solovyev, John W Cassidy, Nirmesh Patel, Harry W\n  Clifford", "title": "DRIVE: Machine Learning to Identify Drivers of Cancer with\n  High-Dimensional Genomic Data & Imputed Labels", "comments": "Submission to the 33rd Conference on Neural Information Processing\n  Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying the mutations that drive cancer growth is key in clinical\ndecision making and precision oncology. As driver mutations confer selective\nadvantage and thus have an increased likelihood of occurrence, frequency-based\nstatistical models are currently favoured. These methods are not suited to\nrare, low frequency, driver mutations. The alternative approach to address this\nis through functional-impact scores, however methods using this approach are\nhighly prone to false positives. In this paper, we propose a novel combination\nmethod for driver mutation identification, which uses the power of both\nstatistical modelling and functional-impact based methods. Initial results show\nthis approach outperforms the state-of-the-art methods in terms of precision,\nand provides comparable performance in terms of area under receiver operating\ncharacteristic curves (AU-ROC). We believe that data-driven systems based on\nmachine learning, such as these, will become an integral part of precision\noncology in the near future.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 13:27:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Akbar", "Adnan", ""], ["Solovyev", "Andrey", ""], ["Cassidy", "John W", ""], ["Patel", "Nirmesh", ""], ["Clifford", "Harry W", ""]]}, {"id": "2105.00470", "submitter": "Wenxiao Wang", "authors": "Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, Hang Zhao", "title": "On Feature Decorrelation in Self-Supervised Learning", "comments": "The first two authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In self-supervised representation learning, a common idea behind most of the\nstate-of-the-art approaches is to enforce the robustness of the representations\nto predefined augmentations. A potential issue of this idea is the existence of\ncompletely collapsed solutions (i.e., constant features), which are typically\navoided implicitly by carefully chosen implementation details. In this work, we\nstudy a relatively concise framework containing the most common components from\nrecent approaches. We verify the existence of complete collapse and discover\nanother reachable collapse pattern that is usually overlooked, namely\ndimensional collapse. We connect dimensional collapse with strong correlations\nbetween axes and consider such connection as a strong motivation for feature\ndecorrelation (i.e., standardizing the covariance matrix). The capability of\ncorrelation as an unsupervised metric and the gains from feature decorrelation\nare verified empirically to highlight the importance and the potential of this\ninsight.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 13:28:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hua", "Tianyu", ""], ["Wang", "Wenxiao", ""], ["Xue", "Zihui", ""], ["Wang", "Yue", ""], ["Ren", "Sucheng", ""], ["Zhao", "Hang", ""]]}, {"id": "2105.00473", "submitter": "Charles-Henry Bertrand Van Ouytsel", "authors": "Charles-Henry Bertrand Van Ouytsel, Thomas Given-Wilson, Jeremy Minet,\n  Julian Roussieau, Axel Legay", "title": "Analysis of Machine Learning Approaches to Packing Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing is an obfuscation technique widely used by malware to hide the\ncontent and behavior of a program. Much prior research has explored how to\ndetect whether a program is packed. This research includes a broad variety of\napproaches such as entropy analysis, syntactic signatures and more recently\nmachine learning classifiers using various features. However, no robust results\nhave indicated which algorithms perform best, or which features are most\nsignificant. This is complicated by considering how to evaluate the results\nsince accuracy, cost, generalization capabilities, and other measures are all\nreasonable. This work explores eleven different machine learning approaches\nusing 119 features to understand: which features are most significant for\npacking detection; which algorithms offer the best performance; and which\nalgorithms are most economical.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 13:37:15 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Van Ouytsel", "Charles-Henry Bertrand", ""], ["Given-Wilson", "Thomas", ""], ["Minet", "Jeremy", ""], ["Roussieau", "Julian", ""], ["Legay", "Axel", ""]]}, {"id": "2105.00495", "submitter": "Luke Chang", "authors": "Luke Chang, Katharina Dost, Kaiqi Zhao, Ambra Demontis, Fabio Roli,\n  Gill Dobbie, J\\\"org Wicker", "title": "Intriguing Usage of Applicability Domain: Lessons from Cheminformatics\n  Applied to Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Defending machine learning models from adversarial attacks is still a\nchallenge: none of the robust models is utterly immune to adversarial examples\nto date. Different defences have been proposed; however, most of them are\ntailored to particular ML models and adversarial attacks, therefore their\neffectiveness and applicability are strongly limited. A similar problem plagues\ncheminformatics: Quantitative Structure-Activity Relationship (QSAR) models\nstruggle to predict biological activity for the entire chemical space because\nthey are trained on a very limited amount of compounds with known effects. This\nproblem is relieved with a technique called Applicability Domain (AD), which\nrejects the unsuitable compounds for the model. Adversarial examples are\nintentionally crafted inputs that exploit the blind spots which the model has\nnot learned to classify, and adversarial defences try to make the classifier\nmore robust by covering these blind spots. There is an apparent similarity\nbetween AD and adversarial defences. Inspired by the concept of AD, we propose\na multi-stage data-driven defence that is testing for: Applicability: abnormal\nvalues, namely inputs not compliant with the intended use case of the model;\nReliability: samples far from the training data; and Decidability: samples\nwhose predictions contradict the predictions of their neighbours.It can be\napplied to any classification model and is not limited to specific types of\nadversarial attacks. With an empirical analysis, this paper demonstrates how\nApplicability Domain can effectively reduce the vulnerability of ML models to\nadversarial examples.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 15:24:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chang", "Luke", ""], ["Dost", "Katharina", ""], ["Zhao", "Kaiqi", ""], ["Demontis", "Ambra", ""], ["Roli", "Fabio", ""], ["Dobbie", "Gill", ""], ["Wicker", "J\u00f6rg", ""]]}, {"id": "2105.00499", "submitter": "Mahdi Rezaei", "authors": "Saeed Tafazzol, Erfan Fathi, Mahdi Rezaei, Ehsan Asali", "title": "Curious Exploration and Return-based Memory Restoration for Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reward engineering and designing an incentive reward function are non-trivial\ntasks to train agents in complex environments. Furthermore, an inaccurate\nreward function may lead to a biased behaviour which is far from an efficient\nand optimised behaviour. In this paper, we focus on training a single agent to\nscore goals with binary success/failure reward function in Half Field Offense\ndomain. As the major advantage of this research, the agent has no presumption\nabout the environment which means it only follows the original formulation of\nreinforcement learning agents. The main challenge of using such a reward\nfunction is the high sparsity of positive reward signals. To address this\nproblem, we use a simple prediction-based exploration strategy (called Curious\nExploration) along with a Return-based Memory Restoration (RMR) technique which\ntends to remember more valuable memories. The proposed method can be utilized\nto train agents in environments with fairly complex state and action spaces.\nOur experimental results show that many recent solutions including our baseline\nmethod fail to learn and perform in complex soccer domain. However, the\nproposed method can converge easily to the nearly optimal behaviour. The video\npresenting the performance of our trained agent is available at\nhttp://bit.ly/HFO_Binary_Reward.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 16:01:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Tafazzol", "Saeed", ""], ["Fathi", "Erfan", ""], ["Rezaei", "Mahdi", ""], ["Asali", "Ehsan", ""]]}, {"id": "2105.00507", "submitter": "Dmitry Yarotsky", "authors": "Maksim Velikanov and Dmitry Yarotsky", "title": "Universal scaling laws in the gradient descent training of neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current theoretical results on optimization trajectories of neural networks\ntrained by gradient descent typically have the form of rigorous but potentially\nloose bounds on the loss values. In the present work we take a different\napproach and show that the learning trajectory can be characterized by an\nexplicit asymptotic at large training times. Specifically, the leading term in\nthe asymptotic expansion of the loss behaves as a power law $L(t) \\sim\nt^{-\\xi}$ with exponent $\\xi$ expressed only through the data dimension, the\nsmoothness of the activation function, and the class of function being\napproximated. Our results are based on spectral analysis of the integral\noperator representing the linearized evolution of a large network trained on\nthe expected loss. Importantly, the techniques we employ do not require\nspecific form of a data distribution, for example Gaussian, thus making our\nfindings sufficiently universal.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 16:46:38 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Velikanov", "Maksim", ""], ["Yarotsky", "Dmitry", ""]]}, {"id": "2105.00528", "submitter": "Arlene John", "authors": "Arlene John, Barry Cardiff, and Deepu John", "title": "A 1D-CNN Based Deep Learning Technique for Sleep Apnea Detection in IoT\n  Sensors", "comments": "Accepted for discussion at the IEEE International Symposium on\n  Circuits and Systems (ISCAS) 2021", "journal-ref": null, "doi": "10.1109/ISCAS51556.2021.9401300.", "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet of Things (IoT) enabled wearable sensors for health monitoring are\nwidely used to reduce the cost of personal healthcare and improve quality of\nlife. The sleep apnea-hypopnea syndrome, characterized by the abnormal\nreduction or pause in breathing, greatly affects the quality of sleep of an\nindividual. This paper introduces a novel method for apnea detection (pause in\nbreathing) from electrocardiogram (ECG) signals obtained from wearable devices.\nThe novelty stems from the high resolution of apnea detection on a\nsecond-by-second basis, and this is achieved using a 1-dimensional\nconvolutional neural network for feature extraction and detection of sleep\napnea events. The proposed method exhibits an accuracy of 99.56% and a\nsensitivity of 96.05%. This model outperforms several lower resolution\nstate-of-the-art apnea detection methods. The complexity of the proposed model\nis analyzed. We also analyze the feasibility of model pruning and binarization\nto reduce the resource requirements on a wearable IoT device. The pruned model\nwith 80\\% sparsity exhibited an accuracy of 97.34% and a sensitivity of 86.48%.\nThe binarized model exhibited an accuracy of 75.59% and sensitivity of 63.23%.\nThe performance of low complexity patient-specific models derived from the\ngeneric model is also studied to analyze the feasibility of retraining existing\nmodels to fit patient-specific requirements. The patient-specific models on\naverage exhibited an accuracy of 97.79% and sensitivity of 92.23%. The source\ncode for this work is made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 18:35:57 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["John", "Arlene", ""], ["Cardiff", "Barry", ""], ["John", "Deepu", ""]]}, {"id": "2105.00529", "submitter": "Jingjing Deng", "authors": "Hanchi Ren, Jingjing Deng and Xianghua Xie", "title": "GRNN: Generative Regression Neural Network -- A Data Leakage Attack for\n  Federated Learning", "comments": "Submitted to ACM Transactions on Intelligent Systems and Technology.\n  For associated source file, see https://github.com/Rand2AI/GRNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data privacy has become an increasingly important issue in machine learning.\nMany approaches have been developed to tackle this issue, e.g., cryptography\n(Homomorphic Encryption, Differential Privacy, etc.) and collaborative training\n(Secure Multi-Party Computation, Distributed Learning and Federated Learning).\nThese techniques have a particular focus on data encryption or secure local\ncomputation. They transfer the intermediate information to the third-party to\ncompute the final result. Gradient exchanging is commonly considered to be a\nsecure way of training a robust model collaboratively in deep learning.\nHowever, recent researches have demonstrated that sensitive information can be\nrecovered from the shared gradient. Generative Adversarial Networks (GAN), in\nparticular, have shown to be effective in recovering those information.\nHowever, GAN based techniques require additional information, such as class\nlabels which are generally unavailable for privacy persevered learning. In this\npaper, we show that, in Federated Learning (FL) system, image-based privacy\ndata can be easily recovered in full from the shared gradient only via our\nproposed Generative Regression Neural Network (GRNN). We formulate the attack\nto be a regression problem and optimise two branches of the generative model by\nminimising the distance between gradients. We evaluate our method on several\nimage classification tasks. The results illustrate that our proposed GRNN\noutperforms state-of-the-art methods with better stability, stronger\nrobustness, and higher accuracy. It also has no convergence requirement to the\nglobal FL model. Moreover, we demonstrate information leakage using face\nre-identification. Some defense strategies are also discussed in this work.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 18:39:37 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ren", "Hanchi", ""], ["Deng", "Jingjing", ""], ["Xie", "Xianghua", ""]]}, {"id": "2105.00557", "submitter": "Hao Sun", "authors": "Chengping Rao, Hao Sun, Yang Liu", "title": "Hard Encoding of Physics for Learning Spatiotemporal Dynamics", "comments": null, "journal-ref": "ICLR 2021 SimDL Workshop", "doi": null, "report-no": null, "categories": "cs.LG cs.AI physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling nonlinear spatiotemporal dynamical systems has primarily relied on\npartial differential equations (PDEs). However, the explicit formulation of\nPDEs for many underexplored processes, such as climate systems, biochemical\nreaction and epidemiology, remains uncertain or partially unknown, where very\nlimited measurement data is yet available. To tackle this challenge, we propose\na novel deep learning architecture that forcibly encodes known physics\nknowledge to facilitate learning in a data-driven manner. The coercive encoding\nmechanism of physics, which is fundamentally different from the penalty-based\nphysics-informed learning, ensures the network to rigorously obey given\nphysics. Instead of using nonlinear activation functions, we propose a novel\nelementwise product operation to achieve the nonlinearity of the model.\nNumerical experiment demonstrates that the resulting physics-encoded learning\nparadigm possesses remarkable robustness against data noise/scarcity and\ngeneralizability compared with some state-of-the-art models for data-driven\nmodeling.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 21:40:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Rao", "Chengping", ""], ["Sun", "Hao", ""], ["Liu", "Yang", ""]]}, {"id": "2105.00562", "submitter": "Saeed Vahidian", "authors": "Saeed Vahidian and Mahdi Morafah and Bill Lin", "title": "Personalized Federated Learning by Structured and Unstructured Pruning\n  under Data Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The traditional approach in FL tries to learn a single global model\ncollaboratively with the help of many clients under the orchestration of a\ncentral server. However, learning a single global model might not work well for\nall clients participating in the FL under data heterogeneity. Therefore, the\npersonalization of the global model becomes crucial in handling the challenges\nthat arise with statistical heterogeneity and the non-IID distribution of data.\nUnlike prior works, in this work we propose a new approach for obtaining a\npersonalized model from a client-level objective. This further motivates all\nclients to participate in federation even under statistical heterogeneity in\norder to improve their performance, instead of merely being a source of data\nand model training for the central server. To realize this personalization, we\nleverage finding a small subnetwork for each client by applying hybrid pruning\n(combination of structured and unstructured pruning), and unstructured pruning.\nThrough a range of experiments on different benchmarks, we observed that the\nclients with similar data (labels) share similar personal parameters. By\nfinding a subnetwork for each client ...\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 22:10:46 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 00:43:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Vahidian", "Saeed", ""], ["Morafah", "Mahdi", ""], ["Lin", "Bill", ""]]}, {"id": "2105.00568", "submitter": "Markel Sanz Ausin", "authors": "Markel Sanz Ausin, Hamoon Azizsoltani, Song Ju, Yeo Jin Kim, Min Chi", "title": "InferNet for Delayed Reinforcement Tasks: Addressing the Temporal Credit\n  Assignment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The temporal Credit Assignment Problem (CAP) is a well-known and challenging\ntask in AI. While Reinforcement Learning (RL), especially Deep RL, works well\nwhen immediate rewards are available, it can fail when only delayed rewards are\navailable or when the reward function is noisy. In this work, we propose\ndelegating the CAP to a Neural Network-based algorithm named InferNet that\nexplicitly learns to infer the immediate rewards from the delayed rewards. The\neffectiveness of InferNet was evaluated on two online RL tasks: a simple\nGridWorld and 40 Atari games; and two offline RL tasks: GridWorld and a\nreal-life Sepsis treatment task. For all tasks, the effectiveness of using the\nInferNet inferred rewards is compared against the immediate and the delayed\nrewards with two settings: with noisy rewards and without noise. Overall, our\nresults show that the effectiveness of InferNet is robust against noisy reward\nfunctions and is an effective add-on mechanism for solving temporal CAP in a\nwide range of RL tasks, from classic RL simulation environments to a real-world\nRL problem and for both online and offline learning.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 22:52:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ausin", "Markel Sanz", ""], ["Azizsoltani", "Hamoon", ""], ["Ju", "Song", ""], ["Kim", "Yeo Jin", ""], ["Chi", "Min", ""]]}, {"id": "2105.00574", "submitter": "Workneh Y. Ayele", "authors": "W. Y. Ayele", "title": "Adapting CRISP-DM for Idea Mining: A Data Mining Process for Generating\n  Ideas Using a Textual Dataset", "comments": "13 pages, 14 figures. International Journal of Advanced Computer\n  Science and Applications, 2020", "journal-ref": null, "doi": "10.14569/issn.2156-5570", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data mining project managers can benefit from using standard data mining\nprocess models. The benefits of using standard process models for data mining,\nsuch as the de facto and the most popular, Cross-Industry-Standard-Process\nmodel for Data Mining (CRISP-DM) are reduced cost and time. Also, standard\nmodels facilitate knowledge transfer, reuse of best practices, and minimize\nknowledge requirements. On the other hand, to unlock the potential of\never-growing textual data such as publications, patents, social media data, and\ndocuments of various forms, digital innovation is increasingly needed.\nFurthermore, the introduction of cutting-edge machine learning tools and\ntechniques enable the elicitation of ideas. The processing of unstructured\ntextual data to generate new and useful ideas is referred to as idea mining.\nExisting literature about idea mining merely overlooks the utilization of\nstandard data mining process models. Therefore, the purpose of this paper is to\npropose a reusable model to generate ideas, CRISP-DM, for Idea Mining\n(CRISP-IM). The design and development of the CRISP-IM are done following the\ndesign science approach. The CRISP-IM facilitates idea generation, through the\nuse of Dynamic Topic Modeling (DTM), unsupervised machine learning, and\nsubsequent statistical analysis on a dataset of scholarly articles. The adapted\nCRISP-IM can be used to guide the process of identifying trends using scholarly\nliterature datasets or temporally organized patent or any other textual dataset\nof any domain to elicit ideas. The ex-post evaluation of the CRISP-IM is left\nfor future study.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 23:24:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ayele", "W. Y.", ""]]}, {"id": "2105.00579", "submitter": "Zaynah Javed", "authors": "Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, Dawn Song", "title": "BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research has confirmed the feasibility of backdoor attacks in deep\nreinforcement learning (RL) systems. However, the existing attacks require the\nability to arbitrarily modify an agent's observation, constraining the\napplication scope to simple RL systems such as Atari games. In this paper, we\nmigrate backdoor attacks to more complex RL systems involving multiple agents\nand explore the possibility of triggering the backdoor without directly\nmanipulating the agent's observation. As a proof of concept, we demonstrate\nthat an adversary agent can trigger the backdoor of the victim agent with its\nown action in two-player competitive RL systems. We prototype and evaluate\nBACKDOORL in four competitive environments. The results show that when the\nbackdoor is activated, the winning rate of the victim drops by 17% to 37%\ncompared to when not activated.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 23:47:55 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 22:40:51 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Lun", ""], ["Javed", "Zaynah", ""], ["Wu", "Xian", ""], ["Guo", "Wenbo", ""], ["Xing", "Xinyu", ""], ["Song", "Dawn", ""]]}, {"id": "2105.00581", "submitter": "Guanhua Chen", "authors": "Rui Chen, Jared D. Huling, Guanhua Chen, Menggang Yu", "title": "Robust Sample Weighting to Facilitate Individualized Treatment Rule\n  Learning for a Target Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning individualized treatment rules (ITRs) is an important topic in\nprecision medicine. Current literature mainly focuses on deriving ITRs from a\nsingle source population. We consider the observational data setting when the\nsource population differs from a target population of interest. We assume\nsubject covariates are available from both populations, but treatment and\noutcome data are only available from the source population. Although adjusting\nfor differences between source and target populations can potentially lead to\nan improved ITR for the target population, it can substantially increase the\nvariability in ITR estimation. To address this dilemma, we develop a weighting\nframework that aims to tailor an ITR for a given target population and protect\nagainst high variability due to superfluous covariate shift adjustments. Our\nmethod seeks covariate balance over a nonparametric function class\ncharacterized by a reproducing kernel Hilbert space and can improve many ITR\nlearning methods that rely on weights. We show that the proposed method\nencompasses importance weights and the so-called overlap weights as two extreme\ncases, allowing for a better bias-variance trade-off in between. Numerical\nexamples demonstrate that the use of our weighting method can greatly improve\nITR estimation for the target population compared with other weighting methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 00:05:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Rui", ""], ["Huling", "Jared D.", ""], ["Chen", "Guanhua", ""], ["Yu", "Menggang", ""]]}, {"id": "2105.00582", "submitter": "Emily Lin", "authors": "Emily Lin, Weicheng Kuo, Esther Yuh", "title": "Noisy Student learning for cross-institution brain hemorrhage detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computed tomography (CT) is the imaging modality used in the diagnosis of\nneurological emergencies, including acute stroke and traumatic brain injury.\nAdvances in deep learning have led to models that can detect and segment\nhemorrhage on head CT. PatchFCN, one such supervised fully convolutional\nnetwork (FCN), recently demonstrated expert-level detection of intracranial\nhemorrhage on in-sample data. However, its potential for similar accuracy\noutside the training domain is hindered by its need for pixel-labeled data from\noutside institutions. Also recently, a semi-supervised technique, Noisy Student\n(NS) learning, demonstrated state-of-the-art performance on ImageNet by moving\nfrom a fully-supervised to a semi-supervised learning paradigm. We combine the\nPatchFCN and Noisy Student approaches, extending semi-supervised learning to an\nintracranial hemorrhage segmentation task. Surprisingly, the NS model\nperformance surpasses that of a fully-supervised oracle model trained with\nimage-level labels on the same data. It also performs comparably to another\nrecently reported supervised model trained on a labeled dataset 600x larger\nthan that used to train the NS model. To our knowledge, we are the first to\ndemonstrate the effectiveness of semi-supervised learning on a head CT\ndetection and segmentation task.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 00:14:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lin", "Emily", ""], ["Kuo", "Weicheng", ""], ["Yuh", "Esther", ""]]}, {"id": "2105.00591", "submitter": "Juliano S. Assine", "authors": "Juliano S. Assine, J. C. S. Santos Filho, Eduardo Valle", "title": "Single-Training Collaborative Object Detectors Adaptive to Bandwidth and\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, mobile deep-learning deployment progressed by leaps\nand bounds, but solutions still struggle to accommodate its severe and\nfluctuating operational restrictions, which include bandwidth, latency,\ncomputation, and energy. In this work, we help to bridge that gap, introducing\nthe first configurable solution for object detection that manages the triple\ncommunication-computation-accuracy trade-off with a single set of weights. Our\nsolution shows state-of-the-art results on COCO-2017, adding only a minor\npenalty on the base EfficientDet-D2 architecture. Our design is robust to the\nchoice of base architecture and compressor and should adapt well for future\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 01:08:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Assine", "Juliano S.", ""], ["Filho", "J. C. S. Santos", ""], ["Valle", "Eduardo", ""]]}, {"id": "2105.00594", "submitter": "Seyed Amir Hossein Aqajari", "authors": "Seyed Amir Hossein Aqajari, Rui Cao, Amir Hosein Afandizadeh Zargari,\n  and Amir M. Rahmani", "title": "An End-to-End and Accurate PPG-based Respiratory Rate Estimation\n  Approach Using Cycle Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respiratory rate (RR) is a clinical sign representing ventilation. An\nabnormal change in RR is often the first sign of health deterioration as the\nbody attempts to maintain oxygen delivery to its tissues. There has been a\ngrowing interest in remotely monitoring of RR in everyday settings which has\nmade photoplethysmography (PPG) monitoring wearable devices an attractive\nchoice. PPG signals are useful sources for RR extraction due to the presence of\nrespiration-induced modulations in them. The existing PPG-based RR estimation\nmethods mainly rely on hand-crafted rules and manual parameters tuning. An\nend-to-end deep learning approach was recently proposed, however, despite its\nautomatic nature, the performance of this method is not ideal using the real\nworld data. In this paper, we present an end-to-end and accurate pipeline for\nRR estimation using Cycle Generative Adversarial Networks (CycleGAN) to\nreconstruct respiratory signals from raw PPG signals. Our results demonstrate a\nhigher RR estimation accuracy of up to 2$\\times$ (mean absolute error of\n1.9$\\pm$0.3 using five fold cross validation) compared to the state-of-th-art\nusing a identical publicly available dataset. Our results suggest that CycleGAN\ncan be a valuable method for RR estimation from raw PPG signals.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 01:16:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Aqajari", "Seyed Amir Hossein", ""], ["Cao", "Rui", ""], ["Zargari", "Amir Hosein Afandizadeh", ""], ["Rahmani", "Amir M.", ""]]}, {"id": "2105.00602", "submitter": "Shuo Wang", "authors": "Shuo Wang, Surya Nepal, Kristen Moore, Marthie Grobler, Carsten\n  Rudolph, Alsharif Abuadbba", "title": "OCTOPUS: Overcoming Performance andPrivatization Bottlenecks in\n  Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The diversity and quantity of the data warehousing, gathering data from\ndistributed devices such as mobile phones, can enhance machine learning\nalgorithms' success and robustness. Federated learning enables distributed\nparticipants to collaboratively learn a commonly-shared model while holding\ndata locally. However, it is also faced with expensive communication and\nlimitations due to the heterogeneity of distributed data sources and lack of\naccess to global data. In this paper, we investigate a practical distributed\nlearning scenario where multiple downstream tasks (e.g., classifiers) could be\nlearned from dynamically-updated and non-iid distributed data sources,\nefficiently and providing local privatization. We introduce a new distributed\nlearning scheme to address communication overhead via latent compression,\nleveraging global data while providing local privatization of local data\nwithout additional cost due to encryption or perturbation. This scheme divides\nthe learning into (1) informative feature encoding, extracting and transmitting\nthe latent space compressed representation features of local data at each node\nto address communication overhead; (2) downstream tasks centralized at the\nserver using the encoded codes gathered from each node to address computing and\nstorage overhead. Besides, a disentanglement strategy is applied to address the\nprivatization of sensitive components of local data. Extensive experiments are\nconducted on image and speech datasets. The results demonstrate that downstream\ntasks on the compact latent representations can achieve comparable accuracy to\ncentralized learning with the privatization of local data.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 02:24:53 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Shuo", ""], ["Nepal", "Surya", ""], ["Moore", "Kristen", ""], ["Grobler", "Marthie", ""], ["Rudolph", "Carsten", ""], ["Abuadbba", "Alsharif", ""]]}, {"id": "2105.00607", "submitter": "Seewoo Lee", "authors": "Seewoo Lee, Youngduck Choi, Juneyoung Park, Byungsoo Kim and Jinwoo\n  Shin", "title": "Consistency and Monotonicity Regularization for Neural Knowledge Tracing", "comments": "11 pages including reference (1 page) and appendix (4 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Tracing (KT), tracking a human's knowledge acquisition, is a\ncentral component in online learning and AI in Education. In this paper, we\npresent a simple, yet effective strategy to improve the generalization ability\nof KT models: we propose three types of novel data augmentation, coined\nreplacement, insertion, and deletion, along with corresponding regularization\nlosses that impose certain consistency or monotonicity biases on the model's\npredictions for the original and augmented sequence. Extensive experiments on\nvarious KT benchmarks show that our regularization scheme consistently improves\nthe model performances, under 3 widely-used neural networks and 4 public\nbenchmarks, e.g., it yields 6.3% improvement in AUC under the DKT model and the\nASSISTmentsChall dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 02:36:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lee", "Seewoo", ""], ["Choi", "Youngduck", ""], ["Park", "Juneyoung", ""], ["Kim", "Byungsoo", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2105.00619", "submitter": "Salman Ahmed", "authors": "Salman Ahmed, Hammad Naveed", "title": "OpTorch: Optimized deep learning architectures for resource limited\n  environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning algorithms have made many breakthroughs and have various\napplications in real life. Computational resources become a bottleneck as the\ndata and complexity of the deep learning pipeline increases. In this paper, we\npropose optimized deep learning pipelines in multiple aspects of training\nincluding time and memory. OpTorch is a machine learning library designed to\novercome weaknesses in existing implementations of neural network training.\nOpTorch provides features to train complex neural networks with limited\ncomputational resources. OpTorch achieved the same accuracy as existing\nlibraries on Cifar-10 and Cifar-100 datasets while reducing memory usage to\napproximately 50%. We also explore the effect of weights on total memory usage\nin deep learning pipelines. In our experiments, parallel encoding-decoding\nalong with sequential checkpoints results in much improved memory and time\nusage while keeping the accuracy similar to existing pipelines. OpTorch python\npackage is available at available at https://github.com/cbrl-nuces/optorch\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 03:58:57 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 09:25:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ahmed", "Salman", ""], ["Naveed", "Hammad", ""]]}, {"id": "2105.00620", "submitter": "Siawpeng Er", "authors": "Siawpeng Er, Shihao Yang, Tuo Zhao", "title": "COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The global spread of COVID-19, the disease caused by the novel coronavirus\nSARS-CoV-2, has cast a significant threat to mankind. As the COVID-19 situation\ncontinues to evolve, predicting localized disease severity is crucial for\nadvanced resource allocation. This paper proposes a method named COURAGE\n(COUnty aggRegation mixup AuGmEntation) to generate a short-term prediction of\n2-week-ahead COVID-19 related deaths for each county in the United States,\nleveraging modern deep learning techniques. Specifically, our method adopts a\nself-attention model from Natural Language Processing, known as the transformer\nmodel, to capture both short-term and long-term dependencies within the time\nseries while enjoying computational efficiency. Our model fully utilizes\npublicly available information of COVID-19 related confirmed cases, deaths,\ncommunity mobility trends and demographic information, and can produce\nstate-level prediction as an aggregation of the corresponding county-level\npredictions. Our numerical experiments demonstrate that our model achieves the\nstate-of-the-art performance among the publicly available benchmark models.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 04:00:59 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 02:50:06 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Er", "Siawpeng", ""], ["Yang", "Shihao", ""], ["Zhao", "Tuo", ""]]}, {"id": "2105.00636", "submitter": "Dian Chen", "authors": "Dian Chen, Vladlen Koltun, Philipp Kr\\\"ahenb\\\"uhl", "title": "Learning to drive from a world on rails", "comments": "Code and data available at: https://dotchen.github.io/world_on_rails/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn an interactive vision-based driving policy from pre-recorded driving\nlogs via a model-based approach. A forward model of the world supervises a\ndriving policy that predicts the outcome of any potential driving trajectory.\nTo support learning from pre-recorded logs, we assume that the world is on\nrails, meaning neither the agent nor its actions influence the environment.\nThis assumption greatly simplifies the learning problem, factorizing the\ndynamics into a nonreactive world model and a low-dimensional and compact\nforward model of the ego-vehicle. Our approach computes action-values for each\ntraining trajectory using a tabular dynamic-programming evaluation of the\nBellman equations; these action-values in turn supervise the final vision-based\ndriving policy. Despite the world-on-rails assumption, the final driving policy\nacts well in a dynamic and reactive world. Our method ranks first on the CARLA\nleaderboard, attaining a 25% higher driving score while using 40 times less\ndata. Our method is also an order of magnitude more sample-efficient than\nstate-of-the-art model-free reinforcement learning techniques on navigational\ntasks in the ProcGen benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 05:55:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Dian", ""], ["Koltun", "Vladlen", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2105.00644", "submitter": "Saurav Manchanda", "authors": "Saurav Manchanda and Da Zheng and George Karypis", "title": "Schema-Aware Deep Graph Convolutional Networks for Heterogeneous Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional network (GCN) based approaches have achieved significant\nprogress for solving complex, graph-structured problems. GCNs incorporate the\ngraph structure information and the node (or edge) features through message\npassing and computes 'deep' node representations. Despite significant progress\nin the field, designing GCN architectures for heterogeneous graphs still\nremains an open challenge. Due to the schema of a heterogeneous graph, useful\ninformation may reside multiple hops away. A key question is how to perform\nmessage passing to incorporate information of neighbors multiple hops away\nwhile avoiding the well-known over-smoothing problem in GCNs. To address this\nquestion, we propose our GCN framework 'Deep Heterogeneous Graph Convolutional\nNetwork (DHGCN)', which takes advantage of the schema of a heterogeneous graph\nand uses a hierarchical approach to effectively utilize information many hops\naway. It first computes representations of the target nodes based on their\n'schema-derived ego-network' (SEN). It then links the nodes of the same type\nwith various pre-defined metapaths and performs message passing along these\nlinks to compute final node representations. Our design choices naturally\ncapture the way a heterogeneous graph is generated from the schema. The\nexperimental results on real and synthetic datasets corroborate the design\nchoice and illustrate the performance gains relative to competing alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 06:24:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Manchanda", "Saurav", ""], ["Zheng", "Da", ""], ["Karypis", "George", ""]]}, {"id": "2105.00650", "submitter": "Jaydip Sen", "authors": "Gourab Nath and Jaydip Sen", "title": "An Algorithm for Recommending Groceries Based on an Item Ranking Method", "comments": "This is the accepted version of our paper in the IEEE International\n  Conference on Intelligent Technologies (IEEE CONIT), June 25-27, 2021, which\n  will be conducted in Belgaum, INDIA. The paper consists of 7 pages, 1 figure\n  and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes a new recommender system algorithm for online grocery\nshopping. The algorithm is based on the perspective that, since the grocery\nitems are usually bought in bulk, a grocery recommender system should be\ncapable of recommending the items in bulk. The algorithm figures out the\npossible dishes a user may cook based on the items added to the basket and\nrecommends the ingredients accordingly. Our algorithm does not depend on the\nuser ratings. Customers usually do not have the patience to rate the groceries\nthey purchase. Therefore, algorithms that are not dependent on user ratings\nneed to be designed. Instead of using a brute force search, this algorithm\nlimits the search space to a set of only a few probably food categories. Each\nfood category consists of several food subcategories. For example, \"fried rice\"\nand \"biryani\" are food subcategories that belong to the food category \"rice\".\nFor each food category, items are ranked according to how well they can\ndifferentiate a food subcategory. To each food subcategory in the activated\nsearch space, this algorithm attaches a score. The score is calculated based on\nthe rank of the items added to the basket. Once the score exceeds a threshold\nvalue, its corresponding subcategory gets activated. The algorithm then uses a\nbasket-to-recipe similarity measure to identify the best recipe matches within\nthe activated subcategories only. This reduces the search space to a great\nextent. We may argue that this algorithm is similar to the content-based\nrecommender system in some sense, but it does not suffer from the limitations\nlike limited content, over-specialization, or the new user problem.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 06:52:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nath", "Gourab", ""], ["Sen", "Jaydip", ""]]}, {"id": "2105.00667", "submitter": "Boris Ruf", "authors": "Boris Ruf, Marcin Detyniecki", "title": "Explaining how your AI system is fair", "comments": "Accepted at the ACM CHI 2021 Workshop on Operationalizing\n  Human-Centered Perspectives in Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To implement fair machine learning in a sustainable way, choosing the right\nfairness objective is key. Since fairness is a concept of justice which comes\nin various, sometimes conflicting definitions, this is not a trivial task\nthough. The most appropriate fairness definition for an artificial intelligence\n(AI) system is a matter of ethical standards and legal requirements, and the\nright choice depends on the particular use case and its context. In this\nposition paper, we propose to use a decision tree as means to explain and\njustify the implemented kind of fairness to the end users. Such a structure\nwould first of all support AI practitioners in mapping ethical principles to\nfairness definitions for a concrete application and therefore make the\nselection a straightforward and transparent process. However, this approach\nwould also help document the reasoning behind the decision making. Due to the\ngeneral complexity of the topic of fairness in AI, we argue that specifying\n\"fairness\" for a given use case is the best way forward to maintain confidence\nin AI systems. In this case, this could be achieved by sharing the reasons and\nprinciples expressed during the decision making process with the broader\naudience.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 07:52:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ruf", "Boris", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "2105.00687", "submitter": "Simon Enni", "authors": "Simon Enni and Ira Assent", "title": "Learning by Design: Structuring and Documenting the Human Choices in\n  Machine Learning Development", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The influence of machine learning (ML) is quickly spreading, and a number of\nrecent technological innovations have applied ML as a central technology.\nHowever, ML development still requires a substantial amount of human expertise\nto be successful. The deliberation and expert judgment applied during ML\ndevelopment cannot be revisited or scrutinized if not properly documented, and\nthis hinders the further adoption of ML technologies--especially in safety\ncritical situations.\n  In this paper, we present a method consisting of eight design questions, that\noutline the deliberation and normative choices going into creating a ML model.\nOur method affords several benefits, such as supporting critical assessment\nthrough methodological transparency, aiding in model debugging, and anchoring\nmodel explanations by committing to a pre hoc expectation of the model's\nbehavior. We believe that our method can help ML practitioners structure and\njustify their choices and assumptions when developing ML models, and that it\ncan help bridge a gap between those inside and outside the ML field in\nunderstanding how and why ML models are designed and developed the way they\nare.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 08:47:45 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Enni", "Simon", ""], ["Assent", "Ira", ""]]}, {"id": "2105.00693", "submitter": "Qing Ye", "authors": "Jindi Lv and Qing Ye and Yanan Sun and Juan Zhao and Jiancheng Lv", "title": "Heart-Darts: Classification of Heartbeats Using Differentiable\n  Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Arrhythmia is a cardiovascular disease that manifests irregular heartbeats.\nIn arrhythmia detection, the electrocardiogram (ECG) signal is an important\ndiagnostic technique. However, manually evaluating ECG signals is a complicated\nand time-consuming task. With the application of convolutional neural networks\n(CNNs), the evaluation process has been accelerated and the performance is\nimproved. It is noteworthy that the performance of CNNs heavily depends on\ntheir architecture design, which is a complex process grounded on expert\nexperience and trial-and-error. In this paper, we propose a novel approach,\nHeart-Darts, to efficiently classify the ECG signals by automatically designing\nthe CNN model with the differentiable architecture search (i.e., Darts, a\ncell-based neural architecture search method). Specifically, we initially\nsearch a cell architecture by Darts and then customize a novel CNN model for\nECG classification based on the obtained cells. To investigate the efficiency\nof the proposed method, we evaluate the constructed model on the MIT-BIH\narrhythmia database. Additionally, the extensibility of the proposed CNN model\nis validated on two other new databases. Extensive experimental results\ndemonstrate that the proposed method outperforms several state-of-the-art CNN\nmodels in ECG classification in terms of both performance and generalization\ncapability.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 08:57:48 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lv", "Jindi", ""], ["Ye", "Qing", ""], ["Sun", "Yanan", ""], ["Zhao", "Juan", ""], ["Lv", "Jiancheng", ""]]}, {"id": "2105.00694", "submitter": "Emir Zunic", "authors": "Emir Zunic, Kemal Korjenic, Sead Delalic, Zlatko Subara", "title": "Comparison Analysis of Facebook's Prophet, Amazon's DeepAR+ and CNN-QR\n  Algorithms for Successful Real-World Sales Forecasting", "comments": null, "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 13, No 2, April 2021", "doi": "10.5121/ijcsit.2021.13205", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By successfully solving the problem of forecasting, the processes in the work\nof various companies are optimized and savings are achieved. In this process,\nthe analysis of time series data is of particular importance. Since the\ncreation of Facebook's Prophet, and Amazon's DeepAR+ and CNN-QR forecasting\nmodels, algorithms have attracted a great deal of attention. The paper presents\nthe application and comparison of the above algorithms for sales forecasting in\ndistribution companies. A detailed comparison of the performance of algorithms\nover real data with different lengths of sales history was made. The results\nshow that Prophet gives better results for items with a longer history and\nfrequent sales, while Amazon's algorithms show superiority for items without a\nlong history and items that are rarely sold.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:01:24 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zunic", "Emir", ""], ["Korjenic", "Kemal", ""], ["Delalic", "Sead", ""], ["Subara", "Zlatko", ""]]}, {"id": "2105.00695", "submitter": "Indrit Nallbani", "authors": "Indrit Nallbani, Aydin Ayanzadeh, Reyhan Kevser Keser, Nurullah\n  \\c{C}al{\\i}k, Beh\\c{c}et U\\u{g}ur T\\\"oreyin", "title": "Representation Learning using Graph Autoencoders with Residual\n  Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph autoencoders are very efficient at embedding graph-based complex data\nsets. However, most of the autoencoders have shallow depths and their\nefficiency tends to decrease with the increase of layer depth. In this paper,\nwe study the effect of adding residual connections to shallow and deep graph\nvariational and vanilla autoencoders. We show that residual connections improve\nthe accuracy of the deep graph-based autoencoders. Furthermore, we propose\nRes-VGAE, a graph variational autoencoder with different residual connections.\nOur experiments show that our model achieves superior results when compared\nwith other autoencoder-based models for the link prediction task.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:05:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nallbani", "Indrit", ""], ["Ayanzadeh", "Aydin", ""], ["Keser", "Reyhan Kevser", ""], ["\u00c7al\u0131k", "Nurullah", ""], ["T\u00f6reyin", "Beh\u00e7et U\u011fur", ""]]}, {"id": "2105.00696", "submitter": "Feng Xia", "authors": "Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui Pan, Huan\n  Liu", "title": "Graph Learning: A Survey", "comments": "19 pages, 6 figures", "journal-ref": "IEEE Transactions on Artificial Intelligence (2021)", "doi": "10.1109/TAI.2021.3076021", "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphs are widely used as a popular representation of the network structure\nof connected data. Graph data can be found in a broad spectrum of application\ndomains such as social systems, ecosystems, biological networks, knowledge\ngraphs, and information systems. With the continuous penetration of artificial\nintelligence technologies, graph learning (i.e., machine learning on graphs) is\ngaining attention from both researchers and practitioners. Graph learning\nproves effective for many tasks, such as classification, link prediction, and\nmatching. Generally, graph learning methods extract relevant features of graphs\nby taking advantage of machine learning algorithms. In this survey, we present\na comprehensive overview on the state-of-the-art of graph learning. Special\nattention is paid to four categories of existing graph learning methods,\nincluding graph signal processing, matrix factorization, random walk, and deep\nlearning. Major models and algorithms under these categories are reviewed\nrespectively. We examine graph learning applications in areas such as text,\nimages, science, knowledge graphs, and combinatorial optimization. In addition,\nwe discuss several promising research directions in this field.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:06:01 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xia", "Feng", ""], ["Sun", "Ke", ""], ["Yu", "Shuo", ""], ["Aziz", "Abdul", ""], ["Wan", "Liangtian", ""], ["Pan", "Shirui", ""], ["Liu", "Huan", ""]]}, {"id": "2105.00703", "submitter": "Guandong Xu", "authors": "Tri Dung Duong, Qian Li, Guandong Xu", "title": "Prototype-based Counterfactual Explanation for Causal Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Counterfactual explanation is one branch of interpretable machine learning\nthat produces a perturbation sample to change the model's original decision.\nThe generated samples can act as a recommendation for end-users to achieve\ntheir desired outputs. Most of the current counterfactual explanation\napproaches are the gradient-based method, which can only optimize the\ndifferentiable loss functions with continuous variables. Accordingly, the\ngradient-free methods are proposed to handle the categorical variables, which\nhowever present several major limitations: 1) causal relationships among\nfeatures are typically ignored when generating the counterfactuals, possibly\nresulting in impractical guidelines for decision-makers; 2) the generation of\nthe counterfactual sample is prohibitively slow and requires lots of parameter\ntuning for combining different loss functions. In this work, we propose a\ncausal structure model to preserve the causal relationship underlying the\nfeatures of the counterfactual. In addition, we design a novel gradient-free\noptimization based on the multi-objective genetic algorithm that generates the\ncounterfactual explanations for the mixed-type of continuous and categorical\ndata. Numerical experiments demonstrate that our method compares favorably with\nstate-of-the-art methods and therefore is applicable to any prediction model.\nAll the source code and data are available at\n\\textit{\\url{{https://github.com/tridungduong16/multiobj-scm-cf}}}.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:25:59 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 10:24:31 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Duong", "Tri Dung", ""], ["Li", "Qian", ""], ["Xu", "Guandong", ""]]}, {"id": "2105.00704", "submitter": "Haobo Jiang", "authors": "Haobo Jiang, Jin Xie, Jian Yang", "title": "Action Candidate Based Clipped Double Q-learning for Discrete and\n  Continuous Action Tasks", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double Q-learning is a popular reinforcement learning algorithm in Markov\ndecision process (MDP) problems. Clipped Double Q-learning, as an effective\nvariant of Double Q-learning, employs the clipped double estimator to\napproximate the maximum expected action value. Due to the underestimation bias\nof the clipped double estimator, performance of clipped Double Q-learning may\nbe degraded in some stochastic environments. In this paper, in order to reduce\nthe underestimation bias, we propose an action candidate based clipped double\nestimator for Double Q-learning. Specifically, we first select a set of elite\naction candidates with the high action values from one set of estimators. Then,\namong these candidates, we choose the highest valued action from the other set\nof estimators. Finally, we use the maximum value in the second set of\nestimators to clip the action value of the chosen action in the first set of\nestimators and the clipped value is used for approximating the maximum expected\naction value. Theoretically, the underestimation bias in our clipped Double\nQ-learning decays monotonically as the number of the action candidates\ndecreases. Moreover, the number of action candidates controls the trade-off\nbetween the overestimation and underestimation biases. In addition, we also\nextend our clipped Double Q-learning to continuous action tasks via\napproximating the elite continuous action candidates. We empirically verify\nthat our algorithm can more accurately estimate the maximum expected action\nvalue on some toy environments and yield good performance on several benchmark\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:26:49 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jiang", "Haobo", ""], ["Xie", "Jin", ""], ["Yang", "Jian", ""]]}, {"id": "2105.00707", "submitter": "Qing Ye", "authors": "Qiutong Guo and Shun Lei and Qing Ye and Zhiyang Fang", "title": "MRC-LSTM: A Hybrid Approach of Multi-scale Residual CNN and LSTM to\n  Predict Bitcoin Price", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bitcoin, one of the major cryptocurrencies, presents great opportunities and\nchallenges with its tremendous potential returns accompanying high risks. The\nhigh volatility of Bitcoin and the complex factors affecting them make the\nstudy of effective price forecasting methods of great practical importance to\nfinancial investors and researchers worldwide. In this paper, we propose a\nnovel approach called MRC-LSTM, which combines a Multi-scale Residual\nConvolutional neural network (MRC) and a Long Short-Term Memory (LSTM) to\nimplement Bitcoin closing price prediction. Specifically, the Multi-scale\nresidual module is based on one-dimensional convolution, which is not only\ncapable of adaptive detecting features of different time scales in multivariate\ntime series, but also enables the fusion of these features. LSTM has the\nability to learn long-term dependencies in series, which is widely used in\nfinancial time series forecasting. By mixing these two methods, the model is\nable to obtain highly expressive features and efficiently learn trends and\ninteractions of multivariate time series. In the study, the impact of external\nfactors such as macroeconomic variables and investor attention on the Bitcoin\nprice is considered in addition to the trading information of the Bitcoin\nmarket. We performed experiments to predict the daily closing price of Bitcoin\n(USD), and the experimental results show that MRC-LSTM significantly\noutperforms a variety of other network structures. Furthermore, we conduct\nadditional experiments on two other cryptocurrencies, Ethereum and Litecoin, to\nfurther confirm the effectiveness of the MRC-LSTM in short-term forecasting for\nmultivariate time series of cryptocurrencies.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:32:23 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Guo", "Qiutong", ""], ["Lei", "Shun", ""], ["Ye", "Qing", ""], ["Fang", "Zhiyang", ""]]}, {"id": "2105.00767", "submitter": "Xiong Wang", "authors": "Xiong Wang, Riheng Jia", "title": "Mean Field Equilibrium in Multi-Armed Bandit Game with Continuous Reward", "comments": "IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field game facilitates analyzing multi-armed bandit (MAB) for a large\nnumber of agents by approximating their interactions with an average effect.\nExisting mean field models for multi-agent MAB mostly assume a binary reward\nfunction, which leads to tractable analysis but is usually not applicable in\npractical scenarios. In this paper, we study the mean field bandit game with a\ncontinuous reward function. Specifically, we focus on deriving the existence\nand uniqueness of mean field equilibrium (MFE), thereby guaranteeing the\nasymptotic stability of the multi-agent system. To accommodate the continuous\nreward function, we encode the learned reward into an agent state, which is in\nturn mapped to its stochastic arm playing policy and updated using realized\nobservations. We show that the state evolution is upper semi-continuous, based\non which the existence of MFE is obtained. As the Markov analysis is mainly for\nthe case of discrete state, we transform the stochastic continuous state\nevolution into a deterministic ordinary differential equation (ODE). On this\nbasis, we can characterize a contraction mapping for the ODE to ensure a unique\nMFE for the bandit game. Extensive evaluations validate our MFE\ncharacterization, and exhibit tight empirical regret of the MAB problem.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 11:50:06 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 12:37:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Xiong", ""], ["Jia", "Riheng", ""]]}, {"id": "2105.00771", "submitter": "Ioannis Mandralis", "authors": "Ioannis Mandralis, Pascal Weber, Guido Novati, Petros Koumoutsakos", "title": "Learning swimming escape patterns for larval fish under energy\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swimming organisms can escape their predators by creating and harnessing\nunsteady flow fields through their body motions. Stochastic optimization and\nflow simulations have identified escape patterns that are consistent with those\nobserved in natural larval swimmers. However, these patterns have been limited\nby the specification of a particular cost function and depend on a prescribed\nfunctional form of the body motion. Here, we deploy reinforcement learning to\ndiscover swimmer escape patterns for larval fish under energy constraints. The\nidentified patterns include the C-start mechanism, in addition to more\nenergetically efficient escapes. We find that maximizing distance with limited\nenergy requires swimming via short bursts of accelerating motion interlinked\nwith phases of gliding. The present, data efficient, reinforcement learning\nalgorithm results in an array of patterns that reveal practical flow\noptimization principles for efficient swimming and the methodology can be\ntransferred to the control of aquatic robotic devices operating under energy\nconstraints.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 11:58:37 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 09:17:48 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Mandralis", "Ioannis", ""], ["Weber", "Pascal", ""], ["Novati", "Guido", ""], ["Koumoutsakos", "Petros", ""]]}, {"id": "2105.00773", "submitter": "Michael Hughes", "authors": "Gian Marco Visani, Alexandra Hope Lee, Cuong Nguyen, David M. Kent,\n  John B. Wong, Joshua T. Cohen, and Michael C. Hughes", "title": "Approximate Bayesian Computation for an Explicit-Duration Hidden Markov\n  Model of COVID-19 Hospital Trajectories", "comments": "To appear in the Proceedings of the Machine Learning for Healthcare\n  (MLHC) conference, 2021. 20 pages, 7 figures and 1 table. 26 additional pages\n  of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of modeling constrained hospital resources in the\nmidst of the COVID-19 pandemic in order to inform decision-makers of future\ndemand and assess the societal value of possible interventions. For broad\napplicability, we focus on the common yet challenging scenario where\npatient-level data for a region of interest are not available. Instead, given\ndaily admissions counts, we model aggregated counts of observed resource use,\nsuch as the number of patients in the general ward, in the intensive care unit,\nor on a ventilator. In order to explain how individual patient trajectories\nproduce these counts, we propose an aggregate count explicit-duration hidden\nMarkov model, nicknamed the ACED-HMM, with an interpretable, compact\nparameterization. We develop an Approximate Bayesian Computation approach that\ndraws samples from the posterior distribution over the model's transition and\nduration parameters given aggregate counts from a specific location, thus\nadapting the model to a region or individual hospital site of interest. Samples\nfrom this posterior can then be used to produce future forecasts of any counts\nof interest. Using data from the United States and the United Kingdom, we show\nour mechanistic approach provides competitive probabilistic forecasts for the\nfuture even as the dynamics of the pandemic shift. Furthermore, we show how our\nmodel provides insight about recovery probabilities or length of stay\ndistributions, and we suggest its potential to answer challenging what-if\nquestions about the societal value of possible interventions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:32:42 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 15:51:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Visani", "Gian Marco", ""], ["Lee", "Alexandra Hope", ""], ["Nguyen", "Cuong", ""], ["Kent", "David M.", ""], ["Wong", "John B.", ""], ["Cohen", "Joshua T.", ""], ["Hughes", "Michael C.", ""]]}, {"id": "2105.00774", "submitter": "Diego Antognini", "authors": "Diego Antognini and Boi Faltings", "title": "Fast Multi-Step Critiquing for VAE-based Recommender Systems", "comments": "Accepted at RecSys 2021. 19 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that providing personalized explanations alongside\nrecommendations increases trust and perceived quality. Furthermore, it gives\nusers an opportunity to refine the recommendations by critiquing parts of the\nexplanations. On one hand, current recommender systems model the\nrecommendation, explanation, and critiquing objectives jointly, but this\ncreates an inherent trade-off between their respective performance. On the\nother hand, although recent latent linear critiquing approaches are built upon\nan existing recommender system, they suffer from computational inefficiency at\ninference due to the objective optimized at each conversation's turn. We\naddress these deficiencies with M&Ms-VAE, a novel variational autoencoder for\nrecommendation and explanation that is based on multimodal modeling\nassumptions. We train the model under a weak supervision scheme to simulate\nboth fully and partially observed variables. Then, we leverage the\ngeneralization ability of a trained M&Ms-VAE model to embed the user preference\nand the critique separately. Our work's most important innovation is our\ncritiquing module, which is built upon and trained in a self-supervised manner\nwith a simple ranking objective. Experiments on four real-world datasets\ndemonstrate that among state-of-the-art models, our system is the first to\ndominate or match the performance in terms of recommendation, explanation, and\nmulti-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x\nfaster than the best baselines. Finally, we show that our model infers coherent\njoint and cross generation, even under weak supervision, thanks to our\nmultimodal-based modeling and training scheme.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:26:09 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 18:22:16 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Antognini", "Diego", ""], ["Faltings", "Boi", ""]]}, {"id": "2105.00782", "submitter": "Lorenzo Nava", "authors": "Lorenzo Nava, Oriol Monserrat and Filippo Catani", "title": "Improving Landslide Detection on SAR Data through Deep Learning", "comments": "8 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this letter, we use deep-learning convolution neural networks (CNNs) to\nassess the landslide mapping and classification performances on optical images\n(from Sentinel-2) and SAR images (from Sentinel-1). The training and test zones\nused to independently evaluate the performance of the CNNs on different\ndatasets are located in the eastern Iburi subprefecture in Hokkaido, where, at\n03.08 local time (JST) on September 6, 2018, an Mw 6.6 earthquake triggered\nabout 8000 coseismic landslides. We analyzed the conditions before and after\nthe earthquake exploiting multi-polarization SAR as well as optical data by\nmeans of a CNN implemented in TensorFlow that points out the locations where\nthe Landslide class is predicted as more likely. As expected, the CNN run on\noptical images proved itself excellent for the landslide detection task,\nachieving an overall accuracy of 99.20% while CNNs based on the combination of\nground range detected (GRD) SAR data reached overall accuracies beyond 94%. Our\nfindings show that the integrated use of SAR data may also allow for rapid\nmapping even during storms and under dense cloud cover and seems to provide\ncomparable accuracy to classical optical change detection in landslide\nrecognition and mapping.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:37:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nava", "Lorenzo", ""], ["Monserrat", "Oriol", ""], ["Catani", "Filippo", ""]]}, {"id": "2105.00783", "submitter": "Gabriel Mittag", "authors": "Gabriel Mittags, Sebastian M\\\"oller", "title": "Full-Reference Speech Quality Estimation with Attentional Siamese Neural\n  Networks", "comments": "Late upload, presented at ICASSP 2020", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053951", "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a full-reference speech quality prediction model\nwith a deep learning approach. The model determines a feature representation of\nthe reference and the degraded signal through a siamese recurrent convolutional\nnetwork that shares the weights for both signals as input. The resulting\nfeatures are then used to align the signals with an attention mechanism and are\nfinally combined to estimate the overall speech quality. The proposed network\narchitecture represents a simple solution for the time-alignment problem that\noccurs for speech signals transmitted through Voice-Over-IP networks and shows\nhow the clean reference signal can be incorporated into speech quality models\nthat are based on end-to-end trained neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:38:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mittags", "Gabriel", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2105.00795", "submitter": "Hankook Lee", "authors": "Hankook Lee, Sungsoo Ahn, Seung-Woo Seo, You Young Song, Eunho Yang,\n  Sung-Ju Hwang, Jinwoo Shin", "title": "RetCL: A Selection-based Approach for Retrosynthesis via Contrastive\n  Learning", "comments": "Accepted to IJCAI 2021. Short version was accepted to Machine\n  Learning for Molecules Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrosynthesis, of which the goal is to find a set of reactants for\nsynthesizing a target product, is an emerging research area of deep learning.\nWhile the existing approaches have shown promising results, they currently lack\nthe ability to consider availability (e.g., stability or purchasability) of the\nreactants or generalize to unseen reaction templates (i.e., chemical reaction\nrules). In this paper, we propose a new approach that mitigates the issues by\nreformulating retrosynthesis into a selection problem of reactants from a\ncandidate set of commercially available molecules. To this end, we design an\nefficient reactant selection framework, named RetCL (retrosynthesis via\ncontrastive learning), for enumerating all of the candidate molecules based on\nselection scores computed by graph neural networks. For learning the score\nfunctions, we also propose a novel contrastive training scheme with hard\nnegative mining. Extensive experiments demonstrate the benefits of the proposed\nselection-based approach. For example, when all 671k reactants in the USPTO\n{database} are given as candidates, our RetCL achieves top-1 exact match\naccuracy of $71.3\\%$ for the USPTO-50k benchmark, while a recent\ntransformer-based approach achieves $59.6\\%$. We also demonstrate that RetCL\ngeneralizes well to unseen templates in various settings in contrast to\ntemplate-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:47:57 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 16:03:58 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Lee", "Hankook", ""], ["Ahn", "Sungsoo", ""], ["Seo", "Seung-Woo", ""], ["Song", "You Young", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung-Ju", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2105.00809", "submitter": "Dessalew Yohannes", "authors": "Dessalew Yohannes and Yeregal Assabie", "title": "Amharic Text Clustering Using Encyclopedic Knowledge with Neural Word\n  Embedding", "comments": "9 PAGES, AfricaNLP 2021 submission (#33) for the workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this digital era, almost in every discipline people are using automated\nsystems that generate information represented in document format in different\nnatural languages. As a result, there is a growing interest towards better\nsolutions for finding, organizing and analyzing these documents. In this paper,\nwe propose a system that clusters Amharic text documents using Encyclopedic\nKnowledge (EK) with neural word embedding. EK enables the representation of\nrelated concepts and neural word embedding allows us to handle the contexts of\nthe relatedness. During the clustering process, all the text documents pass\nthrough preprocessing stages. Enriched text document features are extracted\nfrom each document by mapping with EK and word embedding model. TF-IDF weighted\nvector of enriched feature was generated. Finally, text documents are clustered\nusing popular spherical K-means algorithm. The proposed system is tested with\nAmharic text corpus and Amharic Wikipedia data. Test results show that the use\nof EK with word embedding for document clustering improves the average accuracy\nover the use of only EK. Furthermore, changing the size of the class has a\nsignificant effect on accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 05:37:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yohannes", "Dessalew", ""], ["Assabie", "Yeregal", ""]]}, {"id": "2105.00813", "submitter": "Preslav Nakov", "authors": "Anton Chernyavskiy, Dmitry Ilvovsky, Preslav Nakov", "title": "Transformers: \"The End of History\" for NLP?", "comments": "Transformers, NLP, BERT, RoBERTa, XLNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural architectures, such as the Transformer, coupled\nwith the emergence of large-scale pre-trained models such as BERT, have\nrevolutionized the field of Natural Language Processing (NLP), pushing the\nstate-of-the-art for a number of NLP tasks. A rich family of variations of\nthese models has been proposed, such as RoBERTa, ALBERT, and XLNet, but\nfundamentally, they all remain limited in their ability to model certain kinds\nof information, and they cannot cope with certain information sources, which\nwas easy for pre-existing models. Thus, here we aim to shed some light on some\nimportant theoretical limitations of pre-trained BERT-style models that are\ninherent in the general Transformer architecture. First, we demonstrate in\npractice on two general types of tasks -- segmentation and segment labeling --\nand four datasets that these limitations are indeed harmful and that addressing\nthem, even in some very simple and naive ways, can yield sizable improvements\nover vanilla RoBERTa and XLNet. Then, we offer a more general discussion on\ndesiderata for future additions to the Transformer architecture that would\nincrease its expressiveness, which we hope could help in the design of the next\ngeneration of deep NLP architectures.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:29:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chernyavskiy", "Anton", ""], ["Ilvovsky", "Dmitry", ""], ["Nakov", "Preslav", ""]]}, {"id": "2105.00817", "submitter": "Andr\\'e Bodmer Dr.", "authors": "Michael Freunek and Andr\\'e Bodmer", "title": "BERT based freedom to operate patent analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method to apply BERT to freedom to operate patent\nanalysis and patent searches. According to the method, BERT is fine-tuned by\ntraining patent descriptions to the independent claims. Each description\nrepresents an invention which is protected by the corresponding claims. Such a\ntrained BERT could be able to identify or order freedom to operate relevant\npatents based on a short description of an invention or product. We tested the\nmethod by training BERT on the patent class G06T1/00 and applied the trained\nBERT on five inventions classified in G06T1/60, described via DOCDB abstracts.\nThe DOCDB abstract are available on ESPACENET of the European Patent Office.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:30:46 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 07:14:13 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Freunek", "Michael", ""], ["Bodmer", "Andr\u00e9", ""]]}, {"id": "2105.00822", "submitter": "Xiaocong Chen", "authors": "Xiaocong Chen, Lina Yao, Xianzhi Wang, Aixin Sun, Wenjie Zhang and\n  Quan Z. Sheng", "title": "Generative Adversarial Reward Learning for Generalized Behavior Tendency\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in reinforcement learning have inspired increasing interest\nin learning user modeling adaptively through dynamic interactions, e.g., in\nreinforcement learning based recommender systems. Reward function is crucial\nfor most of reinforcement learning applications as it can provide the guideline\nabout the optimization. However, current reinforcement-learning-based methods\nrely on manually-defined reward functions, which cannot adapt to dynamic and\nnoisy environments. Besides, they generally use task-specific reward functions\nthat sacrifice generalization ability. We propose a generative inverse\nreinforcement learning for user behavioral preference modelling, to address the\nabove issues. Instead of using predefined reward functions, our model can\nautomatically learn the rewards from user's actions based on discriminative\nactor-critic network and Wasserstein GAN. Our model provides a general way of\ncharacterizing and explaining underlying behavioral tendencies, and our\nexperiments show our method outperforms state-of-the-art methods in a variety\nof scenarios, namely traffic signal control, online recommender systems, and\nscanpath prediction.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 13:14:25 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 13:01:28 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Chen", "Xiaocong", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Sun", "Aixin", ""], ["Zhang", "Wenjie", ""], ["Sheng", "Quan Z.", ""]]}, {"id": "2105.00823", "submitter": "Guy Marshall", "authors": "Guy Marshall and Mokanarangan Thayaparan and Philip Osborne and Andre\n  Freitas", "title": "Switching Contexts: Transportability Measures for NLP", "comments": "10 pages, 4 figures. To appear in IWCS 2021 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the topic of transportability, as a sub-area of\ngeneralisability. By proposing the utilisation of metrics based on\nwell-established statistics, we are able to estimate the change in performance\nof NLP models in new contexts. Defining a new measure for transportability may\nallow for better estimation of NLP system performance in new domains, and is\ncrucial when assessing the performance of NLP systems in new tasks and domains.\nThrough several instances of increasing complexity, we demonstrate how\nlightweight domain similarity measures can be used as estimators for the\ntransportability in NLP applications. The proposed transportability measures\nare evaluated in the context of Named Entity Recognition and Natural Language\nInference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 13:15:24 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Marshall", "Guy", ""], ["Thayaparan", "Mokanarangan", ""], ["Osborne", "Philip", ""], ["Freitas", "Andre", ""]]}, {"id": "2105.00824", "submitter": "Diyah Puspitaningrum", "authors": "Diyah Puspitaningrum", "title": "A Survey of Recent Abstract Summarization Techniques", "comments": "6 tables, 1 figure, additionals (data):\n  https://drive.google.com/drive/folders/152XMSCU3ctshB2BvzEQHY0vo6xkZ9gOl?usp=sharing\n  ,\n  https://drive.google.com/drive/folders/1z09D2-4arE6aOQZxgxcwMVF41xMH4EEH?usp=sharing.\n  Awaiting at https://www.springer.com/gp/book/9789811621017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper surveys several recent abstract summarization methods: T5,\nPegasus, and ProphetNet. We implement the systems in two languages: English and\nIndonesian languages. We investigate the impact of pre-training models (one T5,\nthree Pegasuses, three ProphetNets) on several Wikipedia datasets in English\nand Indonesian language and compare the results to the Wikipedia systems'\nsummaries. The T5-Large, the Pegasus-XSum, and the ProphetNet-CNNDM provide the\nbest summarization. The most significant factors that influence ROUGE\nperformance are coverage, density, and compression. The higher the scores, the\nbetter the summary. Other factors that influence the ROUGE scores are the\npre-training goal, the dataset's characteristics, the dataset used for testing\nthe pre-trained model, and the cross-lingual function. Several suggestions to\nimprove this paper's limitation are: 1) assure that the dataset used for the\npre-training model must sufficiently large, contains adequate instances for\nhandling cross-lingual purpose; 2) Advanced process (finetuning) shall be\nreasonable. We recommend using the large dataset consists of comprehensive\ncoverage of topics from many languages before implementing advanced processes\nsuch as the train-infer-train procedure to the zero-shot translation in the\ntraining stage of the pre-training model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:01:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Puspitaningrum", "Diyah", ""]]}, {"id": "2105.00827", "submitter": "Katikapalli Subramanyam Kalyan", "authors": "Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha", "title": "AMMU -- A Survey of Transformer-based Biomedical Pretrained Language\n  Models", "comments": "Preprint under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based pretrained language models (PLMs) have started a new era in\nmodern natural language processing (NLP). These models combine the power of\ntransformers, transfer learning, and self-supervised learning (SSL). Following\nthe success of these models in the general domain, the biomedical research\ncommunity has developed various in-domain PLMs starting from BioBERT to the\nlatest BioMegatron and CoderBERT models. We strongly believe there is a need\nfor a survey paper that can provide a comprehensive survey of various\ntransformer-based biomedical pretrained language models (BPLMs). In this\nsurvey, we start with a brief overview of foundational concepts like\nself-supervised learning, embedding layer and transformer encoder layers. We\ndiscuss core concepts of transformer-based PLMs like pretraining methods,\npretraining tasks, fine-tuning methods, and various embedding types specific to\nbiomedical domain. We introduce a taxonomy for transformer-based BPLMs and then\ndiscuss all the models. We discuss various challenges and present possible\nsolutions. We conclude by highlighting some of the open issues which will drive\nthe research community to further improve transformer-based BPLMs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:09:51 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kalyan", "Katikapalli Subramanyam", ""], ["Rajasekharan", "Ajit", ""], ["Sangeetha", "Sivanesan", ""]]}, {"id": "2105.00828", "submitter": "Michael T\\\"anzer Mr", "authors": "Michael T\\\"anzer, Sebastian Ruder, Marek Rei", "title": "BERT memorisation and pitfalls in low-resource scenarios", "comments": "14 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art pre-trained models have been shown to memorise facts and\nperform well with limited amounts of training data. To gain a better\nunderstanding of how these models learn, we study their generalisation and\nmemorisation capabilities in noisy and low-resource scenarios. We find that the\ntraining of these models is almost unaffected by label noise and that it is\npossible to reach near-optimal performances even on extremely noisy datasets.\nConversely, we also find that they completely fail when tested on low-resource\ntasks such as few-shot learning and rare entity recognition. To mitigate such\nlimitations, we propose a novel architecture based on BERT and prototypical\nnetworks that improves performance in low-resource named entity recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:53:19 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["T\u00e4nzer", "Michael", ""], ["Ruder", "Sebastian", ""], ["Rei", "Marek", ""]]}, {"id": "2105.00831", "submitter": "Lodovico Giaretta", "authors": "Daniel Garcia Bernal, Lodovico Giaretta, Sarunas Girdzijauskas, Magnus\n  Sahlgren", "title": "Federated Word2Vec: Leveraging Federated Learning to Encourage\n  Collaborative Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large scale contextual representation models have significantly advanced NLP\nin recent years, understanding the semantics of text to a degree never seen\nbefore. However, they need to process large amounts of data to achieve\nhigh-quality results. Joining and accessing all these data from multiple\nsources can be extremely challenging due to privacy and regulatory reasons.\nFederated Learning can solve these limitations by training models in a\ndistributed fashion, taking advantage of the hardware of the devices that\ngenerate the data. We show the viability of training NLP models, specifically\nWord2Vec, with the Federated Learning protocol. In particular, we focus on a\nscenario in which a small number of organizations each hold a relatively large\ncorpus. The results show that neither the quality of the results nor the\nconvergence time in Federated Word2Vec deteriorates as compared to centralised\nWord2Vec.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:39:02 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bernal", "Daniel Garcia", ""], ["Giaretta", "Lodovico", ""], ["Girdzijauskas", "Sarunas", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "2105.00842", "submitter": "Panagiotis Diamantoulakis", "authors": "Pavlos S. Bouzinis, Panagiotis D. Diamantoulakis, George K.\n  Karagiannidis", "title": "Wireless Federated Learning (WFL) for 6G Networks -- Part I: Research\n  Challenges and Future Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional machine learning techniques are conducted in a centralized\nmanner. Recently, the massive volume of generated wireless data, the privacy\nconcerns and the increasing computing capabilities of wireless end-devices have\nled to the emergence of a promising decentralized solution, termed as Wireless\nFederated Learning (WFL). In this first of the two parts paper, we present the\napplication of WFL in the sixth generation of wireless networks (6G), which is\nenvisioned to be an integrated communication and computing platform. After\nanalyzing the key concepts of WFL, we discuss the core challenges of WFL\nimposed by the wireless (or mobile communication) environment. Finally, we shed\nlight to the future directions of WFL, aiming to compose a constructive\nintegration of FL into the future wireless networks.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 18:10:05 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bouzinis", "Pavlos S.", ""], ["Diamantoulakis", "Panagiotis D.", ""], ["Karagiannidis", "George K.", ""]]}, {"id": "2105.00854", "submitter": "Pei Lv", "authors": "Pei Lv, Boya Xu, Chaochao Li, Qingqing Yu, Bing Zhou, Mingliang Xu", "title": "Antagonistic Crowd Simulation Model Integrating Emotion Contagion and\n  Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.GR physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The antagonistic behavior of the crowd often exacerbates the seriousness of\nthe situation in sudden riots, where the spreading of antagonistic emotion and\nbehavioral decision making in the crowd play very important roles. However, the\nmechanism of complex emotion influencing decision making, especially in the\nenvironment of sudden confrontation, has not yet been explored clearly. In this\npaper, we propose one new antagonistic crowd simulation model by combing\nemotional contagion and deep reinforcement learning (ACSED). Firstly, we build\na group emotional contagion model based on the improved SIS contagion disease\nmodel, and estimate the emotional state of the group at each time step during\nthe simulation. Then, the tendency of group antagonistic behavior is modeled\nbased on Deep Q Network (DQN), where the agent can learn the combat behavior\nautonomously, and leverages the mean field theory to quickly calculate the\ninfluence of other surrounding individuals on the central one. Finally, the\nrationality of the predicted behaviors by the DQN is further analyzed in\ncombination with group emotion, and the final combat behavior of the agent is\ndetermined. The method proposed in this paper is verified through several\ndifferent settings of experiments. The results prove that emotions have a vital\nimpact on the group combat, and positive emotional states are more conducive to\ncombat. Moreover, by comparing the simulation results with real scenes, the\nfeasibility of the method is further verified, which can provide good reference\nfor formulating battle plans and improving the winning rate of righteous groups\nbattles in a variety of situations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:18:13 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lv", "Pei", ""], ["Xu", "Boya", ""], ["Li", "Chaochao", ""], ["Yu", "Qingqing", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""]]}, {"id": "2105.00856", "submitter": "Dong Song", "authors": "Dong H. Song and Daniel M. Tartakovsky", "title": "Transfer Learning on Multi-Fidelity Data", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NNs) are often used as surrogates or emulators of partial\ndifferential equations (PDEs) that describe the dynamics of complex systems. A\nvirtually negligible computational cost of such surrogates renders them an\nattractive tool for ensemble-based computation, which requires a large number\nof repeated PDE solves. Since the latter are also needed to generate sufficient\ndata for NN training, the usefulness of NN-based surrogates hinges on the\nbalance between the training cost and the computational gain stemming from\ntheir deployment. We rely on multi-fidelity simulations to reduce the cost of\ndata generation for subsequent training of a deep convolutional NN (CNN) using\ntransfer learning. High- and low-fidelity images are generated by solving PDEs\non fine and coarse meshes, respectively. We use theoretical results for\nmultilevel Monte Carlo to guide our choice of the numbers of images of each\nkind. We demonstrate the performance of this multi-fidelity training strategy\non the problem of estimation of the distribution of a quantity of interest,\nwhose dynamics is governed by a system of nonlinear PDEs (parabolic PDEs of\nmulti-phase flow in heterogeneous porous media) with uncertain/random\nparameters. Our numerical experiments demonstrate that a mixture of a\ncomparatively large number of low-fidelity data and smaller numbers of high-\nand low-fidelity data provides an optimal balance of computational speed-up and\nprediction accuracy. The former is reported relative to both CNN training on\nhigh-fidelity images only and Monte Carlo solution of the PDEs. The latter is\nexpressed in terms of both the Wasserstein distance and the Kullback-Leibler\ndivergence.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:06:19 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Song", "Dong H.", ""], ["Tartakovsky", "Daniel M.", ""]]}, {"id": "2105.00862", "submitter": "Franz Martin Rohrhofer", "authors": "Franz M. Rohrhofer, Stefan Posch, Bernhard C. Geiger", "title": "On the Pareto Front of Physics-Informed Neural Networks", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a new type of deep learning method has emerged, called\nphysics-informed neural networks. Despite their success in solving problems\nthat are governed by partial differential equations, physics-informed neural\nnetworks are often difficult to train. Frequently reported convergence issues\nare still poorly understood and complicate the inference of correct system\ndynamics. In this paper, we shed light on the training process of\nphysics-informed neural networks. By trading between data- and physics-based\nconstraints in the network training, we study the Pareto front in\nmulti-objective optimization problems. We use the diffusion equation and\nNavier-Stokes equations in various test environments to analyze the effects of\nsystem parameters on the shape of the Pareto front. Additionally, we assess the\neffectiveness of state-of-the-art adaptive activation functions and adaptive\nloss weighting methods. Our results demonstrate the prominent role of system\nparameters in the multi-objective optimization and contribute to understanding\nconvergence properties of physics-informed neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 13:47:45 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Rohrhofer", "Franz M.", ""], ["Posch", "Stefan", ""], ["Geiger", "Bernhard C.", ""]]}, {"id": "2105.00866", "submitter": "Lin Zhang", "authors": "Zhiwei Xing, Lin Zhang, Huan Xia, Qian Luo, and Zhao-xin Chen", "title": "Causal Discovery of Flight Service Process Based on Event Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of the civil aviation industry has continuously increased the\nrequirements for the efficiency of airport ground support services. In the\nexisting ground support research, there has not yet been a process model that\ndirectly obtains support from the ground support log to study the causal\nrelationship between service nodes and flight delays. Most ground support\nstudies mainly use machine learning methods to predict flight delays, and the\nflight support model they are based on is an ideal model. The study did not\nconduct an in-depth study of the causal mechanism behind the ground support\nlink and did not reveal the true cause of flight delays. Therefore, there is a\ncertain deviation in the prediction of flight delays by machine learning, and\nthere is a certain deviation between the ideal model based on the research and\nthe actual service process. Therefore, it is of practical significance to\nobtain the process model from the guarantee log and analyze its causality.\nHowever, the existing process causal factor discovery methods only do certain\nresearch when the assumption of causal sufficiency is established and does not\nconsider the existence of latent variables. Therefore, this article proposes a\nframework to realize the discovery of process causal factors without assuming\ncausal sufficiency. The optimized fuzzy mining process model is used as the\nservice benchmark model, and the local causal discovery algorithm is used to\ndiscover the causal factors. Under this framework, this paper proposes a new\nMarkov blanket discovery algorithm that does not assume causal sufficiency to\ndiscover causal factors and uses benchmark data sets for testing. Finally, the\nactual flight service data is used.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 12:59:44 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xing", "Zhiwei", ""], ["Zhang", "Lin", ""], ["Xia", "Huan", ""], ["Luo", "Qian", ""], ["Chen", "Zhao-xin", ""]]}, {"id": "2105.00872", "submitter": "Shuo Wan", "authors": "Shuo Wan, Jiaxun Lu, Pingyi Fan, Yunfeng Shao, Chenghui Peng and\n  Khaled B. letaief", "title": "Convergence Analysis and System Design for Federated Learning over\n  Wireless Networks", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has recently emerged as an important and promising\nlearning scheme in IoT, enabling devices to jointly learn a model without\nsharing their raw data sets. However, as the training data in FL is not\ncollected and stored centrally, FL training requires frequent model exchange,\nwhich is largely affected by the wireless communication network. Therein,\nlimited bandwidth and random package loss restrict interactions in training.\nMeanwhile, the insufficient message synchronization among distributed clients\ncould also affect FL convergence. In this paper, we analyze the convergence\nrate of FL training considering the joint impact of communication network and\ntraining settings. Further by considering the training costs in terms of time\nand power, the optimal scheduling problems for communication networks are\nformulated. The developed theoretical results can be used to assist the system\nparameter selections and explain the principle of how the wireless\ncommunication system could influence the distributed training process and\nnetwork scheduling.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:33:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wan", "Shuo", ""], ["Lu", "Jiaxun", ""], ["Fan", "Pingyi", ""], ["Shao", "Yunfeng", ""], ["Peng", "Chenghui", ""], ["letaief", "Khaled B.", ""]]}, {"id": "2105.00875", "submitter": "Masoud Fetanat", "authors": "Masoud Fetanat, Michael Stevens, Christopher Hayward and Nigel H.\n  Lovell", "title": "A Sensorless Control System for an Implantable Heart Pump using a\n  Real-time Deep Convolutional Neural Network", "comments": null, "journal-ref": "IEEE Transactions on Biomedical Engineering, 2021", "doi": "10.1109/TBME.2021.3061405", "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Left ventricular assist devices (LVADs) are mechanical pumps, which can be\nused to support heart failure (HF) patients as bridge to transplant and\ndestination therapy. To automatically adjust the LVAD speed, a physiological\ncontrol system needs to be designed to respond to variations of patient\nhemodynamics across a variety of clinical scenarios. These control systems\nrequire pressure feedback signals from the cardiovascular system. However,\nthere are no suitable long-term implantable sensors available. In this study, a\nnovel real-time deep convolutional neural network (CNN) for estimation of\npreload based on the LVAD flow was proposed. A new sensorless adaptive\nphysiological control system for an LVAD pump was developed using the full\ndynamic form of model free adaptive control (FFDL-MFAC) and the proposed\npreload estimator to maintain the patient conditions in safe physiological\nranges. The CNN model for preload estimation was trained and evaluated through\n10-fold cross validation on 100 different patient conditions and the proposed\nsensorless control system was assessed on a new testing set of 30 different\npatient conditions across six different patient scenarios. The proposed preload\nestimator was extremely accurate with a correlation coefficient of 0.97, root\nmean squared error of 0.84 mmHg, reproducibility coefficient of 1.56 mmHg,\ncoefficient of variation of 14.44 %, and bias of 0.29 mmHg for the testing\ndataset. The results also indicate that the proposed sensorless physiological\ncontroller works similarly to the preload-based physiological control system\nfor LVAD using measured preload to prevent ventricular suction and pulmonary\ncongestion. This study shows that the LVADs can respond appropriately to\nchanging patient states and physiological demands without the need for\nadditional pressure or flow measurements.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 05:12:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Fetanat", "Masoud", ""], ["Stevens", "Michael", ""], ["Hayward", "Christopher", ""], ["Lovell", "Nigel H.", ""]]}, {"id": "2105.00884", "submitter": "Giulia Milan", "authors": "Giulia Milan, Luca Vassio, Idilio Drago, Marco Mellia", "title": "RL-IoT: Reinforcement Learning to Interact with IoT Devices", "comments": "9 pages, 11 figures, submitted to IEEE COINS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our life is getting filled by Internet of Things (IoT) devices. These devices\noften rely on closed or poorly documented protocols, with unknown formats and\nsemantics. Learning how to interact with such devices in an autonomous manner\nis the key for interoperability and automatic verification of their\ncapabilities. In this paper, we propose RL-IoT, a system that explores how to\nautomatically interact with possibly unknown IoT devices. We leverage\nreinforcement learning (RL) to recover the semantics of protocol messages and\nto take control of the device to reach a given goal, while minimizing the\nnumber of interactions. We assume to know only a database of possible IoT\nprotocol messages, whose semantics are however unknown. RL-IoT exchanges\nmessages with the target IoT device, learning those commands that are useful to\nreach the given goal. Our results show that RL-IoT is able to solve both simple\nand complex tasks. With properly tuned parameters, RL-IoT learns how to perform\nactions with the target device, a Yeelight smart bulb in our case study,\ncompleting non-trivial patterns with as few as 400 interactions. RL-IoT paves\nthe road for automatic interactions with poorly documented IoT protocols, thus\nenabling interoperable systems.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:09:03 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 10:48:44 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Milan", "Giulia", ""], ["Vassio", "Luca", ""], ["Drago", "Idilio", ""], ["Mellia", "Marco", ""]]}, {"id": "2105.00894", "submitter": "George De Ath", "authors": "George De Ath, Richard Everson and Jonathan Fieldsend", "title": "How Bayesian Should Bayesian Optimisation Be?", "comments": "To appear in the Proceedings of Genetic and Evolutionary Computation\n  Conference Companion (GECCO 2021), ACM. 10 pages (main paper) + 26 pages\n  (supplement)", "journal-ref": null, "doi": "10.1145/3449726.3463164", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimisation (BO) uses probabilistic surrogate models - usually\nGaussian processes (GPs) - for the optimisation of expensive black-box\nfunctions. At each BO iteration, the GP hyperparameters are fit to\npreviously-evaluated data by maximising the marginal likelihood. However, this\nfails to account for uncertainty in the hyperparameters themselves, leading to\noverconfident model predictions. This uncertainty can be accounted for by\ntaking the Bayesian approach of marginalising out the model hyperparameters.\n  We investigate whether a fully-Bayesian treatment of the Gaussian process\nhyperparameters in BO (FBBO) leads to improved optimisation performance. Since\nan analytic approach is intractable, we compare FBBO using three approximate\ninference schemes to the maximum likelihood approach, using the Expected\nImprovement (EI) and Upper Confidence Bound (UCB) acquisition functions paired\nwith ARD and isotropic Matern kernels, across 15 well-known benchmark problems\nfor 4 observational noise settings. FBBO using EI with an ARD kernel leads to\nthe best performance in the noise-free setting, with much less difference\nbetween combinations of BO components when the noise is increased. FBBO leads\nto over-exploration with UCB, but is not detrimental with EI. Therefore, we\nrecommend that FBBO using EI with an ARD kernel as the default choice for BO.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:28:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["De Ath", "George", ""], ["Everson", "Richard", ""], ["Fieldsend", "Jonathan", ""]]}, {"id": "2105.00899", "submitter": "Gabriel Michau Dr.", "authors": "Gabriel Michau and Olga Fink", "title": "Fully Learnable Deep Wavelet Transform for Unsupervised Monitoring of\n  High-Frequency Time Series", "comments": "15 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-Frequency (HF) signal are ubiquitous in the industrial world and are of\ngreat use for the monitoring of industrial assets. Most deep learning tools are\ndesigned for inputs of fixed and/or very limited size and many successful\napplications of deep learning to the industrial context use as inputs extracted\nfeatures, which is a manually and often arduously obtained compact\nrepresentation of the original signal. In this paper, we propose a fully\nunsupervised deep learning framework that is able to extract meaningful and\nsparse representation of raw HF signals. We embed in our architecture important\nproperties of the fast discrete wavelet transformation (FDWT) such as (1) the\ncascade algorithm, (2) the quadrature mirror filter property that relates\ntogether the wavelet, the scaling and transposed filter functions, and (3) the\ncoefficient denoising. Using deep learning, we make this architecture fully\nlearnable: both the wavelet bases and the wavelet coefficient denoising are\nlearnable. To achieve this objective, we introduce a new activation function\nthat performs a learnable hard-thresholding of the wavelet coefficients. With\nour framework, the denoising FDWT becomes a fully learnable unsupervised tool\nthat does neither require any type of pre- nor post-processing, nor any prior\nknowledge on wavelet transform. We demonstrate the benefit of embedding all\nthese properties on three machine-learning tasks performed on open source sound\ndatasets. We achieve results well above baseline and we perform an ablation\nstudy of the impact of each property on the performance of the architecture.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:35:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Michau", "Gabriel", ""], ["Fink", "Olga", ""]]}, {"id": "2105.00924", "submitter": "Devang Mahesh", "authors": "Narayani Bhatia, Devang Mahesh, Jashandeep Singh, and Manan Suri", "title": "Bird-Area Water-Bodies Dataset (BAWD) and Predictive AI Model for Avian\n  Botulism Outbreak (AVI-BoT)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Avian botulism caused by a bacterium, Clostridium botulinum, causes a\nparalytic disease in birds often leading to high fatality, and is usually\ndiagnosed using molecular techniques. Diagnostic techniques for Avian botulism\ninclude: Mouse Bioassay, ELISA, PCR, all of which are time-consuming, laborious\nand require invasive sample collection from affected sites. In this study, we\nbuild a first-ever multi-spectral, remote-sensing imagery based global\nBird-Area Water-bodies Dataset (BAWD) (i.e. fused satellite images of\nwater-body sites important for avian fauna) backed by on-ground reporting\nevidence of outbreaks. In the current version, BAWD covers a total ground area\nof 904 sq.km from two open source satellite projects (Sentinel and Landsat).\nBAWD consists of 17 topographically diverse global sites spanning across 4\ncontinents, with locations monitored over a time-span of 3 years (2016-2020).\nUsing BAWD and state-of-the-art deep-learning techniques we propose a\nfirst-ever Artificial Intelligence based (AI) model to predict potential\noutbreak of Avian botulism called AVI-BoT (Aerosol, Visible, Infra-red\n(NIR/SWIR) and Bands of Thermal). AVI-BoT uses fused multi-spectral satellite\nimages of water-bodies (10-bands) as input to generate a spatial prediction map\ndepicting probability of potential Avian botulism outbreaks. We also train and\ninvestigate a simpler (5-band) Causative-Factor model (based on prominent\nphysiological factors reported in literature as conducive for outbreak) to\npredict Avian botulism. Using AVI-BoT, we achieve a training accuracy of 0.94\nand validation accuracy of 0.96 on BAWD, far superior in comparison to our\nCausative factors model. The proposed technique presents a scale-able,\nlow-cost, non-invasive methodology for continuous monitoring of bird-habitats\nagainst botulism outbreaks with the potential of saving valuable fauna lives.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:00:12 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bhatia", "Narayani", ""], ["Mahesh", "Devang", ""], ["Singh", "Jashandeep", ""], ["Suri", "Manan", ""]]}, {"id": "2105.00925", "submitter": "Georgios Leontidis", "authors": "Aiden Durrant and Georgios Leontidis", "title": "Hyperspherically Regularized Networks for BYOL Improves Feature\n  Uniformity and Separability", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap Your Own Latent (BYOL) introduced an approach to self-supervised\nlearning avoiding the contrastive paradigm and subsequently removing the\ncomputational burden of negative sampling. However, feature representations\nunder this paradigm are poorly distributed on the surface of the\nunit-hypersphere representation space compared to contrastive methods. This\nwork empirically demonstrates that feature diversity enforced by contrastive\nlosses is beneficial when employed in BYOL, and as such, provides greater\ninter-class feature separability. Therefore to achieve a more uniform\ndistribution of features, we advocate the minimization of hyperspherical energy\n(i.e. maximization of entropy) in BYOL network weights. We show that directly\noptimizing a measure of uniformity alongside the standard loss, or regularizing\nthe networks of the BYOL architecture to minimize the hyperspherical energy of\nneurons can produce more uniformly distributed and better performing\nrepresentations for downstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:57:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Durrant", "Aiden", ""], ["Leontidis", "Georgios", ""]]}, {"id": "2105.00930", "submitter": "Arnab Karmakar", "authors": "Arnab Karmakar and Deepak Mishra", "title": "Pose Invariant Person Re-Identification using Robust Pose-transformation\n  GAN", "comments": "Undergraduate thesis at Indian Institute of Space Science and\n  Technology, Under review in IEEE Systems, Man and Cybernetics (SMCA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The objective of person re-identification (re-ID) is to retrieve a person's\nimages from an image gallery, given a single instance of the person of\ninterest. Despite several advancements, learning discriminative\nidentity-sensitive and viewpoint invariant features for robust Person\nRe-identification is a major challenge owing to the large pose variation of\nhumans. This paper proposes a re-ID pipeline that utilizes the image generation\ncapability of Generative Adversarial Networks combined with pose clustering and\nfeature fusion to achieve pose invariant feature learning. The objective is to\nmodel a given person under different viewpoints and large pose changes and\nextract the most discriminative features from all the appearances. The pose\ntransformational GAN (pt-GAN) module is trained to generate a person's image in\nany given pose. In order to identify the most significant poses for\ndiscriminative feature extraction, a Pose Clustering module is proposed. The\ngiven instance of the person is modelled in varying poses and these features\nare effectively combined through the Feature Fusion Network. The final re-ID\nmodel consisting of these 3 sub-blocks, alleviates the pose dependence in\nperson re-ID. Also, The proposed model is robust to occlusion, scale, rotation\nand illumination, providing a framework for viewpoint invariant feature\nlearning. The proposed method outperforms the state-of-the-art GAN based models\nin 4 benchmark datasets. It also surpasses the state-of-the-art models that\nreport higher re-ID accuracy in terms of improvement over baseline.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 15:47:03 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 10:01:51 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Karmakar", "Arnab", ""], ["Mishra", "Deepak", ""]]}, {"id": "2105.00931", "submitter": "Unnat Jain", "authors": "Unnat Jain, Iou-Jen Liu, Svetlana Lazebnik, Aniruddha Kembhavi, Luca\n  Weihs, Alexander Schwing", "title": "GridToPix: Training Embodied Agents with Minimal Supervision", "comments": "Project page: https://unnat.github.io/gridtopix/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep reinforcement learning (RL) promises freedom from hand-labeled\ndata, great successes, especially for Embodied AI, require significant work to\ncreate supervision via carefully shaped rewards. Indeed, without shaped\nrewards, i.e., with only terminal rewards, present-day Embodied AI results\ndegrade significantly across Embodied AI problems from single-agent\nHabitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent\nAI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent\nGoogle Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1).\nAs training from shaped rewards doesn't scale to more realistic tasks, the\ncommunity needs to improve the success of training with terminal rewards. For\nthis we propose GridToPix: 1) train agents with terminal rewards in gridworlds\nthat generically mirror Embodied AI environments, i.e., they are independent of\nthe task; 2) distill the learned policy into agents that reside in complex\nvisual worlds. Despite learning from only terminal rewards with identical\nmodels and RL algorithms, GridToPix significantly improves results across\ntasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture\nMoving (success improves from 1% to 25%) to football gameplay (game score\nimproves from 0.1 to 0.6). GridToPix even helps to improve the results of\nshaped reward training.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:59:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jain", "Unnat", ""], ["Liu", "Iou-Jen", ""], ["Lazebnik", "Svetlana", ""], ["Kembhavi", "Aniruddha", ""], ["Weihs", "Luca", ""], ["Schwing", "Alexander", ""]]}, {"id": "2105.00933", "submitter": "Saranga Mahanta", "authors": "Saranga Kingkor Mahanta, Abdullah Faiz Ur Rahman Khilji, Partha Pakray", "title": "Deep Neural Network for Musical Instrument Recognition using MFCCs", "comments": "Was suggested to upload on a later date", "journal-ref": "Computacion y Sistemas, Vol 25, No 2 (2021): 25(2) 2021", "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of efficient automatic music classification is of vital importance\nand forms the basis for various advanced applications of AI in the musical\ndomain. Musical instrument recognition is the task of instrument identification\nby virtue of its audio. This audio, also termed as the sound vibrations are\nleveraged by the model to match with the instrument classes. In this paper, we\nuse an artificial neural network (ANN) model that was trained to perform\nclassification on twenty different classes of musical instruments. Here we use\nuse only the mel-frequency cepstral coefficients (MFCCs) of the audio data. Our\nproposed model trains on the full London philharmonic orchestra dataset which\ncontains twenty classes of instruments belonging to the four families viz.\nwoodwinds, brass, percussion, and strings. Based on experimental results our\nmodel achieves state-of-the-art accuracy on the same.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:10:34 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 13:32:28 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mahanta", "Saranga Kingkor", ""], ["Khilji", "Abdullah Faiz Ur Rahman", ""], ["Pakray", "Partha", ""]]}, {"id": "2105.00934", "submitter": "Una Pale", "authors": "Una Pale, Tomas Teijeiro, David Atienza", "title": "Systematic Assessment of Hyperdimensional Computing for Epileptic\n  Seizure Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperdimensional computing is a promising novel paradigm for low-power\nembedded machine learning. It has been applied on different biomedical\napplications, and particularly on epileptic seizure detection. Unfortunately,\ndue to differences in data preparation, segmentation, encoding strategies, and\nperformance metrics, results are hard to compare, which makes building upon\nthat knowledge difficult. Thus, the main goal of this work is to perform a\nsystematic assessment of the HD computing framework for the detection of\nepileptic seizures, comparing different feature approaches mapped to HD\nvectors. More precisely, we test two previously implemented features as well as\nseveral novel approaches with HD computing on epileptic seizure detection. We\nevaluate them in a comparable way, i.e., with the same preprocessing setup, and\nwith the identical performance measures. We use two different datasets in order\nto assess the generalizability of our conclusions. The systematic assessment\ninvolved three primary aspects relevant for potential wearable implementations:\n1) detection performance, 2) memory requirements, and 3) computational\ncomplexity. Our analysis shows a significant difference in detection\nperformance between approaches, but also that the ones with the highest\nperformance might not be ideal for wearable applications due to their high\nmemory or computational requirements. Furthermore, we evaluate a\npost-processing strategy to adjust the predictions to the dynamics of epileptic\nseizures, showing that performance is significantly improved in all the\napproaches and also that after post-processing, differences in performance are\nmuch smaller between approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:11:08 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Pale", "Una", ""], ["Teijeiro", "Tomas", ""], ["Atienza", "David", ""]]}, {"id": "2105.00937", "submitter": "Kwang Hee Lee", "authors": "Kwang Hee Lee, Chaewon Park, Junghyun Oh, Nojun Kwak", "title": "LFI-CAM: Learning Feature Importance for Better Visual Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Class Activation Mapping (CAM) is a powerful technique used to understand the\ndecision making of Convolutional Neural Network (CNN) in computer vision.\nRecently, there have been attempts not only to generate better visual\nexplanations, but also to improve classification performance using visual\nexplanations. However, the previous works still have their own drawbacks. In\nthis paper, we propose a novel architecture, LFI-CAM, which is trainable for\nimage classification and visual explanation in an end-to-end manner. LFI-CAM\ngenerates an attention map for visual explanation during forward propagation,\nat the same time, leverages the attention map to improve the classification\nperformance through the attention mechanism. Our Feature Importance Network\n(FIN) focuses on learning the feature importance instead of directly learning\nthe attention map to obtain a more reliable and consistent attention map. We\nconfirmed that LFI-CAM model is optimized not only by learning the feature\nimportance but also by enhancing the backbone feature representation to focus\nmore on important features of the input image. Experimental results show that\nLFI-CAM outperforms the baseline models's accuracy on the classification tasks\nas well as significantly improves on the previous works in terms of attention\nmap quality and stability over different hyper-parameters.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:12:21 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lee", "Kwang Hee", ""], ["Park", "Chaewon", ""], ["Oh", "Junghyun", ""], ["Kwak", "Nojun", ""]]}, {"id": "2105.00944", "submitter": "Antonio Bruto da Costa", "authors": "Priyanka Sinha, Pabitra Mitra, Antonio Anastasio Bruto da Costa,\n  Nikolaos Kekatos", "title": "Explaining Outcomes of Multi-Party Dialogues using Causal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-party dialogues are common in enterprise social media on technical as\nwell as non-technical topics. The outcome of a conversation may be positive or\nnegative. It is important to analyze why a dialogue ends with a particular\nsentiment from the point of view of conflict analysis as well as future\ncollaboration design. We propose an explainable time series mining algorithm\nfor such analysis. A dialogue is represented as an attributed time series of\noccurrences of keywords, EMPATH categories, and inferred sentiments at various\npoints in its progress. A special decision tree, with decision metrics that\ntake into account temporal relationships between dialogue events, is used for\npredicting the cause of the outcome sentiment. Interpretable rules mined from\nthe classifier are used to explain the prediction. Experimental results are\npresented for the enterprise social media posts in a large company.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:18:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Sinha", "Priyanka", ""], ["Mitra", "Pabitra", ""], ["da Costa", "Antonio Anastasio Bruto", ""], ["Kekatos", "Nikolaos", ""]]}, {"id": "2105.00956", "submitter": "Jing Huang", "authors": "Jing Huang, Jie Yang", "title": "UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hypergraph, an expressive structure with flexibility to model the\nhigher-order correlations among entities, has recently attracted increasing\nattention from various research domains. Despite the success of Graph Neural\nNetworks (GNNs) for graph representation learning, how to adapt the powerful\nGNN-variants directly into hypergraphs remains a challenging problem. In this\npaper, we propose UniGNN, a unified framework for interpreting the message\npassing process in graph and hypergraph neural networks, which can generalize\ngeneral GNN models into hypergraphs. In this framework, meticulously-designed\narchitectures aiming to deepen GNNs can also be incorporated into hypergraphs\nwith the least effort. Extensive experiments have been conducted to demonstrate\nthe effectiveness of UniGNN on multiple real-world datasets, which outperform\nthe state-of-the-art approaches with a large margin. Especially for the DBLP\ndataset, we increase the accuracy from 77.4\\% to 88.8\\% in the semi-supervised\nhypernode classification task. We further prove that the proposed\nmessage-passing based UniGNN models are at most as powerful as the\n1-dimensional Generalized Weisfeiler-Leman (1-GWL) algorithm in terms of\ndistinguishing non-isomorphic hypergraphs. Our code is available at\n\\url{https://github.com/OneForward/UniGNN}.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:48:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Huang", "Jing", ""], ["Yang", "Jie", ""]]}, {"id": "2105.00990", "submitter": "Jaime Ide", "authors": "Adrian P. Pope, Jaime S. Ide, Daria Micovic, Henry Diaz, David\n  Rosenbluth, Lee Ritholtz, Jason C. Twedt, Thayne T. Walker, Kevin Alcedo and\n  Daniel Javorsek", "title": "Hierarchical Reinforcement Learning for Air-to-Air Combat", "comments": "10 pages, 10 figures, The 2021 International Conference on Unmanned\n  Aircraft System (ICUAS 21), June 15-18, 2021, Athens, Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) is becoming a critical component in the defense\nindustry, as recently demonstrated by DARPA`s AlphaDogfight Trials (ADT). ADT\nsought to vet the feasibility of AI algorithms capable of piloting an F-16 in\nsimulated air-to-air combat. As a participant in ADT, Lockheed Martin`s (LM)\napproach combines a hierarchical architecture with maximum-entropy\nreinforcement learning (RL), integrates expert knowledge through reward\nshaping, and supports modularity of policies. This approach achieved a $2^{nd}$\nplace finish in the final ADT event (among eight total competitors) and\ndefeated a graduate of the US Air Force's (USAF) F-16 Weapons Instructor Course\nin match play.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:40:00 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 15:36:35 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Pope", "Adrian P.", ""], ["Ide", "Jaime S.", ""], ["Micovic", "Daria", ""], ["Diaz", "Henry", ""], ["Rosenbluth", "David", ""], ["Ritholtz", "Lee", ""], ["Twedt", "Jason C.", ""], ["Walker", "Thayne T.", ""], ["Alcedo", "Kevin", ""], ["Javorsek", "Daniel", ""]]}, {"id": "2105.00996", "submitter": "Arash Amini", "authors": "Arash Amini, Guangyi Liu, Nader Motee", "title": "Robust Learning of Recurrent Neural Networks in Presence of Exogenous\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent Neural networks (RNN) have shown promising potential for learning\ndynamics of sequential data. However, artificial neural networks are known to\nexhibit poor robustness in presence of input noise, where the sequential\narchitecture of RNNs exacerbates the problem. In this paper, we will use ideas\nfrom control and estimation theories to propose a tractable robustness analysis\nfor RNN models that are subject to input noise. The variance of the output of\nthe noisy system is adopted as a robustness measure to quantify the impact of\nnoise on learning. It is shown that the robustness measure can be estimated\nefficiently using linearization techniques. Using these results, we proposed a\nlearning method to enhance robustness of a RNN with respect to exogenous\nGaussian noise with known statistics. Our extensive simulations on benchmark\nproblems reveal that our proposed methodology significantly improves robustness\nof recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:45:05 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 15:08:48 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Amini", "Arash", ""], ["Liu", "Guangyi", ""], ["Motee", "Nader", ""]]}, {"id": "2105.00997", "submitter": "Daphna Keidar", "authors": "Cristina Guzman, Daphna Keidar, Tristan Meynier, Andreas Opedal,\n  Niklas Stoehr", "title": "Recovering Barab\\'asi-Albert Parameters of Graphs through\n  Disentanglement", "comments": "Accepted at the 9th International Conference on Learning\n  Representations (ICLR 2021), Workshop on Geometrical and Topological\n  Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Classical graph modeling approaches such as Erd\\H{o}s R\\'{e}nyi (ER) random\ngraphs or Barab\\'asi-Albert (BA) graphs, here referred to as stylized models,\naim to reproduce properties of real-world graphs in an interpretable way. While\nuseful, graph generation with stylized models requires domain knowledge and\niterative trial and error simulation. Previous work by Stoehr et al. (2019)\naddresses these issues by learning the generation process from graph data,\nusing a disentanglement-focused deep autoencoding framework, more specifically,\na $\\beta$-Variational Autoencoder ($\\beta$-VAE). While they successfully\nrecover the generative parameters of ER graphs through the model's latent\nvariables, their model performs badly on sequentially generated graphs such as\nBA graphs, due to their oversimplified decoder. We focus on recovering the\ngenerative parameters of BA graphs by replacing their $\\beta$-VAE decoder with\na sequential one. We first learn the generative BA parameters in a supervised\nfashion using a Graph Neural Network (GNN) and a Random Forest Regressor, by\nminimizing the squared loss between the true generative parameters and the\nlatent variables. Next, we train a $\\beta$-VAE model, combining the GNN encoder\nfrom the first stage with an LSTM-based decoder with a customized loss.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:45:43 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 10:40:07 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Guzman", "Cristina", ""], ["Keidar", "Daphna", ""], ["Meynier", "Tristan", ""], ["Opedal", "Andreas", ""], ["Stoehr", "Niklas", ""]]}, {"id": "2105.01004", "submitter": "Manjeet Dahiya", "authors": "Sanidhya Singal, Piyush Singh, Manjeet Dahiya", "title": "Automatic Collection Creation and Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a collection recommender system that can automatically create and\nrecommend collections of items at a user level. Unlike regular recommender\nsystems, which output top-N relevant items, a collection recommender system\noutputs collections of items such that the items in the collections are\nrelevant to a user, and the items within a collection follow a specific theme.\nOur system builds on top of the user-item representations learnt by item\nrecommender systems. We employ dimensionality reduction and clustering\ntechniques along with intuitive heuristics to create collections with their\nratings and titles.\n  We test these ideas in a real-world setting of music recommendation, within a\npopular music streaming service. We find that there is a 2.3x increase in\nrecommendation-driven consumption when recommending collections over items.\nFurther, it results in effective utilization of real estate and leads to\nrecommending a more and diverse set of items. To our knowledge, these are first\nof its kind experiments at such a large scale.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:51:21 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Singal", "Sanidhya", ""], ["Singh", "Piyush", ""], ["Dahiya", "Manjeet", ""]]}, {"id": "2105.01006", "submitter": "Yotam Barnoy", "authors": "Yotam Barnoy, Molly O'Brien, Will Wang, Gregory Hager", "title": "Robotic Surgery With Lean Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As surgical robots become more common, automating away some of the burden of\ncomplex direct human operation becomes ever more feasible. Model-free\nreinforcement learning (RL) is a promising direction toward generalizable\nautomated surgical performance, but progress has been slowed by the lack of\nefficient and realistic learning environments. In this paper, we describe\nadding reinforcement learning support to the da Vinci Skill Simulator, a\ntraining simulation used around the world to allow surgeons to learn and\nrehearse technical skills. We successfully teach an RL-based agent to perform\nsub-tasks in the simulator environment, using either image or state data. As\nfar as we know, this is the first time an RL-based agent is taught from visual\ndata in a surgical robotics environment. Additionally, we tackle the sample\ninefficiency of RL using a simple-to-implement system which we term\nhybrid-batch learning (HBL), effectively adding a second, long-term replay\nbuffer to the Q-learning process. Additionally, this allows us to bootstrap\nlearning from images from the data collected using the easier task of learning\nfrom state. We show that HBL decreases our learning times significantly.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:52:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Barnoy", "Yotam", ""], ["O'Brien", "Molly", ""], ["Wang", "Will", ""], ["Hager", "Gregory", ""]]}, {"id": "2105.01015", "submitter": "Thomas Elsken", "authors": "Julia Guerrero-Viu, Sven Hauns, Sergio Izquierdo, Guilherme Miotto,\n  Simon Schrodi, Andre Biedenkapp, Thomas Elsken, Difan Deng, Marius Lindauer,\n  Frank Hutter", "title": "Bag of Baselines for Multi-objective Joint Neural Architecture Search\n  and Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural architecture search (NAS) and hyperparameter optimization (HPO) make\ndeep learning accessible to non-experts by automatically finding the\narchitecture of the deep neural network to use and tuning the hyperparameters\nof the used training pipeline. While both NAS and HPO have been studied\nextensively in recent years, NAS methods typically assume fixed hyperparameters\nand vice versa - there exists little work on joint NAS + HPO. Furthermore, NAS\nhas recently often been framed as a multi-objective optimization problem, in\norder to take, e.g., resource requirements into account. In this paper, we\npropose a set of methods that extend current approaches to jointly optimize\nneural architectures and hyperparameters with respect to multiple objectives.\nWe hope that these methods will serve as simple baselines for future research\non multi-objective joint NAS + HPO. To facilitate this, all our code is\navailable at https://github.com/automl/multi-obj-baselines.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:04:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Guerrero-Viu", "Julia", ""], ["Hauns", "Sven", ""], ["Izquierdo", "Sergio", ""], ["Miotto", "Guilherme", ""], ["Schrodi", "Simon", ""], ["Biedenkapp", "Andre", ""], ["Elsken", "Thomas", ""], ["Deng", "Difan", ""], ["Lindauer", "Marius", ""], ["Hutter", "Frank", ""]]}, {"id": "2105.01016", "submitter": "Yueying Ni", "authors": "Yueying Ni, Yin Li, Patrick Lachance, Rupert A. C. Croft, Tiziana Di\n  Matteo, Simeon Bird, Yu Feng", "title": "AI-assisted super-resolution cosmological simulations II: Halo\n  substructures, velocities and higher order statistics", "comments": "13 pages, 11 figures, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we expand and test the capabilities of our recently developed\nsuper-resolution (SR) model to generate high-resolution (HR) realizations of\nthe full phase-space matter distribution, including both displacement and\nvelocity, from computationally cheap low-resolution (LR) cosmological N-body\nsimulations. The SR model enhances the simulation resolution by generating 512\ntimes more tracer particles, extending into the deeply non-linear regime where\ncomplex structure formation processes take place. We validate the SR model by\ndeploying the model in 10 test simulations of box size 100 Mpc/h, and examine\nthe matter power spectra, bispectra and 2D power spectra in redshift space. We\nfind the generated SR field matches the true HR result at percent level down to\nscales of k ~ 10 h/Mpc. We also identify and inspect dark matter halos and\ntheir substructures. Our SR model generate visually authentic small-scale\nstructures, that cannot be resolved by the LR input, and are in good\nstatistical agreement with the real HR results. The SR model performs\nsatisfactorily on the halo occupation distribution, halo correlations in both\nreal and redshift space, and the pairwise velocity distribution, matching the\nHR results with comparable scatter, thus demonstrating its potential in making\nmock halo catalogs. The SR technique can be a powerful and promising tool for\nmodelling small-scale galaxy formation physics in large cosmological volumes.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:08:21 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ni", "Yueying", ""], ["Li", "Yin", ""], ["Lachance", "Patrick", ""], ["Croft", "Rupert A. C.", ""], ["Di Matteo", "Tiziana", ""], ["Bird", "Simeon", ""], ["Feng", "Yu", ""]]}, {"id": "2105.01029", "submitter": "Mikhail Khodak", "authors": "Mikhail Khodak and Neil Tenenholtz and Lester Mackey and Nicol\\`o Fusi", "title": "Initialization and Regularization of Factorized Neural Layers", "comments": "ICLR 2021 Camera-Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorized layers--operations parameterized by products of two or more\nmatrices--occur in a variety of deep learning contexts, including compressed\nmodel training, certain types of knowledge distillation, and multi-head\nself-attention architectures. We study how to initialize and regularize deep\nnets containing such layers, examining two simple, understudied schemes,\nspectral initialization and Frobenius decay, for improving their performance.\nThe guiding insight is to design optimization routines for these networks that\nare as close as possible to that of their well-tuned, non-decomposed\ncounterparts; we back this intuition with an analysis of how the initialization\nand regularization schemes impact training with gradient descent, drawing on\nmodern attempts to understand the interplay of weight-decay and\nbatch-normalization. Empirically, we highlight the benefits of spectral\ninitialization and Frobenius decay across a variety of settings. In model\ncompression, we show that they enable low-rank methods to significantly\noutperform both unstructured sparsity and tensor methods on the task of\ntraining low-memory residual networks; analogs of the schemes also improve the\nperformance of tensor decomposition techniques. For knowledge distillation,\nFrobenius decay enables a simple, overcomplete baseline that yields a compact\nmodel from over-parameterized training without requiring retraining with or\npruning a teacher network. Finally, we show how both schemes applied to\nmulti-head attention lead to improved performance on both translation and\nunsupervised pre-training.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:28:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Khodak", "Mikhail", ""], ["Tenenholtz", "Neil", ""], ["Mackey", "Lester", ""], ["Fusi", "Nicol\u00f2", ""]]}, {"id": "2105.01030", "submitter": "Jonathan F. MacArt", "authors": "Jonathan F. MacArt, Justin Sirignano, Jonathan B. Freund", "title": "Embedded training of neural-network sub-grid-scale turbulence models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The weights of a deep neural network model are optimized in conjunction with\nthe governing flow equations to provide a model for sub-grid-scale stresses in\na temporally developing plane turbulent jet at Reynolds number $Re_0=6\\,000$.\nThe objective function for training is first based on the instantaneous\nfiltered velocity fields from a corresponding direct numerical simulation, and\nthe training is by a stochastic gradient descent method, which uses the adjoint\nNavier--Stokes equations to provide the end-to-end sensitivities of the model\nweights to the velocity fields. In-sample and out-of-sample testing on multiple\ndual-jet configurations show that its required mesh density in each coordinate\ndirection for prediction of mean flow, Reynolds stresses, and spectra is half\nthat needed by the dynamic Smagorinsky model for comparable accuracy. The same\nneural-network model trained directly to match filtered sub-grid-scale stresses\n-- without the constraint of being embedded within the flow equations during\nthe training -- fails to provide a qualitatively correct prediction. The\ncoupled formulation is generalized to train based only on mean-flow and\nReynolds stresses, which are more readily available in experiments. The\nmean-flow training provides a robust model, which is important, though a\nsomewhat less accurate prediction for the same coarse meshes, as might be\nanticipated due to the reduced information available for training in this case.\nThe anticipated advantage of the formulation is that the inclusion of resolved\nphysics in the training increases its capacity to extrapolate. This is assessed\nfor the case of passive scalar transport, for which it outperforms established\nmodels due to improved mixing predictions.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:28:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["MacArt", "Jonathan F.", ""], ["Sirignano", "Justin", ""], ["Freund", "Jonathan B.", ""]]}, {"id": "2105.01031", "submitter": "Catherine Stinson", "authors": "Catherine Stinson", "title": "Algorithms are not neutral: Bias in collaborative filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Discussions of algorithmic bias tend to focus on examples where either the\ndata or the people building the algorithms are biased. This gives the\nimpression that clean data and good intentions could eliminate bias. The\nneutrality of the algorithms themselves is defended by prominent Artificial\nIntelligence researchers. However, algorithms are not neutral. In addition to\nbiased data and biased algorithm makers, AI algorithms themselves can be\nbiased. This is illustrated with the example of collaborative filtering, which\nis known to suffer from popularity, and homogenizing biases. Iterative\ninformation filtering algorithms in general create a selection bias in the\ncourse of learning from user responses to documents that the algorithm\nrecommended. These are not merely biases in the statistical sense; these\nstatistical biases can cause discriminatory outcomes. Data points on the\nmargins of distributions of human data tend to correspond to marginalized\npeople. Popularity and homogenizing biases have the effect of further\nmarginalizing the already marginal. This source of bias warrants serious\nattention given the ubiquity of algorithmic decision-making.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:28:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Stinson", "Catherine", ""]]}, {"id": "2105.01060", "submitter": "Chuang Gan", "authors": "Yilun Du, Chuang Gan, Phillip Isola", "title": "Curious Representation Learning for Embodied Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Self-supervised representation learning has achieved remarkable success in\nrecent years. By subverting the need for supervised labels, such approaches are\nable to utilize the numerous unlabeled images that exist on the Internet and in\nphotographic datasets. Yet to build truly intelligent agents, we must construct\nrepresentation learning algorithms that can learn not only from datasets but\nalso learn from environments. An agent in a natural environment will not\ntypically be fed curated data. Instead, it must explore its environment to\nacquire the data it will learn from. We propose a framework, curious\nrepresentation learning (CRL), which jointly learns a reinforcement learning\npolicy and a visual representation model. The policy is trained to maximize the\nerror of the representation learner, and in doing so is incentivized to explore\nits environment. At the same time, the learned representation becomes stronger\nand stronger as the policy feeds it ever harder data to learn from. Our learned\nrepresentations enable promising transfer to downstream navigation tasks,\nperforming better than or comparably to ImageNet pretraining without using any\nsupervision at all. In addition, despite being trained in simulation, our\nlearned representations can obtain interpretable results on real images.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:59:20 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Du", "Yilun", ""], ["Gan", "Chuang", ""], ["Isola", "Phillip", ""]]}, {"id": "2105.01064", "submitter": "Xiaocong Du", "authors": "Xiaocong Du, Bhargav Bhushanam, Jiecao Yu, Dhruv Choudhary, Tianxiang\n  Gao, Sherman Wong, Louis Feng, Jongsoo Park, Yu Cao, Arun Kejariwal", "title": "Alternate Model Growth and Pruning for Efficient Training of\n  Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning recommendation systems at scale have provided remarkable gains\nthrough increasing model capacity (i.e. wider and deeper neural networks), but\nit comes at significant training cost and infrastructure cost. Model pruning is\nan effective technique to reduce computation overhead for deep neural networks\nby removing redundant parameters. However, modern recommendation systems are\nstill thirsty for model capacity due to the demand for handling big data. Thus,\npruning a recommendation model at scale results in a smaller model capacity and\nconsequently lower accuracy. To reduce computation cost without sacrificing\nmodel capacity, we propose a dynamic training scheme, namely alternate model\ngrowth and pruning, to alternatively construct and prune weights in the course\nof training. Our method leverages structured sparsification to reduce\ncomputational cost without hurting the model capacity at the end of offline\ntraining so that a full-size model is available in the recurring training stage\nto learn new data in real-time. To the best of our knowledge, this is the first\nwork to provide in-depth experiments and discussion of applying structural\ndynamics to recommendation systems at scale to reduce training cost. The\nproposed method is validated with an open-source deep-learning recommendation\nmodel (DLRM) and state-of-the-art industrial-scale production models.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 03:14:30 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Du", "Xiaocong", ""], ["Bhushanam", "Bhargav", ""], ["Yu", "Jiecao", ""], ["Choudhary", "Dhruv", ""], ["Gao", "Tianxiang", ""], ["Wong", "Sherman", ""], ["Feng", "Louis", ""], ["Park", "Jongsoo", ""], ["Cao", "Yu", ""], ["Kejariwal", "Arun", ""]]}, {"id": "2105.01092", "submitter": "Johannes De Smedt", "authors": "Johannes De Smedt, Anton Yeshchenko, Artem Polyvyanyy, Jochen De\n  Weerdt, Jan Mendling", "title": "Process Model Forecasting Using Time Series Analysis of Event Sequence\n  Data", "comments": "Accepted at the International Conference on Conceptual Modeling 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process analytics is an umbrella of data-driven techniques which includes\nmaking predictions for individual process instances or overall process models.\nAt the instance level, various novel techniques have been recently devised,\ntackling next activity, remaining time, and outcome prediction. At the model\nlevel, there is a notable void. It is the ambition of this paper to fill this\ngap. To this end, we develop a technique to forecast the entire process model\nfrom historical event data. A forecasted model is a will-be process model\nrepresenting a probable future state of the overall process. Such a forecast\nhelps to investigate the consequences of drift and emerging bottlenecks. Our\ntechnique builds on a representation of event data as multiple time series,\neach capturing the evolution of a behavioural aspect of the process model, such\nthat corresponding forecasting techniques can be applied. Our implementation\ndemonstrates the accuracy of our technique on real-world event log data.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:00:27 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 11:56:43 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["De Smedt", "Johannes", ""], ["Yeshchenko", "Anton", ""], ["Polyvyanyy", "Artem", ""], ["De Weerdt", "Jochen", ""], ["Mendling", "Jan", ""]]}, {"id": "2105.01099", "submitter": "Zhiwei Qin", "authors": "Zhiwei Qin, Hongtu Zhu, and Jieping Ye", "title": "Reinforcement Learning for Ridesharing: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present a comprehensive, in-depth survey of the literature\non reinforcement learning approaches to ridesharing problems. Papers on the\ntopics of rideshare matching, vehicle repositioning, ride-pooling, and dynamic\npricing are covered. Popular data sets and open simulation environments are\nalso introduced. Subsequently, we discuss a number of challenges and\nopportunities for reinforcement learning research on this important domain.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:09:58 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Qin", "Zhiwei", ""], ["Zhu", "Hongtu", ""], ["Ye", "Jieping", ""]]}, {"id": "2105.01119", "submitter": "Ankit Vani", "authors": "Ankit Vani, Max Schwarzer, Yuchen Lu, Eeshan Dhekane, Aaron Courville", "title": "Iterated learning for emergent systematicity in VQA", "comments": "Published as a conference paper at ICLR 2021. 9 pages main, 21 pages\n  total including references and appendix", "journal-ref": "9th International Conference on Learning Representations (ICLR\n  2021)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although neural module networks have an architectural bias towards\ncompositionality, they require gold standard layouts to generalize\nsystematically in practice. When instead learning layouts and modules jointly,\ncompositionality does not arise automatically and an explicit pressure is\nnecessary for the emergence of layouts exhibiting the right structure. We\npropose to address this problem using iterated learning, a cognitive science\ntheory of the emergence of compositional languages in nature that has primarily\nbeen applied to simple referential games in machine learning. Considering the\nlayouts of module networks as samples from an emergent language, we use\niterated learning to encourage the development of structure within this\nlanguage. We show that the resulting layouts support systematic generalization\nin neural agents solving the more complex task of visual question-answering.\nOur regularized iterated learning method can outperform baselines without\niterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a\nnew split of the SHAPES dataset we introduce to evaluate systematic\ngeneralization, and on CLOSURE, an extension of CLEVR also designed to test\nsystematic generalization. We demonstrate superior performance in recovering\nground-truth compositional program structure with limited supervision on both\nSHAPES-SyGeT and CLEVR.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:44:06 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Vani", "Ankit", ""], ["Schwarzer", "Max", ""], ["Lu", "Yuchen", ""], ["Dhekane", "Eeshan", ""], ["Courville", "Aaron", ""]]}, {"id": "2105.01125", "submitter": "Rui Henriques", "authors": "Cl\\'audio Sardinha, Anna C. Finamore, Rui Henriques", "title": "Context-aware demand prediction in bike sharing systems: incorporating\n  spatial, meteorological and calendrical context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bike sharing demand is increasing in large cities worldwide. The proper\nfunctioning of bike-sharing systems is, nevertheless, dependent on a balanced\ngeographical distribution of bicycles throughout a day. In this context,\nunderstanding the spatiotemporal distribution of check-ins and check-outs is\nkey for station balancing and bike relocation initiatives. Still, recent\ncontributions from deep learning and distance-based predictors show limited\nsuccess on forecasting bike sharing demand. This consistent observation is\nhypothesized to be driven by: i) the strong dependence between demand and the\nmeteorological and situational context of stations; and ii) the absence of\nspatial awareness as most predictors are unable to model the effects of\nhigh-low station load on nearby stations.\n  This work proposes a comprehensive set of new principles to incorporate both\nhistorical and prospective sources of spatial, meteorological, situational and\ncalendrical context in predictive models of station demand. To this end, a new\nrecurrent neural network layering composed by serial long-short term memory\n(LSTM) components is proposed with two major contributions: i) the feeding of\nmultivariate time series masks produced from historical context data at the\ninput layer, and ii) the time-dependent regularization of the forecasted time\nseries using prospective context data. This work further assesses the impact of\nincorporating different sources of context, showing the relevance of the\nproposed principles for the community even though not all improvements from the\ncontext-aware predictors yield statistical significance.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:53:58 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sardinha", "Cl\u00e1udio", ""], ["Finamore", "Anna C.", ""], ["Henriques", "Rui", ""]]}, {"id": "2105.01128", "submitter": "Eloy Philip Theo Geenjaar", "authors": "Eloy Geenjaar, Noah Lewis, Zening Fu, Rohan Venkatdas, Sergey Plis,\n  Vince Calhoun", "title": "Fusing multimodal neuroimaging data with a variational autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuroimaging studies often involve the collection of multiple data\nmodalities. These modalities contain both shared and mutually exclusive\ninformation about the brain. This work aims at finding a scalable and\ninterpretable method to fuse the information of multiple neuroimaging\nmodalities using a variational autoencoder (VAE). To provide an initial\nassessment, this work evaluates the representations that are learned using a\nschizophrenia classification task. A support vector machine trained on the\nrepresentations achieves an area under the curve for the classifier's receiver\noperating characteristic (ROC-AUC) of 0.8610.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 19:03:41 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Geenjaar", "Eloy", ""], ["Lewis", "Noah", ""], ["Fu", "Zening", ""], ["Venkatdas", "Rohan", ""], ["Plis", "Sergey", ""], ["Calhoun", "Vince", ""]]}, {"id": "2105.01136", "submitter": "Chengzhuo Ni", "authors": "Chengzhuo Ni, Anru Zhang, Yaqi Duan, Mengdi Wang", "title": "Learning Good State and Action Representations via Tensor Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The transition kernel of a continuous-state-action Markov decision process\n(MDP) admits a natural tensor structure. This paper proposes a tensor-inspired\nunsupervised learning method to identify meaningful low-dimensional state and\naction representations from empirical trajectories. The method exploits the\nMDP's tensor structure by kernelization, importance sampling and\nlow-Tucker-rank approximation. This method can be further used to cluster\nstates and actions respectively and find the best discrete MDP abstraction. We\nprovide sharp statistical error bounds for tensor concentration and the\npreservation of diffusion distance after embedding.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 19:24:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ni", "Chengzhuo", ""], ["Zhang", "Anru", ""], ["Duan", "Yaqi", ""], ["Wang", "Mengdi", ""]]}, {"id": "2105.01159", "submitter": "Farnaz H Foomani", "authors": "Farnaz H. Foomani, D. M. Anisuzzaman, Jeffrey Niezgoda, Jonathan\n  Niezgoda, William Guns, Sandeep Gopalakrishnan, Zeyun Yu", "title": "Synthesizing time-series wound prognosis factors from electronic medical\n  records using generative adversarial networks", "comments": "20 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wound prognostic models not only provide an estimate of wound healing time to\nmotivate patients to follow up their treatments but also can help clinicians to\ndecide whether to use a standard care or adjuvant therapies and to assist them\nwith designing clinical trials. However, collecting prognosis factors from\nElectronic Medical Records (EMR) of patients is challenging due to privacy,\nsensitivity, and confidentiality. In this study, we developed time series\nmedical generative adversarial networks (GANs) to generate synthetic wound\nprognosis factors using very limited information collected during routine care\nin a specialized wound care facility. The generated prognosis variables are\nused in developing a predictive model for chronic wound healing trajectory. Our\nnovel medical GAN can produce both continuous and categorical features from\nEMR. Moreover, we applied temporal information to our model by considering data\ncollected from the weekly follow-ups of patients. Conditional training\nstrategies were utilized to enhance training and generate classified data in\nterms of healing or non-healing. The ability of the proposed model to generate\nrealistic EMR data was evaluated by TSTR (test on the synthetic, train on the\nreal), discriminative accuracy, and visualization. We utilized samples\ngenerated by our proposed GAN in training a prognosis model to demonstrate its\nreal-life application. Using the generated samples in training predictive\nmodels improved the classification accuracy by 6.66-10.01% compared to the\nprevious EMR-GAN. Additionally, the suggested prognosis classifier has achieved\nthe area under the curve (AUC) of 0.975, 0.968, and 0.849 when training the\nnetwork using data from the first three visits, first two visits, and first\nvisit, respectively. These results indicate a significant improvement in wound\nhealing prediction compared to the previous prognosis models.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 20:26:48 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Foomani", "Farnaz H.", ""], ["Anisuzzaman", "D. M.", ""], ["Niezgoda", "Jeffrey", ""], ["Niezgoda", "Jonathan", ""], ["Guns", "William", ""], ["Gopalakrishnan", "Sandeep", ""], ["Yu", "Zeyun", ""]]}, {"id": "2105.01160", "submitter": "David Rousseau", "authors": "Sabrina Amrouche, Laurent Basara, Paolo Calafiura, Dmitry Emeliyanov,\n  Victor Estrade, Steven Farrell, C\\'ecile Germain, Vladimir Vava Gligorov,\n  Tobias Golling, Sergey Gorbunov, Heather Gray, Isabelle Guyon, Mikhail\n  Hushchyn, Vincenzo Innocente, Moritz Kiehn, Marcel Kunze, Edward Moyse, David\n  Rousseau, Andreas Salzburger, Andrey Ustyuzhanin, Jean-Roch Vlimant", "title": "The Tracking Machine Learning challenge : Throughput phase", "comments": "submitted to Computing and Software for Big Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports on the second \"Throughput\" phase of the Tracking Machine\nLearning (TrackML) challenge on the Codalab platform. As in the first\n\"Accuracy\" phase, the participants had to solve a difficult experimental\nproblem linked to tracking accurately the trajectory of particles as e.g.\ncreated at the Large Hadron Collider (LHC): given O($10^5$) points, the\nparticipants had to connect them into O($10^4$) individual groups that\nrepresent the particle trajectories which are approximated helical. While in\nthe first phase only the accuracy mattered, the goal of this second phase was a\ncompromise between the accuracy and the speed of inference. Both were measured\non the Codalab platform where the participants had to upload their software.\nThe best three participants had solutions with good accuracy and speed an order\nof magnitude faster than the state of the art when the challenge was designed.\nAlthough the core algorithms were less diverse than in the first phase, a\ndiversity of techniques have been used and are described in this paper. The\nperformance of the algorithms are analysed in depth and lessons derived.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 20:31:20 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 09:29:11 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Amrouche", "Sabrina", ""], ["Basara", "Laurent", ""], ["Calafiura", "Paolo", ""], ["Emeliyanov", "Dmitry", ""], ["Estrade", "Victor", ""], ["Farrell", "Steven", ""], ["Germain", "C\u00e9cile", ""], ["Gligorov", "Vladimir Vava", ""], ["Golling", "Tobias", ""], ["Gorbunov", "Sergey", ""], ["Gray", "Heather", ""], ["Guyon", "Isabelle", ""], ["Hushchyn", "Mikhail", ""], ["Innocente", "Vincenzo", ""], ["Kiehn", "Moritz", ""], ["Kunze", "Marcel", ""], ["Moyse", "Edward", ""], ["Rousseau", "David", ""], ["Salzburger", "Andreas", ""], ["Ustyuzhanin", "Andrey", ""], ["Vlimant", "Jean-Roch", ""]]}, {"id": "2105.01171", "submitter": "Kexin Huang", "authors": "Kexin Huang, Cao Xiao, Lucas M. Glass, Cathy W. Critchlow, Greg\n  Gibson, Jimeng Sun", "title": "Machine Learning Applications for Therapeutic Tasks with Genomics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the increasing availability of genomics and other biomedical data,\nmany machine learning approaches have been proposed for a wide range of\ntherapeutic discovery and development tasks. In this survey, we review the\nliterature on machine learning applications for genomics through the lens of\ntherapeutic development. We investigate the interplay among genomics,\ncompounds, proteins, electronic health records (EHR), cellular images, and\nclinical texts. We identify twenty-two machine learning in genomics\napplications across the entire therapeutics pipeline, from discovering novel\ntargets, personalized medicine, developing gene-editing tools all the way to\nclinical trials and post-market studies. We also pinpoint seven important\nchallenges in this field with opportunities for expansion and impact. This\nsurvey overviews recent research at the intersection of machine learning,\ngenomics, and therapeutic development.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:20:20 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Huang", "Kexin", ""], ["Xiao", "Cao", ""], ["Glass", "Lucas M.", ""], ["Critchlow", "Cathy W.", ""], ["Gibson", "Greg", ""], ["Sun", "Jimeng", ""]]}, {"id": "2105.01181", "submitter": "Ecem Sogancioglu", "authors": "Ecem Sogancioglu, Keelin Murphy, Ernst Th. Scholten, Luuk H. Boulogne,\n  Mathias Prokop, and Bram van Ginneken", "title": "Automated Estimation of Total Lung Volume using Chest Radiographs and\n  Deep Learning", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Total lung volume is an important quantitative biomarker and is used for the\nassessment of restrictive lung diseases. In this study, we investigate the\nperformance of several deep-learning approaches for automated measurement of\ntotal lung volume from chest radiographs. 7621 posteroanterior and lateral view\nchest radiographs (CXR) were collected from patients with chest CT available.\nSimilarly, 928 CXR studies were chosen from patients with pulmonary function\ntest (PFT) results. The reference total lung volume was calculated from lung\nsegmentation on CT or PFT data, respectively. This dataset was used to train\ndeep-learning architectures to predict total lung volume from chest\nradiographs. The experiments were constructed in a step-wise fashion with\nincreasing complexity to demonstrate the effect of training with CT-derived\nlabels only and the sources of error. The optimal models were tested on 291 CXR\nstudies with reference lung volume obtained from PFT. The optimal deep-learning\nregression model showed an MAE of 408 ml and a MAPE of 8.1\\% and Pearson's r =\n0.92 using both frontal and lateral chest radiographs as input. CT-derived\nlabels were useful for pre-training but the optimal performance was obtained by\nfine-tuning the network with PFT-derived labels. We demonstrate, for the first\ntime, that state-of-the-art deep learning solutions can accurately measure\ntotal lung volume from plain chest radiographs. The proposed model can be used\nto obtain total lung volume from routinely acquired chest radiographs at no\nadditional cost and could be a useful tool to identify trends over time in\npatients referred regularly for chest x-rays.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:35:16 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sogancioglu", "Ecem", ""], ["Murphy", "Keelin", ""], ["Scholten", "Ernst Th.", ""], ["Boulogne", "Luuk H.", ""], ["Prokop", "Mathias", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2105.01187", "submitter": "Zhengling Qi", "authors": "Zhengling Qi, Rui Miao, Xiaoke Zhang", "title": "Proximal Learning for Individualized Treatment Regimes Under Unmeasured\n  Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven individualized decision making has recently received increasing\nresearch interests. Most existing methods rely on the assumption of no\nunmeasured confounding, which unfortunately cannot be ensured in practice\nespecially in observational studies. Motivated by the recent proposed proximal\ncausal inference, we develop several proximal learning approaches to estimating\noptimal individualized treatment regimes (ITRs) in the presence of unmeasured\nconfounding. In particular, we establish several identification results for\ndifferent classes of ITRs, exhibiting the trade-off between the risk of making\nuntestable assumptions and the value function improvement in decision making.\nBased on these results, we propose several classification-based approaches to\nfinding a variety of restricted in-class optimal ITRs and develop their\ntheoretical properties. The appealing numerical performance of our proposed\nmethods is demonstrated via an extensive simulation study and one real data\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:49:49 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:35:00 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Qi", "Zhengling", ""], ["Miao", "Rui", ""], ["Zhang", "Xiaoke", ""]]}, {"id": "2105.01196", "submitter": "Patryk Orzechowski", "authors": "Pawe{\\l} Renc, Patryk Orzechowski, Aleksander Byrski, Jaros{\\l}aw\n  W\\k{a}s, and Jason H. Moore", "title": "EBIC.JL -- an Efficient Implementation of Evolutionary Biclustering\n  Algorithm in Julia", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": "10.1145/3449726.3463197", "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering is a data mining technique which searches for local patterns in\nnumeric tabular data with main application in bioinformatics. This technique\nhas shown promise in multiple areas, including development of biomarkers for\ncancer, disease subtype identification, or gene-drug interactions among others.\nIn this paper we introduce EBIC.JL - an implementation of one of the most\naccurate biclustering algorithms in Julia, a modern highly parallelizable\nprogramming language for data science. We show that the new version maintains\ncomparable accuracy to its predecessor EBIC while converging faster for the\nmajority of the problems. We hope that this open source software in a\nhigh-level programming language will foster research in this promising field of\nbioinformatics and expedite development of new biclustering methods for big\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:30:38 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Renc", "Pawe\u0142", ""], ["Orzechowski", "Patryk", ""], ["Byrski", "Aleksander", ""], ["W\u0105s", "Jaros\u0142aw", ""], ["Moore", "Jason H.", ""]]}, {"id": "2105.01198", "submitter": "Maysam Behmanesh", "authors": "Maysam Behmanesh, Peyman Adibi, Hossein Karshenas", "title": "Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set\n  Theory for Imbalanced Data Classification", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Support vector machines (SVMs) are powerful supervised learning tools\ndeveloped to solve classification problems. However, SVMs are likely to perform\npoorly in the classification of imbalanced data. The rough set theory presents\na mathematical tool for inference in nondeterministic cases that provides\nmethods for removing irrelevant information from data. In this work, we propose\nan approach that efficiently used fuzzy rough set theory in weighted least\nsquares twin support vector machine called FRLSTSVM for classification of\nimbalanced data. The first innovation is introducing a new fuzzy rough\nset-based under-sampling strategy to make the classifier robust in terms of the\nimbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM,\ndata points from the minority class remain unchanged while a subset of data\npoints in the majority class are selected using a new method. In this model, we\nembed the weight biases in the LSTSVM formulations to overcome the bias\nphenomenon in the original twin SVM for the classification of imbalanced data.\nIn order to determine these weights in this formulation, we introduce a new\nstrategy that uses fuzzy rough set theory as the second innovation.\nExperimental results on the famous imbalanced datasets, compared to the related\ntraditional SVM-based methods, demonstrate the superiority of the proposed\nFRLSTSVM model in the imbalanced data classification.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:33:39 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 20:29:05 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Behmanesh", "Maysam", ""], ["Adibi", "Peyman", ""], ["Karshenas", "Hossein", ""]]}, {"id": "2105.01202", "submitter": "Anli Ji", "authors": "Anli Ji, Berkay Aydin, Manolis K. Georgoulis, Rafal Angryk", "title": "All-Clear Flare Prediction Using Interval-based Time Series Classifiers", "comments": null, "journal-ref": null, "doi": "10.1109/BigData50022.2020.9377906", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An all-clear flare prediction is a type of solar flare forecasting that puts\nmore emphasis on predicting non-flaring instances (often relatively small\nflares and flare quiet regions) with high precision while still maintaining\nvaluable predictive results. While many flare prediction studies do not address\nthis problem directly, all-clear predictions can be useful in operational\ncontext. However, in all-clear predictions, finding the right balance between\navoiding false negatives (misses) and reducing the false positives (false\nalarms) is often challenging. Our study focuses on training and testing a set\nof interval-based time series classifiers named Time Series Forest (TSF). These\nclassifiers will be used towards building an all-clear flare prediction system\nby utilizing multivariate time series data. Throughout this paper, we\ndemonstrate our data collection, predictive model building and evaluation\nprocesses, and compare our time series classification models with baselines\nusing our benchmark datasets. Our results show that time series classifiers\nprovide better forecasting results in terms of skill scores, precision and\nrecall metrics, and they can be further improved for more precise all-clear\nforecasts by tuning model hyperparameters.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:40:05 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ji", "Anli", ""], ["Aydin", "Berkay", ""], ["Georgoulis", "Manolis K.", ""], ["Angryk", "Rafal", ""]]}, {"id": "2105.01209", "submitter": "Fabi\\'an Villena", "authors": "Fabi\\'an Villena", "title": "LaboRecommender: A crazy-easy to use Python-based recommender system for\n  laboratory tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Laboratory tests play a major role in clinical decision making because they\nare essential for the confirmation of diagnostics suspicions and influence\nmedical decisions. The number of different laboratory tests available to\nphysicians in our age has been expanding very rapidly due to the rapid advances\nin laboratory technology. To find the correct desired tests within this\nexpanding plethora of elements, the Health Information System must provide a\npowerful search engine and the practitioner need to remember the exact name of\nthe laboratory test to correctly select the bag of tests to order. Recommender\nsystems are platforms which suggest appropriate items to a user after learning\nthe users' behaviour. A neighbourhood-based collaborative filtering method was\nused to model the recommender system, where similar bags, clustered using\nnearest neighbours algorithm, are used to make recommendations of tests for\neach other similar bag of laboratory tests. The recommender system developed in\nthis paper achieved 95.54 % in the mean average precision metric. A fully\ndocumented Python package named LaboRecommender was developed to implement the\nalgorithm proposed in this paper\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 23:06:24 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Villena", "Fabi\u00e1n", ""]]}, {"id": "2105.01231", "submitter": "Xin Zhang", "authors": "Xin Zhang, Jia Liu, Zhengyuan Zhu, and Elizabeth S. Bentley", "title": "GT-STORM: Taming Sample, Communication, and Memory Complexities in\n  Decentralized Non-Convex Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized nonconvex optimization has received increasing attention in\nrecent years in machine learning due to its advantages in system robustness,\ndata privacy, and implementation simplicity. However, three fundamental\nchallenges in designing decentralized optimization algorithms are how to reduce\ntheir sample, communication, and memory complexities. In this paper, we propose\na \\underline{g}radient-\\underline{t}racking-based \\underline{sto}chastic\n\\underline{r}ecursive \\underline{m}omentum (GT-STORM) algorithm for efficiently\nsolving nonconvex optimization problems. We show that to reach an\n$\\epsilon^2$-stationary solution, the total number of sample evaluations of our\nalgorithm is $\\tilde{O}(m^{1/2}\\epsilon^{-3})$ and the number of communication\nrounds is $\\tilde{O}(m^{-1/2}\\epsilon^{-3})$, which improve the\n$O(\\epsilon^{-4})$ costs of sample evaluations and communications for the\nexisting decentralized stochastic gradient algorithms. We conduct extensive\nexperiments with a variety of learning models, including non-convex logistical\nregression and convolutional neural networks, to verify our theoretical\nfindings. Collectively, our results contribute to the state of the art of\ntheories and algorithms for decentralized network optimization.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 00:44:48 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 04:28:42 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhang", "Xin", ""], ["Liu", "Jia", ""], ["Zhu", "Zhengyuan", ""], ["Bentley", "Elizabeth S.", ""]]}, {"id": "2105.01238", "submitter": "Yue Li", "authors": "Ziyang Song, Xavier Sumba Toral, Yixin Xu, Aihua Liu, Liming Guo,\n  Guido Powell, Aman Verma, David Buckeridge, Ariane Marelli, Yue Li", "title": "Supervised multi-specialist topic model with applications on large-scale\n  electronic health record data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Electronic health record (EHR) data provides a new venue to\nelucidate disease comorbidities and latent phenotypes for precision medicine.\nTo fully exploit its potential, a realistic data generative process of the EHR\ndata needs to be modelled. We present MixEHR-S to jointly infer\nspecialist-disease topics from the EHR data. As the key contribution, we model\nthe specialist assignments and ICD-coded diagnoses as the latent topics based\non patient's underlying disease topic mixture in a novel unified supervised\nhierarchical Bayesian topic model. For efficient inference, we developed a\nclosed-form collapsed variational inference algorithm to learn the model\ndistributions of MixEHR-S. We applied MixEHR-S to two independent large-scale\nEHR databases in Quebec with three targeted applications: (1) Congenital Heart\nDisease (CHD) diagnostic prediction among 154,775 patients; (2) Chronic\nobstructive pulmonary disease (COPD) diagnostic prediction among 73,791\npatients; (3) future insulin treatment prediction among 78,712 patients\ndiagnosed with diabetes as a mean to assess the disease exacerbation. In all\nthree applications, MixEHR-S conferred clinically meaningful latent topics\namong the most predictive latent topics and achieved superior target prediction\naccuracy compared to the existing methods, providing opportunities for\nprioritizing high-risk patients for healthcare services. MixEHR-S source code\nand scripts of the experiments are freely available at\nhttps://github.com/li-lab-mcgill/mixehrS\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 01:27:11 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Song", "Ziyang", ""], ["Toral", "Xavier Sumba", ""], ["Xu", "Yixin", ""], ["Liu", "Aihua", ""], ["Guo", "Liming", ""], ["Powell", "Guido", ""], ["Verma", "Aman", ""], ["Buckeridge", "David", ""], ["Marelli", "Ariane", ""], ["Li", "Yue", ""]]}, {"id": "2105.01244", "submitter": "Oron Sabag", "authors": "Oron Sabag and Gautam Goel and Sahin Lale and Babak Hassibi", "title": "Regret-Optimal Full-Information Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the infinite-horizon, discrete-time full-information control\nproblem. Motivated by learning theory, as a criterion for controller design we\nfocus on regret, defined as the difference between the LQR cost of a causal\ncontroller (that has only access to past and current disturbances) and the LQR\ncost of a clairvoyant one (that has also access to future disturbances). In the\nfull-information setting, there is a unique optimal non-causal controller that\nin terms of LQR cost dominates all other controllers. Since the regret itself\nis a function of the disturbances, we consider the worst-case regret over all\npossible bounded energy disturbances, and propose to find a causal controller\nthat minimizes this worst-case regret. The resulting controller has the\ninterpretation of guaranteeing the smallest possible regret compared to the\nbest non-causal controller, no matter what the future disturbances are. We show\nthat the regret-optimal control problem can be reduced to a Nehari problem,\ni.e., to approximate an anticausal operator with a causal one in the operator\nnorm. In the state-space setting, explicit formulas for the optimal regret and\nfor the regret-optimal controller (in both the causal and the strictly causal\nsettings) are derived. The regret-optimal controller is the sum of the\nclassical $H_2$ state-feedback law and a finite-dimensional controller obtained\nfrom the Nehari problem. The controller construction simply requires the\nsolution to the standard LQR Riccati equation, in addition to two Lyapunov\nequations. Simulations over a range of plants demonstrates that the\nregret-optimal controller interpolates nicely between the $H_2$ and the\n$H_\\infty$ optimal controllers, and generally has $H_2$ and $H_\\infty$ costs\nthat are simultaneously close to their optimal values. The regret-optimal\ncontroller thus presents itself as a viable option for control system design.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 01:51:00 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sabag", "Oron", ""], ["Goel", "Gautam", ""], ["Lale", "Sahin", ""], ["Hassibi", "Babak", ""]]}, {"id": "2105.01254", "submitter": "Chanwoo Kim", "authors": "Chanwoo Kim, Abhinav Garg, Dhananjaya Gowda, Seongkyu Mun, and\n  Changwoo Han", "title": "Streaming end-to-end speech recognition with jointly trained neural\n  feature enhancement", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a streaming end-to-end speech recognition model\nbased on Monotonic Chunkwise Attention (MoCha) jointly trained with enhancement\nlayers. Even though the MoCha attention enables streaming speech recognition\nwith recognition accuracy comparable to a full attention-based approach,\ntraining this model is sensitive to various factors such as the difficulty of\ntraining examples, hyper-parameters, and so on. Because of these issues, speech\nrecognition accuracy of a MoCha-based model for clean speech drops\nsignificantly when a multi-style training approach is applied. Inspired by\nCurriculum Learning [1], we introduce two training strategies: Gradual\nApplication of Enhanced Features (GAEF) and Gradual Reduction of Enhanced Loss\n(GREL). With GAEF, the model is initially trained using clean features.\nSubsequently, the portion of outputs from the enhancement layers gradually\nincreases. With GREL, the portion of the Mean Squared Error (MSE) loss for the\nenhanced output gradually reduces as training proceeds. In experimental results\non the LibriSpeech corpus and noisy far-field test sets, the proposed model\nwith GAEF-GREL training strategies shows significantly better results than the\nconventional multi-style training approach.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 02:25:41 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kim", "Chanwoo", ""], ["Garg", "Abhinav", ""], ["Gowda", "Dhananjaya", ""], ["Mun", "Seongkyu", ""], ["Han", "Changwoo", ""]]}, {"id": "2105.01260", "submitter": "Songyan Xue", "authors": "Songyan Xue, Yi Ma, Na Yi", "title": "End-to-End Learning for Uplink MU-SIMO Joint Transmitter and\n  Non-Coherent Receiver Design in Fading Channels", "comments": "26 pages, 14 figures, transaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel end-to-end learning approach, namely JTRD-Net, is\nproposed for uplink multiuser single-input multiple-output (MU-SIMO) joint\ntransmitter and non-coherent receiver design (JTRD) in fading channels. The\nbasic idea lies in the use of artificial neural networks (ANNs) to replace\ntraditional communication modules at both transmitter and receiver sides. More\nspecifically, the transmitter side is modeled as a group of parallel linear\nlayers, which are responsible for multiuser waveform design; and the\nnon-coherent receiver is formed by a deep feed-forward neural network (DFNN) so\nas to provide multiuser detection (MUD) capabilities. The entire JTRD-Net can\nbe trained from end to end to adapt to channel statistics through deep\nlearning. After training, JTRD-Net can work efficiently in a non-coherent\nmanner without requiring any levels of channel state information (CSI). In\naddition to the network architecture, a novel weight-initialization method,\nnamely symmetrical-interval initialization, is proposed for JTRD-Net. It is\nshown that the symmetrical-interval initialization outperforms the conventional\nmethod (e.g. Xavier initialization) in terms of well-balanced convergence-rate\namong users. Simulation results show that the proposed JTRD-Net approach takes\nsignificant advantages in terms of reliability and scalability over baseline\nschemes on both i.i.d. complex Gaussian channels and spatially-correlated\nchannels.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 02:47:59 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Xue", "Songyan", ""], ["Ma", "Yi", ""], ["Yi", "Na", ""]]}, {"id": "2105.01274", "submitter": "Billy Pik Lik Lau", "authors": "Sumudu HasalaMarakkalage, Billy Pik Lik Lau, Yuren Zhou, Ran Liu, Chau\n  Yuen, Wei Quin Yow, Keng Hua Chong", "title": "WiFi Fingerprint Clustering for Urban Mobility Analysis", "comments": "accepted by IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3077583", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an unsupervised learning approach to identify the\nuser points of interest (POI) by exploiting WiFi measurements from smartphone\napplication data. Due to the lack of GPS positioning accuracy in indoor,\nsheltered, and high rise building environments, we rely on widely available\nWiFi access points (AP) in contemporary urban areas to accurately identify POI\nand mobility patterns, by comparing the similarity in the WiFi measurements. We\npropose a system architecture to scan the surrounding WiFi AP, and perform\nunsupervised learning to demonstrate that it is possible to identify three\nmajor insights, namely the indoor POI within a building, neighbourhood\nactivity, and micro-mobility of the users. Our results show that it is possible\nto identify the aforementioned insights, with the fusion of WiFi and GPS, which\nare not possible to identify by only using GPS.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 03:46:14 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["HasalaMarakkalage", "Sumudu", ""], ["Lau", "Billy Pik Lik", ""], ["Zhou", "Yuren", ""], ["Liu", "Ran", ""], ["Yuen", "Chau", ""], ["Yow", "Wei Quin", ""], ["Chong", "Keng Hua", ""]]}, {"id": "2105.01275", "submitter": "Yunxiang Zhao", "authors": "Yunsheng Pang, Yunxiang Zhao, Dongsheng Li", "title": "Graph Pooling via Coarsened Graph Infomax", "comments": null, "journal-ref": null, "doi": "10.1145/3404835.3463074", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pooling that summaries the information in a large graph into a compact\nform is essential in hierarchical graph representation learning. Existing graph\npooling methods either suffer from high computational complexity or cannot\ncapture the global dependencies between graphs before and after pooling. To\naddress the problems of existing graph pooling methods, we propose Coarsened\nGraph Infomax Pooling (CGIPool) that maximizes the mutual information between\nthe input and the coarsened graph of each pooling layer to preserve graph-level\ndependencies. To achieve mutual information neural maximization, we apply\ncontrastive learning and propose a self-attention-based algorithm for learning\npositive and negative samples. Extensive experimental results on seven datasets\nillustrate the superiority of CGIPool comparing to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 03:50:21 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:08:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pang", "Yunsheng", ""], ["Zhao", "Yunxiang", ""], ["Li", "Dongsheng", ""]]}, {"id": "2105.01276", "submitter": "Weijia Zhang", "authors": "Weijia Zhang", "title": "Non-I.I.D. Multi-Instance Learning for Predicting Instance and Bag\n  Labels using Variational Auto-Encoder", "comments": "To appear in Proceedings of the 30th International Joint Conference\n  on Artificial Intelligence (IJCAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance learning is a type of weakly supervised learning. It deals\nwith tasks where the data is a set of bags and each bag is a set of instances.\nOnly the bag labels are observed whereas the labels for the instances are\nunknown. An important advantage of multi-instance learning is that by\nrepresenting objects as a bag of instances, it is able to preserve the inherent\ndependencies among parts of the objects. Unfortunately, most existing\nalgorithms assume all instances to be \\textit{identically and independently\ndistributed}, which violates real-world scenarios since the instances within a\nbag are rarely independent. In this work, we propose the Multi-Instance\nVariational Auto-Encoder (MIVAE) algorithm which explicitly models the\ndependencies among the instances for predicting both bag labels and instance\nlabels. Experimental results on several multi-instance benchmarks and\nend-to-end medical imaging datasets demonstrate that MIVAE performs better than\nstate-of-the-art algorithms for both instance label and bag label prediction\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 03:50:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhang", "Weijia", ""]]}, {"id": "2105.01280", "submitter": "Feng Shi", "authors": "Feng Shi, Ahren Yiqiao Jin, Song-Chun Zhu", "title": "VersaGNN: a Versatile accelerator for Graph neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textit{Graph Neural Network} (GNN) is a promising approach for analyzing\ngraph-structured data that tactfully captures their dependency information via\nnode-level message passing. It has achieved state-of-the-art performances in\nmany tasks, such as node classification, graph matching, clustering, and graph\ngeneration. As GNNs operate on non-Euclidean data, their irregular data access\npatterns cause considerable computational costs and overhead on conventional\narchitectures, such as GPU and CPU. Our analysis shows that GNN adopts a hybrid\ncomputing model. The \\textit{Aggregation} (or \\textit{Message Passing}) phase\nperforms vector additions where vectors are fetched with irregular strides. The\n\\textit{Transformation} (or \\textit{Node Embedding}) phase can be either dense\nor sparse-dense matrix multiplication. In this work, We propose\n\\textit{VersaGNN}, an ultra-efficient, systolic-array-based versatile hardware\naccelerator that unifies dense and sparse matrix multiplication. By applying\nthis single optimized systolic array to both aggregation and transformation\nphases, we have significantly reduced chip sizes and energy consumption. We\nthen divide the computing engine into blocked systolic arrays to support the\n\\textit{Strassen}'s algorithm for dense matrix multiplication, dramatically\nscaling down the number of multiplications and enabling high-throughput\ncomputation of GNNs. To balance the workload of sparse-dense matrix\nmultiplication, we also introduced a greedy algorithm to combine sparse\nsub-matrices of compressed format into condensed ones to reduce computational\ncycles. Compared with current state-of-the-art GNN software frameworks,\n\\textit{VersaGNN} achieves on average 3712$\\times$ speedup with 1301.25$\\times$\nenergy reduction on CPU, and 35.4$\\times$ speedup with 17.66$\\times$ energy\nreduction on GPU.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 04:10:48 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Shi", "Feng", ""], ["Jin", "Ahren Yiqiao", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2105.01281", "submitter": "Chengliang Zhang Dr", "authors": "Chengliang Zhang, Junzhe Xia, Baichen Yang, Huancheng Puyang, Wei\n  Wang, Ruichuan Chen, Istemi Ekin Akkus, Paarijaat Aditya, Feng Yan", "title": "Citadel: Protecting Data Privacy and Model Confidentiality for\n  Collaborative Learning with SGX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of machine learning (ML) and its growing awareness, many\norganizations who own data but not ML expertise (data owner) would like to pool\ntheir data and collaborate with those who have expertise but need data from\ndiverse sources to train truly generalizable models (model owner). In such\ncollaborative ML, the data owner wants to protect the privacy of its training\ndata, while the model owner desires the confidentiality of the model and the\ntraining method which may contain intellectual properties. However, existing\nprivate ML solutions, such as federated learning and split learning, cannot\nmeet the privacy requirements of both data and model owners at the same time.\n  This paper presents Citadel, a scalable collaborative ML system that protects\nthe privacy of both data owner and model owner in untrusted infrastructures\nwith the help of Intel SGX. Citadel performs distributed training across\nmultiple training enclaves running on behalf of data owners and an aggregator\nenclave on behalf of the model owner. Citadel further establishes a strong\ninformation barrier between these enclaves by means of zero-sum masking and\nhierarchical aggregation to prevent data/model leakage during collaborative\ntraining. Compared with the existing SGX-protected training systems, Citadel\nenables better scalability and stronger privacy guarantees for collaborative\nML. Cloud deployment with various ML models shows that Citadel scales to a\nlarge number of enclaves with less than 1.73X slowdown caused by SGX.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 04:17:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhang", "Chengliang", ""], ["Xia", "Junzhe", ""], ["Yang", "Baichen", ""], ["Puyang", "Huancheng", ""], ["Wang", "Wei", ""], ["Chen", "Ruichuan", ""], ["Akkus", "Istemi Ekin", ""], ["Aditya", "Paarijaat", ""], ["Yan", "Feng", ""]]}, {"id": "2105.01282", "submitter": "Nima Safaei", "authors": "Amit Kumar Srivastava, Nima Safaei, Saeed Khaki, Gina Lopez, Wenzhi\n  Zeng, Frank Ewert, Thomas Gaiser, Jaber Rahimi", "title": "Comparison of Machine Learning Methods for Predicting Winter Wheat Yield\n  in Germany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study analyzed the performance of different machine learning methods for\nwinter wheat yield prediction using extensive datasets of weather, soil, and\ncrop phenology. To address the seasonality, weekly features were used that\nexplicitly take soil moisture conditions and meteorological events into\naccount. Our results indicated that nonlinear models such as deep neural\nnetworks (DNN) and XGboost are more effective in finding the functional\nrelationship between the crop yield and input data compared to linear models.\nThe results also revealed that the deep neural networks often had a higher\nprediction accuracy than XGboost. One of the main limitations of machine\nlearning models is their black box property. As a result, we moved beyond\nprediction and performed feature selection, as it provides key results towards\nexplaining yield prediction (variable importance by time). The feature\nselection method estimated the individual effect of weather components, soil\nconditions, and phenology variables as well as the time that these variables\nbecome important. As such, our study indicates which variables have the most\nsignificant effect on winter wheat yield.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 04:40:53 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Srivastava", "Amit Kumar", ""], ["Safaei", "Nima", ""], ["Khaki", "Saeed", ""], ["Lopez", "Gina", ""], ["Zeng", "Wenzhi", ""], ["Ewert", "Frank", ""], ["Gaiser", "Thomas", ""], ["Rahimi", "Jaber", ""]]}, {"id": "2105.01289", "submitter": "Aniket Anand Deshmukh", "authors": "Aniket Anand Deshmukh, Jayanth Reddy Regatti, Eren Manavoglu, and Urun\n  Dogan", "title": "Representation Learning for Clustering via Building Consensus", "comments": "arXiv admin note: text overlap with arXiv:2010.01245", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on deep clustering and unsupervised representation\nlearning for images. Recent advances in deep clustering and unsupervised\nrepresentation learning are based on the idea that different views of an input\nimage (generated through data augmentation techniques) must be closer in the\nrepresentation space (exemplar consistency), and/or similar images have a\nsimilar cluster assignment (population consistency). We define an additional\nnotion of consistency, consensus consistency, which ensures that\nrepresentations are learnt to induce similar partitions for variations in the\nrepresentation space, different clustering algorithms or different\ninitializations of a clustering algorithm. We define a clustering loss by\nperforming variations in the representation space and seamlessly integrate all\nthree consistencies (consensus, exemplar and population) into an end-to-end\nlearning framework. The proposed algorithm, Consensus Clustering using\nUnsupervised Representation Learning (ConCURL) improves the clustering\nperformance over state-of-the art methods on four out of five image datasets.\nFurther, we extend the evaluation procedure for clustering to reflect the\nchallenges in real world clustering tasks, such as clustering performance in\nthe case of distribution shift. We also perform a detailed ablation study for a\ndeeper understanding of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:04:03 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Deshmukh", "Aniket Anand", ""], ["Regatti", "Jayanth Reddy", ""], ["Manavoglu", "Eren", ""], ["Dogan", "Urun", ""]]}, {"id": "2105.01296", "submitter": "Anubhav Jangra", "authors": "Anubhav Jangra, Raghav Jain, Vaibhav Mavi, Sriparna Saha, Pushpak\n  Bhattacharyya", "title": "Semantic Extractor-Paraphraser based Abstractive Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The anthology of spoken languages today is inundated with textual\ninformation, necessitating the development of automatic summarization models.\nIn this manuscript, we propose an extractor-paraphraser based abstractive\nsummarization system that exploits semantic overlap as opposed to its\npredecessors that focus more on syntactic information overlap. Our model\noutperforms the state-of-the-art baselines in terms of ROUGE, METEOR and word\nmover similarity (WMS), establishing the superiority of the proposed system via\nextensive ablation experiments. We have also challenged the summarization\ncapabilities of the state of the art Pointer Generator Network (PGN), and\nthrough thorough experimentation, shown that PGN is more of a paraphraser,\ncontrary to the prevailing notion of a summarizer; illustrating it's\nincapability to accumulate information across multiple sentences.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:24:28 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Jangra", "Anubhav", ""], ["Jain", "Raghav", ""], ["Mavi", "Vaibhav", ""], ["Saha", "Sriparna", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2105.01303", "submitter": "Yue Guo", "authors": "Yue Guo, Felix Dietrich, Tom Bertalan, Danimir T. Doncevic, Manuel\n  Dahmen, Ioannis G. Kevrekidis, Qianxiao Li", "title": "Personalized Algorithm Generation: A Case Study in Meta-Learning ODE\n  Integrators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the meta-learning of numerical algorithms for scientific computing,\nwhich combines the mathematically driven, handcrafted design of general\nalgorithm structure with a data-driven adaptation to specific classes of tasks.\nThis represents a departure from the classical approaches in numerical\nanalysis, which typically do not feature such learning-based adaptations. As a\ncase study, we develop a machine learning approach that automatically learns\neffective solvers for initial value problems in the form of ordinary\ndifferential equations (ODEs), based on the Runge-Kutta (RK) integrator\narchitecture. By combining neural network approximations and meta-learning, we\nshow that we can obtain high-order integrators for targeted families of\ndifferential equations without the need for computing integrator coefficients\nby hand. Moreover, we demonstrate that in certain cases we can obtain superior\nperformance to classical RK methods. This can be attributed to certain\nproperties of the ODE families being identified and exploited by the approach.\nOverall, this work demonstrates an effective, learning-based approach to the\ndesign of algorithms for the numerical solution of differential equations, an\napproach that can be readily extended to other numerical tasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:42:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Guo", "Yue", ""], ["Dietrich", "Felix", ""], ["Bertalan", "Tom", ""], ["Doncevic", "Danimir T.", ""], ["Dahmen", "Manuel", ""], ["Kevrekidis", "Ioannis G.", ""], ["Li", "Qianxiao", ""]]}, {"id": "2105.01331", "submitter": "Hasan Kemik", "authors": "Hasan Kemik, Nusret \\\"Ozate\\c{s}, Meysam Asgari-Chenaghlu, Erik\n  Cambria", "title": "BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on\n  Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Protection of human rights is one of the most important problems of our\nworld. In this paper, our aim is to provide a dataset which covers one of the\nmost significant human rights contradiction in recent months affected the whole\nworld, George Floyd incident. We propose a labeled dataset for topic detection\nthat contains 17 million tweets. These Tweets are collected from 25 May 2020 to\n21 August 2020 that covers 89 days from start of this incident. We labeled the\ndataset by monitoring most trending news topics from global and local\nnewspapers. Apart from that, we present two baselines, TF-IDF and LDA. We\nevaluated the results of these two methods with three different k values for\nmetrics of precision, recall and f1-score. The collected dataset is available\nat https://github.com/MeysamAsgariC/BLMT.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:27:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kemik", "Hasan", ""], ["\u00d6zate\u015f", "Nusret", ""], ["Asgari-Chenaghlu", "Meysam", ""], ["Cambria", "Erik", ""]]}, {"id": "2105.01334", "submitter": "Meng Tang", "authors": "Meng Tang, Xin Ju, Louis J. Durlofsky", "title": "Deep-learning-based coupled flow-geomechanics surrogate model for CO$_2$\n  sequestration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep-learning-based surrogate model capable of predicting flow and\ngeomechanical responses in CO2 storage operations is presented and applied. The\n3D recurrent R-U-Net model combines deep convolutional and recurrent neural\nnetworks to capture the spatial distribution and temporal evolution of\nsaturation, pressure and surface displacement fields. The method is trained\nusing high-fidelity simulation results for 2000 storage-aquifer realizations\ncharacterized by multi-Gaussian porosity and log-permeability fields. These\nnumerical solutions are expensive because the domain that must be considered\nfor the coupled problem includes not only the storage aquifer but also a\nsurrounding region, overburden and bedrock. The surrogate model is trained to\npredict the 3D CO2 saturation and pressure fields in the storage aquifer, and\n2D displacement maps at the Earth's surface. Detailed comparisons between\nsurrogate model and full-order simulation results for new (test-case)\nstorage-aquifer realizations are presented. The saturation, pressure and\nsurface displacement fields provided by the surrogate model display a high\ndegree of accuracy, both for individual test-case realizations and for ensemble\nstatistics. Finally, the the recurrent R-U-Net surrogate model is applied with\na rejection sampling procedure for data assimilation. Although the observations\nconsist of only a small number of surface displacement measurements,\nsignificant uncertainty reduction in pressure buildup at the top of the storage\naquifer (caprock) is achieved.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:34:15 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tang", "Meng", ""], ["Ju", "Xin", ""], ["Durlofsky", "Louis J.", ""]]}, {"id": "2105.01341", "submitter": "Teodor Knapik", "authors": "Teodor Knapik (ISEA)", "title": "Signal automata and hidden Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generic method for inferring a dynamical hidden Markov model from a time\nseries is proposed. Under reasonable hypothesis, the model is updated in\nconstant time whenever a new measurement arrives.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:41:10 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Knapik", "Teodor", "", "ISEA"]]}, {"id": "2105.01346", "submitter": "Hachem Kadri", "authors": "Paolo Milanesi (QARMA), Hachem Kadri (LIS, QARMA, AMU SCI), St\\'ephane\n  Ayache (QARMA), Thierry Arti\\`eres (QARMA)", "title": "Implicit Regularization in Deep Tensor Factorization", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN), Jul\n  2021, Online, China", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempts of studying implicit regularization associated to gradient descent\n(GD) have identified matrix completion as a suitable test-bed. Late findings\nsuggest that this phenomenon cannot be phrased as a minimization-norm problem,\nimplying that a paradigm shift is required and that dynamics has to be taken\ninto account. In the present work we address the more general setup of tensor\ncompletion by leveraging two popularized tensor factorization, namely Tucker\nand TensorTrain (TT). We track relevant quantities such as tensor nuclear norm,\neffective rank, generalized singular values and we introduce deep Tucker and TT\nunconstrained factorization to deal with the completion task. Experiments on\nboth synthetic and real data show that gradient descent promotes solution with\nlow-rank, and validate the conjecture saying that the phenomenon has to be\naddressed from a dynamical perspective.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:48:40 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Milanesi", "Paolo", "", "QARMA"], ["Kadri", "Hachem", "", "LIS, QARMA, AMU SCI"], ["Ayache", "St\u00e9phane", "", "QARMA"], ["Arti\u00e8res", "Thierry", "", "QARMA"]]}, {"id": "2105.01365", "submitter": "Alberto Perotti", "authors": "Anahid Robert Safavi, Alberto G. Perotti, Branislav M. Popovic, Mahdi\n  Boloursaz Mashhadi, Deniz Gunduz", "title": "Deep Extended Feedback Codes", "comments": "7 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new deep-neural-network (DNN) based error correction encoder architecture\nfor channels with feedback, called Deep Extended Feedback (DEF), is presented\nin this paper. The encoder in the DEF architecture transmits an information\nmessage followed by a sequence of parity symbols which are generated based on\nthe message as well as the observations of the past forward channel outputs\nsent to the transmitter through a feedback channel. DEF codes generalize\nDeepcode [1] in several ways: parity symbols are generated based on\nforward-channel output observations over longer time intervals in order to\nprovide better error correction capability; and high-order modulation formats\nare deployed in the encoder so as to achieve increased spectral efficiency.\nPerformance evaluations show that DEF codes have better performance compared to\nother DNN-based codes for channels with feedback.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 08:41:14 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Safavi", "Anahid Robert", ""], ["Perotti", "Alberto G.", ""], ["Popovic", "Branislav M.", ""], ["Mashhadi", "Mahdi Boloursaz", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2105.01386", "submitter": "Thrupthi Ann John", "authors": "Thrupthi Ann John, Vineeth N Balasubramanian, C V Jawahar", "title": "Canonical Saliency Maps: Decoding Deep Face Models", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Deep Neural Network models for face processing tasks approach human-like\nperformance, their deployment in critical applications such as law enforcement\nand access control has seen an upswing, where any failure may have far-reaching\nconsequences. We need methods to build trust in deployed systems by making\ntheir working as transparent as possible. Existing visualization algorithms are\ndesigned for object recognition and do not give insightful results when applied\nto the face domain. In this work, we present 'Canonical Saliency Maps', a new\nmethod that highlights relevant facial areas by projecting saliency maps onto a\ncanonical face model. We present two kinds of Canonical Saliency Maps:\nimage-level maps and model-level maps. Image-level maps highlight facial\nfeatures responsible for the decision made by a deep face model on a given\nimage, thus helping to understand how a DNN made a prediction on the image.\nModel-level maps provide an understanding of what the entire DNN model focuses\non in each task and thus can be used to detect biases in the model. Our\nqualitative and quantitative results show the usefulness of the proposed\ncanonical saliency maps, which can be used on any deep face model regardless of\nthe architecture.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 09:42:56 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["John", "Thrupthi Ann", ""], ["Balasubramanian", "Vineeth N", ""], ["Jawahar", "C V", ""]]}, {"id": "2105.01390", "submitter": "Siddharth Barman", "authors": "Siddharth Barman, Ramakrishnan Krishnamurthy, Saladi Rahul", "title": "Optimal Algorithms for Range Searching over Multi-Armed Bandits", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a multi-armed bandit (MAB) version of the range-searching\nproblem. In its basic form, range searching considers as input a set of points\n(on the real line) and a collection of (real) intervals. Here, with each\nspecified point, we have an associated weight, and the problem objective is to\nfind a maximum-weight point within every given interval.\n  The current work addresses range searching with stochastic weights: each\npoint corresponds to an arm (that admits sample access) and the point's weight\nis the (unknown) mean of the underlying distribution. In this MAB setup, we\ndevelop sample-efficient algorithms that find, with high probability,\nnear-optimal arms within the given intervals, i.e., we obtain PAC (probably\napproximately correct) guarantees. We also provide an algorithm for a\ngeneralization wherein the weight of each point is a multi-dimensional vector.\nThe sample complexities of our algorithms depend, in particular, on the size of\nthe optimal hitting set of the given intervals.\n  Finally, we establish lower bounds proving that the obtained sample\ncomplexities are essentially tight. Our results highlight the significance of\ngeometric constructs -- specifically, hitting sets -- in our MAB setting.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 09:52:16 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Barman", "Siddharth", ""], ["Krishnamurthy", "Ramakrishnan", ""], ["Rahul", "Saladi", ""]]}, {"id": "2105.01402", "submitter": "Roderick Karlemstrand", "authors": "Roderick Karlemstrand, Ebba Leckstr\\\"om", "title": "Using Twitter Attribute Information to Predict Stock Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict stock prices might be the unspoken wish of stock\ninvestors. Although stock prices are complicated to predict, there are many\ntheories about what affects their movements, including interest rates, news and\nsocial media. With the help of Machine Learning, complex patterns in data can\nbe identified beyond the human intellect. In this thesis, a Machine Learning\nmodel for time series forecasting is created and tested to predict stock\nprices. The model is based on a neural network with several layers of LSTM and\nfully connected layers. It is trained with historical stock values, technical\nindicators and Twitter attribute information retrieved, extracted and\ncalculated from posts on the social media platform Twitter. These attributes\nare sentiment score, favourites, followers, retweets and if an account is\nverified. To collect data from Twitter, Twitter's API is used. Sentiment\nanalysis is conducted with VADER. The results show that by adding more Twitter\nattributes, the MSE between the predicted prices and the actual prices improved\nby 3%. With technical analysis taken into account, MSE decreases from 0.1617 to\n0.1437, which is an improvement of around 11%. The restrictions of this study\ninclude that the selected stock has to be publicly listed on the stock market\nand popular on Twitter and among individual investors. Besides, the stock\nmarkets' opening hours differ from Twitter, which constantly available. It may\ntherefore introduce noises in the model.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 10:27:37 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Karlemstrand", "Roderick", ""], ["Leckstr\u00f6m", "Ebba", ""]]}, {"id": "2105.01404", "submitter": "Diogo Seca", "authors": "Diogo Seca", "title": "TimeGym: Debugging for Time Series Modeling in Python", "comments": "Paper and python package ongoing active development", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the TimeGym Forecasting Debugging Toolkit, a Python library for\ntesting and debugging time series forecasting pipelines. TimeGym simplifies the\ntesting forecasting pipeline by providing generic tests for forecasting\npipelines fresh out of the box. These tests are based on common modeling\nchallenges of time series. Our library enables forecasters to apply a\nTest-Driven Development approach to forecast modeling, using specified oracles\nto generate artificial data with noise.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 10:36:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Seca", "Diogo", ""]]}, {"id": "2105.01407", "submitter": "Diogo Seca", "authors": "Diogo Seca", "title": "A Review on Oracle Issues in Machine Learning", "comments": "Paper ongoing active research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning contrasts with traditional software development in that the\noracle is the data, and the data is not always a correct representation of the\nproblem that machine learning tries to model. We present a survey of the oracle\nissues found in machine learning and state-of-the-art solutions for dealing\nwith these issues. These include lines of research for differential testing,\nmetamorphic testing, and test coverage. We also review some recent improvements\nto robustness during modeling that reduce the impact of oracle issues, as well\nas tools and frameworks for assisting in testing and discovering issues\nspecific to the dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 10:41:34 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Seca", "Diogo", ""]]}, {"id": "2105.01420", "submitter": "Burak Bartan", "authors": "Burak Bartan, Mert Pilanci", "title": "Training Quantized Neural Networks to Global Optimality via Semidefinite\n  Programming", "comments": "v2: Minor edits in the text. The results are unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NNs) have been extremely successful across many tasks in\nmachine learning. Quantization of NN weights has become an important topic due\nto its impact on their energy efficiency, inference time and deployment on\nhardware. Although post-training quantization is well-studied, training optimal\nquantized NNs involves combinatorial non-convex optimization problems which\nappear intractable. In this work, we introduce a convex optimization strategy\nto train quantized NNs with polynomial activations. Our method leverages hidden\nconvexity in two-layer neural networks from the recent literature, semidefinite\nlifting, and Grothendieck's identity. Surprisingly, we show that certain\nquantized NN problems can be solved to global optimality in polynomial-time in\nall relevant parameters via semidefinite relaxations. We present numerical\nexamples to illustrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 11:11:33 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 09:29:13 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bartan", "Burak", ""], ["Pilanci", "Mert", ""]]}, {"id": "2105.01423", "submitter": "Bilal Thonnam Thodi", "authors": "Bilal Thonnam Thodi, Zaid Saeed Khan, Saif Eddin Jabari and Monica\n  Menendez", "title": "Learning Traffic Speed Dynamics from Visualizations", "comments": "8 pages, 9 figures; Submitted to the 2021 IEEE International\n  Intelligent Transportation Systems Conference (ITSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space-time visualizations of macroscopic or microscopic traffic variables is\na qualitative tool used by traffic engineers to understand and analyze\ndifferent aspects of road traffic dynamics. We present a deep learning method\nto learn the macroscopic traffic speed dynamics from these space-time\nvisualizations, and demonstrate its application in the framework of traffic\nstate estimation. Compared to existing estimation approaches, our approach\nallows a finer estimation resolution, eliminates the dependence on the initial\nconditions, and is agnostic to external factors such as traffic demand, road\ninhomogeneities and driving behaviors. Our model respects causality in traffic\ndynamics, which improves the robustness of estimation. We present the\nhigh-resolution traffic speed fields estimated for several freeway sections\nusing the data obtained from the Next Generation Simulation Program (NGSIM) and\nGerman Highway (HighD) datasets. We further demonstrate the quality and utility\nof the estimation by inferring vehicle trajectories from the estimated speed\nfields, and discuss the benefits of deep neural network models in approximating\nthe traffic dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 11:17:43 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Thodi", "Bilal Thonnam", ""], ["Khan", "Zaid Saeed", ""], ["Jabari", "Saif Eddin", ""], ["Menendez", "Monica", ""]]}, {"id": "2105.01424", "submitter": "Sajad Khodadadian", "authors": "Sajad Khodadadian, Prakirt Raj Jhunjhunwala, Sushil Mahavir Varma,\n  Siva Theja Maguluri", "title": "On the Linear convergence of Natural Policy Gradient Algorithm", "comments": "19 pages, 1 figure, A version of this paper was first submitted to a\n  conference in Mar 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov Decision Processes are classically solved using Value Iteration and\nPolicy Iteration algorithms. Recent interest in Reinforcement Learning has\nmotivated the study of methods inspired by optimization, such as gradient\nascent. Among these, a popular algorithm is the Natural Policy Gradient, which\nis a mirror descent variant for MDPs. This algorithm forms the basis of several\npopular Reinforcement Learning algorithms such as Natural actor-critic, TRPO,\nPPO, etc, and so is being studied with growing interest. It has been shown that\nNatural Policy Gradient with constant step size converges with a sublinear rate\nof O(1/k) to the global optimal. In this paper, we present improved finite time\nconvergence bounds, and show that this algorithm has geometric (also known as\nlinear) asymptotic convergence rate. We further improve this convergence result\nby introducing a variant of Natural Policy Gradient with adaptive step sizes.\nFinally, we compare different variants of policy gradient methods\nexperimentally.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 11:26:12 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Khodadadian", "Sajad", ""], ["Jhunjhunwala", "Prakirt Raj", ""], ["Varma", "Sushil Mahavir", ""], ["Maguluri", "Siva Theja", ""]]}, {"id": "2105.01429", "submitter": "Yingjun Shen", "authors": "Yingjun Shen, Zhe Song and Andrew Kusiak", "title": "Enhancing Generalizability of Predictive Models with Synergy of Data and\n  Physics", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wind farm needs prediction models for predictive maintenance. There is a need\nto predict values of non-observable parameters beyond ranges reflected in\navailable data. A prediction model developed for one machine many not perform\nwell in another similar machine. This is usually due to lack of\ngeneralizability of data-driven models. To increase generalizability of\npredictive models, this research integrates the data mining with\nfirst-principle knowledge. Physics-based principles are combined with machine\nlearning algorithms through feature engineering, strong rules and\ndivide-and-conquer. The proposed synergy concept is illustrated with the wind\nturbine blade icing prediction and achieves significant prediction accuracy\nacross different turbines. The proposed process is widely accepted by wind\nenergy predictive maintenance practitioners because of its simplicity and\nefficiency. Furthermore, this paper demonstrates the importance of embedding\nphysical principles within the machine learning process, and also highlight an\nimportant point that the need for more complex machine learning algorithms in\nindustrial big data mining is often much less than it is in other applications,\nmaking it essential to incorporate physics and follow Less is More philosophy.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 11:34:44 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Shen", "Yingjun", ""], ["Song", "Zhe", ""], ["Kusiak", "Andrew", ""]]}, {"id": "2105.01441", "submitter": "Christoph Kern", "authors": "Matthias Kuppler, Christoph Kern, Ruben L. Bach, Frauke Kreuter", "title": "Distributive Justice and Fairness Metrics in Automated Decision-making:\n  How Much Overlap Is There?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of powerful prediction algorithms led to increased automation of\nhigh-stake decisions regarding the allocation of scarce resources such as\ngovernment spending and welfare support. This automation bears the risk of\nperpetuating unwanted discrimination against vulnerable and historically\ndisadvantaged groups. Research on algorithmic discrimination in computer\nscience and other disciplines developed a plethora of fairness metrics to\ndetect and correct discriminatory algorithms. Drawing on robust sociological\nand philosophical discourse on distributive justice, we identify the\nlimitations and problematic implications of prominent fairness metrics. We show\nthat metrics implementing equality of opportunity only apply when resource\nallocations are based on deservingness, but fail when allocations should\nreflect concerns about egalitarianism, sufficiency, and priority. We argue that\nby cleanly distinguishing between prediction tasks and decision tasks, research\non fair machine learning could take better advantage of the rich literature on\ndistributive justice.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:09:26 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 10:26:34 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Kuppler", "Matthias", ""], ["Kern", "Christoph", ""], ["Bach", "Ruben L.", ""], ["Kreuter", "Frauke", ""]]}, {"id": "2105.01442", "submitter": "Victor Guimar\\~aes", "authors": "Victor Guimar\\~aes and V\\'itor Santos Costa", "title": "NeuralLog: a Neural Logic Language", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Application domains that require considering relationships among objects\nwhich have real-valued attributes are becoming even more important. In this\npaper we propose NeuralLog, a first-order logic language that is compiled to a\nneural network. The main goal of NeuralLog is to bridge logic programming and\ndeep learning, allowing advances in both fields to be combined in order to\nobtain better machine learning models. The main advantages of NeuralLog are: to\nallow neural networks to be defined as logic programs; and to be able to handle\nnumeric attributes and functions. We compared NeuralLog with two distinct\nsystems that use first-order logic to build neural networks. We have also shown\nthat NeuralLog can learn link prediction and classification tasks, using the\nsame theory as the compared systems, achieving better results for the area\nunder the ROC curve in four datasets: Cora and UWCSE for link prediction; and\nYelp and PAKDD15 for classification; and comparable results for link prediction\nin the WordNet dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:09:35 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Guimar\u00e3es", "Victor", ""], ["Costa", "V\u00edtor Santos", ""]]}, {"id": "2105.01443", "submitter": "Daniel Grahn", "authors": "Daniel Grahn", "title": "mil-benchmarks: Standardized Evaluation of Deep Multiple-Instance\n  Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-instance learning is a subset of weakly supervised learning where\nlabels are applied to sets of instances rather than the instances themselves.\nUnder the standard assumption, a set is positive only there is if at least one\ninstance in the set which is positive.\n  This paper introduces a series of multiple-instance learning benchmarks\ngenerated from MNIST, Fashion-MNIST, and CIFAR10. These benchmarks test the\nstandard, presence, absence, and complex assumptions and provide a framework\nfor future benchmarks to be distributed. I implement and evaluate several\nmultiple-instance learning techniques against the benchmarks. Further, I\nevaluate the Noisy-And method with label noise and find mixed results with\ndifferent datasets. The models are implemented in TensorFlow 2.4.1 and are\navailable on GitHub. The benchmarks are available from PyPi as mil-benchmarks\nand on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:09:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Grahn", "Daniel", ""]]}, {"id": "2105.01445", "submitter": "Xuetong Wu", "authors": "Xuetong Wu, Jonathan H. Manton, Uwe Aickelin, Jingge Zhu", "title": "Online Transfer Learning: Negative Transfer and Effect of Prior\n  Knowledge", "comments": "Paper accepted to ISIT2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a machine learning paradigm where the knowledge from one\ntask is utilized to resolve the problem in a related task. On the one hand, it\nis conceivable that knowledge from one task could be useful for solving a\nrelated problem. On the other hand, it is also recognized that if not executed\nproperly, transfer learning algorithms could in fact impair the learning\nperformance instead of improving it - commonly known as \"negative transfer\". In\nthis paper, we study the online transfer learning problems where the source\nsamples are given in an offline way while the target samples arrive\nsequentially. We define the expected regret of the online transfer learning\nproblem and provide upper bounds on the regret using information-theoretic\nquantities. We also obtain exact expressions for the bounds when the sample\nsize becomes large. Examples show that the derived bounds are accurate even for\nsmall sample sizes. Furthermore, the obtained bounds give valuable insight on\nthe effect of prior knowledge for transfer learning in our formulation. In\nparticular, we formally characterize the conditions under which negative\ntransfer occurs.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:12:14 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Wu", "Xuetong", ""], ["Manton", "Jonathan H.", ""], ["Aickelin", "Uwe", ""], ["Zhu", "Jingge", ""]]}, {"id": "2105.01463", "submitter": "Berkan Kadioglu", "authors": "Berkan Kadioglu, Peng Tian, Jennifer Dy, Deniz Erdogmus and Stratis\n  Ioannidis", "title": "On the Sample Complexity of Rank Regression from Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a rank regression setting, in which a dataset of $N$ samples with\nfeatures in $\\mathbb{R}^d$ is ranked by an oracle via $M$ pairwise comparisons.\nSpecifically, there exists a latent total ordering of the samples; when\npresented with a pair of samples, a noisy oracle identifies the one ranked\nhigher with respect to the underlying total ordering. A learner observes a\ndataset of such comparisons and wishes to regress sample ranks from their\nfeatures. We show that to learn the model parameters with $\\epsilon > 0$\naccuracy, it suffices to conduct $M \\in \\Omega(dN\\log^3 N/\\epsilon^2)$\ncomparisons uniformly at random when $N$ is $\\Omega(d/\\epsilon^2)$.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:45:53 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kadioglu", "Berkan", ""], ["Tian", "Peng", ""], ["Dy", "Jennifer", ""], ["Erdogmus", "Deniz", ""], ["Ioannidis", "Stratis", ""]]}, {"id": "2105.01468", "submitter": "Muhammad Zain Ali", "authors": "Muhammad Zain Ali, Kashif Javed, Ehsan ul Haq, Anoshka Tariq", "title": "Sentiment and Emotion Classification of Epidemic Related Bilingual data\n  from Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, sentiment analysis and emotion classification are two of the\nmost abundantly used techniques in the field of Natural Language Processing\n(NLP). Although sentiment analysis and emotion classification are used commonly\nin applications such as analyzing customer reviews, the popularity of\ncandidates contesting in elections, and comments about various sporting events;\nhowever, in this study, we have examined their application for epidemic\noutbreak detection. Early outbreak detection is the key to deal with epidemics\neffectively, however, the traditional ways of outbreak detection are\ntime-consuming which inhibits prompt response from the respective departments.\nSocial media platforms such as Twitter, Facebook, Instagram, etc. allow the\nusers to express their thoughts related to different aspects of life, and\ntherefore, serve as a substantial source of information in such situations. The\nproposed study exploits the bilingual (Urdu and English) data from Twitter and\nNEWS websites related to the dengue epidemic in Pakistan, and sentiment\nanalysis and emotion classification are performed to acquire deep insights from\nthe data set for gaining a fair idea related to an epidemic outbreak. Machine\nlearning and deep learning algorithms have been used to train and implement the\nmodels for the execution of both tasks. The comparative performance of each\nmodel has been evaluated using accuracy, precision, recall, and f1-measure.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:51:18 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ali", "Muhammad Zain", ""], ["Javed", "Kashif", ""], ["Haq", "Ehsan ul", ""], ["Tariq", "Anoshka", ""]]}, {"id": "2105.01478", "submitter": "Jithin Jagannath", "authors": "Keyvan Ramezanpour and Jithin Jagannath", "title": "Intelligent Zero Trust Architecture for 5G/6G Tactical Networks:\n  Principles, Challenges, and the Role of Machine Learning", "comments": "Submitted for possible publication in IEEE Journal. For\n  non-commercial use only. Please contact Dr. Jithin Jagannath for any other\n  use case", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this position paper, we discuss the critical need for integrating zero\ntrust (ZT) principles into next-generation communication networks (5G/6G) for\nboth tactical and commercial applications. We highlight the challenges and\nintroduce the concept of an intelligent zero trust architecture (i-ZTA) as a\nsecurity framework in 5G/6G networks with untrusted components. While network\nvirtualization, software-defined networking (SDN), and service-based\narchitectures (SBA) are key enablers of 5G networks, operating in an untrusted\nenvironment has also become a key feature of the networks. Further, seamless\nconnectivity to a high volume of devices in multi-radio access technology (RAT)\nhas broadened the attack surface on information infrastructure. Network\nassurance in a dynamic untrusted environment calls for revolutionary\narchitectures beyond existing static security frameworks. This paper presents\nthe architectural design of an i-ZTA upon which modern artificial intelligence\n(AI) algorithms can be developed to provide information security in untrusted\nnetworks. We introduce key ZT principles as real-time Monitoring of the\nsecurity state of network assets, Evaluating the risk of individual access\nrequests, and Deciding on access authorization using a dynamic trust algorithm,\ncalled MED components. The envisioned architecture adopts an SBA-based design,\nsimilar to the 3GPP specification of 5G networks, by leveraging the open radio\naccess network (O-RAN) architecture with appropriate real-time engines and\nnetwork interfaces for collecting necessary machine learning data. The i-ZTA is\nalso expected to exploit the multi-access edge computing (MEC) technology of 5G\nas a key enabler of intelligent MED components for resource-constraint devices.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 13:14:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ramezanpour", "Keyvan", ""], ["Jagannath", "Jithin", ""]]}, {"id": "2105.01480", "submitter": "Alberto Archetti", "authors": "Alberto Archetti, Marco Cannici, Matteo Matteucci", "title": "Neural Weighted A*: Learning Graph Costs and Heuristics with\n  Differentiable Anytime A*", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the trend of incorporating differentiable algorithms into deep\nlearning architectures arose in machine learning research, as the fusion of\nneural layers and algorithmic layers has been beneficial for handling\ncombinatorial data, such as shortest paths on graphs. Recent works related to\ndata-driven planning aim at learning either cost functions or heuristic\nfunctions, but not both. We propose Neural Weighted A*, a differentiable\nanytime planner able to produce improved representations of planar maps as\ngraph costs and heuristics. Training occurs end-to-end on raw images with\ndirect supervision on planning examples, thanks to a differentiable A* solver\nintegrated into the architecture. More importantly, the user can trade off\nplanning accuracy for efficiency at run-time, using a single, real-valued\nparameter. The solution suboptimality is constrained within a linear bound\nequal to the optimal path cost multiplied by the tradeoff parameter. We\nexperimentally show the validity of our claims by testing Neural Weighted A*\nagainst several baselines, introducing a novel, tile-based navigation dataset.\nWe outperform similar architectures in planning accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 13:17:30 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Archetti", "Alberto", ""], ["Cannici", "Marco", ""], ["Matteucci", "Matteo", ""]]}, {"id": "2105.01510", "submitter": "Saumik Bhattacharya", "authors": "Rangan Das, Bikram Boote, Saumik Bhattacharya, Ujjwal Maulik", "title": "Multipath Graph Convolutional Neural Networks", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph convolution networks have recently garnered a lot of attention for\nrepresentation learning on non-Euclidean feature spaces. Recent research has\nfocused on stacking multiple layers like in convolutional neural networks for\nthe increased expressive power of graph convolution networks. However, simply\nstacking multiple graph convolution layers lead to issues like vanishing\ngradient, over-fitting and over-smoothing. Such problems are much less when\nusing shallower networks, even though the shallow networks have lower\nexpressive power. In this work, we propose a novel Multipath Graph\nconvolutional neural network that aggregates the output of multiple different\nshallow networks. We train and test our model on various benchmarks datasets\nfor the task of node property prediction. Results show that the proposed method\nnot only attains increased test accuracy but also requires fewer training\nepochs to converge. The full implementation is available at\nhttps://github.com/rangan2510/MultiPathGCN\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:11:20 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Das", "Rangan", ""], ["Boote", "Bikram", ""], ["Bhattacharya", "Saumik", ""], ["Maulik", "Ujjwal", ""]]}, {"id": "2105.01517", "submitter": "Yanbei Chen", "authors": "Yanbei Chen, Thomas Hummel, A. Sophia Koepke, Zeynep Akata", "title": "Where and When: Space-Time Attention for Audio-Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explaining the decision of a multi-modal decision-maker requires to determine\nthe evidence from both modalities. Recent advances in XAI provide explanations\nfor models trained on still images. However, when it comes to modeling multiple\nsensory modalities in a dynamic world, it remains underexplored how to\ndemystify the mysterious dynamics of a complex multi-modal model. In this work,\nwe take a crucial step forward and explore learnable explanations for\naudio-visual recognition. Specifically, we propose a novel space-time attention\nnetwork that uncovers the synergistic dynamics of audio and visual data over\nboth space and time. Our model is capable of predicting the audio-visual video\nevents, while justifying its decision by localizing where the relevant visual\ncues appear, and when the predicted sounds occur in videos. We benchmark our\nmodel on three audio-visual video event datasets, comparing extensively to\nmultiple recent multi-modal representation learners and intrinsic explanation\nmodels. Experimental results demonstrate the clear superior performance of our\nmodel over the existing methods on audio-visual video event recognition.\nMoreover, we conduct an in-depth study to analyze the explainability of our\nmodel based on robustness analysis via perturbation tests and pointing games\nusing human annotations.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:16:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Chen", "Yanbei", ""], ["Hummel", "Thomas", ""], ["Koepke", "A. Sophia", ""], ["Akata", "Zeynep", ""]]}, {"id": "2105.01531", "submitter": "Javier Nistal", "authors": "Javier Nistal, Cyran Aouameur, Stefan Lattner, and Ga\\\"el Richard", "title": "VQCPC-GAN: Variable-length Adversarial Audio Synthesis using\n  Vector-Quantized Contrastive Predictive Coding", "comments": "5 pages, 1 figure, 1 table; under review for WASPAA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Influenced by the field of Computer Vision, Generative Adversarial Networks\n(GANs) are often adopted for the audio domain using fixed-size two-dimensional\nspectrogram representations as the \"image data\". However, in the (musical)\naudio domain, it is often desired to generate output of variable duration. This\npaper presents VQCPC-GAN, an adversarial framework for synthesizing\nvariable-length audio by exploiting Vector-Quantized Contrastive Predictive\nCoding (VQCPC). A sequence of VQCPC tokens extracted from real audio data\nserves as conditional input to a GAN architecture, providing step-wise\ntime-dependent features of the generated content. The input noise z\n(characteristic in adversarial architectures) remains fixed over time, ensuring\ntemporal consistency of global features. We evaluate the proposed model by\ncomparing a diverse set of metrics against various strong baselines. Results\nshow that, even though the baselines score best, VQCPC-GAN achieves comparable\nperformance even when generating variable-length audio. Numerous sound examples\nare provided in the accompanying website, and we release the code for\nreproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:35:51 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Nistal", "Javier", ""], ["Aouameur", "Cyran", ""], ["Lattner", "Stefan", ""], ["Richard", "Ga\u00ebl", ""]]}, {"id": "2105.01541", "submitter": "Mingtian Gao", "authors": "Yichi Lu, Mingtian Gao, Ryosuke Saga", "title": "Apparel Recommender System based on Bilateral image shape features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic matrix factorization (PMF) is a well-known model of recommender\nsystems. With the development of image recognition technology, some PMF\nrecommender systems that combine images have emerged. Some of these systems use\nthe image shape features of the recommended products to achieve better results\ncompared to those of the traditional PMF. However, in the existing methods, no\nPMF recommender system can combine the image features of products previously\npurchased by customers and of recommended products. Thus, this study proposes a\nnovel probabilistic model that integrates double convolutional neural networks\n(CNNs) into PMF. For apparel goods, two trained CNNs from the image shape\nfeatures of users and items are combined, and the latent variables of users and\nitems are optimized based on the vectorized features of CNNs and ratings.\nExtensive experiments show that our model predicts outcome more accurately than\ndo other recommender models.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:48:38 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Lu", "Yichi", ""], ["Gao", "Mingtian", ""], ["Saga", "Ryosuke", ""]]}, {"id": "2105.01545", "submitter": "Bin Liu", "authors": "Bin Liu and Grigorios Tsoumakas", "title": "Optimizing Area Under the Curve Measures via Matrix Factorization for\n  Drug-Target Interaction Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In drug discovery, identifying drug-target interactions (DTIs) via\nexperimental approaches is a tedious and expensive procedure. Computational\nmethods efficiently predict DTIs and recommend a small part of potential\ninteracting pairs for further experimental confirmation, accelerating the drug\ndiscovery process. Area under the precision-recall curve (AUPR) that emphasizes\nthe accuracy of top-ranked pairs and area under the receiver operating\ncharacteristic curve (AUC) that heavily punishes the existence of low ranked\ninteracting pairs are two widely used evaluation metrics in the DTI prediction\ntask. However, the two metrics are seldom considered as losses within existing\nDTI prediction methods. This paper proposes two matrix factorization methods\nthat optimize AUPR and AUC, respectively. The two methods utilize graph\nregularization to ensure the local invariance of training drugs and targets in\nthe latent feature space, and leverage the optimal decay coefficient to infer\nmore reliable latent features of new drugs and targets. Experimental results\nover four updated benchmark datasets containing more recently verified\ninteractions show the superiority of the proposed methods in terms of the\ncorresponding evaluation metric they optimize.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 14:48:32 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liu", "Bin", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2105.01550", "submitter": "Yutao Zhong", "authors": "Pranjal Awasthi, Anqi Mao, Mehryar Mohri, Yutao Zhong", "title": "A Finer Calibration Analysis for Adversarial Robustness", "comments": "arXiv admin note: text overlap with arXiv:2104.09658", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a more general analysis of $H$-calibration for adversarially\nrobust classification. By adopting a finer definition of calibration, we can\ncover settings beyond the restricted hypothesis sets studied in previous work.\nIn particular, our results hold for most common hypothesis sets used in machine\nlearning. We both fix some previous calibration results (Bao et al., 2020) and\ngeneralize others (Awasthi et al., 2021). Moreover, our calibration results,\ncombined with the previous study of consistency by Awasthi et al. (2021), also\nlead to more general $H$-consistency results covering common hypothesis sets.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:59:39 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 15:59:29 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Mao", "Anqi", ""], ["Mohri", "Mehryar", ""], ["Zhong", "Yutao", ""]]}, {"id": "2105.01560", "submitter": "Guy Barash", "authors": "Guy Barash, Eitan Farchi, Sarit Kraus, Onn Shehory", "title": "Broadly Applicable Targeted Data Sample Omission Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel clean-label targeted poisoning attack on learning\nmechanisms. While classical poisoning attacks typically corrupt data via\naddition, modification and omission, our attack focuses on data omission only.\nOur attack misclassifies a single, targeted test sample of choice, without\nmanipulating that sample. We demonstrate the effectiveness of omission attacks\nagainst a large variety of learners including deep neural networks, SVM and\ndecision trees, using several datasets including MNIST, IMDB and CIFAR. The\nfocus of our attack on data omission only is beneficial as well, as it is\nsimpler to implement and analyze. We show that, with a low attack budget, our\nattack's success rate is above 80%, and in some cases 100%, for white-box\nlearning. It is systematically above the reference benchmark for black-box\nlearning. For both white-box and black-box cases, changes in model accuracy are\nnegligible, regardless of the specific learner and dataset. We also prove\ntheoretically in a simplified agnostic PAC learning framework that, subject to\ndataset size and distribution, our omission attack succeeds with high\nprobability against any successful simplified agnostic PAC learner.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:20:54 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 14:27:05 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Barash", "Guy", ""], ["Farchi", "Eitan", ""], ["Kraus", "Sarit", ""], ["Shehory", "Onn", ""]]}, {"id": "2105.01564", "submitter": "Haggai Roitman", "authors": "Yotam Eshel, Or Levi, Haggai Roitman, Alexander Nus", "title": "PreSizE: Predicting Size in E-Commerce using Transformers", "comments": "Accepted for publication in SIGIR'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in the e-commerce fashion industry have led to an exploration\nof novel ways to enhance buyer experience via improved personalization.\nPredicting a proper size for an item to recommend is an important\npersonalization challenge, and is being studied in this work. Earlier works in\nthis field either focused on modeling explicit buyer fitment feedback or\nmodeling of only a single aspect of the problem (e.g., specific category,\nbrand, etc.). More recent works proposed richer models, either content-based or\nsequence-based, better accounting for content-based aspects of the problem or\nbetter modeling the buyer's online journey. However, both these approaches fail\nin certain scenarios: either when encountering unseen items (sequence-based\nmodels) or when encountering new users (content-based models).\n  To address the aforementioned gaps, we propose PreSizE - a novel deep\nlearning framework which utilizes Transformers for accurate size prediction.\nPreSizE models the effect of both content-based attributes, such as brand and\ncategory, and the buyer's purchase history on her size preferences. Using an\nextensive set of experiments on a large-scale e-commerce dataset, we\ndemonstrate that PreSizE is capable of achieving superior prediction\nperformance compared to previous state-of-the-art baselines. By encoding item\nattributes, PreSizE better handles cold-start cases with unseen items, and\ncases where buyers have little past purchase data. As a proof of concept, we\ndemonstrate that size predictions made by PreSizE can be effectively integrated\ninto an existing production recommender system yielding very effective features\nand significantly improving recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:23:59 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Eshel", "Yotam", ""], ["Levi", "Or", ""], ["Roitman", "Haggai", ""], ["Nus", "Alexander", ""]]}, {"id": "2105.01567", "submitter": "Amy Parkes Miss", "authors": "A. I. Parkes, A. J. Sobey and D. A. Hudson", "title": "Towards Error Measures which Influence a Learners Inductive Bias to the\n  Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence is applied in a range of sectors, and is relied upon\nfor decisions requiring a high level of trust. For regression methods, trust is\nincreased if they approximate the true input-output relationships and perform\naccurately outside the bounds of the training data. But often performance\noff-test-set is poor, especially when data is sparse. This is because the\nconditional average, which in many scenarios is a good approximation of the\n`ground truth', is only modelled with conventional Minkowski-r error measures\nwhen the data set adheres to restrictive assumptions, with many real data sets\nviolating these. To combat this there are several methods that use prior\nknowledge to approximate the `ground truth'. However, prior knowledge is not\nalways available, and this paper investigates how error measures affect the\nability for a regression method to model the `ground truth' in these scenarios.\nCurrent error measures are shown to create an unhelpful bias and a new error\nmeasure is derived which does not exhibit this behaviour. This is tested on 36\nrepresentative data sets with different characteristics, showing that it is\nmore consistent in determining the `ground truth' and in giving improved\npredictions in regions beyond the range of the training data.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:33:58 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Parkes", "A. I.", ""], ["Sobey", "A. J.", ""], ["Hudson", "D. A.", ""]]}, {"id": "2105.01571", "submitter": "Xiao Zhou", "authors": "Xiao Zhou, Weizhong Zhang, Hang Xu, Tong Zhang", "title": "Effective Sparsification of Neural Networks with Global Sparsity\n  Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weight pruning is an effective technique to reduce the model size and\ninference time for deep neural networks in real-world deployments. However,\nsince magnitudes and relative importance of weights are very different for\ndifferent layers of a neural network, existing methods rely on either manual\ntuning or handcrafted heuristic rules to find appropriate pruning rates\nindividually for each layer. This approach generally leads to suboptimal\nperformance. In this paper, by directly working on the probability space, we\npropose an effective network sparsification method called {\\it probabilistic\nmasking} (ProbMask), which solves a natural sparsification formulation under\nglobal sparsity constraint. The key idea is to use probability as a global\ncriterion for all layers to measure the weight importance. An appealing feature\nof ProbMask is that the amounts of weight redundancy can be learned\nautomatically via our constraint and thus we avoid the problem of tuning\npruning rates individually for different layers in a network. Extensive\nexperimental results on CIFAR-10/100 and ImageNet demonstrate that our method\nis highly effective, and can outperform previous state-of-the-art methods by a\nsignificant margin, especially in the high pruning rate situation. Notably, the\ngap of Top-1 accuracy between our ProbMask and existing methods can be up to\n10\\%. As a by-product, we show ProbMask is also highly effective in identifying\nsupermasks, which are subnetworks with high performance in a randomly weighted\ndense neural network.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:13:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhou", "Xiao", ""], ["Zhang", "Weizhong", ""], ["Xu", "Hang", ""], ["Zhang", "Tong", ""]]}, {"id": "2105.01583", "submitter": "Du Nguyen", "authors": "Du Nguyen", "title": "Riemannian Geometry with differentiable ambient space and metric\n  operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.LG cs.SY eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show Riemannian geometry could be studied by identifying the tangent\nbundle of a Riemannian manifold $\\mathcal{M}$ with a subbundle of the trivial\nbundle $\\mathcal{M} \\times \\mathcal{E}$, obtained by embedding $\\mathcal{M}$\ndifferentiably in a Euclidean space $\\mathcal{E}$. Given such an embedding, we\ncan extend the metric tensor on $\\mathcal{M}$ to a (positive-definite)\noperator-valued function acting on $\\mathcal{E}$, giving us an embedded ambient\nstructure. The formulas for the Christoffel symbols and Riemannian curvature in\nlocal coordinates have simple generalizations to this setup. For a Riemannian\nsubmersion $\\mathfrak{q}:\\mathcal{M}\\to \\mathcal{B}$ from an embedded manifold\n$\\mathcal{M}\\subset \\mathcal{E}$, we define a submersed ambient structure and\nobtain similar formulas, with the O'Neil tensor expressed in terms of the\nprojection to the horizontal bundle $\\mathcal{H}\\mathcal{M}$. Using this\nframework, we provide the embedded and submersed ambient structures for the\ndouble tangent bundle $\\mathcal{T}\\mathcal{T}\\mathcal{M}$ and the tangent of\nthe horizontal bundle $\\mathcal{T}\\mathcal{H}\\mathcal{M}$, describe the\nfibration of a horizontal bundle over the tangent bundle of the base manifold\nand extend the notion of a canonical flip to the submersion case. We obtain a\nformula for horizontal lifts of Jacobi fields, and a new closed-form formula\nfor Jacobi fields of naturally reductive homogeneous spaces. We construct\nnatural metrics on these double tangent bundles, in particular, extending\nSasaki and other natural metrics to the submersion case. We illustrate by\nproviding explicit calculations for several manifolds.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:47:45 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Nguyen", "Du", ""]]}, {"id": "2105.01593", "submitter": "Daniel Vial", "authors": "Daniel Vial, Advait Parulekar, Sanjay Shakkottai, R. Srikant", "title": "Regret Bounds for Stochastic Shortest Path Problems with Linear Function\n  Approximation", "comments": "Main addition to prior version is a computationally efficient\n  algorithm with K^{3/4} regret", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two algorithms that use linear function approximation (LFA) for\nstochastic shortest path (SSP) and bound their regret over $K$ episodes. When\nall stationary policies are proper, our first algorithm obtains sublinear\nregret ($K^{3/4}$), is computationally efficient, and uses stationary policies.\nThis is the first LFA algorithm with these three properties, to the best of our\nknowledge. Our second algorithm improves the regret to $\\sqrt{K}$ when the\nfeature vectors satisfy certain assumptions. Both algorithms are special cases\nof a more general one, which has $\\sqrt{K}$ regret for general features given\naccess to a certain computation oracle. These algorithms and regret bounds are\nthe first for SSP with function approximation.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:05:08 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 21:34:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Vial", "Daniel", ""], ["Parulekar", "Advait", ""], ["Shakkottai", "Sanjay", ""], ["Srikant", "R.", ""]]}, {"id": "2105.01601", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas\n  Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas\n  Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey\n  Dosovitskiy", "title": "MLP-Mixer: An all-MLP Architecture for Vision", "comments": "v2: Fixed parameter counts in Table 1. v3: Added results on JFT-3B in\n  Figure 2(right); Added Section 3.4 on the input permutations. v4: Updated the\n  x label in Figure 2(right)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are the go-to model for computer vision.\nRecently, attention-based networks, such as the Vision Transformer, have also\nbecome popular. In this paper we show that while convolutions and attention are\nboth sufficient for good performance, neither of them are necessary. We present\nMLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).\nMLP-Mixer contains two types of layers: one with MLPs applied independently to\nimage patches (i.e. \"mixing\" the per-location features), and one with MLPs\napplied across patches (i.e. \"mixing\" spatial information). When trained on\nlarge datasets, or with modern regularization schemes, MLP-Mixer attains\ncompetitive scores on image classification benchmarks, with pre-training and\ninference cost comparable to state-of-the-art models. We hope that these\nresults spark further research beyond the realms of well established CNNs and\nTransformers.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:17:21 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 12:48:26 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 09:50:52 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 09:36:50 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Houlsby", "Neil", ""], ["Kolesnikov", "Alexander", ""], ["Beyer", "Lucas", ""], ["Zhai", "Xiaohua", ""], ["Unterthiner", "Thomas", ""], ["Yung", "Jessica", ""], ["Steiner", "Andreas", ""], ["Keysers", "Daniel", ""], ["Uszkoreit", "Jakob", ""], ["Lucic", "Mario", ""], ["Dosovitskiy", "Alexey", ""]]}, {"id": "2105.01603", "submitter": "Lifang He", "authors": "Sicong Che, Hao Peng, Lichao Sun, Yong Chen, Lifang He", "title": "Federated Multi-View Learning for Private Medical Data Integration and\n  Analysis", "comments": "22 pages, 6 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the rapid expansion of information technology and digitalization\nof health data, there is an increasing concern on maintaining data privacy\nwhile garnering the benefits in medical field. Two critical challenges are\nidentified: Firstly, medical data is naturally distributed across multiple\nlocal sites, making it difficult to collectively train machine learning models\nwithout data leakage. Secondly, in medical applications, data are often\ncollected from different sources and views, resulting in heterogeneity and\ncomplexity that requires reconciliation. This paper aims to provide a generic\nFederated Multi-View Learning (FedMV) framework for multi-view data leakage\nprevention, which is based on different types of local data availability and\nenables to accommodate two types of problems: Vertical Federated Multi-View\nLearning (V-FedMV) and Horizontal Federated Multi-View Learning (H-FedMV). We\nexperimented with real-world keyboard data collected from BiAffect study. The\nresults demonstrated that the proposed FedMV approach can make full use of\nmulti-view data in a privacy-preserving way, and both V-FedMV and H-FedMV\nmethods perform better than their single-view and pairwise counterparts.\nBesides, the proposed model can be easily adapted to deal with multi-view\nsequential data in a federated environment, which has been modeled and\nexperimentally studied. To the best of our knowledge, this framework is the\nfirst to consider both vertical and horizontal diversification in the\nmulti-view setting, as well as their sequential federated learning.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:23:25 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Che", "Sicong", ""], ["Peng", "Hao", ""], ["Sun", "Lichao", ""], ["Chen", "Yong", ""], ["He", "Lifang", ""]]}, {"id": "2105.01604", "submitter": "Gal Metzer", "authors": "Gal Metzer, Rana Hanocka, Denis Zorin, Raja Giryes, Daniele Panozzo,\n  Daniel Cohen-Or", "title": "Orienting Point Clouds with Dipole Propagation", "comments": "SIGGRAPH 2021", "journal-ref": null, "doi": "10.1145/3450626.3459835", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing a consistent normal orientation for point clouds is a\nnotoriously difficult problem in geometry processing, requiring attention to\nboth local and global shape characteristics. The normal direction of a point is\na function of the local surface neighborhood; yet, point clouds do not disclose\nthe full underlying surface structure. Even assuming known geodesic proximity,\ncalculating a consistent normal orientation requires the global context. In\nthis work, we introduce a novel approach for establishing a globally consistent\nnormal orientation for point clouds. Our solution separates the local and\nglobal components into two different sub-problems. In the local phase, we train\na neural network to learn a coherent normal direction per patch (i.e.,\nconsistently oriented normals within a single patch). In the global phase, we\npropagate the orientation across all coherent patches using a dipole\npropagation. Our dipole propagation decides to orient each patch using the\nelectric field defined by all previously orientated patches. This gives rise to\na global propagation that is stable, as well as being robust to nearby\nsurfaces, holes, sharp features and noise.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:25:36 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Metzer", "Gal", ""], ["Hanocka", "Rana", ""], ["Zorin", "Denis", ""], ["Giryes", "Raja", ""], ["Panozzo", "Daniele", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2105.01605", "submitter": "Xiaojun Chang", "authors": "Xiangtan Lin and Pengzhen Ren and Yun Xiao and Xiaojun Chang and Alex\n  Hauptmann", "title": "Person Search Challenges and Solutions: A Survey", "comments": "8 pages; Accepted by IJCAI 2021 Survey Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Person search has drawn increasing attention due to its real-world\napplications and research significance. Person search aims to find a probe\nperson in a gallery of scene images with a wide range of applications, such as\ncriminals search, multicamera tracking, missing person search, etc. Early\nperson search works focused on image-based person search, which uses person\nimage as the search query. Text-based person search is another major person\nsearch category that uses free-form natural language as the search query.\nPerson search is challenging, and corresponding solutions are diverse and\ncomplex. Therefore, systematic surveys on this topic are essential. This paper\nsurveyed the recent works on image-based and text-based person search from the\nperspective of challenges and solutions. Specifically, we provide a brief\nanalysis of highly influential person search methods considering the three\nsignificant challenges: the discriminative person features, the query-person\ngap, and the detection-identification inconsistency. We summarise and compare\nevaluation results. Finally, we discuss open issues and some promising future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:10:20 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Lin", "Xiangtan", ""], ["Ren", "Pengzhen", ""], ["Xiao", "Yun", ""], ["Chang", "Xiaojun", ""], ["Hauptmann", "Alex", ""]]}, {"id": "2105.01606", "submitter": "Sarra Alqahtani", "authors": "Ashley Peake, Joe McCalmon, Yixin Zhang, Daniel Myers, Sarra\n  Alqahtani, Paul Pauca", "title": "Deep Reinforcement Learning for Adaptive Exploration of Unknown\n  Environments", "comments": null, "journal-ref": "The 2021 International Conference On Unmanned Aircraft System\n  (ICUAS '21)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing autonomous exploration is essential for unmanned aerial vehicles\n(UAVs) operating in unknown environments. Often, these missions start with\nbuilding a map for the environment via pure exploration and subsequently using\n(i.e. exploiting) the generated map for downstream navigation tasks.\nAccomplishing these navigation tasks in two separate steps is not always\npossible or even disadvantageous for UAVs deployed in outdoor and dynamically\nchanging environments. Current exploration approaches either use a priori\nhuman-generated maps or use heuristics such as frontier-based exploration.\nOther approaches use learning but focus only on learning policies for specific\ntasks by either using sample inefficient random exploration or by making\nimpractical assumptions about full map availability. In this paper, we develop\nan adaptive exploration approach to trade off between exploration and\nexploitation in one single step for UAVs searching for areas of interest (AoIs)\nin unknown environments using Deep Reinforcement Learning (DRL). The proposed\napproach uses a map segmentation technique to decompose the environment map\ninto smaller, tractable maps. Then, a simple information gain function is\nrepeatedly computed to determine the best target region to search during each\niteration of the process. DDQN and A2C algorithms are extended with a stack of\nLSTM layers and trained to generate optimal policies for the exploration and\nexploitation, respectively. We tested our approach in 3 different tasks against\n4 baselines. The results demonstrate that our proposed approach is capable of\nnavigating through randomly generated environments and covering more AoI in\nless time steps compared to the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:29:44 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Peake", "Ashley", ""], ["McCalmon", "Joe", ""], ["Zhang", "Yixin", ""], ["Myers", "Daniel", ""], ["Alqahtani", "Sarra", ""], ["Pauca", "Paul", ""]]}, {"id": "2105.01620", "submitter": "Lixin Zou", "authors": "Lixin Zou, Long Xia, Linfang Hou, Xiangyu Zhao, and Dawei Yin", "title": "Data-Efficient Reinforcement Learning for Malaria Control", "comments": "7 pages, 4 figures, IJCAI 2021 Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequential decision-making under cost-sensitive tasks is prohibitively\ndaunting, especially for the problem that has a significant impact on people's\ndaily lives, such as malaria control, treatment recommendation. The main\nchallenge faced by policymakers is to learn a policy from scratch by\ninteracting with a complex environment in a few trials. This work introduces a\npractical, data-efficient policy learning method, named Variance-Bonus Monte\nCarlo Tree Search~(VB-MCTS), which can copy with very little data and\nfacilitate learning from scratch in only a few trials. Specifically, the\nsolution is a model-based reinforcement learning method. To avoid model bias,\nwe apply Gaussian Process~(GP) regression to estimate the transitions\nexplicitly. With the GP world model, we propose a variance-bonus reward to\nmeasure the uncertainty about the world. Adding the reward to the planning with\nMCTS can result in more efficient and effective exploration. Furthermore, the\nderived polynomial sample complexity indicates that VB-MCTS is sample\nefficient. Finally, outstanding performance on a competitive world-level RL\ncompetition and extensive experimental results verify its advantage over the\nstate-of-the-art on the challenging malaria control task.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:54:16 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 04:19:07 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zou", "Lixin", ""], ["Xia", "Long", ""], ["Hou", "Linfang", ""], ["Zhao", "Xiangyu", ""], ["Yin", "Dawei", ""]]}, {"id": "2105.01622", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini", "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised machine learning models learn from a (small) set of labeled\ntraining examples, and a (large) set of unlabeled training examples.\nState-of-the-art models can reach within a few percentage points of\nfully-supervised training, while requiring 100x less labeled data.\n  We study a new class of vulnerabilities: poisoning attacks that modify the\nunlabeled dataset. In order to be useful, unlabeled datasets are given strictly\nless review than labeled datasets, and adversaries can therefore poison them\neasily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%\nof the dataset size, we can manipulate a model trained on this poisoned dataset\nto misclassify arbitrary examples at test time (as any desired label). Our\nattacks are highly effective across datasets and semi-supervised learning\nmethods.\n  We find that more accurate methods (thus more likely to be used) are\nsignificantly more vulnerable to poisoning attacks, and as such better training\nmethods are unlikely to prevent this attack. To counter this we explore the\nspace of defenses, and propose two methods that mitigate our attack.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:55:20 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Carlini", "Nicholas", ""]]}, {"id": "2105.01636", "submitter": "Andreas Mayr", "authors": "Andreas Mayr, Sebastian Lehner, Arno Mayrhofer, Christoph Kloss, Sepp\n  Hochreiter, Johannes Brandstetter", "title": "Learning 3D Granular Flow Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the application of machine learning models has gained momentum in\nnatural sciences and engineering, which is a natural fit due to the abundance\nof data in these fields. However, the modeling of physical processes from\nsimulation data without first principle solutions remains difficult. Here, we\npresent a Graph Neural Networks approach towards accurate modeling of complex\n3D granular flow simulation processes created by the discrete element method\nLIGGGHTS and concentrate on simulations of physical systems found in real world\napplications like rotating drums and hoppers. We discuss how to implement Graph\nNeural Networks that deal with 3D objects, boundary conditions, particle -\nparticle, and particle - boundary interactions such that an accurate modeling\nof relevant physical quantities is made possible. Finally, we compare the\nmachine learning based trajectories to LIGGGHTS trajectories in terms of\nparticle flows and mixing entropies.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:27:59 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Mayr", "Andreas", ""], ["Lehner", "Sebastian", ""], ["Mayrhofer", "Arno", ""], ["Kloss", "Christoph", ""], ["Hochreiter", "Sepp", ""], ["Brandstetter", "Johannes", ""]]}, {"id": "2105.01637", "submitter": "Quentin Bertrand", "authors": "Quentin Bertrand, Quentin Klopfenstein, Mathurin Massias, Mathieu\n  Blondel, Samuel Vaiter, Alexandre Gramfort, Joseph Salmon", "title": "Implicit differentiation for fast hyperparameter selection in non-smooth\n  convex learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the optimal hyperparameters of a model can be cast as a bilevel\noptimization problem, typically solved using zero-order techniques. In this\nwork we study first-order methods when the inner optimization problem is convex\nbut non-smooth. We show that the forward-mode differentiation of proximal\ngradient descent and proximal coordinate descent yield sequences of Jacobians\nconverging toward the exact Jacobian. Using implicit differentiation, we show\nit is possible to leverage the non-smoothness of the inner problem to speed up\nthe computation. Finally, we provide a bound on the error made on the\nhypergradient when the inner optimization problem is solved approximately.\nResults on regression and classification problems reveal computational benefits\nfor hyperparameter optimization, especially when multiple hyperparameters are\nrequired.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:31:28 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 13:07:28 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bertrand", "Quentin", ""], ["Klopfenstein", "Quentin", ""], ["Massias", "Mathurin", ""], ["Blondel", "Mathieu", ""], ["Vaiter", "Samuel", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "2105.01648", "submitter": "Robert Tjarko Lange", "authors": "Marc Aurel Vischer, Robert Tjarko Lange, Henning Sprekeler", "title": "On Lottery Tickets and Minimal Task Representations in Deep\n  Reinforcement Learning", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lottery ticket hypothesis questions the role of overparameterization in\nsupervised deep learning. But how is the performance of winning lottery tickets\naffected by the distributional shift inherent to reinforcement learning\nproblems? In this work, we address this question by comparing sparse agents who\nhave to address the non-stationarity of the exploration-exploitation problem\nwith supervised agents trained to imitate an expert. We show that feed-forward\nnetworks trained via reinforcement learning and imitation learning can be\npruned to the same level of sparsity, suggesting that the distributional shift\nhas a limited impact on the size of winning tickets. Using a set of carefully\ndesigned baseline conditions, we find that the majority of the lottery ticket\neffect in both learning paradigms can be attributed to the identified mask\nrather than the weight initialization. The input layer mask selectively prunes\nentire input dimensions that turn out to be irrelevant for the task at hand. At\na moderate level of sparsity the mask identified by iterative magnitude pruning\nyields minimal task-relevant representations, i.e., an interpretable inductive\nbias. Finally, we propose a simple initialization rescaling which promotes the\nrobust identification of sparse task representations in low-dimensional control\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:47:39 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 04:24:37 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Vischer", "Marc Aurel", ""], ["Lange", "Robert Tjarko", ""], ["Sprekeler", "Henning", ""]]}, {"id": "2105.01650", "submitter": "Stephan Wojtowytsch", "authors": "Stephan Wojtowytsch", "title": "Stochastic gradient descent with noise of machine learning type. Part I:\n  Discrete time analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is one of the most popular algorithms in\nmodern machine learning. The noise encountered in these applications is\ndifferent from that in many theoretical analyses of stochastic gradient\nalgorithms. In this article, we discuss some of the common properties of energy\nlandscapes and stochastic noise encountered in machine learning problems, and\nhow they affect SGD-based optimization.\n  In particular, we show that the learning rate in SGD with machine learning\nnoise can be chosen to be small, but uniformly positive for all times if the\nenergy landscape resembles that of overparametrized deep learning problems. If\nthe objective function satisfies a Lojasiewicz inequality, SGD converges to the\nglobal minimum exponentially fast, and even for functions which may have local\nminima, we establish almost sure convergence to the global minimum at an\nexponential rate from any finite energy initialization. The assumptions that we\nmake in this result concern the behavior where the objective function is either\nsmall or large and the nature of the gradient noise, but the energy landscape\nis fairly unconstrained on the domain where the objective function takes values\nin an intermediate regime.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:52:20 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Wojtowytsch", "Stephan", ""]]}, {"id": "2105.01683", "submitter": "Nhan Tran", "authors": "Giuseppe Di Guglielmo, Farah Fahim, Christian Herwig, Manuel Blanco\n  Valentin, Javier Duarte, Cristian Gingu, Philip Harris, James Hirschauer,\n  Martin Kwok, Vladimir Loncar, Yingyi Luo, Llovizna Miranda, Jennifer\n  Ngadiuba, Daniel Noonan, Seda Ogrenci-Memik, Maurizio Pierini, Sioni Summers,\n  Nhan Tran", "title": "A reconfigurable neural network ASIC for detector front-end data\n  compression at the HL-LHC", "comments": "9 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": "FERMILAB-PUB-21-217-CMS-E-SCD", "categories": "physics.ins-det cs.LG hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advances in the programmable logic capabilities of modern trigger\nsystems, a significant bottleneck remains in the amount of data to be\ntransported from the detector to off-detector logic where trigger decisions are\nmade. We demonstrate that a neural network autoencoder model can be implemented\nin a radiation tolerant ASIC to perform lossy data compression alleviating the\ndata transmission problem while preserving critical information of the detector\nenergy profile. For our application, we consider the high-granularity\ncalorimeter from the CMS experiment at the CERN Large Hadron Collider. The\nadvantage of the machine learning approach is in the flexibility and\nconfigurability of the algorithm. By changing the neural network weights, a\nunique data compression algorithm can be deployed for each sensor in different\ndetector regions, and changing detector or collider conditions. To meet area,\nperformance, and power constraints, we perform a quantization-aware training to\ncreate an optimized neural network hardware implementation. The design is\nachieved through the use of high-level synthesis tools and the hls4ml\nframework, and was processed through synthesis and physical layout flows based\non a LP CMOS 65 nm technology node. The flow anticipates 200 Mrad of ionizing\nradiation to select gates, and reports a total area of 3.6 mm^2 and consumes 95\nmW of power. The simulated energy consumption per inference is 2.4 nJ. This is\nthe first radiation tolerant on-detector ASIC implementation of a neural\nnetwork that has been designed for particle physics applications.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:06:23 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Di Guglielmo", "Giuseppe", ""], ["Fahim", "Farah", ""], ["Herwig", "Christian", ""], ["Valentin", "Manuel Blanco", ""], ["Duarte", "Javier", ""], ["Gingu", "Cristian", ""], ["Harris", "Philip", ""], ["Hirschauer", "James", ""], ["Kwok", "Martin", ""], ["Loncar", "Vladimir", ""], ["Luo", "Yingyi", ""], ["Miranda", "Llovizna", ""], ["Ngadiuba", "Jennifer", ""], ["Noonan", "Daniel", ""], ["Ogrenci-Memik", "Seda", ""], ["Pierini", "Maurizio", ""], ["Summers", "Sioni", ""], ["Tran", "Nhan", ""]]}, {"id": "2105.01688", "submitter": "Anusua Trivedi", "authors": "Anusua Trivedi, Mohit Jain, Nikhil Kumar Gupta, Markus Hinsche,\n  Prashant Singh, Markus Matiaschek, Tristan Behrens, Mirco Militeri, Cameron\n  Birge, Shivangi Kaushik, Archisman Mohapatra, Rita Chatterjee, Rahul Dodhia,\n  Juan Lavista Ferres", "title": "Height Estimation of Children under Five Years using Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malnutrition is a global health crisis and is the leading cause of death\namong children under five. Detecting malnutrition requires anthropometric\nmeasurements of weight, height, and middle-upper arm circumference. However,\nmeasuring them accurately is a challenge, especially in the global south, due\nto limited resources. In this work, we propose a CNN-based approach to estimate\nthe height of standing children under five years from depth images collected\nusing a smart-phone. According to the SMART Methodology Manual [5], the\nacceptable accuracy for height is less than 1.4 cm. On training our deep\nlearning model on 87131 depth images, our model achieved an average mean\nabsolute error of 1.64% on 57064 test images. For 70.3% test images, we\nestimated height accurately within the acceptable 1.4 cm range. Thus, our\nproposed solution can accurately detect stunting (low height-for-age) in\nstanding children below five years of age.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:15:57 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Trivedi", "Anusua", ""], ["Jain", "Mohit", ""], ["Gupta", "Nikhil Kumar", ""], ["Hinsche", "Markus", ""], ["Singh", "Prashant", ""], ["Matiaschek", "Markus", ""], ["Behrens", "Tristan", ""], ["Militeri", "Mirco", ""], ["Birge", "Cameron", ""], ["Kaushik", "Shivangi", ""], ["Mohapatra", "Archisman", ""], ["Chatterjee", "Rita", ""], ["Dodhia", "Rahul", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "2105.01696", "submitter": "Haoran Sun", "authors": "Haoran Sun, Wenqiang Pu, Xiao Fu, Tsung-Hui Chang, Mingyi Hong", "title": "Learning to Continuously Optimize Wireless Resource in a Dynamic\n  Environment: A Bilevel Optimization Perspective", "comments": "arXiv admin note: text overlap with arXiv:2011.07782", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing interest in developing data-driven, and in\nparticular deep neural network (DNN) based methods for modern communication\ntasks. For a few popular tasks such as power control, beamforming, and MIMO\ndetection, these methods achieve state-of-the-art performance while requiring\nless computational efforts, less resources for acquiring channel state\ninformation (CSI), etc. However, it is often challenging for these approaches\nto learn in a dynamic environment.\n  This work develops a new approach that enables data-driven methods to\ncontinuously learn and optimize resource allocation strategies in a dynamic\nenvironment. Specifically, we consider an ``episodically dynamic\" setting where\nthe environment statistics change in ``episodes\", and in each episode the\nenvironment is stationary. We propose to build the notion of continual learning\n(CL) into wireless system design, so that the learning model can incrementally\nadapt to the new episodes, {\\it without forgetting} knowledge learned from the\nprevious episodes. Our design is based on a novel bilevel optimization\nformulation which ensures certain ``fairness\" across different data samples. We\ndemonstrate the effectiveness of the CL approach by integrating it with two\npopular DNN based models for power control and beamforming, respectively, and\ntesting using both synthetic and ray-tracing based data sets. These numerical\nresults show that the proposed CL approach is not only able to adapt to the new\nscenarios quickly and seamlessly, but importantly, it also maintains high\nperformance over the previously encountered scenarios as well.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 07:23:39 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Sun", "Haoran", ""], ["Pu", "Wenqiang", ""], ["Fu", "Xiao", ""], ["Chang", "Tsung-Hui", ""], ["Hong", "Mingyi", ""]]}, {"id": "2105.01705", "submitter": "Marc G\\'orriz Blanch", "authors": "Marc Gorriz Blanch, Issa Khalifeh, Alan Smeaton, Noel O'Connor, Marta\n  Mrak", "title": "Attention-based Stylisation for Exemplar Image Colourisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Exemplar-based colourisation aims to add plausible colours to a grayscale\nimage using the guidance of a colour reference image. Most of the existing\nmethods tackle the task as a style transfer problem, using a convolutional\nneural network (CNN) to obtain deep representations of the content of both\ninputs. Stylised outputs are then obtained by computing similarities between\nboth feature representations in order to transfer the style of the reference to\nthe content of the target input. However, in order to gain robustness towards\ndissimilar references, the stylised outputs need to be refined with a second\ncolourisation network, which significantly increases the overall system\ncomplexity. This work reformulates the existing methodology introducing a novel\nend-to-end colourisation network that unifies the feature matching with the\ncolourisation process. The proposed architecture integrates attention modules\nat different resolutions that learn how to perform the style transfer task in\nan unsupervised way towards decoding realistic colour predictions. Moreover,\naxial attention is proposed to simplify the attention operations and to obtain\na fast but robust cost-effective architecture. Experimental validations\ndemonstrate efficiency of the proposed methodology which generates high quality\nand visual appealing colourisation. Furthermore, the complexity of the proposed\nmethodology is reduced compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:56:26 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Blanch", "Marc Gorriz", ""], ["Khalifeh", "Issa", ""], ["Smeaton", "Alan", ""], ["O'Connor", "Noel", ""], ["Mrak", "Marta", ""]]}, {"id": "2105.01706", "submitter": "Chiheb Daaloul", "authors": "Chiheb Daaloul (1), Thibaut Le Gouic (2), Jacques Liandrat (1), Magali\n  Tournus (1) ((1) Aix-Marseille Univ., CNRS, I2M, UMR7373, Centrale Marseille,\n  Marseille, France, (2) Massachusetts Institute of Technology, Department of\n  Mathematics, USA)", "title": "Sampling From the Wasserstein Barycenter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an algorithm to sample from the Wasserstein barycenter of\nabsolutely continuous measures. Our method is based on the gradient flow of the\nmultimarginal formulation of the Wasserstein barycenter, with an additive\npenalization to account for the marginal constraints. We prove that the minimum\nof this penalized multimarginal formulation is achieved for a coupling that is\nclose to the Wasserstein barycenter. The performances of the algorithm are\nshowcased in several settings.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:57:41 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Daaloul", "Chiheb", ""], ["Gouic", "Thibaut Le", ""], ["Liandrat", "Jacques", ""], ["Tournus", "Magali", ""]]}, {"id": "2105.01710", "submitter": "Pengcheng Xi", "authors": "Jianxing Zhang, Pengcheng Xi, Ashkan Ebadi, Hilda Azimi, Stephane\n  Tremblay, Alexander Wong", "title": "COVID-19 Detection from Chest X-ray Images using Imprinted Weights\n  Approach", "comments": "Accepted to ICLR 2021 Workshop: Machine Learning for Preventing and\n  Combating Pandemics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has had devastating effects on the well-being of the\nglobal population. The pandemic has been so prominent partly due to the high\ninfection rate of the virus and its variants. In response, one of the most\neffective ways to stop infection is rapid diagnosis. The main-stream screening\nmethod, reverse transcription-polymerase chain reaction (RT-PCR), is\ntime-consuming, laborious and in short supply. Chest radiography is an\nalternative screening method for the COVID-19 and computer-aided diagnosis\n(CAD) has proven to be a viable solution at low cost and with fast speed;\nhowever, one of the challenges in training the CAD models is the limited number\nof training data, especially at the onset of the pandemic. This becomes\noutstanding precisely when the quick and cheap type of diagnosis is critically\nneeded for flattening the infection curve. To address this challenge, we\npropose the use of a low-shot learning approach named imprinted weights, taking\nadvantage of the abundance of samples from known illnesses such as pneumonia to\nimprove the detection performance on COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 19:01:40 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhang", "Jianxing", ""], ["Xi", "Pengcheng", ""], ["Ebadi", "Ashkan", ""], ["Azimi", "Hilda", ""], ["Tremblay", "Stephane", ""], ["Wong", "Alexander", ""]]}, {"id": "2105.01714", "submitter": "Juan Cabral", "authors": "J. B. Cabral, M. Lares, S. Gurovich, D. Minniti, P. M. Granitto", "title": "Drifting Features: Detection and evaluation in the context of automatic\n  RRLs identification in VVV", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As most of the modern astronomical sky surveys produce data faster than\nhumans can analyze it, Machine Learning (ML) has become a central tool in\nAstronomy. Modern ML methods can be characterized as highly resistant to some\nexperimental errors. However, small changes on the data over long distances or\nlong periods of time, which cannot be easily detected by statistical methods,\ncan be harmful to these methods. We develop a new strategy to cope with this\nproblem, also using ML methods in an innovative way, to identify these\npotentially harmful features. We introduce and discuss the notion of Drifting\nFeatures, related with small changes in the properties as measured in the data\nfeatures. We use the identification of RRLs in VVV based on an earlier work and\nintroduce a method for detecting Drifting Features. Our method forces a\nclassifier to learn the tile of origin of diverse sources (mostly stellar\n'point sources'), and select the features more relevant to the task of finding\ncandidates to Drifting Features. We show that this method can efficiently\nidentify a reduced set of features that contains useful information about the\ntile of origin of the sources. For our particular example of detecting RRLs in\nVVV, we find that Drifting Features are mostly related to color indices. On the\nother hand, we show that, even if we have a clear set of Drifting Features in\nour problem, they are mostly insensitive to the identification of RRLs.\nDrifting Features can be efficiently identified using ML methods. However, in\nour example, removing Drifting Features does not improve the identification of\nRRLs.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 19:07:32 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 00:52:13 GMT"}, {"version": "v3", "created": "Sun, 23 May 2021 03:44:04 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cabral", "J. B.", ""], ["Lares", "M.", ""], ["Gurovich", "S.", ""], ["Minniti", "D.", ""], ["Granitto", "P. M.", ""]]}, {"id": "2105.01727", "submitter": "Stanislava Fedorova", "authors": "Stanislava Fedorova", "title": "GANs for Urban Design", "comments": "Presented in SimAUD 2021 9 pages, 8 figures Related github\n  repositories: https://github.com/STASYA00/urban_datasets\n  https://github.com/STASYA00/UrbanGen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Development and diffusion of machine learning and big data tools provide a\nnew tool for architects and urban planners that could be used as analytical or\ndesign instruments. The topic investigated in this paper is the application of\nGenerative Adversarial Networks to the design of an urban block. The research\npresents a flexible model able to adapt to the morphological characteristics of\na city. This method does not define explicitly any of the parameters of an\nurban block typical for a city, the algorithm learns them from the existing\nurban context. This approach has been applied to the cities with different\nmorphology: Milan, Amsterdam, Tallinn, Turin, and Bengaluru in order to see the\nperformance of the model and the possibility of style translation between\ndifferent cities. The data are gathered from Openstreetmap and Open Data\nportals of the cities. This research presents the results of the experiments\nand their quantitative and qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 19:50:24 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Fedorova", "Stanislava", ""]]}, {"id": "2105.01733", "submitter": "Bart J. A. Mertens", "authors": "Bart J. A. Mertens", "title": "Calibration of prediction rules for life-time outcomes using prognostic\n  Cox regression survival models and multiple imputations to account for\n  missing predictor data with cross-validatory assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we expand the methodology presented in Mertens et. al (2020,\nBiometrical Journal) to the study of life-time (survival) outcome which is\nsubject to censoring and when imputation is used to account for missing values.\nWe consider the problem where missing values can occur in both the calibration\ndata as well as newly - to-be-predicted - observations (validation). We focus\non the Cox model. Methods are described to combine imputation with predictive\ncalibration in survival modeling subject to censoring. Application to\ncross-validation is discussed. We demonstrate how conclusions broadly confirm\nthe first paper which restricted to the study of binary outcomes only.\nSpecifically prediction-averaging appears to have superior statistical\nproperties, especially smaller predictive variation, as opposed to a direct\napplication of Rubin's rules. Distinct methods for dealing with the baseline\nhazards are discussed when using Rubin's rules-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:10:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mertens", "Bart J. A.", ""]]}, {"id": "2105.01735", "submitter": "Piotr Rybak", "authors": "Robert Mroczkowski, Piotr Rybak, Alina Wr\\'oblewska, Ireneusz Gawlik", "title": "HerBERT: Efficiently Pretrained Transformer-based Language Model for\n  Polish", "comments": "Published in Proceedings of the 8th Workshop on Balto-Slavic Natural\n  Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT-based models are currently used for solving nearly all Natural Language\nProcessing (NLP) tasks and most often achieve state-of-the-art results.\nTherefore, the NLP community conducts extensive research on understanding these\nmodels, but above all on designing effective and efficient training procedures.\nSeveral ablation studies investigating how to train BERT-like models have been\ncarried out, but the vast majority of them concerned only the English language.\nA training procedure designed for English does not have to be universal and\napplicable to other especially typologically different languages. Therefore,\nthis paper presents the first ablation study focused on Polish, which, unlike\nthe isolating English language, is a fusional language. We design and\nthoroughly evaluate a pretraining procedure of transferring knowledge from\nmultilingual to monolingual BERT-based models. In addition to multilingual\nmodel initialization, other factors that possibly influence pretraining are\nalso explored, i.e. training objective, corpus size, BPE-Dropout, and\npretraining length. Based on the proposed procedure, a Polish BERT-based\nlanguage model -- HerBERT -- is trained. This model achieves state-of-the-art\nresults on multiple downstream tasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:16:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mroczkowski", "Robert", ""], ["Rybak", "Piotr", ""], ["Wr\u00f3blewska", "Alina", ""], ["Gawlik", "Ireneusz", ""]]}, {"id": "2105.01746", "submitter": "Chao Li", "authors": "Chao Li, Hang Zhang, Jinwei Zhang, Pascal Spincemaille, Thanh\n  D.Nguyen, Yi Wang", "title": "Motion Artifact Reduction in Quantitative Susceptibility Mapping using\n  Deep Neural Network", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An approach to reduce motion artifacts in Quantitative Susceptibility Mapping\nusing deep learning is proposed. We use an affine motion model with randomly\ncreated motion profiles to simulate motion-corrupted QSM images. The simulated\nQSM image is paired with its motion-free reference to train a neural network\nusing supervised learning. The trained network is tested on unseen simulated\nmotion-corrupted QSM images, in healthy volunteers and in Parkinson's disease\npatients. The results show that motion artifacts, such as ringing and ghosting,\nwere successfully suppressed.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:32:02 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Li", "Chao", ""], ["Zhang", "Hang", ""], ["Zhang", "Jinwei", ""], ["Spincemaille", "Pascal", ""], ["Nguyen", "Thanh D.", ""], ["Wang", "Yi", ""]]}, {"id": "2105.01747", "submitter": "Pradeep Kr. Banerjee", "authors": "Pradeep Kr. Banerjee, Guido Mont\\'ufar", "title": "Information Complexity and Generalization Bounds", "comments": "Accepted for presentation at 2021 IEEE International Symposium on\n  Information Theory (ISIT); 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a unifying picture of PAC-Bayesian and mutual information-based\nupper bounds on the generalization error of randomized learning algorithms. As\nwe show, Tong Zhang's information exponential inequality (IEI) gives a general\nrecipe for constructing bounds of both flavors. We show that several important\nresults in the literature can be obtained as simple corollaries of the IEI\nunder different assumptions on the loss function. Moreover, we obtain new\nbounds for data-dependent priors and unbounded loss functions. Optimizing the\nbounds gives rise to variants of the Gibbs algorithm, for which we discuss two\npractical examples for learning with neural networks, namely, Entropy- and\nPAC-Bayes- SGD. Further, we use an Occam's factor argument to show a\nPAC-Bayesian bound that incorporates second-order curvature information of the\ntraining loss.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:37:57 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Banerjee", "Pradeep Kr.", ""], ["Mont\u00fafar", "Guido", ""]]}, {"id": "2105.01748", "submitter": "Hassan Mostafa Engineer", "authors": "Ahmed Farag Seddik, Hassan Mostafa Ahmed", "title": "Ovarian Cancer Detection based on Dimensionality Reduction Techniques\n  and Genetic Algorithm", "comments": "7 Pages", "journal-ref": "EMBS ISC 2013", "doi": "10.13140/RG.2.1.3517.2324", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this research, we have two serum SELDI (surface-enhanced laser desorption\nand ionization) mass spectra (MS) datasets to be used to select features\namongst them to identify proteomic cancerous serums from normal serums.\nFeatures selection techniques have been applied and classification techniques\nhave been applied as well. Amongst the features selection techniques we have\nchosen to evaluate the performance of PCA (Principal Component Analysis ) and\nGA (Genetic algorithm), and amongst the classification techniques we have\nchosen the LDA (Linear Discriminant Analysis) and Neural networks so as to\nevaluate the ability of the selected features in identifying the cancerous\npatterns. Results were obtained for two combinations of features selection\ntechniques and classification techniques, the first one was PCA+(t-test)\ntechnique for features selection and LDA for accuracy tracking yielded an\naccuracy of 93.0233 % , the other one was genetic algorithm and neural network\nyielded an accuracy of 100%. So, we conclude that GA is more efficient for\nfeatures selection and hence for cancerous patterns detection than PCA\ntechnique.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:38:29 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Seddik", "Ahmed Farag", ""], ["Ahmed", "Hassan Mostafa", ""]]}, {"id": "2105.01751", "submitter": "Ilya Volkovich", "authors": "Vishwas Bhargava, Shubhangi Saraf, Ilya Volkovich", "title": "Reconstruction Algorithms for Low-Rank Tensors and Depth-3 Multilinear\n  Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give new and efficient black-box reconstruction algorithms for some\nclasses of depth-$3$ arithmetic circuits. As a consequence, we obtain the first\nefficient algorithm for computing the tensor rank and for finding the optimal\ntensor decomposition as a sum of rank-one tensors when then input is a\nconstant-rank tensor. More specifically, we provide efficient learning\nalgorithms that run in randomized polynomial time over general fields and in\ndeterministic polynomial time over the reals and the complex numbers for the\nfollowing classes:\n  (1) Set-multilinear depth-$3$ circuits of constant top fan-in\n$\\Sigma\\Pi\\Sigma\\{\\sqcup_j X_j\\}(k)$ circuits). As a consequence of our\nalgorithm, we obtain the first polynomial time algorithm for tensor rank\ncomputation and optimal tensor decomposition of constant-rank tensors. This\nresult holds for $d$ dimensional tensors for any $d$, but is interesting even\nfor $d=3$.\n  (2) Sums of powers of constantly many linear forms ($\\Sigma\\wedge\\Sigma$\ncircuits). As a consequence we obtain the first polynomial-time algorithm for\ntensor rank computation and optimal tensor decomposition of constant-rank\nsymmetric tensors.\n  (3) Multilinear depth-3 circuits of constant top fan-in (multilinear\n$\\Sigma\\Pi\\Sigma(k)$ circuits). Our algorithm works over all fields of\ncharacteristic 0 or large enough characteristic. Prior to our work the only\nefficient algorithms known were over polynomially-sized finite fields (see.\nKarnin-Shpilka 09').\n  Prior to our work, the only polynomial-time or even subexponential-time\nalgorithms known (deterministic or randomized) for subclasses of\n$\\Sigma\\Pi\\Sigma(k)$ circuits that also work over large/infinite fields were\nfor the setting when the top fan-in $k$ is at most $2$ (see Sinha 16' and Sinha\n20').\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:45:07 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bhargava", "Vishwas", ""], ["Saraf", "Shubhangi", ""], ["Volkovich", "Ilya", ""]]}, {"id": "2105.01753", "submitter": "Matej Kr\\'alik", "authors": "Matej Kr\\'alik, Marek \\v{S}uppa", "title": "WaveGlove: Transformer-based hand gesture recognition using multiple\n  inertial sensors", "comments": "Accepted to EUSIPCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hand Gesture Recognition (HGR) based on inertial data has grown considerably\nin recent years, with the state-of-the-art approaches utilizing a single\nhandheld sensor and a vocabulary comprised of simple gestures.\n  In this work we explore the benefits of using multiple inertial sensors.\nUsing WaveGlove, a custom hardware prototype in the form of a glove with five\ninertial sensors, we acquire two datasets consisting of over $11000$ samples.\n  To make them comparable with prior work, they are normalized along with $9$\nother publicly available datasets, and subsequently used to evaluate a range of\nMachine Learning approaches for gesture recognition, including a newly proposed\nTransformer-based architecture. Our results show that even complex gestures\ninvolving different fingers can be recognized with high accuracy.\n  An ablation study performed on the acquired datasets demonstrates the\nimportance of multiple sensors, with an increase in performance when using up\nto three sensors and no significant improvements beyond that.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:50:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Kr\u00e1lik", "Matej", ""], ["\u0160uppa", "Marek", ""]]}, {"id": "2105.01755", "submitter": "Lukas Cavigelli", "authors": "Xavier Timoneda, Lukas Cavigelli", "title": "Reinforcement Learning for Scalable Logic Optimization with Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic optimization is an NP-hard problem commonly approached through\nhand-engineered heuristics. We propose to combine graph convolutional networks\nwith reinforcement learning and a novel, scalable node embedding method to\nlearn which local transforms should be applied to the logic graph. We show that\nthis method achieves a similar size reduction as ABC on smaller circuits and\noutperforms it by 1.5-1.75x on larger random graphs.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:51:54 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Timoneda", "Xavier", ""], ["Cavigelli", "Lukas", ""]]}, {"id": "2105.01768", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja", "title": "Texture for Colors: Natural Representations of Colors Using Variable\n  Bit-Depth Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous methods have been proposed to transform color and grayscale images\nto their single bit-per-pixel binary counterparts. Commonly, the goal is to\nenhance specific attributes of the original image to make it more amenable for\nanalysis. However, when the resulting binarized image is intended for human\nviewing, aesthetics must also be considered. Binarization techniques, such as\nhalf-toning, stippling, and hatching, have been widely used for modeling the\noriginal image's intensity profile. We present an automated method to transform\nan image to a set of binary textures that represent not only the intensities,\nbut also the colors of the original. The foundation of our method is\ninformation preservation: creating a set of textures that allows for the\nreconstruction of the original image's colors solely from the binarized\nrepresentation. We present techniques to ensure that the textures created are\nnot visually distracting, preserve the intensity profile of the images, and are\nnatural in that they map sets of colors that are perceptually similar to\npatterns that are similar. The approach uses deep-neural networks and is\nentirely self-supervised; no examples of good vs. bad binarizations are\nrequired. The system yields aesthetically pleasing binary images when tested on\na variety of image sources.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:22:02 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Baluja", "Shumeet", ""]]}, {"id": "2105.01771", "submitter": "Sumudu Samarakoon Dr.", "authors": "Sumudu Samarakoon and Jihong Park and Mehdi Bennis", "title": "Robust Reconfigurable Intelligent Surfaces via Invariant Risk and Causal\n  Representations", "comments": "5 pages, 5 figures, conference: The 22nd IEEE International Workshop\n  on Signal Processing Advances in Wireless Communications (SPAWC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of robust reconfigurable intelligent surface (RIS)\nsystem design under changes in data distributions is investigated. Using the\nnotion of invariant risk minimization (IRM), an invariant causal representation\nacross multiple environments is used such that the predictor is simultaneously\noptimal for each environment. A neural network-based solution is adopted to\nseek the predictor and its performance is validated via simulations against an\nempirical risk minimization-based design. Results show that leveraging\ninvariance yields more robustness against unseen and out-of-distribution\ntesting environments.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:36:31 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Samarakoon", "Sumudu", ""], ["Park", "Jihong", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2105.01777", "submitter": "Hao Ya Hsueh", "authors": "Alexandru-Iosif Toma, Hao-Ya Hsueh, Hussein Ali Jaafar, Riku Murai,\n  Paul H.J. Kelly, Sajad Saeedi", "title": "PathBench: A Benchmarking Platform for Classical and Learned Path\n  Planning Algorithms", "comments": "The Conference on Robots and Vision (CRV2021), Supplementary Website:\n  https://sites.google.com/view/pathbench/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path planning is a key component in mobile robotics. A wide range of path\nplanning algorithms exist, but few attempts have been made to benchmark the\nalgorithms holistically or unify their interface. Moreover, with the recent\nadvances in deep neural networks, there is an urgent need to facilitate the\ndevelopment and benchmarking of such learning-based planning algorithms. This\npaper presents PathBench, a platform for developing, visualizing, training,\ntesting, and benchmarking of existing and future, classical and learned 2D and\n3D path planning algorithms, while offering support for Robot Oper-ating System\n(ROS). Many existing path planning algorithms are supported; e.g. A*,\nwavefront, rapidly-exploring random tree, value iteration networks, gated path\nplanning networks; and integrating new algorithms is easy and clearly\nspecified. We demonstrate the benchmarking capability of PathBench by comparing\nimplemented classical and learned algorithms for metrics, such as path length,\nsuccess rate, computational time and path deviation. These evaluations are done\non built-in PathBench maps and external path planning environments from video\ngames and real world databases. PathBench is open source.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:48:18 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Toma", "Alexandru-Iosif", ""], ["Hsueh", "Hao-Ya", ""], ["Jaafar", "Hussein Ali", ""], ["Murai", "Riku", ""], ["Kelly", "Paul H. J.", ""], ["Saeedi", "Sajad", ""]]}, {"id": "2105.01778", "submitter": "Yair Carmon", "authors": "Yair Carmon, Arun Jambulapati, Yujia Jin, Aaron Sidford", "title": "Thinking Inside the Ball: Near-Optimal Minimization of the Maximal Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the complexity of minimizing $\\max_{i\\in[N]} f_i(x)$ for\nconvex, Lipschitz functions $f_1,\\ldots, f_N$. For non-smooth functions,\nexisting methods require $O(N\\epsilon^{-2})$ queries to a first-order oracle to\ncompute an $\\epsilon$-suboptimal point and $\\tilde{O}(N\\epsilon^{-1})$ queries\nif the $f_i$ are $O(1/\\epsilon)$-smooth. We develop methods with improved\ncomplexity bounds of $\\tilde{O}(N\\epsilon^{-2/3} + \\epsilon^{-8/3})$ in the\nnon-smooth case and $\\tilde{O}(N\\epsilon^{-2/3} + \\sqrt{N}\\epsilon^{-1})$ in\nthe $O(1/\\epsilon)$-smooth case. Our methods consist of a recently proposed\nball optimization oracle acceleration algorithm (which we refine) and a careful\nimplementation of said oracle for the softmax function. We also prove an oracle\ncomplexity lower bound scaling as $\\Omega(N\\epsilon^{-2/3})$, showing that our\ndependence on $N$ is optimal up to polylogarithmic factors.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:49:15 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Carmon", "Yair", ""], ["Jambulapati", "Arun", ""], ["Jin", "Yujia", ""], ["Sidford", "Aaron", ""]]}, {"id": "2105.01783", "submitter": "Chanwoo Lee", "authors": "Chanwoo Lee, Lexin Li, Hao Helen Zhang, and Miaoyan Wang", "title": "Nonparametric Trace Regression in High Dimensions via Sign Series\n  Representation", "comments": "66 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning of matrix-valued data has recently surged in a range of scientific\nand business applications. Trace regression is a widely used method to model\neffects of matrix predictors and has shown great success in matrix learning.\nHowever, nearly all existing trace regression solutions rely on two\nassumptions: (i) a known functional form of the conditional mean, and (ii) a\nglobal low-rank structure in the entire range of the regression function, both\nof which may be violated in practice. In this article, we relax these\nassumptions by developing a general framework for nonparametric trace\nregression models via structured sign series representations of high\ndimensional functions. The new model embraces both linear and nonlinear trace\neffects, and enjoys rank invariance to order-preserving transformations of the\nresponse. In the context of matrix completion, our framework leads to a\nsubstantially richer model based on what we coin as the \"sign rank\" of a\nmatrix. We show that the sign series can be statistically characterized by\nweighted classification tasks. Based on this connection, we propose a learning\nreduction approach to learn the regression model via a series of classifiers,\nand develop a parallelable computation algorithm to implement sign series\naggregations. We establish the excess risk bounds, estimation error rates, and\nsample complexities. Our proposal provides a broad nonparametric paradigm to\nmany important matrix learning problems, including matrix regression, matrix\ncompletion, multi-task learning, and compressed sensing. We demonstrate the\nadvantages of our method through simulations and two applications, one on brain\nconnectivity study and the other on high-rank image completion.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 22:20:00 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Lee", "Chanwoo", ""], ["Li", "Lexin", ""], ["Zhang", "Hao Helen", ""], ["Wang", "Miaoyan", ""]]}, {"id": "2105.01811", "submitter": "Jayesh Gupta", "authors": "Kunal Menda, Jayesh K. Gupta, Zachary Manchester and Mykel J.\n  Kochenderfer", "title": "Training Structured Mechanical Models by Minimizing Discrete\n  Euler-Lagrange Residual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Model-based paradigms for decision-making and control are becoming ubiquitous\nin robotics. They rely on the ability to efficiently learn a model of the\nsystem from data. Structured Mechanical Models (SMMs) are a data-efficient\nblack-box parameterization of mechanical systems, typically fit to data by\nminimizing the error between predicted and observed accelerations or next\nstates. In this work, we propose a methodology for fitting SMMs to data by\nminimizing the discrete Euler-Lagrange residual. To study our methodology, we\nfit models to joint-angle time-series from undamped and damped\ndouble-pendulums, studying the quality of learned models fit to data with and\nwithout observation noise. Experiments show that our methodology learns models\nthat are better in accuracy to those of the conventional schemes for fitting\nSMMs. We identify use cases in which our method is a more appropriate\nmethodology. Source code for reproducing the experiments is available at\nhttps://github.com/sisl/delsmm.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 00:44:01 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Menda", "Kunal", ""], ["Gupta", "Jayesh K.", ""], ["Manchester", "Zachary", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2105.01816", "submitter": "Zichen Li", "authors": "Yuchen Ding, Zichen Li, David Yastremsky", "title": "Real-time Face Mask Detection in Video Data", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response to the ongoing COVID-19 pandemic, we present a robust deep\nlearning pipeline that is capable of identifying correct and incorrect\nmask-wearing from real-time video streams. To accomplish this goal, we devised\ntwo separate approaches and evaluated their performance and run-time\nefficiency. The first approach leverages a pre-trained face detector in\ncombination with a mask-wearing image classifier trained on a large-scale\nsynthetic dataset. The second approach utilizes a state-of-the-art object\ndetection network to perform localization and classification of faces in one\nshot, fine-tuned on a small set of labeled real-world images. The first\npipeline achieved a test accuracy of 99.97% on the synthetic dataset and\nmaintained 6 FPS running on video data. The second pipeline achieved a mAP(0.5)\nof 89% on real-world images while sustaining 52 FPS on video data. We have\nconcluded that if a larger dataset with bounding-box labels can be curated,\nthis task is best suited using object detection architectures such as YOLO and\nSSD due to their superior inference speed and satisfactory performance on key\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 01:03:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ding", "Yuchen", ""], ["Li", "Zichen", ""], ["Yastremsky", "David", ""]]}, {"id": "2105.01820", "submitter": "Qi Dai", "authors": "Qi Dai, Di Shen, Jinhong Wang, Suzhou Huang and Dimitar Filev", "title": "Calibration of Human Driving Behavior and Preference Using Naturalistic\n  Traffic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding human driving behaviors quantitatively is critical even in the\nera when connected and autonomous vehicles and smart infrastructure are\nbecoming ever more prevalent. This is particularly so as that mixed traffic\nsettings, where autonomous vehicles and human driven vehicles co-exist, are\nexpected to persist for quite some time. Towards this end it is necessary that\nwe have a comprehensive modeling framework for decision-making within which\nhuman driving preferences can be inferred statistically from observed driving\nbehaviors in realistic and naturalistic traffic settings. Leveraging a recently\nproposed computational framework for smart vehicles in a smart world using\nmulti-agent based simulation and optimization, we first recapitulate how the\nforward problem of driving decision-making is modeled as a state space model.\nWe then show how the model can be inverted to estimate driver preferences from\nnaturalistic traffic data using the standard Kalman filter technique. We\nexplicitly illustrate our approach using the vehicle trajectory data from\nSugiyama experiment that was originally meant to demonstrate how stop-and-go\nshockwave can arise spontaneously without bottlenecks. Not only the estimated\nstate filter can fit the observed data well for each individual vehicle, the\ninferred utility functions can also re-produce quantitatively similar pattern\nof the observed collective behaviors. One distinct advantage of our approach is\nthe drastically reduced computational burden. This is possible because our\nforward model treats driving decision process, which is intrinsically dynamic\nwith multi-agent interactions, as a sequence of independent static optimization\nproblems contingent on the state with a finite look ahead anticipation.\nConsequently we can practically sidestep solving an interacting dynamic\ninversion problem that would have been much more computationally demanding.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 01:20:03 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Dai", "Qi", ""], ["Shen", "Di", ""], ["Wang", "Jinhong", ""], ["Huang", "Suzhou", ""], ["Filev", "Dimitar", ""]]}, {"id": "2105.01834", "submitter": "Du Nguyen", "authors": "Du Nguyen", "title": "Curvatures of Stiefel manifolds with deformation metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.CV cs.LG cs.SY eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute curvatures of a family of tractable metrics on Stiefel manifolds,\nintroduced recently by H{\\\"u}per, Markina and Silva Leite, which includes the\nwell-known embedded and canonical metrics on Stiefel manifolds as special\ncases. The metrics could be identified with the Cheeger deformation metrics. We\nidentify parameter values in the family to make a Stiefel manifold an Einstein\nmanifold and show Stiefel manifolds always carry an Einstein metric. We analyze\nthe sectional curvature range and identify the parameter range where the\nmanifold has non-negative sectional curvature. We provide the exact sectional\ncurvature range when the number of columns in a Stiefel matrix is $2$, and a\nconjectural range for other cases. We derive the formulas from two approaches,\none from a global curvature formula derived in our recent work, another using\ncurvature formulas for left-invariant metrics. The second approach leads to\ncurvature formulas for Cheeger deformation metrics on normal homogeneous\nspaces.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:13:38 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Nguyen", "Du", ""]]}, {"id": "2105.01838", "submitter": "Jian Cheng Wong", "authors": "Jian Cheng Wong, Chinchun Ooi, Pao-Hsiung Chiu, My Ha Dao", "title": "Improved Surrogate Modeling of Fluid Dynamics with Physics-Informed\n  Neural Networks", "comments": "No comment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph physics.flu-dyn", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physics-Informed Neural Networks (PINNs) have recently shown great promise as\na way of incorporating physics-based domain knowledge, including fundamental\ngoverning equations, into neural network models for many complex engineering\nsystems. They have been particularly effective in the area of inverse problems,\nwhere boundary conditions may be ill-defined, and data-absent scenarios, where\ntypical supervised learning approaches will fail. Here, we further explore the\nuse of this modeling methodology to surrogate modeling of a fluid dynamical\nsystem, and demonstrate additional undiscussed and interesting advantages of\nsuch a modeling methodology over conventional data-driven approaches: 1)\nimproving the model's predictive performance even with incomplete description\nof the underlying physics; 2) improving the robustness of the model to noise in\nthe dataset; 3) reduced effort to convergence during optimization for a new,\npreviously unseen scenario by transfer optimization of a pre-existing model.\nHence, we noticed the inclusion of a physics-based regularization term can\nsubstantially improve the equivalent data-driven surrogate model in many\nsubstantive ways, including an order of magnitude improvement in test error\nwhen the dataset is very noisy, and a 2-3x improvement when only partial\nphysics is included. In addition, we propose a novel transfer optimization\nscheme for use in such surrogate modeling scenarios and demonstrate an\napproximately 3x improvement in speed to convergence and an order of magnitude\nimprovement in predictive performance over conventional Xavier initialization\nfor training of new scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:23:20 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wong", "Jian Cheng", ""], ["Ooi", "Chinchun", ""], ["Chiu", "Pao-Hsiung", ""], ["Dao", "My Ha", ""]]}, {"id": "2105.01840", "submitter": "Chi-Shiang Wang", "authors": "Chi-Shiang Wang, Fang-Yi Su, Tsung-Lu Michael Lee, Yi-Shan Tsai,\n  Jung-Hsien Chiang", "title": "CUAB: Convolutional Uncertainty Attention Block Enhanced the Chest X-ray\n  Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, convolutional neural networks (CNNs) have been successfully\nimplemented to various image recognition applications, such as medical image\nanalysis, object detection, and image segmentation. Many studies and\napplications have been working on improving the performance of CNN algorithms\nand models. The strategies that aim to improve the performance of CNNs can be\ngrouped into three major approaches: (1) deeper and wider network architecture,\n(2) automatic architecture search, and (3) convolutional attention block.\nUnlike approaches (1) and (2), the convolutional attention block approach is\nmore flexible with lower cost. It enhances the CNN performance by extracting\nmore efficient features. However, the existing attention blocks focus on\nenhancing the significant features, which lose some potential features in the\nuncertainty information. Inspired by the test time augmentation and test-time\ndropout approaches, we developed a novel convolutional uncertainty attention\nblock (CUAB) that can leverage the uncertainty information to improve CNN-based\nmodels. The proposed module discovers potential information from the uncertain\nregions on feature maps in computer vision tasks. It is a flexible functional\nattention block that can be applied to any position in the convolutional block\nin CNN models. We evaluated the CUAB with notable backbone models, ResNet and\nResNeXt, on a medical image segmentation task. The CUAB achieved a dice score\nof 73% and 84% in pneumonia and pneumothorax segmentation, respectively,\nthereby outperforming the original model and other notable attention\napproaches. The results demonstrated that the CUAB can efficiently utilize the\nuncertainty information to improve the model performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:28:04 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wang", "Chi-Shiang", ""], ["Su", "Fang-Yi", ""], ["Lee", "Tsung-Lu Michael", ""], ["Tsai", "Yi-Shan", ""], ["Chiang", "Jung-Hsien", ""]]}, {"id": "2105.01844", "submitter": "Mohamed Elmahdy", "authors": "Mohamed S. Elmahdy, Laurens Beljaards, Sahar Yousefi, Hessam Sokooti,\n  Fons Verbeek, U. A. van der Heide, and Marius Staring", "title": "Joint Registration and Segmentation via Multi-Task Learning for Adaptive\n  Radiotherapy of Prostate Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image registration and segmentation are two of the most frequent\ntasks in medical image analysis. As these tasks are complementary and\ncorrelated, it would be beneficial to apply them simultaneously in a joint\nmanner. In this paper, we formulate registration and segmentation as a joint\nproblem via a Multi-Task Learning (MTL) setting, allowing these tasks to\nleverage their strengths and mitigate their weaknesses through the sharing of\nbeneficial information. We propose to merge these tasks not only on the loss\nlevel, but on the architectural level as well. We studied this approach in the\ncontext of adaptive image-guided radiotherapy for prostate cancer, where\nplanning and follow-up CT images as well as their corresponding contours are\navailable for training. The study involves two datasets from different\nmanufacturers and institutes. The first dataset was divided into training (12\npatients) and validation (6 patients), and was used to optimize and validate\nthe methodology, while the second dataset (14 patients) was used as an\nindependent test set. We carried out an extensive quantitative comparison\nbetween the quality of the automatically generated contours from different\nnetwork architectures as well as loss weighting methods. Moreover, we evaluated\nthe quality of the generated deformation vector field (DVF). We show that MTL\nalgorithms outperform their Single-Task Learning (STL) counterparts and achieve\nbetter generalization on the independent test set. The best algorithm achieved\na mean surface distance of $1.06 \\pm 0.3$ mm, $1.27 \\pm 0.4$ mm, $0.91 \\pm 0.4$\nmm, and $1.76 \\pm 0.8$ mm on the validation set for the prostate, seminal\nvesicles, bladder, and rectum, respectively. The high accuracy of the proposed\nmethod combined with the fast inference speed, makes it a promising method for\nautomatic re-contouring of follow-up scans for adaptive radiotherapy.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:45:49 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Elmahdy", "Mohamed S.", ""], ["Beljaards", "Laurens", ""], ["Yousefi", "Sahar", ""], ["Sokooti", "Hessam", ""], ["Verbeek", "Fons", ""], ["van der Heide", "U. A.", ""], ["Staring", "Marius", ""]]}, {"id": "2105.01850", "submitter": "Kush Bhatia", "authors": "Kush Bhatia, Ashwin Pananjady, Peter L. Bartlett, Anca D. Dragan,\n  Martin J. Wainwright", "title": "Preference learning along multiple criteria: A game-theoretic\n  perspective", "comments": "47 pages; published as a conference paper at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on ranking from ordinal data is vast, and there are several\nways to aggregate overall preferences from pairwise comparisons between\nobjects. In particular, it is well known that any Nash equilibrium of the zero\nsum game induced by the preference matrix defines a natural solution concept\n(winning distribution over objects) known as a von Neumann winner. Many\nreal-world problems, however, are inevitably multi-criteria, with different\npairwise preferences governing the different criteria. In this work, we\ngeneralize the notion of a von Neumann winner to the multi-criteria setting by\ntaking inspiration from Blackwell's approachability. Our framework allows for\nnon-linear aggregation of preferences across criteria, and generalizes the\nlinearization-based approach from multi-objective optimization.\n  From a theoretical standpoint, we show that the Blackwell winner of a\nmulti-criteria problem instance can be computed as the solution to a convex\noptimization problem. Furthermore, given random samples of pairwise\ncomparisons, we show that a simple plug-in estimator achieves near-optimal\nminimax sample complexity. Finally, we showcase the practical utility of our\nframework in a user study on autonomous driving, where we find that the\nBlackwell winner outperforms the von Neumann winner for the overall\npreferences.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 03:23:11 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bhatia", "Kush", ""], ["Pananjady", "Ashwin", ""], ["Bartlett", "Peter L.", ""], ["Dragan", "Anca D.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "2105.01853", "submitter": "An Liu Dr", "authors": "An Liu, Rui Yang, Tony Q. S. Quek and Min-Jian Zhao", "title": "Two-Stage Stochastic Optimization via Primal-Dual Decomposition and Deep\n  Unrolling", "comments": "16 pages, 8 figures, accepted by IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2021.3079807", "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a two-stage stochastic optimization problem, in which a long-term\noptimization variable is coupled with a set of short-term optimization\nvariables in both objective and constraint functions. Despite that two-stage\nstochastic optimization plays a critical role in various engineering and\nscientific applications, there still lack efficient algorithms, especially when\nthe long-term and short-term variables are coupled in the constraints. To\novercome the challenge caused by tightly coupled stochastic constraints, we\nfirst establish a two-stage primal-dual decomposition (PDD) method to decompose\nthe two-stage problem into a long-term problem and a family of short-term\nsubproblems. Then we propose a PDD-based stochastic successive convex\napproximation (PDD-SSCA) algorithmic framework to find KKT solutions for\ntwo-stage stochastic optimization problems. At each iteration, PDD-SSCA first\nruns a short-term sub-algorithm to find stationary points of the short-term\nsubproblems associated with a mini-batch of the state samples. Then it\nconstructs a convex surrogate for the long-term problem based on the deep\nunrolling of the short-term sub-algorithm and the back propagation method.\nFinally, the optimal solution of the convex surrogate problem is solved to\ngenerate the next iterate. We establish the almost sure convergence of PDD-SSCA\nand customize the algorithmic framework to solve two important application\nproblems. Simulations show that PDD-SSCA can achieve superior performance over\nexisting solutions.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 03:36:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "An", ""], ["Yang", "Rui", ""], ["Quek", "Tony Q. S.", ""], ["Zhao", "Min-Jian", ""]]}, {"id": "2105.01867", "submitter": "Devansh Bisla", "authors": "Devansh Bisla, Apoorva Nandini Saridena, Anna Choromanska", "title": "A Theoretical-Empirical Approach to Estimating Sample Complexity of DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper focuses on understanding how the generalization error scales with\nthe amount of the training data for deep neural networks (DNNs). Existing\ntechniques in statistical learning require computation of capacity measures,\nsuch as VC dimension, to provably bound this error. It is however unclear how\nto extend these measures to DNNs and therefore the existing analyses are\napplicable to simple neural networks, which are not used in practice, e.g.,\nlinear or shallow ones or otherwise multi-layer perceptrons. Moreover, many\ntheoretical error bounds are not empirically verifiable. We derive estimates of\nthe generalization error that hold for deep networks and do not rely on\nunattainable capacity measures. The enabling technique in our approach hinges\non two major assumptions: i) the network achieves zero training error, ii) the\nprobability of making an error on a test point is proportional to the distance\nbetween this point and its nearest training point in the feature space and at a\ncertain maximal distance (that we call radius) it saturates. Based on these\nassumptions we estimate the generalization error of DNNs. The obtained estimate\nscales as O(1/(\\delta N^{1/d})), where N is the size of the training data and\nis parameterized by two quantities, the effective dimensionality of the data as\nperceived by the network (d) and the aforementioned radius (\\delta), both of\nwhich we find empirically. We show that our estimates match with the\nexperimentally obtained behavior of the error on multiple learning tasks using\nbenchmark data-sets and realistic models. Estimating training data requirements\nis essential for deployment of safety critical applications such as autonomous\ndriving etc. Furthermore, collecting and annotating training data requires a\nhuge amount of financial, computational and human resources. Our empirical\nestimates will help to efficiently allocate resources.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:14:08 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bisla", "Devansh", ""], ["Saridena", "Apoorva Nandini", ""], ["Choromanska", "Anna", ""]]}, {"id": "2105.01868", "submitter": "Se Jung Kwon", "authors": "Byeongwook Kim, Dongsoo Lee, Yeonju Ro, Yongkweon Jeon, Se Jung Kwon,\n  Baeseong Park, Daehwan Oh", "title": "Q-Rater: Non-Convex Optimization for Post-Training Uniform Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various post-training uniform quantization methods have usually been studied\nbased on convex optimization. As a result, most previous ones rely on the\nquantization error minimization and/or quadratic approximations. Such\napproaches are computationally efficient and reasonable when a large number of\nquantization bits are employed. When the number of quantization bits is\nrelatively low, however, non-convex optimization is unavoidable to improve\nmodel accuracy. In this paper, we propose a new post-training uniform\nquantization technique considering non-convexity. We empirically show that\nhyper-parameters for clipping and rounding of weights and activations can be\nexplored by monitoring task loss. Then, an optimally searched set of\nhyper-parameters is frozen to proceed to the next layer such that an\nincremental non-convex optimization is enabled for post-training quantization.\nThroughout extensive experimental results using various models, our proposed\ntechnique presents higher model accuracy, especially for a low-bit\nquantization.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:14:22 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Kim", "Byeongwook", ""], ["Lee", "Dongsoo", ""], ["Ro", "Yeonju", ""], ["Jeon", "Yongkweon", ""], ["Kwon", "Se Jung", ""], ["Park", "Baeseong", ""], ["Oh", "Daehwan", ""]]}, {"id": "2105.01869", "submitter": "Se Jung Kwon", "authors": "Baeseong Park, Se Jung Kwon, Dongsoo Lee, Daehwan Oh, Byeongwook Kim,\n  Yongkweon Jeon, Yeonju Ro", "title": "Sequential Encryption of Sparse Neural Networks Toward Optimum\n  Representation of Irregular Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though fine-grained pruning techniques achieve a high compression ratio,\nconventional sparsity representations (such as CSR) associated with irregular\nsparsity degrade parallelism significantly. Practical pruning methods, thus,\nusually lower pruning rates (by structured pruning) to improve parallelism. In\nthis paper, we study fixed-to-fixed (lossless) encryption\narchitecture/algorithm to support fine-grained pruning methods such that sparse\nneural networks can be stored in a highly regular structure. We first estimate\nthe maximum compression ratio of encryption-based compression using entropy.\nThen, as an effort to push the compression ratio to the theoretical maximum (by\nentropy), we propose a sequential fixed-to-fixed encryption scheme. We\ndemonstrate that our proposed compression scheme achieves almost the maximum\ncompression ratio for the Transformer and ResNet-50 pruned by various\nfine-grained pruning methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:14:50 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Park", "Baeseong", ""], ["Kwon", "Se Jung", ""], ["Lee", "Dongsoo", ""], ["Oh", "Daehwan", ""], ["Kim", "Byeongwook", ""], ["Jeon", "Yongkweon", ""], ["Ro", "Yeonju", ""]]}, {"id": "2105.01875", "submitter": "Se Jung Kwon", "authors": "Dongsoo Lee, Se Jung Kwon, Byeongwook Kim, Jeongin Yun, Baeseong Park,\n  Yongkweon Jeon", "title": "Modulating Regularization Frequency for Efficient Compression-Aware\n  Model Training", "comments": "arXiv admin note: text overlap with arXiv:1905.10145", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While model compression is increasingly important because of large neural\nnetwork size, compression-aware training is challenging as it needs\nsophisticated model modifications and longer training time.In this paper, we\nintroduce regularization frequency (i.e., how often compression is performed\nduring training) as a new regularization technique for a practical and\nefficient compression-aware training method. For various regularization\ntechniques, such as weight decay and dropout, optimizing the regularization\nstrength is crucial to improve generalization in Deep Neural Networks (DNNs).\nWhile model compression also demands the right amount of regularization, the\nregularization strength incurred by model compression has been controlled only\nby compression ratio. Throughout various experiments, we show that\nregularization frequency critically affects the regularization strength of\nmodel compression. Combining regularization frequency and compression ratio,\nthe amount of weight updates by model compression per mini-batch can be\noptimized to achieve the best model accuracy. Modulating regularization\nfrequency is implemented by occasional model compression while conventional\ncompression-aware training is usually performed for every mini-batch.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:44:15 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Lee", "Dongsoo", ""], ["Kwon", "Se Jung", ""], ["Kim", "Byeongwook", ""], ["Yun", "Jeongin", ""], ["Park", "Baeseong", ""], ["Jeon", "Yongkweon", ""]]}, {"id": "2105.01876", "submitter": "Chaoqi Yang", "authors": "Chaoqi Yang, Cao Xiao, Lucas Glass, Jimeng Sun", "title": "Change Matters: Medication Change Prediction with Recurrent Residual\n  Networks", "comments": "Accepted in IJCAI'21, this is the long version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning is revolutionizing predictive healthcare, including\nrecommending medications to patients with complex health conditions. Existing\napproaches focus on predicting all medications for the current visit, which\noften overlaps with medications from previous visits. A more clinically\nrelevant task is to identify medication changes.\n  In this paper, we propose a new recurrent residual network, named MICRON, for\nmedication change prediction. MICRON takes the changes in patient health\nrecords as input and learns to update a hidden medication vector and the\nmedication set recurrently with a reconstruction design. The medication vector\nis like the memory cell that encodes longitudinal information of medications.\nUnlike traditional methods that require the entire patient history for\nprediction, MICRON has a residual-based inference that allows for sequential\nupdating based only on new patient features (e.g., new diagnoses in the recent\nvisit) more efficiently.\n  We evaluated MICRON on real inpatient and outpatient datasets. MICRON\nachieves 3.5% and 7.8% relative improvements over the best baseline in F1\nscore, respectively. MICRON also requires fewer parameters, which significantly\nreduces the training time to 38.3s per epoch with 1.5x speed-up.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:51:30 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yang", "Chaoqi", ""], ["Xiao", "Cao", ""], ["Glass", "Lucas", ""], ["Sun", "Jimeng", ""]]}, {"id": "2105.01879", "submitter": "Rui Huang", "authors": "Rui Huang and Yixuan Li", "title": "MOS: Towards Scaling Out-of-distribution Detection for Large Semantic\n  Space", "comments": "Paper accepted as an oral presentation in CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting out-of-distribution (OOD) inputs is a central challenge for safely\ndeploying machine learning models in the real world. Existing solutions are\nmainly driven by small datasets, with low resolution and very few class labels\n(e.g., CIFAR). As a result, OOD detection for large-scale image classification\ntasks remains largely unexplored. In this paper, we bridge this critical gap by\nproposing a group-based OOD detection framework, along with a novel OOD scoring\nfunction termed MOS. Our key idea is to decompose the large semantic space into\nsmaller groups with similar concepts, which allows simplifying the decision\nboundaries between in- vs. out-of-distribution data for effective OOD\ndetection. Our method scales substantially better for high-dimensional class\nspace than previous approaches. We evaluate models trained on ImageNet against\nfour carefully curated OOD datasets, spanning diverse semantics. MOS\nestablishes state-of-the-art performance, reducing the average FPR95 by 14.33%\nwhile achieving 6x speedup in inference compared to the previous best method.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:58:29 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Huang", "Rui", ""], ["Li", "Yixuan", ""]]}, {"id": "2105.01883", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Xiangyu Zhang, Jungong Han, Guiguang Ding", "title": "RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for\n  Image Recognition", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RepMLP, a multi-layer-perceptron-style neural network building\nblock for image recognition, which is composed of a series of fully-connected\n(FC) layers. Compared to convolutional layers, FC layers are more efficient,\nbetter at modeling the long-range dependencies and positional patterns, but\nworse at capturing the local structures, hence usually less favored for image\nrecognition. We propose a structural re-parameterization technique that adds\nlocal prior into an FC to make it powerful for image recognition. Specifically,\nwe construct convolutional layers inside a RepMLP during training and merge\nthem into the FC for inference. On CIFAR, a simple pure-MLP model shows\nperformance very close to CNN. By inserting RepMLP in traditional CNN, we\nimprove ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and\n2.3% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight\nthat combining the global representational capacity and positional perception\nof FC with the local prior of convolution can improve the performance of neural\nnetwork with faster speed on both the tasks with translation invariance (e.g.,\nsemantic segmentation) and those with aligned images and positional patterns\n(e.g., face recognition). The code and models are available at\nhttps://github.com/DingXiaoH/RepMLP.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 06:17:40 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ding", "Xiaohan", ""], ["Zhang", "Xiangyu", ""], ["Han", "Jungong", ""], ["Ding", "Guiguang", ""]]}, {"id": "2105.01891", "submitter": "Pol Van Rijn", "authors": "Pol van Rijn, Silvan Mertes, Dominik Schiller, Peter M. C. Harrison,\n  Pauline Larrouy-Maestri, Elisabeth Andr\\'e, Nori Jacoby", "title": "Exploring emotional prototypes in a high dimensional TTS latent space", "comments": "Submitted to INTERSPEECH'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent TTS systems are able to generate prosodically varied and realistic\nspeech. However, it is unclear how this prosodic variation contributes to the\nperception of speakers' emotional states. Here we use the recent psychological\nparadigm 'Gibbs Sampling with People' to search the prosodic latent space in a\ntrained GST Tacotron model to explore prototypes of emotional prosody.\nParticipants are recruited online and collectively manipulate the latent space\nof the generative speech model in a sequentially adaptive way so that the\nstimulus presented to one group of participants is determined by the response\nof the previous groups. We demonstrate that (1) particular regions of the\nmodel's latent space are reliably associated with particular emotions, (2) the\nresulting emotional prototypes are well-recognized by a separate group of human\nraters, and (3) these emotional prototypes can be effectively transferred to\nnew sentences. Collectively, these experiments demonstrate a novel approach to\nthe understanding of emotional speech by providing a tool to explore the\nrelation between the latent space of generative models and human semantics.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 06:49:21 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["van Rijn", "Pol", ""], ["Mertes", "Silvan", ""], ["Schiller", "Dominik", ""], ["Harrison", "Peter M. C.", ""], ["Larrouy-Maestri", "Pauline", ""], ["Andr\u00e9", "Elisabeth", ""], ["Jacoby", "Nori", ""]]}, {"id": "2105.01898", "submitter": "Qijing Huang", "authors": "Qijing Huang, Minwoo Kang, Grace Dinh, Thomas Norell, Aravind Kalaiah,\n  James Demmel, John Wawrzynek, Yakun Sophia Shao", "title": "CoSA: Scheduling by Constrained Optimization for Spatial Accelerators", "comments": "in Proceedings of the International Symposium on Computer\n  Architecture (ISCA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Deep Neural Networks (DNNs) have led to active development\nof specialized DNN accelerators, many of which feature a large number of\nprocessing elements laid out spatially, together with a multi-level memory\nhierarchy and flexible interconnect. While DNN accelerators can take advantage\nof data reuse and achieve high peak throughput, they also expose a large number\nof runtime parameters to the programmers who need to explicitly manage how\ncomputation is scheduled both spatially and temporally. In fact, different\nscheduling choices can lead to wide variations in performance and efficiency,\nmotivating the need for a fast and efficient search strategy to navigate the\nvast scheduling space.\n  To address this challenge, we present CoSA, a constrained-optimization-based\napproach for scheduling DNN accelerators. As opposed to existing approaches\nthat either rely on designers' heuristics or iterative methods to navigate the\nsearch space, CoSA expresses scheduling decisions as a constrained-optimization\nproblem that can be deterministically solved using mathematical optimization\ntechniques. Specifically, CoSA leverages the regularities in DNN operators and\nhardware to formulate the DNN scheduling space into a mixed-integer programming\n(MIP) problem with algorithmic and architectural constraints, which can be\nsolved to automatically generate a highly efficient schedule in one shot. We\ndemonstrate that CoSA-generated schedules significantly outperform\nstate-of-the-art approaches by a geometric mean of up to 2.5x across a wide\nrange of DNN networks while improving the time-to-solution by 90x.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 07:17:25 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Huang", "Qijing", ""], ["Kang", "Minwoo", ""], ["Dinh", "Grace", ""], ["Norell", "Thomas", ""], ["Kalaiah", "Aravind", ""], ["Demmel", "James", ""], ["Wawrzynek", "John", ""], ["Shao", "Yakun Sophia", ""]]}, {"id": "2105.01899", "submitter": "Tsung Wei Tsai", "authors": "Tsung Wei Tsai, Chongxuan Li, Jun Zhu", "title": "MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering", "comments": "International Conference on Learning Representations (ICLR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Mixture of Contrastive Experts (MiCE), a unified probabilistic\nclustering framework that simultaneously exploits the discriminative\nrepresentations learned by contrastive learning and the semantic structures\ncaptured by a latent mixture model. Motivated by the mixture of experts, MiCE\nemploys a gating function to partition an unlabeled dataset into subsets\naccording to the latent semantics and multiple experts to discriminate distinct\nsubsets of instances assigned to them in a contrastive learning manner. To\nsolve the nontrivial inference and learning problems caused by the latent\nvariables, we further develop a scalable variant of the\nExpectation-Maximization (EM) algorithm for MiCE and provide proof of the\nconvergence. Empirically, we evaluate the clustering performance of MiCE on\nfour widely adopted natural image datasets. MiCE achieves significantly better\nresults than various previous methods and a strong contrastive learning\nbaseline.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 07:17:57 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tsai", "Tsung Wei", ""], ["Li", "Chongxuan", ""], ["Zhu", "Jun", ""]]}, {"id": "2105.01903", "submitter": "Hojjat Navidan", "authors": "Mohammad Nabati, Hojjat Navidan, Reza Shahbazian, Seyed Ali Ghorashi\n  and David Windridge", "title": "Using Synthetic Data to Enhance the Accuracy of Fingerprint-Based\n  Localization: A Deep Learning Approach", "comments": null, "journal-ref": "IEEE Sensors Letters (Volume: 4, Issue: 4, April 2020)", "doi": "10.1109/LSENS.2020.2971555", "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-centered data collection is typically costly and implicates issues of\nprivacy. Various solutions have been proposed in the literature to reduce this\ncost, such as crowdsourced data collection, or the use of semi-supervised\nalgorithms. However, semi-supervised algorithms require a source of unlabeled\ndata, and crowd-sourcing methods require numbers of active participants. An\nalternative passive data collection modality is fingerprint-based localization.\nSuch methods use received signal strength (RSS) or channel state information\n(CSI) in wireless sensor networks to localize users in indoor/outdoor\nenvironments. In this paper, we introduce a novel approach to reduce training\ndata collection costs in fingerprint-based localization by using synthetic\ndata. Generative adversarial networks (GANs) are used to learn the distribution\nof a limited sample of collected data and, following this, to produce synthetic\ndata that can be used to augment the real collected data in order to increase\noverall positioning accuracy. Experimental results on a benchmark dataset show\nthat by applying the proposed method and using a combination of 10% collected\ndata and 90% synthetic data, we can obtain essentially similar positioning\naccuracy to that which would be obtained by using the full set of collected\ndata. This means that by employing GAN-generated synthetic data, we can use 90%\nless real data, thereby reduce data-collection costs while achieving acceptable\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 07:36:01 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 07:57:37 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Nabati", "Mohammad", ""], ["Navidan", "Hojjat", ""], ["Shahbazian", "Reza", ""], ["Ghorashi", "Seyed Ali", ""], ["Windridge", "David", ""]]}, {"id": "2105.01904", "submitter": "Yaron Shoham", "authors": "Yaron Shoham, Gal Elidan", "title": "Solving Sokoban with forward-backward reinforcement learning", "comments": "To be published in SoCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite seminal advances in reinforcement learning in recent years, many\ndomains where the rewards are sparse, e.g. given only at task completion,\nremain quite challenging. In such cases, it can be beneficial to tackle the\ntask both from its beginning and end, and make the two ends meet. Existing\napproaches that do so, however, are not effective in the common scenario where\nthe strategy needed near the end goal is very different from the one that is\neffective earlier on.\n  In this work we propose a novel RL approach for such settings. In short, we\nfirst train a backward-looking agent with a simple relaxed goal, and then\naugment the state representation of the forward-looking agent with\nstraightforward hint features. This allows the learned forward agent to\nleverage information from backward plans, without mimicking their policy.\n  We demonstrate the efficacy of our approach on the challenging game of\nSokoban, where we substantially surpass learned solvers that generalize across\nlevels, and are competitive with SOTA performance of the best highly-crafted\nsystems. Impressively, we achieve these results while learning from a small\nnumber of practice levels and using simple RL techniques.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 07:37:57 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 15:15:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shoham", "Yaron", ""], ["Elidan", "Gal", ""]]}, {"id": "2105.01924", "submitter": "Jonas Wurst", "authors": "Jonas Wurst, Lakshman Balasubramanian, Michael Botsch and Wolfgang\n  Utschick", "title": "Novelty Detection and Analysis of Traffic Scenario Infrastructures in\n  the Latent Space of a Vision Transformer-Based Triplet Autoencoder", "comments": "Accepted for IEEE Intelligent Vehicles 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting unknown and untested scenarios is crucial for scenario-based\ntesting. Scenario-based testing is considered to be a possible approach to\nvalidate autonomous vehicles. A traffic scenario consists of multiple\ncomponents, with infrastructure being one of it. In this work, a method to\ndetect novel traffic scenarios based on their infrastructure images is\npresented. An autoencoder triplet network provides latent representations for\ninfrastructure images which are used for outlier detection. The triplet\ntraining of the network is based on the connectivity graphs of the\ninfrastructure. By using the proposed architecture, expert-knowledge is used to\nshape the latent space such that it incorporates a pre-defined similarity in\nthe neighborhood relationships of an autoencoder. An ablation study on the\narchitecture is highlighting the importance of the triplet autoencoder\ncombination. The best performing architecture is based on vision transformers,\na convolution-free attention-based network. The presented method outperforms\nother state-of-the-art outlier detection approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:24:03 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wurst", "Jonas", ""], ["Balasubramanian", "Lakshman", ""], ["Botsch", "Michael", ""], ["Utschick", "Wolfgang", ""]]}, {"id": "2105.01933", "submitter": "Stephan Bialonski", "authors": "Niklas Grieger, Justus T. C. Schwabedal, Stefanie Wendel, Yvonne\n  Ritze, Stephan Bialonski", "title": "Automated scoring of pre-REM sleep in mice with deep learning", "comments": "14 pages, 5 figures", "journal-ref": "S. Scientific Reports 11, 12245 (2021)", "doi": "10.1038/s41598-021-91286-0", "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable automation of the labor-intensive manual task of scoring animal\nsleep can facilitate the analysis of long-term sleep studies. In recent years,\ndeep-learning-based systems, which learn optimal features from the data,\nincreased scoring accuracies for the classical sleep stages of Wake, REM, and\nNon-REM. Meanwhile, it has been recognized that the statistics of transitional\nstages such as pre-REM, found between Non-REM and REM, may hold additional\ninsight into the physiology of sleep and are now under vivid investigation. We\npropose a classification system based on a simple neural network architecture\nthat scores the classical stages as well as pre-REM sleep in mice. When\nrestricted to the classical stages, the optimized network showed\nstate-of-the-art classification performance with an out-of-sample F1 score of\n0.95 in male C57BL/6J mice. When unrestricted, the network showed lower F1\nscores on pre-REM (0.5) compared to the classical stages. The result is\ncomparable to previous attempts to score transitional stages in other species\nsuch as transition sleep in rats or N1 sleep in humans. Nevertheless, we\nobserved that the sequence of predictions including pre-REM typically\ntransitioned from Non-REM to REM reflecting sleep dynamics observed by human\nscorers. Our findings provide further evidence for the difficulty of scoring\ntransitional sleep stages, likely because such stages of sleep are\nunder-represented in typical data sets or show large inter-scorer variability.\nWe further provide our source code and an online platform to run predictions\nwith our trained network.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:03:03 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 10:43:30 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Grieger", "Niklas", ""], ["Schwabedal", "Justus T. C.", ""], ["Wendel", "Stefanie", ""], ["Ritze", "Yvonne", ""], ["Bialonski", "Stephan", ""]]}, {"id": "2105.01937", "submitter": "Brian Gordon", "authors": "Brian Gordon, Sigal Raab, Guy Azov, Raja Giryes, Daniel Cohen-Or", "title": "FLEX: Parameter-free Multi-view 3D Human Motion Reconstruction", "comments": "Project page: https://briang13.github.io/FLEX/ Video:\n  https://youtu.be/nMMmfWxA3xI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing availability of video recordings made by multiple cameras has\noffered new means for mitigating occlusion and depth ambiguities in pose and\nmotion reconstruction methods. Yet, multi-view algorithms strongly depend on\ncamera parameters, in particular, the relative positions among the cameras.\nSuch dependency becomes a hurdle once shifting to dynamic capture in\nuncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), an\nend-to-end parameter-free multi-view model. FLEX is parameter-free in the sense\nthat it does not require any camera parameters, neither intrinsic nor\nextrinsic. Our key idea is that the 3D angles between skeletal parts, as well\nas bone lengths, are invariant to the camera position. Hence, learning 3D\nrotations and bone lengths rather than locations allows predicting common\nvalues for all camera views. Our network takes multiple video streams, learns\nfused deep features through a novel multi-view fusion layer, and reconstructs a\nsingle consistent skeleton with temporally coherent joint rotations. We\ndemonstrate quantitative and qualitative results on the Human3.6M and KTH\nMulti-view Football II datasets. We compare our model to state-of-the-art\nmethods that are not parameter-free and show that in the absence of camera\nparameters, we outperform them by a large margin while obtaining comparable\nresults when camera parameters are available. Code, trained models, video\ndemonstration, and additional materials will be available on our project page.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:08:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Gordon", "Brian", ""], ["Raab", "Sigal", ""], ["Azov", "Guy", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2105.01946", "submitter": "Vassilis Vassiliades", "authors": "Giorgos Demosthenous and Vassilis Vassiliades", "title": "Continual Learning on the Edge with TensorFlow Lite", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deploying sophisticated deep learning models on embedded devices with the\npurpose of solving real-world problems is a struggle using today's technology.\nPrivacy and data limitations, network connection issues, and the need for fast\nmodel adaptation are some of the challenges that constitute today's approaches\nunfit for many applications on the edge and make real-time on-device training a\nnecessity. Google is currently working on tackling these challenges by\nembedding an experimental transfer learning API to their TensorFlow Lite,\nmachine learning library. In this paper, we show that although transfer\nlearning is a good first step for on-device model training, it suffers from\ncatastrophic forgetting when faced with more realistic scenarios. We present\nthis issue by testing a simple transfer learning model on the CORe50 benchmark\nas well as by demonstrating its limitations directly on an Android application\nwe developed. In addition, we expand the TensorFlow Lite library to include\ncontinual learning capabilities, by integrating a simple replay approach into\nthe head of the current transfer learning model. We test our continual learning\nmodel on the CORe50 benchmark to show that it tackles catastrophic forgetting,\nand we demonstrate its ability to continually learn, even under non-ideal\nconditions, using the application we developed. Finally, we open-source the\ncode of our Android application to enable developers to integrate continual\nlearning to their own smartphone applications, as well as to facilitate further\ndevelopment of continual learning functionality into the TensorFlow Lite\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:32:06 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Demosthenous", "Giorgos", ""], ["Vassiliades", "Vassilis", ""]]}, {"id": "2105.01957", "submitter": "Dmitry Nikulin", "authors": "Dmitry Nikulin, Roman Suvorov, Aleksei Ivakhnenko, Victor Lempitsky", "title": "Perceptual Gradient Networks", "comments": "28 pages, 15 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of deep learning for image generation use perceptual losses\nfor either training or fine-tuning of the generator networks. The use of\nperceptual loss however incurs repeated forward-backward passes in a large\nimage classification network as well as a considerable memory overhead required\nto store the activations of this network. It is therefore desirable or\nsometimes even critical to get rid of these overheads.\n  In this work, we propose a way to train generator networks using\napproximations of perceptual loss that are computed without forward-backward\npasses. Instead, we use a simpler perceptual gradient network that directly\nsynthesizes the gradient field of a perceptual loss. We introduce the concept\nof proxy targets, which stabilize the predicted gradient, meaning that learning\nwith it does not lead to divergence or oscillations. In addition, our method\nallows interpretation of the predicted gradient, providing insight into the\ninternals of perceptual loss and suggesting potential ways to improve it in\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:58:22 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Nikulin", "Dmitry", ""], ["Suvorov", "Roman", ""], ["Ivakhnenko", "Aleksei", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2105.01959", "submitter": "Matthew Watson", "authors": "Matthew Watson (1) and Noura Al Moubayed (1) ((1) Durham University,\n  Durham, UK)", "title": "Attack-agnostic Adversarial Detection on Medical Data Using Explainable\n  Machine Learning", "comments": "13 pages, 6 figures, accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Explainable machine learning has become increasingly prevalent, especially in\nhealthcare where explainable models are vital for ethical and trusted automated\ndecision making. Work on the susceptibility of deep learning models to\nadversarial attacks has shown the ease of designing samples to mislead a model\ninto making incorrect predictions. In this work, we propose a model agnostic\nexplainability-based method for the accurate detection of adversarial samples\non two datasets with different complexity and properties: Electronic Health\nRecord (EHR) and chest X-ray (CXR) data. On the MIMIC-III and Henan-Renmin EHR\ndatasets, we report a detection accuracy of 77% against the Longitudinal\nAdversarial Attack. On the MIMIC-CXR dataset, we achieve an accuracy of 88%;\nsignificantly improving on the state of the art of adversarial detection in\nboth datasets by over 10% in all settings. We propose an anomaly detection\nbased method using explainability techniques to detect adversarial samples\nwhich is able to generalise to different attack methods without a need for\nretraining.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 10:01:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Watson", "Matthew", ""], ["Moubayed", "Noura Al", ""]]}, {"id": "2105.01978", "submitter": "Huma Samin", "authors": "Huma Samin (1), Luis H. Garcia Paucar (1), Nelly Bencomo (1), Cesar M.\n  Carranza Hurtado (2), Erik M. Fredericks (3) ((1) SEA, Aston University,\n  Birmingham, UK, (2) Universidad Pontificia Cat\\'olica del Per\\'u, Lima,\n  Per\\'u, (3) Grand Valley State University, Michigan, USA)", "title": "RDMSim: An Exemplar for Evaluation and Comparison of Decision-Making\n  Techniques for Self-Adaptation", "comments": "Accepted at the 16th International Symposium on Software Engineering\n  for Adaptive and Self-Managing Systems (SEAMS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-making for self-adaptation approaches need to address different\nchallenges, including the quantification of the uncertainty of events that\ncannot be foreseen in advance and their effects, and dealing with conflicting\nobjectives that inherently involve multi-objective decision making (e.g.,\navoiding costs vs. providing reliable service). To enable researchers to\nevaluate and compare decision-making techniques for self-adaptation, we present\nthe RDMSim exemplar. RDMSim enables researchers to evaluate and compare\ntechniques for decision-making under environmental uncertainty that support\nself-adaptation. The focus of the exemplar is on the domain problem related to\nRemote Data Mirroring, which gives opportunity to face the challenges described\nabove. RDMSim provides probe and effector components for easy integration with\nexternal adaptation managers, which are associated with decision-making\ntechniques and based on the MAPE-K loop. Specifically, the paper presents (i)\nRDMSim, a simulator for real-world experimentation, (ii) a set of realistic\nsimulation scenarios that can be used for experimentation and comparison\npurposes, (iii) data for the sake of comparison.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 11:03:16 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Samin", "Huma", ""], ["Paucar", "Luis H. Garcia", ""], ["Bencomo", "Nelly", ""], ["Hurtado", "Cesar M. Carranza", ""], ["Fredericks", "Erik M.", ""]]}, {"id": "2105.01984", "submitter": "Silverio Mart\\'inez-Fern\\'andez", "authors": "Silverio Mart\\'inez-Fern\\'andez, Justus Bogner, Xavier Franch, Marc\n  Oriol, Julien Siebert, Adam Trendowicz, Anna Maria Vollmer, Stefan Wagner", "title": "Software Engineering for AI-Based Systems: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI-based systems are software systems with functionalities enabled by at\nleast one AI component (e.g., for image- and speech-recognition, and autonomous\ndriving). AI-based systems are becoming pervasive in society due to advances in\nAI. However, there is limited synthesized knowledge on Software Engineering\n(SE) approaches for building, operating, and maintaining AI-based systems. To\ncollect and analyze state-of-the-art knowledge about SE for AI-based systems,\nwe conducted a systematic mapping study. We considered 248 studies published\nbetween January 2010 and March 2020. SE for AI-based systems is an emerging\nresearch area, where more than 2/3 of the studies have been published since\n2018. The most studied properties of AI-based systems are dependability and\nsafety. We identified multiple SE approaches for AI-based systems, which we\nclassified according to the SWEBOK areas. Studies related to software testing\nand software quality are very prevalent, while areas like software maintenance\nseem neglected. Data-related issues are the most recurrent challenges. Our\nresults are valuable for: researchers, to quickly understand the state of the\nart and learn which topics need more research; practitioners, to learn about\nthe approaches and challenges that SE entails for AI-based systems; and,\neducators, to bridge the gap among SE and AI in their curricula.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 11:22:08 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mart\u00ednez-Fern\u00e1ndez", "Silverio", ""], ["Bogner", "Justus", ""], ["Franch", "Xavier", ""], ["Oriol", "Marc", ""], ["Siebert", "Julien", ""], ["Trendowicz", "Adam", ""], ["Vollmer", "Anna Maria", ""], ["Wagner", "Stefan", ""]]}, {"id": "2105.02027", "submitter": "Daniel Weber", "authors": "Daniel Weber and Clemens G\\\"uhmann", "title": "Non-Autoregressive vs Autoregressive Neural Networks for System\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of neural networks to non-linear dynamic system\nidentification tasks has a long history, which consists mostly of\nautoregressive approaches. Autoregression, the usage of the model outputs of\nprevious time steps, is a method of transferring a system state between time\nsteps, which is not necessary for modeling dynamic systems with modern neural\nnetwork structures, such as gated recurrent units (GRUs) and Temporal\nConvolutional Networks (TCNs). We compare the accuracy and execution\nperformance of autoregressive and non-autoregressive implementations of a GRU\nand TCN on the simulation task of three publicly available system\nidentification benchmarks. Our results show, that the non-autoregressive neural\nnetworks are significantly faster and at least as accurate as their\nautoregressive counterparts. Comparisons with other state-of-the-art black-box\nsystem identification methods show, that our implementation of the\nnon-autoregressive GRU is the best performing neural network-based system\nidentification method, and in the benchmarks without extrapolation, the best\nperforming black-box method.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:52:06 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Weber", "Daniel", ""], ["G\u00fchmann", "Clemens", ""]]}, {"id": "2105.02046", "submitter": "Yuan Zhou", "authors": "Yuan Zhou, Yanrong Guo, Shijie Hao, Richang Hong, Meng Wang", "title": "MCGNet: Partial Multi-view Few-shot Learning via Meta-alignment and\n  Context Gated-aggregation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new challenging task named as \\textbf{partial\nmulti-view few-shot learning}, which unifies two tasks, i.e. few-shot learning\nand partial multi-view learning, together. Different from the traditional\nfew-shot learning, this task aims to solve the few-shot learning problem given\nthe incomplete multi-view prior knowledge, which conforms more with the\nreal-world applications. However, this brings about two difficulties within\nthis task. First, the gaps among different views can be large and hard to\nreduce, especially with sample scarcity. Second, due to the incomplete view\ninformation, few-shot learning becomes more challenging than the traditional\none. To deal with the above issues, we propose a new \\textbf{Meta-alignment and\nContext Gated-aggregation Network} by equipping meta-alignment and context\ngated-aggregation with partial multi-view GNNs. Specifically, the\nmeta-alignment effectively maps the features from different views into a more\ncompact latent space, thereby reducing the view gaps. Moreover, the context\ngated-aggregation alleviates the view-missing influence by leveraging the\ncross-view context. Extensive experiments are conducted on the PIE and ORL\ndataset for evaluating our proposed method. By comparing with other few-shot\nlearning methods, our method obtains the state-of-the-art performance\nespecially with heavily-missing views.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:34:43 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhou", "Yuan", ""], ["Guo", "Yanrong", ""], ["Hao", "Shijie", ""], ["Hong", "Richang", ""], ["Wang", "Meng", ""]]}, {"id": "2105.02048", "submitter": "Ramin Ansari", "authors": "Ramin Ansari and Amirata Ghorbani", "title": "Accurate Prediction of Free Solvation Energy of Organic Molecules via\n  Graph Attention Network and Message Passing Neural Network from Pairwise\n  Atomistic Interactions", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG physics.comp-ph q-bio.BM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning based methods have been widely applied to predict various kinds\nof molecular properties in the pharmaceutical industry with increasingly more\nsuccess. Solvation free energy is an important index in the field of organic\nsynthesis, medicinal chemistry, drug delivery, and biological processes.\nHowever, accurate solvation free energy determination is a time-consuming\nexperimental process. Furthermore, it could be useful to assess solvation free\nenergy in the absence of a physical sample. In this study, we propose two novel\nmodels for the problem of free solvation energy predictions, based on the Graph\nNeural Network (GNN) architectures: Message Passing Neural Network (MPNN) and\nGraph Attention Network (GAT). GNNs are capable of summarizing the predictive\ninformation of a molecule as low-dimensional features directly from its graph\nstructure without relying on an extensive amount of intra-molecular\ndescriptors. As a result, these models are capable of making accurate\npredictions of the molecular properties without the time consuming process of\nrunning an experiment on each molecule. We show that our proposed models\noutperform all quantum mechanical and molecular dynamics methods in addition to\nexisting alternative machine learning based approaches in the task of solvation\nfree energy prediction. We believe such promising predictive models will be\napplicable to enhancing the efficiency of the screening of drug molecules and\nbe a useful tool to promote the development of molecular pharmaceutics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 22:15:18 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ansari", "Ramin", ""], ["Ghorbani", "Amirata", ""]]}, {"id": "2105.02062", "submitter": "Chengli Tan", "authors": "Chengli Tan, Jiangshe Zhang, and Junmin Liu", "title": "Understanding Short-Range Memory Effects in Deep Neural Networks", "comments": "15pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is of fundamental importance in deep\nlearning. Despite its simplicity, elucidating its efficacy remains challenging.\nConventionally, the success of SGD is ascribed to the stochastic gradient noise\n(SGN) incurred in the training process. Based on this consensus, SGD is\nfrequently treated and analyzed as the Euler-Maruyama discretization of\nstochastic differential equations (SDEs) driven by either Brownian or Levy\nstable motion. In this study, we argue that SGN is neither Gaussian nor Levy\nstable. Instead, inspired by the short-range correlation emerging in the SGN\nseries, we propose that SGD can be viewed as a discretization of an SDE driven\nby fractional Brownian motion (FBM). Accordingly, the different convergence\nbehavior of SGD dynamics is well-grounded. Moreover, the first passage time of\nan SDE driven by FBM is approximately derived. The result suggests a lower\nescaping rate for a larger Hurst parameter, and thus SGD stays longer in flat\nminima. This happens to coincide with the well-known phenomenon that SGD favors\nflat minima that generalize well. Extensive experiments are conducted to\nvalidate our conjecture, and it is demonstrated that short-range memory effects\npersist across various model architectures, datasets, and training strategies.\nOur study opens up a new perspective and may contribute to a better\nunderstanding of SGD.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:54:26 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 02:49:44 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 03:23:42 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 08:50:25 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Tan", "Chengli", ""], ["Zhang", "Jiangshe", ""], ["Liu", "Junmin", ""]]}, {"id": "2105.02085", "submitter": "Yuyang Gao", "authors": "Yuyang Gao, Giorgio A. Ascoli, Liang Zhao", "title": "Schematic Memory Persistence and Transience for Efficient and Robust\n  Continual Learning", "comments": "Under review at Neural Networks - Special Issue on AI and Brain\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning is considered a promising step towards next-generation\nArtificial Intelligence (AI), where deep neural networks (DNNs) make decisions\nby continuously learning a sequence of different tasks akin to human learning\nprocesses. It is still quite primitive, with existing works focusing primarily\non avoiding (catastrophic) forgetting. However, since forgetting is inevitable\ngiven bounded memory and unbounded task loads, 'how to reasonably forget' is a\nproblem continual learning must address in order to reduce the performance gap\nbetween AIs and humans, in terms of 1) memory efficiency, 2) generalizability,\nand 3) robustness when dealing with noisy data. To address this, we propose a\nnovel ScheMAtic memory peRsistence and Transience (SMART) framework for\ncontinual learning with external memory that builds on recent advances in\nneuroscience. The efficiency and generalizability are enhanced by a novel\nlong-term forgetting mechanism and schematic memory, using sparsity and\n'backward positive transfer' constraints with theoretical guarantees on the\nerror bound. Robust enhancement is achieved using a novel short-term forgetting\nmechanism inspired by background information-gated learning. Finally, an\nextensive experimental analysis on both benchmark and real-world datasets\ndemonstrates the effectiveness and efficiency of our model.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:32:47 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Gao", "Yuyang", ""], ["Ascoli", "Giorgio A.", ""], ["Zhao", "Liang", ""]]}, {"id": "2105.02091", "submitter": "Avijit Ghosh", "authors": "Avijit Ghosh, Ritam Dutt, Christo Wilson", "title": "When Fair Ranking Meets Uncertain Inference", "comments": "Accepted as full paper at SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462850", "report-no": null, "categories": "cs.IR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing fair ranking systems, especially those designed to be\ndemographically fair, assume that accurate demographic information about\nindividuals is available to the ranking algorithm. In practice, however, this\nassumption may not hold -- in real-world contexts like ranking job applicants\nor credit seekers, social and legal barriers may prevent algorithm operators\nfrom collecting peoples' demographic information. In these cases, algorithm\noperators may attempt to infer peoples' demographics and then supply these\ninferences as inputs to the ranking algorithm.\n  In this study, we investigate how uncertainty and errors in demographic\ninference impact the fairness offered by fair ranking algorithms. Using\nsimulations and three case studies with real datasets, we show how demographic\ninferences drawn from real systems can lead to unfair rankings. Our results\nsuggest that developers should not use inferred demographic data as input to\nfair ranking algorithms, unless the inferences are extremely accurate.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:40:07 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ghosh", "Avijit", ""], ["Dutt", "Ritam", ""], ["Wilson", "Christo", ""]]}, {"id": "2105.02095", "submitter": "Yury Korolev", "authors": "Yury Korolev", "title": "Two-layer neural networks with values in a Banach space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.FA math.NA math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study two-layer neural networks whose domain and range are Banach spaces\nwith separable preduals. In addition, we assume that the image space is\nequipped with a partial order, i.e. it is a Riesz space. As the nonlinearity we\nchoose the lattice operation of taking the positive part; in case of $\\mathbb\nR^d$-valued neural networks this corresponds to the ReLU activation function.\nWe prove inverse and direct approximation theorems with Monte-Carlo rates,\nextending existing results for the finite-dimensional case. In the second part\nof the paper, we consider training such networks using a finite amount of noisy\nobservations from the regularisation theory viewpoint. We discuss regularity\nconditions known as source conditions and obtain convergence rates in a Bregman\ndistance in the regime when both the noise level goes to zero and the number of\nsamples goes to infinity at appropriate rates.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:54:24 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Korolev", "Yury", ""]]}, {"id": "2105.02096", "submitter": "Hakan Erdogan", "authors": "Soumi Maiti, Hakan Erdogan, Kevin Wilson, Scott Wisdom, Shinji\n  Watanabe and John R. Hershey", "title": "End-to-End Diarization for Variable Number of Speakers with Local-Global\n  Networks and Discriminative Speaker Embeddings", "comments": "5 pages, 2 figures, ICASSP 2021", "journal-ref": "ICASSP 2021, SPE-54.1", "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end deep network model that performs meeting diarization\nfrom single-channel audio recordings. End-to-end diarization models have the\nadvantage of handling speaker overlap and enabling straightforward handling of\ndiscriminative training, unlike traditional clustering-based diarization\nmethods. The proposed system is designed to handle meetings with unknown\nnumbers of speakers, using variable-number permutation-invariant cross-entropy\nbased loss functions. We introduce several components that appear to help with\ndiarization performance, including a local convolutional network followed by a\nglobal self-attention module, multi-task transfer learning using a speaker\nidentification component, and a sequential approach where the model is refined\nwith a second stage. These are trained and validated on simulated meeting data\nbased on LibriSpeech and LibriTTS datasets; final evaluations are done using\nLibriCSS, which consists of simulated meetings recorded using real acoustics\nvia loudspeaker playback. The proposed model performs better than previously\nproposed end-to-end diarization models on these data.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:55:29 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Maiti", "Soumi", ""], ["Erdogan", "Hakan", ""], ["Wilson", "Kevin", ""], ["Wisdom", "Scott", ""], ["Watanabe", "Shinji", ""], ["Hershey", "John R.", ""]]}, {"id": "2105.02103", "submitter": "Evgeny Smirnov", "authors": "Evgeny Smirnov, Nikita Garaev, Vasiliy Galyuk", "title": "Prototype Memory for Large-scale Face Representation Learning", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face representation learning using datasets with massive number of identities\nrequires appropriate training methods. Softmax-based approach, currently the\nstate-of-the-art in face recognition, in its usual \"full softmax\" form is not\nsuitable for datasets with millions of persons. Several methods, based on the\n\"sampled softmax\" approach, were proposed to remove this limitation. These\nmethods, however, have a set of disadvantages. One of them is a problem of\n\"prototype obsolescence\": classifier weights (prototypes) of the rarely sampled\nclasses, receive too scarce gradients and become outdated and detached from the\ncurrent encoder state, resulting in an incorrect training signals. This problem\nis especially serious in ultra-large-scale datasets. In this paper, we propose\na novel face representation learning model called Prototype Memory, which\nalleviates this problem and allows training on a dataset of any size. Prototype\nMemory consists of the limited-size memory module for storing recent class\nprototypes and employs a set of algorithms to update it in appropriate way. New\nclass prototypes are generated on the fly using exemplar embeddings in the\ncurrent mini-batch. These prototypes are enqueued to the memory and used in a\nrole of classifier weights for usual softmax classification-based training. To\nprevent obsolescence and keep the memory in close connection with encoder,\nprototypes are regularly refreshed, and oldest ones are dequeued and disposed.\nPrototype Memory is computationally efficient and independent of dataset size.\nIt can be used with various loss functions, hard example mining algorithms and\nencoder architectures. We prove the effectiveness of the proposed model by\nextensive experiments on popular face recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:08:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Smirnov", "Evgeny", ""], ["Garaev", "Nikita", ""], ["Galyuk", "Vasiliy", ""]]}, {"id": "2105.02132", "submitter": "Eduardo Fonseca", "authors": "Eduardo Fonseca, Aren Jansen, Daniel P. W. Ellis, Scott Wisdom, Marco\n  Tagliasacchi, John R. Hershey, Manoj Plakal, Shawn Hershey, R. Channing\n  Moore, Xavier Serra", "title": "Self-Supervised Learning from Automatically Separated Sound Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world sound scenes consist of time-varying collections of sound sources,\neach generating characteristic sound events that are mixed together in audio\nrecordings. The association of these constituent sound events with their\nmixture and each other is semantically constrained: the sound scene contains\nthe union of source classes and not all classes naturally co-occur. With this\nmotivation, this paper explores the use of unsupervised automatic sound\nseparation to decompose unlabeled sound scenes into multiple\nsemantically-linked views for use in self-supervised contrastive learning. We\nfind that learning to associate input mixtures with their automatically\nseparated outputs yields stronger representations than past approaches that use\nthe mixtures alone. Further, we discover that optimal source separation is not\nrequired for successful contrastive learning by demonstrating that a range of\nseparation system convergence states all lead to useful and often complementary\nexample transformations. Our best system incorporates these unsupervised\nseparation models into a single augmentation front-end and jointly optimizes\nsimilarity maximization and coincidence prediction objectives across the views.\nThe result is an unsupervised audio representation that rivals state-of-the-art\nalternatives on the established shallow AudioSet classification benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:37:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Fonseca", "Eduardo", ""], ["Jansen", "Aren", ""], ["Ellis", "Daniel P. W.", ""], ["Wisdom", "Scott", ""], ["Tagliasacchi", "Marco", ""], ["Hershey", "John R.", ""], ["Plakal", "Manoj", ""], ["Hershey", "Shawn", ""], ["Moore", "R. Channing", ""], ["Serra", "Xavier", ""]]}, {"id": "2105.02135", "submitter": "Denis Belomestny", "authors": "D. Belomestny, I. Levin, E. Moulines, A. Naumov, S. Samsonov, V.\n  Zorina", "title": "UVIP: Model-Free Approach to Evaluate Reinforcement Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Policy evaluation is an important instrument for the comparison of different\nalgorithms in Reinforcement Learning (RL). Yet even a precise knowledge of the\nvalue function $V^{\\pi}$ corresponding to a policy $\\pi$ does not provide\nreliable information on how far is the policy $\\pi$ from the optimal one. We\npresent a novel model-free upper value iteration procedure $({\\sf UVIP})$ that\nallows us to estimate the suboptimality gap $V^{\\star}(x) - V^{\\pi}(x)$ from\nabove and to construct confidence intervals for $V^\\star$. Our approach relies\non upper bounds to the solution of the Bellman optimality equation via\nmartingale approach. We provide theoretical guarantees for ${\\sf UVIP}$ under\ngeneral assumptions and illustrate its performance on a number of benchmark RL\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:38:36 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:47:30 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 17:47:26 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Belomestny", "D.", ""], ["Levin", "I.", ""], ["Moulines", "E.", ""], ["Naumov", "A.", ""], ["Samsonov", "S.", ""], ["Zorina", "V.", ""]]}, {"id": "2105.02138", "submitter": "Benjamin Rivi\\`ere", "authors": "Benjamin Rivi\\`ere and Soon-Jo Chung", "title": "H-TD2: Hybrid Temporal Difference Learning for Adaptive Urban Taxi\n  Dispatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present H-TD2: Hybrid Temporal Difference Learning for Taxi Dispatch, a\nmodel-free, adaptive decision-making algorithm to coordinate a large fleet of\nautomated taxis in a dynamic urban environment to minimize expected customer\nwaiting times. Our scalable algorithm exploits the natural transportation\nnetwork company topology by switching between two behaviors: distributed\ntemporal-difference learning computed locally at each taxi and infrequent\ncentralized Bellman updates computed at the dispatch center. We derive a regret\nbound and design the trigger condition between the two behaviors to explicitly\ncontrol the trade-off between computational complexity and the individual taxi\npolicy's bounded sub-optimality; this advances the state of the art by enabling\ndistributed operation with bounded-suboptimality. Additionally, unlike recent\nreinforcement learning dispatch methods, this policy estimation is adaptive and\nrobust to out-of-training domain events. This result is enabled by a two-step\nmodelling approach: the policy is learned on an agent-agnostic, cell-based\nMarkov Decision Process and individual taxis are coordinated using the learned\npolicy in a distributed game-theoretic task assignment. We validate our\nalgorithm against a receding horizon control baseline in a Gridworld\nenvironment with a simulated customer dataset, where the proposed solution\ndecreases average customer waiting time by 50% over a wide range of parameters.\nWe also validate in a Chicago city environment with real customer requests from\nthe Chicago taxi public dataset where the proposed solution decreases average\ncustomer waiting time by 26% over irregular customer distributions during a\n2016 Major League Baseball World Series game.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:42:31 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Rivi\u00e8re", "Benjamin", ""], ["Chung", "Soon-Jo", ""]]}, {"id": "2105.02216", "submitter": "Junhwa Hur", "authors": "Junhwa Hur, Stefan Roth", "title": "Self-Supervised Multi-Frame Monocular Scene Flow", "comments": "To appear at CVPR 2021. Code available:\n  https://github.com/visinf/multi-mono-sf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D scene flow from a sequence of monocular images has been gaining\nincreased attention due to the simple, economical capture setup. Owing to the\nsevere ill-posedness of the problem, the accuracy of current methods has been\nlimited, especially that of efficient, real-time approaches. In this paper, we\nintroduce a multi-frame monocular scene flow network based on self-supervised\nlearning, improving the accuracy over previous networks while retaining\nreal-time efficiency. Based on an advanced two-frame baseline with a\nsplit-decoder design, we propose (i) a multi-frame model using a triple frame\ninput and convolutional LSTM connections, (ii) an occlusion-aware census loss\nfor better accuracy, and (iii) a gradient detaching strategy to improve\ntraining stability. On the KITTI dataset, we observe state-of-the-art accuracy\namong monocular scene flow methods based on self-supervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:49:55 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Hur", "Junhwa", ""], ["Roth", "Stefan", ""]]}, {"id": "2105.02221", "submitter": "Kurtland Chua", "authors": "Kurtland Chua, Qi Lei, Jason D. Lee", "title": "How Fine-Tuning Allows for Effective Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning has been widely studied in the context of\nmeta-learning, enabling rapid learning of new tasks through shared\nrepresentations. Recent works such as MAML have explored using\nfine-tuning-based metrics, which measure the ease by which fine-tuning can\nachieve good performance, as proxies for obtaining representations. We present\na theoretical framework for analyzing representations derived from a MAML-like\nalgorithm, assuming the available tasks use approximately the same underlying\nrepresentation. We then provide risk bounds on the best predictor found by\nfine-tuning via gradient descent, demonstrating that the algorithm can provably\nleverage the shared structure. The upper bound applies to general function\nclasses, which we demonstrate by instantiating the guarantees of our framework\nin the logistic regression and neural network settings. In contrast, we\nestablish the existence of settings where any algorithm, using a representation\ntrained with no consideration for task-specific fine-tuning, performs as well\nas a learner with no access to source tasks in the worst case. This separation\nresult underscores the benefit of fine-tuning-based methods, such as MAML, over\nmethods with \"frozen representation\" objectives in few-shot learning.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:56:00 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Chua", "Kurtland", ""], ["Lei", "Qi", ""], ["Lee", "Jason D.", ""]]}, {"id": "2105.02266", "submitter": "Zhishuai Guo", "authors": "Zhishuai Guo, Quanqi Hu, Lijun Zhang, Tianbao Yang", "title": "Randomized Stochastic Variance-Reduced Methods for Multi-Task Stochastic\n  Bilevel Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider non-convex stochastic bilevel optimization (SBO)\nproblems that have many applications in machine learning. Although numerous\nstudies have proposed stochastic algorithms for solving these problems, they\nare limited in two perspectives: (i) their sample complexities are high, which\ndo not match the state-of-the-art result for non-convex stochastic\noptimization; (ii) their algorithms are tailored to problems with only one\nlower-level problem. When there are many lower-level problems, it could be\nprohibitive to process all these lower-level problems at each iteration. To\naddress these limitations, this paper proposes fast randomized stochastic\nalgorithms for non-convex SBO problems. First, we present a stochastic method\nfor non-convex SBO with only one lower problem and establish its sample\ncomplexity of $O(1/\\epsilon^3)$ for finding an $\\epsilon$-stationary point\nunder Lipschitz continuous conditions of stochastic oracles, matching the lower\nbound for stochastic smooth non-convex optimization. Second, we present a\nrandomized stochastic method for non-convex SBO with $m>1$ lower level problems\n(multi-task SBO) by processing a constant number of lower problems at each\niteration, and establish its sample complexity no worse than $O(m/\\epsilon^3)$,\nwhich could be a better complexity than that of simply processing all $m$ lower\nproblems at each iteration. Lastly, we establish even faster convergence\nresults for gradient-dominant functions. To the best of our knowledge, this is\nthe first work considering multi-task SBO and developing state-of-the-art\nsample complexity results.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:28:42 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 06:45:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Guo", "Zhishuai", ""], ["Hu", "Quanqi", ""], ["Zhang", "Lijun", ""], ["Yang", "Tianbao", ""]]}, {"id": "2105.02276", "submitter": "Thomas Hubregtsen", "authors": "Thomas Hubregtsen, David Wierichs, Elies Gil-Fuster, Peter-Jan H. S.\n  Derks, Paul K. Faehrmann, Johannes Jakob Meyer", "title": "Training Quantum Embedding Kernels on Near-Term Quantum Computers", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel methods are a cornerstone of classical machine learning. The idea of\nusing quantum computers to compute kernels has recently attracted attention.\nQuantum embedding kernels (QEKs) constructed by embedding data into the Hilbert\nspace of a quantum computer are a particular quantum kernel technique that\nallows to gather insights into learning problems and that are particularly\nsuitable for noisy intermediate-scale quantum devices. In this work, we first\nprovide an accessible introduction to quantum embedding kernels and then\nanalyze the practical issues arising when realizing them on a noisy near-term\nquantum computer. We focus on quantum embedding kernels with variational\nparameters. These variational parameters are optimized for a given dataset by\nincreasing the kernel-target alignment, a heuristic connected to the achievable\nclassification accuracy. We further show under which conditions noise from\ndevice imperfections influences the predicted kernel and provide a strategy to\nmitigate these detrimental effects which is tailored to quantum embedding\nkernels. We also address the influence of finite sampling and derive bounds\nthat put guarantees on the quality of the kernel matrix. We illustrate our\nfindings by numerical experiments and tests on actual hardware.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:41:13 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hubregtsen", "Thomas", ""], ["Wierichs", "David", ""], ["Gil-Fuster", "Elies", ""], ["Derks", "Peter-Jan H. S.", ""], ["Faehrmann", "Paul K.", ""], ["Meyer", "Johannes Jakob", ""]]}, {"id": "2105.02295", "submitter": "Hanieh Hashemi", "authors": "Hanieh Hashemi, Yongqin Wang, Chuan Guo, Murali Annavaram", "title": "Byzantine-Robust and Privacy-Preserving Framework for FedML", "comments": null, "journal-ref": "Security and Safety in Machine Learning Systems Workshop in ICLR\n  2021", "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as a popular paradigm for collaboratively\ntraining a model from data distributed among a set of clients. This learning\nsetting presents, among others, two unique challenges: how to protect privacy\nof the clients' data during training, and how to ensure integrity of the\ntrained model. We propose a two-pronged solution that aims to address both\nchallenges under a single framework. First, we propose to create secure\nenclaves using a trusted execution environment (TEE) within the server. Each\nclient can then encrypt their gradients and send them to verifiable enclaves.\nThe gradients are decrypted within the enclave without the fear of privacy\nbreaches. However, robustness check computations in a TEE are computationally\nprohibitive. Hence, in the second step, we perform a novel gradient encoding\nthat enables TEEs to encode the gradients and then offloading Byzantine check\ncomputations to accelerators such as GPUs. Our proposed approach provides\ntheoretical bounds on information leakage and offers a significant speed-up\nover the baseline in empirical evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 19:36:21 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hashemi", "Hanieh", ""], ["Wang", "Yongqin", ""], ["Guo", "Chuan", ""], ["Annavaram", "Murali", ""]]}, {"id": "2105.02315", "submitter": "Marco Serafini", "authors": "Marco Serafini, Hui Guan", "title": "Scalable Graph Neural Network Training: The Case for Sampling", "comments": "9 pages, 2 figures", "journal-ref": "ACM SIGOPS Operating Systems Review, Volume 55, Issue 1, July\n  2021, pp 68-76", "doi": "10.1145/3469379.3469387", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) are a new and increasingly popular family of\ndeep neural network architectures to perform learning on graphs. Training them\nefficiently is challenging due to the irregular nature of graph data. The\nproblem becomes even more challenging when scaling to large graphs that exceed\nthe capacity of single devices. Standard approaches to distributed DNN\ntraining, such as data and model parallelism, do not directly apply to GNNs.\nInstead, two different approaches have emerged in the literature: whole-graph\nand sample-based training.\n  In this paper, we review and compare the two approaches. Scalability is\nchallenging with both approaches, but we make a case that research should focus\non sample-based training since it is a more promising approach. Finally, we\nreview recent systems supporting sample-based training.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 20:44:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Serafini", "Marco", ""], ["Guan", "Hui", ""]]}, {"id": "2105.02318", "submitter": "Kishor Jothimurugan", "authors": "Kishor Jothimurugan, Matthew Andrews, Jeongran Lee and Lorenzo Maggi", "title": "Learning Algorithms for Regenerative Stopping Problems with Applications\n  to Shipping Consolidation in Logistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study regenerative stopping problems in which the system starts anew\nwhenever the controller decides to stop and the long-term average cost is to be\nminimized. Traditional model-based solutions involve estimating the underlying\nprocess from data and computing strategies for the estimated model. In this\npaper, we compare such solutions to deep reinforcement learning and imitation\nlearning which involve learning a neural network policy from simulations. We\nevaluate the different approaches on a real-world problem of shipping\nconsolidation in logistics and demonstrate that deep learning can be\neffectively used to solve such problems.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 20:45:46 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Jothimurugan", "Kishor", ""], ["Andrews", "Matthew", ""], ["Lee", "Jeongran", ""], ["Maggi", "Lorenzo", ""]]}, {"id": "2105.02337", "submitter": "Yijun Zuo", "authors": "Yijun Zuo", "title": "Non-asymptotic analysis and inference for an outlyingness induced\n  winsorized mean", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation of a mean vector, a topic regarded as obsolete in the\ntraditional robust statistics community, has recently surged in machine\nlearning literature in the last decade. The latest focus is on the sub-Gaussian\nperformance and computability of the estimators in a non-asymptotic setting.\nNumerous traditional robust estimators are computationally intractable, which\npartly contributes to the renewal of the interest in the robust mean\nestimation.\n  Robust centrality estimators, however, include the trimmed mean and the\nsample median. The latter has the best robustness but suffers a low-efficiency\ndrawback. Trimmed mean and median of means, %as robust alternatives to the\nsample mean, and achieving sub-Gaussian performance have been proposed and\nstudied in the literature.\n  This article investigates the robustness of leading sub-Gaussian estimators\nof mean and reveals that none of them can resist greater than $25\\%$\ncontamination in data and consequently introduces an outlyingness induced\nwinsorized mean which has the best possible robustness (can resist up to $50\\%$\ncontamination without breakdown) meanwhile achieving high efficiency.\nFurthermore, it has a sub-Gaussian performance for uncontaminated samples and a\nbounded estimation error for contaminated samples at a given confidence level\nin a finite sample setting. It can be computed in linear time.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 21:35:24 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zuo", "Yijun", ""]]}, {"id": "2105.02340", "submitter": "Bartosz Krawczyk", "authors": "Damien Dablain, Bartosz Krawczyk, Nitesh V. Chawla", "title": "DeepSMOTE: Fusing Deep Learning and SMOTE for Imbalanced Data", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite over two decades of progress, imbalanced data is still considered a\nsignificant challenge for contemporary machine learning models. Modern advances\nin deep learning have magnified the importance of the imbalanced data problem.\nThe two main approaches to address this issue are based on loss function\nmodifications and instance resampling. Instance sampling is typically based on\nGenerative Adversarial Networks (GANs), which may suffer from mode collapse.\nTherefore, there is a need for an oversampling method that is specifically\ntailored to deep learning models, can work on raw images while preserving their\nproperties, and is capable of generating high quality, artificial images that\ncan enhance minority classes and balance the training set. We propose DeepSMOTE\n- a novel oversampling algorithm for deep learning models. It is simple, yet\neffective in its design. It consists of three major components: (i) an\nencoder/decoder framework; (ii) SMOTE-based oversampling; and (iii) a dedicated\nloss function that is enhanced with a penalty term. An important advantage of\nDeepSMOTE over GAN-based oversampling is that DeepSMOTE does not require a\ndiscriminator, and it generates high-quality artificial images that are both\ninformation-rich and suitable for visual inspection. DeepSMOTE code is publicly\navailable at: https://github.com/dd1github/DeepSMOTE\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 21:49:37 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Dablain", "Damien", ""], ["Krawczyk", "Bartosz", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "2105.02343", "submitter": "V\\'it Musil", "authors": "Anselm Paulus and Michal Rol\\'inek and V\\'it Musil and Brandon Amos\n  and Georg Martius", "title": "CombOptNet: Fit the Right NP-Hard Problem by Learning Integer\n  Programming Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bridging logical and algorithmic reasoning with modern machine learning\ntechniques is a fundamental challenge with potentially transformative impact.\nOn the algorithmic side, many NP-hard problems can be expressed as integer\nprograms, in which the constraints play the role of their \"combinatorial\nspecification\". In this work, we aim to integrate integer programming solvers\ninto neural network architectures as layers capable of learning both the cost\nterms and the constraints. The resulting end-to-end trainable architectures\njointly extract features from raw data and solve a suitable (learned)\ncombinatorial problem with state-of-the-art integer programming solvers. We\ndemonstrate the potential of such layers with an extensive performance analysis\non synthetic data and with a demonstration on a competitive computer vision\nkeypoint matching benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 21:52:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Paulus", "Anselm", ""], ["Rol\u00ednek", "Michal", ""], ["Musil", "V\u00edt", ""], ["Amos", "Brandon", ""], ["Martius", "Georg", ""]]}, {"id": "2105.02344", "submitter": "Ruohan Zhan", "authors": "Ruohan Zhan, Zhimei Ren, Susan Athey, Zhengyuan Zhou", "title": "Policy Learning with Adaptively Collected Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning optimal policies from historical data enables the gains from\npersonalization to be realized in a wide variety of applications. The growing\npolicy learning literature focuses on a setting where the treatment assignment\npolicy does not adapt to the data. However, adaptive data collection is\nbecoming more common in practice, from two primary sources: 1) data collected\nfrom adaptive experiments that are designed to improve inferential efficiency;\n2) data collected from production systems that are adaptively evolving an\noperational policy to improve performance over time (e.g. contextual bandits).\nIn this paper, we aim to address the challenge of learning the optimal policy\nwith adaptively collected data and provide one of the first theoretical\ninquiries into this problem. We propose an algorithm based on generalized\naugmented inverse propensity weighted estimators and establish its\nfinite-sample regret bound. We complement this regret upper bound with a lower\nbound that characterizes the fundamental difficulty of policy learning with\nadaptive data. Finally, we demonstrate our algorithm's effectiveness using both\nsynthetic data and public benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:03:10 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhan", "Ruohan", ""], ["Ren", "Zhimei", ""], ["Athey", "Susan", ""], ["Zhou", "Zhengyuan", ""]]}, {"id": "2105.02357", "submitter": "Samanta Knapi\\v{c}", "authors": "Samanta Knapi\\v{c}, Avleen Malhi, Rohit Saluja, Kary Fr\\\"amling", "title": "Explainable Artificial Intelligence for Human Decision-Support System in\n  Medical Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the present paper we present the potential of Explainable Artificial\nIntelligence methods for decision-support in medical image analysis scenarios.\nWith three types of explainable methods applied to the same medical image data\nset our aim was to improve the comprehensibility of the decisions provided by\nthe Convolutional Neural Network (CNN). The visual explanations were provided\non in-vivo gastral images obtained from a Video capsule endoscopy (VCE), with\nthe goal of increasing the health professionals' trust in the black box\npredictions. We implemented two post-hoc interpretable machine learning methods\nLIME and SHAP and the alternative explanation approach CIU, centered on the\nContextual Value and Utility (CIU). The produced explanations were evaluated\nusing human evaluation. We conducted three user studies based on the\nexplanations provided by LIME, SHAP and CIU. Users from different non-medical\nbackgrounds carried out a series of tests in the web-based survey setting and\nstated their experience and understanding of the given explanations. Three user\ngroups (n=20, 20, 20) with three distinct forms of explanations were\nquantitatively analyzed. We have found that, as hypothesized, the CIU\nexplainable method performed better than both LIME and SHAP methods in terms of\nincreasing support for human decision-making as well as being more transparent\nand thus understandable to users. Additionally, CIU outperformed LIME and SHAP\nby generating explanations more rapidly. Our findings suggest that there are\nnotable differences in human decision-making between various explanation\nsupport settings. In line with that, we present three potential explainable\nmethods that can with future improvements in implementation be generalized on\ndifferent medical data sets and can provide great decision-support for medical\nexperts.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:29:28 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Knapi\u010d", "Samanta", ""], ["Malhi", "Avleen", ""], ["Saluja", "Rohit", ""], ["Fr\u00e4mling", "Kary", ""]]}, {"id": "2105.02359", "submitter": "Borja Bovcon", "authors": "Borja Bovcon, Jon Muhovi\\v{c}, Du\\v{s}ko Vranac, Dean Mozeti\\v{c},\n  Janez Per\\v{s}, Matej Kristan", "title": "MODS -- A USV-oriented object detection and obstacle segmentation\n  benchmark", "comments": "15 pages, 15 figures. The dataset, as well as the proposed evaluation\n  metrics, will be published on our website: www.vicos.si", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Small-sized unmanned surface vehicles (USV) are coastal water devices with a\nbroad range of applications such as environmental control and surveillance. A\ncrucial capability for autonomous operation is obstacle detection for timely\nreaction and collision avoidance, which has been recently explored in the\ncontext of camera-based visual scene interpretation. Owing to curated datasets,\nsubstantial advances in scene interpretation have been made in a related field\nof unmanned ground vehicles. However, the current maritime datasets do not\nadequately capture the complexity of real-world USV scenes and the evaluation\nprotocols are not standardised, which makes cross-paper comparison of different\nmethods difficult and hiders the progress. To address these issues, we\nintroduce a new obstacle detection benchmark MODS, which considers two major\nperception tasks: maritime object detection and the more general maritime\nobstacle segmentation. We present a new diverse maritime evaluation dataset\ncontaining approximately 81k stereo images synchronized with an on-board IMU,\nwith over 60k objects annotated. We propose a new obstacle segmentation\nperformance evaluation protocol that reflects the detection accuracy in a way\nmeaningful for practical USV navigation. Seventeen recent state-of-the-art\nobject detection and obstacle segmentation methods are evaluated using the\nproposed protocol, creating a benchmark to facilitate development of the field.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:40:27 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bovcon", "Borja", ""], ["Muhovi\u010d", "Jon", ""], ["Vranac", "Du\u0161ko", ""], ["Mozeti\u010d", "Dean", ""], ["Per\u0161", "Janez", ""], ["Kristan", "Matej", ""]]}, {"id": "2105.02368", "submitter": "Hao Sun", "authors": "Fangzheng Sun, Yang Liu, Hao Sun", "title": "Physics-informed Spline Learning for Nonlinear Dynamics Discovery", "comments": null, "journal-ref": "The 30th International Joint Conference on Artificial Intelligence\n  (IJCAI-2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical systems are typically governed by a set of linear/nonlinear\ndifferential equations. Distilling the analytical form of these equations from\nvery limited data remains intractable in many disciplines such as physics,\nbiology, climate science, engineering and social science. To address this\nfundamental challenge, we propose a novel Physics-informed Spline Learning\n(PiSL) framework to discover parsimonious governing equations for nonlinear\ndynamics, based on sparsely sampled noisy data. The key concept is to (1)\nleverage splines to interpolate locally the dynamics, perform analytical\ndifferentiation and build the library of candidate terms, (2) employ sparse\nrepresentation of the governing equations, and (3) use the physics residual in\nturn to inform the spline learning. The synergy between splines and discovered\nunderlying physics leads to the robust capacity of dealing with high-level data\nscarcity and noise. A hybrid sparsity-promoting alternating direction\noptimization strategy is developed for systematically pruning the sparse\ncoefficients that form the structure and explicit expression of the governing\nequations. The efficacy and superiority of the proposed method have been\ndemonstrated by multiple well-known nonlinear dynamical systems, in comparison\nwith two state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 23:32:43 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 00:41:11 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sun", "Fangzheng", ""], ["Liu", "Yang", ""], ["Sun", "Hao", ""]]}, {"id": "2105.02371", "submitter": "Arvin Tashakori", "authors": "Arvin Tashakori", "title": "Survey on Multi-Agent Q-Learning frameworks for resource management in\n  wireless sensor network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report aims to survey multi-agent Q-Learning algorithms, analyze\ndifferent game theory frameworks used, address each framework's applications,\nand report challenges and future directions. The target application for this\nstudy is resource management in the wireless sensor network.\n  In the first section, the author provided an introduction regarding the\napplications of wireless sensor networks. After that, the author presented a\nsummary of the Q-Learning algorithm, a well-known classic solution for\nmodel-free reinforcement learning problems.\n  In the third section, the author extended the Q-Learning algorithm for\nmulti-agent scenarios and discussed its challenges.\n  In the fourth section, the author surveyed sets of game-theoretic frameworks\nthat researchers used to address this problem for resource allocation and task\nscheduling in the wireless sensor networks. Lastly, the author mentioned some\ninteresting open challenges in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 23:43:30 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Tashakori", "Arvin", ""]]}, {"id": "2105.02375", "submitter": "Qing Qu", "authors": "Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias\n  Sulam, and Qing Qu", "title": "A Geometric Analysis of Neural Collapse with Unconstrained Features", "comments": "42 pages, 8 figures, 1 table; the first two authors contributed to\n  this work equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first global optimization landscape analysis of\n$Neural\\;Collapse$ -- an intriguing empirical phenomenon that arises in the\nlast-layer classifiers and features of neural networks during the terminal\nphase of training. As recently reported by Papyan et al., this phenomenon\nimplies that ($i$) the class means and the last-layer classifiers all collapse\nto the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and\n($ii$) cross-example within-class variability of last-layer activations\ncollapses to zero. We study the problem based on a simplified\n$unconstrained\\;feature\\;model$, which isolates the topmost layers from the\nclassifier of the neural network. In this context, we show that the classical\ncross-entropy loss with weight decay has a benign global landscape, in the\nsense that the only global minimizers are the Simplex ETFs while all other\ncritical points are strict saddles whose Hessian exhibit negative curvature\ndirections. In contrast to existing landscape analysis for deep neural networks\nwhich is often disconnected from practice, our analysis of the simplified model\nnot only does it explain what kind of features are learned in the last layer,\nbut it also shows why they can be efficiently optimized in the simplified\nsettings, matching the empirical observations in practical deep network\narchitectures. These findings could have profound implications for\noptimization, generalization, and robustness of broad interests. For example,\nour experiments demonstrate that one may set the feature dimension equal to the\nnumber of classes and fix the last-layer classifier to be a Simplex ETF for\nnetwork training, which reduces memory cost by over $20\\%$ on ResNet18 without\nsacrificing the generalization performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 00:00:50 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhu", "Zhihui", ""], ["Ding", "Tianyu", ""], ["Zhou", "Jinxin", ""], ["Li", "Xiao", ""], ["You", "Chong", ""], ["Sulam", "Jeremias", ""], ["Qu", "Qing", ""]]}, {"id": "2105.02377", "submitter": "Ruohan Zhan", "authors": "Ruohan Zhan, Konstantina Christakopoulou, Ya Le, Jayden Ooi, Martin\n  Mladenov, Alex Beutel, Craig Boutilier, Ed H. Chi, Minmin Chen", "title": "Towards Content Provider Aware Recommender Systems: A Simulation Study\n  on the Interplay between User and Provider Utilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing recommender systems focus primarily on matching users to\ncontent which maximizes user satisfaction on the platform. It is increasingly\nobvious, however, that content providers have a critical influence on user\nsatisfaction through content creation, largely determining the content pool\navailable for recommendation. A natural question thus arises: can we design\nrecommenders taking into account the long-term utility of both users and\ncontent providers? By doing so, we hope to sustain more providers and a more\ndiverse content pool for long-term user satisfaction. Understanding the full\nimpact of recommendations on both user and provider groups is challenging. This\npaper aims to serve as a research investigation of one approach toward building\na provider-aware recommender, and evaluating its impact in a simulated setup.\n  To characterize the user-recommender-provider interdependence, we complement\nuser modeling by formalizing provider dynamics as well. The resulting joint\ndynamical system gives rise to a weakly-coupled partially observable Markov\ndecision process driven by recommender actions and user feedback to providers.\nWe then build a REINFORCE recommender agent, coined EcoAgent, to optimize a\njoint objective of user utility and the counterfactual utility lift of the\nprovider associated with the recommended content, which we show to be\nequivalent to maximizing overall user utility and the utilities of all\nproviders on the platform under some mild assumptions. To evaluate our\napproach, we introduce a simulation environment capturing the key interactions\namong users, providers, and the recommender. We offer a number of simulated\nexperiments that shed light on both the benefits and the limitations of our\napproach. These results help understand how and when a provider-aware\nrecommender agent is of benefit in building multi-stakeholder recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 00:02:58 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhan", "Ruohan", ""], ["Christakopoulou", "Konstantina", ""], ["Le", "Ya", ""], ["Ooi", "Jayden", ""], ["Mladenov", "Martin", ""], ["Beutel", "Alex", ""], ["Boutilier", "Craig", ""], ["Chi", "Ed H.", ""], ["Chen", "Minmin", ""]]}, {"id": "2105.02386", "submitter": "Noah Ziems", "authors": "Noah Ziems, Shaoen Wu, Jim Norman", "title": "Automated Primary Hyperparathyroidism Screening with Neural Networks", "comments": "IEEE GLOBECOM", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Primary Hyperparathyroidism(PHPT) is a relatively common disease, affecting\nabout one in every 1,000 adults. However, screening for PHPT can be difficult,\nmeaning it often goes undiagnosed for long periods of time. While looking at\nspecific blood test results independently can help indicate whether a patient\nhas PHPT, often these blood result levels can all be within their respective\nnormal ranges despite the patient having PHPT. Based on the clinic data from\nthe real world, in this work, we propose a novel approach to screening PHPT\nwith neural network (NN) architecture, achieving over 97\\% accuracy with common\nblood values as inputs. Further, we propose a second model achieving over 99\\%\naccuracy with additional lab test values as inputs. Moreover, compared to\ntraditional PHPT screening methods, our NN models can reduce the false\nnegatives of traditional screening methods by 99\\%.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 01:14:27 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ziems", "Noah", ""], ["Wu", "Shaoen", ""], ["Norman", "Jim", ""]]}, {"id": "2105.02410", "submitter": "Tong Wang", "authors": "Tong Wang, Jingyi Yang, Yunyi Li, Boxiang Wang", "title": "Partially Interpretable Estimators (PIE): Black-Box-Refined\n  Interpretable Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Partially Interpretable Estimators (PIE) which attribute a\nprediction to individual features via an interpretable model, while a\n(possibly) small part of the PIE prediction is attributed to the interaction of\nfeatures via a black-box model, with the goal to boost the predictive\nperformance while maintaining interpretability. As such, the interpretable\nmodel captures the main contributions of features, and the black-box model\nattempts to complement the interpretable piece by capturing the \"nuances\" of\nfeature interactions as a refinement. We design an iterative training algorithm\nto jointly train the two types of models. Experimental results show that PIE is\nhighly competitive to black-box models while outperforming interpretable\nbaselines. In addition, the understandability of PIE is comparable to simple\nlinear models as validated via a human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 03:06:34 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Tong", ""], ["Yang", "Jingyi", ""], ["Li", "Yunyi", ""], ["Wang", "Boxiang", ""]]}, {"id": "2105.02446", "submitter": "Jinglin Liu", "authors": "Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, Peng Liu, Zhou Zhao", "title": "DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism", "comments": "acoustic model, singing voice synthesis, text to speech, diffusion\n  model, shallow diffusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singing voice synthesis (SVS) system is built to synthesize high-quality and\nexpressive singing voice, in which the acoustic model generates the acoustic\nfeatures (e.g., mel-spectrogram) given a music score. Previous singing acoustic\nmodels adopt simple loss (e.g., L1 and L2) or generative adversarial network\n(GAN) to reconstruct the acoustic features, while they suffer from\nover-smoothing and unstable training issues respectively, which hinder the\nnaturalness of synthesized singing. In this work, we propose DiffSinger, an\nacoustic model for SVS based on the diffusion probabilistic model. DiffSinger\nis a parameterized Markov chain which iteratively converts the noise into\nmel-spectrogram conditioned on the music score. By implicitly optimizing\nvariational bound, DiffSinger can be stably trained and generates realistic\noutputs. To further improve the voice quality and speed up inference, we\nintroduce a shallow diffusion mechanism to make better use of the prior\nknowledge learned by the simple loss. Specifically, DiffSinger starts\ngeneration at a shallow step smaller than the total number of diffusion steps,\naccording to the intersection of the diffusion trajectories of the ground-truth\nmel-spectrogram and the one predicted by a simple mel-spectrogram decoder.\nBesides, we train a boundary prediction network to locate the intersection and\ndetermine the shallow step adaptively. The evaluations conducted on the Chinese\nsinging dataset demonstrate that DiffSinger outperforms state-of-the-art SVS\nwork. Our extensional experiments also prove the generalization of DiffSinger\non text-to-speech task.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 05:21:42 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 13:24:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Jinglin", ""], ["Li", "Chengxi", ""], ["Ren", "Yi", ""], ["Chen", "Feiyang", ""], ["Liu", "Peng", ""], ["Zhao", "Zhou", ""]]}, {"id": "2105.02451", "submitter": "Peizhuo Li", "authors": "Peizhuo Li, Kfir Aberman, Rana Hanocka, Libin Liu, Olga\n  Sorkine-Hornung, Baoquan Chen", "title": "Learning Skeletal Articulations with Neural Blend Shapes", "comments": "SIGGRAPH 2021. Project page:\n  https://peizhuoli.github.io/neural-blend-shapes/ , Video:\n  https://youtu.be/antc20EFh6k", "journal-ref": null, "doi": "10.1145/3450626.3459852", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animating a newly designed character using motion capture (mocap) data is a\nlong standing problem in computer animation. A key consideration is the\nskeletal structure that should correspond to the available mocap data, and the\nshape deformation in the joint regions, which often requires a tailored,\npose-specific refinement. In this work, we develop a neural technique for\narticulating 3D characters using enveloping with a pre-defined skeletal\nstructure which produces high quality pose dependent deformations. Our\nframework learns to rig and skin characters with the same articulation\nstructure (e.g., bipeds or quadrupeds), and builds the desired skeleton\nhierarchy into the network architecture. Furthermore, we propose neural blend\nshapes--a set of corrective pose-dependent shapes which improve the deformation\nquality in the joint regions in order to address the notorious artifacts\nresulting from standard rigging and skinning. Our system estimates neural blend\nshapes for input meshes with arbitrary connectivity, as well as weighting\ncoefficients which are conditioned on the input joint rotations. Unlike recent\ndeep learning techniques which supervise the network with ground-truth rigging\nand skinning parameters, our approach does not assume that the training data\nhas a specific underlying deformation model. Instead, during training, the\nnetwork observes deformed shapes and learns to infer the corresponding rig,\nskin and blend shapes using indirect supervision. During inference, we\ndemonstrate that our network generalizes to unseen characters with arbitrary\nmesh connectivity, including unrigged characters built by 3D artists.\nConforming to standard skeletal animation models enables direct plug-and-play\nin standard animation software, as well as game engines.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 05:58:13 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Peizhuo", ""], ["Aberman", "Kfir", ""], ["Hanocka", "Rana", ""], ["Liu", "Libin", ""], ["Sorkine-Hornung", "Olga", ""], ["Chen", "Baoquan", ""]]}, {"id": "2105.02468", "submitter": "Leonardo Petrini", "authors": "Leonardo Petrini, Alessandro Favero, Mario Geiger, Matthieu Wyart", "title": "Relative stability toward diffeomorphisms indicates performance in deep\n  nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why deep nets can classify data in large dimensions remains a\nchallenge. It has been proposed that they do so by becoming stable to\ndiffeomorphisms, yet existing empirical measurements support that it is often\nnot the case. We revisit this question by defining a maximum-entropy\ndistribution on diffeomorphisms, that allows to study typical diffeomorphisms\nof a given norm. We confirm that stability toward diffeomorphisms does not\nstrongly correlate to performance on benchmark data sets of images. By\ncontrast, we find that the stability toward diffeomorphisms relative to that of\ngeneric transformations $R_f$ correlates remarkably with the test error\n$\\epsilon_t$. It is of order unity at initialization but decreases by several\ndecades during training for state-of-the-art architectures. For CIFAR10 and 15\nknown architectures, we find $\\epsilon_t\\approx 0.2\\sqrt{R_f}$, suggesting that\nobtaining a small $R_f$ is important to achieve good performance. We study how\n$R_f$ depends on the size of the training set and compare it to a simple model\nof invariant learning.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:03:30 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 13:18:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Petrini", "Leonardo", ""], ["Favero", "Alessandro", ""], ["Geiger", "Mario", ""], ["Wyart", "Matthieu", ""]]}, {"id": "2105.02469", "submitter": "Krishna Subramani", "authors": "Krishna Subramani, Paris Smaragdis", "title": "Point Cloud Audio Processing", "comments": "Accepted at WASPAA 2021, Code:\n  https://github.com/SubramaniKrishna/point-cloud-audio", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most audio processing pipelines involve transformations that act on\nfixed-dimensional input representations of audio. For example, when using the\nShort Time Fourier Transform (STFT) the DFT size specifies a fixed dimension\nfor the input representation. As a consequence, most audio machine learning\nmodels are designed to process fixed-size vector inputs which often prohibits\nthe repurposing of learned models on audio with different sampling rates or\nalternative representations. We note, however, that the intrinsic spectral\ninformation in the audio signal is invariant to the choice of the input\nrepresentation or the sampling rate. Motivated by this, we introduce a novel\nway of processing audio signals by treating them as a collection of points in\nfeature space, and we use point cloud machine learning models that give us\ninvariance to the choice of representation parameters, such as DFT size or the\nsampling rate. Additionally, we observe that these methods result in smaller\nmodels, and allow us to significantly subsample the input representation with\nminimal effects to a trained model performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:04:59 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 06:32:18 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Subramani", "Krishna", ""], ["Smaragdis", "Paris", ""]]}, {"id": "2105.02470", "submitter": "Thomas M. Sutter", "authors": "Thomas M. Sutter and Imant Daunhawer, Julia E. Vogt", "title": "Generalized Multimodal ELBO", "comments": "2021 ICLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multiple data types naturally co-occur when describing real-world phenomena\nand learning from them is a long-standing goal in machine learning research.\nHowever, existing self-supervised generative models approximating an ELBO are\nnot able to fulfill all desired requirements of multimodal models: their\nposterior approximation functions lead to a trade-off between the semantic\ncoherence and the ability to learn the joint data distribution. We propose a\nnew, generalized ELBO formulation for multimodal data that overcomes these\nlimitations. The new objective encompasses two previous methods as special\ncases and combines their benefits without compromises. In extensive\nexperiments, we demonstrate the advantage of the proposed method compared to\nstate-of-the-art models in self-supervised, generative learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:05:00 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 08:09:04 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Sutter", "Thomas M.", ""], ["Daunhawer", "Imant", ""], ["Vogt", "Julia E.", ""]]}, {"id": "2105.02487", "submitter": "Boxin Zhao", "authors": "Boxin Zhao, Shengjun Zhai, Y. Samuel Wang, Mladen Kolar", "title": "High-dimensional Functional Graphical Model Structure Learning via\n  Neighborhood Selection Approach", "comments": "52 pages, 1 figure and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Undirected graphical models have been widely used to model the conditional\nindependence structure of high-dimensional random vector data for years. In\nmany modern applications such as EEG and fMRI data, the observations are\nmultivariate random functions rather than scalars. To model the conditional\nindependence of this type of data, functional graphical models are proposed and\nhave attracted an increasing attention in recent years. In this paper, we\npropose a neighborhood selection approach to estimate Gaussian functional\ngraphical models. We first estimate the neighborhood of all nodes via\nfunction-on-function regression, and then we can recover the whole graph\nstructure based on the neighborhood information. By estimating conditional\nstructure directly, we can circumvent the need of a well-defined precision\noperator which generally does not exist. Besides, we can better explore the\neffect of the choice of function basis for dimension reduction. We give a\ncriterion for choosing the best function basis and motivate two practically\nuseful choices, which we justified by both theory and experiments and show that\nthey are better than expanding each function onto its own FPCA basis as in\nprevious literature. In addition, the neighborhood selection approach is\ncomputationally more efficient than fglasso as it is more easy to do parallel\ncomputing. The statistical consistency of our proposed methods in\nhigh-dimensional setting are supported by both theory and experiment.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:38:50 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhao", "Boxin", ""], ["Zhai", "Shengjun", ""], ["Wang", "Y. Samuel", ""], ["Kolar", "Mladen", ""]]}, {"id": "2105.02489", "submitter": "Tianyuan Huang", "authors": "Tianyuan Huang, Zhecheng Wang, Hao Sheng, Andrew Y. Ng, Ram Rajagopal", "title": "Learning Neighborhood Representation from Multi-Modal Multi-Graph:\n  Image, Text, Mobility Graph and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent urbanization has coincided with the enrichment of geotagged data, such\nas street view and point-of-interest (POI). Region embedding enhanced by the\nricher data modalities has enabled researchers and city administrators to\nunderstand the built environment, socioeconomics, and the dynamics of cities\nbetter. While some efforts have been made to simultaneously use multi-modal\ninputs, existing methods can be improved by incorporating different measures of\n'proximity' in the same embedding space - leveraging not only the data that\ncharacterizes the regions (e.g., street view, local businesses pattern) but\nalso those that depict the relationship between regions (e.g., trips, road\nnetwork). To this end, we propose a novel approach to integrate multi-modal\ngeotagged inputs as either node or edge features of a multi-graph based on\ntheir relations with the neighborhood region (e.g., tiles, census block, ZIP\ncode region, etc.). We then learn the neighborhood representation based on a\ncontrastive-sampling scheme from the multi-graph. Specifically, we use street\nview images and POI features to characterize neighborhoods (nodes) and use\nhuman mobility to characterize the relationship between neighborhoods (directed\nedges). We show the effectiveness of the proposed methods with quantitative\ndownstream tasks as well as qualitative analysis of the embedding space: The\nembedding we trained outperforms the ones using only unimodal data as regional\ninputs.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:44:05 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Huang", "Tianyuan", ""], ["Wang", "Zhecheng", ""], ["Sheng", "Hao", ""], ["Ng", "Andrew Y.", ""], ["Rajagopal", "Ram", ""]]}, {"id": "2105.02498", "submitter": "Yue Song", "authors": "Yue Song, Nicu Sebe, Wei Wang", "title": "Why Approximate Matrix Square Root Outperforms Accurate SVD in Global\n  Covariance Pooling?", "comments": "Accepted by ICCV21 as poster presetation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Global covariance pooling (GCP) aims at exploiting the second-order\nstatistics of the convolutional feature. Its effectiveness has been\ndemonstrated in boosting the classification performance of Convolutional Neural\nNetworks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute\nthe matrix square root. However, the approximate matrix square root calculated\nusing Newton-Schulz iteration \\cite{li2018towards} outperforms the accurate one\ncomputed via SVD \\cite{li2017second}. We empirically analyze the reason behind\nthe performance gap from the perspectives of data precision and gradient\nsmoothness. Various remedies for computing smooth SVD gradients are\ninvestigated. Based on our observation and analyses, a hybrid training protocol\nis proposed for SVD-based GCP meta-layers such that competitive performances\ncan be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP\nmeta-layer that uses SVD in the forward pass, and Pad\\'e Approximants in the\nbackward propagation to compute the gradients. The proposed meta-layer has been\nintegrated into different CNN models and achieves state-of-the-art performances\non both large-scale and fine-grained datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:03:45 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 09:52:15 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Song", "Yue", ""], ["Sebe", "Nicu", ""], ["Wang", "Wei", ""]]}, {"id": "2105.02522", "submitter": "Alexis Bellot", "authors": "Alexis Bellot, Kim Branson and Mihaela van der Schaar", "title": "Consistency of mechanistic causal discovery in continuous-time using\n  Neural ODEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of causal mechanisms from time series data is a key problem in\nfields working with complex systems. Most identifiability results and learning\nalgorithms assume the underlying dynamics to be discrete in time. Comparatively\nfew, in contrast, explicitly define causal associations in infinitesimal\nintervals of time, independently of the scale of observation and of the\nregularity of sampling. In this paper, we consider causal discovery in\ncontinuous-time for the study of dynamical systems. We prove that for vector\nfields parameterized in a large class of neural networks, adaptive\nregularization schemes consistently recover causal graphs in systems of\nordinary differential equations (ODEs). Using this insight, we propose a causal\ndiscovery algorithm based on penalized Neural ODEs that we show to be\napplicable to the general setting of irregularly-sampled multivariate time\nseries and to strongly outperform the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:48:02 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bellot", "Alexis", ""], ["Branson", "Kim", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2105.02531", "submitter": "Seyed Mehdi Ayyoubzadeh", "authors": "Seyed Mehdi Ayyoubzadeh, Ali Royat", "title": "(ASNA) An Attention-based Siamese-Difference Neural Network with\n  Surrogate Ranking Loss function for Perceptual Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural networks (DCNN) that leverage the\nadversarial training framework for image restoration and enhancement have\nsignificantly improved the processed images' sharpness. Surprisingly, although\nthese DCNNs produced crispier images than other methods visually, they may get\na lower quality score when popular measures are employed for evaluating them.\nTherefore it is necessary to develop a quantitative metric to reflect their\nperformances, which is well-aligned with the perceived quality of an image.\nFamous quantitative metrics such as Peak signal-to-noise ratio (PSNR), The\nstructural similarity index measure (SSIM), and Perceptual Index (PI) are not\nwell-correlated with the mean opinion score (MOS) for an image, especially for\nthe neural networks trained with adversarial loss functions.\n  This paper has proposed a convolutional neural network using an extension\narchitecture of the traditional Siamese network so-called Siamese-Difference\nneural network. We have equipped this architecture with the spatial and\nchannel-wise attention mechanism to increase our method's performance.\n  Finally, we employed an auxiliary loss function to train our model. The\nsuggested additional cost function surrogates ranking loss to increase\nSpearman's rank correlation coefficient while it is differentiable concerning\nthe neural network parameters. Our method achieved superior performance in\n\\textbf{\\textit{NTIRE 2021 Perceptual Image Quality Assessment}} Challenge. The\nimplementations of our proposed method are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 09:04:21 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ayyoubzadeh", "Seyed Mehdi", ""], ["Royat", "Ali", ""]]}, {"id": "2105.02543", "submitter": "Xiaofeng Cao", "authors": "Xiaofeng Cao and Ivor W. Tsang", "title": "Bayesian Active Learning by Disagreements: A Geometric Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present geometric Bayesian active learning by disagreements (GBALD), a\nframework that performs BALD on its core-set construction interacting with\nmodel uncertainty estimation. Technically, GBALD constructs core-set on\nellipsoid, not typical sphere, preventing low-representative elements from\nspherical boundaries. The improvements are twofold: 1) relieve uninformative\nprior and 2) reduce redundant estimations. Theoretically, geodesic search with\nellipsoid can derive tighter lower bound on error and easier to achieve zero\nerror than with sphere. Experiments show that GBALD has slight perturbations to\nnoisy and repeated samples, and outperforms BALD, BatchBALD and other existing\ndeep active learning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 09:37:59 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Cao", "Xiaofeng", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "2105.02551", "submitter": "Jary Pomponi", "authors": "Jary Pomponi, Simone Scardapane, and Aurelio Uncini", "title": "Structured Ensembles: an Approach to Reduce the Memory Footprint of\n  Ensemble Methods", "comments": "Preprint submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel ensembling technique for deep neural\nnetworks, which is able to drastically reduce the required memory compared to\nalternative approaches. In particular, we propose to extract multiple\nsub-networks from a single, untrained neural network by solving an end-to-end\noptimization task combining differentiable scaling over the original\narchitecture, with multiple regularization terms favouring the diversity of the\nensemble. Since our proposal aims to detect and extract sub-structures, we call\nit Structured Ensemble. On a large experimental evaluation, we show that our\nmethod can achieve higher or comparable accuracy to competing methods while\nrequiring significantly less storage. In addition, we evaluate our ensembles in\nterms of predictive calibration and uncertainty, showing they compare\nfavourably with the state-of-the-art. Finally, we draw a link with the\ncontinual learning literature, and we propose a modification of our framework\nto handle continuous streams of tasks with a sub-linear memory cost. We compare\nwith a number of alternative strategies to mitigate catastrophic forgetting,\nhighlighting advantages in terms of average accuracy and memory.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 09:56:01 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Pomponi", "Jary", ""], ["Scardapane", "Simone", ""], ["Uncini", "Aurelio", ""]]}, {"id": "2105.02556", "submitter": "Clemens Hutter", "authors": "Clemens Hutter, Recep G\\\"ul, Helmut B\\\"olcskei", "title": "Metric Entropy Limits on Recurrent Neural Network Learning of Linear\n  Dynamical Systems", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most influential results in neural network theory is the universal\napproximation theorem [1, 2, 3] which states that continuous functions can be\napproximated to within arbitrary accuracy by single-hidden-layer feedforward\nneural networks. The purpose of this paper is to establish a result in this\nspirit for the approximation of general discrete-time linear dynamical systems\n- including time-varying systems - by recurrent neural networks (RNNs). For the\nsubclass of linear time-invariant (LTI) systems, we devise a quantitative\nversion of this statement. Specifically, measuring the complexity of the\nconsidered class of LTI systems through metric entropy according to [4], we\nshow that RNNs can optimally learn - or identify in system-theory parlance -\nstable LTI systems. For LTI systems whose input-output relation is\ncharacterized through a difference equation, this means that RNNs can learn the\ndifference equation from input-output traces in a metric-entropy optimal\nmanner.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:12:30 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hutter", "Clemens", ""], ["G\u00fcl", "Recep", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "2105.02565", "submitter": "Islem Rekik", "authors": "Alaa Bessadok and Mohamed Ali Mahjoub and Islem Rekik", "title": "Brain Multigraph Prediction using Topology-Aware Adversarial Graph\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Brain graphs (i.e, connectomes) constructed from medical scans such as\nmagnetic resonance imaging (MRI) have become increasingly important tools to\ncharacterize the abnormal changes in the human brain. Due to the high\nacquisition cost and processing time of multimodal MRI, existing deep learning\nframeworks based on Generative Adversarial Network (GAN) focused on predicting\nthe missing multimodal medical images from a few existing modalities. While\nbrain graphs help better understand how a particular disorder can change the\nconnectional facets of the brain, synthesizing a target brain multigraph (i.e,\nmultiple brain graphs) from a single source brain graph is strikingly lacking.\nAdditionally, existing graph generation works mainly learn one model for each\ntarget domain which limits their scalability in jointly predicting multiple\ntarget domains. Besides, while they consider the global topological scale of a\ngraph (i.e., graph connectivity structure), they overlook the local topology at\nthe node scale (e.g., how central a node is in the graph). To address these\nlimitations, we introduce topology-aware graph GAN architecture (topoGAN),\nwhich jointly predicts multiple brain graphs from a single brain graph while\npreserving the topological structure of each target graph. Its three key\ninnovations are: (i) designing a novel graph adversarial auto-encoder for\npredicting multiple brain graphs from a single one, (ii) clustering the encoded\nsource graphs in order to handle the mode collapse issue of GAN and proposing a\ncluster-specific decoder, (iii) introducing a topological loss to force the\nprediction of topologically sound target brain graphs. The experimental results\nusing five target domains demonstrated the outperformance of our method in\nbrain multigraph prediction from a single graph in comparison with baseline\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:20:45 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bessadok", "Alaa", ""], ["Mahjoub", "Mohamed Ali", ""], ["Rekik", "Islem", ""]]}, {"id": "2105.02566", "submitter": "Francesca Lizzi", "authors": "Francesca Lizzi, Abramo Agosti, Francesca Brero, Raffaella Fiamma\n  Cabini, Maria Evelina Fantacci, Silvia Figini, Alessandro Lascialfari,\n  Francesco Laruina, Piernicola Oliva, Stefano Piffer, Ian Postuma, Lisa\n  Rinaldi, Cinzia Talamonti, Alessandra Retico", "title": "Quantification of pulmonary involvement in COVID-19 pneumonia by means\n  of a cascade oftwo U-nets: training and assessment on multipledatasets using\n  different annotation criteria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The automatic assignment of a severity score to the CT scans of patients\naffected by COVID-19 pneumonia could reduce the workload in radiology\ndepartments. This study aims at exploiting Artificial intelligence (AI) for the\nidentification, segmentation and quantification of COVID-19 pulmonary lesions.\nWe investigated the effects of using multiple datasets, heterogeneously\npopulated and annotated according to different criteria. We developed an\nautomated analysis pipeline, the LungQuant system, based on a cascade of two\nU-nets. The first one (U-net_1) is devoted to the identification of the lung\nparenchyma, the second one (U-net_2) acts on a bounding box enclosing the\nsegmented lungs to identify the areas affected by COVID-19 lesions. Different\npublic datasets were used to train the U-nets and to evaluate their\nsegmentation performances, which have been quantified in terms of the Dice\nindex. The accuracy in predicting the CT-Severity Score (CT-SS) of the\nLungQuant system has been also evaluated. Both Dice and accuracy showed a\ndependency on the quality of annotations of the available data samples. On an\nindependent and publicly available benchmark dataset, the Dice values measured\nbetween the masks predicted by LungQuant system and the reference ones were\n0.95$\\pm$0.01 and 0.66$\\pm$0.13 for the segmentation of lungs and COVID-19\nlesions, respectively. The accuracy of 90% in the identification of the CT-SS\non this benchmark dataset was achieved. We analysed the impact of using data\nsamples with different annotation criteria in training an AI-based\nquantification system for pulmonary involvement in COVID-19 pneumonia. In terms\nof the Dice index, the U-net segmentation quality strongly depends on the\nquality of the lesion annotations. Nevertheless, the CT-SS can be accurately\npredicted on independent validation sets, demonstrating the satisfactory\ngeneralization ability of the LungQuant.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:21:28 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lizzi", "Francesca", ""], ["Agosti", "Abramo", ""], ["Brero", "Francesca", ""], ["Cabini", "Raffaella Fiamma", ""], ["Fantacci", "Maria Evelina", ""], ["Figini", "Silvia", ""], ["Lascialfari", "Alessandro", ""], ["Laruina", "Francesco", ""], ["Oliva", "Piernicola", ""], ["Piffer", "Stefano", ""], ["Postuma", "Ian", ""], ["Rinaldi", "Lisa", ""], ["Talamonti", "Cinzia", ""], ["Retico", "Alessandra", ""]]}, {"id": "2105.02569", "submitter": "Qingfeng Liu", "authors": "Qingfeng Liu and Yang Feng", "title": "Machine Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new ensemble framework for supervised learning, called machine\ncollaboration (MaC), using a collection of base machines for prediction tasks.\nUnlike bagging/stacking (a parallel & independent framework) and boosting (a\nsequential & top-down framework), MaC is a type of circular & interactive\nlearning framework. The circular & interactive feature helps the base machines\nto transfer information circularly and update their structures and parameters\naccordingly. The theoretical result on the risk bound of the estimator from MaC\nreveals that the circular & interactive feature can help MaC reduce risk via a\nparsimonious ensemble. We conduct extensive experiments on MaC using both\nsimulated data and 119 benchmark real datasets. The results demonstrate that in\nmost cases, MaC performs significantly better than several other\nstate-of-the-art methods, including classification and regression trees, neural\nnetworks, stacking, and boosting.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:27:03 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 04:29:34 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Liu", "Qingfeng", ""], ["Feng", "Yang", ""]]}, {"id": "2105.02579", "submitter": "Fernando Llorente Fern\\'andez", "authors": "F. Llorente, E. Curbelo, L. Martino, V. Elvira, D. Delgado", "title": "MCMC-driven importance samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods are the standard procedure for estimating complicated\nintegrals of multidimensional Bayesian posterior distributions. In this work,\nwe focus on LAIS, a class of adaptive importance samplers where Markov chain\nMonte Carlo (MCMC) algorithms are employed to drive an underlying multiple\nimportance sampling (IS) scheme. Its power lies in the simplicity of the\nlayered framework: the upper layer locates proposal densities by means of MCMC\nalgorithms; while the lower layer handles the multiple IS scheme, in order to\ncompute the final estimators. The modular nature of LAIS allows for different\npossible choices in the upper and lower layers, that will have different\nperformance and computational costs. In this work, we propose different\nenhancements in order to increase the efficiency and reduce the computational\ncost, of both upper and lower layers. The different variants are essential if\nwe aim to address computational challenges arising in real-world applications,\nsuch as highly concentrated posterior distributions (due to large amounts of\ndata, etc.). Hamiltonian-driven importance samplers are presented and tested.\nFurthermore, we introduce different strategies for designing cheaper schemes,\nfor instance, recycling samples generated in the upper layer and using them in\nthe final estimators in the lower layer. Numerical experiments show the\nbenefits of the proposed schemes as compared to the vanilla version of LAIS and\nother benchmark methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:59:02 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 14:15:51 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 07:50:28 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Llorente", "F.", ""], ["Curbelo", "E.", ""], ["Martino", "L.", ""], ["Elvira", "V.", ""], ["Delgado", "D.", ""]]}, {"id": "2105.02580", "submitter": "Yeo Jin Kim", "authors": "Yeo Jin Kim and Min Chi", "title": "Time-Aware Q-Networks: Resolving Temporal Irregularity for Deep\n  Reinforcement Learning", "comments": "36 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep Reinforcement Learning (DRL) has shown outstanding performance on\ninducing effective action policies that maximize expected long-term return on\nmany complex tasks. Much of DRL work has been focused on sequences of events\nwith discrete time steps and ignores the irregular time intervals between\nconsecutive events. Given that in many real-world domains, data often consists\nof temporal sequences with irregular time intervals, and it is important to\nconsider the time intervals between temporal events to capture latent\nprogressive patterns of states. In this work, we present a general Time-Aware\nRL framework: Time-aware Q-Networks (TQN), which takes into account physical\ntime intervals within a deep RL framework. TQN deals with time irregularity\nfrom two aspects: 1) elapsed time in the past and an expected next observation\ntime for time-aware state approximation, and 2) action time window for the\nfuture for time-aware discounting of rewards. Experimental results show that by\ncapturing the underlying structures in the sequences with time irregularities\nfrom both aspects, TQNs significantly outperform DQN in four types of contexts\nwith irregular time intervals. More specifically, our results show that in\nclassic RL tasks such as CartPole and MountainCar and Atari benchmark with\nrandomly segmented time intervals, time-aware discounting alone is more\nimportant while in the real-world tasks such as nuclear reactor operation and\nseptic patient treatment with intrinsic time intervals, both time-aware state\nand time-aware discounting are crucial. Moreover, to improve the agent's\nlearning capacity, we explored three boosting methods: Double networks, Dueling\nnetworks, and Prioritized Experience Replay, and our results show that for the\ntwo real-world tasks, combining all three boosting methods with TQN is\nespecially effective.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 11:00:59 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Kim", "Yeo Jin", ""], ["Chi", "Min", ""]]}, {"id": "2105.02585", "submitter": "Chao Yang", "authors": "Bi-Ying Yan and Chao Yang and Feng Chen and Kohei Takeda and Changjun\n  Wang", "title": "FDNet: A Deep Learning Approach with Two Parallel Cross Encoding\n  Pathways for Precipitation Nowcasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the goal of predicting the future rainfall intensity in a local region\nover a relatively short period time, precipitation nowcasting has been a\nlong-time scientific challenge with great social and economic impact. The radar\necho extrapolation approaches for precipitation nowcasting take radar echo\nimages as input, aiming to generate future radar echo images by learning from\nthe historical images. To effectively handle complex and high non-stationary\nevolution of radar echoes, we propose to decompose the movement into optical\nflow field motion and morphologic deformation. Following this idea, we\nintroduce Flow-Deformation Network (FDNet), a neural network that models flow\nand deformation in two parallel cross pathways. The flow encoder captures the\noptical flow field motion between consecutive images and the deformation\nencoder distinguishes the change of shape from the translational motion of\nradar echoes. We evaluate the proposed network architecture on two real-world\nradar echo datasets. Our model achieves state-of-the-art prediction results\ncompared with recent approaches. To the best of our knowledge, this is the\nfirst network architecture with flow and deformation separation to model the\nevolution of radar echoes for precipitation nowcasting. We believe that the\ngeneral idea of this work could not only inspire much more effective approaches\nbut also be applied to other similar spatiotemporal prediction tasks\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 11:18:24 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yan", "Bi-Ying", ""], ["Yang", "Chao", ""], ["Chen", "Feng", ""], ["Takeda", "Kohei", ""], ["Wang", "Changjun", ""]]}, {"id": "2105.02589", "submitter": "Soumajyoti Sarkar Mr.", "authors": "Soumajyoti Sarkar", "title": "Bandit based centralized matching in two-sided markets for peer to peer\n  lending", "comments": "arXiv admin note: text overlap with arXiv:2011.04400", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequential fundraising in two sided online platforms enable peer to peer\nlending by sequentially bringing potential contributors, each of whose\ndecisions impact other contributors in the market. However, understanding the\ndynamics of sequential contributions in online platforms for peer lending has\nbeen an open ended research question. The centralized investment mechanism in\nthese platforms makes it difficult to understand the implicit competition that\nborrowers face from a single lender at any point in time. Matching markets are\na model of pairing agents where the preferences of agents from both sides in\nterms of their preferred pairing for transactions can allow to decentralize the\nmarket. We study investment designs in two sided platforms using matching\nmarkets when the investors or lenders also face restrictions on the investments\nbased on borrower preferences. This situation creates an implicit competition\namong the lenders in addition to the existing borrower competition, especially\nwhen the lenders are uncertain about their standing in the market and thereby\nthe probability of their investments being accepted or the borrower loan\nrequests for projects reaching the reserve price. We devise a technique based\non sequential decision making that allows the lenders to adjust their choices\nbased on the dynamics of uncertainty from competition over time. We simulate\ntwo sided market matchings in a sequential decision framework and show the\ndynamics of the lender regret amassed compared to the optimal borrower-lender\nmatching and find that the lender regret depends on the initial preferences set\nby the lenders which could affect their learning over decision making steps.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 11:23:26 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Sarkar", "Soumajyoti", ""]]}, {"id": "2105.02590", "submitter": "Samson Tan", "authors": "Samson Tan, Shafiq Joty, Kathy Baxter, Araz Taeihagh, Gregory A.\n  Bennett, Min-Yen Kan", "title": "Reliability Testing for Natural Language Processing Systems", "comments": "Accepted to ACL-IJCNLP 2021 (main conference). Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Questions of fairness, robustness, and transparency are paramount to address\nbefore deploying NLP systems. Central to these concerns is the question of\nreliability: Can NLP systems reliably treat different demographics fairly and\nfunction correctly in diverse and noisy environments? To address this, we argue\nfor the need for reliability testing and contextualize it among existing work\non improving accountability. We show how adversarial attacks can be reframed\nfor this goal, via a framework for developing reliability tests. We argue that\nreliability testing -- with an emphasis on interdisciplinary collaboration --\nwill enable rigorous and targeted testing, and aid in the enactment and\nenforcement of industry standards.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 11:24:58 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 04:17:44 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 03:55:40 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Tan", "Samson", ""], ["Joty", "Shafiq", ""], ["Baxter", "Kathy", ""], ["Taeihagh", "Araz", ""], ["Bennett", "Gregory A.", ""], ["Kan", "Min-Yen", ""]]}, {"id": "2105.02595", "submitter": "John T\\\"ornblom", "authors": "John T\\\"ornblom and Simin Nadjm-Tehrani", "title": "Scaling up Memory-Efficient Formal Verification Tools for Tree Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To guarantee that machine learning models yield outputs that are not only\naccurate, but also robust, recent works propose formally verifying robustness\nproperties of machine learning models. To be applicable to realistic\nsafety-critical systems, the used verification algorithms need to manage the\ncombinatorial explosion resulting from vast variations in the input domain, and\nbe able to verify correctness properties derived from versatile and\ndomain-specific requirements.\n  In this paper, we formalise the VoTE algorithm presented earlier as a tool\ndescription, and extend the tool set with mechanisms for systematic scalability\nstudies. In particular, we show a) how the separation of property checking from\nthe core verification engine enables verification of versatile requirements, b)\nthe scalability of the tool, both in terms of time taken for verification and\nuse of memory, and c) that the algorithm has attractive properties that lend\nthemselves well for massive parallelisation.\n  We demonstrate the application of the tool in two case studies, namely digit\nrecognition and aircraft collision avoidance, where the first case study serves\nto assess the resource utilisation of the tool, and the second to assess the\nability to verify versatile correctness properties.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 11:50:22 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["T\u00f6rnblom", "John", ""], ["Nadjm-Tehrani", "Simin", ""]]}, {"id": "2105.02613", "submitter": "Hamid Tabani", "authors": "Hamid Tabani, Ajay Balasubramaniam, Elahe Arani, Bahram Zonooz", "title": "Challenges and Obstacles Towards Deploying Deep Learning Models on\n  Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  From computer vision and speech recognition to forecasting trajectories in\nautonomous vehicles, deep learning approaches are at the forefront of so many\ndomains. Deep learning models are developed using plethora of high-level,\ngeneric frameworks and libraries. Running those models on the mobile devices\nrequire hardware-aware optimizations and in most cases converting the models to\nother formats or using a third-party framework. In reality, most of the\ndeveloped models need to undergo a process of conversion, adaptation, and, in\nsome cases, full retraining to match the requirements and features of the\nframework that is deploying the model on the target platform. Variety of\nhardware platforms with heterogeneous computing elements, from wearable devices\nto high-performance GPU clusters are used to run deep learning models. In this\npaper, we present the existing challenges, obstacles, and practical solutions\ntowards deploying deep learning models on mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 12:40:28 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Tabani", "Hamid", ""], ["Balasubramaniam", "Ajay", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2105.02625", "submitter": "Boyla Mainsah", "authors": "Anish Karpurapu, Adam Krekorian, Ye Tian, Leslie M. Collins, Ravi\n  Karra, Aaron Franklin and Boyla O. Mainsah", "title": "Evaluating the Effect of Longitudinal Dose and INR Data on Maintenance\n  Warfarin Dose Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Warfarin, a commonly prescribed drug to prevent blood clots, has a highly\nvariable individual response. Determining a maintenance warfarin dose that\nachieves a therapeutic blood clotting time, as measured by the international\nnormalized ratio (INR), is crucial in preventing complications. Machine\nlearning algorithms are increasingly being used for warfarin dosing; usually,\nan initial dose is predicted with clinical and genotype factors, and this dose\nis revised after a few days based on previous doses and current INR. Since a\nsequence of prior doses and INR better capture the variability in individual\nwarfarin response, we hypothesized that longitudinal dose response data will\nimprove maintenance dose predictions. To test this hypothesis, we analyzed a\ndataset from the COAG warfarin dosing study, which includes clinical data,\nwarfarin doses and INR measurements over the study period, and maintenance dose\nwhen therapeutic INR was achieved. Various machine learning regression models\nto predict maintenance warfarin dose were trained with clinical factors and\ndosing history and INR data as features. Overall, dose revision algorithms with\na single dose and INR achieved comparable performance as the baseline dose\nrevision algorithm. In contrast, dose revision algorithms with longitudinal\ndose and INR data provided maintenance dose predictions that were statistically\nsignificantly much closer to the true maintenance dose. Focusing on the best\nperforming model, gradient boosting (GB), the proportion of ideal estimated\ndose, i.e., defined as within $\\pm$20% of the true dose, increased from the\nbaseline (54.92%) to the GB model with the single (63.11%) and longitudinal\n(75.41%) INR. More accurate maintenance dose predictions with longitudinal dose\nresponse data can potentially achieve therapeutic INR faster, reduce\ndrug-related complications and improve patient outcomes with warfarin therapy.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:01:42 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Karpurapu", "Anish", ""], ["Krekorian", "Adam", ""], ["Tian", "Ye", ""], ["Collins", "Leslie M.", ""], ["Karra", "Ravi", ""], ["Franklin", "Aaron", ""], ["Mainsah", "Boyla O.", ""]]}, {"id": "2105.02626", "submitter": "Xingjian Zhen", "authors": "Varun Nagaraj Rao, Xingjian Zhen, Karen Hovsepian, Mingwei Shen", "title": "A First Look: Towards Explainable TextVQA Models via Visual and Textual\n  Explanations", "comments": "This paper is done when Xingjian was an intern in Amazon PARS group,\n  summer 2020. This paper is accepted by NAACL-MAI-Workshop, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable deep learning models are advantageous in many situations. Prior\nwork mostly provide unimodal explanations through post-hoc approaches not part\nof the original system design. Explanation mechanisms also ignore useful\ntextual information present in images. In this paper, we propose MTXNet, an\nend-to-end trainable multimodal architecture to generate multimodal\nexplanations, which focuses on the text in the image. We curate a novel dataset\nTextVQA-X, containing ground truth visual and multi-reference textual\nexplanations that can be leveraged during both training and evaluation. We then\nquantitatively show that training with multimodal explanations complements\nmodel performance and surpasses unimodal baselines by up to 7% in CIDEr scores\nand 2% in IoU. More importantly, we demonstrate that the multimodal\nexplanations are consistent with human interpretations, help justify the\nmodels' decision, and provide useful insights to help diagnose an incorrect\nprediction. Finally, we describe a real-world e-commerce application for using\nthe generated multimodal explanations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:36:17 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rao", "Varun Nagaraj", ""], ["Zhen", "Xingjian", ""], ["Hovsepian", "Karen", ""], ["Shen", "Mingwei", ""]]}, {"id": "2105.02637", "submitter": "Ryan-Rhys Griffiths", "authors": "Ryan-Rhys Griffiths, Philippe Schwaller, Alpha A. Lee", "title": "Dataset Bias in the Natural Sciences: A Case Study in Chemical Reaction\n  Prediction and Synthesis Design", "comments": "Presented at the 2018 NeurIPS Workshop on Machine Learning for\n  Molecules and Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets in the Natural Sciences are often curated with the goal of aiding\nscientific understanding and hence may not always be in a form that facilitates\nthe application of machine learning. In this paper, we identify three trends\nwithin the fields of chemical reaction prediction and synthesis design that\nrequire a change in direction. First, the manner in which reaction datasets are\nsplit into reactants and reagents encourages testing models in an\nunrealistically generous manner. Second, we highlight the prevalence of\nmislabelled data, and suggest that the focus should be on outlier removal\nrather than data fitting only. Lastly, we discuss the problem of reagent\nprediction, in addition to reactant prediction, in order to solve the full\nsynthesis design problem, highlighting the mismatch between what machine\nlearning solves and what a lab chemist would need. Our critiques are also\nrelevant to the burgeoning field of using machine learning to accelerate\nprogress in experimental Natural Sciences, where datasets are often split in a\nbiased way, are highly noisy, and contextual variables that are not evident\nfrom the data strongly influence the outcome of experiments.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:11:56 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Griffiths", "Ryan-Rhys", ""], ["Schwaller", "Philippe", ""], ["Lee", "Alpha A.", ""]]}, {"id": "2105.02652", "submitter": "Bicheng Yan", "authors": "Bicheng Yan, Dylan Robert Harp, Rajesh J. Pawar", "title": "A Gradient-based Deep Neural Network Model for Simulating Multiphase\n  Flow in Porous Media", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Simulation of multiphase flow in porous media is crucial for the effective\nmanagement of subsurface energy and environment related activities. The\nnumerical simulators used for modeling such processes rely on spatial and\ntemporal discretization of the governing partial-differential equations (PDEs)\ninto algebraic systems via numerical methods. These simulators usually require\ndedicated software development and maintenance, and suffer low efficiency from\na runtime and memory standpoint. Therefore, developing cost-effective,\ndata-driven models can become a practical choice since deep learning approaches\nare considered to be universal approximations. In this paper, we describe a\ngradient-based deep neural network (GDNN) constrained by the physics related to\nmultiphase flow in porous media. We tackle the nonlinearity of flow in porous\nmedia induced by rock heterogeneity, fluid properties and fluid-rock\ninteractions by decomposing the nonlinear PDEs into a dictionary of elementary\ndifferential operators. We use a combination of operators to handle rock\nspatial heterogeneity and fluid flow by advection. Since the augmented\ndifferential operators are inherently related to the physics of fluid flow, we\ntreat them as first principles prior knowledge to regularize the GDNN training.\nWe use the example of pressure management at geologic CO2 storage sites, where\nCO2 is injected in saline aquifers and brine is produced, and apply GDNN to\nconstruct a predictive model that is trained from physics-based simulation data\nand emulates the physics process. We demonstrate that GDNN can effectively\npredict the nonlinear patterns of subsurface responses including the\ntemporal-spatial evolution of the pressure and saturation plumes. GDNN has\ngreat potential to tackle challenging problems that are governed by highly\nnonlinear physics and enables development of data-driven models with higher\nfidelity.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:14:00 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yan", "Bicheng", ""], ["Harp", "Dylan Robert", ""], ["Pawar", "Rajesh J.", ""]]}, {"id": "2105.02653", "submitter": "Yanzhe Bekkemoen", "authors": "Yanzhe Bekkemoen, Helge Langseth", "title": "Correcting Classification: A Bayesian Framework Using Explanation\n  Feedback to Improve Classification Abilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks (NNs) have shown high predictive performance, however, with\nshortcomings. Firstly, the reasons behind the classifications are not fully\nunderstood. Several explanation methods have been developed, but they do not\nprovide mechanisms for users to interact with the explanations. Explanations\nare social, meaning they are a transfer of knowledge through interactions.\nNonetheless, current explanation methods contribute only to one-way\ncommunication. Secondly, NNs tend to be overconfident, providing unreasonable\nuncertainty estimates on out-of-distribution observations. We overcome these\ndifficulties by training a Bayesian convolutional neural network (CNN) that\nuses explanation feedback. After training, the model presents explanations of\ntraining sample classifications to an annotator. Based on the provided\ninformation, the annotator can accept or reject the explanations by providing\nfeedback. Our proposed method utilizes this feedback for fine-tuning to correct\nthe model such that the explanations and classifications improve. We use\nexisting CNN architectures to demonstrate the method's effectiveness on one toy\ndataset (decoy MNIST) and two real-world datasets (Dogs vs. Cats and ISIC skin\ncancer). The experiments indicate that few annotated explanations and\nfine-tuning epochs are needed to improve the model and predictive performance,\nmaking the model more trustworthy and understandable.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 13:59:21 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 15:37:31 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Bekkemoen", "Yanzhe", ""], ["Langseth", "Helge", ""]]}, {"id": "2105.02656", "submitter": "Khalid Alhazmi", "authors": "Khalid Alhazmi, Fahad Albalawi, and S. Mani Sarathy", "title": "A Reinforcement Learning-based Economic Model Predictive Control\n  Framework for Autonomous Operation of Chemical Reactors", "comments": "This work has been submitted to Chemical Engineering Journal (CEJ) in\n  June 2020 and currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Economic model predictive control (EMPC) is a promising methodology for\noptimal operation of dynamical processes that has been shown to improve process\neconomics considerably. However, EMPC performance relies heavily on the\naccuracy of the process model used. As an alternative to model-based control\nstrategies, reinforcement learning (RL) has been investigated as a model-free\ncontrol methodology, but issues regarding its safety and stability remain an\nopen research challenge. This work presents a novel framework for integrating\nEMPC and RL for online model parameter estimation of a class of nonlinear\nsystems. In this framework, EMPC optimally operates the closed loop system\nwhile maintaining closed loop stability and recursive feasibility. At the same\ntime, to optimize the process, the RL agent continuously compares the measured\nstate of the process with the model's predictions (nominal states), and\nmodifies model parameters accordingly. The major advantage of this framework is\nits simplicity; state-of-the-art RL algorithms and EMPC schemes can be employed\nwith minimal modifications. The performance of the proposed framework is\nillustrated on a network of reactions with challenging dynamics and practical\nsignificance. This framework allows control, optimization, and model correction\nto be performed online and continuously, making autonomous reactor operation\nmore attainable.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:34:30 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Alhazmi", "Khalid", ""], ["Albalawi", "Fahad", ""], ["Sarathy", "S. Mani", ""]]}, {"id": "2105.02671", "submitter": "Mahesh Banavar", "authors": "Mahesh K. Banavar, Shandeepa Wickramasinghe, Monalisa Achalla, Jie Sun", "title": "Ordinal UNLOC: Target Localization with Noisy and Incomplete Distance\n  Measures", "comments": "This manuscript has been accepted by the IEEE Internet of Things\n  Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A main challenge in target localization arises from the lack of reliable\ndistance measures. This issue is especially pronounced in indoor settings due\nto the presence of walls, floors, furniture, and other dynamically changing\nconditions such as the movement of people and goods, varying temperature, and\nairflows. Here, we develop a new computational framework to estimate the\nlocation of a target without the need for reliable distance measures. The\nmethod, which we term Ordinal UNLOC, uses only ordinal data obtained from\ncomparing the signal strength from anchor pairs at known locations to the\ntarget. Our estimation technique utilizes rank aggregation, function learning\nas well as proximity-based unfolding optimization. As a result, it yields\naccurate target localization for common transmission models with unknown\nparameters and noisy observations that are reminiscent of practical settings.\nOur results are validated by both numerical simulations and hardware\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:54:31 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Banavar", "Mahesh K.", ""], ["Wickramasinghe", "Shandeepa", ""], ["Achalla", "Monalisa", ""], ["Sun", "Jie", ""]]}, {"id": "2105.02675", "submitter": "Ali Shojaie", "authors": "Ali Shojaie and Emily B. Fox", "title": "Granger Causality: A Review and Recent Advances", "comments": "40 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Introduced more than a half century ago, Granger causality has become a\npopular tool for analyzing time series data in many application domains, from\neconomics and finance to genomics and neuroscience. Despite this popularity,\nthe validity of this notion for inferring causal relationships among time\nseries has remained the topic of continuous debate. Moreover, while the\noriginal definition was general, limitations in computational tools have\nprimarily limited the applications of Granger causality to simple bivariate\nvector auto-regressive processes or pairwise relationships among a set of\nvariables. Starting with a review of early developments and debates, this paper\ndiscusses recent advances that address various shortcomings of the earlier\napproaches, from models for high-dimensional time series to more recent\ndevelopments that account for nonlinear and non-Gaussian observations and allow\nfor sub-sampled and mixed frequency time series.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:37:18 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 02:38:08 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Shojaie", "Ali", ""], ["Fox", "Emily B.", ""]]}, {"id": "2105.02693", "submitter": "Hongbo Zhang Dr.", "authors": "Jia-Xing Zhong, Hongbo Zhang", "title": "Uncertainty-aware INVASE: Enhanced Breast Cancer Diagnosis Feature\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an uncertainty-aware INVASE to quantify predictive\nconfidence of healthcare problem. By introducing learnable Gaussian\ndistributions, we lever-age their variances to measure the degree of\nuncertainty. Based on the vanilla INVASE, two additional modules are proposed,\ni.e., an uncertainty quantification module in the predictor, and a reward\nshaping module in the selector. We conduct extensive experiments on UCI-WDBC\ndataset. Notably, our method eliminates almost all predictive bias with only\nabout 20% queries, while the uncertainty-agnostic counterpart requires nearly\n100% queries. The open-source implementation with a detailed tutorial is\navailable at\nhttps://github.com/jx-zhong-for-academic-purpose/Uncertainty-aware-INVASE/blob/main/tutorialinvase%2B.ipynb.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:30:33 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhong", "Jia-Xing", ""], ["Zhang", "Hongbo", ""]]}, {"id": "2105.02702", "submitter": "Yohei Kawaguchi", "authors": "Ryo Tanabe, Harsh Purohit, Kota Dohi, Takashi Endo, Yuki Nikaido,\n  Toshiki Nakamura, and Yohei Kawaguchi", "title": "MIMII DUE: Sound Dataset for Malfunctioning Industrial Machine\n  Investigation and Inspection with Domain Shifts due to Changes in Operational\n  and Environmental Conditions", "comments": "5 pages, under review for WASPAA 2021, disambiguation (in 2.2, sound\n  proof room -> sound isolation booth, anechoic room -> anechoic chamber)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new dataset for malfunctioning industrial\nmachine investigation and inspection with domain shifts due to changes in\noperational and environmental conditions (MIMII DUE). Conventional methods for\nanomalous sound detection face challenges in practice because the distribution\nof features changes between the training and operational phases (called domain\nshift) due to some real-world factors. To check the robustness against domain\nshifts, we need a dataset with domain shifts, but such a dataset does not exist\nso far. The new dataset consists of normal and abnormal operating sounds of\nindustrial machines of five different types under two different\noperational/environmental conditions (source domain and target domain)\nindependent of normal/abnormal, with domain shifts occurring between the two\ndomains. Experimental results show significant performance differences between\nthe source and target domains, and the dataset contains the domain shifts.\nThese results indicate that the dataset will be helpful to check the robustness\nagainst domain shifts. The dataset is a subset of the dataset for DCASE 2021\nChallenge Task 2 and freely available for download at\nhttps://zenodo.org/record/4740355\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:18:24 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 13:56:38 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Tanabe", "Ryo", ""], ["Purohit", "Harsh", ""], ["Dohi", "Kota", ""], ["Endo", "Takashi", ""], ["Nikaido", "Yuki", ""], ["Nakamura", "Toshiki", ""], ["Kawaguchi", "Yohei", ""]]}, {"id": "2105.02711", "submitter": "Chaoqi Yang", "authors": "Chaoqi Yang, Cao Xiao, Fenglong Ma, Lucas Glass, Jimeng Sun", "title": "SafeDrug: Dual Molecular Graph Encoders for Safe Drug Recommendations", "comments": "Accepted in IJCAI 2021, this is the full version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medication recommendation is an essential task of AI for healthcare. Existing\nworks focused on recommending drug combinations for patients with complex\nhealth conditions solely based on their electronic health records. Thus, they\nhave the following limitations: (1) some important data such as drug molecule\nstructures have not been utilized in the recommendation process. (2) drug-drug\ninteractions (DDI) are modeled implicitly, which can lead to sub-optimal\nresults. To address these limitations, we propose a DDI-controllable drug\nrecommendation model named SafeDrug to leverage drugs' molecule structures and\nmodel DDIs explicitly. SafeDrug is equipped with a global message passing\nneural network (MPNN) module and a local bipartite learning module to fully\nencode the connectivity and functionality of drug molecules. SafeDrug also has\na controllable loss function to control DDI levels in the recommended drug\ncombinations effectively. On a benchmark dataset, our SafeDrug is relatively\nshown to reduce DDI by 19.43% and improves 2.88% on Jaccard similarity between\nrecommended and actually prescribed drug combinations over previous approaches.\nMoreover, SafeDrug also requires much fewer parameters than previous deep\nlearning-based approaches, leading to faster training by about 14% and around\n2x speed-up in inference.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 00:20:48 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yang", "Chaoqi", ""], ["Xiao", "Cao", ""], ["Ma", "Fenglong", ""], ["Glass", "Lucas", ""], ["Sun", "Jimeng", ""]]}, {"id": "2105.02716", "submitter": "Hidenori Tanaka", "authors": "Hidenori Tanaka, Daniel Kunin", "title": "Noether's Learning Dynamics: The Role of Kinetic Symmetry Breaking in\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nature, symmetry governs regularities, while symmetry breaking brings\ntexture. Here, we reveal a novel role of symmetry breaking behind efficiency\nand stability in learning, a critical issue in machine learning. Recent\nexperiments suggest that the symmetry of the loss function is closely related\nto the learning performance. This raises a fundamental question. Is such\nsymmetry beneficial, harmful, or irrelevant to the success of learning? Here,\nwe demystify this question and pose symmetry breaking as a new design principle\nby considering the symmetry of the learning rule in addition to the loss\nfunction. We model the discrete learning dynamics using a continuous-time\nLagrangian formulation, in which the learning rule corresponds to the kinetic\nenergy and the loss function corresponds to the potential energy. We identify\nkinetic asymmetry unique to learning systems, where the kinetic energy often\ndoes not have the same symmetry as the potential (loss) function reflecting the\nnon-physical symmetries of the loss function and the non-Euclidean metric used\nin learning rules. We generalize Noether's theorem known in physics to\nexplicitly take into account this kinetic asymmetry and derive the resulting\nmotion of the Noether charge. Finally, we apply our theory to modern deep\nnetworks with normalization layers and reveal a mechanism of implicit adaptive\noptimization induced by the kinetic symmetry breaking.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:36:10 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Tanaka", "Hidenori", ""], ["Kunin", "Daniel", ""]]}, {"id": "2105.02725", "submitter": "Ahmad Khajenezhad", "authors": "Ahmad Khajehnejad, Moein Khajehnejad, Mahmoudreza Babaei, Krishna P.\n  Gummadi, Adrian Weller, Baharan Mirzasoleiman", "title": "CrossWalk: Fairness-enhanced Node Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The potential for machine learning systems to amplify social inequities and\nunfairness is receiving increasing popular and academic attention. Much recent\nwork has focused on developing algorithmic tools to assess and mitigate such\nunfairness. However, there is little work on enhancing fairness in graph\nalgorithms. Here, we develop a simple, effective and general method, CrossWalk,\nthat enhances fairness of various graph algorithms, including influence\nmaximization, link prediction and node classification, applied to node\nembeddings. CrossWalk is applicable to any random walk based node\nrepresentation learning algorithm, such as DeepWalk and Node2Vec. The key idea\nis to bias random walks to cross group boundaries, by upweighting edges which\n(1) are closer to the groups' peripheries or (2) connect different groups in\nthe network. CrossWalk pulls nodes that are near groups' peripheries towards\ntheir neighbors from other groups in the embedding space, while preserving the\nnecessary structural information from the graph. Extensive experiments show the\neffectiveness of our algorithm to enhance fairness in various graph algorithms,\nincluding influence maximization, link prediction and node classification in\nsynthetic and real networks, with only a very small decrease in performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:45:34 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Khajehnejad", "Ahmad", ""], ["Khajehnejad", "Moein", ""], ["Babaei", "Mahmoudreza", ""], ["Gummadi", "Krishna P.", ""], ["Weller", "Adrian", ""], ["Mirzasoleiman", "Baharan", ""]]}, {"id": "2105.02726", "submitter": "Marvin Lerousseau", "authors": "Marvin Lerousseau and Maria Vakalopoulou and Nikos Paragios and Eric\n  Deutsch", "title": "Sparse convolutional context-aware multiple instance learning for whole\n  slide image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Whole slide microscopic slides display many cues about the underlying tissue\nguiding diagnostic and the choice of therapy for many diseases. However, their\nenormous size often in gigapixels hampers the use of traditional neural network\narchitectures. To tackle this issue, multiple instance learning (MIL)\nclassifies bags of patches instead of whole slide images. Most MIL strategies\nconsider that patches are independent and identically distributed. Our approach\npresents a paradigm shift through the integration of spatial information of\npatches with a sparse-input convolutional-based MIL strategy. The formulated\nframework is generic, flexible, scalable and is the first to introduce\ncontextual dependencies between decisions taken at the patch level. It achieved\nstate-of-the-art performance in pan-cancer subtype classification. The code of\nthis work will be made available.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:46:09 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lerousseau", "Marvin", ""], ["Vakalopoulou", "Maria", ""], ["Paragios", "Nikos", ""], ["Deutsch", "Eric", ""]]}, {"id": "2105.02730", "submitter": "Yi Wang", "authors": "Kun Lei, Peng Guo, Yi Wang, Xiao Wu, Wenchao Zhao", "title": "Solve routing problems with a residual edge-graph attention neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For NP-hard combinatorial optimization problems, it is usually difficult to\nfind high-quality solutions in polynomial time. The design of either an exact\nalgorithm or an approximate algorithm for these problems often requires\nsignificantly specialized knowledge. Recently, deep learning methods provide\nnew directions to solve such problems. In this paper, an end-to-end deep\nreinforcement learning framework is proposed to solve this type of\ncombinatorial optimization problems. This framework can be applied to different\nproblems with only slight changes of input (for example, for a traveling\nsalesman problem (TSP), the input is the two-dimensional coordinates of nodes;\nwhile for a capacity-constrained vehicle routing problem (CVRP), the input is\nsimply changed to three-dimensional vectors including the two-dimensional\ncoordinates and the customer demands of nodes), masks and decoder context\nvectors. The proposed framework is aiming to improve the models in literacy in\nterms of the neural network model and the training algorithm. The solution\nquality of TSP and the CVRP up to 100 nodes are significantly improved via our\nframework. Specifically, the average optimality gap is reduced from 4.53\\%\n(reported best \\cite{R22}) to 3.67\\% for TSP with 100 nodes and from 7.34\\%\n(reported best \\cite{R22}) to 6.68\\% for CVRP with 100 nodes when using the\ngreedy decoding strategy. Furthermore, our framework uses about 1/3$\\sim$3/4\ntraining samples compared with other existing learning methods while achieving\nbetter results. The results performed on randomly generated instances and the\nbenchmark instances from TSPLIB and CVRPLIB confirm that our framework has a\nlinear running time on the problem size (number of nodes) during the testing\nphase, and has a good generalization performance from random instance training\nto real-world instance testing.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:47:47 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lei", "Kun", ""], ["Guo", "Peng", ""], ["Wang", "Yi", ""], ["Wu", "Xiao", ""], ["Zhao", "Wenchao", ""]]}, {"id": "2105.02752", "submitter": "Arlindo Oliveira L", "authors": "M\\'ario Cardoso, Andr\\'e Cavalheiro, Alexandre Borges, Ana F. Duarte,\n  Am\\'ilcar Soares, Maria Jo\\~ao Pereira, Nuno J. Nunes, Leonardo Azevedo,\n  Arlindo L. Oliveira", "title": "Modeling the geospatial evolution of COVID-19 using spatio-temporal\n  convolutional sequence-to-sequence neural networks", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Europe was hit hard by the COVID-19 pandemic and Portugal was one of the most\naffected countries, having suffered three waves in the first twelve months.\nApproximately between Jan 19th and Feb 5th 2021 Portugal was the country in the\nworld with the largest incidence rate, with 14-days incidence rates per 100,000\ninhabitants in excess of 1000. Despite its importance, accurate prediction of\nthe geospatial evolution of COVID-19 remains a challenge, since existing\nanalytical methods fail to capture the complex dynamics that result from both\nthe contagion within a region and the spreading of the infection from infected\nneighboring regions.\n  We use a previously developed methodology and official municipality level\ndata from the Portuguese Directorate-General for Health (DGS), relative to the\nfirst twelve months of the pandemic, to compute an estimate of the incidence\nrate in each location of mainland Portugal. The resulting sequence of incidence\nrate maps was then used as a gold standard to test the effectiveness of\ndifferent approaches in the prediction of the spatial-temporal evolution of the\nincidence rate. Four different methods were tested: a simple cell level\nautoregressive moving average (ARMA) model, a cell level vector autoregressive\n(VAR) model, a municipality-by-municipality compartmental SIRD model followed\nby direct block sequential simulation and a convolutional sequence-to-sequence\nneural network model based on the STConvS2S architecture. We conclude that the\nconvolutional sequence-to-sequence neural network is the best performing\nmethod, when predicting the medium-term future incidence rate, using the\navailable information.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:24:00 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Cardoso", "M\u00e1rio", ""], ["Cavalheiro", "Andr\u00e9", ""], ["Borges", "Alexandre", ""], ["Duarte", "Ana F.", ""], ["Soares", "Am\u00edlcar", ""], ["Pereira", "Maria Jo\u00e3o", ""], ["Nunes", "Nuno J.", ""], ["Azevedo", "Leonardo", ""], ["Oliveira", "Arlindo L.", ""]]}, {"id": "2105.02755", "submitter": "Bechir Hamdaoui", "authors": "Bechir Hamdaoui, Abdurrahman Elmaghbub, Seifeddine Mejri", "title": "Deep Neural Network Feature Designs for RF Data-Driven Wireless Device\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/MNET.011.2000492", "report-no": null, "categories": "eess.SP cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most prior works on deep learning-based wireless device classification using\nradio frequency (RF) data apply off-the-shelf deep neural network (DNN) models,\nwhich were matured mainly for domains like vision and language. However,\nwireless RF data possesses unique characteristics that differentiate it from\nthese other domains. For instance, RF data encompasses intermingled time and\nfrequency features that are dictated by the underlying hardware and protocol\nconfigurations. In addition, wireless RF communication signals exhibit\ncyclostationarity due to repeated patterns (PHY pilots, frame prefixes, etc.)\nthat these signals inherently contain. In this paper, we begin by explaining\nand showing the unsuitability as well as limitations of existing DNN feature\ndesign approaches currently proposed to be used for wireless device\nclassification. We then present novel feature design approaches that exploit\nthe distinct structures of the RF communication signals and the spectrum\nemissions caused by transmitter hardware impairments to custom-make DNN models\nsuitable for classifying wireless devices using RF signal data. Our proposed\nDNN feature designs substantially improve classification robustness in terms of\nscalability, accuracy, signature anti-cloning, and insensitivity to environment\nperturbations. We end the paper by presenting other feature design strategies\nthat have great potentials for providing further performance improvements of\nthe DNN-based wireless device classification, and discuss the open research\nchallenges related to these proposed strategies.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 20:19:05 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hamdaoui", "Bechir", ""], ["Elmaghbub", "Abdurrahman", ""], ["Mejri", "Seifeddine", ""]]}, {"id": "2105.02761", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Petar Veli\\v{c}kovi\\'c, Charles Blundell", "title": "Neural Algorithmic Reasoning", "comments": "Accepted as an Opinion paper in Patterns. 7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms have been fundamental to recent global technological advances and,\nin particular, they have been the cornerstone of technical advances in one\nfield rapidly being applied to another. We argue that algorithms possess\nfundamentally different qualities to deep learning methods, and this strongly\nsuggests that, were deep learning methods better able to mimic algorithms,\ngeneralisation of the sort seen with algorithms would become possible with deep\nlearning -- something far out of the reach of current machine learning methods.\nFurthermore, by representing elements in a continuous space of learnt\nalgorithms, neural networks are able to adapt known algorithms more closely to\nreal-world problems, potentially finding more efficient and pragmatic solutions\nthan those proposed by human computer scientists.\n  Here we present neural algorithmic reasoning -- the art of building neural\nnetworks that are able to execute algorithmic computation -- and provide our\nopinion on its transformative potential for running classical algorithms on\ninputs previously considered inaccessible to them.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:33:57 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Veli\u010dkovi\u0107", "Petar", ""], ["Blundell", "Charles", ""]]}, {"id": "2105.02769", "submitter": "Yaroslav Ganin", "authors": "Yaroslav Ganin, Sergey Bartunov, Yujia Li, Ethan Keller, Stefano\n  Saliceti", "title": "Computer-Aided Design as Language", "comments": "24 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-Aided Design (CAD) applications are used in manufacturing to model\neverything from coffee mugs to sports cars. These programs are complex and\nrequire years of training and experience to master. A component of all CAD\nmodels particularly difficult to make are the highly structured 2D sketches\nthat lie at the heart of every 3D construction. In this work, we propose a\nmachine learning model capable of automatically generating such sketches.\nThrough this, we pave the way for developing intelligent tools that would help\nengineers create better designs with less effort. Our method is a combination\nof a general-purpose language modeling technique alongside an off-the-shelf\ndata serialization protocol. We show that our approach has enough flexibility\nto accommodate the complexity of the domain and performs well for both\nunconditional synthesis and image-to-sketch translation.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:43:10 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Bartunov", "Sergey", ""], ["Li", "Yujia", ""], ["Keller", "Ethan", ""], ["Saliceti", "Stefano", ""]]}, {"id": "2105.02777", "submitter": "Xintao Yan", "authors": "Xintao Yan, Yan Zhao, Henry X. Liu", "title": "A probabilistic model for missing traffic volume reconstruction based on\n  data fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic volume information is critical for intelligent transportation\nsystems. It serves as a key input to transportation planning, roadway design,\nand traffic signal control. However, the traffic volume data collected by\nfixed-location sensors, such as loop detectors, often suffer from the missing\ndata problem and low coverage problem. The missing data problem could be caused\nby hardware malfunction. The low coverage problem is due to the limited\ncoverage of fixed-location sensors in the transportation network, which\nrestrains our understanding of the traffic at the network level. To tackle\nthese problems, we propose a probabilistic model for traffic volume\nreconstruction by fusing fixed-location sensor data and probe vehicle data. We\napply the probabilistic principal component analysis (PPCA) to capture the\ncorrelations in traffic volume data. An innovative contribution of this work is\nthat we also integrate probe vehicle data into the framework, which allows the\nmodel to solve both of the above-mentioned two problems. Using a real-world\ntraffic volume dataset, we show that the proposed method outperforms\nstate-of-the-art methods for the extensively studied missing data problem.\nMoreover, for the low coverage problem, which cannot be handled by most\nexisting methods, the proposed model can also achieve high accuracy. The\nexperiments also show that even when the missing ratio reaches 80%, the\nproposed method can still give an accurate estimate of the unknown traffic\nvolumes with only a 10% probe vehicle penetration rate. The results validate\nthe effectiveness and robustness of the proposed model and demonstrate its\npotential for practical applications.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:16:35 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yan", "Xintao", ""], ["Zhao", "Yan", ""], ["Liu", "Henry X.", ""]]}, {"id": "2105.02786", "submitter": "Yi Ding", "authors": "Yi Ding, Neethu Robinson, Qiuhao Zeng, Cuntai Guan", "title": "LGGNet: Learning from Local-Global-Graph Representations for\n  Brain-Computer Interface", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose LGG, a neurologically inspired graph neural\nnetwork, to learn local-global-graph representations from\nElectroencephalography (EEG) for a Brain-Computer Interface (BCI). A temporal\nconvolutional layer with multi-scale 1D convolutional kernels and kernel-level\nattention fusion is proposed to learn the temporal dynamics of EEG. Inspired by\nneurological knowledge of cognitive processes in the brain, we propose local\nand global graph-filtering layers to learn the brain activities within and\nbetween different functional areas of the brain to model the complex relations\namong them during the cognitive processes. Under the robust nested\ncross-validation settings, the proposed method is evaluated on the publicly\navailable dataset DEAP, and the classification performance is compared with\nstate-of-the-art methods, such as FBFgMDM, FBTSC, Unsupervised learning,\nDeepConvNet, ShallowConvNet, EEGNet, and TSception. The results show that the\nproposed method outperforms all these state-of-the-art methods, and the\nimprovements are statistically significant (p<0.05) in most cases. The source\ncode can be found at: https://github.com/yi-ding-cs/LGG\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:06:34 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ding", "Yi", ""], ["Robinson", "Neethu", ""], ["Zeng", "Qiuhao", ""], ["Guan", "Cuntai", ""]]}, {"id": "2105.02788", "submitter": "Julien Martel", "authors": "Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan,\n  Marco Monteiro and Gordon Wetzstein", "title": "ACORN: Adaptive Coordinate Networks for Neural Scene Representation", "comments": "J. N. P. Martel and D. B. Lindell equally contributed to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural representations have emerged as a new paradigm for applications in\nrendering, imaging, geometric modeling, and simulation. Compared to traditional\nrepresentations such as meshes, point clouds, or volumes they can be flexibly\nincorporated into differentiable learning-based pipelines. While recent\nimprovements to neural representations now make it possible to represent\nsignals with fine details at moderate resolutions (e.g., for images and 3D\nshapes), adequately representing large-scale or complex scenes has proven a\nchallenge. Current neural representations fail to accurately represent images\nat resolutions greater than a megapixel or 3D scenes with more than a few\nhundred thousand polygons. Here, we introduce a new hybrid implicit-explicit\nnetwork architecture and training strategy that adaptively allocates resources\nduring training and inference based on the local complexity of a signal of\ninterest. Our approach uses a multiscale block-coordinate decomposition,\nsimilar to a quadtree or octree, that is optimized during training. The network\narchitecture operates in two stages: using the bulk of the network parameters,\na coordinate encoder generates a feature grid in a single forward pass. Then,\nhundreds or thousands of samples within each block can be efficiently evaluated\nusing a lightweight feature decoder. With this hybrid implicit-explicit network\narchitecture, we demonstrate the first experiments that fit gigapixel images to\nnearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in\nscale of over 1000x compared to the resolution of previously demonstrated\nimage-fitting experiments. Moreover, our approach is able to represent 3D\nshapes significantly faster and better than previous techniques; it reduces\ntraining times from days to hours or minutes and memory requirements by over an\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:21:38 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Martel", "Julien N. P.", ""], ["Lindell", "David B.", ""], ["Lin", "Connor Z.", ""], ["Chan", "Eric R.", ""], ["Monteiro", "Marco", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2105.02796", "submitter": "Christian Fiedler", "authors": "Christian Fiedler, Carsten W. Scherer, Sebastian Trimpe", "title": "Practical and Rigorous Uncertainty Bounds for Gaussian Process\n  Regression", "comments": "Contains supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Process Regression is a popular nonparametric regression method\nbased on Bayesian principles that provides uncertainty estimates for its\npredictions. However, these estimates are of a Bayesian nature, whereas for\nsome important applications, like learning-based control with safety\nguarantees, frequentist uncertainty bounds are required. Although such rigorous\nbounds are available for Gaussian Processes, they are too conservative to be\nuseful in applications. This often leads practitioners to replacing these\nbounds by heuristics, thus breaking all theoretical guarantees. To address this\nproblem, we introduce new uncertainty bounds that are rigorous, yet practically\nuseful at the same time. In particular, the bounds can be explicitly evaluated\nand are much less conservative than state of the art results. Furthermore, we\nshow that certain model misspecifications lead to only graceful degradation. We\ndemonstrate these advantages and the usefulness of our results for\nlearning-based control with numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:41:04 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Fiedler", "Christian", ""], ["Scherer", "Carsten W.", ""], ["Trimpe", "Sebastian", ""]]}, {"id": "2105.02808", "submitter": "Lara Orlandic", "authors": "Lara Orlandic, Adriana Arza Valdes, David Atienza", "title": "Wearable and Continuous Prediction of Passage of Time Perception for\n  Monitoring Mental Health", "comments": null, "journal-ref": null, "doi": "10.1109/CBMS52027.2021.00050", "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A person's passage of time perception (POTP) is strongly linked to their\nmental state and stress response, and can therefore provide an easily\nquantifiable means of continuous mental health monitoring. In this work, we\ndevelop a custom experiment and Machine Learning (ML) models for predicting\nPOTP from biomarkers acquired from wearable biosensors. We first confirm that\nindividuals experience time passing slower than usual during fear or sadness (p\n= 0.046) and faster than usual during cognitive tasks (p = 2 x 10^-5). Then, we\ngroup together the experimental segments associated with fast, slow, and normal\nPOTP, and train a ML model to classify between these states based on a person's\nbiomarkers. The classifier had a weighted average F-1 score of 79%, with the\nfast-passing time class having the highest F-1 score of 93%. Next, we classify\neach individual's POTP regardless of the task at hand, achieving an F-1 score\nof 77.1% when distinguishing time passing faster rather than slower than usual.\nIn the two classifiers, biomarkers derived from the respiration,\nelectrocardiogram, skin conductance, and skin temperature signals contributed\nmost to the classifier output, thus enabling real-time POTP monitoring using\nnoninvasive, wearable biosensors.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 11:05:21 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Orlandic", "Lara", ""], ["Valdes", "Adriana Arza", ""], ["Atienza", "David", ""]]}, {"id": "2105.02810", "submitter": "Bryar Hassan Dr.", "authors": "Bryar A. Hassan, TarikA. Rashid, Seyedali Mirjalili", "title": "Performance evaluation results of evolutionary clustering algorithm star\n  for clustering heterogeneous datasets", "comments": null, "journal-ref": null, "doi": "10.1016/j.dib.2021.107044", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article presents the data used to evaluate the performance of\nevolutionary clustering algorithm star (ECA*) compared to five traditional and\nmodern clustering algorithms. Two experimental methods are employed to examine\nthe performance of ECA* against genetic algorithm for clustering++\n(GENCLUST++), learning vector quantisation (LVQ) , expectation maximisation\n(EM) , K-means++ (KM++) and K-means (KM). These algorithms are applied to 32\nheterogenous and multi-featured datasets to determine which one performs well\non the three tests. For one, ther paper examines the efficiency of ECA* in\ncontradiction of its corresponding algorithms using clustering evaluation\nmeasures. These validation criteria are objective function and cluster quality\nmeasures. For another, it suggests a performance rating framework to measurethe\nthe performance sensitivity of these algorithms on varos dataset features\n(cluster dimensionality, number of clusters, cluster overlap, cluster shape and\ncluster structure). The contributions of these experiments are two-folds: (i)\nECA* exceeds its counterpart aloriths in ability to find out the right cluster\nnumber; (ii) ECA* is less sensitive towards dataset features compared to its\ncompetitive techniques. Nonetheless, the results of the experiments performed\ndemonstrate some limitations in the ECA*: (i) ECA* is not fully applied based\non the premise that no prior knowledge exists; (ii) Adapting and utilising ECA*\non several real applications has not been achieved yet.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:17:19 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hassan", "Bryar A.", ""], ["Rashid", "TarikA.", ""], ["Mirjalili", "Seyedali", ""]]}, {"id": "2105.02813", "submitter": "Subhayan De", "authors": "Subhayan De, Bhuiyan Shameem Mahmood Ebna Hai, Alireza Doostan, Markus\n  Bause", "title": "Prediction of Ultrasonic Guided Wave Propagation in Solid-fluid and\n  their Interface under Uncertainty using Machine Learning", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural health monitoring (SHM) systems use the non-destructive testing\nprinciple for damage identification. As part of SHM, the propagation of\nultrasonic guided waves (UGWs) is tracked and analyzed for the changes in the\nassociated wave pattern. These changes help identify the location of a\nstructural damage, if any. We advance existing research by accounting for\nuncertainty in the material and geometric properties of a structure. The\nphysics model used in this study comprises of a monolithically coupled system\nof acoustic and elastic wave equations, known as the wave propagation in\nfluid-solid and their interface (WpFSI) problem. As the UGWs propagate in the\nsolid, fluid, and their interface, the wave signal displacement measurements\nare contrasted against the benchmark pattern. For the numerical solution, we\ndevelop an efficient algorithm that successfully addresses the inherent\ncomplexity of solving the multiphysics problem under uncertainty. We present a\nprocedure that uses Gaussian process regression and convolutional neural\nnetwork for predicting the UGW propagation in a solid-fluid and their interface\nunder uncertainty. First, a set of training images for different realizations\nof the uncertain parameters of the inclusion inside the structure is generated\nusing a monolithically-coupled system of acoustic and elastic wave equations.\nNext, Gaussian processes trained with these images are used for predicting the\npropagated wave with convolutional neural networks for further enhancement to\nproduce high-quality images of the wave patterns for new realizations of the\nuncertainty. The results indicate that the proposed approach provides an\naccurate prediction for the WpFSI problem in the presence of uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 01:05:14 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["De", "Subhayan", ""], ["Hai", "Bhuiyan Shameem Mahmood Ebna", ""], ["Doostan", "Alireza", ""], ["Bause", "Markus", ""]]}, {"id": "2105.02816", "submitter": "Aria Nosratinia", "authors": "Mohammad Esmaeili and Hussein Metwaly Saad and Aria Nosratinia", "title": "Semidefinite Programming for Community Detection with Side Information", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper produces an efficient Semidefinite Programming (SDP) solution for\ncommunity detection that incorporates non-graph data, which in this context is\nknown as side information. SDP is an efficient solution for standard community\ndetection on graphs. We formulate a semi-definite relaxation for the maximum\nlikelihood estimation of node labels, subject to observing both graph and\nnon-graph data. This formulation is distinct from the SDP solution of standard\ncommunity detection, but maintains its desirable properties. We calculate the\nexact recovery threshold for three types of non-graph information, which in\nthis paper are called side information: partially revealed labels, noisy\nlabels, as well as multiple observations (features) per node with arbitrary but\nfinite cardinality. We find that SDP has the same exact recovery threshold in\nthe presence of side information as maximum likelihood with side information.\nThus, the methods developed herein are computationally efficient as well as\nasymptotically accurate for the solution of community detection in the presence\nof side information. Simulations show that the asymptotic results of this paper\ncan also shed light on the performance of SDP for graphs of modest size.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:00:34 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Esmaeili", "Mohammad", ""], ["Saad", "Hussein Metwaly", ""], ["Nosratinia", "Aria", ""]]}, {"id": "2105.02823", "submitter": "Ziyu Wang", "authors": "Ziyu Wang, Jie Yang and Mohamad Sawan", "title": "A Novel Multi-scale Dilated 3D CNN for Epileptic Seizure Prediction", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of epileptic seizures allows patients to take preventive\nmeasures in advance to avoid possible injuries. In this work, a novel\nconvolutional neural network (CNN) is proposed to analyze time, frequency, and\nchannel information of electroencephalography (EEG) signals. The model uses\nthree-dimensional (3D) kernels to facilitate the feature extraction over the\nthree dimensions. The application of multiscale dilated convolution enables the\n3D kernel to have more flexible receptive fields. The proposed CNN model is\nevaluated with the CHB-MIT EEG database, the experimental results indicate that\nour model outperforms the existing state-of-the-art, achieves 80.5% accuracy,\n85.8% sensitivity and 75.1% specificity.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 07:13:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Ziyu", ""], ["Yang", "Jie", ""], ["Sawan", "Mohamad", ""]]}, {"id": "2105.02824", "submitter": "Mohammad Arif Ul Alam", "authors": "Mohammad Arif Ul Alam", "title": "Activity-Aware Deep Cognitive Fatigue Assessment using Wearables", "comments": "Submitted to EMBC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive fatigue has been a common problem among workers which has become an\nincreasing global problem since the emergence of COVID-19 as a global pandemic.\nWhile existing multi-modal wearable sensors-aided automatic cognitive fatigue\nmonitoring tools have focused on physical and physiological sensors (ECG, PPG,\nActigraphy) analytic on specific group of people (say gamers, athletes,\nconstruction workers), activity-awareness is utmost importance due to its\ndifferent responses on physiology in different person. In this paper, we\npropose a novel framework, Activity-Aware Recurrent Neural Network\n(\\emph{AcRoNN}), that can generalize individual activity recognition and\nimprove cognitive fatigue estimation significantly. We evaluate and compare our\nproposed method with state-of-art methods using one real-time collected dataset\nfrom 5 individuals and another publicly available dataset from 27 individuals\nachieving max. 19% improvement.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:41:11 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Alam", "Mohammad Arif Ul", ""]]}, {"id": "2105.02829", "submitter": "Stylianos E. Trevlakis Mr.", "authors": "Stylianos E. Trevlakis, Alexandros-Apostolos A. Boulogeorgos, and\n  Nestor D. Chatzidiamantis", "title": "Pathloss modeling for in-body optical wireless communications", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Optical wireless communications (OWCs) have been recognized as a candidate\nenabler of next generation in-body nano-scale networks and implants. The\ndevelopment of an accurate channel model capable of accommodating the\nparticularities of different type of tissues is expected to boost the design of\noptimized communication protocols for such applications. Motivated by this,\nthis paper focuses on presenting a general pathloss model for in-body OWCs. In\nparticular, we use experimental measurements in order to extract analytical\nexpressions for the absorption coefficients of the five main tissues'\nconstitutions, namely oxygenated and de-oxygenated blood, water, fat, and\nmelanin. Building upon these expressions, we derive a general formula for the\nabsorption coefficient evaluation of any biological tissue. To verify the\nvalidity of this formula, we compute the absorption coefficient of complex\ntissues and compare them against respective experimental results reported by\nindependent research works. Interestingly, we observe that the analytical\nformula has high accuracy and is capable of modeling the pathloss and,\ntherefore, the penetration depth in complex tissues.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:17:45 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Trevlakis", "Stylianos E.", ""], ["Boulogeorgos", "Alexandros-Apostolos A.", ""], ["Chatzidiamantis", "Nestor D.", ""]]}, {"id": "2105.02831", "submitter": "Peter Hinz", "authors": "Peter Hinz", "title": "The layer-wise L1 Loss Landscape of Neural Nets is more complex around\n  local minima", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For fixed training data and network parameters in the other layers the L1\nloss of a ReLU neural network as a function of the first layer's parameters is\na piece-wise affine function. We use the Deep ReLU Simplex algorithm to\niteratively minimize the loss monotonically on adjacent vertices and analyze\nthe trajectory of these vertex positions. We empirically observe that in a\nneighbourhood around a local minimum, the iterations behave differently such\nthat conclusions on loss level and proximity of the local minimum can be made\nbefore it has been found: Firstly the loss seems to decay exponentially slow at\niterated adjacent vertices such that the loss level at the local minimum can be\nestimated from the loss levels of subsequently iterated vertices, and secondly\nwe observe a strong increase of the vertex density around local minima. This\ncould have far-reaching consequences for the design of new gradient-descent\nalgorithms that might improve convergence rate by exploiting these facts.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:18:44 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hinz", "Peter", ""]]}, {"id": "2105.02838", "submitter": "Matthew Ricci", "authors": "Matthew Ricci, Minju Jung, Yuwei Zhang, Mathieu Chalvidal, Aneri Soni,\n  Thomas Serre", "title": "KuraNet: Systems of Coupled Oscillators that Learn to Synchronize", "comments": "9 pages, 4 figures, 5 videos, supplementary information and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks of coupled oscillators are some of the most studied objects in the\ntheory of dynamical systems. Two important areas of current interest are the\nstudy of synchrony in highly disordered systems and the modeling of systems\nwith adaptive network structures. Here, we present a single approach to both of\nthese problems in the form of \"KuraNet\", a deep-learning-based system of\ncoupled oscillators that can learn to synchronize across a distribution of\ndisordered network conditions. The key feature of the model is the replacement\nof the traditionally static couplings with a coupling function which can learn\noptimal interactions within heterogeneous oscillator populations. We apply our\napproach to the eponymous Kuramoto model and demonstrate how KuraNet can learn\ndata-dependent coupling structures that promote either global or cluster\nsynchrony. For example, we show how KuraNet can be used to empirically explore\nthe conditions of global synchrony in analytically impenetrable models with\ndisordered natural frequencies, external field strengths, and interaction\ndelays. In a sequence of cluster synchrony experiments, we further show how\nKuraNet can function as a data classifier by synchronizing into coherent\nassemblies. In all cases, we show how KuraNet can generalize to both new data\nand new network scales, making it easy to work with small systems and form\nhypotheses about the thermodynamic limit. Our proposed learning-based approach\nis broadly applicable to arbitrary dynamical systems with wide-ranging\nrelevance to modeling in physics and systems biology.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:26:33 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ricci", "Matthew", ""], ["Jung", "Minju", ""], ["Zhang", "Yuwei", ""], ["Chalvidal", "Mathieu", ""], ["Soni", "Aneri", ""], ["Serre", "Thomas", ""]]}, {"id": "2105.02858", "submitter": "Alexander Siemenn", "authors": "Alexander E. Siemenn, Matthew Beveridge, Tonio Buonassisi, Iddo Drori", "title": "Online Preconditioning of Experimental Inkjet Hardware by Bayesian\n  Optimization in Loop", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-performance semiconductor optoelectronics such as perovskites have\nhigh-dimensional and vast composition spaces that govern the performance\nproperties of the material. To cost-effectively search these composition\nspaces, we utilize a high-throughput experimentation method of rapidly printing\ndiscrete droplets via inkjet deposition, in which each droplet is comprised of\na unique permutation of semiconductor materials. However, inkjet printer\nsystems are not optimized to run high-throughput experimentation on\nsemiconductor materials. Thus, in this work, we develop a computer\nvision-driven Bayesian optimization framework for optimizing the deposited\ndroplet structures from an inkjet printer such that it is tuned to perform\nhigh-throughput experimentation on semiconductor materials. The goal of this\nframework is to tune to the hardware conditions of the inkjet printer in the\nshortest amount of time using the fewest number of droplet samples such that we\nminimize the time and resources spent on setting the system up for material\ndiscovery applications. We demonstrate convergence on optimum inkjet hardware\nconditions in 10 minutes using Bayesian optimization of computer vision-scored\ndroplet structures. We compare our Bayesian optimization results with\nstochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:46:16 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Siemenn", "Alexander E.", ""], ["Beveridge", "Matthew", ""], ["Buonassisi", "Tonio", ""], ["Drori", "Iddo", ""]]}, {"id": "2105.02866", "submitter": "Umang Gupta", "authors": "Umang Gupta, Dimitris Stripelis, Pradeep K. Lam, Paul M. Thompson,\n  Jos\\'e Luis Ambite, Greg Ver Steeg", "title": "Membership Inference Attacks on Deep Regression Models for Neuroimaging", "comments": "To appear at Medical Imaging with Deep Learning 2021 (MIDL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring the privacy of research participants is vital, even more so in\nhealthcare environments. Deep learning approaches to neuroimaging require large\ndatasets, and this often necessitates sharing data between multiple sites,\nwhich is antithetical to the privacy objectives. Federated learning is a\ncommonly proposed solution to this problem. It circumvents the need for data\nsharing by sharing parameters during the training process. However, we\ndemonstrate that allowing access to parameters may leak private information\neven if data is never directly shared. In particular, we show that it is\npossible to infer if a sample was used to train the model given only access to\nthe model prediction (black-box) or access to the model itself (white-box) and\nsome leaked samples from the training data distribution. Such attacks are\ncommonly referred to as Membership Inference attacks. We show realistic\nMembership Inference attacks on deep learning models trained for 3D\nneuroimaging tasks in a centralized as well as decentralized setup. We\ndemonstrate feasible attacks on brain age prediction models (deep learning\nmodels that predict a person's age from their brain MRI scan). We correctly\nidentified whether an MRI scan was used in model training with a 60% to over\n80% success rate depending on model complexity and security assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:51:06 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 08:02:00 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Gupta", "Umang", ""], ["Stripelis", "Dimitris", ""], ["Lam", "Pradeep K.", ""], ["Thompson", "Paul M.", ""], ["Ambite", "Jos\u00e9 Luis", ""], ["Steeg", "Greg Ver", ""]]}, {"id": "2105.02869", "submitter": "Nicha Dvornek", "authors": "Nicha C. Dvornek, Pamela Ventola, and James S. Duncan", "title": "Estimating Reproducible Functional Networks Associated with Task\n  Dynamics using Unsupervised LSTMs", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI) 2020", "journal-ref": "2020 IEEE 17th International Symposium on Biomedical Imaging\n  (ISBI), 2020, p. 1395-1398", "doi": "10.1109/ISBI45749.2020.9098377", "report-no": null, "categories": "q-bio.QM cs.LG eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for estimating more reproducible functional networks that\nare more strongly associated with dynamic task activity by using recurrent\nneural networks with long short term memory (LSTMs). The LSTM model is trained\nin an unsupervised manner to learn to generate the functional magnetic\nresonance imaging (fMRI) time-series data in regions of interest. The learned\nfunctional networks can then be used for further analysis, e.g., correlation\nanalysis to determine functional networks that are strongly associated with an\nfMRI task paradigm. We test our approach and compare to other methods for\ndecomposing functional networks from fMRI activity on 2 related but separate\ndatasets that employ a biological motion perception task. We demonstrate that\nthe functional networks learned by the LSTM model are more strongly associated\nwith the task activity and dynamics compared to other approaches. Furthermore,\nthe patterns of network association are more closely replicated across subjects\nwithin the same dataset as well as across datasets. More reproducible\nfunctional networks are essential for better characterizing the neural\ncorrelates of a target task.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:53:22 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Dvornek", "Nicha C.", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "2105.02873", "submitter": "Bj\\\"orn H Eriksson Mr.", "authors": "Bj\\\"orn H Eriksson", "title": "Contextual Bandits with Sparse Data in Web setting", "comments": "4 pages, 3 tables, review paper, scoping study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper is a scoping study to identify current methods used in handling\nsparse data with contextual bandits in web settings. The area is highly current\nand state of the art methods are identified. The years 2017-2020 are\ninvestigated, and 19 method articles are identified, and two review articles.\nFive categories of methods are described, making it easy to choose how to\naddress sparse data using contextual bandits with a method available for\nmodification in the specific setting of concern. In addition, each method has\nmultiple techniques to choose from for future evaluation. The problem areas are\nalso mentioned that each article covers. An overall updated understanding of\nsparse data problems using contextual bandits in web settings is given. The\nidentified methods are policy evaluation (off-line and on-line) ,\nhybrid-method, model representation (clusters and deep neural networks),\ndimensionality reduction, and simulation.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:58:13 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Eriksson", "Bj\u00f6rn H", ""]]}, {"id": "2105.02874", "submitter": "Nicha Dvornek", "authors": "Shiyu Wang and Nicha C. Dvornek", "title": "A Metamodel Structure For Regression Analysis: Application To Prediction\n  Of Autism Spectrum Disorder Severity", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional regression models do not generalize well when learning from small\nand noisy datasets. Here we propose a novel metamodel structure to improve the\nregression result. The metamodel is composed of multiple classification base\nmodels and a regression model built upon the base models. We test this\nstructure on the prediction of autism spectrum disorder (ASD) severity as\nmeasured by the ADOS communication (ADOS COMM) score from resting-state fMRI\ndata, using a variety of base models. The metamodel outperforms traditional\nregression models as measured by the Pearson correlation coefficient between\ntrue and predicted scores and stability. In addition, we found that the\nmetamodel is more flexible and more generalizable.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:58:16 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Shiyu", ""], ["Dvornek", "Nicha C.", ""]]}, {"id": "2105.02875", "submitter": "Valentin Deschaintre", "authors": "Valentin Deschaintre, Yiming Lin and Abhijeet Ghosh", "title": "Deep Polarization Imaging for 3D shape and SVBRDF Acquisition", "comments": "CVPR 2021 Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for efficient acquisition of shape and spatially\nvarying reflectance of 3D objects using polarization cues. Unlike previous\nworks that have exploited polarization to estimate material or object\nappearance under certain constraints (known shape or multiview acquisition), we\nlift such restrictions by coupling polarization imaging with deep learning to\nachieve high quality estimate of 3D object shape (surface normals and depth)\nand SVBRDF using single-view polarization imaging under frontal flash\nillumination. In addition to acquired polarization images, we provide our deep\nnetwork with strong novel cues related to shape and reflectance, in the form of\na normalized Stokes map and an estimate of diffuse color. We additionally\ndescribe modifications to network architecture and training loss which provide\nfurther qualitative improvements. We demonstrate our approach to achieve\nsuperior results compared to recent works employing deep learning in\nconjunction with flash illumination.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:58:43 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Deschaintre", "Valentin", ""], ["Lin", "Yiming", ""], ["Ghosh", "Abhijeet", ""]]}, {"id": "2105.02935", "submitter": "Vedant Bahel", "authors": "Vedant Bahel and Achamma Thomas", "title": "Text similarity analysis for evaluation of descriptive answers", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keeping in mind the necessity of intelligent system in educational sector,\nthis paper proposes a text analysis based automated approach for automatic\nevaluation of the descriptive answers in an examination. In particular, the\nresearch focuses on the use of intelligent concepts of Natural Language\nProcessing and Data Mining for computer aided examination evaluation system.\nThe paper present an architecture for fair evaluation of answer sheet. In this\narchitecture, the examiner creates a sample answer sheet for given sets of\nquestion. By using the concept of text summarization, text semantics and\nkeywords summarization, the final score for each answer is calculated. The text\nsimilarity model is based on Siamese Manhattan LSTM (MaLSTM). The results of\nthis research were compared to manually graded assignments and other existing\nsystem. This approach was found to be very efficient in order to be implemented\nin an institution or in an university.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:19:58 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bahel", "Vedant", ""], ["Thomas", "Achamma", ""]]}, {"id": "2105.02936", "submitter": "Edward Raff", "authors": "Edward Raff", "title": "Exact Acceleration of K-Means++ and K-Means$\\|$", "comments": "to appear in the 30th International Joint Conference on Artificial\n  Intelligence (IJCAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Means++ and its distributed variant K-Means$\\|$ have become de facto tools\nfor selecting the initial seeds of K-means. While alternatives have been\ndeveloped, the effectiveness, ease of implementation, and theoretical grounding\nof the K-means++ and $\\|$ methods have made them difficult to \"best\" from a\nholistic perspective. By considering the limited opportunities within seed\nselection to perform pruning, we develop specialized triangle inequality\npruning strategies and a dynamic priority queue to show the first acceleration\nof K-Means++ and K-Means$\\|$ that is faster in run-time while being\nalgorithmicly equivalent. For both algorithms we are able to reduce distance\ncomputations by over $500\\times$. For K-means++ this results in up to a\n17$\\times$ speedup in run-time and a $551\\times$ speedup for K-means$\\|$. We\nachieve this with simple, but carefully chosen, modifications to known\ntechniques which makes it easy to integrate our approach into existing\nimplementations of these algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:22:55 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Raff", "Edward", ""]]}, {"id": "2105.02939", "submitter": "Bjorn Lutjens", "authors": "Bj\\\"orn L\\\"utjens, Catherine H. Crawford, Mark Veillette, Dava Newman", "title": "PCE-PINNs: Physics-Informed Neural Networks for Uncertainty Propagation\n  in Ocean Modeling", "comments": "Presented at ICRL 2021 Workshop on AI for Modeling Oceans and Climate\n  Change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate models project an uncertainty range of possible warming scenarios\nfrom 1.5 to 5 degree Celsius global temperature increase until 2100, according\nto the CMIP6 model ensemble. Climate risk management and infrastructure\nadaptation requires the accurate quantification of the uncertainties at the\nlocal level. Ensembles of high-resolution climate models could accurately\nquantify the uncertainties, but most physics-based climate models are\ncomputationally too expensive to run as ensemble. Recent works in\nphysics-informed neural networks (PINNs) have combined deep learning and the\nphysical sciences to learn up to 15k faster copies of climate submodels.\nHowever, the application of PINNs in climate modeling has so far been mostly\nlimited to deterministic models. We leverage a novel method that combines\npolynomial chaos expansion (PCE), a classic technique for uncertainty\npropagation, with PINNs. The PCE-PINNs learn a fast surrogate model that is\ndemonstrated for uncertainty propagation of known parameter uncertainties. We\nshowcase the effectiveness in ocean modeling by using the local\nadvection-diffusion equation.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:52:21 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["L\u00fctjens", "Bj\u00f6rn", ""], ["Crawford", "Catherine H.", ""], ["Veillette", "Mark", ""], ["Newman", "Dava", ""]]}, {"id": "2105.02942", "submitter": "Peilin Kang", "authors": "Peilin Kang, Seyed-Mohsen Moosavi-Dezfooli", "title": "Understanding Catastrophic Overfitting in Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, FGSM adversarial training is found to be able to train a robust\nmodel which is comparable to the one trained by PGD but an order of magnitude\nfaster. However, there is a failure mode called catastrophic overfitting (CO)\nthat the classifier loses its robustness suddenly during the training and\nhardly recovers by itself. In this paper, we find CO is not only limited to\nFGSM, but also happens in $\\mbox{DF}^{\\infty}$-1 adversarial training. Then, we\nanalyze the geometric properties for both FGSM and $\\mbox{DF}^{\\infty}$-1 and\nfind they have totally different decision boundaries after CO. For FGSM, a new\ndecision boundary is generated along the direction of perturbation and makes\nthe small perturbation more effective than the large one. While for\n$\\mbox{DF}^{\\infty}$-1, there is no new decision boundary generated along the\ndirection of perturbation, instead the perturbation generated by\n$\\mbox{DF}^{\\infty}$-1 becomes smaller after CO and thus loses its\neffectiveness. We also experimentally analyze three hypotheses on potential\nfactors causing CO. And then based on the empirical analysis, we modify the\nRS-FGSM by not projecting perturbation back to the $l_\\infty$ ball. By this\nsmall modification, we could achieve $47.56 \\pm 0.37\\% $ PGD-50-10 accuracy on\nCIFAR10 with $\\epsilon=8/255$ in contrast to $43.57 \\pm 0.30\\% $ by RS-FGSM and\nalso further extend the working range of $\\epsilon$ from 8/255 to 11/255 on\nCIFAR10 without CO occurring.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:39:51 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kang", "Peilin", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""]]}, {"id": "2105.02947", "submitter": "Marvin M. Ag\\\"uero-Torales", "authors": "Marvin M. Ag\\\"uero-Torales, David Vilares, Antonio G. L\\'opez-Herrera", "title": "On the logistical difficulties and findings of Jopara Sentiment Analysis", "comments": "Accepted in the CALCS 2021 (co-located with NAACL 2021) - Fifth\n  Workshop on Computational Approaches to Linguistic Code Switching, to appear\n  (June 2021)", "journal-ref": "Proceedings on CALCS 2021 (co-located with NAACL 2021) - Fifth\n  Workshop on Computational Approaches to Linguistic Code Switching", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of sentiment analysis for Jopara, a\ncode-switching language between Guarani and Spanish. We first collect a corpus\nof Guarani-dominant tweets and discuss on the difficulties of finding quality\ndata for even relatively easy-to-annotate tasks, such as sentiment analysis.\nThen, we train a set of neural models, including pre-trained language models,\nand explore whether they perform better than traditional machine learning ones\nin this low-resource setup. Transformer architectures obtain the best results,\ndespite not considering Guarani during pre-training, but traditional machine\nlearning models perform close due to the low-resource nature of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:52:29 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:45:30 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ag\u00fcero-Torales", "Marvin M.", ""], ["Vilares", "David", ""], ["L\u00f3pez-Herrera", "Antonio G.", ""]]}, {"id": "2105.02953", "submitter": "Andrei Velichko", "authors": "Y. A. Izotov, A. A. Velichko, A. A. Ivshin and R. E. Novitskiy", "title": "Recognition of handwritten MNIST digits on low-memory 2 Kb RAM Arduino\n  board using LogNNet reservoir neural network", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": "10.1088/1757-899X/1155/1/012056", "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented compact algorithm for recognizing handwritten digits of the\nMNIST database, created on the LogNNet reservoir neural network, reaches the\nrecognition accuracy of 82%. The algorithm was tested on a low-memory Arduino\nboard with 2 Kb static RAM low-power microcontroller. The dependences of the\naccuracy and time of image recognition on the number of neurons in the\nreservoir have been investigated. The memory allocation demonstrates that the\nalgorithm stores all the necessary information in RAM without using additional\ndata storage, and operates with original images without preliminary processing.\nThe simple structure of the algorithm, with appropriate training, can be\nadapted for wide practical application, for example, for creating mobile\nbiosensors for early diagnosis of adverse events in medicine. The study results\nare important for the implementation of artificial intelligence on peripheral\nconstrained IoT devices and for edge computing.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:16:23 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Izotov", "Y. A.", ""], ["Velichko", "A. A.", ""], ["Ivshin", "A. A.", ""], ["Novitskiy", "R. E.", ""]]}, {"id": "2105.02954", "submitter": "Mohammed Tolba", "authors": "Mohammed F. Tolba, Huruy Tekle Tesfai, Hani Saleh, Baker Mohammad, and\n  Mahmoud Al-Qutayri", "title": "Deep Neural Networks Based Weight Approximation and Computation Reuse\n  for 2-D Image Classification", "comments": "10 pages 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are computationally and memory intensive, which\nmakes their hardware implementation a challenging task especially for resource\nconstrained devices such as IoT nodes. To address this challenge, this paper\nintroduces a new method to improve DNNs performance by fusing approximate\ncomputing with data reuse techniques to be used for image recognition\napplications. DNNs weights are approximated based on the linear and quadratic\napproximation methods during the training phase, then, all of the weights are\nreplaced with the linear/quadratic coefficients to execute the inference in a\nway where different weights could be computed using the same coefficients. This\nleads to a repetition of the weights across the processing element (PE) array,\nwhich in turn enables the reuse of the DNN sub-computations (computational\nreuse) and leverage the same data (data reuse) to reduce DNNs computations,\nmemory accesses, and improve energy efficiency albeit at the cost of increased\ntraining time. Complete analysis for both MNIST and CIFAR 10 datasets is\npresented for image recognition , where LeNet 5 revealed a reduction in the\nnumber of parameters by a factor of 1211.3x with a drop of less than 0.9% in\naccuracy. When compared to the state of the art Row Stationary (RS) method, the\nproposed architecture saved 54% of the total number of adders and multipliers\nneeded. Overall, the proposed approach is suitable for IoT edge devices as it\nreduces the memory size requirement as well as the number of needed memory\naccesses.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 10:16:53 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Tolba", "Mohammed F.", ""], ["Tesfai", "Huruy Tekle", ""], ["Saleh", "Hani", ""], ["Mohammad", "Baker", ""], ["Al-Qutayri", "Mahmoud", ""]]}, {"id": "2105.02958", "submitter": "Andrew Soroka", "authors": "Andrey Soroka (1), Alex Meshcheryakov (2), Sergey Gerasimov (1) ((1)\n  Faculty of Computational Mathematics and Cybernetics Lomonosov Moscow State\n  University, (2) Space Research Institute of RAS)", "title": "Morphological classification of astronomical images with limited\n  labelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.GA cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of morphological classification is complex for simple\nparameterization, but important for research in the galaxy evolution field.\nFuture galaxy surveys (e.g. EUCLID) will collect data about more than a $10^9$\ngalaxies. To obtain morphological information one needs to involve people to\nmark up galaxy images, which requires either a considerable amount of money or\na huge number of volunteers. We propose an effective semi-supervised approach\nfor galaxy morphology classification task, based on active learning of\nadversarial autoencoder (AAE) model. For a binary classification problem (top\nlevel question of Galaxy Zoo 2 decision tree) we achieved accuracy 93.1% on the\ntest part with only 0.86 millions markup actions, this model can easily scale\nup on any number of images. Our best model with additional markup achieves\naccuracy of 95.5%. To the best of our knowledge it is a first time AAE\nsemi-supervised learning model used in astronomy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:26:27 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Soroka", "Andrey", ""], ["Meshcheryakov", "Alex", ""], ["Gerasimov", "Sergey", ""]]}, {"id": "2105.02959", "submitter": "Ekanki Sharma", "authors": "Ekanki Sharma and Wilfried Elmenreich", "title": "A review on physical and data-driven based nowcasting methods using sky\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amongst all the renewable energy resources (RES), solar is the most popular\nform of energy source and is of particular interest for its widely integration\ninto the power grid. However, due to the intermittent nature of solar source,\nit is of the greatest significance to forecast solar irradiance to ensure\nuninterrupted and reliable power supply to serve the energy demand. There are\nseveral approaches to perform solar irradiance forecasting, for instance\nsatellite-based methods, sky image-based methods, machine learning-based\nmethods, and numerical weather prediction-based methods. In this paper, we\npresent a review on short-term intra-hour solar prediction techniques known as\nnowcasting methods using sky images. Along with this, we also report and\ndiscuss which sky image features are significant for the nowcasting methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 10:20:52 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Sharma", "Ekanki", ""], ["Elmenreich", "Wilfried", ""]]}, {"id": "2105.02961", "submitter": "Peter Meltzer", "authors": "Peter Meltzer, Hooman Shayani, Amir Khasahmadi, Pradeep Kumar\n  Jayaraman, Aditya Sanghi and Joseph Lambourne", "title": "UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity\n  Measure for B-Reps", "comments": "13 pages, 20 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Boundary Representations (B-Reps) are the industry standard in 3D Computer\nAided Design/Manufacturing (CAD/CAM) and industrial design due to their\nfidelity in representing stylistic details. However, they have been ignored in\nthe 3D style research. Existing 3D style metrics typically operate on meshes or\npointclouds, and fail to account for end-user subjectivity by adopting fixed\ndefinitions of style, either through crowd-sourcing for style labels or\nhand-crafted features. We propose UVStyle-Net, a style similarity measure for\nB-Reps that leverages the style signals in the second order statistics of the\nactivations in a pre-trained (unsupervised) 3D encoder, and learns their\nrelative importance to a subjective end-user through few-shot learning. Our\napproach differs from all existing data-driven 3D style methods since it may be\nused in completely unsupervised settings, which is desirable given the lack of\npublicly available labelled B-Rep datasets. More importantly, the few-shot\nlearning accounts for the inherent subjectivity associated with style. We show\nquantitatively that our proposed method with B-Reps is able to capture stronger\nstyle signals than alternative methods on meshes and pointclouds despite its\nsignificantly greater computational efficiency. We also show it is able to\ngenerate meaningful style gradients with respect to the input shape, and that\nfew-shot learning with as few as two positive examples selected by an end-user\nis sufficient to significantly improve the style measure. Finally, we\ndemonstrate its efficacy on a large unlabeled public dataset of CAD models.\nSource code and data will be released in the future.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:14:01 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 08:38:46 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Meltzer", "Peter", ""], ["Shayani", "Hooman", ""], ["Khasahmadi", "Amir", ""], ["Jayaraman", "Pradeep Kumar", ""], ["Sanghi", "Aditya", ""], ["Lambourne", "Joseph", ""]]}, {"id": "2105.02963", "submitter": "Praveen Ravirathinam", "authors": "Rahul Ghosh, Praveen Ravirathinam, Xiaowei Jia, Chenxi Lin, Zhenong\n  Jin, Vipin Kumar", "title": "Attention-augmented Spatio-Temporal Segmentation for Land Cover Mapping", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of massive earth observing satellite data provide huge\nopportunities for land use and land cover mapping. However, such mapping effort\nis challenging due to the existence of various land cover classes, noisy data,\nand the lack of proper labels. Also, each land cover class typically has its\nown unique temporal pattern and can be identified only during certain periods.\nIn this article, we introduce a novel architecture that incorporates the UNet\nstructure with Bidirectional LSTM and Attention mechanism to jointly exploit\nthe spatial and temporal nature of satellite data and to better identify the\nunique temporal patterns of each land cover. We evaluate this method for\nmapping crops in multiple regions over the world. We compare our method with\nother state-of-the-art methods both quantitatively and qualitatively on two\nreal-world datasets which involve multiple land cover classes. We also\nvisualise the attention weights to study its effectiveness in mitigating noise\nand identifying discriminative time period.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 05:39:42 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ghosh", "Rahul", ""], ["Ravirathinam", "Praveen", ""], ["Jia", "Xiaowei", ""], ["Lin", "Chenxi", ""], ["Jin", "Zhenong", ""], ["Kumar", "Vipin", ""]]}, {"id": "2105.02964", "submitter": "Vlad Velici", "authors": "Vlad Velici, Adam Pr\\\"ugel-Bennett", "title": "Object detection for crabs in top-view seabed imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This report presents the application of object detection on a database of\nunderwater images of different species of crabs, as well as aerial images of\nsea lions and finally the Pascal VOC dataset. The model is an end-to-end object\ndetection neural network based on a convolutional network base and a Long\nShort-Term Memory detector.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 00:05:17 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Velici", "Vlad", ""], ["Pr\u00fcgel-Bennett", "Adam", ""]]}, {"id": "2105.02965", "submitter": "Maarten Bieshaar", "authors": "Felix M\\\"oller, Diego Botache, Denis Huseljic, Florian Heidecker,\n  Maarten Bieshaar and Bernhard Sick", "title": "Out-of-distribution Detection and Generation using Soft Brownian Offset\n  Sampling and Autoencoders", "comments": "10 pages, 7 figures, accepted for publication at CVPR 2021 Workshop\n  Safe Artificial Intelligence for Automated Driving (SAIAD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks often suffer from overconfidence which can be partly\nremedied by improved out-of-distribution detection. For this purpose, we\npropose a novel approach that allows for the generation of out-of-distribution\ndatasets based on a given in-distribution dataset. This new dataset can then be\nused to improve out-of-distribution detection for the given dataset and machine\nlearning task at hand. The samples in this dataset are with respect to the\nfeature space close to the in-distribution dataset and therefore realistic and\nplausible. Hence, this dataset can also be used to safeguard neural networks,\ni.e., to validate the generalization performance. Our approach first generates\nsuitable representations of an in-distribution dataset using an autoencoder and\nthen transforms them using our novel proposed Soft Brownian Offset method.\nAfter transformation, the decoder part of the autoencoder allows for the\ngeneration of these implicit out-of-distribution samples. This newly generated\ndataset then allows for mixing with other datasets and thus improved training\nof an out-of-distribution classifier, increasing its performance.\nExperimentally, we show that our approach is promising for time series using\nsynthetic data. Using our new method, we also show in a quantitative case study\nthat we can improve the out-of-distribution detection for the MNIST dataset.\nFinally, we provide another case study on the synthetic generation of\nout-of-distribution trajectories, which can be used to validate trajectory\nprediction algorithms for automated driving.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 06:59:24 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["M\u00f6ller", "Felix", ""], ["Botache", "Diego", ""], ["Huseljic", "Denis", ""], ["Heidecker", "Florian", ""], ["Bieshaar", "Maarten", ""], ["Sick", "Bernhard", ""]]}, {"id": "2105.02966", "submitter": "Daniele Loiacono", "authors": "Edoardo Giacomello, Pier Luca Lanzi, Daniele Loiacono, Luca Nassano", "title": "Image Embedding and Model Ensembling for Automated Chest X-Ray\n  Interpretation", "comments": "Accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray (CXR) is perhaps the most frequently-performed radiological\ninvestigation globally. In this work, we present and study several machine\nlearning approaches to develop automated CXR diagnostic models. In particular,\nwe trained several Convolutional Neural Networks (CNN) on the CheXpert dataset,\na large collection of more than 200k CXR labeled images. Then, we used the\ntrained CNNs to compute embeddings of the CXR images, in order to train two\nsets of tree-based classifiers from them. Finally, we described and compared\nthree ensembling strategies to combine together the classifiers trained. Rather\nthan expecting some performance-wise benefits, our goal in this work is showing\nthat the above two methodologies, i.e., the extraction of image embeddings and\nmodels ensembling, can be effective and viable to solve tasks that require\nmedical imaging understanding. Our results in that perspective are encouraging\nand worthy of further investigation.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:48:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Giacomello", "Edoardo", ""], ["Lanzi", "Pier Luca", ""], ["Loiacono", "Daniele", ""], ["Nassano", "Luca", ""]]}, {"id": "2105.02968", "submitter": "Jonas Kohler", "authors": "Adrian Hoffmann, Claudio Fanconi, Rahul Rade, Jonas Kohler", "title": "This Looks Like That... Does it? Shortcomings of Latent Space Prototype\n  Interpretability in Deep Networks", "comments": null, "journal-ref": "ICML 2021 Workshop on Theoretic Foundation, Criticism, and\n  Application Trend of Explainable AI", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks that yield human interpretable decisions by\narchitectural design have lately become an increasingly popular alternative to\npost hoc interpretation of traditional black-box models. Among these networks,\nthe arguably most widespread approach is so-called prototype learning, where\nsimilarities to learned latent prototypes serve as the basis of classifying an\nunseen data point. In this work, we point to an important shortcoming of such\napproaches. Namely, there is a semantic gap between similarity in latent space\nand similarity in input space, which can corrupt interpretability. We design\ntwo experiments that exemplify this issue on the so-called ProtoPNet.\nSpecifically, we find that this network's interpretability mechanism can be led\nastray by intentionally crafted or even JPEG compression artefacts, which can\nproduce incomprehensible decisions. We argue that practitioners ought to have\nthis shortcoming in mind when deploying prototype-based models in practice.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:28:34 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 08:48:08 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 10:16:34 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 12:28:10 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Hoffmann", "Adrian", ""], ["Fanconi", "Claudio", ""], ["Rade", "Rahul", ""], ["Kohler", "Jonas", ""]]}, {"id": "2105.02993", "submitter": "Sam Earle", "authors": "Sam Earle, Maria Edwards, Ahmed Khalifa, Philip Bontrager and Julian\n  Togelius", "title": "Learning Controllable Content Generators", "comments": "8 pages, 11 figures, submitted to CoG '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that reinforcement learning can be used to train\ngenerators capable of producing high-quality game levels, with quality defined\nin terms of some user-specified heuristic. To ensure that these generators'\noutput is sufficiently diverse (that is, not amounting to the reproduction of a\nsingle optimal level configuration), the generation process is constrained such\nthat the initial seed results in some variance in the generator's output.\nHowever, this results in a loss of control over the generated content for the\nhuman user. We propose to train generators capable of producing controllably\ndiverse output, by making them \"goal-aware.\" To this end, we add conditional\ninputs representing how close a generator is to some heuristic, and also modify\nthe reward mechanism to incorporate that value. Testing on multiple domains, we\nshow that the resulting level generators are capable of exploring the space of\npossible levels in a targeted, controllable manner, producing levels of\ncomparable quality as their goal-unaware counterparts, that are diverse along\ndesigner-specified dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 22:15:51 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Earle", "Sam", ""], ["Edwards", "Maria", ""], ["Khalifa", "Ahmed", ""], ["Bontrager", "Philip", ""], ["Togelius", "Julian", ""]]}, {"id": "2105.03019", "submitter": "Man Xie", "authors": "Mandy Xie, Anqi Li, Karl Van Wyk, Frank Dellaert, Byron Boots, Nathan\n  Ratliff", "title": "Imitation Learning via Simultaneous Optimization of Policies and\n  Auxiliary Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning (IL) is a frequently used approach for data-efficient\npolicy learning. Many IL methods, such as Dataset Aggregation (DAgger), combat\nchallenges like distributional shift by interacting with oracular experts.\nUnfortunately, assuming access to oracular experts is often unrealistic in\npractice; data used in IL frequently comes from offline processes such as\nlead-through or teleoperation. In this paper, we present a novel imitation\nlearning technique called Collocation for Demonstration Encoding (CoDE) that\noperates on only a fixed set of trajectory demonstrations. We circumvent\nchallenges with methods like back-propagation-through-time by introducing an\nauxiliary trajectory network, which takes inspiration from collocation\ntechniques in optimal control. Our method generalizes well and more accurately\nreproduces the demonstrated behavior with fewer guiding trajectories when\ncompared to standard behavioral cloning methods. We present simulation results\non a 7-degree-of-freedom (DoF) robotic manipulator that learns to exhibit\nlifting, target-reaching, and obstacle avoidance behaviors.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 00:34:43 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 15:59:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xie", "Mandy", ""], ["Li", "Anqi", ""], ["Van Wyk", "Karl", ""], ["Dellaert", "Frank", ""], ["Boots", "Byron", ""], ["Ratliff", "Nathan", ""]]}, {"id": "2105.03020", "submitter": "Christian Garbin", "authors": "Christian Garbin, Pranav Rajpurkar, Jeremy Irvin, Matthew P. Lungren,\n  Oge Marques", "title": "Structured dataset documentation: a datasheet for CheXpert", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Billions of X-ray images are taken worldwide each year. Machine learning, and\ndeep learning in particular, has shown potential to help radiologists triage\nand diagnose images. However, deep learning requires large datasets with\nreliable labels. The CheXpert dataset was created with the participation of\nboard-certified radiologists, resulting in the strong ground truth needed to\ntrain deep learning networks. Following the structured format of Datasheets for\nDatasets, this paper expands on the original CheXpert paper and other sources\nto show the critical role played by radiologists in the creation of reliable\nlabels and to describe the different aspects of the dataset composition in\ndetail. Such structured documentation intends to increase the awareness in the\nmachine learning and medical communities of the strengths, applications, and\nevolution of CheXpert, thereby advancing the field of medical image analysis.\nAnother objective of this paper is to put forward this dataset datasheet as an\nexample to the community of how to create detailed and structured descriptions\nof datasets. We believe that clearly documenting the creation process, the\ncontents, and applications of datasets accelerates the creation of useful and\nreliable models.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 00:45:03 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Garbin", "Christian", ""], ["Rajpurkar", "Pranav", ""], ["Irvin", "Jeremy", ""], ["Lungren", "Matthew P.", ""], ["Marques", "Oge", ""]]}, {"id": "2105.03027", "submitter": "Abdulrahman Alhaidari", "authors": "Abdullah Alsuhaibani and Abdulrahman Alhaidari", "title": "Weather impact on daily cases of COVID-19 in Saudi Arabia using machine\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  COVID-19 was announced by the World Health Organisation (WHO) as a global\npandemic. The severity of the disease spread is determined by various factors\nsuch as the countries' health care capacity and the enforced lockdown. However,\nit is not clear if a country's climate acts as a contributing factor towards\nthe number of infected cases. This paper aims to examine the relationship\nbetween COVID-19 and the weather of 89 cities in Saudi Arabia using machine\nlearning techniques. We compiled and preprocessed data using the official daily\nreport of the Ministry of Health of Saudi Arabia for COVID-19 cases and\nobtained historical weather data aligned with the reported case daily reports.\nWe preprocessed and prepared the data to be used in models' training and\nevaluation. Our results show that temperature and wind have the strongest\nassociation with the spread of the pandemic. Our main contribution is data\ncollection, preprocessing, and prediction of daily cases. For all tested\nmodels, we used cross-validation of K-fold of K=5. Our best model is the random\nforest that has a Mean Square Error(MSE), Root Mean Square (RMSE), Mean\nAbsolute Error (MAE), and R{2} of 97.30, 9.86, 1.85, and 82.3\\%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 01:33:09 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Alsuhaibani", "Abdullah", ""], ["Alhaidari", "Abdulrahman", ""]]}, {"id": "2105.03033", "submitter": "Yilin Kang", "authors": "Yilin Kang, Yong Liu, Jian Li, Weiping Wang", "title": "Towards Sharper Utility Bounds for Differentially Private Pairwise\n  Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise learning focuses on learning tasks with pairwise loss functions,\ndepends on pairs of training instances, and naturally fits for modeling\nrelationships between pairs of samples. In this paper, we focus on the privacy\nof pairwise learning and propose a new differential privacy paradigm for\npairwise learning, based on gradient perturbation. Except for the privacy\nguarantees, we also analyze the excess population risk and give corresponding\nbounds under both expectation and high probability conditions. We use the\n\\textit{on-average stability} and the \\textit{pairwise locally elastic\nstability} theories to analyze the expectation bound and the high probability\nbound, respectively. Moreover, our analyzed utility bounds do not require\nconvex pairwise loss functions, which means that our method is general to both\nconvex and non-convex conditions. Under these circumstances, the utility bounds\nare similar to (or better than) previous bounds under convexity or strongly\nconvexity assumption, which are attractive results.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 02:20:23 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 06:28:25 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kang", "Yilin", ""], ["Liu", "Yong", ""], ["Li", "Jian", ""], ["Wang", "Weiping", ""]]}, {"id": "2105.03037", "submitter": "Fenglong Ma", "authors": "Guanjie Huang and Fenglong Ma", "title": "ConCAD: Contrastive Learning-based Cross Attention for Sleep Apnea\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advancements in deep learning methods, automatically learning\ndeep features from the original data is becoming an effective and widespread\napproach. However, the hand-crafted expert knowledge-based features are still\ninsightful. These expert-curated features can increase the model's\ngeneralization and remind the model of some data characteristics, such as the\ntime interval between two patterns. It is particularly advantageous in tasks\nwith the clinically-relevant data, where the data are usually limited and\ncomplex. To keep both implicit deep features and expert-curated explicit\nfeatures together, an effective fusion strategy is becoming indispensable. In\nthis work, we focus on a specific clinical application, i.e., sleep apnea\ndetection. In this context, we propose a contrastive learning-based cross\nattention framework for sleep apnea detection (named ConCAD). The cross\nattention mechanism can fuse the deep and expert features by automatically\nassigning attention weights based on their importance. Contrastive learning can\nlearn better representations by keeping the instances of each class closer and\npushing away instances from different classes in the embedding space\nconcurrently. Furthermore, a new hybrid loss is designed to simultaneously\nconduct contrastive learning and classification by integrating a supervised\ncontrastive loss with a cross-entropy loss. Our proposed framework can be\neasily integrated into standard deep learning models to utilize expert\nknowledge and contrastive learning to boost performance. As demonstrated on two\npublic ECG dataset with sleep apnea annotation, ConCAD significantly improves\nthe detection performance and outperforms state-of-art benchmark methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 02:38:56 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Huang", "Guanjie", ""], ["Ma", "Fenglong", ""]]}, {"id": "2105.03041", "submitter": "Taisei Hashimoto", "authors": "Taisei Hashimoto and Yoshimasa Tsuruoka", "title": "Utilizing Skipped Frames in Action Repeats via Pseudo-Actions", "comments": "Deep Reinforcement Learning Workshop, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many deep reinforcement learning settings, when an agent takes an action,\nit repeats the same action a predefined number of times without observing the\nstates until the next action-decision point. This technique of action\nrepetition has several merits in training the agent, but the data between\naction-decision points (i.e., intermediate frames) are, in effect, discarded.\nSince the amount of training data is inversely proportional to the interval of\naction repeats, they can have a negative impact on the sample efficiency of\ntraining. In this paper, we propose a simple but effective approach to\nalleviate to this problem by introducing the concept of pseudo-actions. The key\nidea of our method is making the transition between action-decision points\nusable as training data by considering pseudo-actions. Pseudo-actions for\ncontinuous control tasks are obtained as the average of the action sequence\nstraddling an action-decision point. For discrete control tasks, pseudo-actions\nare computed from learned action embeddings. This method can be combined with\nany model-free reinforcement learning algorithm that involves the learning of\nQ-functions. We demonstrate the effectiveness of our approach on both\ncontinuous and discrete control tasks in OpenAI Gym.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 02:43:44 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hashimoto", "Taisei", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "2105.03045", "submitter": "MohammadMahdi Behzadi", "authors": "Mohammad Mahdi Behzadi, Horea T. Ilies", "title": "GANTL: Towards Practical and Real-Time Topology Optimization with\n  Conditional GANs and Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning methods have been recently developed to circumvent the\nhigh computational cost of the gradient-based topology optimization. These\nmethods typically require extensive and costly datasets for training, have a\ndifficult time generalizing to unseen boundary and loading conditions and to\nnew domains, and do not take into consideration topological constraints of the\npredictions, which produces predictions with inconsistent topologies. We\npresent a deep learning method based on generative adversarial networks for\ngenerative design exploration. The proposed method combines the generative\npower of conditional GANs with the knowledge transfer capabilities of transfer\nlearning methods to predict optimal topologies for unseen boundary conditions.\nWe also show that the knowledge transfer capabilities embedded in the design of\nthe proposed algorithm significantly reduces the size of the training dataset\ncompared to the traditional deep learning neural or adversarial networks.\nMoreover, we formulate a topological loss function based on the bottleneck\ndistance obtained from the persistent diagram of the structures and demonstrate\na significant improvement in the topological connectivity of the predicted\nstructures. We use numerous examples to explore the efficiency and accuracy of\nthe proposed approach for both seen and unseen boundary conditions in 2D.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 03:13:32 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Behzadi", "Mohammad Mahdi", ""], ["Ilies", "Horea T.", ""]]}, {"id": "2105.03057", "submitter": "Luis Briceno-Mena", "authors": "Luis A. Briceno-Mena and Christopher G. Arges and Jose A. Romagnoli", "title": "PEMNET: A Transfer Learning-based Modeling Approach of High-Temperature\n  Polymer Electrolyte Membrane Electrochemical Systems", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Widespread adoption of high-temperature polymer electrolyte membrane fuel\ncells (HT-PEMFCs) and HT-PEM electrochemical hydrogen pumps (HT-PEM ECHPs)\nrequires models and computational tools that provide accurate scale-up and\noptimization. Knowledge-based modeling has limitations as it is time consuming\nand requires information about the system that is not always available (e.g.,\nmaterial properties and interfacial behavior between different materials).\nData-driven modeling on the other hand, is easier to implement, but often\nnecessitates large datasets that could be difficult to obtain. In this\ncontribution, knowledge-based modeling and data-driven modeling are uniquely\ncombined by implementing a Few-Shot Learning (FSL) approach. A knowledge-based\nmodel originally developed for a HT-PEMFC was used to generate simulated data\n(887,735 points) and used to pretrain a neural network source model.\nFurthermore, the source model developed for HT-PEMFCs was successfully applied\nto HT-PEM ECHPs - a different electrochemical system that utilizes similar\nmaterials to the fuel cell. Experimental datasets from both HT-PEMFCs and\nHT-PEM ECHPs with different materials and operating conditions (~50 points\neach) were used to train 8 target models via FSL. Models for the unseen data\nreached high accuracies in all cases (rRMSE between 1.04 and 3.73% for HT-PEMCs\nand between 6.38 and 8.46% for HT-PEM ECHPs).\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 04:01:26 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 16:03:30 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Briceno-Mena", "Luis A.", ""], ["Arges", "Christopher G.", ""], ["Romagnoli", "Jose A.", ""]]}, {"id": "2105.03058", "submitter": "Mehrnaz Najafi", "authors": "Mehrnaz Najafi and Lifang He and Philip S. Yu", "title": "Error-Robust Multi-View Clustering: Progress, Challenges and\n  Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in data collection from multiple sources, multi-view\ndata has received significant attention. In multi-view data, each view\nrepresents a different perspective of data. Since label information is often\nexpensive to acquire, multi-view clustering has gained growing interest, which\naims to obtain better clustering solution by exploiting complementary and\nconsistent information across all views rather than only using an individual\nview. Due to inevitable sensor failures, data in each view may contain error.\nError often exhibits as noise or feature-specific corruptions or outliers.\nMulti-view data may contain any or combination of these error types. Blindly\nclustering multi-view data i.e., without considering possible error in view(s)\ncould significantly degrade the performance. The goal of error-robust\nmulti-view clustering is to obtain useful outcome even if the multi-view data\nis corrupted. Existing error-robust multi-view clustering approaches with\nexplicit error removal formulation can be structured into five broad research\ncategories - sparsity norm based approaches, graph based methods, subspace\nbased learning approaches, deep learning based methods and hybrid approaches,\nthis survey summarizes and reviews recent advances in error-robust clustering\nfor multi-view data. Finally, we highlight the challenges and provide future\nresearch opportunities.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 04:03:02 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Najafi", "Mehrnaz", ""], ["He", "Lifang", ""], ["Yu", "Philip S.", ""]]}, {"id": "2105.03061", "submitter": "Dongmyung Shin", "authors": "Dongmyung Shin, Younghoon Kim, Chungseok Oh, Hongjun An, Juhyung Park,\n  Jiye Kim, and Jongho Lee", "title": "DeepRF: Deep Reinforcement Learning Designed RadioFrequency Waveform in\n  MRI", "comments": "35 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A carefully engineered radiofrequency (RF) pulse plays a key role in a number\nof systems such as mobile phone, radar, and magnetic resonance imaging (MRI).\nThe design of an RF waveform, however, is often posed as an inverse problem\nthat has no general solution. As a result, various design methods each with a\nspecific purpose have been developed based on the intuition of human experts.\nIn this work, we propose an artificial intelligence-powered RF pulse design\nframework, DeepRF, which utilizes the self-learning characteristics of deep\nreinforcement learning (DRL) to generate a novel RF beyond human intuition.\nAdditionally, the method can design various types of RF pulses via customized\nreward functions. The algorithm of DeepRF consists of two modules: the RF\ngeneration module, which utilizes DRL to explore new RF pulses, and the RF\nrefinement module, which optimizes the seed RF pulses from the generation\nmodule via gradient ascent. The effectiveness of DeepRF is demonstrated using\nfour exemplary RF pulses, slice-selective excitation pulse, slice-selective\ninversion pulse, B1-insensitive volume inversion pulse, and B1-insensitive\nselective inversion pulse, that are commonly used in MRI. The results show that\nthe DeepRF-designed pulses successfully satisfy the design criteria while\nimproving specific absorption rates when compared to those of the conventional\nRF pulses. Further analyses suggest that the DeepRF-designed pulses utilize new\nmechanisms of magnetization manipulation that are difficult to be explained by\nconventional theory, suggesting the potentials of DeepRF in discovering unseen\ndesign dimensions beyond human intuition. This work may lay the foundation for\nan emerging field of AI-driven RF waveform design.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 04:22:27 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Shin", "Dongmyung", ""], ["Kim", "Younghoon", ""], ["Oh", "Chungseok", ""], ["An", "Hongjun", ""], ["Park", "Juhyung", ""], ["Kim", "Jiye", ""], ["Lee", "Jongho", ""]]}, {"id": "2105.03070", "submitter": "Yi-Chen Chen", "authors": "Yi-Chen Chen, Po-Han Chi, Shu-wen Yang, Kai-Wei Chang, Jheng-hao Lin,\n  Sung-Feng Huang, Da-Rong Liu, Chi-Liang Liu, Cheng-Kuang Lee, Hung-yi Lee", "title": "SpeechNet: A Universal Modularized Model for Speech Processing Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a wide variety of speech processing tasks ranging from extracting\ncontent information from speech signals to generating speech signals. For\ndifferent tasks, model networks are usually designed and tuned separately. If a\nuniversal model can perform multiple speech processing tasks, some tasks might\nbe improved with the related abilities learned from other tasks. The multi-task\nlearning of a wide variety of speech processing tasks with a universal model\nhas not been studied. This paper proposes a universal modularized model,\nSpeechNet, which treats all speech processing tasks into a speech/text input\nand speech/text output format. We select five essential speech processing tasks\nfor multi-task learning experiments with SpeechNet. We show that SpeechNet\nlearns all of the above tasks, and we further analyze which tasks can be\nimproved by other tasks. SpeechNet is modularized and flexible for\nincorporating more modules, tasks, or training approaches in the future. We\nrelease the code and experimental settings to facilitate the research of\nmodularized universal models and multi-task learning of speech processing\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 05:31:34 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 11:05:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Yi-Chen", ""], ["Chi", "Po-Han", ""], ["Yang", "Shu-wen", ""], ["Chang", "Kai-Wei", ""], ["Lin", "Jheng-hao", ""], ["Huang", "Sung-Feng", ""], ["Liu", "Da-Rong", ""], ["Liu", "Chi-Liang", ""], ["Lee", "Cheng-Kuang", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2105.03075", "submitter": "Steven Y. Feng", "authors": "Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush\n  Vosoughi, Teruko Mitamura, Eduard Hovy", "title": "A Survey of Data Augmentation Approaches for NLP", "comments": "Accepted to ACL 2021 Findings. GitHub repo with paper list at\n  https://github.com/styfeng/DataAug4NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 06:03:45 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 23:39:31 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 21:57:45 GMT"}, {"version": "v4", "created": "Fri, 2 Jul 2021 20:34:55 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Feng", "Steven Y.", ""], ["Gangal", "Varun", ""], ["Wei", "Jason", ""], ["Chandar", "Sarath", ""], ["Vosoughi", "Soroush", ""], ["Mitamura", "Teruko", ""], ["Hovy", "Eduard", ""]]}, {"id": "2105.03092", "submitter": "Marcus Kalander", "authors": "Keli Zhang, Marcus Kalander, Min Zhou, Xi Zhang and Junjian Ye", "title": "An Influence-based Approach for Root Cause Alarm Discovery in Telecom\n  Networks", "comments": null, "journal-ref": "International Workshop on Artificial Intelligence for IT\n  Operations (AIOPS) 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alarm root cause analysis is a significant component in the day-to-day\ntelecommunication network maintenance, and it is critical for efficient and\naccurate fault localization and failure recovery. In practice, accurate and\nself-adjustable alarm root cause analysis is a great challenge due to network\ncomplexity and vast amounts of alarms. A popular approach for failure root\ncause identification is to construct a graph with approximate edges, commonly\nbased on either event co-occurrences or conditional independence tests.\nHowever, considerable expert knowledge is typically required for edge pruning.\nWe propose a novel data-driven framework for root cause alarm localization,\ncombining both causal inference and network embedding techniques. In this\nframework, we design a hybrid causal graph learning method (HPCI), which\ncombines Hawkes Process with Conditional Independence tests, as well as propose\na novel Causal Propagation-Based Embedding algorithm (CPBE) to infer edge\nweights. We subsequently discover root cause alarms in a real-time data stream\nby applying an influence maximization algorithm on the weighted graph. We\nevaluate our method on artificial data and real-world telecom data, showing a\nsignificant improvement over the best baselines.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 07:41:46 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhang", "Keli", ""], ["Kalander", "Marcus", ""], ["Zhou", "Min", ""], ["Zhang", "Xi", ""], ["Ye", "Junjian", ""]]}, {"id": "2105.03109", "submitter": "Marius Hobbhahn", "authors": "Marius Hobbhahn, Philipp Hennig", "title": "Laplace Matching for fast Approximate Inference in Generalized Linear\n  Models", "comments": "Currently under review at JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian inference in generalized linear models (GLMs), i.e.~Gaussian\nregression with non-Gaussian likelihoods, is generally non-analytic and\nrequires computationally expensive approximations, such as sampling or\nvariational inference. We propose an approximate inference framework primarily\ndesigned to be computationally cheap while still achieving high approximation\nquality. The concept, which we call \\emph{Laplace Matching}, involves\nclosed-form, approximate, bi-directional transformations between the parameter\nspaces of exponential families. These are constructed from Laplace\napproximations under custom-designed basis transformations. The mappings can\nthen be leveraged to effectively turn a latent Gaussian distribution into a\nconjugate prior for a rich class of observable variables. This effectively\nturns inference in GLMs into conjugate inference (with small approximation\nerrors). We empirically evaluate the method in two different GLMs, showing\napproximation quality comparable to state-of-the-art approximate inference\ntechniques at a drastic reduction in computational cost. More specifically, our\nmethod has a cost comparable to the \\emph{very first} step of the iterative\noptimization usually employed in standard GLM inference.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:25:17 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hobbhahn", "Marius", ""], ["Hennig", "Philipp", ""]]}, {"id": "2105.03120", "submitter": "Berivan Isik", "authors": "Berivan Isik", "title": "Neural 3D Scene Compression via Model Compression", "comments": "Stanford CS 231A Final Project, 2021. WiCV at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering 3D scenes requires access to arbitrary viewpoints from the scene.\nStorage of such a 3D scene can be done in two ways; (1) storing 2D images taken\nfrom the 3D scene that can reconstruct the scene back through interpolations,\nor (2) storing a representation of the 3D scene itself that already encodes\nviews from all directions. So far, traditional 3D compression methods have\nfocused on the first type of storage and compressed the original 2D images with\nimage compression techniques. With this approach, the user first decodes the\nstored 2D images and then renders the 3D scene. However, this separated\nprocedure is inefficient since a large amount of 2D images have to be stored.\nIn this work, we take a different approach and compress a functional\nrepresentation of 3D scenes. In particular, we introduce a method to compress\n3D scenes by compressing the neural networks that represent the scenes as\nneural radiance fields. Our method provides more efficient storage of 3D scenes\nsince it does not store 2D images -- which are redundant when we render the\nscene from the neural functional representation.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:50:00 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Isik", "Berivan", ""]]}, {"id": "2105.03131", "submitter": "Zeki Bilgin", "authors": "Zeki Bilgin", "title": "Code2Image: Intelligent Code Analysis by Computer Vision Techniques and\n  Application to Vulnerability Prediction", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Intelligent code analysis has received increasing attention in parallel with\nthe remarkable advances in the field of machine learning (ML) in recent years.\nA major challenge in leveraging ML for this purpose is to represent source code\nin a useful form that ML algorithms can accept as input. In this study, we\npresent a novel method to represent source code as image while preserving\nsemantic and syntactic properties, which paves the way for leveraging computer\nvision techniques to use for code analysis. Indeed the method makes it possible\nto directly enter the resulting image representation of source codes into deep\nlearning (DL) algorithms as input without requiring any further data\npre-processing or feature extraction step. We demonstrate feasibility and\neffectiveness of our method by realizing a vulnerability prediction use case\nover a public dataset containing a large number of real-world source code\nsamples with performance evaluation in comparison to the state-of-art\nsolutions. Our implementation is publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 09:10:20 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bilgin", "Zeki", ""]]}, {"id": "2105.03153", "submitter": "Matth\\\"aus Kleindessner", "authors": "Matth\\\"aus Kleindessner, Samira Samadi, Muhammad Bilal Zafar,\n  Krishnaram Kenthapadi, Chris Russell", "title": "Pairwise Fairness for Ordinal Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of fairness for ordinal regression, or ordinal\nclassification. We adapt two fairness notions previously considered in fair\nranking and propose a strategy for training a predictor that is approximately\nfair according to either notion. Our predictor consists of a threshold model,\ncomposed of a scoring function and a set of thresholds, and our strategy is\nbased on a reduction to fair binary classification for learning the scoring\nfunction and local search for choosing the thresholds. We can control the\nextent to which we care about the accuracy vs the fairness of the predictor via\na parameter. In extensive experiments we show that our strategy allows us to\neffectively explore the accuracy-vs-fairness trade-off and that it often\ncompares favorably to \"unfair\" state-of-the-art methods for ordinal regression\nin that it yields predictors that are only slightly less accurate, but\nsignificantly more fair.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 10:33:42 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kleindessner", "Matth\u00e4us", ""], ["Samadi", "Samira", ""], ["Zafar", "Muhammad Bilal", ""], ["Kenthapadi", "Krishnaram", ""], ["Russell", "Chris", ""]]}, {"id": "2105.03155", "submitter": "Tangjun Wang", "authors": "Tangjun Wang, Zehao Dou, Chenglong Bao, Zuoqiang Shi", "title": "Diff-ResNets for Few-shot Learning -- an ODE Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Interpreting deep neural networks from the ordinary differential equations\n(ODEs) perspective has inspired many efficient and robust network\narchitectures. However, existing ODE based approaches ignore the relationship\namong data points, which is a critical component in many problems including\nfew-shot learning and semi-supervised learning. In this paper, inspired by the\ndiffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to\nstrengthen the interactions among data points. Under the structured data\nassumption, it is proved that the diffusion mechanism can decrease the\ndistance-diameter ratio that improves the separability of inter-class points\nand reduces the distance among local intra-class points. This property can be\neasily adopted by the residual networks for constructing the separable\nhyperplanes. The synthetic binary classification experiments demonstrate the\neffectiveness of the proposed diffusion mechanism. Moreover, extensive\nexperiments of few-shot image classification and semi-supervised graph node\nclassification in various datasets validate the advantages of the proposed\nDiff-ResNet over existing few-shot learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 10:42:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wang", "Tangjun", ""], ["Dou", "Zehao", ""], ["Bao", "Chenglong", ""], ["Shi", "Zuoqiang", ""]]}, {"id": "2105.03168", "submitter": "Keenan Jones", "authors": "Keenan Jones, Jason R. C. Nurse, Shujun Li", "title": "The Shadowy Lives of Emojis: An Analysis of a Hacktivist Collective's\n  Use of Emojis on Twitter", "comments": "10 pages, 1 figure, 7 tables", "journal-ref": null, "doi": "10.36190/2021.04", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emojis have established themselves as a popular means of communication in\nonline messaging. Despite the apparent ubiquity in these image-based tokens,\nhowever, interpretation and ambiguity may allow for unique uses of emojis to\nappear. In this paper, we present the first examination of emoji usage by\nhacktivist groups via a study of the Anonymous collective on Twitter. This\nresearch aims to identify whether Anonymous affiliates have evolved their own\napproach to using emojis. To do this, we compare a large dataset of Anonymous\ntweets to a baseline tweet dataset from randomly sampled Twitter users using\ncomputational and qualitative analysis to compare their emoji usage. We utilise\nWord2Vec language models to examine the semantic relationships between emojis,\nidentifying clear distinctions in the emoji-emoji relationships of Anonymous\nusers. We then explore how emojis are used as a means of conveying emotions,\nfinding that despite little commonality in emoji-emoji semantic ties, Anonymous\nemoji usage displays similar patterns of emotional purpose to the emojis of\nbaseline Twitter users. Finally, we explore the textual context in which these\nemojis occur, finding that although similarities exist between the emoji usage\nof our Anonymous and baseline Twitter datasets, Anonymous users appear to have\nadopted more specific interpretations of certain emojis. This includes the use\nof emojis as a means of expressing adoration and infatuation towards notable\nAnonymous affiliates. These findings indicate that emojis appear to retain a\nconsiderable degree of similarity within Anonymous accounts as compared to more\ntypical Twitter users. However, their are signs that emoji usage in Anonymous\naccounts has evolved somewhat, gaining additional group-specific associations\nthat reveal new insights into the behaviours of this unusual collective.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:21:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Jones", "Keenan", ""], ["Nurse", "Jason R. C.", ""], ["Li", "Shujun", ""]]}, {"id": "2105.03170", "submitter": "Weibo Hu", "authors": "Chuan Chen, Weibo Hu, Ziyue Xu, Zibin Zheng", "title": "FedGL: Federated Graph Learning Framework with Global Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph data are ubiquitous in the real world. Graph learning (GL) tries to\nmine and analyze graph data so that valuable information can be discovered.\nExisting GL methods are designed for centralized scenarios. However, in\npractical scenarios, graph data are usually distributed in different\norganizations, i.e., the curse of isolated data islands. To address this\nproblem, we incorporate federated learning into GL and propose a general\nFederated Graph Learning framework FedGL, which is capable of obtaining a\nhigh-quality global graph model while protecting data privacy by discovering\nthe global self-supervision information during the federated training.\nConcretely, we propose to upload the prediction results and node embeddings to\nthe server for discovering the global pseudo label and global pseudo graph,\nwhich are distributed to each client to enrich the training labels and\ncomplement the graph structure respectively, thereby improving the quality of\neach local model. Moreover, the global self-supervision enables the information\nof each client to flow and share in a privacy-preserving manner, thus\nalleviating the heterogeneity and utilizing the complementarity of graph data\namong different clients. Finally, experimental results show that FedGL\nsignificantly outperforms baselines on four widely used graph datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:27:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Chen", "Chuan", ""], ["Hu", "Weibo", ""], ["Xu", "Ziyue", ""], ["Zheng", "Zibin", ""]]}, {"id": "2105.03172", "submitter": "Hlynur Dav{\\i}{\\dh} Hlynsson", "authors": "Hlynur Dav\\'i{\\dh} Hlynsson, Laurenz Wiskott", "title": "Reward prediction for representation learning and reward shaping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the fundamental challenges in reinforcement learning (RL) is the one\nof data efficiency: modern algorithms require a very large number of training\nsamples, especially compared to humans, for solving environments with\nhigh-dimensional observations. The severity of this problem is increased when\nthe reward signal is sparse. In this work, we propose learning a state\nrepresentation in a self-supervised manner for reward prediction. The reward\npredictor learns to estimate either a raw or a smoothed version of the true\nreward signal in environment with a single, terminating, goal state. We augment\nthe training of out-of-the-box RL agents by shaping the reward using our reward\npredictor during policy learning. Using our representation for preprocessing\nhigh-dimensional observations, as well as using the predictor for reward\nshaping, is shown to significantly enhance Actor Critic using\nKronecker-factored Trust Region and Proximal Policy Optimization in single-goal\nenvironments with visual inputs.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:29:32 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hlynsson", "Hlynur Dav\u00ed\u00f0", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "2105.03173", "submitter": "Luigi Riso", "authors": "Luigi Riso", "title": "Use of High Dimensional Modeling for automatic variables selection: the\n  best path algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm for automatic variables selection. In\nparticular, using the Graphical Models properties it is possible to develop a\nmethod that can be used in the contest of large dataset. The advantage of this\nalgorithm is that can be combined with different forecasting models. In this\nresearch we have used the OLS method and we have compared the result with the\nLASSO method.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:33:06 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Riso", "Luigi", ""]]}, {"id": "2105.03176", "submitter": "Matthias Wess", "authors": "Matthias Wess, Matvey Ivanov, Anvesh Nookala, Christoph Unger,\n  Alexander Wendt, Axel Jantsch", "title": "ANNETTE: Accurate Neural Network Execution Time Estimation with Stacked\n  Models", "comments": null, "journal-ref": "in IEEE Access, vol. 9, pp. 3545-3556, 2021", "doi": "10.1109/ACCESS.2020.3047259", "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With new accelerator hardware for DNN, the computing power for AI\napplications has increased rapidly. However, as DNN algorithms become more\ncomplex and optimized for specific applications, latency requirements remain\nchallenging, and it is critical to find the optimal points in the design space.\nTo decouple the architectural search from the target hardware, we propose a\ntime estimation framework that allows for modeling the inference latency of\nDNNs on hardware accelerators based on mapping and layer-wise estimation\nmodels. The proposed methodology extracts a set of models from micro-kernel and\nmulti-layer benchmarks and generates a stacked model for mapping and network\nexecution time estimation. We compare estimation accuracy and fidelity of the\ngenerated mixed models, statistical models with the roofline model, and a\nrefined roofline model for evaluation. We test the mixed models on the ZCU102\nSoC board with DNNDK and Intel Neural Compute Stick 2 on a set of 12\nstate-of-the-art neural networks. It shows an average estimation error of 3.47%\nfor the DNNDK and 7.44% for the NCS2, outperforming the statistical and\nanalytical layer models for almost all selected networks. For a randomly\nselected subset of 34 networks of the NASBench dataset, the mixed model reaches\nfidelity of 0.988 in Spearman's rank correlation coefficient metric. The code\nof ANNETTE is publicly available at\nhttps://github.com/embedded-machine-learning/annette.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:39:05 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wess", "Matthias", ""], ["Ivanov", "Matvey", ""], ["Nookala", "Anvesh", ""], ["Unger", "Christoph", ""], ["Wendt", "Alexander", ""], ["Jantsch", "Axel", ""]]}, {"id": "2105.03178", "submitter": "Gongxu Luo", "authors": "Gongxu Luo, Jianxin Li, Hao Peng, Carl Yang, Lichao Sun, Philip S. Yu,\n  Lifang He", "title": "Graph Entropy Guided Node Embedding Dimension Selection for Graph Neural\n  Networks", "comments": "Accept by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representation learning has achieved great success in many areas,\nincluding e-commerce, chemistry, biology, etc. However, the fundamental problem\nof choosing the appropriate dimension of node embedding for a given graph still\nremains unsolved. The commonly used strategies for Node Embedding Dimension\nSelection (NEDS) based on grid search or empirical knowledge suffer from heavy\ncomputation and poor model performance. In this paper, we revisit NEDS from the\nperspective of minimum entropy principle. Subsequently, we propose a novel\nMinimum Graph Entropy (MinGE) algorithm for NEDS with graph data. To be\nspecific, MinGE considers both feature entropy and structure entropy on graphs,\nwhich are carefully designed according to the characteristics of the rich\ninformation in them. The feature entropy, which assumes the embeddings of\nadjacent nodes to be more similar, connects node features and link topology on\ngraphs. The structure entropy takes the normalized degree as basic unit to\nfurther measure the higher-order structure of graphs. Based on them, we design\nMinGE to directly calculate the ideal node embedding dimension for any graph.\nFinally, comprehensive experiments with popular Graph Neural Networks (GNNs) on\nbenchmark datasets demonstrate the effectiveness and generalizability of our\nproposed MinGE.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:40:29 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:29:48 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 06:50:03 GMT"}, {"version": "v4", "created": "Mon, 24 May 2021 06:30:01 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Luo", "Gongxu", ""], ["Li", "Jianxin", ""], ["Peng", "Hao", ""], ["Yang", "Carl", ""], ["Sun", "Lichao", ""], ["Yu", "Philip S.", ""], ["He", "Lifang", ""]]}, {"id": "2105.03193", "submitter": "Duong Le Hoang", "authors": "Duong H. Le, Binh-Son Hua", "title": "Network Pruning That Matters: A Case Study on Retraining Variants", "comments": "Accepted at ICLR 2021 (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Network pruning is an effective method to reduce the computational expense of\nover-parameterized neural networks for deployment on low-resource systems.\nRecent state-of-the-art techniques for retraining pruned networks such as\nweight rewinding and learning rate rewinding have been shown to outperform the\ntraditional fine-tuning technique in recovering the lost accuracy (Renda et\nal., 2020), but so far it is unclear what accounts for such performance. In\nthis work, we conduct extensive experiments to verify and analyze the uncanny\neffectiveness of learning rate rewinding. We find that the reason behind the\nsuccess of learning rate rewinding is the usage of a large learning rate.\nSimilar phenomenon can be observed in other learning rate schedules that\ninvolve large learning rates, e.g., the 1-cycle learning rate schedule (Smith\net al., 2019). By leveraging the right learning rate schedule in retraining, we\ndemonstrate a counter-intuitive phenomenon in that randomly pruned networks\ncould even achieve better performance than methodically pruned networks\n(fine-tuned with the conventional approach). Our results emphasize the\ncruciality of the learning rate schedule in pruned network retraining - a\ndetail often overlooked by practitioners during the implementation of network\npruning. One-sentence Summary: We study the effective of different retraining\nmechanisms while doing pruning\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 12:03:24 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Le", "Duong H.", ""], ["Hua", "Binh-Son", ""]]}, {"id": "2105.03215", "submitter": "Yida Wang", "authors": "Zhi Chen, Cody Hao Yu, Trevor Morris, Jorn Tuyls, Yi-Hsiang Lai, Jared\n  Roesch, Elliott Delaye, Vin Sharma, Yida Wang", "title": "Bring Your Own Codegen to Deep Learning Compiler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been ubiquitously applied in many\napplications, and accelerators are emerged as an enabler to support the fast\nand efficient inference tasks of these applications. However, to achieve high\nmodel coverage with high performance, each accelerator vendor has to develop a\nfull compiler stack to ingest, optimize, and execute the DNNs. This poses\nsignificant challenges in the development and maintenance of the software\nstack. In addition, the vendors have to contiguously update their hardware\nand/or software to cope with the rapid evolution of the DNN model architectures\nand operators. To address these issues, this paper proposes an open source\nframework that enables users to only concentrate on the development of their\nproprietary code generation tools by reusing as many as possible components in\nthe existing deep learning compilers. Our framework provides users flexible and\neasy-to-use interfaces to partition their models into segments that can be\nexecuted on \"the best\" processors to take advantage of the powerful computation\ncapability of accelerators. Our case study shows that our framework has been\ndeployed in multiple commercial vendors' compiler stacks with only a few\nthousand lines of code.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:22:25 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Chen", "Zhi", ""], ["Yu", "Cody Hao", ""], ["Morris", "Trevor", ""], ["Tuyls", "Jorn", ""], ["Lai", "Yi-Hsiang", ""], ["Roesch", "Jared", ""], ["Delaye", "Elliott", ""], ["Sharma", "Vin", ""], ["Wang", "Yida", ""]]}, {"id": "2105.03232", "submitter": "Umair Iqbal Mr", "authors": "Umair Iqbal, Johan Barthelemy, Wanqing Li and Pascal Perez", "title": "Automating Visual Blockage Classification of Culverts with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockage of culverts by transported debris materials is reported as main\ncontributor in originating urban flash floods. Conventional modelling\napproaches had no success in addressing the problem largely because of\nunavailability of peak floods hydraulic data and highly non-linear behaviour of\ndebris at culvert. This article explores a new dimension to investigate the\nissue by proposing the use of Intelligent Video Analytic (IVA) algorithms for\nextracting blockage related information. Potential of using existing\nConvolutional Neural Network (CNN) algorithms (i.e., DarkNet53, DenseNet121,\nInceptionResNetV2, InceptionV3, MobileNet, ResNet50, VGG16, EfficientNetB3,\nNASNet) is investigated over a custom collected blockage dataset (i.e., Images\nof Culvert Openings and Blockage (ICOB)) to predict the blockage in a given\nimage. Models were evaluated based on their performance on test dataset (i.e.,\naccuracy, loss, precision, recall, F1-score, Jaccard-Index), Floating Point\nOperations Per Second (FLOPs) and response times to process a single test\ninstance. From the results, NASNet was reported most efficient in classifying\nthe blockage with the accuracy of 85\\%; however, EfficientNetB3 was recommended\nfor the hardware implementation because of its improved response time with\naccuracy comparable to NASNet (i.e., 83\\%). False Negative (FN) instances,\nFalse Positive (FP) instances and CNN layers activation suggested that\nbackground noise and oversimplified labelling criteria were two contributing\nfactors in degraded performance of existing CNN algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:40:09 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Iqbal", "Umair", ""], ["Barthelemy", "Johan", ""], ["Li", "Wanqing", ""], ["Perez", "Pascal", ""]]}, {"id": "2105.03233", "submitter": "Umair Iqbal Mr", "authors": "Umair Iqbal, Johan Barthelemy, Wanqing Li and Pascal Perez", "title": "Regression on Deep Visual Features using Artificial Neural Networks\n  (ANNs) to Predict Hydraulic Blockage at Culverts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross drainage hydraulic structures (i.e., culverts, bridges) in urban\nlandscapes are prone to getting blocked by transported debris which often\nresults in causing the flash floods. In context of Australia, Wollongong City\nCouncil (WCC) blockage conduit policy is the only formal guideline to consider\nblockage in design process. However, many argue that this policy is based on\nthe post floods visual inspections and hence can not be considered accurate\nrepresentation of hydraulic blockage. As a result of this on-going debate,\nvisual blockage and hydraulic blockage are considered two distinct terms with\nno established quantifiable relation among both. This paper attempts to relate\nboth terms by proposing the use of deep visual features for prediction of\nhydraulic blockage at a given culvert. An end-to-end machine learning pipeline\nis propounded which takes an image of culvert as input, extract visual features\nusing deep learning models, pre-process the visual features and feed into\nregression model to predict the corresponding hydraulic blockage. Dataset\n(i.e., Hydrology-Lab Dataset (HD), Visual Hydrology-Lab Dataset (VHD)) used in\nthis research was collected from in-lab experiments carried out using scaled\nphysical models of culverts where multiple blockage scenarios were replicated\nat scale. Performance of regression models was assessed using standard\nevaluation metrics. Furthermore, performance of overall machine learning\npipeline was assessed in terms of processing times for relative comparison of\nmodels and hardware requirement analysis. From the results ANN used with\nMobileNet extracted visual features achieved the best regression performance\nwith $R^{2}$ score of 0.7855. Positive value of $R^{2}$ score indicated the\npresence of correlation between visual features and hydraulic blockage and\nsuggested that both can be interrelated with each other.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 14:58:46 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Iqbal", "Umair", ""], ["Barthelemy", "Johan", ""], ["Li", "Wanqing", ""], ["Perez", "Pascal", ""]]}, {"id": "2105.03245", "submitter": "Yulin Wang", "authors": "Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, Gao\n  Huang", "title": "Adaptive Focus for Efficient Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the spatial redundancy in video recognition with\nthe aim to improve the computational efficiency. It is observed that the most\ninformative region in each frame of a video is usually a small image patch,\nwhich shifts smoothly across frames. Therefore, we model the patch localization\nproblem as a sequential decision task, and propose a reinforcement learning\nbased approach for efficient spatially adaptive video recognition (AdaFocus).\nIn specific, a light-weighted ConvNet is first adopted to quickly process the\nfull video sequence, whose features are used by a recurrent policy network to\nlocalize the most task-relevant regions. Then the selected patches are inferred\nby a high-capacity network for the final prediction. During offline inference,\nonce the informative patch sequence has been generated, the bulk of computation\ncan be done in parallel, and is efficient on modern GPU devices. In addition,\nwe demonstrate that the proposed method can be easily extended by further\nconsidering the temporal redundancy, e.g., dynamically skipping less valuable\nframes. Extensive experiments on five benchmark datasets, i.e., ActivityNet,\nFCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is\nsignificantly more efficient than the competitive baselines. Code will be\navailable at https://github.com/blackfeather-wang/AdaFocus.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:24:47 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wang", "Yulin", ""], ["Chen", "Zhaoxi", ""], ["Jiang", "Haojun", ""], ["Song", "Shiji", ""], ["Han", "Yizeng", ""], ["Huang", "Gao", ""]]}, {"id": "2105.03248", "submitter": "David Heckerman", "authors": "Dan Geiger and David Heckerman", "title": "Parameter Priors for Directed Acyclic Graphical Models and the\n  Characterization of Several Probability Distributions", "comments": "This version has improved pointers to the literature. arXiv admin\n  note: substantial text overlap with arXiv:1301.6697", "journal-ref": "The Annals of Statistics, 30: 1412-1440, 2002", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop simple methods for constructing parameter priors for model choice\namong Directed Acyclic Graphical (DAG) models. In particular, we introduce\nseveral assumptions that permit the construction of parameter priors for a\nlarge number of DAG models from a small set of assessments. We then present a\nmethod for directly computing the marginal likelihood of every DAG model given\na random sample with no missing observations. We apply this methodology to\nGaussian DAG models which consist of a recursive set of linear regression\nmodels. We show that the only parameter prior for complete Gaussian DAG models\nthat satisfies our assumptions is the normal-Wishart distribution. Our analysis\nis based on the following new characterization of the Wishart distribution: let\n$W$ be an $n \\times n$, $n \\ge 3$, positive-definite symmetric matrix of random\nvariables and $f(W)$ be a pdf of $W$. Then, f$(W)$ is a Wishart distribution if\nand only if $W_{11} - W_{12} W_{22}^{-1} W'_{12}$ is independent of\n$\\{W_{12},W_{22}\\}$ for every block partitioning $W_{11},W_{12}, W'_{12},\nW_{22}$ of $W$. Similar characterizations of the normal and normal-Wishart\ndistributions are provided as well.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:01:11 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 19:44:37 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""]]}, {"id": "2105.03251", "submitter": "Faiq Khalid", "authors": "Faiq Khalid, Muhammad Abdullah Hanif, Muhammad Shafique", "title": "Exploiting Vulnerabilities in Deep Neural Networks: Adversarial and\n  Fault-Injection Attacks", "comments": "CYBER 2020, The Fifth International Conference on Cyber-Technologies\n  and Cyber-Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From tiny pacemaker chips to aircraft collision avoidance systems, the\nstate-of-the-art Cyber-Physical Systems (CPS) have increasingly started to rely\non Deep Neural Networks (DNNs). However, as concluded in various studies, DNNs\nare highly susceptible to security threats, including adversarial attacks. In\nthis paper, we first discuss different vulnerabilities that can be exploited\nfor generating security attacks for neural network-based systems. We then\nprovide an overview of existing adversarial and fault-injection-based attacks\non DNNs. We also present a brief analysis to highlight different challenges in\nthe practical implementation of adversarial attacks. Finally, we also discuss\nvarious prospective ways to develop robust DNN-based systems that are resilient\nto adversarial and fault-injection attacks.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:11:03 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Khalid", "Faiq", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2105.03266", "submitter": "Samyak Prajapati", "authors": "Samyak Prajapati, Aman Swaraj, Ronak Lalwani, Akhil Narwal, Karan\n  Verma, Ghanshyam Singh, Ashok Kumar", "title": "Comparison of Traditional and Hybrid Time Series Models for Forecasting\n  COVID-19 Cases", "comments": null, "journal-ref": null, "doi": "10.21203/rs.3.rs-493195/v1", "report-no": null, "categories": "cs.SI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Time series forecasting methods play critical role in estimating the spread\nof an epidemic. The coronavirus outbreak of December 2019 has already infected\nmillions all over the world and continues to spread on. Just when the curve of\nthe outbreak had started to flatten, many countries have again started to\nwitness a rise in cases which is now being referred as the 2nd wave of the\npandemic. A thorough analysis of time-series forecasting models is therefore\nrequired to equip state authorities and health officials with immediate\nstrategies for future times. This aims of the study are three-fold: (a) To\nmodel the overall trend of the spread; (b) To generate a short-term forecast of\n10 days in countries with the highest incidence of confirmed cases (USA, India\nand Brazil); (c) To quantitatively determine the algorithm that is best suited\nfor precise modelling of the linear and non-linear features of the time series.\nThe comparison of forecasting models for the total cumulative cases of each\ncountry is carried out by comparing the reported data and the predicted value,\nand then ranking the algorithms (Prophet, Holt-Winters, LSTM, ARIMA, and\nARIMA-NARNN) based on their RMSE, MAE and MAPE values. The hybrid combination\nof ARIMA and NARNN (Nonlinear Auto-Regression Neural Network) gave the best\nresult among the selected models with a reduced RMSE, which proved to be almost\n35.3% better than one of the most prevalent method of time-series prediction\n(ARIMA). The results demonstrated the efficacy of the hybrid implementation of\nthe ARIMA-NARNN model over other forecasting methods such as Prophet, Holt\nWinters, LSTM, and the ARIMA model in encapsulating the linear as well as\nnon-linear patterns of the epidemical datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:56:27 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Prajapati", "Samyak", ""], ["Swaraj", "Aman", ""], ["Lalwani", "Ronak", ""], ["Narwal", "Akhil", ""], ["Verma", "Karan", ""], ["Singh", "Ghanshyam", ""], ["Kumar", "Ashok", ""]]}, {"id": "2105.03270", "submitter": "Ergin Genc", "authors": "Ergin Utku Genc, Nilesh Ahuja, Ibrahima J Ndiour, Omesh Tickoo", "title": "Energy-Based Anomaly Detection and Localization", "comments": "9 pages, 3 figures, as submitted to EBM ICLR 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief sketches initial progress towards a unified energy-based solution\nfor the semi-supervised visual anomaly detection and localization problem. In\nthis setup, we have access to only anomaly-free training data and want to\ndetect and identify anomalies of an arbitrary nature on test data. We employ\nthe density estimates from the energy-based model (EBM) as normalcy scores that\ncan be used to discriminate normal images from anomalous ones. Further, we\nback-propagate the gradients of the energy score with respect to the image in\norder to generate a gradient map that provides pixel-level spatial localization\nof the anomalies in the image. In addition to the spatial localization, we show\nthat simple processing of the gradient map can also provide alternative\nnormalcy scores that either match or surpass the detection performance obtained\nwith the energy value. To quantitatively validate the performance of the\nproposed method, we conduct experiments on the MVTec industrial dataset. Though\nstill preliminary, our results are very promising and reveal the potential of\nEBMs for simultaneously detecting and localizing unforeseen anomalies in\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:49:17 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Genc", "Ergin Utku", ""], ["Ahuja", "Nilesh", ""], ["Ndiour", "Ibrahima J", ""], ["Tickoo", "Omesh", ""]]}, {"id": "2105.03279", "submitter": "Mantas Luko\\v{s}evi\\v{c}ius", "authors": "Lukas Stankevi\\v{c}ius and Mantas Luko\\v{s}evi\\v{c}ius", "title": "Generating abstractive summaries of Lithuanian news articles using a\n  transformer model", "comments": "Accepted in ICIST 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we train the first monolingual Lithuanian transformer model on\na relatively large corpus of Lithuanian news articles and compare various\noutput decoding algorithms for abstractive news summarization. We achieve an\naverage ROUGE-2 score 0.163, generated summaries are coherent and look\nimpressive at first glance. However, some of them contain misleading\ninformation that is not so easy to spot. We describe all the technical details\nand share our trained model and accompanying code in an online open-source\nrepository, as well as some characteristic samples of the generated summaries.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 20:10:42 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 13:36:37 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Stankevi\u010dius", "Lukas", ""], ["Luko\u0161evi\u010dius", "Mantas", ""]]}, {"id": "2105.03280", "submitter": "Tosin Adewumi", "authors": "Tosin P. Adewumi, Saleha Javed, Roshanak Vadoodi, Aparajita Tripathy,\n  Konstantina Nikolaidou, Foteini Liwicki and Marcus Liwicki", "title": "Potential Idiomatic Expression (PIE)-English: Corpus for Classes of\n  Idioms", "comments": "7 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a fairly large, Potential Idiomatic Expression (PIE) dataset for\nNatural Language Processing (NLP) in English. The challenges with NLP systems\nwith regards to tasks such as Machine Translation (MT), word sense\ndisambiguation (WSD) and information retrieval make it imperative to have a\nlabelled idioms dataset with classes such as it is in this work. To the best of\nthe authors' knowledge, this is the first idioms corpus with classes of idioms\nbeyond the literal and the general idioms classification. In particular, the\nfollowing classes are labelled in the dataset: metaphor, simile, euphemism,\nparallelism, personification, oxymoron, paradox, hyperbole, irony and literal.\nMany past efforts have been limited in the corpus size and classes of samples\nbut this dataset contains over 20,100 samples with almost 1,200 cases of idioms\n(with their meanings) from 10 classes (or senses). The corpus may also be\nextended by researchers to meet specific needs. The corpus has part of speech\n(PoS) tagging from the NLTK library. Classification experiments performed on\nthe corpus to obtain a baseline and comparison among three common models,\nincluding the BERT model, give good results. We also make publicly available\nthe corpus and the relevant codes for working with it for NLP tasks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 13:05:29 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Adewumi", "Tosin P.", ""], ["Javed", "Saleha", ""], ["Vadoodi", "Roshanak", ""], ["Tripathy", "Aparajita", ""], ["Nikolaidou", "Konstantina", ""], ["Liwicki", "Foteini", ""], ["Liwicki", "Marcus", ""]]}, {"id": "2105.03287", "submitter": "Stefan Schouten BSc", "authors": "Michael Neely, Stefan F. Schouten, Maurits J. R. Bleeker, and Ana\n  Lucic", "title": "Order in the Court: Explainable AI Methods Prone to Disagreement", "comments": "Accepted for presentation at the ICML Workshop on Theoretic\n  Foundation, Criticism, and Application Trend of Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By computing the rank correlation between attention weights and\nfeature-additive explanation methods, previous analyses either invalidate or\nsupport the role of attention-based explanations as a faithful and plausible\nmeasure of salience. To investigate whether this approach is appropriate, we\ncompare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and\nattention-based explanations, applied to two neural architectures trained on\nsingle- and pair-sequence language tasks. In most cases, we find that none of\nour chosen methods agree. Based on our empirical observations and theoretical\nobjections, we conclude that rank correlation does not measure the quality of\nfeature-additive methods. Practitioners should instead use the numerous and\nrigorous diagnostic methods proposed by the community.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:27:37 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:45:39 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 09:51:48 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Neely", "Michael", ""], ["Schouten", "Stefan F.", ""], ["Bleeker", "Maurits J. R.", ""], ["Lucic", "Ana", ""]]}, {"id": "2105.03288", "submitter": "Ahmet M. Elbir", "authors": "Ahmet M. Elbir and Sinem Coleri", "title": "A Family of Hybrid Federated and Centralized Learning Architectures in\n  Machine Learning", "comments": "This paper introduces the first implementation of hybrid technique\n  involving FL and CL, in which all of the clients can participate to the\n  training regardless of their resources for model computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the machine learning tasks focus on centralized learning (CL), which\nrequires the transmission of local datasets from the clients to a parameter\nserver (PS) entailing huge communication overhead. To overcome this, federated\nlearning (FL) has been a promising tool, wherein the clients send only the\nmodel updates to the PS instead of the whole dataset. However, FL demands\npowerful computational resources from the clients. Therefore, not all the\nclients can participate in training if they do not have enough computational\nresources. To address this issue, we introduce a more practical approach called\nhybrid federated and centralized learning (HFCL), wherein only the clients with\nsufficient resources employ FL, while the remaining ones send their datasets to\nthe PS, which computes the model on behalf of them. Then, the model parameters\ncorresponding to all clients are aggregated at the PS. To improve the\nefficiency of dataset transmission, we propose two different techniques:\nincreased computation-per-client and sequential data transmission. The HFCL\nframeworks outperform FL with up to $20\\%$ improvement in the learning accuracy\nwhen only half of the clients perform FL while having $50\\%$ less communication\noverhead than CL since all the clients collaborate on the learning process with\ntheir datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:28:33 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Elbir", "Ahmet M.", ""], ["Coleri", "Sinem", ""]]}, {"id": "2105.03289", "submitter": "Hichem Mrabet", "authors": "Hichem Mrabet, Elias Giaccoumidis and Iyad Dayoub", "title": "A Survey of Applied Machine Learning Techniques for Optical OFDM based\n  Networks", "comments": "26 pages, 4 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this survey, we analyze the newest machine learning (ML) techniques for\noptical orthogonal frequency division multiplexing (O-OFDM)-based optical\ncommunications. ML has been proposed to mitigate channel and transceiver\nimperfections. For instance, ML can improve the signal quality under low\nmodulation extinction ratio or can tackle both determinist and\nstochastic-induced nonlinearities such as parametric noise amplification in\nlong-haul transmission. The proposed ML algorithms for O-OFDM can in\nparticularly tackle inter-subcarrier nonlinear effects such as four-wave mixing\nand cross-phase modulation. In essence, these ML techniques could be beneficial\nfor any multi-carrier approach (e.g. filter bank modulation). Supervised and\nunsupervised ML techniques are analyzed in terms of both O-OFDM transmission\nperformance and computational complexity for potential real-time\nimplementation. We indicate the strict conditions under which a ML algorithm\nshould perform classification, regression or clustering. The survey also\ndiscusses open research issues and future directions towards the ML\nimplementation.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:29:25 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Mrabet", "Hichem", ""], ["Giaccoumidis", "Elias", ""], ["Dayoub", "Iyad", ""]]}, {"id": "2105.03299", "submitter": "Yunshan Ma", "authors": "Yujuan Ding, Yunshan Ma, Lizi Liao, Wai Keung Wong, Tat-Seng Chua", "title": "Leveraging Multiple Relations for Fashion Trend Forecasting Based on\n  Social Media", "comments": "12 pages, 8 figures", "journal-ref": "IEEE Transaction on Multimedia, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion trend forecasting is of great research significance in providing\nuseful suggestions for both fashion companies and fashion lovers. Although\nvarious studies have been devoted to tackling this challenging task, they only\nstudied limited fashion elements with highly seasonal or simple patterns, which\ncould hardly reveal the real complex fashion trends. Moreover, the mainstream\nsolutions for this task are still statistical-based and solely focus on\ntime-series data modeling, which limit the forecast accuracy. Towards\ninsightful fashion trend forecasting, previous work [1] proposed to analyze\nmore fine-grained fashion elements which can informatively reveal fashion\ntrends. Specifically, it focused on detailed fashion element trend forecasting\nfor specific user groups based on social media data. In addition, it proposed a\nneural network-based method, namely KERN, to address the problem of fashion\ntrend modeling and forecasting. In this work, to extend the previous work, we\npropose an improved model named Relation Enhanced Attention Recurrent (REAR)\nnetwork. Compared to KERN, the REAR model leverages not only the relations\namong fashion elements but also those among user groups, thus capturing more\ntypes of correlations among various fashion trends. To further improve the\nperformance of long-range trend forecasting, the REAR method devises a sliding\ntemporal attention mechanism, which is able to capture temporal patterns on\nfuture horizons better. Extensive experiments and more analysis have been\nconducted on the FIT and GeoStyle datasets to evaluate the performance of REAR.\nExperimental and analytical results demonstrate the effectiveness of the\nproposed REAR model in fashion trend forecasting, which also show the\nimprovement of REAR compared to the KERN.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:52:03 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 07:41:25 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ding", "Yujuan", ""], ["Ma", "Yunshan", ""], ["Liao", "Lizi", ""], ["Wong", "Wai Keung", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2105.03308", "submitter": "Daniel Rudolf", "authors": "Viacheslav Natarovskii, Daniel Rudolf, Bj\\\"orn Sprungk", "title": "Geometric convergence of elliptical slice sampling", "comments": "13 pages, 2 figures, Accepted in the Proceedings of the 38th\n  International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Bayesian learning, given likelihood function and Gaussian prior, the\nelliptical slice sampler, introduced by Murray, Adams and MacKay 2010, provides\na tool for the construction of a Markov chain for approximate sampling of the\nunderlying posterior distribution. Besides of its wide applicability and\nsimplicity its main feature is that no tuning is necessary. Under weak\nregularity assumptions on the posterior density we show that the corresponding\nMarkov chain is geometrically ergodic and therefore yield qualitative\nconvergence guarantees. We illustrate our result for Gaussian posteriors as\nthey appear in Gaussian process regression, as well as in a setting of a\nmulti-modal distribution. Remarkably, our numerical experiments indicate a\ndimension-independent performance of elliptical slice sampling even in\nsituations where our ergodicity result does not apply.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:00:30 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 07:55:37 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 19:13:43 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Natarovskii", "Viacheslav", ""], ["Rudolf", "Daniel", ""], ["Sprungk", "Bj\u00f6rn", ""]]}, {"id": "2105.03310", "submitter": "Yuan Pu", "authors": "Yuan Pu, Shaochen Wang, Xin Yao, Bin Li", "title": "Context-Based Soft Actor Critic for Environments with Non-stationary\n  Dynamics", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of deep reinforcement learning methods prone to degenerate\nwhen applied to environments with non-stationary dynamics. In this paper, we\nutilize the latent context recurrent encoders motivated by recent Meta-RL\nmaterials, and propose the Latent Context-based Soft Actor Critic (LC-SAC)\nmethod to address aforementioned issues. By minimizing the contrastive\nprediction loss function, the learned context variables capture the information\nof the environment dynamics and the recent behavior of the agent. Then combined\nwith the soft policy iteration paradigm, the LC-SAC method alternates between\nsoft policy evaluation and soft policy improvement until it converges to the\noptimal policy. Experimental results show that the performance of LC-SAC is\nsignificantly better than the SAC algorithm on the MetaWorld ML1 tasks whose\ndynamics changes drasticly among different episodes, and is comparable to SAC\non the continuous control benchmark task MuJoCo whose dynamics changes slowly\nor doesn't change between different episodes. In addition, we also conduct\nrelevant experiments to determine the impact of different hyperparameter\nsettings on the performance of the LC-SAC algorithm and give the reasonable\nsuggestions of hyperparameter setting.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:00:59 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 09:25:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Pu", "Yuan", ""], ["Wang", "Shaochen", ""], ["Yao", "Xin", ""], ["Li", "Bin", ""]]}, {"id": "2105.03316", "submitter": "Shalin Shah", "authors": "Shalin Shah, Ryan Siskind", "title": "Multi-Task Learning of Query Intent and Named Entities using Transfer\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Named entity recognition (NER) has been studied extensively and the earlier\nalgorithms were based on sequence labeling like Hidden Markov Models (HMM) and\nconditional random fields (CRF). These were followed by neural network based\ndeep learning models. Recently, BERT has shown new state of the art accuracy in\nsequence labeling tasks like NER. In this short article, we study various\napproaches to task specific NER. Task specific NER has two components -\nidentifying the intent of a piece of text (like search queries), and then\nlabeling the query with task specific named entities. For example, we consider\nthe task of labeling Target store locations in a search query (which could be\nentered in a search box or spoken in a device like Alexa or Google Home). Store\nlocations are highly ambiguous and sometimes it is difficult to differentiate\nbetween say a location and a non-location. For example, \"pickup my order at\norange store\" has \"orange\" as the store location, while \"buy orange at target\"\nhas \"orange\" as a fruit. We explore this difficulty by doing multi-task\nlearning which we call global to local transfer of information. We jointly\nlearn the query intent (i.e. store lookup) and the named entities by using\nmultiple loss functions in our BERT based model and find interesting results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 23:59:00 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Shah", "Shalin", ""], ["Siskind", "Ryan", ""]]}, {"id": "2105.03317", "submitter": "Celine Lee", "authors": "Celine Lee (1 and 2), Justin Gottschlich (1 and 2), Dan Roth (2) ((1)\n  Intel Labs, (2) University of Pennsylvania)", "title": "Toward Code Generation: A Survey and Lessons from Semantic Parsing", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of natural language processing techniques and demand for\nimproved software engineering efficiency, there is an emerging interest in\ntranslating intention from human languages to programming languages. In this\nsurvey paper, we attempt to provide an overview of the growing body of research\nin this space. We begin by reviewing natural language semantic parsing\ntechniques and draw parallels with program synthesis efforts. We then consider\nsemantic parsing works from an evolutionary perspective, with specific analyses\non neuro-symbolic methods, architecture, and supervision. We then analyze\nadvancements in frameworks for semantic parsing for code generation. In\nclosing, we present what we believe are some of the emerging open challenges in\nthis domain.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 22:05:22 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lee", "Celine", "", "1 and 2"], ["Gottschlich", "Justin", "", "1 and 2"], ["Roth", "Dan", ""]]}, {"id": "2105.03322", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen\n  Qin, Donald Metzler", "title": "Are Pre-trained Convolutions Better than Pre-trained Transformers?", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of pre-trained language models, Transformers are the de facto\nchoice of model architectures. While recent research has shown promise in\nentirely convolutional, or CNN, architectures, they have not been explored\nusing the pre-train-fine-tune paradigm. In the context of language models, are\nconvolutional models competitive to Transformers when pre-trained? This paper\ninvestigates this research question and presents several interesting findings.\nAcross an extensive set of experiments on 8 datasets/tasks, we find that\nCNN-based pre-trained models are competitive and outperform their Transformer\ncounterpart in certain scenarios, albeit with caveats. Overall, the findings\noutlined in this paper suggest that conflating pre-training and architectural\nadvances is misguided and that both advances should be considered\nindependently. We believe our research paves the way for a healthy amount of\noptimism in alternative architectures.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:13:30 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Gupta", "Jai", ""], ["Bahri", "Dara", ""], ["Aribandi", "Vamsi", ""], ["Qin", "Zhen", ""], ["Metzler", "Donald", ""]]}, {"id": "2105.03323", "submitter": "Anna Weber", "authors": "Anna Weber, Jannis Born and Mar\\'ia Rodr\\'iguez Mart\\'inez", "title": "TITAN: T Cell Receptor Specificity Prediction with Bimodal Attention\n  Networks", "comments": "9 pages, 5 figures, to be published in ISMB 2021 conference\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivation: The activity of the adaptive immune system is governed by T-cells\nand their specific T-cell receptors (TCR), which selectively recognize foreign\nantigens. Recent advances in experimental techniques have enabled sequencing of\nTCRs and their antigenic targets (epitopes), allowing to research the missing\nlink between TCR sequence and epitope binding specificity. Scarcity of data and\na large sequence space make this task challenging, and to date only models\nlimited to a small set of epitopes have achieved good performance. Here, we\nestablish a k-nearest-neighbor (K-NN) classifier as a strong baseline and then\npropose TITAN (Tcr epITope bimodal Attention Networks), a bimodal neural\nnetwork that explicitly encodes both TCR sequences and epitopes to enable the\nindependent study of generalization capabilities to unseen TCRs and/or\nepitopes. Results: By encoding epitopes at the atomic level with SMILES\nsequences, we leverage transfer learning and data augmentation to enrich the\ninput data space and boost performance. TITAN achieves high performance in the\nprediction of specificity of unseen TCRs (ROC-AUC 0.87 in 10-fold CV) and\nsurpasses the results of the current state-of-the-art (ImRex) by a large\nmargin. Notably, our Levenshtein-distance-based K-NN classifier also exhibits\ncompetitive performance on unseen TCRs. While the generalization to unseen\nepitopes remains challenging, we report two major breakthroughs. First, by\ndissecting the attention heatmaps, we demonstrate that the sparsity of\navailable epitope data favors an implicit treatment of epitopes as classes.\nThis may be a general problem that limits unseen epitope performance for\nsufficiently complex models. Second, we show that TITAN nevertheless exhibits\nsignificantly improved performance on unseen epitopes and is capable of\nfocusing attention on chemically meaningful molecular structures.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 09:25:14 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Weber", "Anna", ""], ["Born", "Jannis", ""], ["Mart\u00ednez", "Mar\u00eda Rodr\u00edguez", ""]]}, {"id": "2105.03332", "submitter": "Joongoo Jeon", "authors": "Joongoo Jeon, Sung Joong Kim", "title": "FVM Network to Reduce Computational Cost of CFD Simulation", "comments": "17 pages, 13 figures (except supplementary materials)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid growth of CPU performance, the computational cost to\nsimulate the chemically reacting flow is still infeasible in many cases. There\nare few studies to accelerate the CFD simulation by using neural network\nmodels. However, they noted that it is still difficult to predict multi-step\nCFD time series data. The finite volume method (FVM) which is the basic\nprinciple of most CFD codes seems not to be sufficiently considered in the\nprevious network models. In this study, a FVM network (FVMN) which simulate the\nprinciples of FVM by the tier-input and derivative-output system was proposed.\nThe performance of this baseline model was evaluated using unsteady reacting\nflow datasets. It was confirmed that the maximum relative error of the FVMN\n(0.04%) was much smaller than the general model (1.12%) in the training\ndataset. This difference in error size was more prominent in the prediction\ndatasets. In addition, it was observed that the calculation speed was about 10\ntimes faster in FVMN than CFD solver even under the same CPU condition.\nAlthough the relative error with the ground truth data was significantly\nreduced in the proposed model, the linearly increasing gradient error is a\nremaining issue in longer transient calculations. Therefore, we additionally\nsuggested Machine learning aided CFD framework which can substantially\naccelerate the CFD simulation through alternating computations.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:33:49 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Jeon", "Joongoo", ""], ["Kim", "Sung Joong", ""]]}, {"id": "2105.03336", "submitter": "Tingwei Meng", "authors": "J\\'er\\^ome Darbon and Peter M. Dower and Tingwei Meng", "title": "Neural network architectures using min plus algebra for solving certain\n  high dimensional optimal control problems and Hamilton-Jacobi PDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Solving high dimensional optimal control problems and corresponding\nHamilton-Jacobi PDEs are important but challenging problems in control\nengineering. In this paper, we propose two abstract neural network\narchitectures which respectively represent the value function and the state\nfeedback characterisation of the optimal control for certain class of high\ndimensional optimal control problems. We provide the mathematical analysis for\nthe two abstract architectures. We also show several numerical results computed\nusing the deep neural network implementations of these abstract architectures.\nThis work paves the way to leverage efficient dedicated hardware designed for\nneural networks to solve high dimensional optimal control problems and\nHamilton-Jacobi PDEs.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:40:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Darbon", "J\u00e9r\u00f4me", ""], ["Dower", "Peter M.", ""], ["Meng", "Tingwei", ""]]}, {"id": "2105.03343", "submitter": "Yang Gao", "authors": "Yang Gao and Nicolo Colombo and Wei Wang", "title": "Adapting by Pruning: A Case Study on BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adapting pre-trained neural models to downstream tasks has become the\nstandard practice for obtaining high-quality models. In this work, we propose a\nnovel model adaptation paradigm, adapting by pruning, which prunes neural\nconnections in the pre-trained model to optimise the performance on the target\ntask; all remaining connections have their weights intact. We formulate\nadapting-by-pruning as an optimisation problem with a differentiable loss and\npropose an efficient algorithm to prune the model. We prove that the algorithm\nis near-optimal under standard assumptions and apply the algorithm to adapt\nBERT to some GLUE tasks. Results suggest that our method can prune up to 50%\nweights in BERT while yielding similar performance compared to the fine-tuned\nfull model. We also compare our method with other state-of-the-art pruning\nmethods and study the topological differences of their obtained sub-networks.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:51:08 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Gao", "Yang", ""], ["Colombo", "Nicolo", ""], ["Wang", "Wei", ""]]}, {"id": "2105.03346", "submitter": "Antonino Sabetta", "authors": "Therese Fehrer, Roc\\'io Cabrera Lozoya, Antonino Sabetta, Dario Di\n  Nucci, Damian A. Tamburri", "title": "Detecting Security Fixes in Open-Source Repositories using Static Code\n  Analyzers", "comments": "Submitted to ESEC/FSE 2021, Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sources of reliable, code-level information about vulnerabilities that\naffect open-source software (OSS) are scarce, which hinders a broad adoption of\nadvanced tools that provide code-level detection and assessment of vulnerable\nOSS dependencies.\n  In this paper, we study the extent to which the output of off-the-shelf\nstatic code analyzers can be used as a source of features to represent commits\nin Machine Learning (ML) applications. In particular, we investigate how such\nfeatures can be used to construct embeddings and train ML models to\nautomatically identify source code commits that contain vulnerability fixes.\n  We analyze such embeddings for security-relevant and non-security-relevant\ncommits, and we show that, although in isolation they are not different in a\nstatistically significant manner, it is possible to use them to construct a ML\npipeline that achieves results comparable with the state of the art.\n  We also found that the combination of our method with commit2vec represents a\ntangible improvement over the state of the art in the automatic identification\nof commits that fix vulnerabilities: the ML models we construct and commit2vec\nare complementary, the former being more generally applicable, albeit not as\naccurate.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:57:17 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Fehrer", "Therese", ""], ["Lozoya", "Roc\u00edo Cabrera", ""], ["Sabetta", "Antonino", ""], ["Di Nucci", "Dario", ""], ["Tamburri", "Damian A.", ""]]}, {"id": "2105.03358", "submitter": "Mohammad Abuzar Shaikh", "authors": "Soumyya Kanti Datta, Mohammad Abuzar Shaikh, Sargur N. Srihari,\n  Mingchen Gao", "title": "Soft-Attention Improves Skin Cancer Classification Performance", "comments": "8 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In clinical applications, neural networks must focus on and highlight the\nmost important parts of an input image. Soft-Attention mechanism enables a\nneural network toachieve this goal. This paper investigates the effectiveness\nof Soft-Attention in deep neural architectures. The central aim of\nSoft-Attention is to boost the value of important features and suppress the\nnoise-inducing features. We compare the performance of VGG, ResNet,\nInceptionResNetv2 and DenseNet architectures with and without the\nSoft-Attention mechanism, while classifying skin lesions. The original network\nwhen coupled with Soft-Attention outperforms the baseline[16] by 4.7% while\nachieving a precision of 93.7% on HAM10000 dataset [25]. Additionally,\nSoft-Attention coupling improves the sensitivity score by 3.8% compared to\nbaseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly\navailable at github.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 00:13:23 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:25:53 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 19:24:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Datta", "Soumyya Kanti", ""], ["Shaikh", "Mohammad Abuzar", ""], ["Srihari", "Sargur N.", ""], ["Gao", "Mingchen", ""]]}, {"id": "2105.03361", "submitter": "Rahul Parhi", "authors": "Rahul Parhi, Robert D. Nowak", "title": "What Kinds of Functions do Deep Neural Networks Learn? Insights from\n  Variational Spline Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a variational framework to understand the properties of functions\nlearned by deep neural networks with ReLU activation functions fit to data. We\npropose a new function space, which is reminiscent of classical bounded\nvariation spaces, that captures the compositional structure associated with\ndeep neural networks. We derive a representer theorem showing that deep ReLU\nnetworks are solutions to regularized data fitting problems in this function\nspace. The function space consists of compositions of functions from the\n(non-reflexive) Banach spaces of second-order bounded variation in the Radon\ndomain. These are Banach spaces with sparsity-promoting norms, giving insight\ninto the role of sparsity in deep neural networks. The neural network solutions\nhave skip connections and rank bounded weight matrices, providing new\ntheoretical support for these common architectural choices. The variational\nproblem we study can be recast as a finite-dimensional neural network training\nproblem with regularization schemes related to the notions of weight decay and\npath-norm regularization. Finally, our analysis builds on techniques from\nvariational spline theory, providing new connections between deep neural\nnetworks and splines.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:18:22 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:02:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Parhi", "Rahul", ""], ["Nowak", "Robert D.", ""]]}, {"id": "2105.03363", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Xihuai Wang, Jian Shen, Ming Zhou", "title": "Model-based Multi-agent Policy Optimization with Adaptive Opponent-wise\n  Rollouts", "comments": "Paper accepted at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the model-based methods in multi-agent reinforcement\nlearning (MARL). We specify the dynamics sample complexity and the opponent\nsample complexity in MARL, and conduct a theoretic analysis of return\ndiscrepancy upper bound. To reduce the upper bound with the intention of low\nsample complexity during the whole learning process, we propose a novel\ndecentralized model-based MARL method, named Adaptive Opponent-wise Rollout\nPolicy Optimization (AORPO). In AORPO, each agent builds its multi-agent\nenvironment model, consisting of a dynamics model and multiple opponent models,\nand trains its policy with the adaptive opponent-wise rollout. We further prove\nthe theoretic convergence of AORPO under reasonable assumptions. Empirical\nexperiments on competitive and cooperative tasks demonstrate that AORPO can\nachieve improved sample efficiency with comparable asymptotic performance over\nthe compared MARL methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:20:22 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 14:48:20 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhang", "Weinan", ""], ["Wang", "Xihuai", ""], ["Shen", "Jian", ""], ["Zhou", "Ming", ""]]}, {"id": "2105.03371", "submitter": "Haoyu Ren", "authors": "Haoyu Ren, Darko Anicic, Thomas Runkler", "title": "The Synergy of Complex Event Processing and Tiny Machine Learning in\n  Industrial IoT", "comments": "Accepted by The 15th ACM International Conference on Distributed and\n  Event-based Systems (DEBS) 2021", "journal-ref": null, "doi": "10.1145/3465480.3466928", "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on comprehensive networking, big data, and artificial intelligence,\nthe Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness\nin factory operations. Various sensors and field devices play a central role,\nas they generate a vast amount of real-time data that can provide insights into\nmanufacturing. The synergy of complex event processing (CEP) and machine\nlearning (ML) has been developed actively in the last years in IIoT to identify\npatterns in heterogeneous data streams and fuse raw data into tangible facts.\nIn a traditional compute-centric paradigm, the raw field data are continuously\nsent to the cloud and processed centrally. As IIoT devices become increasingly\npervasive and ubiquitous, concerns are raised since transmitting such amount of\ndata is energy-intensive, vulnerable to be intercepted, and subjected to high\nlatency. The data-centric paradigm can essentially solve these problems by\nempowering IIoT to perform decentralized on-device ML and CEP, keeping data\nprimarily on edge devices and minimizing communications. However, this is no\nmean feat because most IIoT edge devices are designed to be computationally\nconstrained with low power consumption. This paper proposes a framework that\nexploits ML and CEP's synergy at the edge in distributed sensor networks. By\nleveraging tiny ML and micro CEP, we shift the computation from the cloud to\nthe power-constrained IIoT devices and allow users to adapt the on-device ML\nmodel and the CEP reasoning logic flexibly on the fly without requiring to\nreupload the whole program. Lastly, we evaluate the proposed solution and show\nits effectiveness and feasibility using an industrial use case of machine\nsafety monitoring.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:58:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ren", "Haoyu", ""], ["Anicic", "Darko", ""], ["Runkler", "Thomas", ""]]}, {"id": "2105.03388", "submitter": "Stanislav Sobolevsky", "authors": "Stanislav Sobolevsky", "title": "Hierarchical Graph Neural Networks", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.CO physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the recent years, Graph Neural Networks have become increasingly popular\nin network analytic and beyond. With that, their architecture noticeable\ndiverges from the classical multi-layered hierarchical organization of the\ntraditional neural networks. At the same time, many conventional approaches in\nnetwork science efficiently utilize the hierarchical approaches to account for\nthe hierarchical organization of the networks, and recent works emphasize their\ncritical importance. This paper aims to connect the dots between the\ntraditional Neural Network and the Graph Neural Network architectures as well\nas the network science approaches, harnessing the power of the hierarchical\nnetwork organization. A Hierarchical Graph Neural Network architecture is\nproposed, supplementing the original input network layer with the hierarchy of\nauxiliary network layers and organizing the computational scheme updating the\nnode features through both - horizontal network connections within each layer\nas well as the vertical connection between the layers. It enables simultaneous\nlearning of the individual node features along with the aggregated network\nfeatures at variable resolution and uses them to improve the convergence and\nstability of the individual node feature learning. The proposed Hierarchical\nGraph Neural network architecture is successfully evaluated on the network\nembedding and modeling as well as network classification, node labeling, and\ncommunity tasks and demonstrates increased efficiency in those.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:47:18 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 14:35:38 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Sobolevsky", "Stanislav", ""]]}, {"id": "2105.03397", "submitter": "Christian Fiedler", "authors": "Christian Fiedler, Carsten W. Scherer, Sebastian Trimpe", "title": "Learning-enhanced robust controller synthesis with rigorous statistical\n  and control-theoretic guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of machine learning with control offers many opportunities,\nin particular for robust control. However, due to strong safety and reliability\nrequirements in many real-world applications, providing rigorous statistical\nand control-theoretic guarantees is of utmost importance, yet difficult to\nachieve for learning-based control schemes. We present a general framework for\nlearning-enhanced robust control that allows for systematic integration of\nprior engineering knowledge, is fully compatible with modern robust control and\nstill comes with rigorous and practically meaningful guarantees. Building on\nthe established Linear Fractional Representation and Integral Quadratic\nConstraints framework, we integrate Gaussian Process Regression as a learning\ncomponent and state-of-the-art robust controller synthesis. In a concrete\nrobust control example, our approach is demonstrated to yield improved\nperformance with more data, while guarantees are maintained throughout.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:11:33 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Fiedler", "Christian", ""], ["Scherer", "Carsten W.", ""], ["Trimpe", "Sebastian", ""]]}, {"id": "2105.03410", "submitter": "Spencer Farrell", "authors": "Spencer Farrell, Arnold Mitnitski, Kenneth Rockwood, Andrew Rutenberg", "title": "Interpretable machine learning for high-dimensional trajectories of\n  aging health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have built a computational model for individual aging trajectories of\nhealth and survival, which contains physical, functional, and biological\nvariables, and is conditioned on demographic, lifestyle, and medical background\ninformation. We combine techniques of modern machine learning with an\ninterpretable interaction network, where health variables are coupled by\nexplicit pair-wise interactions within a stochastic dynamical system. Our model\nis scalable to large longitudinal data sets, is predictive of individual\nhigh-dimensional health trajectories and survival from baseline health states,\nand infers an interpretable network of directed interactions between the health\nvariables. The network identifies plausible physiological connections between\nhealth variables and clusters of strongly connected heath variables. We use\nEnglish Longitudinal Study of Aging (ELSA) data to train our model and show\nthat it performs better than dedicated linear models for health outcomes and\nsurvival. Our model can also be used to generate synthetic individuals that age\nrealistically, to impute missing data, and to simulate future aging outcomes\ngiven arbitrary initial health states.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:42:15 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Farrell", "Spencer", ""], ["Mitnitski", "Arnold", ""], ["Rockwood", "Kenneth", ""], ["Rutenberg", "Andrew", ""]]}, {"id": "2105.03418", "submitter": "Sam Foreman", "authors": "Sam Foreman, Xiao-Yong Jin, and James C. Osborn", "title": "Deep Learning Hamiltonian Monte Carlo", "comments": "8 pages, 7 figures, Published as a workshop paper at ICLR 2021 SimDL\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-lat cond-mat.stat-mech cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We generalize the Hamiltonian Monte Carlo algorithm with a stack of neural\nnetwork layers and evaluate its ability to sample from different topologies in\na two dimensional lattice gauge theory. We demonstrate that our model is able\nto successfully mix between modes of different topologies, significantly\nreducing the computational cost required to generated independent gauge field\nconfigurations. Our implementation is available at\nhttps://github.com/saforem2/l2hmc-qcd .\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:50:18 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Foreman", "Sam", ""], ["Jin", "Xiao-Yong", ""], ["Osborn", "James C.", ""]]}, {"id": "2105.03425", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Yao Xie", "title": "Kernel MMD Two-Sample Tests for Manifold Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a study of kernel MMD two-sample test statistics in the manifold\nsetting, assuming the high-dimensional observations are close to a\nlow-dimensional manifold. We characterize the property of the test (level and\npower) in relation to the kernel bandwidth, the number of samples, and the\nintrinsic dimensionality of the manifold. Specifically, we show that when data\ndensities are supported on a $d$-dimensional sub-manifold $\\mathcal{M}$\nembedded in an $m$-dimensional space, the kernel MMD two-sample test for data\nsampled from a pair of distributions $(p, q)$ that are H\\\"older with order\n$\\beta$ is consistent and powerful when the number of samples $n$ is greater\nthan $\\delta_2(p,q)^{-2-d/\\beta}$ up to certain constant, where $\\delta_2$ is\nthe squared $\\ell_2$-divergence between two distributions on manifold.\nMoreover, to achieve testing consistency under this scaling of $n$, our theory\nsuggests that the kernel bandwidth $\\gamma$ scales with $n^{-1/(d+2\\beta)}$.\nThese results indicate that the kernel MMD two-sample test does not have a\ncurse-of-dimensionality when the data lie on the low-dimensional manifold. We\ndemonstrate the validity of our theory and the property of the MMD test for\nmanifold data using several numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:56:45 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Xie", "Yao", ""]]}, {"id": "2105.03432", "submitter": "Giulio Zhou", "authors": "Giulio Zhou and Gerasimos Lampouras", "title": "Generalising Multilingual Concept-to-Text NLG with Language Agnostic\n  Delexicalisation", "comments": "To be published in the proceedings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept-to-text Natural Language Generation is the task of expressing an\ninput meaning representation in natural language. Previous approaches in this\ntask have been able to generalise to rare or unseen instances by relying on a\ndelexicalisation of the input. However, this often requires that the input\nappears verbatim in the output text. This poses challenges in multilingual\nsettings, where the task expands to generate the output text in multiple\nlanguages given the same input. In this paper, we explore the application of\nmultilingual models in concept-to-text and propose Language Agnostic\nDelexicalisation, a novel delexicalisation method that uses multilingual\npretrained embeddings, and employs a character-level post-editing model to\ninflect words in their correct form during relexicalisation. Our experiments\nacross five datasets and five languages show that multilingual models\noutperform monolingual models in concept-to-text and that our framework\noutperforms previous approaches, especially for low resource languages.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:48:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhou", "Giulio", ""], ["Lampouras", "Gerasimos", ""]]}, {"id": "2105.03464", "submitter": "Andrea Sabo", "authors": "Andrea Sabo, Sina Mehdizadeh, Andrea Iaboni, Babak Taati", "title": "Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults\n  with Dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drug-induced parkinsonism affects many older adults with dementia, often\ncausing gait disturbances. New advances in vision-based human pose-estimation\nhave opened possibilities for frequent and unobtrusive analysis of gait in\nresidential settings. This work proposes novel spatial-temporal graph\nconvolutional network (ST-GCN) architectures and training procedures to predict\nclinical scores of parkinsonism in gait from video of individuals with\ndementia. We propose a two-stage training approach consisting of a\nself-supervised pretraining stage that encourages the ST-GCN model to learn\nabout gait patterns before predicting clinical scores in the finetuning stage.\nThe proposed ST-GCN models are evaluated on joint trajectories extracted from\nvideo and are compared against traditional (ordinal, linear, random forest)\nregression models and temporal convolutional network baselines. Three 2D human\npose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft\nKinect (2D and 3D) are used to extract joint trajectories of 4787 natural\nwalking bouts from 53 older adults with dementia. A subset of 399 walks from 14\nparticipants is annotated with scores of parkinsonism severity on the gait\ncriteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the\nSimpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating\non 3D joint trajectories extracted from the Kinect consistently outperform all\nother models and feature sets. Prediction of parkinsonism scores in natural\nwalking bouts of unseen participants remains a challenging task, with the best\nmodels achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02\nfor UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for\nthis work is available:\nhttps://github.com/TaatiTeam/stgcn_parkinsonism_prediction.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 18:36:49 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 14:03:33 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Sabo", "Andrea", ""], ["Mehdizadeh", "Sina", ""], ["Iaboni", "Andrea", ""], ["Taati", "Babak", ""]]}, {"id": "2105.03480", "submitter": "Haoya Li", "authors": "Haoya Li, Lexing Ying", "title": "A semigroup method for high dimensional elliptic PDEs and eigenvalue\n  problems based on neural networks", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a semigroup method for solving high-dimensional\nelliptic partial differential equations (PDEs) and the associated eigenvalue\nproblems based on neural networks. For the PDE problems, we reformulate the\noriginal equations as variational problems with the help of semigroup operators\nand then solve the variational problems with neural network (NN)\nparameterization. The main advantages are that no mixed second-order derivative\ncomputation is needed during the stochastic gradient descent training and that\nthe boundary conditions are taken into account automatically by the semigroup\noperator. For eigenvalue problems, a primal-dual method is proposed, resolving\nthe constraint with a scalar dual variable. Numerical results are provided to\ndemonstrate the performance of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 19:49:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Haoya", ""], ["Ying", "Lexing", ""]]}, {"id": "2105.03491", "submitter": "Gregor Bachmann", "authors": "Gregor Bachmann, Seyed-Mohsen Moosavi-Dezfooli, Thomas Hofmann", "title": "Uniform Convergence, Adversarial Spheres and a Simple Remedy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has cast doubt on the general framework of uniform convergence\nand its ability to explain generalization in neural networks. By considering a\nspecific dataset, it was observed that a neural network completely\nmisclassifies a projection of the training data (adversarial set), rendering\nany existing generalization bound based on uniform convergence vacuous. We\nprovide an extensive theoretical investigation of the previously studied data\nsetting through the lens of infinitely-wide models. We prove that the Neural\nTangent Kernel (NTK) also suffers from the same phenomenon and we uncover its\norigin. We highlight the important role of the output bias and show\ntheoretically as well as empirically how a sensible choice completely mitigates\nthe problem. We identify sharp phase transitions in the accuracy on the\nadversarial set and study its dependency on the training sample size. As a\nresult, we are able to characterize critical sample sizes beyond which the\neffect disappears. Moreover, we study decompositions of a neural network into a\nclean and noisy part by considering its canonical decomposition into its\ndifferent eigenfunctions and show empirically that for too small bias the\nadversarial phenomenon still persists.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 20:23:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bachmann", "Gregor", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Hofmann", "Thomas", ""]]}, {"id": "2105.03523", "submitter": "Lori Flynn", "authors": "Lori Flynn and William Snavely and Zachary Kurtz", "title": "Test Suites as a Source of Training Data for Static Analysis Alert\n  Classifiers", "comments": "9 pages, 3 figures, 6 tables, to be published in proceedings of\n  Conference on Automation of Software Test (AST 2021)", "journal-ref": null, "doi": null, "report-no": "DM18-0144", "categories": "cs.SE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Flaw-finding static analysis tools typically generate large volumes of code\nflaw alerts including many false positives. To save on human effort to triage\nthese alerts, a significant body of work attempts to use machine learning to\nclassify and prioritize alerts. Identifying a useful set of training data,\nhowever, remains a fundamental challenge in developing such classifiers in many\ncontexts. We propose using static analysis test suites (i.e., repositories of\n\"benchmark\" programs that are purpose-built to test coverage and precision of\nstatic analysis tools) as a novel source of training data. In a case study, we\ngenerated a large quantity of alerts by executing various static analyzers on\nthe Juliet C/C++ test suite, and we automatically derived ground truth labels\nfor these alerts by referencing the Juliet test suite metadata. Finally, we\nused this data to train classifiers to predict whether an alert is a false\npositive. Our classifiers obtained high precision (90.2%) and recall (88.2%)\nfor a large number of code flaw types on a hold-out test set. This preliminary\nresult suggests that pre-training classifiers on test suite data could help to\njumpstart static analysis alert classification in data-limited contexts.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 22:17:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Flynn", "Lori", ""], ["Snavely", "William", ""], ["Kurtz", "Zachary", ""]]}, {"id": "2105.03534", "submitter": "Eamon Whalen", "authors": "Eamon Whalen, Azariah Beyene, Caitlin Mueller", "title": "SimJEB: Simulated Jet Engine Bracket Dataset", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.14353", "report-no": null, "categories": "cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in geometric deep learning have enabled a new class of\nengineering surrogate models; however, few existing shape datasets are\nwell-suited to evaluate them. This paper introduces the Simulated Jet Engine\nBracket Dataset (SimJEB): a new, public collection of crowdsourced mechanical\nbrackets and high-fidelity structural simulations designed specifically for\nsurrogate modeling. SimJEB models are more complex, diverse, and realistic than\nthe synthetically generated datasets commonly used in parametric surrogate\nmodel evaluation. In contrast to existing engineering shape collections,\nSimJEB's models are all designed for the same engineering function and thus\nhave consistent structural loads and support conditions. The models in SimJEB\nwere collected from the original submissions to the GrabCAD Jet Engine Bracket\nChallenge: an open engineering design competition with over 700 hand-designed\nCAD entries from 320 designers representing 56 countries. Each model has been\ncleaned, categorized, meshed, and simulated with finite element analysis\naccording to the original competition specifications. The result is a\ncollection of diverse, high-quality and application-focused designs for\nadvancing geometric deep learning and engineering surrogate models.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 23:24:21 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Whalen", "Eamon", ""], ["Beyene", "Azariah", ""], ["Mueller", "Caitlin", ""]]}, {"id": "2105.03536", "submitter": "Lisa Wang", "authors": "AmirAli Abdolrashidi, Lisa Wang, Shivani Agrawal, Jonathan Malmaud,\n  Oleg Rybakov, Chas Leichner, Lukasz Lew", "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit", "comments": "8 pages. Accepted at the Efficient Deep Learning for Computer Vision\n  Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization has become a popular technique to compress neural networks and\nreduce compute cost, but most prior work focuses on studying quantization\nwithout changing the network size. Many real-world applications of neural\nnetworks have compute cost and memory budgets, which can be traded off with\nmodel quality by changing the number of parameters. In this work, we use ResNet\nas a case study to systematically investigate the effects of quantization on\ninference compute cost-quality tradeoff curves. Our results suggest that for\neach bfloat16 ResNet model, there are quantized models with lower cost and\nhigher accuracy; in other words, the bfloat16 compute cost-quality tradeoff\ncurve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily\nquantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve\nstate-of-the-art results on ImageNet for 4-bit ResNet-50 with\nquantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We\ndemonstrate the regularizing effect of quantization by measuring the\ngeneralization gap. The quantization method we used is optimized for\npracticality: It requires little tuning and is designed with hardware\ncapabilities in mind. Our work motivates further research into optimal numeric\nformats for quantization, as well as the development of machine learning\naccelerators supporting these formats. As part of this work, we contribute a\nquantization library written in JAX, which is open-sourced at\nhttps://github.com/google-research/google-research/tree/master/aqt.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 23:28:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Abdolrashidi", "AmirAli", ""], ["Wang", "Lisa", ""], ["Agrawal", "Shivani", ""], ["Malmaud", "Jonathan", ""], ["Rybakov", "Oleg", ""], ["Leichner", "Chas", ""], ["Lew", "Lukasz", ""]]}, {"id": "2105.03541", "submitter": "Tianyu Liu", "authors": "Tianyu Liu and Lingyu Zhang", "title": "Apply Artificial Neural Network to Solving Manpower Scheduling Problem", "comments": "none", "journal-ref": "BDAI 2021", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The manpower scheduling problem is a kind of critical combinational\noptimization problem. Researching solutions to scheduling problems can improve\nthe efficiency of companies, hospitals, and other work units. This paper\nproposes a new model combined with deep learning to solve the multi-shift\nmanpower scheduling problem based on the existing research. This model first\nsolves the objective function's optimized value according to the current\nconstraints to find the plan of employee arrangement initially. It will then\nuse the scheduling table generation algorithm to obtain the scheduling result\nin a short time. Moreover, the most prominent feature we propose is that we\nwill use the neural network training method based on the time series to solve\nlong-term and long-period scheduling tasks and obtain manpower arrangement. The\nselection criteria of the neural network and the training process are also\ndescribed in this paper. We demonstrate that our model can make a precise\nforecast based on the improvement of neural networks. This paper also discusses\nthe challenges in the neural network training process and obtains enlightening\nresults after getting the arrangement plan. Our research shows that neural\nnetworks and deep learning strategies have the potential to solve similar\nproblems effectively.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 23:54:00 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liu", "Tianyu", ""], ["Zhang", "Lingyu", ""]]}, {"id": "2105.03542", "submitter": "Aswin Sivaraman", "authors": "Aswin Sivaraman, Minje Kim", "title": "Zero-Shot Personalized Speech Enhancement through Speaker-Informed Model\n  Selection", "comments": "5 pages, 3 figures, submitted to 2021 IEEE Workshop on Applications\n  of Signal Processing to Audio and Acoustics (WASPAA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel zero-shot learning approach towards personalized\nspeech enhancement through the use of a sparsely active ensemble model.\nOptimizing speech denoising systems towards a particular test-time speaker can\nimprove performance and reduce run-time complexity. However, test-time model\nadaptation may be challenging if collecting data from the test-time speaker is\nnot possible. To this end, we propose using an ensemble model wherein each\nspecialist module denoises noisy utterances from a distinct partition of\ntraining set speakers. The gating module inexpensively estimates test-time\nspeaker characteristics in the form of an embedding vector and selects the most\nappropriate specialist module for denoising the test signal. Grouping the\ntraining set speakers into non-overlapping semantically similar groups is\nnon-trivial and ill-defined. To do this, we first train a Siamese network using\nnoisy speech pairs to maximize or minimize the similarity of its output vectors\ndepending on whether the utterances derive from the same speaker or not. Next,\nwe perform k-means clustering on the latent space formed by the averaged\nembedding vectors per training set speaker. In this way, we designate speaker\ngroups and train specialist modules optimized around partitions of the complete\ntraining set. Our experiments show that ensemble models made up of low-capacity\nspecialists can outperform high-capacity generalist models with greater\nefficiency and improved adaptation towards unseen test-time speakers.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 00:15:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sivaraman", "Aswin", ""], ["Kim", "Minje", ""]]}, {"id": "2105.03544", "submitter": "Sunwoo Kim", "authors": "Sunwoo Kim and Minje Kim", "title": "Test-Time Adaptation Toward Personalized Speech Enhancement: Zero-Shot\n  Learning with Knowledge Distillation", "comments": "5 pages, 5 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In realistic speech enhancement settings for end-user devices, we often\nencounter only a few speakers and noise types that tend to reoccur in the\nspecific acoustic environment. We propose a novel personalized speech\nenhancement method to adapt a compact denoising model to the test-time\nspecificity. Our goal in this test-time adaptation is to utilize no clean\nspeech target of the test speaker, thus fulfilling the requirement for\nzero-shot learning. To complement the lack of clean utterance, we employ the\nknowledge distillation framework. Instead of the missing clean utterance\ntarget, we distill the more advanced denoising results from an overly large\nteacher model, and use it as the pseudo target to train the small student\nmodel. This zero-shot learning procedure circumvents the process of collecting\nusers' clean speech, a process that users are reluctant to comply due to\nprivacy concerns and technical difficulty of recording clean voice. Experiments\non various test-time conditions show that the proposed personalization method\nachieves significant performance gains compared to larger baseline networks\ntrained from a large speaker- and noise-agnostic datasets. In addition, since\nthe compact personalized models can outperform larger general-purpose models,\nwe claim that the proposed method performs model compression with no loss of\ndenoising performance.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 00:42:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kim", "Sunwoo", ""], ["Kim", "Minje", ""]]}, {"id": "2105.03546", "submitter": "Austin Nguyen", "authors": "Austin Anhkhoi Nguyen", "title": "Scalable, Decentralized Multi-Agent Reinforcement Learning Methods\n  Inspired by Stigmergy and Ant Colonies", "comments": "50 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bolstering multi-agent learning algorithms to tackle complex coordination and\ncontrol tasks has been a long-standing challenge of on-going research. Numerous\nmethods have been proposed to help reduce the effects of non-stationarity and\nunscalability. In this work, we investigate a novel approach to decentralized\nmulti-agent learning and planning that attempts to address these two\nchallenges. In particular, this method is inspired by the cohesion,\ncoordination, and behavior of ant colonies. As a result, these algorithms are\ndesigned to be naturally scalable to systems with numerous agents. While no\noptimality is guaranteed, the method is intended to work well in practice and\nscale better in efficacy with the number of agents present than others. The\napproach combines single-agent RL and an ant-colony-inspired decentralized,\nstigmergic algorithm for multi-agent path planning and environment\nmodification. Specifically, we apply this algorithm in a setting where agents\nmust navigate to a goal location, learning to push rectangular boxes into holes\nto yield new traversable pathways. It is shown that while the approach yields\npromising success in this particular environment, it may not be as easily\ngeneralized to others. The algorithm designed is notably scalable to numerous\nagents but is limited in its performance due to its relatively simplistic,\nrule-based approach. Furthermore, the composability of RL-trained policies is\ncalled into question, where, while policies are successful in their training\nenvironments, applying trained policies to a larger-scale, multi-agent\nframework results in unpredictable behavior.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 01:04:51 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Nguyen", "Austin Anhkhoi", ""]]}, {"id": "2105.03567", "submitter": "Weibin Li", "authors": "Weibin Li, Qiwei Zhong, Qingyang Zhao, Hongchun Zhang, Xiaonan Meng", "title": "Multimodal and Contrastive Learning for Click Fraud Detection", "comments": "Accepted to DeMal@WWW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advertising click fraud detection plays one of the vital roles in current\nE-commerce websites as advertising is an essential component of its business\nmodel. It aims at, given a set of corresponding features, e.g., demographic\ninformation of users and statistical features of clicks, predicting whether a\nclick is fraudulent or not in the community. Recent efforts attempted to\nincorporate attributed behavior sequence and heterogeneous network for\nextracting complex features of users and achieved significant effects on click\nfraud detection. In this paper, we propose a Multimodal and Contrastive\nlearning network for Click Fraud detection (MCCF). Specifically, motivated by\nthe observations on differences of demographic information, behavior sequences\nand media relationship between fraudsters and genuine users on E-commerce\nplatform, MCCF jointly utilizes wide and deep features, behavior sequence and\nheterogeneous network to distill click representations. Moreover, these three\nmodules are integrated by contrastive learning and collaboratively contribute\nto the final predictions. With the real-world datasets containing 2.54 million\nclicks on Alibaba platform, we investigate the effectiveness of MCCF. The\nexperimental results show that the proposed approach is able to improve AUC by\n7.2% and F1-score by 15.6%, compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:03:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Weibin", ""], ["Zhong", "Qiwei", ""], ["Zhao", "Qingyang", ""], ["Zhang", "Hongchun", ""], ["Meng", "Xiaonan", ""]]}, {"id": "2105.03568", "submitter": "Enrico Mattei", "authors": "Carter N. Brown, Enrico Mattei, Andrew Draganov", "title": "ChaRRNets: Channel Robust Representation Networks for RF Fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present complex-valued Convolutional Neural Networks (CNNs) for RF\nfingerprinting that go beyond translation invariance and appropriately account\nfor the inductive bias with respect to multipath propagation channels, a\nphenomenon that is specific to the fields of wireless signal processing and\ncommunications. We focus on the problem of fingerprinting wireless IoT devices\nin-the-wild using Deep Learning (DL) techniques. Under these real-world\nconditions, the multipath environments represented in the train and test sets\nwill be different. These differences are due to the physics governing the\npropagation of wireless signals, as well as the limitations of practical data\ncollection campaigns. Our approach follows a group-theoretic framework,\nleverages prior work on DL on manifold-valued data, and extends this prior work\nto the wireless signal processing domain. We introduce the Lie group of\ntransformations that a signal experiences under the multipath propagation model\nand define operations that are equivariant and invariant to the frequency\nresponse of a Finite Impulse Response (FIR) filter to build a ChaRRNet. We\npresent results using synthetic and real-world datasets, and we benchmark\nagainst a strong baseline model, that show the efficacy of our approach. Our\nresults provide evidence of the benefits of incorporating appropriate wireless\ndomain biases into DL models. We hope to spur new work in the area of robust RF\nmachine learning, as the 5G revolution increases demand for enhanced security\nmechanisms.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:03:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Brown", "Carter N.", ""], ["Mattei", "Enrico", ""], ["Draganov", "Andrew", ""]]}, {"id": "2105.03584", "submitter": "Alexander Scheinker", "authors": "Alexander Scheinker, Frederick Cropp, Sergio Paiagua, Daniele\n  Filippetto", "title": "Adaptive Latent Space Tuning for Non-Stationary Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.acc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful deep learning tools, such as convolutional neural networks (CNN),\nare able to learn the input-output relationships of large complicated systems\ndirectly from data. Encoder-decoder deep CNNs are able to extract features\ndirectly from images, mix them with scalar inputs within a general\nlow-dimensional latent space, and then generate new complex 2D outputs which\nrepresent complex physical phenomenon. One important challenge faced by deep\nlearning methods is large non-stationary systems whose characteristics change\nquickly with time for which re-training is not feasible. In this paper we\npresent a method for adaptive tuning of the low-dimensional latent space of\ndeep encoder-decoder style CNNs based on real-time feedback to quickly\ncompensate for unknown and fast distribution shifts. We demonstrate our\napproach for predicting the properties of a time-varying charged particle beam\nin a particle accelerator whose components (accelerating electric fields and\nfocusing magnetic fields) are also quickly changing with time.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:50:45 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 05:02:00 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 13:03:11 GMT"}, {"version": "v4", "created": "Sun, 6 Jun 2021 16:45:26 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Scheinker", "Alexander", ""], ["Cropp", "Frederick", ""], ["Paiagua", "Sergio", ""], ["Filippetto", "Daniele", ""]]}, {"id": "2105.03588", "submitter": "Zhuofa Chen", "authors": "Yousif Khaireddin, Zhuofa Chen", "title": "Facial Emotion Recognition: State of the Art Performance on FER2013", "comments": "9 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial emotion recognition (FER) is significant for human-computer\ninteraction such as clinical practice and behavioral description. Accurate and\nrobust FER by computer models remains challenging due to the heterogeneity of\nhuman faces and variations in images such as different facial pose and\nlighting. Among all techniques for FER, deep learning models, especially\nConvolutional Neural Networks (CNNs) have shown great potential due to their\npowerful automatic feature extraction and computational efficiency. In this\nwork, we achieve the highest single-network classification accuracy on the\nFER2013 dataset. We adopt the VGGNet architecture, rigorously fine-tune its\nhyperparameters, and experiment with various optimization methods. To our best\nknowledge, our model achieves state-of-the-art single-network accuracy of 73.28\n% on FER2013 without using extra training data.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 04:20:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Khaireddin", "Yousif", ""], ["Chen", "Zhuofa", ""]]}, {"id": "2105.03591", "submitter": "Pengyuan Zhou", "authors": "Pengyuan Zhou, Pei Fang, Pan Hui", "title": "Loss Tolerant Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated learning has attracted attention in recent years for\ncollaboratively training data on distributed devices with privacy-preservation.\nThe limited network capacity of mobile and IoT devices has been seen as one of\nthe major challenges for cross-device federated learning. Recent solutions have\nbeen focusing on threshold-based client selection schemes to guarantee the\ncommunication efficiency. However, we find this approach can cause biased\nclient selection and results in deteriorated performance. Moreover, we find\nthat the challenge of network limit may be overstated in some cases and the\npacket loss is not always harmful. In this paper, we explore the loss tolerant\nfederated learning (LT-FL) in terms of aggregation, fairness, and\npersonalization. We use ThrowRightAway (TRA) to accelerate the data uploading\nfor low-bandwidth-devices by intentionally ignoring some packet losses. The\nresults suggest that, with proper integration, TRA and other algorithms can\ntogether guarantee the personalization and fairness performance in the face of\npacket loss below a certain fraction (10%-30%).\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 04:44:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhou", "Pengyuan", ""], ["Fang", "Pei", ""], ["Hui", "Pan", ""]]}, {"id": "2105.03592", "submitter": "Chen Wang", "authors": "Jian Chen, Xuxin Zhang, Rui Zhang, Chen Wang, Ling Liu", "title": "De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks", "comments": "To be published in IEEE Transactions on Information Forensics and\n  Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have been widely applied to various applications.\nHowever, they are potentially vulnerable to data poisoning attacks, where\nsophisticated attackers can disrupt the learning procedure by injecting a\nfraction of malicious samples into the training dataset. Existing defense\ntechniques against poisoning attacks are largely attack-specific: they are\ndesigned for one specific type of attacks but do not work for other types,\nmainly due to the distinct principles they follow. Yet few general defense\nstrategies have been developed. In this paper, we propose De-Pois, an\nattack-agnostic defense against poisoning attacks. The key idea of De-Pois is\nto train a mimic model the purpose of which is to imitate the behavior of the\ntarget model trained by clean samples. We take advantage of Generative\nAdversarial Networks (GANs) to facilitate informative training data\naugmentation as well as the mimic model construction. By comparing the\nprediction differences between the mimic model and the target model, De-Pois is\nthus able to distinguish the poisoned samples from clean ones, without explicit\nknowledge of any ML algorithms or types of poisoning attacks. We implement four\ntypes of poisoning attacks and evaluate De-Pois with five typical defense\nmethods on different realistic datasets. The results demonstrate that De-Pois\nis effective and efficient for detecting poisoned data against all the four\ntypes of poisoning attacks, with both the accuracy and F1-score over 0.9 on\naverage.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 04:47:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Jian", ""], ["Zhang", "Xuxin", ""], ["Zhang", "Rui", ""], ["Wang", "Chen", ""], ["Liu", "Ling", ""]]}, {"id": "2105.03594", "submitter": "Li-Yang Tan", "authors": "Guy Blanc and Jane Lange and Li-Yang Tan", "title": "Learning stochastic decision trees", "comments": "To appear in ICALP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a quasipolynomial-time algorithm for learning stochastic decision\ntrees that is optimally resilient to adversarial noise. Given an\n$\\eta$-corrupted set of uniform random samples labeled by a size-$s$ stochastic\ndecision tree, our algorithm runs in time\n$n^{O(\\log(s/\\varepsilon)/\\varepsilon^2)}$ and returns a hypothesis with error\nwithin an additive $2\\eta + \\varepsilon$ of the Bayes optimal. An additive\n$2\\eta$ is the information-theoretic minimum.\n  Previously no non-trivial algorithm with a guarantee of $O(\\eta) +\n\\varepsilon$ was known, even for weaker noise models. Our algorithm is\nfurthermore proper, returning a hypothesis that is itself a decision tree;\npreviously no such algorithm was known even in the noiseless setting.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 04:54:12 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Blanc", "Guy", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2105.03598", "submitter": "Siwei Wang", "authors": "Siwei Wang, Wei Chen", "title": "Pure Exploration Bandit Problem with General Reward Functions Depending\n  on Full Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we study the pure exploration bandit model on general\ndistribution functions, which means that the reward function of each arm\ndepends on the whole distribution, not only its mean. We adapt the racing\nframework and LUCB framework to solve this problem, and design algorithms for\nestimating the value of the reward functions with different types of\ndistributions. Then we show that our estimation methods have correctness\nguarantee with proper parameters, and obtain sample complexity upper bounds for\nthem. Finally, we discuss about some important applications and their\ncorresponding solutions under our learning framework.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 05:13:13 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Siwei", ""], ["Chen", "Wei", ""]]}, {"id": "2105.03603", "submitter": "Periyapattana Narayana Prasad Karthik", "authors": "P. N. Karthik and Rajesh Sundaresan", "title": "Learning to Detect an Odd Restless Markov Arm with a Trembling Hand", "comments": "49 pages. A shorter version of this manuscript has been accepted for\n  presentation at the 2021 IEEE International Symposium on Information Theory.\n  This manuscript contains the proofs of all the main results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the problem of finding an anomalous arm in a multi-armed\nbandit when (a) each arm is a finite-state Markov process, and (b) the arms are\nrestless. Here, anomaly means that the transition probability matrix (TPM) of\none of the arms (the odd arm) is different from the common TPM of each of the\nnon-odd arms. The TPMs are unknown to a decision entity that wishes to find the\nindex of the odd arm as quickly as possible, subject to an upper bound on the\nerror probability. We derive a problem instance-specific asymptotic lower bound\non the expected time required to find the odd arm index, where the asymptotics\nis as the error probability vanishes. Further, we devise a policy based on the\nprinciple of certainty equivalence, and demonstrate that under a continuous\nselection assumption and a certain regularity assumption on the TPMs, the\npolicy achieves the lower bound arbitrarily closely. Thus, while the lower\nbound is shown for all problem instances, the upper bound is shown only for\nthose problem instances satisfying the continuous selection and the regularity\nassumptions. Our achievability analysis is based on resolving the\nidentifiability problem in the context of a certain lifted countable-state\ncontrolled Markov process.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 05:53:12 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 08:17:21 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Karthik", "P. N.", ""], ["Sundaresan", "Rajesh", ""]]}, {"id": "2105.03616", "submitter": "Ryuichi Kanoh", "authors": "Ryuichi Kanoh, Tomu Yanabe", "title": "Interpretable Mixture Density Estimation by use of Differentiable\n  Tree-module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to develop reliable services using machine learning, it is important\nto understand the uncertainty of the model outputs. Often the probability\ndistribution that the prediction target follows has a complex shape, and a\nmixture distribution is assumed as a distribution that uncertainty follows.\nSince the output of mixture density estimation is complicated, its\ninterpretability becomes important when considering its use in real services.\nIn this paper, we propose a method for mixture density estimation that utilizes\nan interpretable tree structure. Further, a fast inference procedure based on\ntime-invariant information cache achieves both high speed and interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 07:29:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kanoh", "Ryuichi", ""], ["Yanabe", "Tomu", ""]]}, {"id": "2105.03625", "submitter": "Zhishun Wang", "authors": "Zhishun Wang, Wei Lu, Kaixin Zhang, Tianhao Li, Zixi Zhao", "title": "A parallel-network continuous quantitative trading model with GARCH and\n  PPO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a difficult task for both professional investors and individual traders\ncontinuously making profit in stock market. With the development of computer\nscience and deep reinforcement learning, Buy\\&Hold (B\\&H) has been oversteped\nby many artificial intelligence trading algorithms. However, the information\nand process are not enough, which limit the performance of reinforcement\nlearning algorithms. Thus, we propose a parallel-network continuous\nquantitative trading model with GARCH and PPO to enrich the basical deep\nreinforcement learning model, where the deep learning parallel network layers\ndeal with 3 different frequencies data (including GARCH information) and\nproximal policy optimization (PPO) algorithm interacts actions and rewards with\nstock trading environment. Experiments in 5 stocks from Chinese stock market\nshow our method achieves more extra profit comparing with basical reinforcement\nlearning methods and bench models.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 08:00:56 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 12:24:40 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Wang", "Zhishun", ""], ["Lu", "Wei", ""], ["Zhang", "Kaixin", ""], ["Li", "Tianhao", ""], ["Zhao", "Zixi", ""]]}, {"id": "2105.03650", "submitter": "David Tolpin", "authors": "David Tolpin", "title": "How To Train Your Program", "comments": "submitted to PROBPROG11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a Bayesian approach to machine learning with probabilistic\nprograms. In our approach, training on available data is implemented as\ninference on a hierarchical model. The posterior distribution of model\nparameters is then used to \\textit{stochastically condition} a complementary\nmodel, such that inference on new data yields the same posterior distribution\nof latent parameters corresponding to the new data as inference on a\nhierachical model on the combination of both previously available and new data,\nat a lower computation cost. We frame the approach as a design pattern of\nprobabilistic programming referred to herein as `stump and fungus', and\nillustrate realization of the pattern on a didactic case study.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:26:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tolpin", "David", ""]]}, {"id": "2105.03654", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei\n  Huang, Kewei Tu", "title": "Improving Named Entity Recognition by External Context Retrieving and\n  Cooperative Learning", "comments": "Accepted to ACL 2021, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in Named Entity Recognition (NER) show that document-level\ncontexts can significantly improve model performance. In many application\nscenarios, however, such contexts are not available. In this paper, we propose\nto find external contexts of a sentence by retrieving and selecting a set of\nsemantically relevant texts through a search engine, with the original sentence\nas the query. We find empirically that the contextual representations computed\non the retrieval-based input view, constructed through the concatenation of a\nsentence and its external contexts, can achieve significantly improved\nperformance compared to the original input view based only on the sentence.\nFurthermore, we can improve the model performance of both input views by\nCooperative Learning, a training method that encourages the two input views to\nproduce similar contextual representations or output label distributions.\nExperiments show that our approach can achieve new state-of-the-art performance\non 8 NER data sets across 5 domains.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:45:21 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 02:08:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Xinyu", ""], ["Jiang", "Yong", ""], ["Bach", "Nguyen", ""], ["Wang", "Tao", ""], ["Huang", "Zhongqiang", ""], ["Huang", "Fei", ""], ["Tu", "Kewei", ""]]}, {"id": "2105.03660", "submitter": "Chenyu Wen", "authors": "Dario Dematties, Chenyu Wen, Mauricio David P\\'erez, Dian Zhou, Shi-Li\n  Zhang", "title": "Deep learning of nanopore sensing signals using a bi-path network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG physics.bio-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Temporary changes in electrical resistance of a nanopore sensor caused by\ntranslocating target analytes are recorded as a sequence of pulses on current\ntraces. Prevalent algorithms for feature extraction in pulse-like signals lack\nobjectivity because empirical amplitude thresholds are user-defined to single\nout the pulses from the noisy background. Here, we use deep learning for\nfeature extraction based on a bi-path network (B-Net). After training, the\nB-Net acquires the prototypical pulses and the ability of both pulse\nrecognition and feature extraction without a priori assigned parameters. The\nB-Net performance is evaluated on generated datasets and further applied to\nexperimental data of DNA and protein translocation. The B-Net results show\nremarkably small relative errors and stable trends. The B-Net is further shown\ncapable of processing data with a signal-to-noise ratio equal to one, an\nimpossibility for threshold-based algorithms. The developed B-Net is generic\nfor pulse-like signals beyond pulsed nanopore currents.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 10:11:22 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Dematties", "Dario", ""], ["Wen", "Chenyu", ""], ["P\u00e9rez", "Mauricio David", ""], ["Zhou", "Dian", ""], ["Zhang", "Shi-Li", ""]]}, {"id": "2105.03663", "submitter": "Mike Yan Michelis", "authors": "Mike Yan Michelis and Quentin Becker", "title": "On Linear Interpolation in the Latent Space of Deep Generative Models", "comments": "For BibTex and Poster: https://openreview.net/forum?id=SL9w_9M-kSj", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The underlying geometrical structure of the latent space in deep generative\nmodels is in most cases not Euclidean, which may lead to biases when comparing\ninterpolation capabilities of two models. Smoothness and plausibility of linear\ninterpolations in latent space are associated with the quality of the\nunderlying generative model. In this paper, we show that not all such\ninterpolations are comparable as they can deviate arbitrarily from the shortest\ninterpolation curve given by the geodesic. This deviation is revealed by\ncomputing curve lengths with the pull-back metric of the generative model,\nfinding shorter curves than the straight line between endpoints, and measuring\na non-zero relative length improvement on this straight line. This leads to a\nstrategy to compare linear interpolations across two generative models. We also\nshow the effect and importance of choosing an appropriate output space for\ncomputing shorter curves. For this computation we derive an extension of the\npull-back metric.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 10:27:07 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Michelis", "Mike Yan", ""], ["Becker", "Quentin", ""]]}, {"id": "2105.03669", "submitter": "Thomas Wollmann", "authors": "Johannes Otterbach, Thomas Wollmann", "title": "Chameleon: A Semi-AutoML framework targeting quick and scalable\n  development and deployment of production-ready ML systems for SMEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Developing, scaling, and deploying modern Machine Learning solutions remains\nchallenging for small- and middle-sized enterprises (SMEs). This is due to a\nhigh entry barrier of building and maintaining a dedicated IT team as well as\nthe difficulties of real-world data (RWD) compared to standard benchmark data.\nTo address this challenge, we discuss the implementation and concepts of\nChameleon, a semi-AutoML framework. The goal of Chameleon is fast and scalable\ndevelopment and deployment of production-ready machine learning systems into\nthe workflow of SMEs. We first discuss the RWD challenges faced by SMEs. After,\nwe outline the central part of the framework which is a model and loss-function\nzoo with RWD-relevant defaults. Subsequently, we present how one can use a\ntemplatable framework in order to automate the experiment iteration cycle, as\nwell as close the gap between development and deployment. Finally, we touch on\nour testing framework component allowing us to investigate common model failure\nmodes and support best practices of model deployment governance.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 10:43:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Otterbach", "Johannes", ""], ["Wollmann", "Thomas", ""]]}, {"id": "2105.03671", "submitter": "Mauro Piva", "authors": "Mauro Piva, Gaia Maselli, Francesco Restuccia", "title": "The Tags Are Alright: Robust Large-Scale RFID Clone Detection Through\n  Federated Data-Augmented Radio Fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Millions of RFID tags are pervasively used all around the globe to\ninexpensively identify a wide variety of everyday-use objects. One of the key\nissues of RFID is that tags cannot use energy-hungry cryptography. For this\nreason, radio fingerprinting (RFP) is a compelling approach that leverages the\nunique imperfections in the tag's wireless circuitry to achieve large-scale\nRFID clone detection. Recent work, however, has unveiled that time-varying\nchannel conditions can significantly decrease the accuracy of the RFP process.\nWe propose the first large-scale investigation into RFP of RFID tags with\ndynamic channel conditions. Specifically, we perform a massive data collection\ncampaign on a testbed composed by 200 off-the-shelf identical RFID tags and a\nsoftware-defined radio (SDR) tag reader. We collect data with different\ntag-reader distances in an over-the-air configuration. To emulate implanted\nRFID tags, we also collect data with two different kinds of porcine meat\ninserted between the tag and the reader. We use this rich dataset to train and\ntest several convolutional neural network (CNN)--based classifiers in a variety\nof channel conditions. Our investigation reveals that training and testing on\ndifferent channel conditions drastically degrades the classifier's accuracy.\nFor this reason, we propose a novel training framework based on federated\nmachine learning (FML) and data augmentation (DAG) to boost the accuracy.\nExtensive experimental results indicate that (i) our FML approach improves\naccuracy by up to 48%; (ii) our DA approach improves the FML performance by up\nto 31%. To the best of our knowledge, this is the first paper experimentally\ndemonstrating the efficacy of FML and DA on a large device population. We are\nsharing with the research community our fully-labeled 200-GB RFID waveform\ndataset, the entirety of our code and trained models.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 10:48:02 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Piva", "Mauro", ""], ["Maselli", "Gaia", ""], ["Restuccia", "Francesco", ""]]}, {"id": "2105.03678", "submitter": "Fan Wu", "authors": "Fan Wu, Patrick Rebeschini", "title": "Nearly Minimax-Optimal Rates for Noisy Sparse Phase Retrieval via\n  Early-Stopped Mirror Descent", "comments": "arXiv admin note: text overlap with arXiv:2010.10168", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies early-stopped mirror descent applied to noisy sparse phase\nretrieval, which is the problem of recovering a $k$-sparse signal\n$\\mathbf{x}^\\star\\in\\mathbb{R}^n$ from a set of quadratic Gaussian measurements\ncorrupted by sub-exponential noise. We consider the (non-convex) unregularized\nempirical risk minimization problem and show that early-stopped mirror descent,\nwhen equipped with the hyperbolic entropy mirror map and proper initialization,\nachieves a nearly minimax-optimal rate of convergence, provided the sample size\nis at least of order $k^2$ (modulo logarithmic term) and the minimum (in\nmodulus) non-zero entry of the signal is on the order of\n$\\|\\mathbf{x}^\\star\\|_2/\\sqrt{k}$. Our theory leads to a simple algorithm that\ndoes not rely on explicit regularization or thresholding steps to promote\nsparsity. More generally, our results establish a connection between mirror\ndescent and sparsity in the non-convex problem of noisy sparse phase retrieval,\nadding to the literature on early stopping that has mostly focused on\nnon-sparse, Euclidean, and convex settings via gradient descent. Our proof\ncombines a potential-based analysis of mirror descent with a quantitative\ncontrol on a variational coherence property that we establish along the path of\nmirror descent, up to a prescribed stopping time.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 11:22:19 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wu", "Fan", ""], ["Rebeschini", "Patrick", ""]]}, {"id": "2105.03679", "submitter": "Rui Lin", "authors": "Rui Lin, Jie Ran, Dongpeng Wang, King Hung Chiu and Ngai Wong", "title": "EZCrop: Energy-Zoned Channels for Robust Output Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results have revealed an interesting observation in a trained\nconvolutional neural network (CNN), namely, the rank of a feature map channel\nmatrix remains surprisingly constant despite the input images. This has led to\nan effective rank-based channel pruning algorithm, yet the constant rank\nphenomenon remains mysterious and unexplained. This work aims at demystifying\nand interpreting such rank behavior from a frequency-domain perspective, which\nas a bonus suggests an extremely efficient Fast Fourier Transform (FFT)-based\nmetric for measuring channel importance without explicitly computing its rank.\nWe achieve remarkable CNN channel pruning based on this analytically sound and\ncomputationally efficient metric and adopt it for repetitive pruning to\ndemonstrate robustness via our scheme named Energy-Zoned Channels for Robust\nOutput Pruning (EZCrop), which shows consistently better results than other\nstate-of-the-art channel pruning methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 11:31:11 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 05:05:44 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lin", "Rui", ""], ["Ran", "Jie", ""], ["Wang", "Dongpeng", ""], ["Chiu", "King Hung", ""], ["Wong", "Ngai", ""]]}, {"id": "2105.03681", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Guanghui Wang, Jinfeng Yi, Tianbao Yang", "title": "A Simple yet Universal Strategy for Online Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several universal methods have been proposed for online convex\noptimization, and attain minimax rates for multiple types of convex functions\nsimultaneously. However, they need to design and optimize one surrogate loss\nfor each type of functions, which makes it difficult to exploit the structure\nof the problem and utilize the vast amount of existing algorithms. In this\npaper, we propose a simple strategy for universal online convex optimization,\nwhich avoids these limitations. The key idea is to construct a set of experts\nto process the original online functions, and deploy a meta-algorithm over the\n\\emph{linearized} losses to aggregate predictions from experts. Specifically,\nwe choose Adapt-ML-Prod to track the best expert, because it has a second-order\nbound and can be used to leverage strong convexity and exponential concavity.\nIn this way, we can plug in off-the-shelf online solvers as black-box experts\nto deliver problem-dependent regret bounds. Furthermore, our strategy inherits\nthe theoretical guarantee of any expert designed for strongly convex functions\nand exponentially concave functions, up to a double logarithmic factor. For\ngeneral convex functions, it maintains the minimax optimality and also achieves\na small-loss bound.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 11:43:49 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 11:40:31 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhang", "Lijun", ""], ["Wang", "Guanghui", ""], ["Yi", "Jinfeng", ""], ["Yang", "Tianbao", ""]]}, {"id": "2105.03682", "submitter": "Andrea Marinoni", "authors": "Andrea Marinoni, Saloua Chlaily, Eduard Khachatrian, Torbj{\\o}rn\n  Eltoft, Sivasakthy Selvakumaran, Mark Girolami, Christian Jutten", "title": "Enhancing ensemble learning and transfer learning in multimodal data\n  analysis by adaptive dimensionality reduction", "comments": "18 pages, 10 figures, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern data analytics take advantage of ensemble learning and transfer\nlearning approaches to tackle some of the most relevant issues in data\nanalysis, such as lack of labeled data to use to train the analysis models,\nsparsity of the information, and unbalanced distributions of the records.\nNonetheless, when applied to multimodal datasets (i.e., datasets acquired by\nmeans of multiple sensing techniques or strategies), the state-of-theart\nmethods for ensemble learning and transfer learning might show some\nlimitations. In fact, in multimodal data analysis, not all observations would\nshow the same level of reliability or information quality, nor an homogeneous\ndistribution of errors and uncertainties. This condition might undermine the\nclassic assumptions ensemble learning and transfer learning methods rely on. In\nthis work, we propose an adaptive approach for dimensionality reduction to\novercome this issue. By means of a graph theory-based approach, the most\nrelevant features across variable size subsets of the considered datasets are\nidentified. This information is then used to set-up ensemble learning and\ntransfer learning architectures. We test our approach on multimodal datasets\nacquired in diverse research fields (remote sensing, brain-computer interfaces,\nphotovoltaic energy). Experimental results show the validity and the robustness\nof our approach, able to outperform state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 11:53:12 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Marinoni", "Andrea", ""], ["Chlaily", "Saloua", ""], ["Khachatrian", "Eduard", ""], ["Eltoft", "Torbj\u00f8rn", ""], ["Selvakumaran", "Sivasakthy", ""], ["Girolami", "Mark", ""], ["Jutten", "Christian", ""]]}, {"id": "2105.03684", "submitter": "Leonard Wossnig", "authors": "Leonard Wossnig", "title": "Quantum Machine Learning For Classical Data", "comments": "PhD thesis. arXiv admin note: text overlap with arXiv:1905.09902", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this dissertation, we study the intersection of quantum computing and\nsupervised machine learning algorithms, which means that we investigate quantum\nalgorithms for supervised machine learning that operate on classical data. This\narea of research falls under the umbrella of quantum machine learning, a\nresearch area of computer science which has recently received wide attention.\nIn particular, we investigate to what extent quantum computers can be used to\naccelerate supervised machine learning algorithms. The aim of this is to\ndevelop a clear understanding of the promises and limitations of the current\nstate of the art of quantum algorithms for supervised machine learning, but\nalso to define directions for future research in this exciting field. We start\nby looking at supervised quantum machine learning (QML) algorithms through the\nlens of statistical learning theory. In this framework, we derive novel bounds\non the computational complexities of a large set of supervised QML algorithms\nunder the requirement of optimal learning rates. Next, we give a new bound for\nHamiltonian simulation of dense Hamiltonians, a major subroutine of most known\nsupervised QML algorithms, and then derive a classical algorithm with nearly\nthe same complexity. We then draw the parallels to recent \"quantum-inspired\"\nresults, and will explain the implications of these results for quantum machine\nlearning applications. Looking for areas which might bear larger advantages for\nQML algorithms, we finally propose a novel algorithm for Quantum Boltzmann\nmachines, and argue that quantum algorithms for quantum data are one of the\nmost promising applications for QML with potentially exponential advantage over\nclassical approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 12:11:44 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 07:52:07 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wossnig", "Leonard", ""]]}, {"id": "2105.03688", "submitter": "Ziyao Li", "authors": "Ziyao Li, Shuwen Yang, Guojie Song and Lingsheng Cai", "title": "HamNet: Conformation-Guided Molecular Representation with Hamiltonian\n  Neural Networks", "comments": "in ICLR-2021 (poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.chem-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Well-designed molecular representations (fingerprints) are vital to combine\nmedical chemistry and deep learning. Whereas incorporating 3D geometry of\nmolecules (i.e. conformations) in their representations seems beneficial,\ncurrent 3D algorithms are still in infancy. In this paper, we propose a novel\nmolecular representation algorithm which preserves 3D conformations of\nmolecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit\npositions and momentums of atoms in a molecule interact in the Hamiltonian\nEngine following the discretized Hamiltonian equations. These implicit\ncoordinations are supervised with real conformations with translation- &\nrotation-invariant losses, and further used as inputs to the Fingerprint\nGenerator, a message-passing neural network. Experiments show that the\nHamiltonian Engine can well preserve molecular conformations, and that the\nfingerprints generated by HamNet achieve state-of-the-art performances on\nMoleculeNet, a standard molecular machine learning benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 12:48:08 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Ziyao", ""], ["Yang", "Shuwen", ""], ["Song", "Guojie", ""], ["Cai", "Lingsheng", ""]]}, {"id": "2105.03689", "submitter": "Leo Yu Zhang Dr.", "authors": "Zhaoxi Zhang, Leo Yu Zhang, Xufei Zheng, Shengshan Hu, Jinyu Tian,\n  Jiantao Zhou", "title": "Self-Supervised Adversarial Example Detection by Disentangled\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning models are known to be vulnerable to adversarial examples that\nare elaborately designed for malicious purposes and are imperceptible to the\nhuman perceptual system. Autoencoder, when trained solely over benign examples,\nhas been widely used for (self-supervised) adversarial detection based on the\nassumption that adversarial examples yield larger reconstruction error.\nHowever, because lacking adversarial examples in its training and the too\nstrong generalization ability of autoencoder, this assumption does not always\nhold true in practice. To alleviate this problem, we explore to detect\nadversarial examples by disentangled representations of images under the\nautoencoder structure. By disentangling input images as class features and\nsemantic features, we train an autoencoder, assisted by a discriminator\nnetwork, over both correctly paired class/semantic features and incorrectly\npaired class/semantic features to reconstruct benign and counterexamples. This\nmimics the behavior of adversarial examples and can reduce the unnecessary\ngeneralization ability of autoencoder. Compared with the state-of-the-art\nself-supervised detection methods, our method exhibits better performance in\nvarious measurements (i.e., AUC, FPR, TPR) over different datasets (MNIST,\nFashion-MNIST and CIFAR-10), different adversarial attack methods (FGSM, BIM,\nPGD, DeepFool, and CW) and different victim models (8-layer CNN and 16-layer\nVGG). We compare our method with the state-of-the-art self-supervised detection\nmethods under different adversarial attacks and different victim models (30\nattack settings), and it exhibits better performance in various measurements\n(AUC, FPR, TPR) for most attacks settings. Ideally, AUC is $1$ and our method\nachieves $0.99+$ on CIFAR-10 for all attacks. Notably, different from other\nAutoencoder-based detectors, our method can provide resistance to the adaptive\nadversary.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 12:48:18 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 12:37:42 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 12:07:49 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhang", "Zhaoxi", ""], ["Zhang", "Leo Yu", ""], ["Zheng", "Xufei", ""], ["Hu", "Shengshan", ""], ["Tian", "Jinyu", ""], ["Zhou", "Jiantao", ""]]}, {"id": "2105.03692", "submitter": "Charles Jin", "authors": "Charles Jin, Melinda Sun, Martin Rinard", "title": "Provable Guarantees against Data Poisoning Using Self-Expansion and\n  Compatibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A recent line of work has shown that deep networks are highly susceptible to\nbackdoor data poisoning attacks. Specifically, by injecting a small amount of\nmalicious data into the training distribution, an adversary gains the ability\nto control the model's behavior during inference. In this work, we propose an\niterative training procedure for removing poisoned data from the training set.\nOur approach consists of two steps. We first train an ensemble of weak learners\nto automatically discover distinct subpopulations in the training set. We then\nleverage a boosting framework to recover the clean data. Empirically, our\nmethod successfully defends against several state-of-the-art backdoor attacks,\nincluding both clean and dirty label attacks. We also present results from an\nindependent third-party evaluation including a recent \\textit{adaptive}\npoisoning adversary. The results indicate our approach is competitive with\nexisting defenses against backdoor attacks on deep neural networks, and\nsignificantly outperforms the state-of-the-art in several scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 13:01:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Jin", "Charles", ""], ["Sun", "Melinda", ""], ["Rinard", "Martin", ""]]}, {"id": "2105.03701", "submitter": "Evgeny Krivosheev", "authors": "Evgeny Krivosheev, Mattia Atzeni, Katsiaryna Mirylenka, Paolo Scotton,\n  Christoph Miksovic, Anton Zorin", "title": "Business Entity Matching with Siamese Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data integration has been studied extensively for decades and approached from\ndifferent angles. However, this domain still remains largely rule-driven and\nlacks universal automation. Recent developments in machine learning and in\nparticular deep learning have opened the way to more general and efficient\nsolutions to data-integration tasks. In this paper, we demonstrate an approach\nthat allows modeling and integrating entities by leveraging their relations and\ncontextual information. This is achieved by combining siamese and graph neural\nnetworks to effectively propagate information between connected entities and\nsupport high scalability. We evaluated our approach on the task of integrating\ndata about business entities, demonstrating that it outperforms both\ntraditional rule-based systems and other deep learning approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 13:47:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Krivosheev", "Evgeny", ""], ["Atzeni", "Mattia", ""], ["Mirylenka", "Katsiaryna", ""], ["Scotton", "Paolo", ""], ["Miksovic", "Christoph", ""], ["Zorin", "Anton", ""]]}, {"id": "2105.03703", "submitter": "Greg Yang", "authors": "Greg Yang, Etai Littwin", "title": "Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel\n  Training Dynamics", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yang (2020a) recently showed that the Neural Tangent Kernel (NTK) at\ninitialization has an infinite-width limit for a large class of architectures\nincluding modern staples such as ResNet and Transformers. However, their\nanalysis does not apply to training. Here, we show the same neural networks (in\nthe so-called NTK parametrization) during training follow a kernel gradient\ndescent dynamics in function space, where the kernel is the infinite-width NTK.\nThis completes the proof of the *architectural universality* of NTK behavior.\nTo achieve this result, we apply the Tensor Programs technique: Write the\nentire SGD dynamics inside a Tensor Program and analyze it via the Master\nTheorem. To facilitate this proof, we develop a graphical notation for Tensor\nPrograms.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 14:05:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yang", "Greg", ""], ["Littwin", "Etai", ""]]}, {"id": "2105.03705", "submitter": "Ding Liu", "authors": "Zhanghao Zhouyin, Ding Liu", "title": "Understanding Neural Networks with Logarithm Determinant Entropy\n  Estimator", "comments": "15pages,22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the informative behaviour of deep neural networks is challenged\nby misused estimators and the complexity of network structure, which leads to\ninconsistent observations and diversified interpretation. Here we propose the\nLogDet estimator -- a reliable matrix-based entropy estimator that approximates\nShannon differential entropy. We construct informative measurements based on\nLogDet estimator, verify our method with comparable experiments and utilize it\nto analyse neural network behaviour. Our results demonstrate the LogDet\nestimator overcomes the drawbacks that emerge from highly diverse and\ndegenerated distribution thus is reliable to estimate entropy in neural\nnetworks. The Network analysis results also find a functional distinction\nbetween shallow and deeper layers, which can help understand the compression\nphenomenon in the Information bottleneck theory of neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 14:07:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhouyin", "Zhanghao", ""], ["Liu", "Ding", ""]]}, {"id": "2105.03714", "submitter": "Shubham Gupta", "authors": "Shubham Gupta and Ambedkar Dukkipati", "title": "Protecting Individual Interests across Clusters: Spectral Clustering\n  with Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies related to fairness in machine learning have recently gained traction\ndue to its ever-expanding role in high-stakes decision making. For example, it\nmay be desirable to ensure that all clusters discovered by an algorithm have\nhigh gender diversity. Previously, these problems have been studied under a\nsetting where sensitive attributes, with respect to which fairness conditions\nimpose diversity across clusters, are assumed to be observable; hence,\nprotected groups are readily available. Most often, this may not be true, and\ndiversity or individual interests can manifest as an intrinsic or latent\nfeature of a social network. For example, depending on latent sensitive\nattributes, individuals interact with each other and represent each other's\ninterests, resulting in a network, which we refer to as a representation graph.\nMotivated by this, we propose an individual fairness criterion for clustering a\ngraph $\\mathcal{G}$ that requires each cluster to contain an adequate number of\nmembers connected to the individual under a representation graph $\\mathcal{R}$.\nWe devise a spectral clustering algorithm to find fair clusters under a given\nrepresentation graph. We further propose a variant of the stochastic block\nmodel and establish our algorithm's weak consistency under this model. Finally,\nwe present experimental results to corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 15:03:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gupta", "Shubham", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "2105.03733", "submitter": "Lingwei Peng", "authors": "Lingwei Peng, Hui Qian, Zhebang Shen, Chao Zhang, Fei Li", "title": "Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free deep reinforcement learning has achieved great success in many\ndomains, such as video games, recommendation systems and robotic control tasks.\nIn continuous control tasks, widely used policies with Gaussian distributions\nresults in ineffective exploration of environments and limited performance of\nalgorithms in many cases. In this paper, we propose a density-free off-policy\nalgorithm, Generative Actor-Critic(GAC), using the push-forward model to\nincrease the expressiveness of policies, which also includes an entropy-like\ntechnique, MMD-entropy regularizer, to balance the exploration and\nexploitation. Additionnally, we devise an adaptive mechanism to automatically\nscale this regularizer, which further improves the stability and robustness of\nGAC. The experiment results show that push-forward policies possess desirable\nfeatures, such as multi-modality, which can improve the efficiency of\nexploration and asymptotic performance of algorithms obviously.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 16:29:20 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 15:29:17 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Peng", "Lingwei", ""], ["Qian", "Hui", ""], ["Shen", "Zhebang", ""], ["Zhang", "Chao", ""], ["Li", "Fei", ""]]}, {"id": "2105.03736", "submitter": "Sourjya Roy", "authors": "Sourjya Roy, Mustafa Ali and Anand Raghunathan", "title": "PIM-DRAM: Accelerating Machine Learning Workloads using Processing in\n  Commodity DRAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) have transformed the field of machine learning\nand are widely deployed in many applications involving image, video, speech and\nnatural language processing. The increasing compute demands of DNNs have been\nwidely addressed through Graphics Processing Units (GPUs) and specialized\naccelerators. However, as model sizes grow, these von Neumann architectures\nrequire very high memory bandwidth to keep the processing elements utilized as\na majority of the data resides in the main memory. Processing in memory has\nbeen proposed as a promising solution for the memory wall bottleneck for ML\nworkloads. In this work, we propose a new DRAM-based processing-in-memory (PIM)\nmultiplication primitive coupled with intra-bank accumulation to accelerate\nmatrix vector operations in ML workloads. The proposed multiplication primitive\nadds < 1% area overhead and does not require any change in the DRAM\nperipherals. Therefore, the proposed multiplication can be easily adopted in\ncommodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture,\ndata mapping scheme and dataflow for executing DNNs within DRAM. System\nevaluations performed on networks like AlexNet, VGG16 and ResNet18 show that\nthe proposed architecture, mapping, and data flow can provide up to 23x speedup\nover an NVIDIA Titan Xp GPU. Furthermore, it achieves upto 6.5x speedup over an\nideal von Neumann architecture with infinite computational throughput,\nhighlighting the need to overcome the memory bottleneck in future generations\nof DNN hardware.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 16:39:24 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 19:47:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Roy", "Sourjya", ""], ["Ali", "Mustafa", ""], ["Raghunathan", "Anand", ""]]}, {"id": "2105.03746", "submitter": "Huangjie Zheng", "authors": "Huangjie Zheng, Xu Chen, Jiangchao Yao, Hongxia Yang, Chunyuan Li, Ya\n  Zhang, Hao Zhang, Ivor Tsang, Jingren Zhou, Mingyuan Zhou", "title": "Contrastive Attraction and Contrastive Repulsion for Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning (CL) is effective in learning data representations\nwithout label supervision, where the encoder needs to contrast each positive\nsample over multiple negative samples via a one-vs-many softmax cross-entropy\nloss. However, conventional CL is sensitive to how many negative samples are\nincluded and how they are selected. Proposed in this paper is a doubly CL\nstrategy that contrasts positive samples and negative ones within themselves\nseparately. We realize this strategy with contrastive attraction and\ncontrastive repulsion (CACR) makes the query not only exert a greater force to\nattract more distant positive samples but also do so to repel closer negative\nsamples. Theoretical analysis reveals the connection between CACR and CL from\nthe perspectives of both positive attraction and negative repulsion and shows\nthe benefits in both efficiency and robustness brought by separately\ncontrasting within the sampled positive and negative pairs. Extensive\nlarge-scale experiments on standard vision tasks show that CACR not only\nconsistently outperforms existing CL methods on benchmark datasets in\nrepresentation learning, but also provides interpretable contrastive weights,\ndemonstrating the efficacy of the proposed doubly contrastive strategy.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 17:25:08 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 04:12:54 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zheng", "Huangjie", ""], ["Chen", "Xu", ""], ["Yao", "Jiangchao", ""], ["Yang", "Hongxia", ""], ["Li", "Chunyuan", ""], ["Zhang", "Ya", ""], ["Zhang", "Hao", ""], ["Tsang", "Ivor", ""], ["Zhou", "Jingren", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2105.03752", "submitter": "Bicheng Yan", "authors": "Bicheng Yan, Dylan Robert Harp, Bailian Chen, Rajesh J. Pawar", "title": "Improving Deep Learning Performance for Predicting Large-Scale\n  Porous-Media Flow through Feature Coarsening", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.AI cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Physics-based simulation for fluid flow in porous media is a computational\ntechnology to predict the temporal-spatial evolution of state variables (e.g.\npressure) in porous media, and usually requires high computational expense due\nto its nonlinearity and the scale of the study domain. This letter describes a\ndeep learning (DL) workflow to predict the pressure evolution as fluid flows in\nlarge-scale 3D heterogeneous porous media. In particular, we apply feature\ncoarsening technique to extract the most representative information and perform\nthe training and prediction of DL at the coarse scale, and further recover the\nresolution at the fine scale by 2D piecewise cubic interpolation. We validate\nthe DL approach that is trained from physics-based simulation data to predict\npressure field in a field-scale 3D geologic CO_2 storage reservoir. We evaluate\nthe impact of feature coarsening on DL performance, and observe that the\nfeature coarsening can not only decrease training time by >74% and reduce\nmemory consumption by >75%, but also maintains temporal error <1.5%. Besides,\nthe DL workflow provides predictive efficiency with ~1400 times speedup\ncompared to physics-based simulation.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 17:58:46 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yan", "Bicheng", ""], ["Harp", "Dylan Robert", ""], ["Chen", "Bailian", ""], ["Pawar", "Rajesh J.", ""]]}, {"id": "2105.03756", "submitter": "Eddy Hudson", "authors": "Eddy Hudson and Garrett Warnell and Peter Stone", "title": "RAIL: A modular framework for Reinforcement-learning-based Adversarial\n  Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While Adversarial Imitation Learning (AIL) algorithms have recently led to\nstate-of-the-art results on various imitation learning benchmarks, it is\nunclear as to what impact various design decisions have on performance. To this\nend, we present here an organizing, modular framework called\nReinforcement-learning-based Adversarial Imitation Learning (RAIL) that\nencompasses and generalizes a popular subclass of existing AIL approaches.\nUsing the view espoused by RAIL, we create two new IfO (Imitation from\nObservation) algorithms, which we term SAIfO: SAC-based Adversarial Imitation\nfrom Observation and SILEM (Skeletal Feature Compensation for Imitation\nLearning with Embodiment Mismatch). We go into greater depth about SILEM in a\nseparate technical report. In this paper, we focus on SAIfO, evaluating it on a\nsuite of locomotion tasks from OpenAI Gym, and showing that it outperforms\ncontemporaneous RAIL algorithms that perform IfO.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 18:16:27 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hudson", "Eddy", ""], ["Warnell", "Garrett", ""], ["Stone", "Peter", ""]]}, {"id": "2105.03761", "submitter": "Maxime Kayser", "authors": "Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde,\n  Virginie Do, Zeynep Akata, Thomas Lukasiewicz", "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in\n  Vision-Language Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, an increasing number of works have introduced models capable of\ngenerating natural language explanations (NLEs) for their predictions on\nvision-language (VL) tasks. Such models are appealing because they can provide\nhuman-friendly and comprehensive explanations. However, there is still a lack\nof unified evaluation approaches for the explanations generated by these\nmodels. Moreover, there are currently only few datasets of NLEs for VL tasks.\nIn this work, we introduce e-ViL, a benchmark for explainable vision-language\ntasks that establishes a unified evaluation framework and provides the first\ncomprehensive comparison of existing approaches that generate NLEs for VL\ntasks. e-ViL spans four models and three datasets. Both automatic metrics and\nhuman evaluation are used to assess model-generated explanations. We also\nintroduce e-SNLI-VE, the largest existing VL dataset with NLEs (over 430k\ninstances). Finally, we propose a new model that combines UNITER, which learns\njoint embeddings of images and text, and GPT-2, a pre-trained language model\nthat is well-suited for text generation. It surpasses the previous\nstate-of-the-art by a large margin across all datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 18:46:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kayser", "Maxime", ""], ["Camburu", "Oana-Maria", ""], ["Salewski", "Leonard", ""], ["Emde", "Cornelius", ""], ["Do", "Virginie", ""], ["Akata", "Zeynep", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2105.03774", "submitter": "Rodrigo de Lamare", "authors": "W. S. Leite and R. C. de Lamare", "title": "Study of List-Based OMP and an Enhanced Model for Direction Finding with\n  Non-Uniform Arrays", "comments": "6 figures, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an enhanced coarray transformation model (EDCTM) and a\nmixed greedy maximum likelihood algorithm called List-Based Maximum Likelihood\nOrthogonal Matching Pursuit (LBML-OMP) for direction-of-arrival estimation with\nnon-uniform linear arrays (NLAs). The proposed EDCTM approach obtains improved\nestimates when Khatri-Rao product-based models are used to generate difference\ncoarrays under the assumption of uncorrelated sources. In the proposed LBML-OMP\ntechnique, for each iteration a set of candidates is generated based on the\ncorrelation-maximization between the dictionary and the residue vector.\nLBML-OMP then chooses the best candidate based on a reduced-complexity\nasymptotic maximum likelihood decision rule. Simulations show the improved\nresults of EDCTM over existing approaches and that LBML-OMP outperforms\nexisting sparse recovery algorithms as well as Spatial Smoothing Multiple\nSignal Classification with NLAs.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 20:43:13 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Leite", "W. S.", ""], ["de Lamare", "R. C.", ""]]}, {"id": "2105.03775", "submitter": "Hossein Basafa", "authors": "Hossein Basafa, Sajad Movahedi, Ali Ebrahimi, Azadeh Shakery and\n  Heshaam Faili", "title": "NLP-IIS@UT at SemEval-2021 Task 4: Machine Reading Comprehension using\n  the Long Document Transformer", "comments": "6 pages, 1 figure. Accepted in SemEval2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technical report of our submission to the 4th task of\nSemEval-2021, titled: Reading Comprehension of Abstract Meaning. In this task,\nwe want to predict the correct answer based on a question given a context.\nUsually, contexts are very lengthy and require a large receptive field from the\nmodel. Thus, common contextualized language models like BERT miss fine\nrepresentation and performance due to the limited capacity of the input tokens.\nTo tackle this problem, we used the Longformer model to better process the\nsequences. Furthermore, we utilized the method proposed in the Longformer\nbenchmark on Wikihop dataset which improved the accuracy on our task data from\n23.01% and 22.95% achieved by the baselines for subtask 1 and 2, respectively,\nto 70.30% and 64.38%.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 20:48:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Basafa", "Hossein", ""], ["Movahedi", "Sajad", ""], ["Ebrahimi", "Ali", ""], ["Shakery", "Azadeh", ""], ["Faili", "Heshaam", ""]]}, {"id": "2105.03781", "submitter": "Yingjun Du", "authors": "Yingjun Du, Haoliang Sun, Xiantong Zhen, Jun Xu, Yilong Yin, Ling\n  Shao, Cees G. M. Snoek", "title": "MetaKernel: Learning Variational Random Features with Limited Labels", "comments": "19 pages,7 figures. arXiv admin note: substantial text overlap with\n  arXiv:2006.06707", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning deals with the fundamental and challenging problem of\nlearning from a few annotated samples, while being able to generalize well on\nnew tasks. The crux of few-shot learning is to extract prior knowledge from\nrelated tasks to enable fast adaptation to a new task with a limited amount of\ndata. In this paper, we propose meta-learning kernels with random Fourier\nfeatures for few-shot learning, we call MetaKernel. Specifically, we propose\nlearning variational random features in a data-driven manner to obtain\ntask-specific kernels by leveraging the shared knowledge provided by related\ntasks in a meta-learning setting. We treat the random feature basis as the\nlatent variable, which is estimated by variational inference. The shared\nknowledge from related tasks is incorporated into a context inference of the\nposterior, which we achieve via a long-short term memory module. To establish\nmore expressive kernels, we deploy conditional normalizing flows based on\ncoupling layers to achieve a richer posterior distribution over random Fourier\nbases. The resultant kernels are more informative and discriminative, which\nfurther improves the few-shot learning. To evaluate our method, we conduct\nextensive experiments on both few-shot image classification and regression\ntasks. A thorough ablation study demonstrates that the effectiveness of each\nintroduced component in our method. The benchmark results on fourteen datasets\ndemonstrate MetaKernel consistently delivers at least comparable and often\nbetter performance than state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 21:24:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Du", "Yingjun", ""], ["Sun", "Haoliang", ""], ["Zhen", "Xiantong", ""], ["Xu", "Jun", ""], ["Yin", "Yilong", ""], ["Shao", "Ling", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2105.03788", "submitter": "Guan-Horng Liu", "authors": "Guan-Horng Liu, Tianrong Chen, and Evangelos A. Theodorou", "title": "Dynamic Game Theoretic Neural Optimizer", "comments": "Accepted in International Conference on Machine Learning (ICML) 2021\n  as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connection between training deep neural networks (DNNs) and optimal\ncontrol theory (OCT) has attracted considerable attention as a principled tool\nof algorithmic design. Despite few attempts being made, they have been limited\nto architectures where the layer propagation resembles a Markovian dynamical\nsystem. This casts doubts on their flexibility to modern networks that heavily\nrely on non-Markovian dependencies between layers (e.g. skip connections in\nresidual networks). In this work, we propose a novel dynamic game perspective\nby viewing each layer as a player in a dynamic game characterized by the DNN\nitself. Through this lens, different classes of optimizers can be seen as\nmatching different types of Nash equilibria, depending on the implicit\ninformation structure of each (p)layer. The resulting method, called Dynamic\nGame Theoretic Neural Optimizer (DGNOpt), not only generalizes OCT-inspired\noptimizers to richer network class; it also motivates a new training principle\nby solving a multi-player cooperative game. DGNOpt shows convergence\nimprovements over existing methods on image classification datasets with\nresidual and inception networks. Our work marries strengths from both OCT and\ngame theory, paving ways to new algorithmic opportunities from robust optimal\ncontrol and bandit-based optimization.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 21:56:14 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 15:18:50 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Liu", "Guan-Horng", ""], ["Chen", "Tianrong", ""], ["Theodorou", "Evangelos A.", ""]]}, {"id": "2105.03793", "submitter": "Yunwen Lei", "authors": "Yunwen Lei, Zhenhuan Yang, Tianbao Yang, Yiming Ying", "title": "Stability and Generalization of Stochastic Gradient Methods for Minimax\n  Problems", "comments": "To appear in ICML 2021 as Long Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many machine learning problems can be formulated as minimax problems such as\nGenerative Adversarial Networks (GANs), AUC maximization and robust estimation,\nto mention but a few. A substantial amount of studies are devoted to studying\nthe convergence behavior of their stochastic gradient-type algorithms. In\ncontrast, there is relatively little work on their generalization, i.e., how\nthe learning models built from training examples would behave on test examples.\nIn this paper, we provide a comprehensive generalization analysis of stochastic\ngradient methods for minimax problems under both convex-concave and\nnonconvex-nonconcave cases through the lens of algorithmic stability. We\nestablish a quantitative connection between stability and several\ngeneralization measures both in expectation and with high probability. For the\nconvex-concave setting, our stability analysis shows that stochastic gradient\ndescent ascent attains optimal generalization bounds for both smooth and\nnonsmooth minimax problems. We also establish generalization bounds for both\nweakly-convex-weakly-concave and gradient-dominated problems.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 22:38:00 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 19:32:31 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lei", "Yunwen", ""], ["Yang", "Zhenhuan", ""], ["Yang", "Tianbao", ""], ["Ying", "Yiming", ""]]}, {"id": "2105.03800", "submitter": "Rafael Lima Goncalves de", "authors": "Rafael Lima", "title": "Fine-Grained $\\epsilon$-Margin Closed-Form Stabilization of Parametric\n  Hawkes Processes", "comments": "Presented as a RobustML workshop paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hawkes Processes have undergone increasing popularity as default tools for\nmodeling self- and mutually exciting interactions of discrete events in\ncontinuous-time event streams. A Maximum Likelihood Estimation (MLE)\nunconstrained optimization procedure over parametrically assumed forms of the\ntriggering kernels of the corresponding intensity function are a widespread\ncost-effective modeling strategy, particularly suitable for data with few\nand/or short sequences. However, the MLE optimization lacks guarantees, except\nfor strong assumptions on the parameters of the triggering kernels, and may\nlead to instability of the resulting parameters .In the present work, we show\nhow a simple stabilization procedure improves the performance of the MLE\noptimization without these overly restrictive assumptions.This stabilized\nversion of the MLE is shown to outperform traditional methods over sequences of\nseveral different lengths.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 23:49:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lima", "Rafael", ""]]}, {"id": "2105.03804", "submitter": "Farzaneh Rajabi", "authors": "Austin Park, Farzaneh Rajabi, Ross Weber", "title": "Slash or burn: Power line and vegetation classification for wildfire\n  prevention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electric utilities are struggling to manage increasing wildfire risk in a\nhotter and drier climate. Utility transmission and distribution lines regularly\nignite destructive fires when they make contact with surrounding vegetation.\nTrimming vegetation to maintain the separation from utility assets is as\ncritical to safety as it is difficult. Each utility has tens of thousands of\nlinear miles to manage, poor knowledge of where those assets are located, and\nno way to prioritize trimming. Feature-enhanced convolutional neural networks\n(CNNs) have proven effective in this problem space. Histograms of oriented\ngradients (HOG) and Hough transforms are used to increase the salience of the\nlinear structures like power lines and poles. Data is frequently taken from\ndrone or satellite footage, but Google Street View offers an even more scalable\nand lower cost solution. This paper uses $1,320$ images scraped from Street\nView, transfer learning on popular CNNs, and feature engineering to place\nimages in one of three classes: (1) no utility systems, (2) utility systems\nwith no overgrown vegetation, or (3) utility systems with overgrown vegetation.\nThe CNN output thus yields a prioritized vegetation management system and\ncreates a geotagged map of utility assets as a byproduct. Test set accuracy\nwith reached $80.15\\%$ using VGG11 with a trained first layer and classifier,\nand a model ensemble correctly classified $88.88\\%$ of images with risky\nvegetation overgrowth.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 00:34:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Park", "Austin", ""], ["Rajabi", "Farzaneh", ""], ["Weber", "Ross", ""]]}, {"id": "2105.03811", "submitter": "Farzaneh Rajabi", "authors": "Farzaneh Rajabi, Jack Siyuan He", "title": "Click-Through Rate Prediction Using Graph Neural Networks and Online\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommendation systems have been extensively studied by many literature in\nthe past and are ubiquitous in online advertisement, shopping\nindustry/e-commerce, query suggestions in search engines, and friend\nrecommendation in social networks. Moreover,\nrestaurant/music/product/movie/news/app recommendations are only a few of the\napplications of a recommender system. A small percent improvement on the CTR\nprediction accuracy has been mentioned to add millions of dollars of revenue to\nthe advertisement industry. Click-Through-Rate (CTR) prediction is a special\nversion of recommender system in which the goal is predicting whether or not a\nuser is going to click on a recommended item. A content-based recommendation\napproach takes into account the past history of the user's behavior, i.e. the\nrecommended products and the users reaction to them. So, a personalized model\nthat recommends the right item to the right user at the right time is the key\nto building such a model. On the other hand, the so-called collaborative\nfiltering approach incorporates the click history of the users who are very\nsimilar to a particular user, thereby helping the recommender to come up with a\nmore confident prediction for that particular user by leveraging the wider\nknowledge of users who share their taste in a connected network of users. In\nthis project, we are interested in building a CTR predictor using Graph Neural\nNetworks complemented by an online learning algorithm that models such dynamic\ninteractions. By framing the problem as a binary classification task, we have\nevaluated this system both on the offline models (GNN, Deep Factorization\nMachines) with test-AUC of 0.7417 and on the online learning model with\ntest-AUC of 0.7585 using a sub-sampled version of Criteo public dataset\nconsisting of 10,000 data points.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:35:49 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Rajabi", "Farzaneh", ""], ["He", "Jack Siyuan", ""]]}, {"id": "2105.03818", "submitter": "Jiashuo Liu", "authors": "Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, Zheyan Shen", "title": "Heterogeneous Risk Minimization", "comments": "Proceedings of the 38th International Conference on Machine Learning,\n  PMLR 139, 2021. (ICML2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms with empirical risk minimization usually suffer\nfrom poor generalization performance due to the greedy exploitation of\ncorrelations among the training data, which are not stable under distributional\nshifts. Recently, some invariant learning methods for out-of-distribution (OOD)\ngeneralization have been proposed by leveraging multiple training environments\nto find invariant relationships. However, modern datasets are frequently\nassembled by merging data from multiple sources without explicit source labels.\nThe resultant unobserved heterogeneity renders many invariant learning methods\ninapplicable. In this paper, we propose Heterogeneous Risk Minimization (HRM)\nframework to achieve joint learning of latent heterogeneity among the data and\ninvariant relationship, which leads to stable prediction despite distributional\nshifts. We theoretically characterize the roles of the environment labels in\ninvariant learning and justify our newly proposed HRM framework. Extensive\nexperimental results validate the effectiveness of our HRM framework.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 02:51:36 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 08:02:48 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 07:58:32 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Jiashuo", ""], ["Hu", "Zheyuan", ""], ["Cui", "Peng", ""], ["Li", "Bo", ""], ["Shen", "Zheyan", ""]]}, {"id": "2105.03819", "submitter": "Seba Susan", "authors": "Anmol Jain, Aishwary Kumar, Seba Susan", "title": "Evaluating Deep Neural Network Ensembles by Majority Voting cum\n  Meta-Learning scheme", "comments": "Included in Proceedings of 3rd ICSCSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep Neural Networks (DNNs) are prone to overfitting and hence have high\nvariance. Overfitted networks do not perform well for a new data instance. So\ninstead of using a single DNN as classifier we propose an ensemble of seven\nindependent DNN learners by varying only the input to these DNNs keeping their\narchitecture and intrinsic properties same. To induce variety in the training\ninput, for each of the seven DNNs, one-seventh of the data is deleted and\nreplenished by bootstrap sampling from the remaining samples. We have proposed\na novel technique for combining the prediction of the DNN learners in the\nensemble. Our method is called pre-filtering by majority voting coupled with\nstacked meta-learner which performs a two-step confi-dence check for the\npredictions before assigning the final class labels. All the algorithms in this\npaper have been tested on five benchmark datasets name-ly, Human Activity\nRecognition (HAR), Gas sensor array drift, Isolet, Spam-base and Internet\nadvertisements. Our ensemble approach achieves higher accuracy than a single\nDNN and the average individual accuracies of DNNs in the ensemble, as well as\nthe baseline approaches of plurality voting and meta-learning.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 03:10:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Jain", "Anmol", ""], ["Kumar", "Aishwary", ""], ["Susan", "Seba", ""]]}, {"id": "2105.03821", "submitter": "Yuheng Lu", "authors": "Yuheng Lu, Jinpeng Chen, ChuXiong Sun, Jie Hu", "title": "Graph Inference Representation: Learning Graph Positional Embeddings\n  with Anchor Path Encoding", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning node representations that incorporate information from graph\nstructure benefits wide range of tasks on graph. The majority of existing graph\nneural networks (GNNs) have limited power in capturing position information for\na given node. The idea of positioning nodes with selected anchors has been\nexploited, yet mainly relying on explicit labeling of distance information.\nHere we propose Graph Inference Representation (GIR), an anchor based GNN model\nencoding path information related to pre-selected anchors for each node.\nAbilities to get position-aware embeddings are theoretically and experimentally\ninvestigated on GIR and its core variants. Further, the complementarity between\nGIRs and typical GNNs is demonstrated. We show that GIRs get outperformed\nresults in position-aware scenarios, and performances on typical GNNs could be\nimproved by fusing GIR embeddings.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 03:25:58 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 14:53:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lu", "Yuheng", ""], ["Chen", "Jinpeng", ""], ["Sun", "ChuXiong", ""], ["Hu", "Jie", ""]]}, {"id": "2105.03822", "submitter": "Huming Qiu", "authors": "Huming Qiu, Hua Ma, Zhi Zhang, Yifeng Zheng, Anmin Fu, Pan Zhou,\n  Yansong Gao, Derek Abbott, Said F. Al-Sarawi", "title": "RBNN: Memory-Efficient Reconfigurable Deep Binary Neural Network with IP\n  Protection for Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep neural network models exhibit outstanding performance for various\napplications, their large model size and extensive floating-point operations\nrender deployment on mobile computing platforms a major challenge, and, in\nparticular, on Internet of Things devices. One appealing solution is model\nquantization that reduces the model size and uses integer operations commonly\nsupported by microcontrollers . To this end, a 1-bit quantized DNN model or\ndeep binary neural network maximizes the memory efficiency, where each\nparameter in a BNN model has only 1-bit. In this paper, we propose a\nreconfigurable BNN (RBNN) to further amplify the memory efficiency for\nresource-constrained IoT devices. Generally, the RBNN can be reconfigured on\ndemand to achieve any one of M (M>1) distinct tasks with the same parameter\nset, thus only a single task determines the memory requirements. In other\nwords, the memory utilization is improved by times M. Our extensive experiments\ncorroborate that up to seven commonly used tasks can co-exist (the value of M\ncan be larger). These tasks with a varying number of classes have no or\nnegligible accuracy drop-off on three binarized popular DNN architectures\nincluding VGG, ResNet, and ReActNet. The tasks span across different domains,\ne.g., computer vision and audio domains validated herein, with the prerequisite\nthat the model architecture can serve those cross-domain tasks. To protect the\nintellectual property of an RBNN model, the reconfiguration can be controlled\nby both a user key and a device-unique root key generated by the intrinsic\nhardware fingerprint. By doing so, an RBNN model can only be used per paid user\nper authorized device, thus benefiting both the user and the model provider.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 03:28:14 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 04:51:44 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Qiu", "Huming", ""], ["Ma", "Hua", ""], ["Zhang", "Zhi", ""], ["Zheng", "Yifeng", ""], ["Fu", "Anmin", ""], ["Zhou", "Pan", ""], ["Gao", "Yansong", ""], ["Abbott", "Derek", ""], ["Al-Sarawi", "Said F.", ""]]}, {"id": "2105.03824", "submitter": "James Lee-Thorp", "authors": "James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon", "title": "FNet: Mixing Tokens with Fourier Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Transformer encoder architectures can be massively sped up, with\nlimited accuracy costs, by replacing the self-attention sublayers with simple\nlinear transformations that \"mix\" input tokens. These linear transformations,\nalong with standard nonlinearities in feed-forward layers, prove competent at\nmodeling semantic relationships in several text classification tasks. Most\nsurprisingly, we find that replacing the self-attention sublayer in a\nTransformer encoder with a standard, unparameterized Fourier Transform achieves\n92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains\nnearly seven times faster on GPUs and twice as fast on TPUs. The resulting\nmodel, FNet, also scales very efficiently to long inputs. Specifically, when\ncompared to the \"efficient\" Transformers on the Long Range Arena benchmark,\nFNet matches the accuracy of the most accurate models, but is faster than the\nfastest models across all sequence lengths on GPUs (and across relatively\nshorter lengths on TPUs). Finally, FNet has a light memory footprint and is\nparticularly efficient at smaller model sizes: for a fixed speed and accuracy\nbudget, small FNet models outperform Transformer counterparts.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 03:32:48 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 18:56:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lee-Thorp", "James", ""], ["Ainslie", "Joshua", ""], ["Eckstein", "Ilya", ""], ["Ontanon", "Santiago", ""]]}, {"id": "2105.03834", "submitter": "Hyung-Jin Yoon", "authors": "Hyung-Jin Yoon, Hamidreza Jafarnejadsani, Petros Voulgaris", "title": "Learning Image Attacks toward Vision Guided Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While adversarial neural networks have been shown successful for static image\nattacks, very few approaches have been developed for attacking online image\nstreams while taking into account the underlying physical dynamics of\nautonomous vehicles, their mission, and environment. This paper presents an\nonline adversarial machine learning framework that can effectively misguide\nautonomous vehicles' missions. In the existing image attack methods devised\ntoward autonomous vehicles, optimization steps are repeated for every image\nframe. This framework removes the need for fully converged optimization at\nevery frame to realize image attacks in real-time. Using reinforcement\nlearning, a generative neural network is trained over a set of image frames to\nobtain an attack policy that is more robust to dynamic and uncertain\nenvironments. A state estimator is introduced for processing image streams to\nreduce the attack policy's sensitivity to physical variables such as unknown\nposition and velocity. A simulation study is provided to validate the results.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 04:34:10 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 19:01:33 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yoon", "Hyung-Jin", ""], ["Jafarnejadsani", "Hamidreza", ""], ["Voulgaris", "Petros", ""]]}, {"id": "2105.03835", "submitter": "Ruian Shi", "authors": "Ruian Shi, Quaid Morris", "title": "Segmenting Hybrid Trajectories using Latent ODEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smooth dynamics interrupted by discontinuities are known as hybrid systems\nand arise commonly in nature. Latent ODEs allow for powerful representation of\nirregularly sampled time series but are not designed to capture trajectories\narising from hybrid systems. Here, we propose the Latent Segmented ODE\n(LatSegODE), which uses Latent ODEs to perform reconstruction and changepoint\ndetection within hybrid trajectories featuring jump discontinuities and\nswitching dynamical modes. Where it is possible to train a Latent ODE on the\nsmooth dynamical flows between discontinuities, we apply the pruned exact\nlinear time (PELT) algorithm to detect changepoints where latent dynamics\nrestart, thereby maximizing the joint probability of a piece-wise continuous\nlatent dynamical representation. We propose usage of the marginal likelihood as\na score function for PELT, circumventing the need for model complexity-based\npenalization. The LatSegODE outperforms baselines in reconstructive and\nsegmentation tasks including synthetic data sets of sine waves, Lotka Volterra\ndynamics, and UCI Character Trajectories.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 04:51:13 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 11:36:26 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Shi", "Ruian", ""], ["Morris", "Quaid", ""]]}, {"id": "2105.03838", "submitter": "Shahar Lutati", "authors": "Shahar Lutati, Lior Wolf", "title": "HyperHyperNetworks for the Design of Antenna Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present deep learning methods for the design of arrays and single\ninstances of small antennas. Each design instance is conditioned on a target\nradiation pattern and is required to conform to specific spatial dimensions and\nto include, as part of its metallic structure, a set of predetermined\nlocations. The solution, in the case of a single antenna, is based on a\ncomposite neural network that combines a simulation network, a hypernetwork,\nand a refinement network. In the design of the antenna array, we add an\nadditional design level and employ a hypernetwork within a hypernetwork. The\nlearning objective is based on measuring the similarity of the obtained\nradiation pattern to the desired one. Our experiments demonstrate that our\napproach is able to design novel antennas and antenna arrays that are compliant\nwith the design requirements, considerably better than the baseline methods. We\ncompare the solutions obtained by our method to existing designs and\ndemonstrate a high level of overlap. When designing the antenna array of a\ncellular phone, the obtained solution displays improved properties over the\nexisting one.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 05:21:28 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lutati", "Shahar", ""], ["Wolf", "Lior", ""]]}, {"id": "2105.03841", "submitter": "Matthew Middlehurst", "authors": "Matthew Middlehurst, James Large, Gavin Cawley, Anthony Bagnall", "title": "The Temporal Dictionary Ensemble (TDE) Classifier for Time Series\n  Classification", "comments": "arXiv admin note: text overlap with arXiv:1911.12008", "journal-ref": "ECML PKDD 2020: Machine Learning and Knowledge Discovery in\n  Databases, pages 660-676, 2020", "doi": "10.1007/978-3-030-67658-2_38", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using bag of words representations of time series is a popular approach to\ntime series classification. These algorithms involve approximating and\ndiscretising windows over a series to form words, then forming a count of words\nover a given dictionary. Classifiers are constructed on the resulting\nhistograms of word counts. A 2017 evaluation of a range of time series\nclassifiers found the bag of symbolic-fourier approximation symbols (BOSS)\nensemble the best of the dictionary based classifiers. It forms one of the\ncomponents of hierarchical vote collective of transformation-based ensembles\n(HIVE-COTE), which represents the current state of the art. Since then, several\nnew dictionary based algorithms have been proposed that are more accurate or\nmore scalable (or both) than BOSS. We propose a further extension of these\ndictionary based classifiers that combines the best elements of the others\ncombined with a novel approach to constructing ensemble members based on an\nadaptive Gaussian process model of the parameter space. We demonstrate that the\ntemporal dictionary ensemble (TDE) is more accurate than other dictionary based\napproaches. Furthermore, unlike the other classifiers, if we replace BOSS in\nHIVE-COTE with TDE, HIVE-COTE is significantly more accurate. We also show this\nnew version of HIVE-COTE is significantly more accurate than the current best\ndeep learning approach, a recently proposed hybrid tree ensemble and a recently\nintroduced competitive classifier making use of highly randomised convolutional\nkernels. This advance represents a new state of the art for time series\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 05:27:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Middlehurst", "Matthew", ""], ["Large", "James", ""], ["Cawley", "Gavin", ""], ["Bagnall", "Anthony", ""]]}, {"id": "2105.03842", "submitter": "Yichong Leng", "authors": "Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu,\n  Tao Qin, Xiang-Yang Li, Ed Lin, Tie-Yan Liu", "title": "FastCorrect: Fast Error Correction with Edit Alignment for Automatic\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the popular NAR models adopted in neural machine translation and\ntext edition by a large margin.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 05:35:36 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 13:20:13 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 02:11:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Leng", "Yichong", ""], ["Tan", "Xu", ""], ["Zhu", "Linchen", ""], ["Xu", "Jin", ""], ["Luo", "Renqian", ""], ["Liu", "Linquan", ""], ["Qin", "Tao", ""], ["Li", "Xiang-Yang", ""], ["Lin", "Ed", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2105.03844", "submitter": "Sihang Chen", "authors": "Sihang Chen, Weiqi Luo and Chao Yu", "title": "Reinforcement Learning with Expert Trajectory For Quantitative Trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, quantitative investment methods combined with artificial\nintelligence have attracted more and more attention from investors and\nresearchers. Existing related methods based on the supervised learning are not\nvery suitable for learning problems with long-term goals and delayed rewards in\nreal futures trading. In this paper, therefore, we model the price prediction\nproblem as a Markov decision process (MDP), and optimize it by reinforcement\nlearning with expert trajectory. In the proposed method, we employ more than\n100 short-term alpha factors instead of price, volume and several technical\nfactors in used existing methods to describe the states of MDP. Furthermore,\nunlike DQN (deep Q-learning) and BC (behavior cloning) in related methods, we\nintroduce expert experience in training stage, and consider both the\nexpert-environment interaction and the agent-environment interaction to design\nthe temporal difference error so that the agents are more adaptable for\ninevitable noise in financial data. Experimental results evaluated on share\nprice index futures in China, including IF (CSI 300) and IC (CSI 500), show\nthat the advantages of the proposed method compared with three typical\ntechnical analysis and two deep leaning based methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 05:49:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Sihang", ""], ["Luo", "Weiqi", ""], ["Yu", "Chao", ""]]}, {"id": "2105.03852", "submitter": "Amin Edrisi Mr", "authors": "Mohammad Amin Edrisi", "title": "Towards Dynamic Feature Selection with Attention to Assist Banking\n  Customers in Establishing a New Business", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing a new business may involve Knowledge acquisition in various\nareas, from personal to business and marketing sources. This task is\nchallenging as it requires examining various data islands to uncover hidden\npatterns and unknown correlations such as purchasing behavior, consumer buying\nsignals, and demographic and socioeconomic attributes of different locations.\nThis paper introduces a novel framework for extracting and identifying\nimportant features from banking and non-banking data sources to address this\nchallenge. We present an attention-based supervised feature selection approach\nto select important and relevant features which contribute most to the\ncustomer's query regarding establishing a new business. We report on the\nexperiment conducted on an openly available dataset created from Kaggle and the\nUCI machine learning repositories.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 06:48:12 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Edrisi", "Mohammad Amin", ""]]}, {"id": "2105.03854", "submitter": "Chin Chun Ooi", "authors": "Quang Tuyen Le, Chin Chun Ooi", "title": "Surrogate Modeling of Fluid Dynamics with a Multigrid Inspired Neural\n  Network Architecture", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Algebraic or geometric multigrid methods are commonly used in numerical\nsolvers as they are a multi-resolution method able to handle problems with\nmultiple scales. In this work, we propose a modification to the commonly-used\nU-Net neural network architecture that is inspired by the principles of\nmultigrid methods, referred to here as U-Net-MG. We then demonstrate that this\nproposed U-Net-MG architecture can successfully reduce the test prediction\nerrors relative to the conventional U-Net architecture when modeling a set of\nfluid dynamic problems. In total, we demonstrate an improvement in the\nprediction of velocity and pressure fields for the canonical fluid dynamics\ncases of flow past a stationary cylinder, flow past 2 cylinders in out-of-phase\nmotion, and flow past an oscillating airfoil in both the propulsion and energy\nharvesting modes. In general, while both the U-Net and U-Net-MG models can\nmodel the systems well with test RMSEs of less than 1%, the use of the U-Net-MG\narchitecture can further reduce RMSEs by between 20% and 70%.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 07:04:30 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Le", "Quang Tuyen", ""], ["Ooi", "Chin Chun", ""]]}, {"id": "2105.03855", "submitter": "Seung Jee Yang", "authors": "Seung Jee Yang, Kyung Joon Cha", "title": "GMOTE: Gaussian based minority oversampling technique for imbalanced\n  classification adapting tail probability of outliers", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification of imbalanced data is one of the common problems in the recent\nfield of data mining. Imbalanced data substantially affects the performance of\nstandard classification models. Data-level approaches mainly use the\noversampling methods to solve the problem, such as synthetic minority\noversampling Technique (SMOTE). However, since the methods such as SMOTE\ngenerate instances by linear interpolation, synthetic data space may look like\na polygonal. Also, the oversampling methods generate outliers of the minority\nclass. In this paper, we proposed Gaussian based minority oversampling\ntechnique (GMOTE) with a statistical perspective for imbalanced datasets. To\navoid linear interpolation and to consider outliers, this proposed method\ngenerates instances by the Gaussian Mixture Model. Motivated by\nclustering-based multivariate Gaussian outlier score (CMGOS), we propose to\nadapt tail probability of instances through the Mahalanobis distance to\nconsider local outliers. The experiment was carried out on a representative set\nof benchmark datasets. The performance of the GMOTE is compared with other\nmethods such as SMOTE. When the GMOTE is combined with classification and\nregression tree (CART) or support vector machine (SVM), it shows better\naccuracy and F1-Score. Experimental results demonstrate the robust performance.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 07:04:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yang", "Seung Jee", ""], ["Cha", "Kyung Joon", ""]]}, {"id": "2105.03863", "submitter": "Wenhao Yang", "authors": "Wenhao Yang, Zhihua Zhang", "title": "Non-asymptotic Performances of Robust Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the non-asymptotic performance of optimal policy on\nrobust value function with true transition dynamics. The optimal robust policy\nis solved from a generative model or offline dataset without access to true\ntransition dynamics. In particular, we consider three different uncertainty\nsets including the $L_1$, $\\chi^2$ and KL balls in both $(s,a)$-rectangular and\n$s$-rectangular assumptions. Our results show that when we assume\n$(s,a)$-rectangular on uncertainty sets, the sample complexity is about\n$\\widetilde{O}\\left(\\frac{|\\mathcal{S}|^2|\\mathcal{A}|}{\\varepsilon^2\\rho^2(1-\\gamma)^4}\\right)$\nin the generative model setting and\n$\\widetilde{O}\\left(\\frac{|\\mathcal{S}|}{\\nu_{\\min}\\varepsilon^2\\rho^2(1-\\gamma)^4}\\right)$\nin the offline dataset setting. While prior works on non-asymptotic\nperformances are restricted with the KL ball and $(s,a)$-rectangular\nassumption, we also extend our results to a more general $s$-rectangular\nassumption, which leads to a larger sample complexity than the\n$(s,a)$-rectangular assumption.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 07:40:45 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yang", "Wenhao", ""], ["Zhang", "Zhihua", ""]]}, {"id": "2105.03868", "submitter": "Hao Chen", "authors": "Hao Chen, Zengde Deng, Yue Xu, Zhoujun Li", "title": "Non-Recursive Graph Convolutional Networks", "comments": "5 pages, 2 figures. Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) are powerful models for node\nrepresentation learning tasks. However, the node representation in existing GCN\nmodels is usually generated by performing recursive neighborhood aggregation\nacross multiple graph convolutional layers with certain sampling methods, which\nmay lead to redundant feature mixing, needless information loss, and extensive\ncomputations. Therefore, in this paper, we propose a novel architecture named\nNon-Recursive Graph Convolutional Network (NRGCN) to improve both the training\nefficiency and the learning performance of GCNs in the context of node\nclassification. Specifically, NRGCN proposes to represent different hops of\nneighbors for each node based on inner-layer aggregation and layer-independent\nsampling. In this way, each node can be directly represented by concatenating\nthe information extracted independently from each hop of its neighbors thereby\navoiding the recursive neighborhood expansion across layers. Moreover, the\nlayer-independent sampling and aggregation can be precomputed before the model\ntraining, thus the training process can be accelerated considerably. Extensive\nexperiments on benchmark datasets verify that our NRGCN outperforms the\nstate-of-the-art GCN models, in terms of the node classification performance\nand reliability.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:12:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Hao", ""], ["Deng", "Zengde", ""], ["Xu", "Yue", ""], ["Li", "Zhoujun", ""]]}, {"id": "2105.03875", "submitter": "Ganesh Del Grosso", "authors": "Ganesh Del Grosso, Georg Pichler, Catuscia Palamidessi, Pablo\n  Piantanida", "title": "Bounding Information Leakage in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine Learning services are being deployed in a large range of applications\nthat make it easy for an adversary, using the algorithm and/or the model, to\ngain access to sensitive data. This paper investigates fundamental bounds on\ninformation leakage. First, we identify and bound the success rate of the\nworst-case membership inference attack, connecting it to the generalization\nerror of the target model. Second, we study the question of how much sensitive\ninformation is stored by the algorithm about the training set and we derive\nbounds on the mutual information between the sensitive attributes and model\nparameters. Although our contributions are mostly of theoretical nature, the\nbounds and involved concepts are of practical relevance. Inspired by our\ntheoretical analysis, we study linear regression and DNN models to illustrate\nhow these bounds can be used to assess the privacy guarantees of ML models.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:49:14 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Del Grosso", "Ganesh", ""], ["Pichler", "Georg", ""], ["Palamidessi", "Catuscia", ""], ["Piantanida", "Pablo", ""]]}, {"id": "2105.03876", "submitter": "Saeed Bakhshi Germi", "authors": "Saeed Bakhshi Germi and Esa Rahtu and Heikki Huttunen", "title": "Selective Probabilistic Classifier Based on Hypothesis Testing", "comments": "Accepted in EUVIP 2021 conference. Comments: Copyright statement\n  added to first page in new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a simple yet effective method to deal with the\nviolation of the Closed-World Assumption for a classifier. Previous works tend\nto apply a threshold either on the classification scores or the loss function\nto reject the inputs that violate the assumption. However, these methods cannot\nachieve the low False Positive Ratio (FPR) required in safety applications. The\nproposed method is a rejection option based on hypothesis testing with\nprobabilistic networks. With probabilistic networks, it is possible to estimate\nthe distribution of outcomes instead of a single output. By utilizing Z-test\nover the mean and standard deviation for each class, the proposed method can\nestimate the statistical significance of the network certainty and reject\nuncertain outputs. The proposed method was experimented on with different\nconfigurations of the COCO and CIFAR datasets. The performance of the proposed\nmethod is compared with the Softmax Response, which is a known top-performing\nmethod. It is shown that the proposed method can achieve a broader range of\noperation and cover a lower FPR than the alternative.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:55:56 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 20:41:58 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Germi", "Saeed Bakhshi", ""], ["Rahtu", "Esa", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2105.03879", "submitter": "Dachao Lin", "authors": "Dachao Lin, Zhihua Zhang", "title": "Directional Convergence Analysis under Spherically Symmetric\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of learning linear predictors (i.e.,\nseparable datasets with zero margin) using neural networks with gradient flow\nor gradient descent. Under the assumption of spherically symmetric data\ndistribution, we show directional convergence guarantees with exact convergence\nrate for two-layer non-linear networks with only two hidden nodes, and (deep)\nlinear networks. Moreover, our discovery is built on dynamic from the\ninitialization without both initial loss and perfect classification constraint\nin contrast to previous works. We also point out and study the challenges in\nfurther strengthening and generalizing our results.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:59:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lin", "Dachao", ""], ["Zhang", "Zhihua", ""]]}, {"id": "2105.03902", "submitter": "Chence Shi", "authors": "Chence Shi, Shitong Luo, Minkai Xu, Jian Tang", "title": "Learning Gradient Fields for Molecular Conformation Generation", "comments": "ICML 2021, Long talk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a fundamental problem in computational chemistry known as molecular\nconformation generation, trying to predict stable 3D structures from 2D\nmolecular graphs. Existing machine learning approaches usually first predict\ndistances between atoms and then generate a 3D structure satisfying the\ndistances, where noise in predicted distances may induce extra errors during 3D\ncoordinate generation. Inspired by the traditional force field methods for\nmolecular dynamics simulation, in this paper, we propose a novel approach\ncalled ConfGF by directly estimating the gradient fields of the log density of\natomic coordinates. The estimated gradient fields allow directly generating\nstable conformations via Langevin dynamics. However, the problem is very\nchallenging as the gradient fields are roto-translation equivariant. We notice\nthat estimating the gradient fields of atomic coordinates can be translated to\nestimating the gradient fields of interatomic distances, and hence develop a\nnovel algorithm based on recent score-based generative models to effectively\nestimate these gradients. Experimental results across multiple tasks show that\nConfGF outperforms previous state-of-the-art baselines by a significant margin.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 10:30:35 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 09:34:31 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 02:30:22 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shi", "Chence", ""], ["Luo", "Shitong", ""], ["Xu", "Minkai", ""], ["Tang", "Jian", ""]]}, {"id": "2105.03905", "submitter": "Ferhat Ozgur Catak", "authors": "Ferhat Ozgur Catak, Evren Catak, Murat Kuzlu, Umit Cali, Devrim Unal", "title": "Security Concerns on Machine Learning Solutions for 6G Networks in\n  mmWave Beam Prediction", "comments": "16 Pages, under review. arXiv admin note: substantial text overlap\n  with arXiv:2103.07268", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6G -- sixth generation -- is the latest cellular technology currently under\ndevelopment for wireless communication systems. In recent years, machine\nlearning algorithms have been applied widely in various fields, such as\nhealthcare, transportation, energy, autonomous car, and many more. Those\nalgorithms have been also using in communication technologies to improve the\nsystem performance in terms of frequency spectrum usage, latency, and security.\nWith the rapid developments of machine learning techniques, especially deep\nlearning, it is critical to take the security concern into account when\napplying the algorithms. While machine learning algorithms offer significant\nadvantages for 6G networks, security concerns on Artificial Intelligent (AI)\nmodels is typically ignored by the scientific community so far. However,\nsecurity is also a vital part of the AI algorithms, this is because the AI\nmodel itself can be poisoned by attackers. This paper proposes a mitigation\nmethod for adversarial attacks against proposed 6G machine learning models for\nthe millimeter-wave (mmWave) beam prediction using adversarial learning. The\nmain idea behind adversarial attacks against machine learning models is to\nproduce faulty results by manipulating trained deep learning models for 6G\napplications for mmWave beam prediction. We also present the adversarial\nlearning mitigation method's performance for 6G security in mmWave beam\nprediction application with fast gradient sign method attack. The mean square\nerrors (MSE) of the defended model under attack are very close to the\nundefended model without attack.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 10:38:53 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 11:17:01 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 10:51:12 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Catak", "Ferhat Ozgur", ""], ["Catak", "Evren", ""], ["Kuzlu", "Murat", ""], ["Cali", "Umit", ""], ["Unal", "Devrim", ""]]}, {"id": "2105.03918", "submitter": "Avik Pal", "authors": "Avik Pal, Yingbo Ma, Viral Shah, Christopher Rackauckas", "title": "Opening the Blackbox: Accelerating Neural Differential Equations by\n  Regularizing Internal Solver Heuristics", "comments": "Proceedings of the 38 th International Conference on Machine\n  Learning, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Democratization of machine learning requires architectures that automatically\nadapt to new problems. Neural Differential Equations (NDEs) have emerged as a\npopular modeling framework by removing the need for ML practitioners to choose\nthe number of layers in a recurrent model. While we can control the\ncomputational cost by choosing the number of layers in standard architectures,\nin NDEs the number of neural network evaluations for a forward pass can depend\non the number of steps of the adaptive ODE solver. But, can we force the NDE to\nlearn the version with the least steps while not increasing the training cost?\nCurrent strategies to overcome slow prediction require high order automatic\ndifferentiation, leading to significantly higher training time. We describe a\nnovel regularization method that uses the internal cost heuristics of adaptive\ndifferential equation solvers combined with discrete adjoint sensitivities to\nguide the training process towards learning NDEs that are easier to solve. This\napproach opens up the blackbox numerical analysis behind the differential\nequation solver's algorithm and directly uses its local error estimates and\nstiffness heuristics as cheap and accurate cost estimates. We incorporate our\nmethod without any change in the underlying NDE framework and show that our\nmethod extends beyond Ordinary Differential Equations to accommodate Neural\nStochastic Differential Equations. We demonstrate how our approach can halve\nthe prediction time and, unlike other methods which can increase the training\ntime by an order of magnitude, we demonstrate similar reduction in training\ntimes. Together this showcases how the knowledge embedded within\nstate-of-the-art equation solvers can be used to enhance machine learning.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 12:03:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Pal", "Avik", ""], ["Ma", "Yingbo", ""], ["Shah", "Viral", ""], ["Rackauckas", "Christopher", ""]]}, {"id": "2105.03923", "submitter": "Shihong Deng", "authors": "Changnan Xiao, Haosen Shi, Jiajun Fan, Shihong Deng", "title": "CASA: A Bridge Between Gradient of Policy Improvement and Policy\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel design of model-free reinforcement learning,\nCASA, Critic AS an Actor. CASA follows the actor-critic framework that\nestimates state-value, state-action-value and policy simultaneously. We prove\nthat CASA integrates a consistent path for the policy evaluation and the policy\nimprovement, which completely eliminates the gradient conflict between the\npolicy improvement and the policy evaluation. The policy evaluation is\nequivalent to a compensational policy improvement, which alleviates the\nfunction approximation error, and is also equivalent to an entropy-regularized\npolicy improvement, which prevents the policy from being trapped into a\nsuboptimal solution. Building on this design, an expectation-correct Doubly\nRobust Trace is introduced to learn state-value and state-action-value, and the\nconvergence is guaranteed. Our experiments show that the design achieves\nState-Of-The-Art on Arcade Learning Environment.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 12:45:13 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 12:50:54 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 16:28:16 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Xiao", "Changnan", ""], ["Shi", "Haosen", ""], ["Fan", "Jiajun", ""], ["Deng", "Shihong", ""]]}, {"id": "2105.03928", "submitter": "Noam Wies", "authors": "Noam Wies, Yoav Levine, Daniel Jannai, Amnon Shashua", "title": "Which transformer architecture fits my data? A vocabulary bottleneck in\n  self-attention", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  After their successful debut in natural language processing, Transformer\narchitectures are now becoming the de-facto standard in many domains. An\nobstacle for their deployment over new modalities is the architectural\nconfiguration: the optimal depth-to-width ratio has been shown to dramatically\nvary across data types (e.g., $10$x larger over images than over language). We\ntheoretically predict the existence of an embedding rank bottleneck that limits\nthe contribution of self-attention width to the Transformer expressivity. We\nthus directly tie the input vocabulary size and rank to the optimal\ndepth-to-width ratio, since a small vocabulary size or rank dictates an added\nadvantage of depth over width. We empirically demonstrate the existence of this\nbottleneck and its implications on the depth-to-width interplay of Transformer\narchitectures, linking the architecture variability across domains to the often\nglossed-over usage of different vocabulary sizes or embedding ranks in\ndifferent domains. As an additional benefit, our rank bottlenecking framework\nallows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models\nsuch as ALBERT and T5.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:08:26 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 17:18:03 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wies", "Noam", ""], ["Levine", "Yoav", ""], ["Jannai", "Daniel", ""], ["Shashua", "Amnon", ""]]}, {"id": "2105.03931", "submitter": "Qi-An Fu", "authors": "Qi-An Fu, Yinpeng Dong, Hang Su, Jun Zhu", "title": "Automated Decision-based Adversarial Attacks", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are vulnerable to adversarial examples, which can fool a\ntarget classifier by imposing imperceptible perturbations onto natural\nexamples. In this work, we consider the practical and challenging\ndecision-based black-box adversarial setting, where the attacker can only\nacquire the final classification labels by querying the target model without\naccess to the model's details. Under this setting, existing works often rely on\nheuristics and exhibit unsatisfactory performance. To better understand the\nrationality of these heuristics and the limitations of existing methods, we\npropose to automatically discover decision-based adversarial attack algorithms.\nIn our approach, we construct a search space using basic mathematical\noperations as building blocks and develop a random search algorithm to\nefficiently explore this space by incorporating several pruning techniques and\nintuitive priors inspired by program synthesis works. Although we use a small\nand fast model to efficiently evaluate attack algorithms during the search,\nextensive experiments demonstrate that the discovered algorithms are simple yet\nquery-efficient when transferred to larger normal and defensive models on the\nCIFAR-10 and ImageNet datasets. They achieve comparable or better performance\nthan the state-of-the-art decision-based attack methods consistently.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:15:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Fu", "Qi-An", ""], ["Dong", "Yinpeng", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2105.03934", "submitter": "Md Shoaib Ahmed", "authors": "Md Shoaib Ahmed, Tanjim Taharat Aurpa, Md. Abul Kalam Azad", "title": "Fish Disease Detection Using Image Based Machine Learning Technique in\n  Aquaculture", "comments": "15 pages, 10 figures, 7 tables. Accepted Manuscript. Journal of King\n  Saud University - Computer and Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fish diseases in aquaculture constitute a significant hazard to nutriment\nsecurity. Identification of infected fishes in aquaculture remains challenging\nto find out at the early stage due to the dearth of necessary infrastructure.\nThe identification of infected fish timely is an obligatory step to thwart from\nspreading disease. In this work, we want to find out the salmon fish disease in\naquaculture, as salmon aquaculture is the fastest-growing food production\nsystem globally, accounting for 70 percent (2.5 million tons) of the market. In\nthe alliance of flawless image processing and machine learning mechanism, we\nidentify the infected fishes caused by the various pathogen. This work divides\ninto two portions. In the rudimentary portion, image pre-processing and\nsegmentation have been applied to reduce noise and exaggerate the image,\nrespectively. In the second portion, we extract the involved features to\nclassify the diseases with the help of the Support Vector Machine (SVM)\nalgorithm of machine learning with a kernel function. The processed images of\nthe first portion have passed through this (SVM) model. Then we harmonize a\ncomprehensive experiment with the proposed combination of techniques on the\nsalmon fish image dataset used to examine the fish disease. We have conveyed\nthis work on a novel dataset compromising with and without image augmentation.\nThe results have bought a judgment of our applied SVM performs notably with\n91.42 and 94.12 percent of accuracy, respectively, with and without\naugmentation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:22:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ahmed", "Md Shoaib", ""], ["Aurpa", "Tanjim Taharat", ""], ["Azad", "Md. Abul Kalam", ""]]}, {"id": "2105.03939", "submitter": "Li Shen", "authors": "Han Huang, Li Shen, Chaoyang He, Weisheng Dong, Haozhi Huang,\n  Guangming Shi", "title": "Lightweight Image Super-Resolution with Hierarchical and Differentiable\n  Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single Image Super-Resolution (SISR) tasks have achieved significant\nperformance with deep neural networks. However, the large number of parameters\nin CNN-based methods for SISR tasks require heavy computations. Although\nseveral efficient SISR models have been recently proposed, most are handcrafted\nand thus lack flexibility. In this work, we propose a novel differentiable\nNeural Architecture Search (NAS) approach on both the cell-level and\nnetwork-level to search for lightweight SISR models. Specifically, the\ncell-level search space is designed based on an information distillation\nmechanism, focusing on the combinations of lightweight operations and aiming to\nbuild a more lightweight and accurate SR structure. The network-level search\nspace is designed to consider the feature connections among the cells and aims\nto find which information flow benefits the cell most to boost the performance.\nUnlike the existing Reinforcement Learning (RL) or Evolutionary Algorithm (EA)\nbased NAS methods for SISR tasks, our search pipeline is fully differentiable,\nand the lightweight SISR models can be efficiently searched on both the\ncell-level and network-level jointly on a single GPU. Experiments show that our\nmethods can achieve state-of-the-art performance on the benchmark datasets in\nterms of PSNR, SSIM, and model complexity with merely 68G Multi-Adds for\n$\\times 2$ and 18G Multi-Adds for $\\times 4$ SR tasks. Code will be available\nat \\url{https://github.com/DawnHH/DLSR-PyTorch}.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:30:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Huang", "Han", ""], ["Shen", "Li", ""], ["He", "Chaoyang", ""], ["Dong", "Weisheng", ""], ["Huang", "Haozhi", ""], ["Shi", "Guangming", ""]]}, {"id": "2105.03941", "submitter": "Lorenzo Minto", "authors": "Lorenzo Minto, Moritz Haller, Hamed Haddadi, Benjamin Livshits", "title": "Stronger Privacy for Federated Collaborative Filtering with Implicit\n  Feedback", "comments": "Accepted for publication at RecSys 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are commonly trained on centrally collected user\ninteraction data like views or clicks. This practice however raises serious\nprivacy concerns regarding the recommender's collection and handling of\npotentially sensitive data. Several privacy-aware recommender systems have been\nproposed in recent literature, but comparatively little attention has been\ngiven to systems at the intersection of implicit feedback and privacy. To\naddress this shortcoming, we propose a practical federated recommender system\nfor implicit data under user-level local differential privacy (LDP). The\nprivacy-utility trade-off is controlled by parameters $\\epsilon$ and $k$,\nregulating the per-update privacy budget and the number of $\\epsilon$-LDP\ngradient updates sent by each user respectively. To further protect the user's\nprivacy, we introduce a proxy network to reduce the fingerprinting surface by\nanonymizing and shuffling the reports before forwarding them to the\nrecommender. We empirically demonstrate the effectiveness of our framework on\nthe MovieLens dataset, achieving up to Hit Ratio with K=10 (HR@10) 0.68 on 50k\nusers with 5k items. Even on the full dataset, we show that it is possible to\nachieve reasonable utility with HR@10>0.5 without compromising user privacy.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:41:45 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 10:05:37 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 09:39:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Minto", "Lorenzo", ""], ["Haller", "Moritz", ""], ["Haddadi", "Hamed", ""], ["Livshits", "Benjamin", ""]]}, {"id": "2105.03958", "submitter": "Fani Deligianni Dr", "authors": "Matthew Malek-Podjaski, Fani Deligianni", "title": "Preserving Privacy in Human-Motion Affect Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human motion is a biomarker used extensively in clinical analysis to monitor\nthe progression of neurological diseases and mood disorders. Since perceptions\nof emotions are also interleaved with body posture and movements, emotion\nrecognition from human gait can be used to quantitatively monitor mood changes\nthat are often related to neurological disorders. Many existing solutions often\nuse shallow machine learning models with raw positional data or manually\nextracted features to achieve this. However, gait is composed of many highly\nexpressive characteristics that can be used to identify human subjects, and\nmost solutions fail to address this, disregarding the subject's privacy. This\nwork evaluates the effectiveness of existing methods at recognising emotions\nusing both 3D temporal joint signals and manually extracted features. We also\nshow that this data can easily be exploited to expose a subject's identity.\nTherefore to this end, we propose a cross-subject transfer learning technique\nfor training a multi-encoder autoencoder deep neural network to learn\ndisentangled latent representations of human motion features. By disentangling\nsubject biometrics from the gait data, we show that the subjects privacy is\npreserved while the affect recognition performance outperforms traditional\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 15:26:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Malek-Podjaski", "Matthew", ""], ["Deligianni", "Fani", ""]]}, {"id": "2105.03962", "submitter": "Arun Verma", "authors": "Arun Verma, Manjesh K. Hanawal", "title": "Stochastic Multi-Armed Bandits with Control Variates", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies a new variant of the stochastic multi-armed bandits\nproblem, where the learner has access to auxiliary information about the arms.\nThe auxiliary information is correlated with the arm rewards, which we treat as\ncontrol variates. In many applications, the arm rewards are a function of some\nexogenous values, whose mean value is known a priori from historical data and\nhence can be used as control variates. We use the control variates to obtain\nmean estimates with smaller variance and tighter confidence bounds. We then\ndevelop an algorithm named UCB-CV that uses improved estimates. We characterize\nthe regret bounds in terms of the correlation between the rewards and control\nvariates. The experiments on synthetic data validate the performance guarantees\nof our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 15:40:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Verma", "Arun", ""], ["Hanawal", "Manjesh K.", ""]]}, {"id": "2105.03966", "submitter": "Huiru Xiao", "authors": "Huiru Xiao, Caigao Jiang, Yangqiu Song, James Zhang, Junwu Xiong", "title": "Unit Ball Model for Embedding Hierarchical Structures in the Complex\n  Hyperbolic Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning the representation of data with hierarchical structures in the\nhyperbolic space attracts increasing attention in recent years. Due to the\nconstant negative curvature, the hyperbolic space resembles tree metrics and\ncaptures the tree-like properties naturally, which enables the hyperbolic\nembeddings to improve over traditional Euclidean models. However, many\nreal-world hierarchically structured data such as taxonomies and multitree\nnetworks have varying local structures and they are not trees, thus they do not\nubiquitously match the constant curvature property of the hyperbolic space. To\naddress this limitation of hyperbolic embeddings, we explore the complex\nhyperbolic space, which has the variable negative curvature, for representation\nlearning. Specifically, we propose to learn the embeddings of hierarchically\nstructured data in the unit ball model of the complex hyperbolic space. The\nunit ball model based embeddings have a more powerful representation capacity\nto capture a variety of hierarchical structures. Through experiments on\nsynthetic and real-world data, we show that our approach improves over the\nhyperbolic embedding models significantly.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 16:09:54 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 11:27:18 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xiao", "Huiru", ""], ["Jiang", "Caigao", ""], ["Song", "Yangqiu", ""], ["Zhang", "James", ""], ["Xiong", "Junwu", ""]]}, {"id": "2105.03995", "submitter": "Md. Kamrul Hasan", "authors": "Chayan Mondal, Md. Kamrul Hasan, Md. Tasnim Jawad, Aishwariya Dutta,\n  Md.Rabiul Islam, Md. Abdul Awal, Mohiuddin Ahmad", "title": "Acute Lymphoblastic Leukemia Detection from Microscopic Images Using\n  Weighted Ensemble of Convolutional Neural Networks", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acute Lymphoblastic Leukemia (ALL) is a blood cell cancer characterized by\nnumerous immature lymphocytes. Even though automation in ALL prognosis is an\nessential aspect of cancer diagnosis, it is challenging due to the\nmorphological correlation between malignant and normal cells. The traditional\nALL classification strategy demands experienced pathologists to carefully read\nthe cell images, which is arduous, time-consuming, and often suffers\ninter-observer variations. This article has automated the ALL detection task\nfrom microscopic cell images, employing deep Convolutional Neural Networks\n(CNNs). We explore the weighted ensemble of different deep CNNs to recommend a\nbetter ALL cell classifier. The weights for the ensemble candidate models are\nestimated from their corresponding metrics, such as accuracy, F1-score, AUC,\nand kappa values. Various data augmentations and pre-processing are\nincorporated for achieving a better generalization of the network. We utilize\nthe publicly available C-NMC-2019 ALL dataset to conduct all the comprehensive\nexperiments. Our proposed weighted ensemble model, using the kappa values of\nthe ensemble candidates as their weights, has outputted a weighted F1-score of\n88.6 %, a balanced accuracy of 86.2 %, and an AUC of 0.941 in the preliminary\ntest set. The qualitative results displaying the gradient class activation maps\nconfirm that the introduced model has a concentrated learned region. In\ncontrast, the ensemble candidate models, such as Xception, VGG-16,\nDenseNet-121, MobileNet, and InceptionResNet-V2, separately produce coarse and\nscatter learned areas for most example cases. Since the proposed kappa\nvalue-based weighted ensemble yields a better result for the aimed task in this\narticle, it can experiment in other domains of medical diagnostic applications.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 18:58:48 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Mondal", "Chayan", ""], ["Hasan", "Md. Kamrul", ""], ["Jawad", "Md. Tasnim", ""], ["Dutta", "Aishwariya", ""], ["Islam", "Md. Rabiul", ""], ["Awal", "Md. Abdul", ""], ["Ahmad", "Mohiuddin", ""]]}, {"id": "2105.04001", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli and Cassio de Campos", "title": "Bayesian Kernelised Test of (In)dependence with Mixed-type Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task in AI is to assess (in)dependence between mixed-type\nvariables (text, image, sound). We propose a Bayesian kernelised correlation\ntest of (in)dependence using a Dirichlet process model. The new measure of\n(in)dependence allows us to answer some fundamental questions: Based on data,\nare (mixed-type) variables independent? How likely is dependence/independence\nto hold? How high is the probability that two mixed-type variables are more\nthan just weakly dependent? We theoretically show the properties of the\napproach, as well as algorithms for fast computation with it. We empirically\ndemonstrate the effectiveness of the proposed method by analysing its\nperformance and by comparing it with other frequentist and Bayesian approaches\non a range of datasets and tasks with mixed-type variables.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 19:21:43 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Benavoli", "Alessio", ""], ["de Campos", "Cassio", ""]]}, {"id": "2105.04003", "submitter": "Abhiroop Bhattacharjee", "authors": "Abhiroop Bhattacharjee, Abhishek Moitra and Priyadarshini Panda", "title": "Efficiency-driven Hardware Optimization for Adversarially Robust Neural\n  Networks", "comments": "6 pages, 8 figures, 3 tables; Accepted in DATE 2021 conference. arXiv\n  admin note: text overlap with arXiv:2008.11298", "journal-ref": "2021 Design, Automation and Test in Europe (DATE) Conference", "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a growing need to enable intelligence in embedded devices in the\nInternet of Things (IoT) era, secure hardware implementation of Deep Neural\nNetworks (DNNs) has become imperative. We will focus on how to address\nadversarial robustness for DNNs through efficiency-driven hardware\noptimizations. Since memory (specifically, dot-product operations) is a key\nenergy-spending component for DNNs, hardware approaches in the past have\nfocused on optimizing the memory. One such approach is approximate digital CMOS\nmemories with hybrid 6T-8T SRAM cells that enable supply voltage (Vdd) scaling\nyielding low-power operation, without significantly affecting the performance\ndue to read/write failures incurred in the 6T cells. In this paper, we show how\nthe bit-errors in the 6T cells of hybrid 6T-8T memories minimize the\nadversarial perturbations in a DNN. Essentially, we find that for different\nconfigurations of 8T-6T ratios and scaledVdd operation, noise incurred in the\nhybrid memory architectures is bound within specific limits. This hardware\nnoise can potentially interfere in the creation of adversarial attacks in DNNs\nyielding robustness. Another memory optimization approach involves using analog\nmemristive crossbars that perform Matrix-Vector-Multiplications (MVMs)\nefficiently with low energy and area requirements. However, crossbars generally\nsuffer from intrinsic non-idealities that cause errors in performing MVMs,\nleading to degradation in the accuracy of the DNNs. We will show how the\nintrinsic hardware variations manifested through crossbar non-idealities yield\nadversarial robustness to the mapped DNNs without any additional optimization.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 19:26:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bhattacharjee", "Abhiroop", ""], ["Moitra", "Abhishek", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2105.04005", "submitter": "Juncheng Wang", "authors": "Juncheng Wang, Ben Liang, Min Dong, Gary Boudreau, and Hatem Abou-zeid", "title": "Delay-Tolerant Constrained OCO with Application to Network Resource\n  Allocation", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online convex optimization (OCO) with multi-slot feedback delay,\nwhere an agent makes a sequence of online decisions to minimize the\naccumulation of time-varying convex loss functions, subject to short-term and\nlong-term constraints that are possibly time-varying. The current convex loss\nfunction and the long-term constraint function are revealed to the agent only\nafter the decision is made, and they may be delayed for multiple time slots.\nExisting work on OCO under this general setting has focused on the static\nregret, which measures the gap of losses between the online decision sequence\nand an offline benchmark that is fixed over time. In this work, we consider\nboth the static regret and the more practically meaningful dynamic regret,\nwhere the benchmark is a time-varying sequence of per-slot optimizers. We\npropose an efficient algorithm, termed Delay-Tolerant Constrained-OCO\n(DTC-OCO), which uses a novel constraint penalty with double regularization to\ntackle the asynchrony between information feedback and decision updates. We\nderive upper bounds on its dynamic regret, static regret, and constraint\nviolation, proving them to be sublinear under mild conditions. We further apply\nDTC-OCO to a general network resource allocation problem, which arises in many\nsystems such as data networks and cloud computing. Simulation results\ndemonstrate substantial performance gain of DTC-OCO over the known best\nalternative.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 19:32:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Juncheng", ""], ["Liang", "Ben", ""], ["Dong", "Min", ""], ["Boudreau", "Gary", ""], ["Abou-zeid", "Hatem", ""]]}, {"id": "2105.04009", "submitter": "Micha{\\l} Koziarski", "authors": "Micha{\\l} Koziarski, Colin Bellinger, Micha{\\l} Wo\\'zniak", "title": "RB-CCR: Radial-Based Combined Cleaning and Resampling algorithm for\n  imbalanced data classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world classification domains, such as medicine, health and safety, and\nfinance, often exhibit imbalanced class priors and have asynchronous\nmisclassification costs. In such cases, the classification model must achieve a\nhigh recall without significantly impacting precision. Resampling the training\ndata is the standard approach to improving classification performance on\nimbalanced binary data. However, the state-of-the-art methods ignore the local\njoint distribution of the data or correct it as a post-processing step. This\ncan causes sub-optimal shifts in the training distribution, particularly when\nthe target data distribution is complex. In this paper, we propose Radial-Based\nCombined Cleaning and Resampling (RB-CCR). RB-CCR utilizes the concept of class\npotential to refine the energy-based resampling approach of CCR. In particular,\nRB-CCR exploits the class potential to accurately locate sub-regions of the\ndata-space for synthetic oversampling. The category sub-region for oversampling\ncan be specified as an input parameter to meet domain-specific needs or be\nautomatically selected via cross-validation. Our $5\\times2$ cross-validated\nresults on 57 benchmark binary datasets with 9 classifiers show that RB-CCR\nachieves a better precision-recall trade-off than CCR and generally\nout-performs the state-of-the-art resampling methods in terms of AUC and\nG-mean.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 19:47:45 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Koziarski", "Micha\u0142", ""], ["Bellinger", "Colin", ""], ["Wo\u017aniak", "Micha\u0142", ""]]}, {"id": "2105.04014", "submitter": "Micha{\\l} Koziarski", "authors": "Micha{\\l} Koziarski, Bogus{\\l}aw Cyganek, Bogus{\\l}aw Olborski,\n  Zbigniew Antosz, Marcin \\.Zydak, Bogdan Kwolek, Pawe{\\l} W\\k{a}sowicz,\n  Andrzej Buka{\\l}a, Jakub Swad\\'zba, Piotr Sitkowski", "title": "DiagSet: a dataset for prostate cancer histopathological image\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer diseases constitute one of the most significant societal challenges.\nIn this paper we introduce a novel histopathological dataset for prostate\ncancer detection. The proposed dataset, consisting of over 2.6 million tissue\npatches extracted from 430 fully annotated scans, 4675 scans with assigned\nbinary diagnosis, and 46 scans with diagnosis given independently by a group of\nhistopathologists, can be found at https://ai-econsilio.diag.pl. Furthermore,\nwe propose a machine learning framework for detection of cancerous tissue\nregions and prediction of scan-level diagnosis, utilizing thresholding and\nstatistical analysis to abstain from the decision in uncertain cases. During\nthe experimental evaluation we identify several factors negatively affecting\nthe performance of considered models, such as presence of label noise, data\nimbalance, and quantity of data, that can serve as a basis for further\nresearch. The proposed approach, composed of ensembles of deep neural networks\noperating on the histopathological scans at different scales, achieves 94.6%\naccuracy in patch-level recognition, and is compared in a scan-level diagnosis\nwith 9 human histopathologists.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 20:06:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Koziarski", "Micha\u0142", ""], ["Cyganek", "Bogus\u0142aw", ""], ["Olborski", "Bogus\u0142aw", ""], ["Antosz", "Zbigniew", ""], ["\u017bydak", "Marcin", ""], ["Kwolek", "Bogdan", ""], ["W\u0105sowicz", "Pawe\u0142", ""], ["Buka\u0142a", "Andrzej", ""], ["Swad\u017aba", "Jakub", ""], ["Sitkowski", "Piotr", ""]]}, {"id": "2105.04019", "submitter": "Felix Petersen", "authors": "Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen", "title": "Differentiable Sorting Networks for Scalable Sorting and Ranking\n  Supervision", "comments": "Published at ICML 2021, Code @\n  https://github.com/Felix-Petersen/diffsort, Video @\n  https://www.youtube.com/watch?v=38dvqdYEs1o", "journal-ref": "PMLR 139:8546-8555, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting and ranking supervision is a method for training neural networks\nend-to-end based on ordering constraints. That is, the ground truth order of\nsets of samples is known, while their absolute values remain unsupervised. For\nthat, we propose differentiable sorting networks by relaxing their pairwise\nconditional swap operations. To address the problems of vanishing gradients and\nextensive blurring that arise with larger numbers of layers, we propose mapping\nactivations to regions with moderate gradients. We consider odd-even as well as\nbitonic sorting networks, which outperform existing relaxations of the sorting\noperation. We show that bitonic sorting networks can achieve stable training on\nlarge input sets of up to 1024 elements.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 20:39:03 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 10:16:01 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Petersen", "Felix", ""], ["Borgelt", "Christian", ""], ["Kuehne", "Hilde", ""], ["Deussen", "Oliver", ""]]}, {"id": "2105.04021", "submitter": "Bhaskar Mitra", "authors": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos and Jimmy\n  Lin", "title": "MS MARCO: Benchmarking Ranking Models in the Large-Data Regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation efforts such as TREC, CLEF, NTCIR and FIRE, alongside public\nleaderboard such as MS MARCO, are intended to encourage research and track our\nprogress, addressing big questions in our field. However, the goal is not\nsimply to identify which run is \"best\", achieving the top score. The goal is to\nmove the field forward by developing new robust techniques, that work in many\ndifferent settings, and are adopted in research and practice. This paper uses\nthe MS MARCO and TREC Deep Learning Track as our case study, comparing it to\nthe case of TREC ad hoc ranking in the 1990s. We show how the design of the\nevaluation effort can encourage or discourage certain outcomes, and raising\nquestions about internal and external validity of results. We provide some\nanalysis of certain pitfalls, and a statement of best practices for avoiding\nsuch pitfalls. We summarize the progress of the effort so far, and describe our\ndesired end state of \"robust usefulness\", along with steps that might be\nrequired to get us there.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 20:57:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Craswell", "Nick", ""], ["Mitra", "Bhaskar", ""], ["Yilmaz", "Emine", ""], ["Campos", "Daniel", ""], ["Lin", "Jimmy", ""]]}, {"id": "2105.04026", "submitter": "Julius Berner", "authors": "Julius Berner, Philipp Grohs, Gitta Kutyniok, Philipp Petersen", "title": "The Modern Mathematics of Deep Learning", "comments": "This review paper will appear as a book chapter in the book \"Theory\n  of Deep Learning\" by Cambridge University Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the new field of mathematical analysis of deep learning. This\nfield emerged around a list of research questions that were not answered within\nthe classical framework of learning theory. These questions concern: the\noutstanding generalization power of overparametrized neural networks, the role\nof depth in deep architectures, the apparent absence of the curse of\ndimensionality, the surprisingly successful optimization performance despite\nthe non-convexity of the problem, understanding what features are learned, why\ndeep architectures perform exceptionally well in physical problems, and which\nfine aspects of an architecture affect the behavior of a learning task in which\nway. We present an overview of modern approaches that yield partial answers to\nthese questions. For selected approaches, we describe the main ideas in more\ndetail.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 21:30:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Berner", "Julius", ""], ["Grohs", "Philipp", ""], ["Kutyniok", "Gitta", ""], ["Petersen", "Philipp", ""]]}, {"id": "2105.04030", "submitter": "Xiantong Zhen", "authors": "Zehao Xiao, Jiayi Shen, Xiantong Zhen, Ling Shao, Cees G. M. Snoek", "title": "A Bit More Bayesian: Domain-Invariant Learning with Uncertainty", "comments": "accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain generalization is challenging due to the domain shift and the\nuncertainty caused by the inaccessibility of target domain data. In this paper,\nwe address both challenges with a probabilistic framework based on variational\nBayesian inference, by incorporating uncertainty into neural network weights.\nWe couple domain invariance in a probabilistic formula with the variational\nBayesian inference. This enables us to explore domain-invariant learning in a\nprincipled way. Specifically, we derive domain-invariant representations and\nclassifiers, which are jointly established in a two-layer Bayesian neural\nnetwork. We empirically demonstrate the effectiveness of our proposal on four\nwidely used cross-domain visual recognition benchmarks. Ablation studies\nvalidate the synergistic benefits of our Bayesian treatment when jointly\nlearning domain-invariant representations and classifiers for domain\ngeneralization. Further, our method consistently delivers state-of-the-art mean\naccuracy on all benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 21:33:27 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 07:15:35 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 23:45:59 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xiao", "Zehao", ""], ["Shen", "Jiayi", ""], ["Zhen", "Xiantong", ""], ["Shao", "Ling", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2105.04037", "submitter": "Liheng Ma", "authors": "Liheng Ma, Reihaneh Rabbany, Adriana Romero-Soriano", "title": "Graph Attention Networks with Positional Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are deep learning methods which provide the\ncurrent state of the art performance in node classification tasks. GNNs often\nassume homophily -- neighboring nodes having similar features and labels--, and\ntherefore may not be at their full potential when dealing with non-homophilic\ngraphs. In this work, we focus on addressing this limitation and enable Graph\nAttention Networks (GAT), a commonly used variant of GNNs, to explore the\nstructural information within each graph locality. Inspired by the positional\nencoding in the Transformers, we propose a framework, termed Graph Attentional\nNetworks with Positional Embeddings (GAT-POS), to enhance GATs with positional\nembeddings which capture structural and positional information of the nodes in\nthe graph. In this framework, the positional embeddings are learned by a model\npredictive of the graph context, plugged into an enhanced GAT architecture,\nwhich is able to leverage both the positional and content information of each\nnode. The model is trained jointly to optimize for the task of node\nclassification as well as the task of predicting graph context. Experimental\nresults show that GAT-POS reaches remarkable improvement compared to strong GNN\nbaselines and recent structural embedding enhanced GNNs on non-homophilic\ngraphs.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 22:13:46 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 03:38:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ma", "Liheng", ""], ["Rabbany", "Reihaneh", ""], ["Romero-Soriano", "Adriana", ""]]}, {"id": "2105.04046", "submitter": "Minwoo Chae", "authors": "Minwoo Chae, Dongha Kim, Yongdai Kim, Lizhen Lin", "title": "A likelihood approach to nonparametric estimation of a singular\n  distribution using deep generative models", "comments": "33 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate statistical properties of a likelihood approach to\nnonparametric estimation of a singular distribution using deep generative\nmodels. More specifically, a deep generative model is used to model\nhigh-dimensional data that are assumed to concentrate around some\nlow-dimensional structure. Estimating the distribution supported on this\nlow-dimensional structure such as a low-dimensional manifold is challenging due\nto its singularity with respect to the Lebesgue measure in the ambient space.\nIn the considered model, a usual likelihood approach can fail to estimate the\ntarget distribution consistently due to the singularity. We prove that a novel\nand effective solution exists by perturbing the data with an instance noise\nwhich leads to consistent estimation of the underlying distribution with\ndesirable convergence rates. We also characterize the class of distributions\nthat can be efficiently estimated via deep generative models. This class is\nsufficiently general to contain various structured distributions such as\nproduct distributions, classically smooth distributions and distributions\nsupported on a low-dimensional manifold. Our analysis provides some insights on\nhow deep generative models can avoid the curse of dimensionality for\nnonparametric distribution estimation. We conduct thorough simulation study and\nreal data analysis to empirically demonstrate that the proposed data\nperturbation technique improves the estimation performance significantly.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 23:13:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chae", "Minwoo", ""], ["Kim", "Dongha", ""], ["Kim", "Yongdai", ""], ["Lin", "Lizhen", ""]]}, {"id": "2105.04051", "submitter": "Changjian Shui", "authors": "Changjian Shui, Zijian Li, Jiaqi Li, Christian Gagn\\'e, Charles Ling,\n  Boyu Wang", "title": "Aggregating From Multiple Target-Shifted Sources", "comments": null, "journal-ref": "ICML2021", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-source domain adaptation aims at leveraging the knowledge from multiple\ntasks for predicting a related target domain. Hence, a crucial aspect is to\nproperly combine different sources based on their relations. In this paper, we\nanalyzed the problem for aggregating source domains with different label\ndistributions, where most recent source selection approaches fail. Our proposed\nalgorithm differs from previous approaches in two key ways: the model\naggregates multiple sources mainly through the similarity of semantic\nconditional distribution rather than marginal distribution; the model proposes\na \\emph{unified} framework to select relevant sources for three popular\nscenarios, i.e., domain adaptation with limited label on target domain,\nunsupervised domain adaptation and label partial unsupervised domain adaption.\nWe evaluate the proposed method through extensive experiments. The empirical\nresults significantly outperform the baselines.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 23:25:29 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 14:57:01 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Shui", "Changjian", ""], ["Li", "Zijian", ""], ["Li", "Jiaqi", ""], ["Gagn\u00e9", "Christian", ""], ["Ling", "Charles", ""], ["Wang", "Boyu", ""]]}, {"id": "2105.04070", "submitter": "Shuo Wang", "authors": "Shuo Wang, Lingjuan Lyu, Surya Nepal, Carsten Rudolph, Marthie\n  Grobler, Kristen Moore", "title": "Robust Training Using Natural Transformation", "comments": "arXiv admin note: text overlap with arXiv:1912.03192,\n  arXiv:2004.02546 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous robustness approaches for deep learning models such as data\naugmentation techniques via data transformation or adversarial training cannot\ncapture real-world variations that preserve the semantics of the input, such as\na change in lighting conditions. To bridge this gap, we present NaTra, an\nadversarial training scheme that is designed to improve the robustness of image\nclassification algorithms. We target attributes of the input images that are\nindependent of the class identification, and manipulate those attributes to\nmimic real-world natural transformations (NaTra) of the inputs, which are then\nused to augment the training dataset of the image classifier. Specifically, we\napply \\textit{Batch Inverse Encoding and Shifting} to map a batch of given\nimages to corresponding disentangled latent codes of well-trained generative\nmodels. \\textit{Latent Codes Expansion} is used to boost image reconstruction\nquality through the incorporation of extended feature maps.\n\\textit{Unsupervised Attribute Directing and Manipulation} enables\nidentification of the latent directions that correspond to specific attribute\nchanges, and then produce interpretable manipulations of those attributes,\nthereby generating natural transformations to the input data. We demonstrate\nthe efficacy of our scheme by utilizing the disentangled latent representations\nderived from well-trained GANs to mimic transformations of an image that are\nsimilar to real-world natural variations (such as lighting conditions or\nhairstyle), and train models to be invariant to these natural transformations.\nExtensive experiments show that our method improves generalization of\nclassification models and increases its robustness to various real-world\ndistortions\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 01:56:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Shuo", ""], ["Lyu", "Lingjuan", ""], ["Nepal", "Surya", ""], ["Rudolph", "Carsten", ""], ["Grobler", "Marthie", ""], ["Moore", "Kristen", ""]]}, {"id": "2105.04072", "submitter": "Tiago Silva", "authors": "Tiago Tiburcio da Silva and Rodrigo Francisquini and Mari\\'a C. V.\n  Nascimento", "title": "Meteorological and human mobility data on predicting COVID-19 cases by a\n  novel hybrid decomposition method with anomaly detection analysis: a case\n  study in the capitals of Brazil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In 2020, Brazil was the leading country in COVID-19 cases in Latin America,\nand capital cities were the most severely affected by the outbreak. Climates\nvary in Brazil due to the territorial extension of the country, its relief,\ngeography, and other factors. Since the most common COVID-19 symptoms are\nrelated to the respiratory system, many researchers have studied the\ncorrelation between the number of COVID-19 cases with meteorological variables\nlike temperature, humidity, rainfall, etc. Also, due to its high transmission\nrate, some researchers have analyzed the impact of human mobility on the\ndynamics of COVID-19 transmission. There is a dearth of literature that\nconsiders these two variables when predicting the spread of COVID-19 cases. In\nthis paper, we analyzed the correlation between the number of COVID-19 cases\nand human mobility, and meteorological data in Brazilian capitals. We found\nthat the correlation between such variables depends on the regions where the\ncities are located. We employed the variables with a significant correlation\nwith COVID-19 cases to predict the number of COVID-19 infections in all\nBrazilian capitals and proposed a prediction method combining the Ensemble\nEmpirical Mode Decomposition (EEMD) method with the Autoregressive Integrated\nMoving Average Exogenous inputs (ARIMAX) method, which we called EEMD-ARIMAX.\nAfter analyzing the results poor predictions were further investigated using a\nsignal processing-based anomaly detection method. Computational tests showed\nthat EEMD-ARIMAX achieved a forecast 26.73% better than ARIMAX. Moreover, an\nimprovement of 30.69% in the average root mean squared error (RMSE) was noticed\nwhen applying the EEMD-ARIMAX method to the data normalized after the anomaly\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 02:06:51 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["da Silva", "Tiago Tiburcio", ""], ["Francisquini", "Rodrigo", ""], ["Nascimento", "Mari\u00e1 C. V.", ""]]}, {"id": "2105.04077", "submitter": "Muhammad Sohaib", "authors": "Muhammad Sohaib, Jongjin Jeong, and Sang-Woon Jeon", "title": "Dynamic Multichannel Access via Multi-agent Reinforcement Learning:\n  Throughput and Fairness Guarantees", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multichannel random access system in which each user accesses a\nsingle channel at each time slot to communicate with an access point (AP).\nUsers arrive to the system at random and be activated for a certain period of\ntime slots and then disappear from the system. Under such dynamic network\nenvironment, we propose a distributed multichannel access protocol based on\nmulti-agent reinforcement learning (RL) to improve both throughput and fairness\nbetween active users. Unlike the previous approaches adjusting channel access\nprobabilities at each time slot, the proposed RL algorithm deterministically\nselects a set of channel access policies for several consecutive time slots. To\neffectively reduce the complexity of the proposed RL algorithm, we adopt a\nbranching dueling Q-network architecture and propose an efficient training\nmethodology for producing proper Q-values over time-varying user sets. We\nperform extensive simulations on realistic traffic environments and demonstrate\nthat the proposed online learning improves both throughput and fairness\ncompared to the conventional RL approaches and centralized scheduling policies.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 02:32:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sohaib", "Muhammad", ""], ["Jeong", "Jongjin", ""], ["Jeon", "Sang-Woon", ""]]}, {"id": "2105.04079", "submitter": "Koichi Saito", "authors": "Koichi Saito, Tomohiko Nakamura, Kohei Yatabe, Yuma Koizumi, Hiroshi\n  Saruwatari", "title": "Sampling-Frequency-Independent Audio Source Separation Using Convolution\n  Layer Based on Impulse Invariant Method", "comments": "5 pages, 3 figures, accepted for European Signal Processing\n  Conference 2021 (EUSIPCO 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Audio source separation is often used as preprocessing of various\napplications, and one of its ultimate goals is to construct a single versatile\nmodel capable of dealing with the varieties of audio signals. Since sampling\nfrequency, one of the audio signal varieties, is usually application specific,\nthe preceding audio source separation model should be able to deal with audio\nsignals of all sampling frequencies specified in the target applications.\nHowever, conventional models based on deep neural networks (DNNs) are trained\nonly at the sampling frequency specified by the training data, and there are no\nguarantees that they work with unseen sampling frequencies. In this paper, we\npropose a convolution layer capable of handling arbitrary sampling frequencies\nby a single DNN. Through music source separation experiments, we show that the\nintroduction of the proposed layer enables a conventional audio source\nseparation model to consistently work with even unseen sampling frequencies.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 02:33:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Saito", "Koichi", ""], ["Nakamura", "Tomohiko", ""], ["Yatabe", "Kohei", ""], ["Koizumi", "Yuma", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "2105.04084", "submitter": "Xiao-Feng Gong", "authors": "Lu-Ming Wang, Ya-Nan Wang, Xiao-Feng Gong, Qiu-Hua Lin, Fei Xiang", "title": "A Coupled Random Projection Approach to Large-Scale Canonical Polyadic\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel algorithm for the computation of canonical polyadic\ndecomposition (CPD) of large-scale tensors. The proposed algorithm generalizes\nthe random projection (RAP) technique, which is often used to compute\nlarge-scale decompositions, from one single projection to multiple but coupled\nrandom projections (CoRAP). The proposed CoRAP technique yields a set of\ntensors that together admits a coupled CPD (C-CPD) and a C-CPD algorithm is\nthen used to jointly decompose these tensors. The results of C-CPD are finally\nfused to obtain factor matrices of the original large-scale data tensor. As\nmore data samples are jointly exploited via C-CPD, the proposed CoRAP based CPD\nis more accurate than RAP based CPD. Experiments are provided to illustrate the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:00:50 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Lu-Ming", ""], ["Wang", "Ya-Nan", ""], ["Gong", "Xiao-Feng", ""], ["Lin", "Qiu-Hua", ""], ["Xiang", "Fei", ""]]}, {"id": "2105.04087", "submitter": "Pengcheng Ren", "authors": "Pengcheng Ren and Tongjiang Yan", "title": "Latency Analysis of Consortium Blockchained Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A decentralized federated learning architecture is proposed to apply to the\nBusinesses-to-Businesses scenarios by introducing the consortium blockchain in\nthis paper. We introduce a model verification mechanism to ensure the quality\nof local models trained by participators. To analyze the latency of the system,\na latency model is constructed by considering the work flow of the\narchitecture. Finally the experiment results show that our latency model does\nwell in quantifying the actual delays.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:14:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ren", "Pengcheng", ""], ["Yan", "Tongjiang", ""]]}, {"id": "2105.04090", "submitter": "Shih-Lun Wu", "authors": "Shih-Lun Wu, Yi-Hsuan Yang", "title": "MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with One\n  Transformer VAE", "comments": "Preprint. 15 pages, 6 figures, and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformers and variational autoencoders (VAE) have been extensively\nemployed for symbolic (e.g., MIDI) domain music generation. While the former\nboast an impressive capability in modeling long sequences, the latter allow\nusers to willingly exert control over different parts (e.g., bars) of the music\nto be generated. In this paper, we are interested in bringing the two together\nto construct a single model that exhibits both strengths. The task is split\ninto two steps. First, we equip Transformer decoders with the ability to accept\nsegment-level, time-varying conditions during sequence generation.\nSubsequently, we combine the developed and tested in-attention decoder with a\nTransformer encoder, and train the resulting MuseMorphose model with the VAE\nobjective to achieve style transfer of long musical pieces, in which users can\nspecify musical attributes including rhythmic intensity and polyphony (i.e.,\nharmonic fullness) they desire, down to the bar level. Experiments show that\nMuseMorphose outperforms recurrent neural network (RNN) based baselines on\nnumerous widely-used metrics for style transfer tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:44:03 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 03:35:13 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wu", "Shih-Lun", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "2105.04093", "submitter": "Abhishek Aich", "authors": "Abhishek Aich", "title": "Elastic Weight Consolidation (EWC): Nuts and Bolts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we present a theoretical support of the continual learning\nmethod \\textbf{Elastic Weight Consolidation}, introduced in paper titled\n`Overcoming catastrophic forgetting in neural networks'. Being one of the most\ncited paper in regularized methods for continual learning, this report\ndisentangles the underlying concept of the proposed objective function. We\nassume that the reader is aware of the basic terminologies of continual\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:48:55 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Aich", "Abhishek", ""]]}, {"id": "2105.04100", "submitter": "Yuzhou Chen", "authors": "Yuzhou Chen, Ignacio Segovia-Dominguez, Yulia R. Gel", "title": "Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series\n  Forecasting", "comments": "Accepted at the International Conference on Machine Learning (ICML)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There recently has been a surge of interest in developing a new class of deep\nlearning (DL) architectures that integrate an explicit time dimension as a\nfundamental building block of learning and representation mechanisms. In turn,\nmany recent results show that topological descriptors of the observed data,\nencoding information on the shape of the dataset in a topological space at\ndifferent scales, that is, persistent homology of the data, may contain\nimportant complementary information, improving both performance and robustness\nof DL. As convergence of these two emerging ideas, we propose to enhance DL\narchitectures with the most salient time-conditioned topological information of\nthe data and introduce the concept of zigzag persistence into time-aware graph\nconvolutional networks (GCNs). Zigzag persistence provides a systematic and\nmathematically rigorous framework to track the most important topological\nfeatures of the observed data that tend to manifest themselves over time. To\nintegrate the extracted time-conditioned topological descriptors into DL, we\ndevelop a new topological summary, zigzag persistence image, and derive its\ntheoretical stability guarantees. We validate the new GCNs with a time-aware\nzigzag topological layer (Z-GCNETs), in application to traffic forecasting and\nEthereum blockchain price prediction. Our results indicate that Z-GCNET\noutperforms 13 state-of-the-art methods on 4 time series datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 04:01:04 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Yuzhou", ""], ["Segovia-Dominguez", "Ignacio", ""], ["Gel", "Yulia R.", ""]]}, {"id": "2105.04103", "submitter": "Mohammad Alawadhi", "authors": "Mohammad Alawadhi and Wei Yan", "title": "BIM Hyperreality: Data Synthesis Using BIM and Hyperrealistic Rendering\n  for Deep Learning", "comments": "Accepted to the 40th Annual Conference of the Association for\n  Computer Aided Design in Architecture (ACADIA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is expected to offer new opportunities and a new paradigm for\nthe field of architecture. One such opportunity is teaching neural networks to\nvisually understand architectural elements from the built environment. However,\nthe availability of large training datasets is one of the biggest limitations\nof neural networks. Also, the vast majority of training data for visual\nrecognition tasks is annotated by humans. In order to resolve this bottleneck,\nwe present a concept of a hybrid system using both building information\nmodeling (BIM) and hyperrealistic (photorealistic) rendering to synthesize\ndatasets for training a neural network for building object recognition in\nphotos. For generating our training dataset BIMrAI, we used an existing BIM\nmodel and a corresponding photo-realistically rendered model of the same\nbuilding. We created methods for using renderings to train a deep learning\nmodel, trained a generative adversarial network (GAN) model using these\nmethods, and tested the output model on real-world photos. For the specific\ncase study presented in this paper, our results show that a neural network\ntrained with synthetic data; i.e., photorealistic renderings and BIM-based\nsemantic labels, can be used to identify building objects from photos without\nusing photos in the training data. Future work can enhance the presented\nmethods using available BIM models and renderings for more generalized mapping\nand description of photographed built environments.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 04:08:24 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Alawadhi", "Mohammad", ""], ["Yan", "Wei", ""]]}, {"id": "2105.04104", "submitter": "Min Li", "authors": "Min Li, Yu Li, Ye Tian, Li Jiang and Qiang Xu", "title": "AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative\n  Architecture for DNN Inference", "comments": "Accepted by DAC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 04:13:35 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 08:53:41 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Li", "Min", ""], ["Li", "Yu", ""], ["Tian", "Ye", ""], ["Jiang", "Li", ""], ["Xu", "Qiang", ""]]}, {"id": "2105.04117", "submitter": "KayYen Wong", "authors": "KayYen Wong, Miriam Redi, Diego Saez-Trumper", "title": "Wiki-Reliability: A Large Scale Dataset for Content Reliability on\n  Wikipedia", "comments": "Proceedings of the 44th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '21), 2021", "journal-ref": null, "doi": "10.1145/3404835.3463253", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Wikipedia is the largest online encyclopedia, used by algorithms and web\nusers as a central hub of reliable information on the web. The quality and\nreliability of Wikipedia content is maintained by a community of volunteer\neditors. Machine learning and information retrieval algorithms could help scale\nup editors' manual efforts around Wikipedia content reliability. However, there\nis a lack of large-scale data to support the development of such research. To\nfill this gap, in this paper, we propose Wiki-Reliability, the first dataset of\nEnglish Wikipedia articles annotated with a wide set of content reliability\nissues. To build this dataset, we rely on Wikipedia \"templates\". Templates are\ntags used by expert Wikipedia editors to indicate content issues, such as the\npresence of \"non-neutral point of view\" or \"contradictory articles\", and serve\nas a strong signal for detecting reliability issues in a revision. We select\nthe 10 most popular reliability-related templates on Wikipedia, and propose an\neffective method to label almost 1M samples of Wikipedia article revisions as\npositive or negative with respect to each template. Each positive/negative\nexample in the dataset comes with the full article text and 20 features from\nthe revision's metadata. We provide an overview of the possible downstream\ntasks enabled by such data, and show that Wiki-Reliability can be used to train\nlarge-scale models for content reliability prediction. We release all data and\ncode for public use.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 05:07:03 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 11:57:14 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wong", "KayYen", ""], ["Redi", "Miriam", ""], ["Saez-Trumper", "Diego", ""]]}, {"id": "2105.04118", "submitter": "Nithin Raveendran", "authors": "Xin Xiao, Nithin Raveendran, Bane Vasic, Shu Lin, and Ravi Tandon", "title": "FAID Diversity via Neural Networks", "comments": "7 pages, 3 figures, 3 tables. A shorter version is submitted to the\n  International Symposium on Topics in Coding, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decoder diversity is a powerful error correction framework in which a\ncollection of decoders collaboratively correct a set of error patterns\notherwise uncorrectable by any individual decoder. In this paper, we propose a\nnew approach to design the decoder diversity of finite alphabet iterative\ndecoders (FAIDs) for Low-Density Parity Check (LDPC) codes over the binary\nsymmetric channel (BSC), for the purpose of lowering the error floor while\nguaranteeing the waterfall performance. The proposed decoder diversity is\nachieved by training a recurrent quantized neural network (RQNN) to\nlearn/design FAIDs. We demonstrated for the first time that a machine-learned\ndecoder can surpass in performance a man-made decoder of the same complexity.\nAs RQNNs can model a broad class of FAIDs, they are capable of learning an\narbitrary FAID. To provide sufficient knowledge of the error floor to the RQNN,\nthe training sets are constructed by sampling from the set of most problematic\nerror patterns - trapping sets. In contrast to the existing methods that use\nthe cross-entropy function as the loss function, we introduce a\nframe-error-rate (FER) based loss function to train the RQNN with the objective\nof correcting specific error patterns rather than reducing the bit error rate\n(BER). The examples and simulation results show that the RQNN-aided decoder\ndiversity increases the error correction capability of LDPC codes and lowers\nthe error floor.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 05:14:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Xiao", "Xin", ""], ["Raveendran", "Nithin", ""], ["Vasic", "Bane", ""], ["Lin", "Shu", ""], ["Tandon", "Ravi", ""]]}, {"id": "2105.04129", "submitter": "Andrew Jacobsen", "authors": "Andrew Jacobsen, Alan Chan", "title": "Parameter-free Gradient Temporal Difference Learning", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning lies at the intersection of several challenges. Many\napplications of interest involve extremely large state spaces, requiring\nfunction approximation to enable tractable computation. In addition, the\nlearner has only a single stream of experience with which to evaluate a large\nnumber of possible courses of action, necessitating algorithms which can learn\noff-policy. However, the combination of off-policy learning with function\napproximation leads to divergence of temporal difference methods. Recent work\ninto gradient-based temporal difference methods has promised a path to\nstability, but at the cost of expensive hyperparameter tuning. In parallel,\nprogress in online learning has provided parameter-free methods that achieve\nminimax optimal guarantees up to logarithmic terms, but their application in\nreinforcement learning has yet to be explored. In this work, we combine these\ntwo lines of attack, deriving parameter-free, gradient-based temporal\ndifference algorithms. Our algorithms run in linear time and achieve\nhigh-probability convergence guarantees matching those of GTD2 up to $\\log$\nfactors. Our experiments demonstrate that our methods maintain high prediction\nperformance relative to fully-tuned baselines, with no tuning whatsoever.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:07:05 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Jacobsen", "Andrew", ""], ["Chan", "Alan", ""]]}, {"id": "2105.04130", "submitter": "Sujie Li", "authors": "Sujie Li, Feng Pan, Pengfei Zhou, Pan Zhang", "title": "Boltzmann machines as two-dimensional tensor networks", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG physics.comp-ph quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines (RBM) and deep Boltzmann machines (DBM) are\nimportant models in machine learning, and recently found numerous applications\nin quantum many-body physics. We show that there are fundamental connections\nbetween them and tensor networks. In particular, we demonstrate that any RBM\nand DBM can be exactly represented as a two-dimensional tensor network. This\nrepresentation gives an understanding of the expressive power of RBM and DBM\nusing entanglement structures of the tensor networks, also provides an\nefficient tensor network contraction algorithm for the computing partition\nfunction of RBM and DBM. Using numerical experiments, we demonstrate that the\nproposed algorithm is much more accurate than the state-of-the-art machine\nlearning methods in estimating the partition function of restricted Boltzmann\nmachines and deep Boltzmann machines, and have potential applications in\ntraining deep Boltzmann machines for general machine learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:14:49 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Sujie", ""], ["Pan", "Feng", ""], ["Zhou", "Pengfei", ""], ["Zhang", "Pan", ""]]}, {"id": "2105.04144", "submitter": "Anubha Pandey", "authors": "Aman Gupta, Deepak Bhatt and Anubha Pandey", "title": "Transitioning from Real to Synthetic data: Quantifying the bias in model", "comments": "Accepted at Synthetic Data Generation Workshop at ICLR 2021\n  https://sdg-quality-privacy-bias.github.io/papers/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of generative modeling techniques, synthetic data and its use\nhas penetrated across various domains from unstructured data such as image,\ntext to structured dataset modeling healthcare outcome, risk decisioning in\nfinancial domain, and many more. It overcomes various challenges such as\nlimited training data, class imbalance, restricted access to dataset owing to\nprivacy issues. To ensure the trained model used for automated decisioning\npurposes makes a fair decision there exist prior work to quantify and mitigate\nthose issues. This study aims to establish a trade-off between bias and\nfairness in the models trained using synthetic data. Variants of synthetic data\ngeneration techniques were studied to understand bias amplification including\ndifferentially private generation schemes. Through experiments on a tabular\ndataset, we demonstrate there exist a varying levels of bias impact on models\ntrained using synthetic data. Techniques generating less correlated feature\nperforms well as evident through fairness metrics with 94\\%, 82\\%, and 88\\%\nrelative drop in DPD (demographic parity difference), EoD (equality of odds)\nand EoP (equality of opportunity) respectively, and 24\\% relative improvement\nin DRP (demographic parity ratio) with respect to the real dataset. We believe\nthe outcome of our research study will help data science practitioners\nunderstand the bias in the use of synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:57:14 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gupta", "Aman", ""], ["Bhatt", "Deepak", ""], ["Pandey", "Anubha", ""]]}, {"id": "2105.04153", "submitter": "Yipeng Zhou", "authors": "Laizhong Cui and Xiaoxin Su and Yipeng Zhou and Yi Pan", "title": "Slashing Communication Traffic in Federated Learning by Transmitting\n  Clustered Model Updates", "comments": "To appear in IEEE Journal on Selected Areas in Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is an emerging decentralized learning framework\nthrough which multiple clients can collaboratively train a learning model.\nHowever, a major obstacle that impedes the wide deployment of FL lies in\nmassive communication traffic. To train high dimensional machine learning\nmodels (such as CNN models), heavy communication traffic can be incurred by\nexchanging model updates via the Internet between clients and the parameter\nserver (PS), implying that the network resource can be easily exhausted.\nCompressing model updates is an effective way to reduce the traffic amount.\nHowever, a flexible unbiased compression algorithm applicable for both uplink\nand downlink compression in FL is still absent from existing works. In this\nwork, we devise the Model Update Compression by Soft Clustering (MUCSC)\nalgorithm to compress model updates transmitted between clients and the PS. In\nMUCSC, it is only necessary to transmit cluster centroids and the cluster ID of\neach model update. Moreover, we prove that: 1) The compressed model updates are\nunbiased estimation of their original values so that the convergence rate by\ntransmitting compressed model updates is unchanged; 2) MUCSC can guarantee that\nthe influence of the compression error on the model accuracy is minimized.\nThen, we further propose the boosted MUCSC (B-MUCSC) algorithm, a biased\ncompression algorithm that can achieve an extremely high compression rate by\ngrouping insignificant model updates into a super cluster. B-MUCSC is suitable\nfor scenarios with very scarce network resource. Ultimately, we conduct\nextensive experiments with the CIFAR-10 and FEMNIST datasets to demonstrate\nthat our algorithms can not only substantially reduce the volume of\ncommunication traffic in FL, but also improve the training efficiency in\npractical networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 07:15:49 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cui", "Laizhong", ""], ["Su", "Xiaoxin", ""], ["Zhou", "Yipeng", ""], ["Pan", "Yi", ""]]}, {"id": "2105.04156", "submitter": "Juncai He", "authors": "Juncai He, Lin Li, Jinchao Xu", "title": "ReLU Deep Neural Networks from the Hierarchical Basis Perspective", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study ReLU deep neural networks (DNNs) by investigating their connections\nwith the hierarchical basis method in finite element methods. First, we show\nthat the approximation schemes of ReLU DNNs for $x^2$ and $xy$ are composition\nversions of the hierarchical basis approximation for these two functions. Based\non this fact, we obtain a geometric interpretation and systematic proof for the\napproximation result of ReLU DNNs for polynomials, which plays an important\nrole in a series of recent exponential approximation results of ReLU DNNs.\nThrough our investigation of connections between ReLU DNNs and the hierarchical\nbasis approximation for $x^2$ and $xy$, we show that ReLU DNNs with this\nspecial structure can be applied only to approximate quadratic functions.\nFurthermore, we obtain a concise representation to explicitly reproduce any\nlinear finite element function on a two-dimensional uniform mesh by using ReLU\nDNNs with only two hidden layers.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 07:25:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["He", "Juncai", ""], ["Li", "Lin", ""], ["Xu", "Jinchao", ""]]}, {"id": "2105.04170", "submitter": "Hande Dong", "authors": "Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen,\n  Guli Lin, Keping Yang", "title": "AutoDebias: Learning to Debias for Recommendation", "comments": "Accepted by SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462919", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data. Towards this research\ngap, we first analyze the origin of biases from the perspective of \\textit{risk\ndiscrepancy} that represents the difference between the expectation empirical\nrisk and the true risk. Remarkably, we derive a general learning framework that\nwell summarizes most existing debiasing strategies by specifying some\nparameters of the general framework. This provides a valuable opportunity to\ndevelop a universal solution for debiasing, e.g., by learning the debiasing\nparameters from data. However, the training data lacks important signal of how\nthe data is biased and what the unbiased data looks like. To move this idea\nforward, we propose \\textit{AotoDebias} that leverages another (small) set of\nuniform data to optimize the debiasing parameters by solving the bi-level\noptimization problem with meta-learning. Through theoretical analyses, we\nderive the generalization bound for AutoDebias and prove its ability to acquire\nthe appropriate debiasing strategy. Extensive experiments on two real datasets\nand a simulated dataset demonstrated effectiveness of AutoDebias. The code is\navailable at \\url{https://github.com/DongHande/AutoDebias}.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:03:48 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 07:46:00 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 07:35:19 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Chen", "Jiawei", ""], ["Dong", "Hande", ""], ["Qiu", "Yang", ""], ["He", "Xiangnan", ""], ["Xin", "Xin", ""], ["Chen", "Liang", ""], ["Lin", "Guli", ""], ["Yang", "Keping", ""]]}, {"id": "2105.04180", "submitter": "Hassan Hafez Kolahi", "authors": "Hassan Hafez-Kolahi, Behrad Moniri, Shohreh Kasaei, Mahdieh Soleymani\n  Baghshah", "title": "Rate-Distortion Analysis of Minimum Excess Risk in Bayesian Learning", "comments": "Accepted at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parametric Bayesian learning, a prior is assumed on the parameter $W$\nwhich determines the distribution of samples. In this setting, Minimum Excess\nRisk (MER) is defined as the difference between the minimum expected loss\nachievable when learning from data and the minimum expected loss that could be\nachieved if $W$ was observed. In this paper, we build upon and extend the\nrecent results of (Xu & Raginsky, 2020) to analyze the MER in Bayesian learning\nand derive information-theoretic bounds on it. We formulate the problem as a\n(constrained) rate-distortion optimization and show how the solution can be\nbounded above and below by two other rate-distortion functions that are easier\nto study. The lower bound represents the minimum possible excess risk\nachievable by any process using $R$ bits of information from the parameter $W$.\nFor the upper bound, the optimization is further constrained to use $R$ bits\nfrom the training set, a setting which relates MER to information-theoretic\nbounds on the generalization gap in frequentist learning. We derive\ninformation-theoretic bounds on the difference between these upper and lower\nbounds and show that they can provide order-wise tight rates for MER under\ncertain conditions. This analysis gives more insight into the\ninformation-theoretic nature of Bayesian learning as well as providing novel\nbounds.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:14:10 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 20:48:09 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Hafez-Kolahi", "Hassan", ""], ["Moniri", "Behrad", ""], ["Kasaei", "Shohreh", ""], ["Baghshah", "Mahdieh Soleymani", ""]]}, {"id": "2105.04184", "submitter": "Hojjat Navidan", "authors": "Hojjat Navidan, Parisa Fard Moshiri, Mohammad Nabati, Reza Shahbazian,\n  Seyed Ali Ghorashi, Vahid Shah-Mansouri and David Windridge", "title": "Generative Adversarial Networks (GANs) in Networking: A Comprehensive\n  Survey & Evaluation", "comments": "Accepted for publication at Journal of Computer Networks", "journal-ref": null, "doi": "10.1016/j.comnet.2021.108149", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite the recency of their conception, Generative Adversarial Networks\n(GANs) constitute an extensively researched machine learning sub-field for the\ncreation of synthetic data through deep generative modeling. GANs have\nconsequently been applied in a number of domains, most notably computer vision,\nin which they are typically used to generate or transform synthetic images.\nGiven their relative ease of use, it is therefore natural that researchers in\nthe field of networking (which has seen extensive application of deep learning\nmethods) should take an interest in GAN-based approaches. The need for a\ncomprehensive survey of such activity is therefore urgent. In this paper, we\ndemonstrate how this branch of machine learning can benefit multiple aspects of\ncomputer and communication networks, including mobile networks, network\nanalysis, internet of things, physical layer, and cybersecurity. In doing so,\nwe shall provide a novel evaluation framework for comparing the performance of\ndifferent models in non-image applications, applying this to a number of\nreference network datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:28:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Navidan", "Hojjat", ""], ["Moshiri", "Parisa Fard", ""], ["Nabati", "Mohammad", ""], ["Shahbazian", "Reza", ""], ["Ghorashi", "Seyed Ali", ""], ["Shah-Mansouri", "Vahid", ""], ["Windridge", "David", ""]]}, {"id": "2105.04187", "submitter": "Patricia Wollstadt", "authors": "Patricia Wollstadt and Sebastian Schmitt and Michael Wibral", "title": "A Rigorous Information-Theoretic Definition of Redundancy and Relevancy\n  in Feature Selection Based on (Partial) Information Decomposition", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting a minimal feature set that is maximally informative about a target\nvariable is a central task in machine learning and statistics. Information\ntheory provides a powerful framework for formulating feature selection\nalgorithms -- yet, a rigorous, information-theoretic definition of feature\nrelevancy, which accounts for feature interactions such as redundant and\nsynergistic contributions, is still missing. We argue that this lack is\ninherent to classical information theory which does not provide measures to\ndecompose the information a set of variables provides about a target into\nunique, redundant, and synergistic contributions. Such a decomposition has been\nintroduced only recently by the partial information decomposition (PID)\nframework. Using PID, we clarify why feature selection is a conceptually\ndifficult problem when approached using information theory and provide a novel\ndefinition of feature relevancy and redundancy in PID terms. From this\ndefinition, we show that the conditional mutual information (CMI) maximizes\nrelevancy while minimizing redundancy and propose an iterative, CMI-based\nalgorithm for practical feature selection. We demonstrate the power of our\nCMI-based algorithm in comparison to the unconditional mutual information on\nbenchmark examples and provide corresponding PID estimates to highlight how PID\nallows to quantify information contribution of features and their interactions\nin feature-selection problems.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:33:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wollstadt", "Patricia", ""], ["Schmitt", "Sebastian", ""], ["Wibral", "Michael", ""]]}, {"id": "2105.04196", "submitter": "Mohammad Parvini", "authors": "Mohammad Parvini, Mohammad Reza Javan, Nader Mokari, Bijan Abbasi, and\n  Eduard A. Jorswieck", "title": "AoI-Aware Resource Allocation for Platoon-Based C-V2X Networks via\n  Multi-Agent Multi-Task Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the problem of age of information (AoI) aware radio\nresource management for a platooning system. Multiple autonomous platoons\nexploit the cellular wireless vehicle-to-everything (C-V2X) communication\ntechnology to disseminate the cooperative awareness messages (CAMs) to their\nfollowers while ensuring timely delivery of safety-critical messages to the\nRoad-Side Unit (RSU). Due to the challenges of dynamic channel conditions,\ncentralized resource management schemes that require global information are\ninefficient and lead to large signaling overheads. Hence, we exploit a\ndistributed resource allocation framework based on multi-agent reinforcement\nlearning (MARL), where each platoon leader (PL) acts as an agent and interacts\nwith the environment to learn its optimal policy. Existing MARL algorithms\nconsider a holistic reward function for the group's collective success, which\noften ends up with unsatisfactory results and cannot guarantee an optimal\npolicy for each agent. Consequently, motivated by the existing literature in\nRL, we propose a novel MARL framework that trains two critics with the\nfollowing goals: A global critic which estimates the global expected reward and\nmotivates the agents toward a cooperating behavior and an exclusive local\ncritic for each agent that estimates the local individual reward. Furthermore,\nbased on the tasks each agent has to accomplish, the individual reward of each\nagent is decomposed into multiple sub-reward functions where task-wise value\nfunctions are learned separately. Numerical results indicate our proposed\nalgorithm's effectiveness compared with the conventional RL methods applied in\nthis area.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:39:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Parvini", "Mohammad", ""], ["Javan", "Mohammad Reza", ""], ["Mokari", "Nader", ""], ["Abbasi", "Bijan", ""], ["Jorswieck", "Eduard A.", ""]]}, {"id": "2105.04207", "submitter": "Mhsn Prghasemian", "authors": "Mohammad Akbari, Mohammad Reza Abedi, Roghayeh Joda, Mohsen\n  Pourghasemian, Nader Mokari, and Melike Erol-Kantarci", "title": "Age of Information Aware VNF Scheduling in Industrial IoT Using Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In delay-sensitive industrial internet of things (IIoT) applications, the age\nof information (AoI) is employed to characterize the freshness of information.\nMeanwhile, the emerging network function virtualization provides flexibility\nand agility for service providers to deliver a given network service using a\nsequence of virtual network functions (VNFs). However, suitable VNF placement\nand scheduling in these schemes is NP-hard and finding a globally optimal\nsolution by traditional approaches is complex. Recently, deep reinforcement\nlearning (DRL) has appeared as a viable way to solve such problems. In this\npaper, we first utilize single agent low-complex compound action actor-critic\nRL to cover both discrete and continuous actions and jointly minimize VNF cost\nand AoI in terms of network resources under end-to end Quality of Service\nconstraints. To surmount the single-agent capacity limitation for learning, we\nthen extend our solution to a multi-agent DRL scheme in which agents\ncollaborate with each other. Simulation results demonstrate that single-agent\nschemes significantly outperform the greedy algorithm in terms of average\nnetwork cost and AoI. Moreover, multi-agent solution decreases the average cost\nby dividing the tasks between the agents. However, it needs more iterations to\nbe learned due to the requirement on the agents collaboration.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:04:49 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Akbari", "Mohammad", ""], ["Abedi", "Mohammad Reza", ""], ["Joda", "Roghayeh", ""], ["Pourghasemian", "Mohsen", ""], ["Mokari", "Nader", ""], ["Erol-Kantarci", "Melike", ""]]}, {"id": "2105.04210", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Yinfei Xu, Qinghe Liu, Zhicheng Liu, Jian Lu and Qiao\n  Wang", "title": "Robust Graph Learning Under Wasserstein Uncertainty", "comments": "21 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are playing a crucial role in different fields since they are powerful\ntools to unveil intrinsic relationships among signals. In many scenarios, an\naccurate graph structure representing signals is not available at all and that\nmotivates people to learn a reliable graph structure directly from observed\nsignals. However, in real life, it is inevitable that there exists uncertainty\nin the observed signals due to noise measurements or limited observability,\nwhich causes a reduction in reliability of the learned graph. To this end, we\npropose a graph learning framework using Wasserstein distributionally robust\noptimization (WDRO) which handles uncertainty in data by defining an\nuncertainty set on distributions of the observed data. Specifically, two models\nare developed, one of which assumes all distributions in uncertainty set are\nGaussian distributions and the other one has no prior distributional\nassumption. Instead of using interior point method directly, we propose two\nalgorithms to solve the corresponding models and show that our algorithms are\nmore time-saving. In addition, we also reformulate both two models into\nSemi-Definite Programming (SDP), and illustrate that they are intractable in\nthe scenario of large-scale graph. Experiments on both synthetic and real world\ndata are carried out to validate the proposed framework, which show that our\nscheme can learn a reliable graph in the context of uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:09:44 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 01:12:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhang", "Xiang", ""], ["Xu", "Yinfei", ""], ["Liu", "Qinghe", ""], ["Liu", "Zhicheng", ""], ["Lu", "Jian", ""], ["Wang", "Qiao", ""]]}, {"id": "2105.04211", "submitter": "Maud Lemercier", "authors": "Maud Lemercier, Cristopher Salvi, Thomas Cass, Edwin V. Bonilla,\n  Theodoros Damoulas, Terry Lyons", "title": "SigGPDE: Scaling Sparse Gaussian Processes on Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making predictions and quantifying their uncertainty when the input data is\nsequential is a fundamental learning challenge, recently attracting increasing\nattention. We develop SigGPDE, a new scalable sparse variational inference\nframework for Gaussian Processes (GPs) on sequential data. Our contribution is\ntwofold. First, we construct inducing variables underpinning the sparse\napproximation so that the resulting evidence lower bound (ELBO) does not\nrequire any matrix inversion. Second, we show that the gradients of the GP\nsignature kernel are solutions of a hyperbolic partial differential equation\n(PDE). This theoretical insight allows us to build an efficient\nback-propagation algorithm to optimize the ELBO. We showcase the significant\ncomputational gains of SigGPDE compared to existing methods, while achieving\nstate-of-the-art performance for classification tasks on large datasets of up\nto 1 million multivariate time series.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:10:17 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lemercier", "Maud", ""], ["Salvi", "Cristopher", ""], ["Cass", "Thomas", ""], ["Bonilla", "Edwin V.", ""], ["Damoulas", "Theodoros", ""], ["Lyons", "Terry", ""]]}, {"id": "2105.04218", "submitter": "Jie Ran", "authors": "Jie Ran, Rui Lin, Hayden K.H. So, Graziano Chesi, Ngai Wong", "title": "Exploiting Elasticity in Tensor Ranks for Compressing Neural Networks", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elasticities in depth, width, kernel size and resolution have been explored\nin compressing deep neural networks (DNNs). Recognizing that the kernels in a\nconvolutional neural network (CNN) are 4-way tensors, we further exploit a new\nelasticity dimension along the input-output channels. Specifically, a novel\nnuclear-norm rank minimization factorization (NRMF) approach is proposed to\ndynamically and globally search for the reduced tensor ranks during training.\nCorrelation between tensor ranks across multiple layers is revealed, and a\ngraceful tradeoff between model size and accuracy is obtained. Experiments then\nshow the superiority of NRMF over the previous non-elastic variational Bayesian\nmatrix factorization (VBMF) scheme.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:26:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ran", "Jie", ""], ["Lin", "Rui", ""], ["So", "Hayden K. H.", ""], ["Chesi", "Graziano", ""], ["Wong", "Ngai", ""]]}, {"id": "2105.04232", "submitter": "Andreas B{\\ae}rentzen", "authors": "Martin O. Elingaard, Niels Aage, J. Andreas B{\\ae}rentzen, Ole Sigmund", "title": "De-homogenization using Convolutional Neural Networks", "comments": "28 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep learning-based de-homogenization method for\nstructural compliance minimization. By using a convolutional neural network to\nparameterize the mapping from a set of lamination parameters on a coarse mesh\nto a one-scale design on a fine mesh, we avoid solving the least square\nproblems associated with traditional de-homogenization approaches and save time\ncorrespondingly. To train the neural network, a two-step custom loss function\nhas been developed which ensures a periodic output field that follows the local\nlamination orientations. A key feature of the proposed method is that the\ntraining is carried out without any use of or reference to the underlying\nstructural optimization problem, which renders the proposed method robust and\ninsensitive wrt. domain size, boundary conditions, and loading. A\npost-processing procedure utilizing a distance transform on the output field\nskeleton is used to project the desired lamination widths onto the output field\nwhile ensuring a predefined minimum length-scale and volume fraction. To\ndemonstrate that the deep learning approach has excellent generalization\nproperties, numerical examples are shown for several different load and\nboundary conditions. For an appropriate choice of parameters, the\nde-homogenized designs perform within $7-25\\%$ of the homogenization-based\nsolution at a fraction of the computational cost. With several options for\nfurther improvements, the scheme may provide the basis for future interactive\nhigh-resolution topology optimization.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:50:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Elingaard", "Martin O.", ""], ["Aage", "Niels", ""], ["B\u00e6rentzen", "J. Andreas", ""], ["Sigmund", "Ole", ""]]}, {"id": "2105.04236", "submitter": "Deevashwer Rathee", "authors": "Deevashwer Rathee, Mayank Rathee, Rahul Kranti Kiran Goli, Divya\n  Gupta, Rahul Sharma, Nishanth Chandran, Aseem Rastogi", "title": "SIRNN: A Math Library for Secure RNN Inference", "comments": "IEEE Security and Privacy 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex machine learning (ML) inference algorithms like recurrent neural\nnetworks (RNNs) use standard functions from math libraries like exponentiation,\nsigmoid, tanh, and reciprocal of square root. Although prior work on secure\n2-party inference provides specialized protocols for convolutional neural\nnetworks (CNNs), existing secure implementations of these math operators rely\non generic 2-party computation (2PC) protocols that suffer from high\ncommunication. We provide new specialized 2PC protocols for math functions that\ncrucially rely on lookup-tables and mixed-bitwidths to address this performance\noverhead; our protocols for math functions communicate up to 423x less data\nthan prior work. Some of the mixed bitwidth operations used by our math\nimplementations are (zero and signed) extensions, different forms of\ntruncations, multiplication of operands of mixed-bitwidths, and digit\ndecomposition (a generalization of bit decomposition to larger digits). For\neach of these primitive operations, we construct specialized 2PC protocols that\nare more communication efficient than generic 2PC, and can be of independent\ninterest. Furthermore, our math implementations are numerically precise, which\nensures that the secure implementations preserve model accuracy of cleartext.\nWe build on top of our novel protocols to build SIRNN, a library for end-to-end\nsecure 2-party DNN inference, that provides the first secure implementations of\nan RNN operating on time series sensor data, an RNN operating on speech data,\nand a state-of-the-art ML architecture that combines CNNs and RNNs for\nidentifying all heads present in images. Our evaluation shows that SIRNN\nachieves up to three orders of magnitude of performance improvement when\ncompared to inference of these models using an existing state-of-the-art 2PC\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:04:46 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Rathee", "Deevashwer", ""], ["Rathee", "Mayank", ""], ["Goli", "Rahul Kranti Kiran", ""], ["Gupta", "Divya", ""], ["Sharma", "Rahul", ""], ["Chandran", "Nishanth", ""], ["Rastogi", "Aseem", ""]]}, {"id": "2105.04240", "submitter": "Jun Lu", "authors": "Jun Lu", "title": "A rigorous introduction for linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This note is meant to provide an introduction to linear models and the\ntheories behind them. Our goal is to give a rigorous introduction to the\nreaders with prior exposure to ordinary least squares. In machine learning, the\noutput is usually a nonlinear function of the input. Deep learning even aims to\nfind a nonlinear dependence with many layers which require a large amount of\ncomputation. However, most of these algorithms build upon simple linear models.\nWe then describe linear models from different views and find the properties and\ntheories behind the models. The linear model is the main technique in\nregression problems and the primary tool for it is the least squares\napproximation which minimizes a sum of squared errors. This is a natural choice\nwhen we're interested in finding the regression function which minimizes the\ncorresponding expected squared error. We first describe ordinary least squares\nfrom three different points of view upon which we disturb the model with random\nnoise and Gaussian noise. By Gaussian noise, the model gives rise to the\nlikelihood so that we introduce a maximum likelihood estimator. It also\ndevelops some distribution theories for it via this Gaussian disturbance. The\ndistribution theory of least squares will help us answer various questions and\nintroduce related applications. We then prove least squares is the best\nunbiased linear model in the sense of mean squared error and most importantly,\nit actually approaches the theoretical limit. We end up with linear models with\nthe Bayesian approach and beyond.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:12:28 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 12:47:29 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Lu", "Jun", ""]]}, {"id": "2105.04241", "submitter": "Yury Zemlyanskiy", "authors": "Yury Zemlyanskiy, Joshua Ainslie, Michiel de Jong, Philip Pham, Ilya\n  Eckstein, Fei Sha", "title": "ReadTwice: Reading Very Large Documents with Memories", "comments": "To appear in the proceedings of NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge-intensive tasks such as question answering often require\nassimilating information from different sections of large inputs such as books\nor article collections. We propose ReadTwice, a simple and effective technique\nthat combines several strengths of prior approaches to model long-range\ndependencies with Transformers. The main idea is to read text in small\nsegments, in parallel, summarizing each segment into a memory table to be used\nin a second read of the text. We show that the method outperforms models of\ncomparable size on several question answering (QA) datasets and sets a new\nstate of the art on the challenging NarrativeQA task, with questions about\nentire books. Source code and pre-trained checkpoints for ReadTwice can be\nfound at https://goo.gle/research-readtwice.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:13:09 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 23:07:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zemlyanskiy", "Yury", ""], ["Ainslie", "Joshua", ""], ["de Jong", "Michiel", ""], ["Pham", "Philip", ""], ["Eckstein", "Ilya", ""], ["Sha", "Fei", ""]]}, {"id": "2105.04242", "submitter": "Mikolaj Wieczorek", "authors": "Barbara Rychalska, Mikolaj Wieczorek, Jacek Dabrowski", "title": "T-EMDE: Sketching-based global similarity for cross-modal retrieval", "comments": "10 pages,5 figures, 4 tables, 1 code snippet", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge in cross-modal retrieval is to find similarities between\nobjects represented with different modalities, such as image and text. However,\neach modality embeddings stem from non-related feature spaces, which causes the\nnotorious 'heterogeneity gap'. Currently, many cross-modal systems try to\nbridge the gap with self-attention. However, self-attention has been widely\ncriticized for its quadratic complexity, which prevents many real-life\napplications. In response to this, we propose T-EMDE - a neural density\nestimator inspired by the recently introduced Efficient Manifold Density\nEstimator (EMDE) from the area of recommender systems. EMDE operates on\nsketches - representations especially suitable for multimodal operations.\nHowever, EMDE is non-differentiable and ingests precomputed, static embeddings.\nWith T-EMDE we introduce a trainable version of EMDE which allows full\nend-to-end training. In contrast to self-attention, the complexity of our\nsolution is linear to the number of tokens/segments. As such, T-EMDE is a\ndrop-in replacement for the self-attention module, with beneficial influence on\nboth speed and metric performance in cross-modal settings. It facilitates\ncommunication between modalities, as each global text/image representation is\nexpressed with a standardized sketch histogram which represents the same\nmanifold structures irrespective of the underlying modality. We evaluate T-EMDE\nby introducing it into two recent cross-modal SOTA models and achieving new\nstate-of-the-art results on multiple datasets and decreasing model latency by\nup to 20%.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:14:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Rychalska", "Barbara", ""], ["Wieczorek", "Mikolaj", ""], ["Dabrowski", "Jacek", ""]]}, {"id": "2105.04246", "submitter": "Marios Fournarakis", "authors": "Marios Fournarakis, Markus Nagel", "title": "In-Hindsight Quantization Range Estimation for Quantized Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantization techniques applied to the inference of deep neural networks have\nenabled fast and efficient execution on resource-constraint devices. The\nsuccess of quantization during inference has motivated the academic community\nto explore fully quantized training, i.e. quantizing back-propagation as well.\nHowever, effective gradient quantization is still an open problem. Gradients\nare unbounded and their distribution changes significantly during training,\nwhich leads to the need for dynamic quantization. As we show, dynamic\nquantization can lead to significant memory overhead and additional data\ntraffic slowing down training. We propose a simple alternative to dynamic\nquantization, in-hindsight range estimation, that uses the quantization ranges\nestimated on previous iterations to quantize the present. Our approach enables\nfast static quantization of gradients and activations while requiring only\nminimal hardware support from the neural network accelerator to keep track of\noutput statistics in an online fashion. It is intended as a drop-in replacement\nfor estimating quantization ranges and can be used in conjunction with other\nadvances in quantized training. We compare our method to existing methods for\nrange estimation from the quantized training literature and demonstrate its\neffectiveness with a range of architectures, including MobileNetV2, on image\nclassification benchmarks (Tiny ImageNet & ImageNet).\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:25:28 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Fournarakis", "Marios", ""], ["Nagel", "Markus", ""]]}, {"id": "2105.04247", "submitter": "Alexander Hagg", "authors": "Alexander Hagg, Sebastian Berns, Alexander Asteroth, Simon Colton,\n  Thomas B\\\"ack", "title": "Expressivity of Parameterized and Data-driven Representations in Quality\n  Diversity Search", "comments": "For code for reproducing experiments, see\n  https://github.com/alexander-hagg/ExpressivityGECCO2021", "journal-ref": null, "doi": "10.1145/3449639.3459287", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider multi-solution optimization and generative models for the\ngeneration of diverse artifacts and the discovery of novel solutions. In cases\nwhere the domain's factors of variation are unknown or too complex to encode\nmanually, generative models can provide a learned latent space to approximate\nthese factors. When used as a search space, however, the range and diversity of\npossible outputs are limited to the expressivity and generative capabilities of\nthe learned model. We compare the output diversity of a quality diversity\nevolutionary search performed in two different search spaces: 1) a predefined\nparameterized space and 2) the latent space of a variational autoencoder model.\nWe find that the search on an explicit parametric encoding creates more diverse\nartifact sets than searching the latent space. A learned model is better at\ninterpolating between known data points than at extrapolating or expanding\ntowards unseen examples. We recommend using a generative model's latent space\nprimarily to measure similarity between artifacts rather than for search and\ngeneration. Whenever a parametric encoding is obtainable, it should be\npreferred over a learned representation as it produces a higher diversity of\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:27:43 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hagg", "Alexander", ""], ["Berns", "Sebastian", ""], ["Asteroth", "Alexander", ""], ["Colton", "Simon", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2105.04249", "submitter": "Junaid Ali", "authors": "Junaid Ali, Preethi Lahoti, Krishna P. Gummadi", "title": "Accounting for Model Uncertainty in Algorithmic Discrimination", "comments": "12 pages, Accepted at AIES 2021", "journal-ref": null, "doi": "10.1145/3461702.3462630", "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional approaches to ensure group fairness in algorithmic decision\nmaking aim to equalize ``total'' error rates for different subgroups in the\npopulation. In contrast, we argue that the fairness approaches should instead\nfocus only on equalizing errors arising due to model uncertainty (a.k.a\nepistemic uncertainty), caused due to lack of knowledge about the best model or\ndue to lack of data. In other words, our proposal calls for ignoring the errors\nthat occur due to uncertainty inherent in the data, i.e., aleatoric\nuncertainty. We draw a connection between predictive multiplicity and model\nuncertainty and argue that the techniques from predictive multiplicity could be\nused to identify errors made due to model uncertainty. We propose scalable\nconvex proxies to come up with classifiers that exhibit predictive multiplicity\nand empirically show that our methods are comparable in performance and up to\nfour orders of magnitude faster than the current state-of-the-art. We further\npropose methods to achieve our goal of equalizing group error rates arising due\nto model uncertainty in algorithmic decision making and demonstrate the\neffectiveness of these methods using synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:34:12 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ali", "Junaid", ""], ["Lahoti", "Preethi", ""], ["Gummadi", "Krishna P.", ""]]}, {"id": "2105.04252", "submitter": "Alexander Hagg", "authors": "Alexander Hagg, Mike Preuss, Alexander Asteroth, Thomas B\\\"ack", "title": "An Analysis of Phenotypic Diversity in Multi-Solution Optimization", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-63710-1_4", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  More and more, optimization methods are used to find diverse solution sets.\nWe compare solution diversity in multi-objective optimization, multimodal\noptimization, and quality diversity in a simple domain. We show that\nmultiobjective optimization does not always produce much diversity, multimodal\noptimization produces higher fitness solutions, and quality diversity is not\nsensitive to genetic neutrality and creates the most diverse set of solutions.\nAn autoencoder is used to discover phenotypic features automatically, producing\nan even more diverse solution set with quality diversity. Finally, we make\nrecommendations about when to use which approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:39:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hagg", "Alexander", ""], ["Preuss", "Mike", ""], ["Asteroth", "Alexander", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2105.04269", "submitter": "Marvin Lerousseau", "authors": "Marvin Lerousseau and Marion Classe and Enzo Battistella and Th\\'eo\n  Estienne and Th\\'eophraste Henry and Amaury Leroy and Roger Sun and Maria\n  Vakalopoulou and Jean-Yves Scoazec and Eric Deutsch and Nikos Paragios", "title": "Weakly supervised pan-cancer segmentation tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The vast majority of semantic segmentation approaches rely on pixel-level\nannotations that are tedious and time consuming to obtain and suffer from\nsignificant inter and intra-expert variability. To address these issues, recent\napproaches have leveraged categorical annotations at the slide-level, that in\ngeneral suffer from robustness and generalization. In this paper, we propose a\nnovel weakly supervised multi-instance learning approach that deciphers\nquantitative slide-level annotations which are fast to obtain and regularly\npresent in clinical routine. The extreme potentials of the proposed approach\nare demonstrated for tumor segmentation of solid cancer subtypes. The proposed\napproach achieves superior performance in out-of-distribution, out-of-location,\nand out-of-domain testing sets.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 11:11:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lerousseau", "Marvin", ""], ["Classe", "Marion", ""], ["Battistella", "Enzo", ""], ["Estienne", "Th\u00e9o", ""], ["Henry", "Th\u00e9ophraste", ""], ["Leroy", "Amaury", ""], ["Sun", "Roger", ""], ["Vakalopoulou", "Maria", ""], ["Scoazec", "Jean-Yves", ""], ["Deutsch", "Eric", ""], ["Paragios", "Nikos", ""]]}, {"id": "2105.04273", "submitter": "Junaid Ali", "authors": "Junaid Ali, Muhammad Bilal Zafar, Adish Singla, Krishna P. Gummadi", "title": "Loss-Aversively Fair Classification", "comments": "8 pages, Accepted at AIES 2019", "journal-ref": "In AAAI/ACM Conference on AI, Ethics, and Society (AIES 2019),\n  January 27-28 2019 Honolulu, HI, USA", "doi": "10.1145/3461702.3462630", "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of algorithmic (learning-based) decision making in scenarios that\naffect human lives has motivated a number of recent studies to investigate such\ndecision making systems for potential unfairness, such as discrimination\nagainst subjects based on their sensitive features like gender or race.\nHowever, when judging the fairness of a newly designed decision making system,\nthese studies have overlooked an important influence on people's perceptions of\nfairness, which is how the new algorithm changes the status quo, i.e.,\ndecisions of the existing decision making system. Motivated by extensive\nliterature in behavioral economics and behavioral psychology (prospect theory),\nwe propose a notion of fair updates that we refer to as loss-averse updates.\nLoss-averse updates constrain the updates to yield improved (more beneficial)\noutcomes to subjects compared to the status quo. We propose tractable proxy\nmeasures that would allow this notion to be incorporated in the training of a\nvariety of linear and non-linear classifiers. We show how our proxy measures\ncan be combined with existing measures for training nondiscriminatory\nclassifiers. Our evaluation using synthetic and real-world datasets\ndemonstrates that the proposed proxy measures are effective for their desired\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 11:19:27 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ali", "Junaid", ""], ["Zafar", "Muhammad Bilal", ""], ["Singla", "Adish", ""], ["Gummadi", "Krishna P.", ""]]}, {"id": "2105.04289", "submitter": "Andrei Margeloiu", "authors": "Andrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi Chen, Mateja\n  Jamnik, Adrian Weller", "title": "Do Concept Bottleneck Models Learn as Intended?", "comments": "Accepted at ICLR 2021 Workshop on Responsible AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept bottleneck models map from raw inputs to concepts, and then from\nconcepts to targets. Such models aim to incorporate pre-specified, high-level\nconcepts into the learning procedure, and have been motivated to meet three\ndesiderata: interpretability, predictability, and intervenability. However, we\nfind that concept bottleneck models struggle to meet these goals. Using post\nhoc interpretability methods, we demonstrate that concepts do not correspond to\nanything semantically meaningful in input space, thus calling into question the\nusefulness of concept bottleneck models in their current form.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:00:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Margeloiu", "Andrei", ""], ["Ashman", "Matthew", ""], ["Bhatt", "Umang", ""], ["Chen", "Yanzhi", ""], ["Jamnik", "Mateja", ""], ["Weller", "Adrian", ""]]}, {"id": "2105.04290", "submitter": "Xingchen Ma", "authors": "Xingchen Ma, Matthew B. Blaschko", "title": "Meta-Cal: Well-controlled Post-hoc Calibration by Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, it is desirable that a classifier not only makes\naccurate predictions, but also outputs calibrated posterior probabilities.\nHowever, many existing classifiers, especially deep neural network classifiers,\ntend to be uncalibrated. Post-hoc calibration is a technique to recalibrate a\nmodel by learning a calibration map. Existing approaches mostly focus on\nconstructing calibration maps with low calibration errors, however, this\nquality is inadequate for a calibrator being useful. In this paper, we\nintroduce two constraints that are worth consideration in designing a\ncalibration map for post-hoc calibration. Then we present Meta-Cal, which is\nbuilt from a base calibrator and a ranking model. Under some mild assumptions,\ntwo high-probability bounds are given with respect to these constraints.\nEmpirical results on CIFAR-10, CIFAR-100 and ImageNet and a range of popular\nnetwork architectures show our proposed method significantly outperforms the\ncurrent state of the art for post-hoc multi-class classification calibration.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:00:54 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 11:51:22 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ma", "Xingchen", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "2105.04294", "submitter": "Tonatiuh Hern\\'andez-Del-Toro M.Sc.", "authors": "Tonatiuh Hern\\'andez-Del-Toro, Carlos A. Reyes-Garc\\'ia, Luis\n  Villase\\~nor-Pineda", "title": "Toward asynchronous EEG-based BCI: Detecting imagined words segments in\n  continuous EEG signals", "comments": "10 pages, 14 figures", "journal-ref": "Biomedical Signal Processing and Control. Volume 65 (2021), 102351", "doi": "10.1016/j.bspc.2020.102351", "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An asynchronous Brain--Computer Interface (BCI) based on imagined speech is a\ntool that allows to control an external device or to emit a message at the\nmoment the user desires to by decoding EEG signals of imagined speech. In order\nto correctly implement these types of BCI, we must be able to detect from a\ncontinuous signal, when the subject starts to imagine words. In this work, five\nmethods of feature extraction based on wavelet decomposition, empirical mode\ndecomposition, frequency energies, fractal dimension and chaos theory features\nare presented to solve the task of detecting imagined words segments from\ncontinuous EEG signals as a preliminary study for a latter implementation of an\nasynchronous BCI based on imagined speech. These methods are tested in three\ndatasets using four different classifiers and the higher F1 scores obtained are\n0.73, 0.79, and 0.68 for each dataset, respectively. This results are promising\nto build a system that automatizes the segmentation of imagined words segments\nfor latter classification.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 00:13:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hern\u00e1ndez-Del-Toro", "Tonatiuh", ""], ["Reyes-Garc\u00eda", "Carlos A.", ""], ["Villase\u00f1or-Pineda", "Luis", ""]]}, {"id": "2105.04297", "submitter": "Shuxin Zheng", "authors": "Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, Tie-Yan Liu", "title": "How could Neural Networks understand Programs?", "comments": null, "journal-ref": "ICML 2021", "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Semantic understanding of programs is a fundamental problem for programming\nlanguage processing (PLP). Recent works that learn representations of code\nbased on pre-training techniques in NLP have pushed the frontiers in this\ndirection. However, the semantics of PL and NL have essential differences.\nThese being ignored, we believe it is difficult to build a model to better\nunderstand programs, by either directly applying off-the-shelf NLP pre-training\ntechniques to the source code, or adding features to the model by the\nheuristic. In fact, the semantics of a program can be rigorously defined by\nformal semantics in PL theory. For example, the operational semantics,\ndescribes the meaning of a valid program as updating the environment (i.e., the\nmemory address-value function) through fundamental operations, such as memory\nI/O and conditional branching. Inspired by this, we propose a novel program\nsemantics learning paradigm, that the model should learn from information\ncomposed of (1) the representations which align well with the fundamental\noperations in operational semantics, and (2) the information of environment\ntransition, which is indispensable for program understanding. To validate our\nproposal, we present a hierarchical Transformer-based pre-training model called\nOSCAR to better facilitate the understanding of programs. OSCAR learns from\nintermediate representation (IR) and an encoded representation derived from\nstatic analysis, which are used for representing the fundamental operations and\napproximating the environment transitions respectively. OSCAR empirically shows\nthe outstanding capability of program semantics understanding on many practical\nsoftware engineering tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:21:42 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 08:44:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Peng", "Dinglan", ""], ["Zheng", "Shuxin", ""], ["Li", "Yatao", ""], ["Ke", "Guolin", ""], ["He", "Di", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2105.04301", "submitter": "Zhewei Chen", "authors": "Zhewei Chen, Linyue Zhou, Wenwen Yu", "title": "ADASYN-Random Forest Based Intrusion Detection Model", "comments": "SPML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrusion detection has been a key topic in the field of cyber security, and\nthe common network threats nowadays have the characteristics of varieties and\nvariation. Considering the serious imbalance of intrusion detection datasets\nwill result in low classification performance on attack behaviors of small\nsample size and difficulty to detect network attacks accurately and\nefficiently, using Adaptive Synthetic Sampling (ADASYN) method to balance\ndatasets was proposed in this paper. In addition, Random Forest algorithm was\nused to train intrusion detection classifiers. Through the comparative\nexperiment of Intrusion detection on CICIDS 2017 dataset, it is found that\nADASYN with Random Forest performs better. Based on the experimental results,\nthe improvement of precision, recall, F1 scores and AUC values after ADASYN is\nthen analyzed. Experiments show that the proposed method can be applied to\nintrusion detection with large data, and can effectively improve the\nclassification accuracy of network attack behaviors. Compared with traditional\nmachine learning models, it has better performance, generalization ability and\nrobustness.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:22:36 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 14:26:18 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 02:04:09 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chen", "Zhewei", ""], ["Zhou", "Linyue", ""], ["Yu", "Wenwen", ""]]}, {"id": "2105.04309", "submitter": "Florian Henkel", "authors": "Florian Henkel and Gerhard Widmer", "title": "Multi-modal Conditional Bounding Box Regression for Music Score\n  Following", "comments": "Accepted for publication in the Proceedings of the 29th European\n  Signal Processing Conference (EUSIPCO), Dublin, Ireland, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of sheet-image-based on-line audio-to-score\nalignment also known as score following. Drawing inspiration from object\ndetection, a conditional neural network architecture is proposed that directly\npredicts x,y coordinates of the matching positions in a complete score sheet\nimage at each point in time for a given musical performance. Experiments are\nconducted on a synthetic polyphonic piano benchmark dataset and the new method\nis compared to several existing approaches from the literature for\nsheet-image-based score following as well as an Optical Music Recognition\nbaseline. The proposed approach achieves new state-of-the-art results and\nfurthermore significantly improves the alignment performance on a set of\nreal-world piano recordings by applying Impulse Responses as a data\naugmentation technique.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:43:35 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Henkel", "Florian", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2105.04313", "submitter": "Marius Lehne", "authors": "Shachar Klaiman and Marius Lehne", "title": "DocReader: Bounding-Box Free Training of a Document Information\n  Extraction Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction from documents is a ubiquitous first step in many\nbusiness applications. During this step, the entries of various fields must\nfirst be read from the images of scanned documents before being further\nprocessed and inserted into the corresponding databases. While many different\nmethods have been developed over the past years in order to automate the above\nextraction step, they all share the requirement of bounding-box or text segment\nannotations of their training documents. In this work we present DocReader, an\nend-to-end neural-network-based information extraction solution which can be\ntrained using solely the images and the target values that need to be read. The\nDocReader can thus leverage existing historical extraction data, completely\neliminating the need for any additional annotations beyond what is naturally\navailable in existing human-operated service centres. We demonstrate that the\nDocReader can reach and surpass other methods which require bounding-boxes for\ntraining, as well as provide a clear path for continual learning during its\ndeployment in production.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:48:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Klaiman", "Shachar", ""], ["Lehne", "Marius", ""]]}, {"id": "2105.04319", "submitter": "Leon Bungert", "authors": "Leon Bungert, Tim Roith, Daniel Tenbrinck, Martin Burger", "title": "A Bregman Learning Framework for Sparse Neural Networks", "comments": "Corrected stochastic convergence proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning framework based on stochastic Bregman iterations to\ntrain sparse neural networks with an inverse scale space approach. We derive a\nbaseline algorithm called LinBreg, an accelerated version using momentum, and\nAdaBreg, which is a Bregmanized generalization of the Adam algorithm. In\ncontrast to established methods for sparse training the proposed family of\nalgorithms constitutes a regrowth strategy for neural networks that is solely\noptimization-based without additional heuristics. Our Bregman learning\nframework starts the training with very few initial parameters, successively\nadding only significant ones to obtain a sparse and expressive network. The\nproposed approach is extremely easy and efficient, yet supported by the rich\nmathematical theory of inverse scale space methods. We derive a statistically\nprofound sparse parameter initialization strategy and provide a rigorous\nstochastic convergence analysis of the loss decay and additional convergence\nproofs in the convex regime. Using only 3.4% of the parameters of ResNet-18 we\nachieve 90.2% test accuracy on CIFAR-10, compared to 93.6% using the dense\nnetwork. Our algorithm also unveils an autoencoder architecture for a denoising\ntask. The proposed framework also has a huge potential for integrating sparse\nbackpropagation and resource-friendly training.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:56:01 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 10:57:49 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bungert", "Leon", ""], ["Roith", "Tim", ""], ["Tenbrinck", "Daniel", ""], ["Burger", "Martin", ""]]}, {"id": "2105.04321", "submitter": "Dominic Rose", "authors": "Avishek Das, Dominic C. Rose, Juan P. Garrahan, David T. Limmer", "title": "Reinforcement learning of rare diffusive dynamics", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to probe rare molecular dynamics trajectories directly\nusing reinforcement learning. We consider trajectories that are conditioned to\ntransition between regions of configuration space in finite time, like those\nrelevant in the study of reactive events, as well as trajectories exhibiting\nrare fluctuations of time-integrated quantities in the long time limit, like\nthose relevant in the calculation of large deviation functions. In both cases,\nreinforcement learning techniques are used to optimize an added force that\nminimizes the Kullback-Leibler divergence between the conditioned trajectory\nensemble and a driven one. Under the optimized added force, the system evolves\nthe rare fluctuation as a typical one, affording a variational estimate of its\nlikelihood in the original trajectory ensemble. Low variance gradients\nemploying value functions are proposed to increase the convergence of the\noptimal force. The method we develop employing these gradients leads to\nefficient and accurate estimates of both the optimal force and the likelihood\nof the rare event for a variety of model systems.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:00:15 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Das", "Avishek", ""], ["Rose", "Dominic C.", ""], ["Garrahan", "Juan P.", ""], ["Limmer", "David T.", ""]]}, {"id": "2105.04332", "submitter": "Hung Tran-The", "authors": "Hung Tran-The, Sunil Gupta, Santu Rana, Svetha Venkatesh", "title": "Bayesian Optimistic Optimisation with Exponentially Decaying Regret", "comments": "To appear at ICML 2021 (21 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian optimisation (BO) is a well-known efficient algorithm for finding\nthe global optimum of expensive, black-box functions. The current practical BO\nalgorithms have regret bounds ranging from $\\mathcal{O}(\\frac{logN}{\\sqrt{N}})$\nto $\\mathcal O(e^{-\\sqrt{N}})$, where $N$ is the number of evaluations. This\npaper explores the possibility of improving the regret bound in the noiseless\nsetting by intertwining concepts from BO and tree-based optimistic optimisation\nwhich are based on partitioning the search space. We propose the BOO algorithm,\na first practical approach which can achieve an exponential regret bound with\norder $\\mathcal O(N^{-\\sqrt{N}})$ under the assumption that the objective\nfunction is sampled from a Gaussian process with a Mat\\'ern kernel with\nsmoothness parameter $\\nu > 4 +\\frac{D}{2}$, where $D$ is the number of\ndimensions. We perform experiments on optimisation of various synthetic\nfunctions and machine learning hyperparameter tuning tasks and show that our\nalgorithm outperforms baselines.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:07:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tran-The", "Hung", ""], ["Gupta", "Sunil", ""], ["Rana", "Santu", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2105.04349", "submitter": "Neel Dey", "authors": "Neel Dey, Mengwei Ren, Adrian V. Dalca, Guido Gerig", "title": "Generative Adversarial Registration for Improved Conditional Deformable\n  Templates", "comments": "24 pages, 15 figures. Code is available at\n  https://github.com/neel-dey/Atlas-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable templates are essential to large-scale medical image registration,\nsegmentation, and population analysis. Current conventional and deep\nnetwork-based methods for template construction use only regularized\nregistration objectives and often yield templates with blurry and/or\nanatomically implausible appearance, confounding downstream biomedical\ninterpretation. We reformulate deformable registration and conditional template\nestimation as an adversarial game wherein we encourage realism in the moved\ntemplates with a generative adversarial registration framework conditioned on\nflexible image covariates. The resulting templates exhibit significant gain in\nspecificity to attributes such as age and disease, better fit underlying\ngroup-wise spatiotemporal trends, and achieve improved sharpness and\ncentrality. These improvements enable more accurate population modeling with\ndiverse covariates for standardized downstream analyses and easier anatomical\ndelineation for structures of interest.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:06:41 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Dey", "Neel", ""], ["Ren", "Mengwei", ""], ["Dalca", "Adrian V.", ""], ["Gerig", "Guido", ""]]}, {"id": "2105.04350", "submitter": "Liangzhen Zheng", "authors": "Liangzhen Zheng, Haidong Lan, Tao Shen, Jiaxiang Wu, Sheng Wang, Wei\n  Liu, Junzhou Huang", "title": "tFold-TR: Combining Deep Learning Enhanced Hybrid Potential Energy for\n  Template-Based Modeling Structure Refinement", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Protein structure prediction has been a grand challenge for over 50 years,\nowing to its broad scientific and application interests. There are two primary\ntypes of modeling algorithms, template-free modeling and template-based\nmodeling. The latter one is suitable for easy prediction tasks and is widely\nadopted in computer-aided drug discoveries for drug design and screening.\nAlthough it has been several decades since its first edition, the current\ntemplate-based modeling approach suffers from two critical problems: 1) there\nare many missing regions in the template-query sequence alignment, and 2) the\naccuracy of the distance pairs from different regions of the template varies,\nand this information is not well introduced into the modeling. To solve these\ntwo problems, we propose a structural optimization process based on template\nmodeling, introducing two neural network models to predict the distance\ninformation of the missing regions and the accuracy of the distance pairs of\ndifferent regions in the template modeling structure. The predicted distances\nand residue pairwise-specific deviations are incorporated into the potential\nenergy function for structural optimization, which significantly improves the\nqualities of the original template modeling decoys.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:32:12 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 11:36:31 GMT"}, {"version": "v3", "created": "Sat, 22 May 2021 15:26:43 GMT"}, {"version": "v4", "created": "Sun, 30 May 2021 08:53:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zheng", "Liangzhen", ""], ["Lan", "Haidong", ""], ["Shen", "Tao", ""], ["Wu", "Jiaxiang", ""], ["Wang", "Sheng", ""], ["Liu", "Wei", ""], ["Huang", "Junzhou", ""]]}, {"id": "2105.04356", "submitter": "Hazrat Ali", "authors": "Muhammad Shakaib Iqbal, Hazrat Ali, Son N. Tran, Talha Iqbal", "title": "Coconut trees detection and segmentation in aerial imagery using mask\n  region-based convolution neural network", "comments": "Published in IET Computer Vision, 09 April 2021", "journal-ref": null, "doi": "10.1049/cvi2.12028", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Food resources face severe damages under extraordinary situations of\ncatastrophes such as earthquakes, cyclones, and tsunamis. Under such scenarios,\nspeedy assessment of food resources from agricultural land is critical as it\nsupports aid activity in the disaster hit areas. In this article, a deep\nlearning approach is presented for the detection and segmentation of coconut\ntress in aerial imagery provided through the AI competition organized by the\nWorld Bank in collaboration with OpenAerialMap and WeRobotics. Maked\nRegion-based Convolutional Neural Network approach was used identification and\nsegmentation of coconut trees. For the segmentation task, Mask R-CNN model with\nResNet50 and ResNet1010 based architectures was used. Several experiments with\ndifferent configuration parameters were performed and the best configuration\nfor the detection of coconut trees with more than 90% confidence factor was\nreported. For the purpose of evaluation, Microsoft COCO dataset evaluation\nmetric namely mean average precision (mAP) was used. An overall 91% mean\naverage precision for coconut trees detection was achieved.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:42:19 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Iqbal", "Muhammad Shakaib", ""], ["Ali", "Hazrat", ""], ["Tran", "Son N.", ""], ["Iqbal", "Talha", ""]]}, {"id": "2105.04373", "submitter": "Jinhang Zuo", "authors": "Jinhang Zuo, Carlee Joe-Wong", "title": "Combinatorial Multi-armed Bandits for Resource Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sequential resource allocation problem where a decision maker\nrepeatedly allocates budgets between resources. Motivating examples include\nallocating limited computing time or wireless spectrum bands to multiple users\n(i.e., resources). At each timestep, the decision maker should distribute its\navailable budgets among different resources to maximize the expected reward, or\nequivalently to minimize the cumulative regret. In doing so, the decision maker\nshould learn the value of the resources allocated for each user from feedback\non each user's received reward. For example, users may send messages of\ndifferent urgency over wireless spectrum bands; the reward generated by\nallocating spectrum to a user then depends on the message's urgency. We assume\neach user's reward follows a random process that is initially unknown. We\ndesign combinatorial multi-armed bandit algorithms to solve this problem with\ndiscrete or continuous budgets. We prove the proposed algorithms achieve\nlogarithmic regrets under semi-bandit feedback.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:55:30 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zuo", "Jinhang", ""], ["Joe-Wong", "Carlee", ""]]}, {"id": "2105.04379", "submitter": "Steven Kleinegesse", "authors": "Steven Kleinegesse and Michael U. Gutmann", "title": "Gradient-based Bayesian Experimental Design for Implicit Models using\n  Mutual Information Lower Bounds", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a framework for Bayesian experimental design (BED) with implicit\nmodels, where the data-generating distribution is intractable but sampling from\nit is still possible. In order to find optimal experimental designs for such\nmodels, our approach maximises mutual information lower bounds that are\nparametrised by neural networks. By training a neural network on sampled data,\nwe simultaneously update network parameters and designs using stochastic\ngradient-ascent. The framework enables experimental design with a variety of\nprominent lower bounds and can be applied to a wide range of scientific tasks,\nsuch as parameter estimation, model discrimination and improving future\npredictions. Using a set of intractable toy models, we provide a comprehensive\nempirical comparison of prominent lower bounds applied to the aforementioned\ntasks. We further validate our framework on a challenging system of stochastic\ndifferential equations from epidemiology.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:59:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kleinegesse", "Steven", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "2105.04402", "submitter": "Yihao Luo", "authors": "Yihao Luo and Ailing Yang and Fupeng Sun and Huafei Sun", "title": "AWCD: An Efficient Point Cloud Processing Approach via Wasserstein\n  Curvature", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the adaptive Wasserstein curvature denoising\n(AWCD), an original processing approach for point cloud data. By collecting\ncurvatures information from Wasserstein distance, AWCD consider more precise\nstructures of data and preserves stability and effectiveness even for data with\nnoise in high density. This paper contains some theoretical analysis about the\nWasserstein curvature and the complete algorithm of AWCD. In addition, we\ndesign digital experiments to show the denoising effect of AWCD. According to\ncomparison results, we present the advantages of AWCD against traditional\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:33:05 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 08:04:21 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Luo", "Yihao", ""], ["Yang", "Ailing", ""], ["Sun", "Fupeng", ""], ["Sun", "Huafei", ""]]}, {"id": "2105.04404", "submitter": "Theo Lacombe", "authors": "Th\\'eo Lacombe (DATASHAPE), Yuichi Ike, Mathieu Carriere, Fr\\'ed\\'eric\n  Chazal, Marc Glisse, Yuhei Umeda", "title": "Topological Uncertainty: Monitoring trained neural networks through\n  persistence of activation graphs", "comments": null, "journal-ref": "2021 International Joint Conference on Artificial Intelligence,\n  Aug 2021, Montr{\\'e}al, Canada", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural networks are capable of reaching astonishing performances on\na wide variety of contexts, properly training networks on complicated tasks\nrequires expertise and can be expensive from a computational perspective. In\nindustrial applications, data coming from an open-world setting might widely\ndiffer from the benchmark datasets on which a network was trained. Being able\nto monitor the presence of such variations without retraining the network is of\ncrucial importance. In this article, we develop a method to monitor trained\nneural networks based on the topological properties of their activation graphs.\nTo each new observation, we assign a Topological Uncertainty, a score that aims\nto assess the reliability of the predictions by investigating the whole network\ninstead of its final layer only, as typically done by practitioners. Our\napproach entirely works at a post-training level and does not require any\nassumption on the network architecture, optimization scheme, nor the use of\ndata augmentation or auxiliary datasets; and can be faithfully applied on a\nlarge range of network architectures and data types. We showcase experimentally\nthe potential of Topological Uncertainty in the context of trained network\nselection, Out-Of-Distribution detection, and shift-detection, both on\nsynthetic and real datasets of images and graphs.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:16:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lacombe", "Th\u00e9o", "", "DATASHAPE"], ["Ike", "Yuichi", ""], ["Carriere", "Mathieu", ""], ["Chazal", "Fr\u00e9d\u00e9ric", ""], ["Glisse", "Marc", ""], ["Umeda", "Yuhei", ""]]}, {"id": "2105.04405", "submitter": "Mohammad Ali Alomrani", "authors": "Mohammad Ali Alomrani", "title": "A Critical Review of Information Bottleneck Theory and its Applications\n  to Deep Learning", "comments": "Experimental error in section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past decade, deep neural networks have seen unparalleled improvements\nthat continue to impact every aspect of today's society. With the development\nof high performance GPUs and the availability of vast amounts of data, learning\ncapabilities of ML systems have skyrocketed, going from classifying digits in a\npicture to beating world-champions in games with super-human performance.\nHowever, even as ML models continue to achieve new frontiers, their practical\nsuccess has been hindered by the lack of a deep theoretical understanding of\ntheir inner workings. Fortunately, a known information-theoretic method called\nthe information bottleneck theory has emerged as a promising approach to better\nunderstand the learning dynamics of neural networks. In principle, IB theory\nmodels learning as a trade-off between the compression of the data and the\nretainment of information. The goal of this survey is to provide a\ncomprehensive review of IB theory covering it's information theoretic roots and\nthe recently proposed applications to understand deep learning models.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:16:38 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 11:50:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Alomrani", "Mohammad Ali", ""]]}, {"id": "2105.04414", "submitter": "Arash Shaban-Nejad", "authors": "Khalid Alghatani, Nariman Ammar, Abdelmounaam Rezgui, Arash\n  Shaban-Nejad", "title": "Predicting Intensive Care Unit Length of Stay and Mortality Using\n  Patient Vital Signs: Machine Learning Model Development and Validation", "comments": "23 Pages, 11 Figures, 13 Tables", "journal-ref": "JMIR Med Inform 2021;9(5):e21347", "doi": "10.2196/21347", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patient monitoring is vital in all stages of care. We here report the\ndevelopment and validation of ICU length of stay and mortality prediction\nmodels. The models will be used in an intelligent ICU patient monitoring module\nof an Intelligent Remote Patient Monitoring (IRPM) framework that monitors the\nhealth status of patients, and generates timely alerts, maneuver guidance, or\nreports when adverse medical conditions are predicted. We utilized the publicly\navailable Medical Information Mart for Intensive Care (MIMIC) database to\nextract ICU stay data for adult patients to build two prediction models: one\nfor mortality prediction and another for ICU length of stay. For the mortality\nmodel, we applied six commonly used machine learning (ML) binary classification\nalgorithms for predicting the discharge status (survived or not). For the\nlength of stay model, we applied the same six ML algorithms for binary\nclassification using the median patient population ICU stay of 2.64 days. For\nthe regression-based classification, we used two ML algorithms for predicting\nthe number of days. We built two variations of each prediction model: one using\n12 baseline demographic and vital sign features, and the other based on our\nproposed quantiles approach, in which we use 21 extra features engineered from\nthe baseline vital sign features, including their modified means, standard\ndeviations, and quantile percentages. We could perform predictive modeling with\nminimal features while maintaining reasonable performance using the quantiles\napproach. The best accuracy achieved in the mortality model was approximately\n89% using the random forest algorithm. The highest accuracy achieved in the\nlength of stay model, based on the population median ICU stay (2.64 days), was\napproximately 65% using the random forest algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:45:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Alghatani", "Khalid", ""], ["Ammar", "Nariman", ""], ["Rezgui", "Abdelmounaam", ""], ["Shaban-Nejad", "Arash", ""]]}, {"id": "2105.04425", "submitter": "Haomin Yu", "authors": "Haomin Yu and Yangli-ao Geng and Yingjun Zhang and Qingyong Li and\n  Jiayu Zhou", "title": "MTNet: A Multi-Task Neural Network for On-Field Calibration of Low-Cost\n  Air Monitoring Sensors", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances of sensor technology enable people to monitor air quality\nthrough widely distributed low-cost sensors. However, measurements from these\nsensors usually encounter high biases and require a calibration step to reach\nan acceptable performance in down-streaming analytical tasks. Most existing\ncalibration methods calibrate one type of sensor at a time, which we call\nsingle-task calibration. Despite the popularity of this single-task schema, it\nmay neglect interactions among calibration tasks of different sensors, which\nencompass underlying information to promote calibration performance. In this\npaper, we propose a multi-task calibration network (MTNet) to calibrate\nmultiple sensors (e.g., carbon monoxide and nitrogen oxide sensors)\nsimultaneously, modeling the interactions among tasks. MTNet consists of a\nsingle shared module, and several task-specific modules. Specifically, in the\nshared module, we extend the multi-gate mixture-of-experts structure to\nharmonize the task conflicts and correlations among different tasks; in each\ntask-specific module, we introduce a feature selection strategy to customize\nthe input for the specific task. These improvements allow MTNet to learn\ninteraction information shared across different tasks, and task-specific\ninformation for each calibration task as well. We evaluate MTNet on three\nreal-world datasets and compare it with several established baselines. The\nexperimental results demonstrate that MTNet achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:36:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yu", "Haomin", ""], ["Geng", "Yangli-ao", ""], ["Zhang", "Yingjun", ""], ["Li", "Qingyong", ""], ["Zhou", "Jiayu", ""]]}, {"id": "2105.04430", "submitter": "Wadii Boulila Prof.", "authors": "Safa Ben Atitallah, Maha Driss, Wadii Boulila, Anis Koubaa, Nesrine\n  Atitallah, Henda Ben Gh\\'ezala", "title": "An Enhanced Randomly Initialized Convolutional Neural Network for\n  Columnar Cactus Recognition in Unmanned Aerial Vehicle Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Convolutional Neural Networks (CNNs) have made a great performance\nfor remote sensing image classification. Plant recognition using CNNs is one of\nthe active deep learning research topics due to its added-value in different\nrelated fields, especially environmental conservation and natural areas\npreservation. Automatic recognition of plants in protected areas helps in the\nsurveillance process of these zones and ensures the sustainability of their\necosystems. In this work, we propose an Enhanced Randomly Initialized\nConvolutional Neural Network (ERI-CNN) for the recognition of columnar cactus,\nwhich is an endemic plant that exists in the Tehuac\\'an-Cuicatl\\'an Valley in\nsoutheastern Mexico. We used a public dataset created by a group of researchers\nthat consists of more than 20000 remote sensing images. The experimental\nresults confirm the effectiveness of the proposed model compared to other\nmodels reported in the literature like InceptionV3 and the modified LeNet-5\nCNN. Our ERI-CNN provides 98% of accuracy, 97% of precision, 97% of recall,\n97.5% as f1-score, and 0.056 loss.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:41:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Atitallah", "Safa Ben", ""], ["Driss", "Maha", ""], ["Boulila", "Wadii", ""], ["Koubaa", "Anis", ""], ["Atitallah", "Nesrine", ""], ["Gh\u00e9zala", "Henda Ben", ""]]}, {"id": "2105.04444", "submitter": "Yujun Shi", "authors": "Yujun Shi, Li Yuan, Yunpeng Chen, Jiashi Feng", "title": "Continual Learning via Bit-Level Information Preserving", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning tackles the setting of learning different tasks\nsequentially. Despite the lots of previous solutions, most of them still suffer\nsignificant forgetting or expensive memory cost. In this work, targeted at\nthese problems, we first study the continual learning process through the lens\nof information theory and observe that forgetting of a model stems from the\nloss of \\emph{information gain} on its parameters from the previous tasks when\nlearning a new task. From this viewpoint, we then propose a novel continual\nlearning approach called Bit-Level Information Preserving (BLIP) that preserves\nthe information gain on model parameters through updating the parameters at the\nbit level, which can be conveniently implemented with parameter quantization.\nMore specifically, BLIP first trains a neural network with weight quantization\non the new incoming task and then estimates information gain on each parameter\nprovided by the task data to determine the bits to be frozen to prevent\nforgetting. We conduct extensive experiments ranging from classification tasks\nto reinforcement learning tasks, and the results show that our method produces\nbetter or on par results comparing to previous state-of-the-arts. Indeed, BLIP\nachieves close to zero forgetting while only requiring constant memory\noverheads throughout continual learning.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:09:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Shi", "Yujun", ""], ["Yuan", "Li", ""], ["Chen", "Yunpeng", ""], ["Feng", "Jiashi", ""]]}, {"id": "2105.04448", "submitter": "Benjamin Nachman", "authors": "Anders Andreassen, Patrick T. Komiske, Eric M. Metodiev, Benjamin\n  Nachman, Adi Suresh, and Jesse Thaler", "title": "Scaffolding Simulations with Deep Learning for High-dimensional\n  Deconvolution", "comments": "6 pages, 1 figure, 1 table", "journal-ref": "ICLR simDL workshop 2021 (https://simdl.github.io/files/12.pdf)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG hep-ex hep-ph physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common setting for scientific inference is the ability to sample from a\nhigh-fidelity forward model (simulation) without having an explicit probability\ndensity of the data. We propose a simulation-based maximum likelihood\ndeconvolution approach in this setting called OmniFold. Deep learning enables\nthis approach to be naturally unbinned and (variable-, and) high-dimensional.\nIn contrast to model parameter estimation, the goal of deconvolution is to\nremove detector distortions in order to enable a variety of down-stream\ninference tasks. Our approach is the deep learning generalization of the common\nRichardson-Lucy approach that is also called Iterative Bayesian Unfolding in\nparticle physics. We show how OmniFold can not only remove detector\ndistortions, but it can also account for noise processes and acceptance\neffects.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:16:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Andreassen", "Anders", ""], ["Komiske", "Patrick T.", ""], ["Metodiev", "Eric M.", ""], ["Nachman", "Benjamin", ""], ["Suresh", "Adi", ""], ["Thaler", "Jesse", ""]]}, {"id": "2105.04458", "submitter": "Shakti Kumar", "authors": "Shakti Kumar, Jithin Pradeep, Hussain Zaidi", "title": "Learning Robust Latent Representations for Controllable Speech Synthesis", "comments": "Accepted in ACL2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  State-of-the-art Variational Auto-Encoders (VAEs) for learning disentangled\nlatent representations give impressive results in discovering features like\npitch, pause duration, and accent in speech data, leading to highly\ncontrollable text-to-speech (TTS) synthesis. However, these LSTM-based VAEs\nfail to learn latent clusters of speaker attributes when trained on either\nlimited or noisy datasets. Further, different latent variables start encoding\nthe same features, limiting the control and expressiveness during speech\nsynthesis. To resolve these issues, we propose RTI-VAE (Reordered Transformer\nwith Information reduction VAE) where we minimize the mutual information\nbetween different latent variables and devise a modified Transformer\narchitecture with layer reordering to learn controllable latent representations\nin speech data. We show that RTI-VAE reduces the cluster overlap of speaker\nattributes by at least 30\\% over LSTM-VAE and by at least 7\\% over vanilla\nTransformer-VAE.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:49:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kumar", "Shakti", ""], ["Pradeep", "Jithin", ""], ["Zaidi", "Hussain", ""]]}, {"id": "2105.04471", "submitter": "Bertrand Charpentier", "authors": "Bertrand Charpentier, Oliver Borchert, Daniel Z\\\"ugner, Simon Geisler,\n  Stephan G\\\"unnemann", "title": "Natural Posterior Network: Deep Bayesian Predictive Uncertainty for\n  Exponential Family Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty awareness is crucial to develop reliable machine learning models.\nIn this work, we propose the Natural Posterior Network (NatPN) for fast and\nhigh-quality uncertainty estimation for any task where the target distribution\nbelongs to the exponential family. Thus, NatPN finds application for both\nclassification and general regression settings. Unlike many previous\napproaches, NatPN does not require out-of-distribution (OOD) data at training\ntime. Instead, it leverages Normalizing Flows to fit a single density on a\nlearned low-dimensional and task-dependent latent space. For any input sample,\nNatPN uses the predicted likelihood to perform a Bayesian update over the\ntarget distribution. Theoretically, NatPN assigns high uncertainty far away\nfrom training data. Empirically, our extensive experiments on calibration and\nOOD detection show that NatPN delivers highly competitive performance for\nclassification, regression and count prediction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:10:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Charpentier", "Bertrand", ""], ["Borchert", "Oliver", ""], ["Z\u00fcgner", "Daniel", ""], ["Geisler", "Simon", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2105.04488", "submitter": "Petros Giannakopoulos", "authors": "Petros Giannakopoulos, Aggelos Pikrakis, Yannis Cotronis", "title": "A Deep Reinforcement Learning Approach to Audio-Based Navigation in a\n  Multi-Speaker Environment", "comments": "To be published in ICASSP 2021", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9415013", "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we use deep reinforcement learning to create an autonomous agent\nthat can navigate in a two-dimensional space using only raw auditory sensory\ninformation from the environment, a problem that has received very little\nattention in the reinforcement learning literature. Our experiments show that\nthe agent can successfully identify a particular target speaker among a set of\n$N$ predefined speakers in a room and move itself towards that speaker, while\navoiding collision with other speakers or going outside the room boundaries.\nThe agent is shown to be robust to speaker pitch shifting and it can learn to\nnavigate the environment, even when a limited number of training utterances are\navailable for each speaker.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:26:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Giannakopoulos", "Petros", ""], ["Pikrakis", "Aggelos", ""], ["Cotronis", "Yannis", ""]]}, {"id": "2105.04489", "submitter": "SouYoung Jin", "authors": "Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio\n  Feris, James Glass, Aude Oliva", "title": "Spoken Moments: Learning Joint Audio-Visual Representations from Video\n  Descriptions", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people observe events, they are able to abstract key information and\nbuild concise summaries of what is happening. These summaries include\ncontextual and semantic information describing the important high-level details\n(what, where, who and how) of the observed event and exclude background\ninformation that is deemed unimportant to the observer. With this in mind, the\ndescriptions people generate for videos of different dynamic events can greatly\nimprove our understanding of the key information of interest in each video.\nThese descriptions can be captured in captions that provide expanded attributes\nfor video labeling (e.g. actions/objects/scenes/sentiment/etc.) while allowing\nus to gain new insight into what people find important or necessary to\nsummarize specific events. Existing caption datasets for video understanding\nare either small in scale or restricted to a specific domain. To address this,\nwe present the Spoken Moments (S-MiT) dataset of 500k spoken captions each\nattributed to a unique short video depicting a broad range of different events.\nWe collect our descriptions using audio recordings to ensure that they remain\nas natural and concise as possible while allowing us to scale the size of a\nlarge classification dataset. In order to utilize our proposed dataset, we\npresent a novel Adaptive Mean Margin (AMM) approach to contrastive learning and\nevaluate our models on video/caption retrieval on multiple datasets. We show\nthat our AMM approach consistently improves our results and that models trained\non our Spoken Moments dataset generalize better than those trained on other\nvideo-caption datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:30:46 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Monfort", "Mathew", ""], ["Jin", "SouYoung", ""], ["Liu", "Alexander", ""], ["Harwath", "David", ""], ["Feris", "Rogerio", ""], ["Glass", "James", ""], ["Oliva", "Aude", ""]]}, {"id": "2105.04493", "submitter": "Wei Jin", "authors": "Wei Jin, Xiaorui Liu, Yao Ma, Tyler Derr, Charu Aggarwal, Jiliang Tang", "title": "Graph Feature Gating Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have received tremendous attention due to their\npower in learning effective representations for graphs. Most GNNs follow a\nmessage-passing scheme where the node representations are updated by\naggregating and transforming the information from the neighborhood. Meanwhile,\nthey adopt the same strategy in aggregating the information from different\nfeature dimensions. However, suggested by social dimension theory and spectral\nembedding, there are potential benefits to treat the dimensions differently\nduring the aggregation process. In this work, we investigate to enable\nheterogeneous contributions of feature dimensions in GNNs. In particular, we\npropose a general graph feature gating network (GFGN) based on the graph signal\ndenoising problem and then correspondingly introduce three graph filters under\nGFGN to allow different levels of contributions from feature dimensions.\nExtensive experiments on various real-world datasets demonstrate the\neffectiveness and robustness of the proposed frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:33:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Jin", "Wei", ""], ["Liu", "Xiaorui", ""], ["Ma", "Yao", ""], ["Derr", "Tyler", ""], ["Aggarwal", "Charu", ""], ["Tang", "Jiliang", ""]]}, {"id": "2105.04504", "submitter": "Vincent Dutordoir", "authors": "Vincent Dutordoir, James Hensman, Mark van der Wilk, Carl Henrik Ek,\n  Zoubin Ghahramani, Nicolas Durrande", "title": "Deep Neural Networks as Point Estimates for Deep Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes (DGPs) have struggled for relevance in applications\ndue to the challenges and cost associated with Bayesian inference. In this\npaper we propose a sparse variational approximation for DGPs for which the\napproximate posterior mean has the same mathematical structure as a Deep Neural\nNetwork (DNN). We make the forward pass through a DGP equivalent to a ReLU DNN\nby finding an interdomain transformation that represents the GP posterior mean\nas a sum of ReLU basis functions. This unification enables the initialisation\nand training of the DGP as a neural network, leveraging the well established\npractice in the deep learning community, and so greatly aiding the inference\ntask. The experiments demonstrate improved accuracy and faster training\ncompared to current DGP methods, while retaining favourable predictive\nuncertainties.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:55:17 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Dutordoir", "Vincent", ""], ["Hensman", "James", ""], ["van der Wilk", "Mark", ""], ["Ek", "Carl Henrik", ""], ["Ghahramani", "Zoubin", ""], ["Durrande", "Nicolas", ""]]}, {"id": "2105.04505", "submitter": "Maximilian Idahl", "authors": "Maximilian Idahl, Lijun Lyu, Ujwal Gadiraju, Avishek Anand", "title": "Towards Benchmarking the Utility of Explanations for Model Debugging", "comments": "Short paper, to appear at TrustNLP @ NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-hoc explanation methods are an important class of approaches that help\nunderstand the rationale underlying a trained model's decision. But how useful\nare they for an end-user towards accomplishing a given task? In this vision\npaper, we argue the need for a benchmark to facilitate evaluations of the\nutility of post-hoc explanation methods. As a first step to this end, we\nenumerate desirable properties that such a benchmark should possess for the\ntask of debugging text classifiers. Additionally, we highlight that such a\nbenchmark facilitates not only assessing the effectiveness of explanations but\nalso their efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:57:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Idahl", "Maximilian", ""], ["Lyu", "Lijun", ""], ["Gadiraju", "Ujwal", ""], ["Anand", "Avishek", ""]]}, {"id": "2105.04522", "submitter": "Erik Englesson", "authors": "Erik Englesson, Hossein Azizpour", "title": "Generalized Jensen-Shannon Divergence Loss for Learning with Noisy\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior works have found it beneficial to combine provably noise-robust loss\nfunctions e.g., mean absolute error (MAE) with standard categorical loss\nfunction e.g. cross entropy (CE) to improve their learnability. Here, we\npropose to use Jensen-Shannon divergence as a noise-robust loss function and\nshow that it interestingly interpolate between CE and MAE with a controllable\nmixing parameter. Furthermore, we make a crucial observation that CE exhibit\nlower consistency around noisy data points. Based on this observation, we adopt\na generalized version of the Jensen-Shannon divergence for multiple\ndistributions to encourage consistency around data points. Using this loss\nfunction, we show state-of-the-art results on both synthetic (CIFAR), and\nreal-world (WebVision) noise with varying noise rates.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:19:38 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:07:03 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 08:57:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Englesson", "Erik", ""], ["Azizpour", "Hossein", ""]]}, {"id": "2105.04528", "submitter": "Hongkuan Zhou", "authors": "Hongkuan Zhou and Ajitesh Srivastava and Hanqing Zeng and Rajgopal\n  Kannan and Viktor Prasanna", "title": "Accelerating Large Scale Real-Time GNN Inference using Channel Pruning", "comments": null, "journal-ref": null, "doi": "10.14778/3461535.3461547", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are proven to be powerful models to generate\nnode embedding for downstream applications. However, due to the high\ncomputation complexity of GNN inference, it is hard to deploy GNNs for\nlarge-scale or real-time applications. In this paper, we propose to accelerate\nGNN inference by pruning the dimensions in each layer with negligible accuracy\nloss. Our pruning framework uses a novel LASSO regression formulation for GNNs\nto identify feature dimensions (channels) that have high influence on the\noutput activation. We identify two inference scenarios and design pruning\nschemes based on their computation and memory usage for each. To further reduce\nthe inference complexity, we effectively store and reuse hidden features of\nvisited nodes, which significantly reduces the number of supporting nodes\nneeded to compute the target embedding. We evaluate the proposed method with\nthe node classification problem on five popular datasets and a real-time spam\ndetection application. We demonstrate that the pruned GNN models greatly reduce\ncomputation and memory usage with little accuracy loss. For full inference, the\nproposed method achieves an average of 3.27x speedup with only 0.002 drop in\nF1-Micro on GPU. For batched inference, the proposed method achieves an average\nof 6.67x speedup with only 0.003 drop in F1-Micro on CPU. To the best of our\nknowledge, we are the first to accelerate large scale real-time GNN inference\nthrough channel pruning.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:28:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhou", "Hongkuan", ""], ["Srivastava", "Ajitesh", ""], ["Zeng", "Hanqing", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor", ""]]}, {"id": "2105.04529", "submitter": "G\\'abor R\\\"od\\\"onyi", "authors": "G. R\\\"od\\\"onyi, G. I. Beintema, R. T\\'oth, M. Schoukens, D. Pup, \\'A.\n  Kisari, Zs. V\\'igh, P. K\\H{o}r\\\"os, A. Soumelidis and J. Bokor", "title": "Identification of the nonlinear steering dynamics of an autonomous\n  vehicle", "comments": "Accepted to SYSID 2021 (revised with reviewer feedback)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated driving applications require accurate vehicle specific models to\nprecisely predict and control the motion dynamics. However, modern vehicles\nhave a wide array of digital and mechatronic components that are difficult to\nmodel, manufactures do not disclose all details required for modelling and even\nexisting models of subcomponents require coefficient estimation to match the\nspecific characteristics of each vehicle and their change over time. Hence, it\nis attractive to use data-driven modelling to capture the relevant vehicle\ndynamics and synthesise model-based control solutions. In this paper, we\naddress identification of the steering system of an autonomous car based on\nmeasured data. We show that the underlying dynamics are highly nonlinear and\nchallenging to be captured, necessitating the use of data-driven methods that\nfuse the approximation capabilities of learning and the efficiency of dynamic\nsystem identification. We demonstrate that such a neural network based\nsubspace-encoder method can successfully capture the underlying dynamics while\nother methods fall short to provide reliable results.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:32:23 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["R\u00f6d\u00f6nyi", "G.", ""], ["Beintema", "G. I.", ""], ["T\u00f3th", "R.", ""], ["Schoukens", "M.", ""], ["Pup", "D.", ""], ["Kisari", "\u00c1.", ""], ["V\u00edgh", "Zs.", ""], ["K\u0151r\u00f6s", "P.", ""], ["Soumelidis", "A.", ""], ["Bokor", "J.", ""]]}, {"id": "2105.04532", "submitter": "Omer Demirel", "authors": "Omer Burak Demirel, Burhaneddin Yaman, Logan Dowdle, Steen Moeller,\n  Luca Vizioli, Essa Yacoub, John Strupp, Cheryl A. Olman, K\\^amil U\\u{g}urbil\n  and Mehmet Ak\\c{c}akaya", "title": "Improved Simultaneous Multi-Slice Functional MRI Using Self-supervised\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional MRI (fMRI) is commonly used for interpreting neural activities\nacross the brain. Numerous accelerated fMRI techniques aim to provide improved\nspatiotemporal resolutions. Among these, simultaneous multi-slice (SMS) imaging\nhas emerged as a powerful strategy, becoming a part of large-scale studies,\nsuch as the Human Connectome Project. However, when SMS imaging is combined\nwith in-plane acceleration for higher acceleration rates, conventional SMS\nreconstruction methods may suffer from noise amplification and other artifacts.\nRecently, deep learning (DL) techniques have gained interest for improving MRI\nreconstruction. However, these methods are typically trained in a supervised\nmanner that necessitates fully-sampled reference data, which is not feasible in\nhighly-accelerated fMRI acquisitions. Self-supervised learning that does not\nrequire fully-sampled data has recently been proposed and has shown similar\nperformance to supervised learning. However, it has only been applied for\nin-plane acceleration. Furthermore the effect of DL reconstruction on\nsubsequent fMRI analysis remains unclear. In this work, we extend\nself-supervised DL reconstruction to SMS imaging. Our results on prospectively\n10-fold accelerated 7T fMRI data show that self-supervised DL reduces\nreconstruction noise and suppresses residual artifacts. Subsequent fMRI\nanalysis remains unaltered by DL processing, while the improved temporal\nsignal-to-noise ratio produces higher coherence estimates between task runs.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:36:27 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Demirel", "Omer Burak", ""], ["Yaman", "Burhaneddin", ""], ["Dowdle", "Logan", ""], ["Moeller", "Steen", ""], ["Vizioli", "Luca", ""], ["Yacoub", "Essa", ""], ["Strupp", "John", ""], ["Olman", "Cheryl A.", ""], ["U\u011furbil", "K\u00e2mil", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2105.04534", "submitter": "Yan Zhou", "authors": "Yan Zhou, Murat Kantarcioglu, Chris Clifton", "title": "Improving Fairness of AI Systems with Lossless De-biasing", "comments": "8 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's society, AI systems are increasingly used to make critical\ndecisions such as credit scoring and patient triage. However, great convenience\nbrought by AI systems comes with troubling prevalence of bias against\nunderrepresented groups. Mitigating bias in AI systems to increase overall\nfairness has emerged as an important challenge. Existing studies on mitigating\nbias in AI systems focus on eliminating sensitive demographic information\nembedded in data. Given the temporal and contextual complexity of\nconceptualizing fairness, lossy treatment of demographic information may\ncontribute to an unnecessary trade-off between accuracy and fairness,\nespecially when demographic attributes and class labels are correlated. In this\npaper, we present an information-lossless de-biasing technique that targets the\nscarcity of data in the disadvantaged group. Unlike the existing work, we\ndemonstrate, both theoretically and empirically, that oversampling\nunderrepresented groups can not only mitigate algorithmic bias in AI systems\nthat consistently predict a favorable outcome for a certain group, but improve\noverall accuracy by mitigating class imbalance within data that leads to a bias\ntowards the majority class. We demonstrate the effectiveness of our technique\non real datasets using a variety of fairness metrics.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:38:38 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhou", "Yan", ""], ["Kantarcioglu", "Murat", ""], ["Clifton", "Chris", ""]]}, {"id": "2105.04538", "submitter": "Yufan Zhou", "authors": "Yufan Zhou, Changyou Chen, Jinhui Xu", "title": "Learning High-Dimensional Distributions with Latent Neural Fokker-Planck\n  Kernels", "comments": "code will be updated at https://github.com/drboog/FPK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning high-dimensional distributions is an important yet challenging\nproblem in machine learning with applications in various domains. In this\npaper, we introduce new techniques to formulate the problem as solving\nFokker-Planck equation in a lower-dimensional latent space, aiming to mitigate\nchallenges in high-dimensional data space. Our proposed model consists of\nlatent-distribution morphing, a generator and a parameterized Fokker-Planck\nkernel function. One fascinating property of our model is that it can be\ntrained with arbitrary steps of latent distribution morphing or even without\nmorphing, which makes it flexible and as efficient as Generative Adversarial\nNetworks (GANs). Furthermore, this property also makes our latent-distribution\nmorphing an efficient plug-and-play scheme, thus can be used to improve\narbitrary GANs, and more interestingly, can effectively correct failure cases\nof the GAN models. Extensive experiments illustrate the advantages of our\nproposed method over existing models.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:42:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhou", "Yufan", ""], ["Chen", "Changyou", ""], ["Xu", "Jinhui", ""]]}, {"id": "2105.04544", "submitter": "Afsaneh Mastouri", "authors": "Afsaneh Mastouri, Yuchen Zhu, Limor Gultchin, Anna Korba, Ricardo\n  Silva, Matt J. Kusner, Arthur Gretton, Krikamol Muandet", "title": "Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment\n  Restriction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of causal effect estimation in the presence of\nunobserved confounding, but where proxies for the latent confounder(s) are\nobserved. We propose two kernel-based methods for nonlinear causal effect\nestimation in this setting: (a) a two-stage regression approach, and (b) a\nmaximum moment restriction approach. We focus on the proximal causal learning\nsetting, but our methods can be used to solve a wider class of inverse problems\ncharacterised by a Fredholm integral equation. In particular, we provide a\nunifying view of two-stage and moment restriction approaches for solving this\nproblem in a nonlinear setting. We provide consistency guarantees for each\nalgorithm, and we demonstrate these approaches achieve competitive results on\nsynthetic data and data simulating a real-world task. In particular, our\napproach outperforms earlier methods that are not suited to leveraging proxy\nvariables.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:52:48 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:29:17 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 22:09:36 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mastouri", "Afsaneh", ""], ["Zhu", "Yuchen", ""], ["Gultchin", "Limor", ""], ["Korba", "Anna", ""], ["Silva", "Ricardo", ""], ["Kusner", "Matt J.", ""], ["Gretton", "Arthur", ""], ["Muandet", "Krikamol", ""]]}, {"id": "2105.04547", "submitter": "Chengdong Yao", "authors": "Chengdong Yao", "title": "Highly Efficient Memory Failure Prediction using Mcelog-based Data\n  Mining and Machine Learning", "comments": "11 pages, 2 figures, 1 table. Codes has been open source to\n  https://www.github.com/ycd2016/acaioc2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.PF cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the data center, unexpected downtime caused by memory failures can lead to\na decline in the stability of the server and even the entire information\ntechnology infrastructure, which harms the business. Therefore, whether the\nmemory failure can be accurately predicted in advance has become one of the\nmost important issues to be studied in the data center. However, for the memory\nfailure prediction in the production system, it is necessary to solve technical\nproblems such as huge data noise and extreme imbalance between positive and\nnegative samples, and at the same time ensure the long-term stability of the\nalgorithm. This paper compares and summarizes some commonly used skills and the\nimprovement they can bring. The single model we proposed won the top 14th in\nthe 2nd Alibaba Cloud AIOps Competition belonging to the 25th PAKDD conference.\nIt takes only 30 minutes to pass the online test, while most of the other\ncontestants' solution need more than 3 hours. Codes has been open source to\nhttps://www.github.com/ycd2016/acaioc2.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 11:38:05 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 05:38:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yao", "Chengdong", ""]]}, {"id": "2105.04550", "submitter": "Keyulu Xu", "authors": "Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, Kenji Kawaguchi", "title": "Optimization of Graph Neural Networks: Implicit Acceleration by Skip\n  Connections and More Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have been studied through the lens of expressive\npower and generalization. However, their optimization properties are less well\nunderstood. We take the first step towards analyzing GNN training by studying\nthe gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that\ndespite the non-convexity of training, convergence to a global minimum at a\nlinear rate is guaranteed under mild assumptions that we validate on real-world\ngraphs. Second, we study what may affect the GNNs' training speed. Our results\nshow that the training of GNNs is implicitly accelerated by skip connections,\nmore depth, and/or a good label distribution. Empirical results confirm that\nour theoretical results for linearized GNNs align with the training behavior of\nnonlinear GNNs. Our results provide the first theoretical support for the\nsuccess of GNNs with skip connections in terms of optimization, and suggest\nthat deep GNNs with skip connections would be promising in practice.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:59:01 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 05:55:42 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Xu", "Keyulu", ""], ["Zhang", "Mozhi", ""], ["Jegelka", "Stefanie", ""], ["Kawaguchi", "Kenji", ""]]}, {"id": "2105.04555", "submitter": "Jaehoon Koo", "authors": "Jaehoon Koo, Prasanna Balaprakash, Michael Kruse, Xingfu Wu, Paul\n  Hovland, Mary Hall", "title": "Customized Monte Carlo Tree Search for LLVM/Polly's Composable Loop\n  Optimization Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Polly is the LLVM project's polyhedral loop nest optimizer. Recently,\nuser-directed loop transformation pragmas were proposed based on LLVM/Clang and\nPolly. The search space exposed by the transformation pragmas is a tree,\nwherein each node represents a specific combination of loop transformations\nthat can be applied to the code resulting from the parent node's loop\ntransformations. We have developed a search algorithm based on Monte Carlo tree\nsearch (MCTS) to find the best combination of loop transformations. Our\nalgorithm consists of two phases: exploring loop transformations at different\ndepths of the tree to identify promising regions in the tree search space and\nexploiting those regions by performing a local search. Moreover, a restart\nmechanism is used to avoid the MCTS getting trapped in a local solution. The\nbest and worst solutions are transferred from the previous phases of the\nrestarts to leverage the search history. We compare our approach with random,\ngreedy, and breadth-first search methods on PolyBench kernels and ECP proxy\napplications. Experimental results show that our MCTS algorithm finds pragma\ncombinations with a speedup of 2.3x over Polly's heuristic optimizations on\naverage.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:57:39 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Koo", "Jaehoon", ""], ["Balaprakash", "Prasanna", ""], ["Kruse", "Michael", ""], ["Wu", "Xingfu", ""], ["Hovland", "Paul", ""], ["Hall", "Mary", ""]]}, {"id": "2105.04580", "submitter": "Saksham Suri", "authors": "Sharath Girish, Saksham Suri, Saketh Rambhatla, Abhinav Shrivastava", "title": "Towards Discovery and Attribution of Open-world GAN Generated Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent progress in Generative Adversarial Networks (GANs), it is\nimperative for media and visual forensics to develop detectors which can\nidentify and attribute images to the model generating them. Existing works have\nshown to attribute images to their corresponding GAN sources with high\naccuracy. However, these works are limited to a closed set scenario, failing to\ngeneralize to GANs unseen during train time and are therefore, not scalable\nwith a steady influx of new GANs. We present an iterative algorithm for\ndiscovering images generated from previously unseen GANs by exploiting the fact\nthat all GANs leave distinct fingerprints on their generated images. Our\nalgorithm consists of multiple components including network training,\nout-of-distribution detection, clustering, merge and refine steps. Through\nextensive experiments, we show that our algorithm discovers unseen GANs with\nhigh accuracy and also generalizes to GANs trained on unseen real datasets. We\nadditionally apply our algorithm to attribution and discovery of GANs in an\nonline fashion as well as to the more standard task of real/fake detection. Our\nexperiments demonstrate the effectiveness of our approach to discover new GANs\nand can be used in an open-world setup.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:00:13 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Girish", "Sharath", ""], ["Suri", "Saksham", ""], ["Rambhatla", "Saketh", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2105.04607", "submitter": "Shadi Endrawis", "authors": "Shadi Endrawis, Gal Leibovich, Guy Jacob, Gal Novik and Aviv Tamar", "title": "Efficient Self-Supervised Data Collection for Offline Robot Learning", "comments": "Accepted in ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical approach to robot reinforcement learning is to first collect a\nlarge batch of real or simulated robot interaction data, using some data\ncollection policy, and then learn from this data to perform various tasks,\nusing offline learning algorithms. Previous work focused on manually designing\nthe data collection policy, and on tasks where suitable policies can easily be\ndesigned, such as random picking policies for collecting data about object\ngrasping. For more complex tasks, however, it may be difficult to find a data\ncollection policy that explores the environment effectively, and produces data\nthat is diverse enough for the downstream task. In this work, we propose that\ndata collection policies should actively explore the environment to collect\ndiverse data. In particular, we develop a simple-yet-effective goal-conditioned\nreinforcement-learning method that actively focuses data collection on novel\nobservations, thereby collecting a diverse data-set. We evaluate our method on\nsimulated robot manipulation tasks with visual inputs and show that the\nimproved diversity of active data collection leads to significant improvements\nin the downstream learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:42:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Endrawis", "Shadi", ""], ["Leibovich", "Gal", ""], ["Jacob", "Guy", ""], ["Novik", "Gal", ""], ["Tamar", "Aviv", ""]]}, {"id": "2105.04615", "submitter": "Mohit Kumar", "authors": "Mohit Kumar", "title": "Differentially Private Transfer Learning with Conditionally Deep\n  Autoencoders", "comments": "arXiv admin note: substantial text overlap with arXiv:2104.07060", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of differentially private semi-supervised\ntransfer learning. The notion of membership-mapping is developed using measure\ntheory basis to learn data representation via a fuzzy membership function. An\nalternative conception of deep autoencoder, referred to as Conditionally Deep\nMembership-Mapping Autoencoder (CDMMA) (that consists of a nested compositions\nof membership-mappings), is considered. Under practice-oriented settings, an\nanalytical solution for the learning of CDMFA can be derived by means of\nvariational optimization. The paper proposes a transfer learning approach that\ncombines CDMMA with a tailored noise adding mechanism to achieve a given level\nof privacy-loss bound with the minimum perturbation of the data. Numerous\nexperiments were carried out using MNIST, USPS, Office, and Caltech256 datasets\nto verify the competitive robust performance of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:52:44 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 05:02:54 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Kumar", "Mohit", ""]]}, {"id": "2105.04619", "submitter": "Stephan R Richter", "authors": "Stephan R. Richter and Hassan Abu AlHaija and Vladlen Koltun", "title": "Enhancing Photorealism Enhancement", "comments": "Code and data available at\n  https://github.com/intel-isl/PhotorealismEnhancement Video available at\n  https://youtu.be/P1IcaBn3ej0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to enhancing the realism of synthetic images. The\nimages are enhanced by a convolutional network that leverages intermediate\nrepresentations produced by conventional rendering pipelines. The network is\ntrained via a novel adversarial objective, which provides strong supervision at\nmultiple perceptual levels. We analyze scene layout distributions in commonly\nused datasets and find that they differ in important ways. We hypothesize that\nthis is one of the causes of strong artifacts that can be observed in the\nresults of many prior methods. To address this we propose a new strategy for\nsampling image patches during training. We also introduce multiple\narchitectural improvements in the deep network modules used for photorealism\nenhancement. We confirm the benefits of our contributions in controlled\nexperiments and report substantial gains in stability and realism in comparison\nto recent image-to-image translation methods and a variety of other baselines.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:00:49 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Richter", "Stephan R.", ""], ["AlHaija", "Hassan Abu", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2105.04637", "submitter": "Hafez Farazi", "authors": "Hafez Farazi, Jan Nogga, Sven Behnke", "title": "Local Frequency Domain Transformer Networks for Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction is commonly referred to as forecasting future frames of a\nvideo sequence provided several past frames thereof. It remains a challenging\ndomain as visual scenes evolve according to complex underlying dynamics, such\nas the camera's egocentric motion or the distinct motility per individual\nobject viewed. These are mostly hidden from the observer and manifest as often\nhighly non-linear transformations between consecutive video frames. Therefore,\nvideo prediction is of interest not only in anticipating visual changes in the\nreal world but has, above all, emerged as an unsupervised learning rule\ntargeting the formation and dynamics of the observed environment. Many of the\ndeep learning-based state-of-the-art models for video prediction utilize some\nform of recurrent layers like Long Short-Term Memory (LSTMs) or Gated Recurrent\nUnits (GRUs) at the core of their models. Although these models can predict the\nfuture frames, they rely entirely on these recurrent structures to\nsimultaneously perform three distinct tasks: extracting transformations,\nprojecting them into the future, and transforming the current frame. In order\nto completely interpret the formed internal representations, it is crucial to\ndisentangle these tasks. This paper proposes a fully differentiable building\nblock that can perform all of those tasks separately while maintaining\ninterpretability. We derive the relevant theoretical foundations and showcase\nresults on synthetic as well as real data. We demonstrate that our method is\nreadily extended to perform motion segmentation and account for the scene's\ncomposition, and learns to produce reliable predictions in an entirely\ninterpretable manner by only observing unlabeled video data.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:48:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Farazi", "Hafez", ""], ["Nogga", "Jan", ""], ["Behnke", "Sven", ""]]}, {"id": "2105.04646", "submitter": "Chengchun Shi", "authors": "Chengchun Shi and Runzhe Wan and Victor Chernozhukov and Rui Song", "title": "Deeply-Debiased Off-Policy Interval Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy evaluation learns a target policy's value with a historical\ndataset generated by a different behavior policy. In addition to a point\nestimate, many applications would benefit significantly from having a\nconfidence interval (CI) that quantifies the uncertainty of the point estimate.\nIn this paper, we propose a novel deeply-debiasing procedure to construct an\nefficient, robust, and flexible CI on a target policy's value. Our method is\njustified by theoretical results and numerical experiments. A Python\nimplementation of the proposed procedure is available at\nhttps://github.com/RunzheStat/D2OPE.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:00:08 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 20:14:34 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shi", "Chengchun", ""], ["Wan", "Runzhe", ""], ["Chernozhukov", "Victor", ""], ["Song", "Rui", ""]]}, {"id": "2105.04655", "submitter": "Wenhao Zhang", "authors": "Wenhao Zhang, Ramin Ramezani, Arash Naeim", "title": "Causal Inference in medicine and in health policy, a summary", "comments": "31 pages, 17 figures, to appear in the second edition of the handbook\n  of computational intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data science task can be deemed as making sense of the data or testing a\nhypothesis about it. The conclusions inferred from data can greatly guide us to\nmake informative decisions. Big data has enabled us to carry out countless\nprediction tasks in conjunction with machine learning, such as identifying high\nrisk patients suffering from a certain disease and taking preventable measures.\nHowever, healthcare practitioners are not content with mere predictions - they\nare also interested in the cause-effect relation between input features and\nclinical outcomes. Understanding such relations will help doctors treat\npatients and reduce the risk effectively. Causality is typically identified by\nrandomized controlled trials. Often such trials are not feasible when\nscientists and researchers turn to observational studies and attempt to draw\ninferences. However, observational studies may also be affected by selection\nand/or confounding biases that can result in wrong causal conclusions. In this\nchapter, we will try to highlight some of the drawbacks that may arise in\ntraditional machine learning and statistical approaches to analyze the\nobservational data, particularly in the healthcare data analytics domain. We\nwill discuss causal inference and ways to discover the cause-effect from\nobservational studies in healthcare domain. Moreover, we will demonstrate the\napplications of causal inference in tackling some common machine learning\nissues such as missing data and model transportability. Finally, we will\ndiscuss the possibility of integrating reinforcement learning with causality as\na way to counter confounding bias.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:25:56 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 23:35:04 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 18:28:50 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhang", "Wenhao", ""], ["Ramezani", "Ramin", ""], ["Naeim", "Arash", ""]]}, {"id": "2105.04656", "submitter": "Chirag Gupta", "authors": "Chirag Gupta, Aaditya K. Ramdas", "title": "Distribution-free calibration guarantees for histogram binning without\n  sample splitting", "comments": "Appears at ICML 2021\n  (http://proceedings.mlr.press/v139/gupta21b.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove calibration guarantees for the popular histogram binning (also\ncalled uniform-mass binning) method of Zadrozny and Elkan [2001]. Histogram\nbinning has displayed strong practical performance, but theoretical guarantees\nhave only been shown for sample split versions that avoid 'double dipping' the\ndata. We demonstrate that the statistical cost of sample splitting is\npractically significant on a credit default dataset. We then prove calibration\nguarantees for the original method that double dips the data, using a certain\nMarkov property of order statistics. Based on our results, we make practical\nrecommendations for choosing the number of bins in histogram binning. In our\nillustrative simulations, we propose a new tool for assessing calibration --\nvalidity plots -- which provide more information than an ECE estimate. Code for\nthis work will be made publicly available at\nhttps://github.com/aigen/df-posthoc-calibration.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:26:26 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 20:09:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Gupta", "Chirag", ""], ["Ramdas", "Aaditya K.", ""]]}, {"id": "2105.04663", "submitter": "Yuanzhong Xu", "authors": "Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping\n  Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello\n  Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu,\n  Zhifeng Chen", "title": "GSPMD: General and Scalable Parallelization for ML Computation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GSPMD, an automatic, compiler-based parallelization system for\ncommon machine learning computation graphs. It allows users to write programs\nin the same way as for a single device, then give hints through a few\nannotations on how to distribute tensors, based on which GSPMD will parallelize\nthe computation. Its representation of partitioning is simple yet general,\nallowing it to express different or mixed paradigms of parallelism on a wide\nvariety of models.\n  GSPMD infers the partitioning for every operator in the graph based on\nlimited user annotations, making it convenient to scale up existing\nsingle-device programs. It solves several technical challenges for production\nusage, such as static shape constraints, uneven partitioning, exchange of halo\ndata, and nested operator partitioning. These techniques allow GSPMD to achieve\n50% to 62% compute utilization on 128 to 2048 Cloud TPUv3 cores for models with\nup to one trillion parameters.\n  GSPMD produces a single program for all devices, which adjusts its behavior\nbased on a run-time partition ID, and uses collective operators for\ncross-device communication. This property allows the system itself to be\nscalable: the compilation time stays constant with increasing number of\ndevices.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:54:58 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Xu", "Yuanzhong", ""], ["Lee", "HyoukJoong", ""], ["Chen", "Dehao", ""], ["Hechtman", "Blake", ""], ["Huang", "Yanping", ""], ["Joshi", "Rahul", ""], ["Krikun", "Maxim", ""], ["Lepikhin", "Dmitry", ""], ["Ly", "Andy", ""], ["Maggioni", "Marcello", ""], ["Pang", "Ruoming", ""], ["Shazeer", "Noam", ""], ["Wang", "Shibo", ""], ["Wang", "Tao", ""], ["Wu", "Yonghui", ""], ["Chen", "Zhifeng", ""]]}, {"id": "2105.04668", "submitter": "Davis Rempe", "authors": "Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath\n  Sridhar, Leonidas J. Guibas", "title": "HuMoR: 3D Human Motion Model for Robust Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal\npose and shape. Though substantial progress has been made in estimating 3D\nhuman motion and shape from dynamic observations, recovering plausible pose\nsequences in the presence of noise and occlusions remains a challenge. For this\npurpose, we propose an expressive generative model in the form of a conditional\nvariational autoencoder, which learns a distribution of the change in pose at\neach step of a motion sequence. Furthermore, we introduce a flexible\noptimization-based approach that leverages HuMoR as a motion prior to robustly\nestimate plausible pose and shape from ambiguous observations. Through\nextensive evaluations, we demonstrate that our model generalizes to diverse\nmotions and body shapes after training on a large motion capture dataset, and\nenables motion reconstruction from multiple input modalities including 3D\nkeypoints and RGB(-D) videos.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:04:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Rempe", "Davis", ""], ["Birdal", "Tolga", ""], ["Hertzmann", "Aaron", ""], ["Yang", "Jimei", ""], ["Sridhar", "Srinath", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2105.04674", "submitter": "Francis Tyers", "authors": "Francis M. Tyers and Josh Meyer", "title": "What shall we do with an hour of data? Speech recognition for the un-\n  and under-served languages of Common Voice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This technical report describes the methods and results of a three-week\nsprint to produce deployable speech recognition models for 31 under-served\nlanguages of the Common Voice project. We outline the preprocessing steps,\nhyperparameter selection, and resulting accuracy on official testing sets. In\naddition to this we evaluate the models on multiple tasks: closed-vocabulary\nspeech recognition, pre-transcription, forced alignment, and key-word spotting.\nThe following experiments use Coqui STT, a toolkit for training and deployment\nof neural Speech-to-Text models.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:16:28 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Tyers", "Francis M.", ""], ["Meyer", "Josh", ""]]}, {"id": "2105.04678", "submitter": "Bishwo Adhikari Mr.", "authors": "Bishwo Adhikari, Esa Rahtu, Heikki Huttunen", "title": "Sample selection for efficient image annotation", "comments": "This work has been accepted in EUVIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised object detection has been proven to be successful in many\nbenchmark datasets achieving human-level performances. However, acquiring a\nlarge amount of labeled image samples for supervised detection training is\ntedious, time-consuming, and costly. In this paper, we propose an efficient\nimage selection approach that samples the most informative images from the\nunlabeled dataset and utilizes human-machine collaboration in an iterative\ntrain-annotate loop. Image features are extracted by the CNN network followed\nby the similarity score calculation, Euclidean distance. Unlabeled images are\nthen sampled into different approaches based on the similarity score. The\nproposed approach is straightforward, simple and sampling takes place prior to\nthe network training. Experiments on datasets show that our method can reduce\nup to 80% of manual annotation workload, compared to full manual labeling\nsetting, and performs better than random sampling.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:25:10 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Adhikari", "Bishwo", ""], ["Rahtu", "Esa", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2105.04682", "submitter": "Michael Lutter", "authors": "Michael Lutter and Shie Mannor and Jan Peters and Dieter Fox and\n  Animesh Garg", "title": "Value Iteration in Continuous Actions, States and Time", "comments": "Accepted at International Conference on Machine Learning (ICML) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical value iteration approaches are not applicable to environments with\ncontinuous states and actions. For such environments, the states and actions\nare usually discretized, which leads to an exponential increase in\ncomputational complexity. In this paper, we propose continuous fitted value\niteration (cFVI). This algorithm enables dynamic programming for continuous\nstates and actions with a known dynamics model. Leveraging the continuous-time\nformulation, the optimal policy can be derived for non-linear control-affine\ndynamics. This closed-form solution enables the efficient extension of value\niteration to continuous environments. We show in non-linear control experiments\nthat the dynamic programming solution obtains the same quantitative performance\nas deep reinforcement learning methods in simulation but excels when\ntransferred to the physical system. The policy obtained by cFVI is more robust\nto changes in the dynamics despite using only a deterministic model and without\nexplicitly incorporating robustness in the optimization. Videos of the physical\nsystem are available at \\url{https://sites.google.com/view/value-iteration}.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:40:56 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lutter", "Michael", ""], ["Mannor", "Shie", ""], ["Peters", "Jan", ""], ["Fox", "Dieter", ""], ["Garg", "Animesh", ""]]}, {"id": "2105.04683", "submitter": "Mattia Rigotti", "authors": "Mattia Rigotti, Rong Zhu", "title": "Deep Bandits Show-Off: Simple and Efficient Exploration with Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing efficient exploration is central to Reinforcement Learning due to\nthe fundamental problem posed by the exploration-exploitation dilemma. Bayesian\nexploration strategies like Thompson Sampling resolve this trade-off in a\nprincipled way by modeling and updating the distribution of the parameters of\nthe the action-value function, the outcome model of the environment. However,\nthis technique becomes infeasible for complex environments due to the\ndifficulty of representing and updating probability distributions over\nparameters of outcome models of corresponding complexity. Moreover, the\napproximation techniques introduced to mitigate this issue typically result in\npoor exploration-exploitation trade-offs, as observed in the case of deep\nneural network models with approximate posterior methods that have been shown\nto underperform in the deep bandit scenario.\n  In this paper we introduce Sample Average Uncertainty (SAU), a simple and\nefficient uncertainty measure for contextual bandits. While Bayesian approaches\nlike Thompson Sampling estimate outcomes uncertainty indirectly by first\nquantifying the variability over the parameters of the outcome model, SAU is a\nfrequentist approach that directly estimates the uncertainty of the outcomes\nbased on the value predictions. Importantly, we show theoretically that the\nuncertainty measure estimated by SAU asymptotically matches the uncertainty\nprovided by Thompson Sampling, as well as its regret bounds. Because of its\nsimplicity SAU can be seamlessly applied to deep contextual bandits as a very\nscalable drop-in replacement for epsilon-greedy exploration. Finally, we\nempirically confirm our theory by showing that SAU-based exploration\noutperforms current state-of-the-art deep Bayesian bandit methods on several\nreal-world datasets at modest computation cost.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:45:01 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Rigotti", "Mattia", ""], ["Zhu", "Rong", ""]]}, {"id": "2105.04699", "submitter": "Girish Joshi", "authors": "Girish Joshi, Girish Chowdhary", "title": "Adaptive Policy Transfer in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient and robust policy transfer remains a key challenge for\nreinforcement learning to become viable for real-wold robotics. Policy transfer\nthrough warm initialization, imitation, or interacting over a large set of\nagents with randomized instances, have been commonly applied to solve a variety\nof Reinforcement Learning tasks. However, this seems far from how skill\ntransfer happens in the biological world: Humans and animals are able to\nquickly adapt the learned behaviors between similar tasks and learn new skills\nwhen presented with new situations. Here we seek to answer the question: Will\nlearning to combine adaptation and exploration lead to a more efficient\ntransfer of policies between domains? We introduce a principled mechanism that\ncan \"Adapt-to-Learn\", that is adapt the source policy to learn to solve a\ntarget task with significant transition differences and uncertainties. We show\nthat the presented method learns to seamlessly combine learning from adaptation\nand exploration and leads to a robust policy transfer algorithm with\nsignificantly reduced sample complexity in transferring skills between related\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 22:42:03 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Joshi", "Girish", ""], ["Chowdhary", "Girish", ""]]}, {"id": "2105.04707", "submitter": "Amita Misra", "authors": "Amita Misra, Zhe Liu and Jalal Mahmud", "title": "Accountable Error Characterization", "comments": "Proceedings of the First Workshop on Trustworthy Natural Language\n  Processing, TrustNLP@NAACL-HLT 2021, June 10, 2021, Association for\n  Computational Linguistics, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Customers of machine learning systems demand accountability from the\ncompanies employing these algorithms for various prediction tasks.\nAccountability requires understanding of system limit and condition of\nerroneous predictions, as customers are often interested in understanding the\nincorrect predictions, and model developers are absorbed in finding methods\nthat can be used to get incremental improvements to an existing system.\nTherefore, we propose an accountable error characterization method, AEC, to\nunderstand when and where errors occur within the existing black-box models.\nAEC, as constructed with human-understandable linguistic features, allows the\nmodel developers to automatically identify the main sources of errors for a\ngiven classification system. It can also be used to sample for the set of most\ninformative input points for a next round of training. We perform error\ndetection for a sentiment analysis task using AEC as a case study. Our results\non the sample sentiment task show that AEC is able to characterize erroneous\npredictions into human understandable categories and also achieves promising\nresults on selecting erroneous samples when compared with the uncertainty-based\nsampling.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 23:40:01 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Misra", "Amita", ""], ["Liu", "Zhe", ""], ["Mahmud", "Jalal", ""]]}, {"id": "2105.04708", "submitter": "Bimal Bhattarai", "authors": "Bimal Bhattarai, Ole-Christoffer Granmo, Lei Jiao", "title": "Word-level Human Interpretable Scoring Mechanism for Novel Text\n  Detection Using Tsetlin Machines", "comments": "18 pages, 11 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in novelty detection focuses mainly on document-level\nclassification, employing deep neural networks (DNN). However, the black-box\nnature of DNNs makes it difficult to extract an exact explanation of why a\ndocument is considered novel. In addition, dealing with novelty at the\nword-level is crucial to provide a more fine-grained analysis than what is\navailable at the document level. In this work, we propose a Tsetlin machine\n(TM)-based architecture for scoring individual words according to their\ncontribution to novelty. Our approach encodes a description of the novel\ndocuments using the linguistic patterns captured by TM clauses. We then adopt\nthis description to measure how much a word contributes to making documents\nnovel. Our experimental results demonstrate how our approach breaks down\nnovelty into interpretable phrases, successfully measuring novelty.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 23:41:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Bhattarai", "Bimal", ""], ["Granmo", "Ole-Christoffer", ""], ["Jiao", "Lei", ""]]}, {"id": "2105.04727", "submitter": "Efthymios Tzinis", "authors": "Efthymios Tzinis, Jonah Casebeer, Zhepei Wang, Paris Smaragdis", "title": "Separate but Together: Unsupervised Federated Learning for Speech\n  Enhancement from Non-IID Data", "comments": "Accepted to WASPAA 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 00:47:18 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 23:11:12 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Tzinis", "Efthymios", ""], ["Casebeer", "Jonah", ""], ["Wang", "Zhepei", ""], ["Smaragdis", "Paris", ""]]}, {"id": "2105.04738", "submitter": "Zhenyuan Yuan", "authors": "Zhenyuan Yuan, Minghui Zhu", "title": "Resource-aware Distributed Gaussian Process Regression for Real-time\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem where a group of agents aim to collaboratively learn a\ncommon latent function through streaming data. We propose a Resource-aware\nGaussian process regression algorithm that is cognizant of agents' limited\ncapabilities in communication, computation and memory. We quantify the\nimprovement that limited inter-agent communication brings to the transient and\nsteady-state performance in predictive variance and predictive mean. A set of\nsimulations is conducted to evaluate the developed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 01:13:22 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 23:08:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Yuan", "Zhenyuan", ""], ["Zhu", "Minghui", ""]]}, {"id": "2105.04752", "submitter": "Marco A. Mart\\'inez Ram\\'irez", "authors": "Marco A. Mart\\'inez Ram\\'irez, Oliver Wang, Paris Smaragdis, Nicholas\n  J. Bryan", "title": "Differentiable Signal Processing With Black-Box Audio Effects", "comments": "Presented at the IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP), June 2021. Source code, demo and audio\n  examples: https://mchijmma.github.io/DeepAFx/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a data-driven approach to automate audio signal processing by\nincorporating stateful third-party, audio effects as layers within a deep\nneural network. We then train a deep encoder to analyze input audio and control\neffect parameters to perform the desired signal manipulation, requiring only\ninput-target paired audio data as supervision. To train our network with\nnon-differentiable black-box effects layers, we use a fast, parallel stochastic\ngradient approximation scheme within a standard auto differentiation graph,\nyielding efficient end-to-end backpropagation. We demonstrate the power of our\napproach with three separate automatic audio production applications: tube\namplifier emulation, automatic removal of breaths and pops from voice\nrecordings, and automatic music mastering. We validate our results with a\nsubjective listening test, showing our approach not only can enable new\nautomatic audio effects tasks, but can yield results comparable to a\nspecialized, state-of-the-art commercial solution for music mastering.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 02:20:22 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ram\u00edrez", "Marco A. Mart\u00ednez", ""], ["Wang", "Oliver", ""], ["Smaragdis", "Paris", ""], ["Bryan", "Nicholas J.", ""]]}, {"id": "2105.04754", "submitter": "Yariv Aizenbud", "authors": "Yariv Aizenbud and Barak Sober", "title": "Non-Parametric Estimation of Manifolds from Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common observation in data-driven applications is that high dimensional\ndata has a low intrinsic dimension, at least locally. In this work, we consider\nthe problem of estimating a $d$ dimensional sub-manifold of $\\mathbb{R}^D$ from\na finite set of noisy samples. Assuming that the data was sampled uniformly\nfrom a tubular neighborhood of $\\mathcal{M}\\in \\mathcal{C}^k$, a compact\nmanifold without boundary, we present an algorithm that takes a point $r$ from\nthe tubular neighborhood and outputs $\\hat p_n\\in \\mathbb{R}^D$, and\n$\\widehat{T_{\\hat p_n}\\mathcal{M}}$ an element in the Grassmanian $Gr(d, D)$.\nWe prove that as the number of samples $n\\to\\infty$ the point $\\hat p_n$\nconverges to $p\\in \\mathcal{M}$ and $\\widehat{T_{\\hat p_n}\\mathcal{M}}$\nconverges to $T_p\\mathcal{M}$ (the tangent space at that point) with high\nprobability. Furthermore, we show that the estimation yields asymptotic rates\nof convergence of $n^{-\\frac{k}{2k + d}}$ for the point estimation and\n$n^{-\\frac{k-1}{2k + d}}$ for the estimation of the tangent space. These rates\nare known to be optimal for the case of function estimation.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 02:29:33 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 18:53:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Aizenbud", "Yariv", ""], ["Sober", "Barak", ""]]}, {"id": "2105.04758", "submitter": "Fanfei Chen", "authors": "Fanfei Chen, Paul Szenher, Yewei Huang, Jinkun Wang, Tixiao Shan, Shi\n  Bai, Brendan Englot", "title": "Zero-Shot Reinforcement Learning on Graphs for Autonomous Exploration\n  Under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of autonomous exploration under localization\nuncertainty for a mobile robot with 3D range sensing. We present a framework\nfor self-learning a high-performance exploration policy in a single simulation\nenvironment, and transferring it to other environments, which may be physical\nor virtual. Recent work in transfer learning achieves encouraging performance\nby domain adaptation and domain randomization to expose an agent to scenarios\nthat fill the inherent gaps in sim2sim and sim2real approaches. However, it is\ninefficient to train an agent in environments with randomized conditions to\nlearn the important features of its current state. An agent can use domain\nknowledge provided by human experts to learn efficiently. We propose a novel\napproach that uses graph neural networks in conjunction with deep reinforcement\nlearning, enabling decision-making over graphs containing relevant exploration\ninformation provided by human experts to predict a robot's optimal sensing\naction in belief space. The policy, which is trained only in a single\nsimulation environment, offers a real-time, scalable, and transferable\ndecision-making strategy, resulting in zero-shot transfer to other simulation\nenvironments and even real-world environments.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 02:42:17 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chen", "Fanfei", ""], ["Szenher", "Paul", ""], ["Huang", "Yewei", ""], ["Wang", "Jinkun", ""], ["Shan", "Tixiao", ""], ["Bai", "Shi", ""], ["Englot", "Brendan", ""]]}, {"id": "2105.04761", "submitter": "Chang Li", "authors": "Chang Li and Hua Ouyang", "title": "Federated Unbiased Learning to Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unbiased Learning to Rank (ULTR) studies the problem of learning a ranking\nfunction based on biased user interactions. In this framework, ULTR algorithms\nhave to rely on a large amount of user data that are collected, stored, and\naggregated by central servers.\n  In this paper, we consider an on-device search setting, where users search\nagainst their personal corpora on their local devices, and the goal is to learn\na ranking function from biased user interactions. Due to privacy constraints,\nusers' queries, personal documents, results lists, and raw interaction data\nwill not leave their devices, and ULTR has to be carried out via Federated\nLearning (FL).\n  Directly applying existing ULTR algorithms on users' devices could suffer\nfrom insufficient training data due to the limited amount of local\ninteractions. To address this problem, we propose the FedIPS algorithm, which\nlearns from user interactions on-device under the coordination of a central\nserver and uses click propensities to remove the position bias in user\ninteractions. Our evaluation of FedIPS on the Yahoo and Istella datasets shows\nthat FedIPS is robust over a range of position biases.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:01:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Chang", ""], ["Ouyang", "Hua", ""]]}, {"id": "2105.04762", "submitter": "Dung Truong", "authors": "Dung Truong, Michael Milham, Scott Makeig, Arnaud Delorme", "title": "Deep Convolutional Neural Network Applied to Electroencephalography: Raw\n  Data vs Spectral Features", "comments": "IEEE Engineering in Medicine and Biology Society Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of deep learning in computer vision has inspired the scientific\ncommunity to explore new analysis methods. Within the field of neuroscience,\nspecifically in electrophysiological neuroimaging, researchers are starting to\nexplore leveraging deep learning to make predictions on their data without\nextensive feature engineering. This paper compares deep learning using\nminimally processed EEG raw data versus deep learning using EEG spectral\nfeatures using two different deep convolutional neural architectures. One of\nthem from Putten et al. (2018) is tailored to process raw data; the other was\nderived from the VGG16 vision network (Simonyan and Zisserman, 2015) which is\ndesigned to process EEG spectral features. We apply them to classify sex on\n24-channel EEG from a large corpus of 1,574 participants. Not only do we\nimprove on state-of-the-art classification performance for this type of\nclassification problem, but we also show that in all cases, raw data\nclassification leads to superior performance as compared to spectral EEG\nfeatures. Interestingly we show that the neural network tailored to process EEG\nspectral features has increased performance when applied to raw data\nclassification. Our approach suggests that the same convolutional networks used\nto process EEG spectral features yield superior performance when applied to EEG\nraw data.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:02:35 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Truong", "Dung", ""], ["Milham", "Michael", ""], ["Makeig", "Scott", ""], ["Delorme", "Arnaud", ""]]}, {"id": "2105.04766", "submitter": "Cemil Dibek", "authors": "Amir Ali Ahmadi, Cemil Dibek, Georgina Hall", "title": "Sums of Separable and Quadratic Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study separable plus quadratic (SPQ) polynomials, i.e., polynomials that\nare the sum of univariate polynomials in different variables and a quadratic\npolynomial. Motivated by the fact that nonnegative separable and nonnegative\nquadratic polynomials are sums of squares, we study whether nonnegative SPQ\npolynomials are (i) the sum of a nonnegative separable and a nonnegative\nquadratic polynomial, and (ii) a sum of squares. We establish that the answer\nto question (i) is positive for univariate plus quadratic polynomials and for\nconvex SPQ polynomials, but negative already for bivariate quartic SPQ\npolynomials. We use our decomposition result for convex SPQ polynomials to show\nthat convex SPQ polynomial optimization problems can be solved by \"small\"\nsemidefinite programs. For question (ii), we provide a complete\ncharacterization of the answer based on the degree and the number of variables\nof the SPQ polynomial. We also prove that testing nonnegativity of SPQ\npolynomials is NP-hard when the degree is at least four. We end by presenting\napplications of SPQ polynomials to upper bounding sparsity of solutions to\nlinear programs, polynomial regression problems in statistics, and a\ngeneralization of Newton's method which incorporates separable higher-order\nderivative information.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:26:46 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Dibek", "Cemil", ""], ["Hall", "Georgina", ""]]}, {"id": "2105.04769", "submitter": "Riku Togashi", "authors": "Riku Togashi, Masahiro Kato, Mayu Otani, Tetsuya Sakai, Shin'ichi\n  Satoh", "title": "Scalable Personalised Item Ranking through Parametric Density Estimation", "comments": "Accepted by SIGIR'21", "journal-ref": null, "doi": "10.1145/3404835.3462933", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from implicit feedback is challenging because of the difficult\nnature of the one-class problem: we can observe only positive examples. Most\nconventional methods use a pairwise ranking approach and negative samplers to\ncope with the one-class problem. However, such methods have two main drawbacks\nparticularly in large-scale applications; (1) the pairwise approach is severely\ninefficient due to the quadratic computational cost; and (2) even recent\nmodel-based samplers (e.g. IRGAN) cannot achieve practical efficiency due to\nthe training of an extra model.\n  In this paper, we propose a learning-to-rank approach, which achieves\nconvergence speed comparable to the pointwise counterpart while performing\nsimilarly to the pairwise counterpart in terms of ranking effectiveness. Our\napproach estimates the probability densities of positive items for each user\nwithin a rich class of distributions, viz. \\emph{exponential family}. In our\nformulation, we derive a loss function and the appropriate negative sampling\ndistribution based on maximum likelihood estimation. We also develop a\npractical technique for risk approximation and a regularisation scheme. We then\ndiscuss that our single-model approach is equivalent to an IRGAN variant under\na certain condition. Through experiments on real-world datasets, our approach\noutperforms the pointwise and pairwise counterparts in terms of effectiveness\nand efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:38:16 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Togashi", "Riku", ""], ["Kato", "Masahiro", ""], ["Otani", "Mayu", ""], ["Sakai", "Tetsuya", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "2105.04770", "submitter": "Qiaosheng Zhang", "authors": "Qiaosheng Zhang, Vincent Y. F. Tan", "title": "Exact Recovery in the General Hypergraph Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper investigates fundamental limits of exact recovery in the general\nd-uniform hypergraph stochastic block model (d-HSBM), wherein n nodes are\npartitioned into k disjoint communities with relative sizes (p1,..., pk). Each\nsubset of nodes with cardinality d is generated independently as an order-d\nhyperedge with a certain probability that depends on the ground-truth\ncommunities that the d nodes belong to. The goal is to exactly recover the k\nhidden communities based on the observed hypergraph. We show that there exists\na sharp threshold such that exact recovery is achievable above the threshold\nand impossible below the threshold (apart from a small regime of parameters\nthat will be specified precisely). This threshold is represented in terms of a\nquantity which we term as the generalized Chernoff-Hellinger divergence between\ncommunities. Our result for this general model recovers prior results for the\nstandard SBM and d-HSBM with two symmetric communities as special cases. En\nroute to proving our achievability results, we develop a polynomial-time\ntwo-stage algorithm that meets the threshold. The first stage adopts a certain\nhypergraph spectral clustering method to obtain a coarse estimate of\ncommunities, and the second stage refines each node individually via local\nrefinement steps to ensure exact recovery.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:39:08 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Zhang", "Qiaosheng", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2105.04771", "submitter": "Jiaxiang Wu", "authors": "Jiaxiang Wu, Shitong Luo, Tao Shen, Haidong Lan, Sheng Wang, Junzhou\n  Huang", "title": "EBM-Fold: Fully-Differentiable Protein Folding Powered by Energy-based\n  Models", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate protein structure prediction from amino-acid sequences is critical\nto better understanding the protein function. Recent advances in this area\nlargely benefit from more precise inter-residue distance and orientation\npredictions, powered by deep neural networks. However, the structure\noptimization procedure is still dominated by traditional tools, e.g. Rosetta,\nwhere the structure is solved via minimizing a pre-defined statistical energy\nfunction (with optional prediction-based restraints). Such energy function may\nnot be optimal in formulating the whole conformation space of proteins. In this\npaper, we propose a fully-differentiable approach for protein structure\noptimization, guided by a data-driven generative network. This network is\ntrained in a denoising manner, attempting to predict the correction signal from\ncorrupted distance matrices between Ca atoms. Once the network is well trained,\nLangevin dynamics based sampling is adopted to gradually optimize structures\nfrom random initialization. Extensive experiments demonstrate that our EBM-Fold\napproach can efficiently produce high-quality decoys, compared against\ntraditional Rosetta-based structure optimization routines.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:40:29 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 16:32:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wu", "Jiaxiang", ""], ["Luo", "Shitong", ""], ["Shen", "Tao", ""], ["Lan", "Haidong", ""], ["Wang", "Sheng", ""], ["Huang", "Junzhou", ""]]}, {"id": "2105.04779", "submitter": "Yu Yan", "authors": "Yu Yan, Jiusheng Chen, Weizhen Qi, Nikhil Bhendawade, Yeyun Gong, Nan\n  Duan and Ruofei Zhang", "title": "EL-Attention: Memory Efficient Lossless Attention for Generation", "comments": "ICML 2021. Version 2: add pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer model with multi-head attention requires caching intermediate\nresults for efficient inference in generation tasks. However, cache brings new\nmemory-related costs and prevents leveraging larger batch size for faster\nspeed. We propose memory-efficient lossless attention (called EL-attention) to\naddress this issue. It avoids heavy operations for building multi-head keys and\nvalues, cache for them is not needed. EL-attention constructs an ensemble of\nattention results by expanding query while keeping key and value shared. It\nproduces the same result as multi-head attention with less GPU memory and\nfaster inference speed. We conduct extensive experiments on Transformer, BART,\nand GPT-2 for summarization and question generation tasks. The results show\nEL-attention speeds up existing models by 1.6x to 5.3x without accuracy loss.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 04:37:52 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 21:18:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yan", "Yu", ""], ["Chen", "Jiusheng", ""], ["Qi", "Weizhen", ""], ["Bhendawade", "Nikhil", ""], ["Gong", "Yeyun", ""], ["Duan", "Nan", ""], ["Zhang", "Ruofei", ""]]}, {"id": "2105.04798", "submitter": "Francesco Zola", "authors": "Francesco Zola, Lander Segurola, Jan Lukas Bruse, Mikel Galar Idoate", "title": "Temporal graph-based approach for behavioural entity classification", "comments": null, "journal-ref": null, "doi": "10.18239/jornadas_2021.34.12", "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph-based analyses have gained a lot of relevance in the past years due to\ntheir high potential in describing complex systems by detailing the actors\ninvolved, their relations and their behaviours. Nevertheless, in scenarios\nwhere these aspects are evolving over time, it is not easy to extract valuable\ninformation or to characterize correctly all the actors. In this study, a two\nphased approach for exploiting the potential of graph structures in the\ncybersecurity domain is presented. The main idea is to convert a network\nclassification problem into a graph-based behavioural one. We extract these\ngraph structures that can represent the evolution of both normal and attack\nentities and apply a temporal dissection approach in order to highlight their\nmicro-dynamics. Further, three clustering techniques are applied to the normal\nentities in order to aggregate similar behaviours, mitigate the imbalance\nproblem and reduce noisy data. Our approach suggests the implementation of two\npromising deep learning paradigms for entity classification based on Graph\nConvolutional Networks.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 06:13:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zola", "Francesco", ""], ["Segurola", "Lander", ""], ["Bruse", "Jan Lukas", ""], ["Idoate", "Mikel Galar", ""]]}, {"id": "2105.04801", "submitter": "Sahil Sidheekh", "authors": "Sahil Sidheekh, Aroof Aimen, Narayanan C. Krishnan", "title": "On Characterizing GAN Convergence Through Proximal Duality Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": "2640-3498", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the accomplishments of Generative Adversarial Networks (GANs) in\nmodeling data distributions, training them remains a challenging task. A\ncontributing factor to this difficulty is the non-intuitive nature of the GAN\nloss curves, which necessitates a subjective evaluation of the generated output\nto infer training progress. Recently, motivated by game theory, duality gap has\nbeen proposed as a domain agnostic measure to monitor GAN training. However, it\nis restricted to the setting when the GAN converges to a Nash equilibrium. But\nGANs need not always converge to a Nash equilibrium to model the data\ndistribution. In this work, we extend the notion of duality gap to proximal\nduality gap that is applicable to the general context of training GANs where\nNash equilibria may not exist. We show theoretically that the proximal duality\ngap is capable of monitoring the convergence of GANs to a wider spectrum of\nequilibria that subsumes Nash equilibria. We also theoretically establish the\nrelationship between the proximal duality gap and the divergence between the\nreal and generated data distributions for different GAN formulations. Our\nresults provide new insights into the nature of GAN convergence. Finally, we\nvalidate experimentally the usefulness of proximal duality gap for monitoring\nand influencing GAN training.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 06:27:27 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 08:56:03 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sidheekh", "Sahil", ""], ["Aimen", "Aroof", ""], ["Krishnan", "Narayanan C.", ""]]}, {"id": "2105.04806", "submitter": "Premjeet Singh", "authors": "Premjeet Singh, Goutam Saha, Md Sahidullah", "title": "Deep scattering network for speech emotion recognition", "comments": "5 pages, 4 figures, Accepted for publication in 2021 European Signal\n  Processing Conference (EUSIPCO 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces scattering transform for speech emotion recognition\n(SER). Scattering transform generates feature representations which remain\nstable to deformations and shifting in time and frequency without much loss of\ninformation. In speech, the emotion cues are spread across time and localised\nin frequency. The time and frequency invariance characteristic of scattering\ncoefficients provides a representation robust against emotion irrelevant\nvariations e.g., different speakers, language, gender etc. while preserving the\nvariations caused by emotion cues. Hence, such a representation captures the\nemotion information more efficiently from speech. We perform experiments to\ncompare scattering coefficients with standard mel-frequency cepstral\ncoefficients (MFCCs) over different databases. It is observed that frequency\nscattering performs better than time-domain scattering and MFCCs. We also\ninvestigate layer-wise scattering coefficients to analyse the importance of\ntime shift and deformation stable scalogram and modulation spectrum\ncoefficients for SER. We observe that layer-wise coefficients taken\nindependently also perform better than MFCCs.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 06:37:41 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Singh", "Premjeet", ""], ["Saha", "Goutam", ""], ["Sahidullah", "Md", ""]]}, {"id": "2105.04816", "submitter": "Matthew J. Holland", "authors": "Matthew J. Holland, El Mehdi Haress", "title": "Spectral risk-based learning using unbounded losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the setting of learning problems under a wide class\nof spectral risk (or \"L-risk\") functions, where a Lipschitz-continuous spectral\ndensity is used to flexibly assign weight to extreme loss values. We obtain\nexcess risk guarantees for a derivative-free learning procedure under unbounded\nheavy-tailed loss distributions, and propose a computationally efficient\nimplementation which empirically outperforms traditional risk minimizers in\nterms of balancing spectral risk and misclassification error.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:08:10 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Holland", "Matthew J.", ""], ["Haress", "El Mehdi", ""]]}, {"id": "2105.04823", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Jiale Zhou, Xuming He", "title": "Learning Implicit Temporal Alignment for Few-shot Video Classification", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot video classification aims to learn new video categories with only a\nfew labeled examples, alleviating the burden of costly annotation in real-world\napplications. However, it is particularly challenging to learn a\nclass-invariant spatial-temporal representation in such a setting. To address\nthis, we propose a novel matching-based few-shot learning strategy for video\nsequences in this work. Our main idea is to introduce an implicit temporal\nalignment for a video pair, capable of estimating the similarity between them\nin an accurate and robust manner. Moreover, we design an effective context\nencoding module to incorporate spatial and feature channel context, resulting\nin better modeling of intra-class variations. To train our model, we develop a\nmulti-task loss for learning video matching, leading to video features with\nbetter generalization. Extensive experimental results on two challenging\nbenchmarks, show that our method outperforms the prior arts with a sizable\nmargin on SomethingSomething-V2 and competitive results on Kinetics.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:18:57 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Zhang", "Songyang", ""], ["Zhou", "Jiale", ""], ["He", "Xuming", ""]]}, {"id": "2105.04837", "submitter": "Diego Antognini", "authors": "Diego Antognini and Boi Faltings", "title": "Rationalization through Concepts", "comments": "Accepted at ACL 2021 (findings). 15 pages, 10 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated predictions require explanations to be interpretable by humans. One\ntype of explanation is a rationale, i.e., a selection of input features such as\nrelevant text snippets from which the model computes the outcome. However, a\nsingle overall selection does not provide a complete explanation, e.g.,\nweighing several aspects for decisions. To this end, we present a novel\nself-interpretable model called ConRAT. Inspired by how human explanations for\nhigh-level decisions are often based on key concepts, ConRAT extracts a set of\ntext snippets as concepts and infers which ones are described in the document.\nThen, it explains the outcome with a linear aggregation of concepts. Two\nregularizers drive ConRAT to build interpretable concepts. In addition, we\npropose two techniques to boost the rationale and predictive performance\nfurther. Experiments on both single- and multi-aspect sentiment classification\ntasks show that ConRAT is the first to generate concepts that align with human\nrationalization while using only the overall label. Further, it outperforms\nstate-of-the-art methods trained on each aspect label independently.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:46:48 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Antognini", "Diego", ""], ["Faltings", "Boi", ""]]}, {"id": "2105.04851", "submitter": "Shi Pu", "authors": "Kun Huang and Shi Pu", "title": "Improving the Transient Times for Distributed Stochastic Gradient\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distributed optimization problem where $n$ agents each\npossessing a local cost function, collaboratively minimize the average of the\n$n$ cost functions over a connected network. Assuming stochastic gradient\ninformation is available, we study a distributed stochastic gradient algorithm,\ncalled exact diffusion with adaptive stepsizes (EDAS) adapted from the Exact\nDiffusion method and NIDS and perform a non-asymptotic convergence analysis. We\nnot only show that EDAS asymptotically achieves the same network independent\nconvergence rate as centralized stochastic gradient descent (SGD) for\nminimizing strongly convex and smooth objective functions, but also\ncharacterize the transient time needed for the algorithm to approach the\nasymptotic convergence rate, which behaves as\n$K_T=\\mathcal{O}\\left(\\frac{n}{1-\\lambda_2}\\right)$, where $1-\\lambda_2$ stands\nfor the spectral gap of the mixing matrix. To the best of our knowledge, EDAS\nachieves the shortest transient time when the average of the $n$ cost functions\nis strongly convex and each cost function is smooth. Numerical simulations\nfurther corroborate and strengthen the obtained theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:09:31 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Huang", "Kun", ""], ["Pu", "Shi", ""]]}, {"id": "2105.04854", "submitter": "Ryan Henderson", "authors": "Ryan Henderson, Djork-Arn\\'e Clevert, Floriane Montanari", "title": "Improving Molecular Graph Neural Network Explainability with\n  Orthonormalization and Induced Sparsity", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rationalizing which parts of a molecule drive the predictions of a molecular\ngraph convolutional neural network (GCNN) can be difficult. To help, we propose\ntwo simple regularization techniques to apply during the training of GCNNs:\nBatch Representation Orthonormalization (BRO) and Gini regularization. BRO,\ninspired by molecular orbital theory, encourages graph convolution operations\nto generate orthonormal node embeddings. Gini regularization is applied to the\nweights of the output layer and constrains the number of dimensions the model\ncan use to make predictions. We show that Gini and BRO regularization can\nimprove the accuracy of state-of-the-art GCNN attribution methods on artificial\nbenchmark datasets. In a real-world setting, we demonstrate that medicinal\nchemists significantly prefer explanations extracted from regularized models.\nWhile we only study these regularizers in the context of GCNNs, both can be\napplied to other types of neural networks\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:13:34 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:07:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Henderson", "Ryan", ""], ["Clevert", "Djork-Arn\u00e9", ""], ["Montanari", "Floriane", ""]]}, {"id": "2105.04857", "submitter": "Eric Wong", "authors": "Eric Wong, Shibani Santurkar, Aleksander M\\k{a}dry", "title": "Leveraging Sparse Linear Layers for Debuggable Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how fitting sparse linear models over learned deep feature\nrepresentations can lead to more debuggable neural networks. These networks\nremain highly accurate while also being more amenable to human interpretation,\nas we demonstrate quantiatively via numerical and human experiments. We further\nillustrate how the resulting sparse explanations can help to identify spurious\ncorrelations, explain misclassifications, and diagnose model biases in vision\nand language tasks. The code for our toolkit can be found at\nhttps://github.com/madrylab/debuggabledeepnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:15:25 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wong", "Eric", ""], ["Santurkar", "Shibani", ""], ["M\u0105dry", "Aleksander", ""]]}, {"id": "2105.04869", "submitter": "Pawan Goyal", "authors": "Pawan Goyal and Peter Benner", "title": "Discovery of Nonlinear Dynamical Systems using a Runge-Kutta Inspired\n  Dictionary-based Sparse Regression Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discovering dynamical models to describe underlying dynamical behavior is\nessential to draw decisive conclusions and engineering studies, e.g.,\noptimizing a process. Experimental data availability notwithstanding has\nincreased significantly, but interpretable and explainable models in science\nand engineering yet remain incomprehensible. In this work, we blend machine\nlearning and dictionary-based learning with numerical analysis tools to\ndiscover governing differential equations from noisy and sparsely-sampled\nmeasurement data. We utilize the fact that given a dictionary containing huge\ncandidate nonlinear functions, dynamical models can often be described by a few\nappropriately chosen candidates. As a result, we obtain interpretable and\nparsimonious models which are prone to generalize better beyond the sampling\nregime. Additionally, we integrate a numerical integration framework with\ndictionary learning that yields differential equations without requiring or\napproximating derivative information at any stage. Hence, it is utterly\neffective in corrupted and sparsely-sampled data. We discuss its extension to\ngoverning equations, containing rational nonlinearities that typically appear\nin biological networks. Moreover, we generalized the method to governing\nequations that are subject to parameter variations and externally controlled\ninputs. We demonstrate the efficiency of the method to discover a number of\ndiverse differential equations using noisy measurements, including a model\ndescribing neural dynamics, chaotic Lorenz model, Michaelis-Menten Kinetics,\nand a parameterized Hopf normal form.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:46:51 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Goyal", "Pawan", ""], ["Benner", "Peter", ""]]}, {"id": "2105.04876", "submitter": "Matthias A{\\ss}enmacher", "authors": "M. A{\\ss}enmacher, P. Schulze, C. Heumann", "title": "Benchmarking down-scaled (not so large) pre-trained language models", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Transformer-based language models are pre-trained on corpora of varying\nsizes, for a different number of steps and with different batch sizes. At the\nsame time, more fundamental components, such as the pre-training objective or\narchitectural hyperparameters, are modified. In total, it is therefore\ndifficult to ascribe changes in performance to specific factors. Since\nsearching the hyperparameter space over the full systems is too costly, we\npre-train down-scaled versions of several popular Transformer-based\narchitectures on a common pre-training corpus and benchmark them on a subset of\nthe GLUE tasks (Wang et al., 2018). Specifically, we systematically compare\nthree pre-training objectives for different shape parameters and model sizes,\nwhile also varying the number of pre-training steps and the batch size. In our\nexperiments MLM + NSP (BERT-style) consistently outperforms MLM (RoBERTa-style)\nas well as the standard LM objective. Furthermore, we find that additional\ncompute should be mainly allocated to an increased model size, while training\nfor more steps is inefficient. Based on these observations, as a final step we\nattempt to scale up several systems using compound scaling (Tan and Le, 2019)\nadapted to Transformer-based language models.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:01:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["A\u00dfenmacher", "M.", ""], ["Schulze", "P.", ""], ["Heumann", "C.", ""]]}, {"id": "2105.04885", "submitter": "Michail Chatzianastasis", "authors": "Michail Chatzianastasis, George Dasoulas, Georgios Siolas, Michalis\n  Vazirgiannis", "title": "Operation Embeddings for Neural Architecture Search", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Architecture Search (NAS) has recently gained increased attention, as\na class of approaches that automatically searches in an input space of network\narchitectures. A crucial part of the NAS pipeline is the encoding of the\narchitecture that consists of the applied computational blocks, namely the\noperations and the links between them. Most of the existing approaches either\nfail to capture the structural properties of the architectures or use a\nhand-engineered vector to encode the operator information. In this paper, we\npropose the replacement of fixed operator encoding with learnable\nrepresentations in the optimization process. This approach, which effectively\ncaptures the relations of different operations, leads to smoother and more\naccurate representations of the architectures and consequently to improved\nperformance of the end task. Our extensive evaluation in ENAS benchmark\ndemonstrates the effectiveness of the proposed operation embeddings to the\ngeneration of highly accurate models, achieving state-of-the-art performance.\nFinally, our method produces top-performing architectures that share similar\noperation and graph patterns, highlighting a strong correlation between\narchitecture's structural properties and performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:17:10 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chatzianastasis", "Michail", ""], ["Dasoulas", "George", ""], ["Siolas", "Georgios", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "2105.04888", "submitter": "Xiaolong Wei", "authors": "Xiaolong Wei, LiFang Yang, Xianglin Huang, Gang Cao, Tao Zhulin,\n  Zhengyang Du, Jing An", "title": "Hierarchical RNNs-Based Transformers MADDPG for Mixed\n  Cooperative-Competitive Environments", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, attention mechanism has been widely applied to the fields of deep\nlearning models. Structural models that based on attention mechanism can not\nonly record the relationships between features position, but also can measure\nthe importance of different features based on their weights. By establishing\ndynamically weighted parameters for choosing relevant and irrelevant features,\nthe key information can be strengthened, and the irrelevant information can be\nweakened. Therefore, the efficiency of deep learning algorithms can be\nsignificantly elevated and improved. Although transformers have been performed\nvery well in many fields including reinforcement learning, there are still many\nproblems and applications can be solved and made with transformers within this\narea. MARL (known as Multi-Agent Reinforcement Learning) can be recognized as a\nset of independent agents trying to adapt and learn through their way to reach\nthe goal. In order to emphasize the relationship between each MDP decision in a\ncertain time period, we applied the hierarchical coding method and validated\nthe effectiveness of this method. This paper proposed a hierarchical\ntransformers MADDPG based on RNN which we call it Hierarchical RNNs-Based\nTransformers MADDPG(HRTMADDPG). It consists of a lower level encoder based on\nRNNs that encodes multiple step sizes in each time sequence, and it also\nconsists of an upper sequence level encoder based on transformer for learning\nthe correlations between multiple sequences so that we can capture the causal\nrelationship between sub-time sequences and make HRTMADDPG more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:22:52 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wei", "Xiaolong", ""], ["Yang", "LiFang", ""], ["Huang", "Xianglin", ""], ["Cao", "Gang", ""], ["Zhulin", "Tao", ""], ["Du", "Zhengyang", ""], ["An", "Jing", ""]]}, {"id": "2105.04906", "submitter": "Adrien Bardes", "authors": "Adrien Bardes and Jean Ponce and Yann LeCun", "title": "VICReg: Variance-Invariance-Covariance Regularization for\n  Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent self-supervised methods for image representation learning are based on\nmaximizing the agreement between embedding vectors from different views of the\nsame image. A trivial solution is obtained when the encoder outputs constant\nvectors. This collapse problem is often avoided through implicit biases in the\nlearning architecture, that often lack a clear justification or interpretation.\nIn this paper, we introduce VICReg (Variance-Invariance-Covariance\nRegularization), a method that explicitly avoids the collapse problem with a\nsimple regularization term on the variance of the embeddings along each\ndimension individually. VICReg combines the variance term with a decorrelation\nmechanism based on redundancy reduction and covariance regularization, and\nachieves results on par with the state of the art on several downstream tasks.\nIn addition, we show that incorporating our new variance term into other\nmethods helps stabilize the training and leads to performance improvements.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:53:21 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Bardes", "Adrien", ""], ["Ponce", "Jean", ""], ["LeCun", "Yann", ""]]}, {"id": "2105.04916", "submitter": "Yanqi Chen", "authors": "Yanqi Chen, Zhaofei Yu, Wei Fang, Tiejun Huang and Yonghong Tian", "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring", "comments": "9 pages, 7 figures, 4 tables. To appear in the 30th International\n  Joint Conference on Artificial Intelligence (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retraining. Our key innovation is to\nredefine the gradient to a new synaptic parameter, allowing better exploration\nof network structures by taking full advantage of the competition between\npruning and regrowth of connections. The experimental results show that the\nproposed method achieves minimal loss of SNNs' performance on MNIST and\nCIFAR-10 dataset so far. Moreover, it reaches a $\\sim$3.5% accuracy loss under\nunprecedented 0.73% connectivity, which reveals remarkable structure refining\ncapability in SNNs. Our work suggests that there exists extremely high\nredundancy in deep SNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:05:53 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:35:21 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 08:38:17 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chen", "Yanqi", ""], ["Yu", "Zhaofei", ""], ["Fang", "Wei", ""], ["Huang", "Tiejun", ""], ["Tian", "Yonghong", ""]]}, {"id": "2105.04920", "submitter": "Vo Nguyen Le Duy", "authors": "Vo Nguyen Le Duy, Ichiro Takeuchi", "title": "More Powerful Conditional Selective Inference for Generalized Lasso by\n  Parametric Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional selective inference (SI) has been studied intensively as a new\nstatistical inference framework for data-driven hypotheses. The basic concept\nof conditional SI is to make the inference conditional on the selection event,\nwhich enables an exact and valid statistical inference to be conducted even\nwhen the hypothesis is selected based on the data. Conditional SI has mainly\nbeen studied in the context of model selection, such as vanilla lasso or\ngeneralized lasso. The main limitation of existing approaches is the low\nstatistical power owing to over-conditioning, which is required for\ncomputational tractability. In this study, we propose a more powerful and\ngeneral conditional SI method for a class of problems that can be converted\ninto quadratic parametric programming, which includes generalized lasso. The\nkey concept is to compute the continuum path of the optimal solution in the\ndirection of the selected test statistic and to identify the subset of the data\nspace that corresponds to the model selection event by following the solution\npath. The proposed parametric programming-based method not only avoids the\naforementioned major drawback of over-conditioning, but also improves the\nperformance and practicality of SI in various respects. We conducted several\nexperiments to demonstrate the effectiveness and efficiency of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:12:00 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Duy", "Vo Nguyen Le", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2105.04922", "submitter": "Mathieu L\\'eonardon", "authors": "Mathieu L\\'eonardon and Vincent Gripon", "title": "Using Deep Neural Networks to Predict and Improve the Performance of\n  Polar Codes", "comments": "Submitted to ISTC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Polar codes can theoretically achieve very competitive Frame Error Rates. In\npractice, their performance may depend on the chosen decoding procedure, as\nwell as other parameters of the communication system they are deployed upon. As\na consequence, designing efficient polar codes for a specific context can\nquickly become challenging. In this paper, we introduce a methodology that\nconsists in training deep neural networks to predict the frame error rate of\npolar codes based on their frozen bit construction sequence. We introduce an\nalgorithm based on Projected Gradient Descent that leverages the gradient of\nthe neural network function to generate promising frozen bit sequences. We\nshowcase on generated datasets the ability of the proposed methodology to\nproduce codes more efficient than those used to train the neural networks, even\nwhen the latter are selected among the most efficient ones.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:24:51 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["L\u00e9onardon", "Mathieu", ""], ["Gripon", "Vincent", ""]]}, {"id": "2105.04944", "submitter": "Susana Nunes", "authors": "Susana Nunes, Rita T. Sousa, Catia Pesquita", "title": "Predicting Gene-Disease Associations with Knowledge Graph Embeddings\n  over Multiple Ontologies", "comments": "4 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontology-based approaches for predicting gene-disease associations include\nthe more classical semantic similarity methods and more recently knowledge\ngraph embeddings. While semantic similarity is typically restricted to\nhierarchical relations within the ontology, knowledge graph embeddings consider\ntheir full breadth. However, embeddings are produced over a single graph and\ncomplex tasks such as gene-disease association may require additional\nontologies. We investigate the impact of employing richer semantic\nrepresentations that are based on more than one ontology, able to represent\nboth genes and diseases and consider multiple kinds of relations within the\nontologies. Our experiments demonstrate the value of employing knowledge graph\nembeddings based on random-walks and highlight the need for a closer\nintegration of different ontologies.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 11:20:38 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 10:44:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Nunes", "Susana", ""], ["Sousa", "Rita T.", ""], ["Pesquita", "Catia", ""]]}, {"id": "2105.04949", "submitter": "Asahi Ushio", "authors": "Asahi Ushio and Luis Espinosa-Anke and Steven Schockaert and Jose\n  Camacho-Collados", "title": "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models\n  Identify Analogies?", "comments": "Accepted by ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analogies play a central role in human commonsense reasoning. The ability to\nrecognize analogies such as \"eye is to seeing what ear is to hearing\",\nsometimes referred to as analogical proportions, shape how we structure\nknowledge and understand language. Surprisingly, however, the task of\nidentifying such analogies has not yet received much attention in the language\nmodel era. In this paper, we analyze the capabilities of transformer-based\nlanguage models on this unsupervised task, using benchmarks obtained from\neducational settings, as well as more commonly used datasets. We find that\noff-the-shelf language models can identify analogies to a certain extent, but\nstruggle with abstract and complex relations, and results are highly sensitive\nto model architecture and hyperparameters. Overall the best results were\nobtained with GPT-2 and RoBERTa, while configurations using BERT were not able\nto outperform word embedding models. Our results raise important questions for\nfuture work about how, and to what extent, pre-trained language models capture\nknowledge about abstract semantic relations.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 11:38:49 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 10:10:38 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 16:39:21 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Ushio", "Asahi", ""], ["Espinosa-Anke", "Luis", ""], ["Schockaert", "Steven", ""], ["Camacho-Collados", "Jose", ""]]}, {"id": "2105.04962", "submitter": "Julen Urain", "authors": "Julen Urain, Anqi Li, Puze Liu, Carlo D'Eramo, Jan Peters", "title": "Composable Energy Policies for Reactive Motion Generation and\n  Reinforcement Learning", "comments": "8 pages, RSS 2021, Robotics: Science and Systems 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reactive motion generation problems are usually solved by computing actions\nas a sum of policies. However, these policies are independent of each other and\nthus, they can have conflicting behaviors when summing their contributions\ntogether. We introduce Composable Energy Policies (CEP), a novel framework for\nmodular reactive motion generation. CEP computes the control action by\noptimization over the product of a set of stochastic policies. This product of\npolicies will provide a high probability to those actions that satisfy all the\ncomponents and low probability to the others. Optimizing over the product of\nthe policies avoids the detrimental effect of conflicting behaviors between\npolicies choosing an action that satisfies all the objectives. Besides, we show\nthat CEP naturally adapts to the Reinforcement Learning problem allowing us to\nintegrate, in a hierarchical fashion, any distribution as prior, from\nmultimodal distributions to non-smooth distributions and learn a new policy\ngiven them.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 11:59:13 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Urain", "Julen", ""], ["Li", "Anqi", ""], ["Liu", "Puze", ""], ["D'Eramo", "Carlo", ""], ["Peters", "Jan", ""]]}, {"id": "2105.04963", "submitter": "Laila El-Hamamsy", "authors": "Laila El-Hamamsy, Vaios Papaspyros, Taavet Kangur, Laura Mathex,\n  Christian Giang, Melissa Skweres, Barbara Bruno, Francesco Mondada", "title": "Exploring a Handwriting Programming Language for Educational Robots", "comments": "To appear in the proceedings of the 12th International Conference on\n  Robotics in Education (RiE, 2021)", "journal-ref": null, "doi": "10.1007/978-3-030-82544-7_25", "report-no": null, "categories": "cs.PL cs.CY cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, introducing computer science and educational robots in compulsory\neducation has received increasing attention. However, the use of screens in\nclassrooms is often met with resistance, especially in primary school. To\naddress this issue, this study presents the development of a handwriting-based\nprogramming language for educational robots. Aiming to align better with\nexisting classroom practices, it allows students to program a robot by drawing\nsymbols with ordinary pens and paper. Regular smartphones are leveraged to\nprocess the hand-drawn instructions using computer vision and machine learning\nalgorithms, and send the commands to the robot for execution. To align with the\nlocal computer science curriculum, an appropriate playground and scaffolded\nlearning tasks were designed. The system was evaluated in a preliminary test\nwith eight teachers, developers and educational researchers. While the\nparticipants pointed out that some technical aspects could be improved, they\nalso acknowledged the potential of the approach to make computer science\neducation in primary school more accessible.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:00:34 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 15:50:01 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["El-Hamamsy", "Laila", ""], ["Papaspyros", "Vaios", ""], ["Kangur", "Taavet", ""], ["Mathex", "Laura", ""], ["Giang", "Christian", ""], ["Skweres", "Melissa", ""], ["Bruno", "Barbara", ""], ["Mondada", "Francesco", ""]]}, {"id": "2105.04979", "submitter": "Souvik Chakraborty", "authors": "Navaneeth N. and Souvik Chakraborty", "title": "Surrogate assisted active subspace and active subspace assisted\n  surrogate -- A new paradigm for high dimensional structural reliability\n  analysis", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing reliability analysis on complex systems is often computationally\nexpensive. In particular, when dealing with systems having high input\ndimensionality, reliability estimation becomes a daunting task. A popular\napproach to overcome the problem associated with time-consuming and expensive\nevaluations is building a surrogate model. However, these computationally\nefficient models often suffer from the curse of dimensionality. Hence, training\na surrogate model for high-dimensional problems is not straightforward.\nHenceforth, this paper presents a framework for solving high-dimensional\nreliability analysis problems. The basic premise is to train the surrogate\nmodel on a low-dimensional manifold, discovered using the active subspace\nalgorithm. However, learning the low-dimensional manifold using active subspace\nis non-trivial as it requires information on the gradient of the response\nvariable. To address this issue, we propose using sparse learning algorithms in\nconjunction with the active subspace algorithm; the resulting algorithm is\nreferred to as the sparse active subspace (SAS) algorithm. We project the\nhigh-dimensional inputs onto the identified low-dimensional manifold identified\nusing SAS. A high-fidelity surrogate model is used to map the inputs on the\nlow-dimensional manifolds to the output response. We illustrate the efficacy of\nthe proposed framework by using three benchmark reliability analysis problems\nfrom the literature. The results obtained indicate the accuracy and efficiency\nof the proposed approach compared to already established reliability analysis\nmethods in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:29:01 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 14:14:40 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["N.", "Navaneeth", ""], ["Chakraborty", "Souvik", ""]]}, {"id": "2105.04983", "submitter": "Yao Lei Xu", "authors": "Yao Lei Xu, Giuseppe G. Calvi, Danilo P. Mandic", "title": "Tensor-Train Recurrent Neural Networks for Interpretable Multi-Way\n  Financial Forecasting", "comments": "International Joint Conference on Neural Networks (IJCNN) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) represent the de facto standard machine\nlearning tool for sequence modelling, owing to their expressive power and\nmemory. However, when dealing with large dimensional data, the corresponding\nexponential increase in the number of parameters imposes a computational\nbottleneck. The necessity to equip RNNs with the ability to deal with the curse\nof dimensionality, such as through the parameter compression ability inherent\nto tensors, has led to the development of the Tensor-Train RNN (TT-RNN).\nDespite achieving promising results in many applications, the full potential of\nthe TT-RNN is yet to be explored in the context of interpretable financial\nmodelling, a notoriously challenging task characterized by multi-modal data\nwith low signal-to-noise ratio. To address this issue, we investigate the\npotential of TT-RNN in the task of financial forecasting of currencies. We\nshow, through the analysis of TT-factors, that the physical meaning underlying\ntensor decomposition, enables the TT-RNN model to aid the interpretability of\nresults, thus mitigating the notorious \"black-box\" issue associated with neural\nnetworks. Furthermore, simulation results highlight the regularization power of\nTT decomposition, demonstrating the superior performance of TT-RNN over its\nuncompressed RNN counterpart and other tensor forecasting methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:38:34 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Xu", "Yao Lei", ""], ["Calvi", "Giuseppe G.", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "2105.04991", "submitter": "Yao Lei Xu", "authors": "Bruno Scalzo Dees, Yao Lei Xu, Anthony G. Constantinides, Danilo P.\n  Mandic", "title": "Graph Theory for Metro Traffic Modelling", "comments": "International Joint Conference on Neural Networks (IJCNN) 2021. arXiv\n  admin note: text overlap with arXiv:1912.05964, arXiv:2001.00426", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unifying graph theoretic framework for the modelling of metro\ntransportation networks is proposed. This is achieved by first introducing a\nbasic graph framework for the modelling of the London underground system from a\ndiffusion law point of view. This forms a basis for the analysis of both\nstation importance and their vulnerability, whereby the concept of graph vertex\ncentrality plays a key role. We next explore k-edge augmentation of a graph\ntopology, and illustrate its usefulness both for improving the network\nrobustness and as a planning tool. Upon establishing the graph theoretic\nattributes of the underlying graph topology, we proceed to introduce models for\nprocessing data on such a metro graph. Commuter movement is shown to obey the\nFick's law of diffusion, where the graph Laplacian provides an analytical model\nfor the diffusion process of commuter population dynamics. Finally, we also\nexplore the application of modern deep learning models, such as graph neural\nnetworks and hyper-graph neural networks, as general purpose models for the\nmodelling and forecasting of underground data, especially in the context of the\nmorning and evening rush hours. Comprehensive simulations including the\npassenger in- and out-flows during the morning rush hour in London demonstrates\nthe advantages of the graph models in metro planning and traffic management, a\nformal mathematical approach with wide economic implications.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:52:52 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Dees", "Bruno Scalzo", ""], ["Xu", "Yao Lei", ""], ["Constantinides", "Anthony G.", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "2105.05001", "submitter": "Zhao Song", "authors": "Baihe Huang, Xiaoxiao Li, Zhao Song, Xin Yang", "title": "FL-NTK: A Neural Tangent Kernel-based Framework for Federated Learning\n  Convergence Analysis", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated Learning (FL) is an emerging learning scheme that allows different\ndistributed clients to train deep neural networks together without data\nsharing. Neural networks have become popular due to their unprecedented\nsuccess. To the best of our knowledge, the theoretical guarantees of FL\nconcerning neural networks with explicit forms and multi-step updates are\nunexplored. Nevertheless, training analysis of neural networks in FL is\nnon-trivial for two reasons: first, the objective loss function we are\noptimizing is non-smooth and non-convex, and second, we are even not updating\nin the gradient direction. Existing convergence results for gradient\ndescent-based methods heavily rely on the fact that the gradient direction is\nused for updating. This paper presents a new class of convergence analysis for\nFL, Federated Learning Neural Tangent Kernel (FL-NTK), which corresponds to\noverparamterized ReLU neural networks trained by gradient descent in FL and is\ninspired by the analysis in Neural Tangent Kernel (NTK). Theoretically, FL-NTK\nconverges to a global-optimal solution at a linear rate with properly tuned\nlearning parameters. Furthermore, with proper distributional assumptions,\nFL-NTK can also achieve good generalization.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:05:53 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Huang", "Baihe", ""], ["Li", "Xiaoxiao", ""], ["Song", "Zhao", ""], ["Yang", "Xin", ""]]}, {"id": "2105.05008", "submitter": "Rishiraj Saha Roy", "authors": "Khanh Hiep Tran, Azin Ghazimatin, Rishiraj Saha Roy", "title": "Counterfactual Explanations for Neural Recommenders", "comments": "SIGIR 2021 Short Paper, 5 pages", "journal-ref": null, "doi": "10.1145/3404835.3463005", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why specific items are recommended to users can significantly\nincrease their trust and satisfaction in the system. While neural recommenders\nhave become the state-of-the-art in recent years, the complexity of deep models\nstill makes the generation of tangible explanations for end users a challenging\nproblem. Existing methods are usually based on attention distributions over a\nvariety of features, which are still questionable regarding their suitability\nas explanations, and rather unwieldy to grasp for an end user. Counterfactual\nexplanations based on a small set of the user's own actions have been shown to\nbe an acceptable solution to the tangibility problem. However, current work on\nsuch counterfactuals cannot be readily applied to neural models. In this work,\nwe propose ACCENT, the first general framework for finding counterfactual\nexplanations for neural recommenders. It extends recently-proposed influence\nfunctions for identifying training points most relevant to a recommendation,\nfrom a single to a pair of items, while deducing a counterfactual set in an\niterative process. We use ACCENT to generate counterfactual explanations for\ntwo popular neural models, Neural Collaborative Filtering (NCF) and Relational\nCollaborative Filtering (RCF), and demonstrate its feasibility on a sample of\nthe popular MovieLens 100K dataset.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:16:18 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Tran", "Khanh Hiep", ""], ["Ghazimatin", "Azin", ""], ["Roy", "Rishiraj Saha", ""]]}, {"id": "2105.05019", "submitter": "Manash Pratim Das", "authors": "Manash Pratim Das, Anirudh Vemula, Mayank Pathak, Sandip Aine, Maxim\n  Likhachev", "title": "Learning Optimal Decision Making for an Industrial Truck Unloading Robot\n  using Minimal Simulator Runs", "comments": "8 pages, 8 figures, Pre-Print. This work has been submitted to the\n  IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a truck filled with boxes of varying size and unknown mass and an\nindustrial robot with end-effectors that can unload multiple boxes from any\nreachable location. In this work, we investigate how would the robot with the\nhelp of a simulator, learn to maximize the number of boxes unloaded by each\naction. Most high-fidelity robotic simulators like ours are time-consuming.\nTherefore, we investigate the above learning problem with a focus on minimizing\nthe number of simulation runs required. The optimal decision-making problem\nunder this setting can be formulated as a multi-class classification problem.\nHowever, to obtain the outcome of any action requires us to run the\ntime-consuming simulator, thereby restricting the amount of training data that\ncan be collected. Thus, we need a data-efficient approach to learn the\nclassifier and generalize it with a minimal amount of data. A high-fidelity\nphysics-based simulator is common in general for complex manipulation tasks\ninvolving multi-body interactions. To this end, we train an optimal decision\ntree as the classifier, and for each branch of the decision tree, we reason\nabout the confidence in the decision using a Probably Approximately Correct\n(PAC) framework to determine whether more simulator data will help reach a\ncertain confidence level. This provides us with a mechanism to evaluate when\nsimulation can be avoided for certain decisions, and when simulation will\nimprove the decision making. For the truck unloading problem, our experiments\nshow that a significant reduction in simulator runs can be achieved using the\nproposed method as compared to naively running the simulator to collect data to\ntrain equally performing decision trees.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 06:22:23 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Das", "Manash Pratim", ""], ["Vemula", "Anirudh", ""], ["Pathak", "Mayank", ""], ["Aine", "Sandip", ""], ["Likhachev", "Maxim", ""]]}, {"id": "2105.05026", "submitter": "Chongxuan Li", "authors": "Guoqiang Wu, Chongxuan Li, Kun Xu, Jun Zhu", "title": "Rethinking and Reweighting the Univariate Losses for Multi-Label\n  Ranking: Consistency and Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (Partial) ranking loss is a commonly used evaluation measure for multi-label\nclassification, which is usually optimized with convex surrogates for\ncomputational efficiency. Prior theoretical work on multi-label ranking mainly\nfocuses on (Fisher) consistency analyses. However, there is a gap between\nexisting theory and practice -- some pairwise losses can lead to promising\nperformance but lack consistency, while some univariate losses are consistent\nbut usually have no clear superiority in practice. In this paper, we attempt to\nfill this gap through a systematic study from two complementary perspectives of\nconsistency and generalization error bounds of learning algorithms. Our results\nshow that learning algorithms with the consistent univariate loss have an error\nbound of $O(c)$ ($c$ is the number of labels), while algorithms with the\ninconsistent pairwise loss depend on $O(\\sqrt{c})$ as shown in prior work. This\nexplains that the latter can achieve better performance than the former in\npractice. Moreover, we present an inconsistent reweighted univariate loss-based\nlearning algorithm that enjoys an error bound of $O(\\sqrt{c})$ for promising\nperformance as well as the computational efficiency of univariate losses.\nFinally, experimental results validate our theoretical analyses.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:23:27 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wu", "Guoqiang", ""], ["Li", "Chongxuan", ""], ["Xu", "Kun", ""], ["Zhu", "Jun", ""]]}, {"id": "2105.05029", "submitter": "Jing Li", "authors": "Tiangang Li", "title": "Adversarial examples attack based on random warm restart mechanism and\n  improved Nesterov momentum", "comments": "9 pages, 7 figures, 5 tables, CCS 2021 Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The deep learning algorithm has achieved great success in the field of\ncomputer vision, but some studies have pointed out that the deep learning model\nis vulnerable to attacks adversarial examples and makes false decisions. This\nchallenges the further development of deep learning, and urges researchers to\npay more attention to the relationship between adversarial examples attacks and\ndeep learning security. This work focuses on adversarial examples, optimizes\nthe generation of adversarial examples from the view of adversarial robustness,\ntakes the perturbations added in adversarial examples as the optimization\nparameter. We propose RWR-NM-PGD attack algorithm based on random warm restart\nmechanism and improved Nesterov momentum from the view of gradient\noptimization. The algorithm introduces improved Nesterov momentum, using its\ncharacteristics of accelerating convergence and improving gradient update\ndirection in optimization algorithm to accelerate the generation of adversarial\nexamples. In addition, the random warm restart mechanism is used for\noptimization, and the projected gradient descent algorithm is used to limit the\nrange of the generated perturbations in each warm restart, which can obtain\nbetter attack effect. Experiments on two public datasets show that the\nalgorithm proposed in this work can improve the success rate of attacking deep\nlearning models without extra time cost. Compared with the benchmark attack\nmethod, the algorithm proposed in this work can achieve better attack success\nrate for both normal training model and defense model. Our method has average\nattack success rate of 46.3077%, which is 27.19% higher than I-FGSM and 9.27%\nhigher than PGD. The attack results in 13 defense models show that the attack\nalgorithm proposed in this work is superior to the benchmark algorithm in\nattack universality and transferability.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 07:24:25 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Tiangang", ""]]}, {"id": "2105.05031", "submitter": "Kyriakos Flouris", "authors": "Kyriakos Flouris, Anna Volokitin, Gustav Bredell, Ender Konukoglu", "title": "Gradient flow encoding with distance optimization adaptive step size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The autoencoder model uses an encoder to map data samples to a lower\ndimensional latent space and then a decoder to map the latent space\nrepresentations back to the data space. Implicitly, it relies on the encoder to\napproximate the inverse of the decoder network, so that samples can be mapped\nto and back from the latent space faithfully. This approximation may lead to\nsub-optimal latent space representations. In this work, we investigate a\ndecoder-only method that uses gradient flow to encode data samples in the\nlatent space. The gradient flow is defined based on a given decoder and aims to\nfind the optimal latent space representation for any given sample through\noptimisation, eliminating the need of an approximate inversion through an\nencoder. Implementing gradient flow through ordinary differential equations\n(ODE), we leverage the adjoint method to train a given decoder. We further show\nempirically that the costly integrals in the adjoint method may not be entirely\nnecessary. Additionally, we propose a $2^{nd}$ order ODE variant to the method,\nwhich approximates Nesterov's accelerated gradient descent, with faster\nconvergence per iteration. Commonly used ODE solvers can be quite sensitive to\nthe integration step-size depending on the stiffness of the ODE. To overcome\nthe sensitivity for gradient flow encoding, we use an adaptive solver that\nprioritises minimising loss at each integration step. We assess the proposed\nmethod in comparison to the autoencoding model. In our experiments, GFE showed\na much higher data-efficiency than the autoencoding model, which can be crucial\nfor data scarce applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:38:23 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Flouris", "Kyriakos", ""], ["Volokitin", "Anna", ""], ["Bredell", "Gustav", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2105.05037", "submitter": "Zhongping Ji", "authors": "Zhongping Ji", "title": "BikNN: Anomaly Estimation in Bilateral Domains with k-Nearest Neighbors", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel framework for anomaly estimation is proposed. The\nbasic idea behind our method is to reduce the data into a two-dimensional space\nand then rank each data point in the reduced space. We attempt to estimate the\ndegree of anomaly in both spatial and density domains. Specifically, we\ntransform the data points into a density space and measure the distances in\ndensity domain between each point and its k-Nearest Neighbors in spatial\ndomain. Then, an anomaly coordinate system is built by collecting two\nunilateral anomalies from k-nearest neighbors of each point. Further more, we\nintroduce two schemes to model their correlation and combine them to get the\nfinal anomaly score. Experiments performed on the synthetic and real world\ndatasets demonstrate that the proposed method performs well and achieve highest\naverage performance. We also show that the proposed method can provide\nvisualization and classification of the anomalies in a simple manner. Due to\nthe complexity of the anomaly, none of the existing methods can perform best on\nall benchmark datasets. Our method takes into account both the spatial domain\nand the density domain and can be adapted to different datasets by adjusting a\nfew parameters manually.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:45:29 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ji", "Zhongping", ""]]}, {"id": "2105.05041", "submitter": "Guillermo C\\'ambara", "authors": "Guillermo C\\'ambara, Alex Peir\\'o-Lilja, Mireia Farr\\'us, Jordi Luque", "title": "English Accent Accuracy Analysis in a State-of-the-Art Automatic Speech\n  Recognition System", "comments": "2 pages, 1 figure, 1 table. To be published in Phonetics and\n  Phonology in Europe 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, research in speech technologies has gotten a lot out thanks to\nrecently created public domain corpora that contain thousands of recording\nhours. These large amounts of data are very helpful for training the new\ncomplex models based on deep learning technologies. However, the lack of\ndialectal diversity in a corpus is known to cause performance biases in speech\nsystems, mainly for underrepresented dialects. In this work, we propose to\nevaluate a state-of-the-art automatic speech recognition (ASR) deep\nlearning-based model, using unseen data from a corpus with a wide variety of\nlabeled English accents from different countries around the world. The model\nhas been trained with 44.5K hours of English speech from an open access corpus\ncalled Multilingual LibriSpeech, showing remarkable results in popular\nbenchmarks. We test the accuracy of such ASR against samples extracted from\nanother public corpus that is continuously growing, the Common Voice dataset.\nThen, we present graphically the accuracy in terms of Word Error Rate of each\nof the different English included accents, showing that there is indeed an\naccuracy bias in terms of accentual variety, favoring the accents most\nprevalent in the training corpus.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:24:33 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["C\u00e1mbara", "Guillermo", ""], ["Peir\u00f3-Lilja", "Alex", ""], ["Farr\u00fas", "Mireia", ""], ["Luque", "Jordi", ""]]}, {"id": "2105.05052", "submitter": "Yang Li", "authors": "Yang Li, Ben Athiwaratkun, Cicero Nogueira dos Santos, Bing Xiang", "title": "Joint Text and Label Generation for Spoken Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization is a central problem in machine learning, especially when data\nis limited. Using prior information to enforce constraints is the principled\nway of encouraging generalization. In this work, we propose to leverage the\nprior information embedded in pretrained language models (LM) to improve\ngeneralization for intent classification and slot labeling tasks with limited\ntraining data. Specifically, we extract prior knowledge from pretrained LM in\nthe form of synthetic data, which encode the prior implicitly. We fine-tune the\nLM to generate an augmented language, which contains not only text but also\nencodes both intent labels and slot labels. The generated synthetic data can be\nused to train a classifier later. Since the generated data may contain noise,\nwe rephrase the learning from generated data as learning with noisy labels. We\nthen utilize the mixout regularization for the classifier and prove its\neffectiveness to resist label noise in generated data. Empirically, our method\ndemonstrates superior performance and outperforms the baseline by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:02:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Yang", ""], ["Athiwaratkun", "Ben", ""], ["Santos", "Cicero Nogueira dos", ""], ["Xiang", "Bing", ""]]}, {"id": "2105.05060", "submitter": "Francesco Di Lauro Mr", "authors": "Abhishek Tomy, Matteo Razzanelli, Francesco Di Lauro, Daniela Rus,\n  Cosimo Della Santina", "title": "Estimating the State of Epidemics Spreading with Graph Neural Networks", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When an epidemic spreads into a population, it is often unpractical or\nimpossible to have a continuous monitoring of all subjects involved. As an\nalternative, algorithmic solutions can be used to infer the state of the whole\npopulation from a limited amount of measures. We analyze the capability of deep\nneural networks to solve this challenging task. Our proposed architecture is\nbased on Graph Convolutional Neural Networks. As such it can reason on the\neffect of the underlying social network structure, which is recognized as the\nmain component in the spreading of an epidemic. We test the proposed\narchitecture with two scenarios modeled on the CoVid-19 pandemic: a generic\nhomogeneous population, and a toy model of Boston metropolitan area.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:54:13 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Tomy", "Abhishek", ""], ["Razzanelli", "Matteo", ""], ["Di Lauro", "Francesco", ""], ["Rus", "Daniela", ""], ["Della Santina", "Cosimo", ""]]}, {"id": "2105.05061", "submitter": "Ujjal Kr Dutta", "authors": "Ujjal Kr Dutta, Mehrtash Harandi, Chellu Chandra Sekhar", "title": "Semi-Supervised Metric Learning: A Deep Resurrection", "comments": "In AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Distance Metric Learning (DML) seeks to learn a discriminative embedding\nwhere similar examples are closer, and dissimilar examples are apart. In this\npaper, we address the problem of Semi-Supervised DML (SSDML) that tries to\nlearn a metric using a few labeled examples, and abundantly available unlabeled\nexamples. SSDML is important because it is infeasible to manually annotate all\nthe examples present in a large dataset. Surprisingly, with the exception of a\nfew classical approaches that learn a linear Mahalanobis metric, SSDML has not\nbeen studied in the recent years, and lacks approaches in the deep SSDML\nscenario. In this paper, we address this challenging problem, and revamp SSDML\nwith respect to deep learning. In particular, we propose a stochastic,\ngraph-based approach that first propagates the affinities between the pairs of\nexamples from labeled data, to that of the unlabeled pairs. The propagated\naffinities are used to mine triplet based constraints for metric learning. We\nimpose orthogonality constraint on the metric parameters, as it leads to a\nbetter performance by avoiding a model collapse.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:28:45 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Dutta", "Ujjal Kr", ""], ["Harandi", "Mehrtash", ""], ["Sekhar", "Chellu Chandra", ""]]}, {"id": "2105.05079", "submitter": "Tahseen Khan", "authors": "Tahseen Khan, Wenhong Tian, Rajkumar Buyya", "title": "Machine Learning (ML)-Centric Resource Management in Cloud Computing: A\n  Review and Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has rapidly emerged as model for delivering Internet-based\nutility computing services. In cloud computing, Infrastructure as a Service\n(IaaS) is one of the most important and rapidly growing fields. Cloud providers\nprovide users/machines resources such as virtual machines, raw (block) storage,\nfirewalls, load balancers, and network devices in this service model. One of\nthe most important aspects of cloud computing for IaaS is resource management.\nScalability, quality of service, optimum utility, reduced overheads, increased\nthroughput, reduced latency, specialised environment, cost effectiveness, and a\nstreamlined interface are some of the advantages of resource management for\nIaaS in cloud computing. Traditionally, resource management has been done\nthrough static policies, which impose certain limitations in various dynamic\nscenarios, prompting cloud service providers to adopt data-driven,\nmachine-learning-based approaches. Machine learning is being used to handle a\nvariety of resource management tasks, including workload estimation, task\nscheduling, VM consolidation, resource optimization, and energy optimization,\namong others. This paper provides a detailed review of challenges in ML-based\nresource management in current research, as well as current approaches to\nresolve these challenges, as well as their advantages and limitations. Finally,\nwe propose potential future research directions based on identified challenges\nand limitations in current research.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:03:58 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Khan", "Tahseen", ""], ["Tian", "Wenhong", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2105.05094", "submitter": "Damiano Brunori", "authors": "Damiano Brunori, Stefania Colonnese, Francesca Cuomo and Luca Iocchi", "title": "A Reinforcement Learning Environment for Multi-Service UAV-enabled\n  Wireless Systems", "comments": null, "journal-ref": "2021 IEEE International Conference on Pervasive Computing and\n  Communications Workshops and other Affiliated Events (PerCom Workshops)", "doi": "10.1109/PerComWorkshops51409.2021.9431048", "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a multi-purpose environment for autonomous UAVs offering different\ncommunication services in a variety of application contexts (e.g., wireless\nmobile connectivity services, edge computing, data gathering). We develop the\nenvironment, based on OpenAI Gym framework, in order to simulate different\ncharacteristics of real operational environments and we adopt the Reinforcement\nLearning to generate policies that maximize some desired performance.The\nquality of the resulting policies are compared with a simple baseline to\nevaluate the system and derive guidelines to adopt this technique in different\nuse cases. The main contribution of this paper is a flexible and extensible\nOpenAI Gym environment, which allows to generate, evaluate, and compare\npolicies for autonomous multi-drone systems in multi-service applications. This\nenvironment allows for comparative evaluation and benchmarking of different\napproaches in a variety of application contexts.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:45:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Brunori", "Damiano", ""], ["Colonnese", "Stefania", ""], ["Cuomo", "Francesca", ""], ["Iocchi", "Luca", ""]]}, {"id": "2105.05115", "submitter": "Dominik Schr\\\"oder", "authors": "Vanessa Piccolo and Dominik Schr\\\"oder", "title": "Analysis of One-Hidden-Layer Neural Networks via the Resolvent Method", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute the asymptotic empirical spectral distribution of a non-linear\nrandom matrix model by using the resolvent method. Motivated by random neural\nnetworks, we consider the random matrix $M = Y Y^\\ast$ with $Y = f(WX)$, where\n$W$ and $X$ are random rectangular matrices with i.i.d. centred entries and $f$\nis a non-linear smooth function which is applied entry-wise. We prove that the\nStieltjes transform of the limiting spectral distribution satisfies a quartic\nself-consistent equation up to some error terms, which is exactly the equation\nobtained by [Pennington, Worah] and [Benigni, P\\'{e}ch\\'{e}] with the moment\nmethod approach. In addition, we extend the previous results to the case of\nadditive bias $Y=f(WX+B)$ with $B$ being an independent rank-one Gaussian\nrandom matrix, closer modelling the neural network infrastructures encountering\nin practice. Our approach following the \\emph{resolvent method} is more robust\nthan the moment method and is expected to provide insights also for models\nwhere the combinatorics of the latter become intractable.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 15:17:39 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Piccolo", "Vanessa", ""], ["Schr\u00f6der", "Dominik", ""]]}, {"id": "2105.05125", "submitter": "Yong Huang", "authors": "Yeji Wang, Shuo Wu, Yanwen Duan, Yong Huang", "title": "ResAtom System: Protein and Ligand Affinity Prediction Model Based on\n  Deep Learning", "comments": "28 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Protein-ligand affinity prediction is an important part of\nstructure-based drug design. It includes molecular docking and affinity\nprediction. Although molecular dynamics can predict affinity with high accuracy\nat present, it is not suitable for large-scale virtual screening. The existing\naffinity prediction and evaluation functions based on deep learning mostly rely\non experimentally-determined conformations. Results: We build a predictive\nmodel of protein-ligand affinity through the ResNet neural network with added\nattention mechanism. The resulting ResAtom-Score model achieves Pearson's\ncorrelation coefficient R = 0.833 on the CASF-2016 benchmark test set. At the\nsame time, we evaluated the performance of a variety of existing scoring\nfunctions in combination with ResAtom-Score in the absence of\nexperimentally-determined conformations. The results show that the use of\n{\\Delta}VinaRF20 in combination with ResAtom-Score can achieve affinity\nprediction close to scoring functions in the presence of\nexperimentally-determined conformations. These results suggest that ResAtom\nsystem may be used for in silico screening of small molecule ligands with\ntarget proteins in the future. Availability: https://github.com/wyji001/ResAtom\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:37:10 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Yeji", ""], ["Wu", "Shuo", ""], ["Duan", "Yanwen", ""], ["Huang", "Yong", ""]]}, {"id": "2105.05130", "submitter": "Li Wang", "authors": "Li Wang", "title": "Towards a Model for LSH", "comments": "arXiv admin note: text overlap with arXiv:2103.01888", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As data volumes continue to grow, clustering and outlier detection algorithms\nare becoming increasingly time-consuming. Classical index structures for\nneighbor search are no longer sustainable due to the \"curse of dimensionality\".\nInstead, approximated index structures offer a good opportunity to\nsignificantly accelerate the neighbor search for clustering and outlier\ndetection and to have the lowest possible error rate in the results of the\nalgorithms. Locality-sensitive hashing is one of those. We indicate directions\nto model the properties of LSH.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 15:39:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Li", ""]]}, {"id": "2105.05145", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Yuhang Hu, Robert Kwiatkowski, Shuran Song, Hod Lipson", "title": "Visual Perspective Taking for Opponent Behavior Modeling", "comments": "ICRA 2021. Website: http://www.cs.columbia.edu/~bchen/vpttob/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to engage in complex social interaction, humans learn at a young age\nto infer what others see and cannot see from a different point-of-view, and\nlearn to predict others' plans and behaviors. These abilities have been mostly\nlacking in robots, sometimes making them appear awkward and socially inept.\nHere we propose an end-to-end long-term visual prediction framework for robots\nto begin to acquire both these critical cognitive skills, known as Visual\nPerspective Taking (VPT) and Theory of Behavior (TOB). We demonstrate our\napproach in the context of visual hide-and-seek - a game that represents a\ncognitive milestone in human development. Unlike traditional visual predictive\nmodel that generates new frames from immediate past frames, our agent can\ndirectly predict to multiple future timestamps (25s), extrapolating by 175%\nbeyond the training horizon. We suggest that visual behavior modeling and\nperspective taking skills will play a critical role in the ability of physical\nrobots to fully integrate into real-world multi-agent activities. Our website\nis at http://www.cs.columbia.edu/~bchen/vpttob/.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:02:32 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chen", "Boyuan", ""], ["Hu", "Yuhang", ""], ["Kwiatkowski", "Robert", ""], ["Song", "Shuran", ""], ["Lipson", "Hod", ""]]}, {"id": "2105.05146", "submitter": "Mouloud Belbahri", "authors": "Mouloud Belbahri, Olivier Gandouet, Alejandro Murua and Vahid Partovi\n  Nia", "title": "A Twin Neural Model for Uplift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Uplift is a particular case of conditional treatment effect modeling. Such\nmodels deal with cause-and-effect inference for a specific factor, such as a\nmarketing intervention or a medical treatment. In practice, these models are\nbuilt on individual data from randomized clinical trials where the goal is to\npartition the participants into heterogeneous groups depending on the uplift.\nMost existing approaches are adaptations of random forests for the uplift case.\nSeveral split criteria have been proposed in the literature, all relying on\nmaximizing heterogeneity. However, in practice, these approaches are prone to\noverfitting. In this work, we bring a new vision to uplift modeling. We propose\na new loss function defined by leveraging a connection with the Bayesian\ninterpretation of the relative risk. Our solution is developed for a specific\ntwin neural network architecture allowing to jointly optimize the marginal\nprobabilities of success for treated and control individuals. We show that this\nmodel is a generalization of the uplift logistic interaction model. We modify\nthe stochastic gradient descent algorithm to allow for structured sparse\nsolutions. This helps training our uplift models to a great extent. We show our\nproposed method is competitive with the state-of-the-art in simulation setting\nand on real data from large scale randomized experiments.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:02:39 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Belbahri", "Mouloud", ""], ["Gandouet", "Olivier", ""], ["Murua", "Alejandro", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "2105.05155", "submitter": "Pranshu Malviya", "authors": "Pranshu Malviya, Sarath Chandar, Balaraman Ravindran", "title": "TAG: Task-based Accumulated Gradients for Lifelong learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an agent encounters a continual stream of new tasks in the lifelong\nlearning setting, it leverages the knowledge it gained from the earlier tasks\nto help learn the new tasks better. In such a scenario, identifying an\nefficient knowledge representation becomes a challenging problem. Most research\nworks propose to either store a subset of examples from the past tasks in a\nreplay buffer, dedicate a separate set of parameters to each task or penalize\nexcessive updates over parameters by introducing a regularization term. While\nexisting methods employ the general task-agnostic stochastic gradient descent\nupdate rule, we propose a task-aware optimizer that adapts the learning rate\nbased on the relatedness among tasks. We utilize the directions taken by the\nparameters during the updates by accumulating the gradients specific to each\ntask. These task-based accumulated gradients act as a knowledge base that is\nmaintained and updated throughout the stream. We empirically show that our\nproposed adaptive learning rate not only accounts for catastrophic forgetting\nbut also allows positive backward transfer. We also show that our method\nperforms better than several state-of-the-art methods in lifelong learning on\ncomplex datasets with a large number of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:10:32 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 04:55:21 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Malviya", "Pranshu", ""], ["Chandar", "Sarath", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "2105.05165", "submitter": "Rameswar Panda", "authors": "Rameswar Panda, Chun-Fu Chen, Quanfu Fan, Ximeng Sun, Kate Saenko,\n  Aude Oliva, Rogerio Feris", "title": "AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal learning, which focuses on utilizing various modalities to\nimprove the performance of a model, is widely used in video recognition. While\ntraditional multi-modal learning offers excellent recognition results, its\ncomputational expense limits its impact for many real-world applications. In\nthis paper, we propose an adaptive multi-modal learning framework, called\nAdaMML, that selects on-the-fly the optimal modalities for each segment\nconditioned on the input for efficient video recognition. Specifically, given a\nvideo segment, a multi-modal policy network is used to decide what modalities\nshould be used for processing by the recognition model, with the goal of\nimproving both accuracy and efficiency. We efficiently train the policy network\njointly with the recognition model using standard back-propagation. Extensive\nexperiments on four challenging diverse datasets demonstrate that our proposed\nadaptive approach yields 35%-55% reduction in computation when compared to the\ntraditional baseline that simply uses all the modalities irrespective of the\ninput, while also achieving consistent improvements in accuracy over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:19:07 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 17:49:10 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Panda", "Rameswar", ""], ["Chen", "Chun-Fu", ""], ["Fan", "Quanfu", ""], ["Sun", "Ximeng", ""], ["Saenko", "Kate", ""], ["Oliva", "Aude", ""], ["Feris", "Rogerio", ""]]}, {"id": "2105.05173", "submitter": "Chin Chun Ooi", "authors": "Quang Tuyen Le, Pao-Hsiung Chiu, Chin Chun Ooi", "title": "U-Net-Based Surrogate Model For Evaluation of Microfluidic Channels", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Microfluidics have shown great promise in multiple applications, especially\nin biomedical diagnostics and separations. While the flow properties of these\nmicrofluidic devices can be solved by numerical methods such as computational\nfluid dynamics (CFD), the process of mesh generation and setting up a numerical\nsolver requires some domain familiarity, while more intuitive commercial\nprograms such as Fluent and StarCCM can be expensive. Hence, in this work, we\ndemonstrated the use of a U-Net convolutional neural network as a surrogate\nmodel for predicting the velocity and pressure fields that would result for a\nparticular set of microfluidic filter designs. The surrogate model is fast,\neasy to set-up and can be used to predict and assess the flow velocity and\npressure fields across the domain for new designs of interest via the input of\na geometry-encoding matrix. In addition, we demonstrate that the same\nmethodology can also be used to train a network to predict pressure based on\nvelocity data, and propose that this can be an alternative to numerical\nalgorithms for calculating pressure based on velocity measurements from\nparticle-image velocimetry measurements. Critically, in both applications, we\ndemonstrate prediction test errors of less than 1%, suggesting that this is\nindeed a viable method.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:27:58 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Le", "Quang Tuyen", ""], ["Chiu", "Pao-Hsiung", ""], ["Ooi", "Chin Chun", ""]]}, {"id": "2105.05180", "submitter": "Antonious Girgis", "authors": "Antonious M. Girgis, Deepesh Data, Suhas Diggavi, Ananda Theertha\n  Suresh, and Peter Kairouz", "title": "On the Renyi Differential Privacy of the Shuffle Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central question studied in this paper is Renyi Differential Privacy\n(RDP) guarantees for general discrete local mechanisms in the shuffle privacy\nmodel. In the shuffle model, each of the $n$ clients randomizes its response\nusing a local differentially private (LDP) mechanism and the untrusted server\nonly receives a random permutation (shuffle) of the client responses without\nassociation to each client. The principal result in this paper is the first\nnon-trivial RDP guarantee for general discrete local randomization mechanisms\nin the shuffled privacy model, and we develop new analysis techniques for\nderiving our results which could be of independent interest. In applications,\nsuch an RDP guarantee is most useful when we use it for composing several\nprivate interactions. We numerically demonstrate that, for important regimes,\nwith composition our bound yields an improvement in privacy guarantee by a\nfactor of $8\\times$ over the state-of-the-art approximate Differential Privacy\n(DP) guarantee (with standard composition) for shuffled models. Moreover,\ncombining with Poisson subsampling, our result leads to at least $10\\times$\nimprovement over subsampled approximate DP with standard composition.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:34:09 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Girgis", "Antonious M.", ""], ["Data", "Deepesh", ""], ["Diggavi", "Suhas", ""], ["Suresh", "Ananda Theertha", ""], ["Kairouz", "Peter", ""]]}, {"id": "2105.05181", "submitter": "Anthony LaTorre", "authors": "Anthony LaTorre", "title": "Factoring Multidimensional Data to Create a Sophisticated Bayes\n  Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we derive an explicit formula for calculating the marginal\nlikelihood of a given factorization of a categorical dataset. Since the\nmarginal likelihood is proportional to the posterior probability of the\nfactorization, these likelihoods can be used to order all possible\nfactorizations and select the \"best\" way to factor the overall distribution\nfrom which the dataset is drawn. The best factorization can then be used to\nconstruct a Bayes classifier which benefits from factoring out mutually\nindependent sets of variables.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:34:12 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 16:29:10 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["LaTorre", "Anthony", ""]]}, {"id": "2105.05197", "submitter": "Batuhan Hangun", "authors": "Onder Eyecioglu, Batuhan Hangun, Korhan Kayisli, Mehmet Yesilbudak", "title": "Performance Comparison of Different Machine Learning Algorithms on the\n  Prediction of Wind Turbine Power Generation", "comments": "2019 8th International Conference on Renewable Energy Research and\n  Applications (ICRERA)", "journal-ref": null, "doi": "10.1109/ICRERA47325.2019.8996541", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decade, wind energy has gained more attention in the world.\nHowever, owing to its indirectness and volatility properties, wind power\npenetration has increased the difficulty and complexity in dispatching and\nplanning of electric power systems. Therefore, it is needed to make the\nhigh-precision wind power prediction in order to balance the electrical power.\nFor this purpose, in this study, the prediction performance of linear\nregression, k-nearest neighbor regression and decision tree regression\nalgorithms is compared in detail. k-nearest neighbor regression algorithm\nprovides lower coefficient of determination values, while decision tree\nregression algorithm produces lower mean absolute error values. In addition,\nthe meteorological parameters of wind speed, wind direction, barometric\npressure and air temperature are evaluated in terms of their importance on the\nwind power parameter. The biggest importance factor is achieved by wind speed\nparameter. In consequence, many useful assessments are made for wind power\npredictions.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:02:24 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Eyecioglu", "Onder", ""], ["Hangun", "Batuhan", ""], ["Kayisli", "Korhan", ""], ["Yesilbudak", "Mehmet", ""]]}, {"id": "2105.05212", "submitter": "Abdesslem Layeb", "authors": "Abdesslem Layeb", "title": "Two novel feature selection algorithms based on crowding distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two novel algorithms for features selection are proposed. The\nfirst one is a filter method while the second is wrapper method. Both the\nproposed algorithms use the crowding distance used in the multiobjective\noptimization as a metric in order to sort the features. The less crowded\nfeatures have great effects on the target attribute (class). The experimental\nresults have shown the effectiveness and the robustness of the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:27:56 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 15:19:50 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 15:16:46 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Layeb", "Abdesslem", ""]]}, {"id": "2105.05222", "submitter": "Kayo Yin", "authors": "Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, Malihe\n  Alikhani", "title": "Including Signed Languages in Natural Language Processing", "comments": "ACL 2021 Best Theme Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signed languages are the primary means of communication for many deaf and\nhard of hearing individuals. Since signed languages exhibit all the fundamental\nlinguistic properties of natural language, we believe that tools and theories\nof Natural Language Processing (NLP) are crucial towards its modeling. However,\nexisting research in Sign Language Processing (SLP) seldom attempt to explore\nand leverage the linguistic organization of signed languages. This position\npaper calls on the NLP community to include signed languages as a research area\nwith high social and scientific impact. We first discuss the linguistic\nproperties of signed languages to consider during their modeling. Then, we\nreview the limitations of current SLP models and identify the open challenges\nto extend NLP to signed languages. Finally, we urge (1) the adoption of an\nefficient tokenization method; (2) the development of linguistically-informed\nmodels; (3) the collection of real-world signed language data; (4) the\ninclusion of local signed language communities as an active and leading voice\nin the direction of research.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:37:55 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 19:12:46 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yin", "Kayo", ""], ["Moryossef", "Amit", ""], ["Hochgesang", "Julie", ""], ["Goldberg", "Yoav", ""], ["Alikhani", "Malihe", ""]]}, {"id": "2105.05227", "submitter": "Yu Guo", "authors": "Yu Guo", "title": "Doing Natural Language Processing in A Natural Way: An NLP toolkit based\n  on object-oriented knowledge base and multi-level grammar base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an NLP toolkit based on object-oriented knowledge base and\nmulti-level grammar base. This toolkit focuses on semantic parsing, it also has\nabilities to discover new knowledge and grammar automatically, new discovered\nknowledge and grammar will be identified by human, and will be used to update\nthe knowledge base and grammar base. This process can be iterated many times to\nimprove the toolkit continuously.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:43:06 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 03:15:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Guo", "Yu", ""]]}, {"id": "2105.05228", "submitter": "Huy Tuan Pham", "authors": "Huy Tuan Pham, Phan-Minh Nguyen", "title": "Global Convergence of Three-layer Neural Networks in the Mean Field\n  Regime", "comments": "Appear in ICLR 2021. This is the conference version of\n  arXiv:2001.11443 (which contains treatment of the multilayer neural nets and\n  their global convergence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the mean field regime, neural networks are appropriately scaled so that as\nthe width tends to infinity, the learning dynamics tends to a nonlinear and\nnontrivial dynamical limit, known as the mean field limit. This lends a way to\nstudy large-width neural networks via analyzing the mean field limit. Recent\nworks have successfully applied such analysis to two-layer networks and\nprovided global convergence guarantees. The extension to multilayer ones\nhowever has been a highly challenging puzzle, and little is known about the\noptimization efficiency in the mean field regime when there are more than two\nlayers.\n  In this work, we prove a global convergence result for unregularized\nfeedforward three-layer networks in the mean field regime. We first develop a\nrigorous framework to establish the mean field limit of three-layer networks\nunder stochastic gradient descent training. To that end, we propose the idea of\na \\textit{neuronal embedding}, which comprises of a fixed probability space\nthat encapsulates neural networks of arbitrary sizes. The identified mean field\nlimit is then used to prove a global convergence guarantee under suitable\nregularity and convergence mode assumptions, which -- unlike previous works on\ntwo-layer networks -- does not rely critically on convexity. Underlying the\nresult is a universal approximation property, natural of neural networks, which\nimportantly is shown to hold at \\textit{any} finite training time (not\nnecessarily at convergence) via an algebraic topology argument.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:45:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Pham", "Huy Tuan", ""], ["Nguyen", "Phan-Minh", ""]]}, {"id": "2105.05233", "submitter": "Prafulla Dhariwal", "authors": "Prafulla Dhariwal, Alex Nichol", "title": "Diffusion Models Beat GANs on Image Synthesis", "comments": "Added compute requirements, ImageNet 256$\\times$256 upsampling FID\n  and samples, DDIM guided sampler, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that diffusion models can achieve image sample quality superior to\nthe current state-of-the-art generative models. We achieve this on\nunconditional image synthesis by finding a better architecture through a series\nof ablations. For conditional image synthesis, we further improve sample\nquality with classifier guidance: a simple, compute-efficient method for\ntrading off diversity for fidelity using gradients from a classifier. We\nachieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet\n256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep\neven with as few as 25 forward passes per sample, all while maintaining better\ncoverage of the distribution. Finally, we find that classifier guidance\ncombines well with upsampling diffusion models, further improving FID to 3.94\non ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our\ncode at https://github.com/openai/guided-diffusion\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:50:24 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 17:57:59 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 17:57:08 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 17:49:49 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Dhariwal", "Prafulla", ""], ["Nichol", "Alex", ""]]}, {"id": "2105.05236", "submitter": "Pg Madhavan", "authors": "PG Madhavan", "title": "Stochastic Formulation of Causal Digital Twin: Kalman Filter Algorithm", "comments": "arXiv admin note: text overlap with arXiv:2104.05828", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide some basic and sensible definitions of different types of digital\ntwins and recommendations on when and how to use them. Following up on our\nrecent publication of the Learning Causal Digital Twin, this article reports on\na stochastic formulation and solution of the problem. Structural Vector\nAutoregressive Model (SVAR) for Causal estimation is recast as a state-space\nmodel. Kalman filter (and smoother) is then employed to estimate causal factors\nin a system of connected machine bearings. The previous neural network\nalgorithm and Kalman Smoother produced very similar results; however, Kalman\nFilter/Smoother may show better performance for noisy data from industrial IoT\nsources.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:51:50 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Madhavan", "PG", ""]]}, {"id": "2105.05241", "submitter": "Jack Bandy", "authors": "Jack Bandy, Nicholas Vincent", "title": "Addressing \"Documentation Debt\" in Machine Learning Research: A\n  Retrospective Datasheet for BookCorpus", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature has underscored the importance of dataset documentation\nwork for machine learning, and part of this work involves addressing\n\"documentation debt\" for datasets that have been used widely but documented\nsparsely. This paper aims to help address documentation debt for BookCorpus, a\npopular text dataset for training large language models. Notably, researchers\nhave used BookCorpus to train OpenAI's GPT-N models and Google's BERT models,\neven though little to no documentation exists about the dataset's motivation,\ncomposition, collection process, etc. We offer a preliminary datasheet that\nprovides key context and information about BookCorpus, highlighting several\nnotable deficiencies. In particular, we find evidence that (1) BookCorpus\nlikely violates copyright restrictions for many books, (2) BookCorpus contains\nthousands of duplicated books, and (3) BookCorpus exhibits significant skews in\ngenre representation. We also find hints of other potential deficiencies that\ncall for future research, including problematic content, potential skews in\nreligious representation, and lopsided author contributions. While more work\nremains, this initial effort to provide a datasheet for BookCorpus adds to\ngrowing literature that urges more careful and systematic documentation for\nmachine learning datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:59:23 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Bandy", "Jack", ""], ["Vincent", "Nicholas", ""]]}, {"id": "2105.05246", "submitter": "Florin Gogianu", "authors": "Florin Gogianu and Tudor Berariu, Mihaela Rosca, Claudia Clopath,\n  Lucian Busoniu, Razvan Pascanu", "title": "Spectral Normalisation for Deep Reinforcement Learning: an Optimisation\n  Perspective", "comments": "Accepted at ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of the recent deep reinforcement learning advances take an RL-centric\nperspective and focus on refinements of the training objective. We diverge from\nthis view and show we can recover the performance of these developments not by\nchanging the objective, but by regularising the value-function estimator.\nConstraining the Lipschitz constant of a single layer using spectral\nnormalisation is sufficient to elevate the performance of a Categorical-DQN\nagent to that of a more elaborated \\rainbow{} agent on the challenging Atari\ndomain. We conduct ablation studies to disentangle the various effects\nnormalisation has on the learning dynamics and show that is sufficient to\nmodulate the parameter updates to recover most of the performance of spectral\nnormalisation. These findings hint towards the need to also focus on the neural\ncomponent and its learning dynamics to tackle the peculiarities of Deep\nReinforcement Learning.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:59:46 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Gogianu", "Florin", ""], ["Berariu", "Tudor", ""], ["Rosca", "Mihaela", ""], ["Clopath", "Claudia", ""], ["Busoniu", "Lucian", ""], ["Pascanu", "Razvan", ""]]}, {"id": "2105.05275", "submitter": "Federico L\\'opez", "authors": "Federico L\\'opez, Beatrice Pozzetti, Steve Trettel, Anna Wienhard", "title": "Hermitian Symmetric Spaces for Graph Embeddings", "comments": "13 pages, 1 figure. Accepted at NeurIPS 2020 workshop on Differential\n  Geometry meets Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning faithful graph representations as sets of vertex embeddings has\nbecome a fundamental intermediary step in a wide range of machine learning\napplications. The quality of the embeddings is usually determined by how well\nthe geometry of the target space matches the structure of the data. In this\nwork we learn continuous representations of graphs in spaces of symmetric\nmatrices over C. These spaces offer a rich geometry that simultaneously admits\nhyperbolic and Euclidean subspaces, and are amenable to analysis and explicit\ncomputations. We implement an efficient method to learn embeddings and compute\ndistances, and develop the tools to operate with such spaces. The proposed\nmodels are able to automatically adapt to very dissimilar arrangements without\nany apriori estimates of graph features. On various datasets with very diverse\nstructural properties and reconstruction measures our model ties the results of\ncompetitive baselines for geometrically pure graphs and outperforms them for\ngraphs with mixed geometric features, showcasing the versatility of our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 18:14:52 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["L\u00f3pez", "Federico", ""], ["Pozzetti", "Beatrice", ""], ["Trettel", "Steve", ""], ["Wienhard", "Anna", ""]]}, {"id": "2105.05278", "submitter": "Minghao Guo", "authors": "Minghao Guo, Wan Shou, Liane Makatura, Timothy Erps, Michael Foshey,\n  Wojciech Matusik", "title": "Polygrammar: Grammar for Digital Polymer Representation and Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cond-mat.mtrl-sci cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Polymers are widely-studied materials with diverse properties and\napplications determined by different molecular structures. It is essential to\nrepresent these structures clearly and explore the full space of achievable\nchemical designs. However, existing approaches are unable to offer\ncomprehensive design models for polymers because of their inherent scale and\nstructural complexity. Here, we present a parametric, context-sensitive grammar\ndesigned specifically for the representation and generation of polymers. As a\ndemonstrative example, we implement our grammar for polyurethanes. Using our\nsymbolic hypergraph representation and 14 simple production rules, our\nPolyGrammar is able to represent and generate all valid polyurethane\nstructures. We also present an algorithm to translate any polyurethane\nstructure from the popular SMILES string format into our PolyGrammar\nrepresentation. We test the representative power of PolyGrammar by translating\na dataset of over 600 polyurethane samples collected from literature.\nFurthermore, we show that PolyGrammar can be easily extended to the other\ncopolymers and homopolymers such as polyacrylates. By offering a complete,\nexplicit representation scheme and an explainable generative model with\nvalidity guarantees, our PolyGrammar takes an important step toward a more\ncomprehensive and practical system for polymer discovery and exploration. As\nthe first bridge between formal languages and chemistry, PolyGrammar also\nserves as a critical blueprint to inform the design of similar grammars for\nother chemistries, including organic and inorganic molecules.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:43:29 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Guo", "Minghao", ""], ["Shou", "Wan", ""], ["Makatura", "Liane", ""], ["Erps", "Timothy", ""], ["Foshey", "Michael", ""], ["Matusik", "Wojciech", ""]]}, {"id": "2105.05303", "submitter": "Thomas Sawczuk", "authors": "Thomas Sawczuk, Anna Palczewska and Ben Jones", "title": "Development of an expected possession value model to analyse team\n  attacking performances in rugby league", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study aimed to provide a framework to evaluate team attacking\nperformances in rugby league using 59,233 plays from 180 Super League matches\nvia expected possession value (EPV) models. The EPV-308 split the pitch into\n308 5m x 5m zones, the EPV-77 split the pitch into 77 10m x 10m zones and the\nEPV-19 split the pitch in 19 zones of variable size dependent on the total zone\nvalue generated during a match. Attacking possessions were considered as Markov\nChains, allowing the value of each zone visited to be estimated based on the\noutcome of the possession. The Kullback-Leibler Divergence was used to evaluate\nthe reproducibility of the value generated from each zone (the reward\ndistribution) by teams between matches. The EPV-308 had the greatest\nvariability and lowest reproducibility, compared to EPV-77 and EPV-19. When six\nprevious matches were considered, the team's subsequent match attacking\nperformances had a similar reward distribution for EPV-19, EPV-77 and EPV-308\non 95 +/- 4%, 51 +/- 12% and 0 +/- 0% of occasions. This study supports the use\nof EPV-19 to evaluate team attacking performance in rugby league and provides a\nsimple framework through which attacking performances can be compared between\nteams.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:54:12 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sawczuk", "Thomas", ""], ["Palczewska", "Anna", ""], ["Jones", "Ben", ""]]}, {"id": "2105.05316", "submitter": "Martin Atzmueller", "authors": "Stefan Bloemheuvel, Jurgen van den Hoogen, Martin Atzmueller", "title": "A Computational Framework for Modeling Complex Sensor Network Data Using\n  Graph Signal Processing and Graph Neural Networks in Structural Health\n  Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks lend themselves to the modeling of multidimensional data,\nsuch as relational and/or temporal data. In particular, when such complex data\nand their inherent relationships need to be formalized, complex network\nmodeling and its resulting graph representations enable a wide range of\npowerful options. In this paper, we target this - connected to specific machine\nlearning approaches on graphs for structural health monitoring on an analysis\nand predictive (maintenance) perspective. Specifically, we present a framework\nbased on Complex Network Modeling, integrating Graph Signal Processing (GSP)\nand Graph Neural Network (GNN) approaches. We demonstrate this framework in our\ntargeted application domain of Structural Health Monitoring (SHM). In\nparticular, we focus on a prominent real-world structural health monitoring use\ncase, i.e., modeling and analyzing sensor data (strain, vibration) of a large\nbridge in the Netherlands. In our experiments, we show that GSP enables the\nidentification of the most important sensors, for which we investigate a set of\nsearch and optimization approaches. Furthermore, GSP enables the detection of\nspecific graph signal patterns (mode shapes), capturing physical functional\nproperties of the sensors in the applied complex network. In addition, we show\nthe efficacy of applying GNNs for strain prediction on this kind of data.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 10:45:57 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Bloemheuvel", "Stefan", ""], ["Hoogen", "Jurgen van den", ""], ["Atzmueller", "Martin", ""]]}, {"id": "2105.05318", "submitter": "Youssef Skandarani", "authors": "Youssef Skandarani, Pierre-Marc Jodoin, Alain Lalande", "title": "GANs for Medical Image Synthesis: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have become increasingly powerful,\ngenerating mind-blowing photorealistic images that mimic the content of\ndatasets they were trained to replicate. One recurrent theme in medical imaging\nis whether GANs can also be effective at generating workable medical data as\nthey are for generating realistic RGB images. In this paper, we perform a\nmulti-GAN and multi-application study to gauge the benefits of GANs in medical\nimaging. We tested various GAN architectures from basic DCGAN to more\nsophisticated style-based GANs on three medical imaging modalities and organs\nnamely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\nwell-known and widely utilized datasets from which their FID score were\ncomputed to measure the visual acuity of their generated images. We further\ntested their usefulness by measuring the segmentation accuracy of a U-Net\ntrained on these generated images.\n  Results reveal that GANs are far from being equal as some are ill-suited for\nmedical imaging applications while others are much better off. The\ntop-performing GANs are capable of generating realistic-looking medical images\nby FID standards that can fool trained experts in a visual Turing test and\ncomply to some metrics. However, segmentation results suggests that no GAN is\ncapable of reproducing the full richness of a medical datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 19:21:39 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 11:51:58 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Skandarani", "Youssef", ""], ["Jodoin", "Pierre-Marc", ""], ["Lalande", "Alain", ""]]}, {"id": "2105.05320", "submitter": "Yiming Wang", "authors": "Yiming Wang, Dongxia Chang, Zhiqian Fu, and Yao Zhao", "title": "Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been considerable research interest in graph clustering\naimed at data partition using the graph information. However, one limitation of\nthe most of graph-based methods is that they assume the graph structure to\noperate is fixed and reliable. And there are inevitably some edges in the graph\nthat are not conducive to graph clustering, which we call spurious edges. This\npaper is the first attempt to employ graph pooling technique for node\nclustering and we propose a novel dual graph embedding network (DGEN), which is\ndesigned as a two-step graph encoder connected by a graph pooling layer to\nlearn the graph embedding. In our model, it is assumed that if a node and its\nnearest neighboring node are close to the same clustering center, this node is\nan informative node and this edge can be considered as a cluster-friendly edge.\nBased on this assumption, the neighbor cluster pooling (NCPool) is devised to\nselect the most informative subset of nodes and the corresponding edges based\non the distance of nodes and their nearest neighbors to the cluster centers.\nThis can effectively alleviate the impact of the spurious edges on the\nclustering. Finally, to obtain the clustering assignment of all nodes, a\nclassifier is trained using the clustering results of the selected nodes.\nExperiments on five benchmark graph datasets demonstrate the superiority of the\nproposed method over state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 06:51:51 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 02:51:53 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Yiming", ""], ["Chang", "Dongxia", ""], ["Fu", "Zhiqian", ""], ["Zhao", "Yao", ""]]}, {"id": "2105.05326", "submitter": "Cheng Qian", "authors": "Cheng Qian, Nikos Kargas, Cao Xiao, Lucas Glass, Nicholas\n  Sidiropoulos, Jimeng Sun", "title": "Multi-version Tensor Completion for Time-delayed Spatio-temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world spatio-temporal data is often incomplete or inaccurate due to\nvarious data loading delays. For example, a location-disease-time tensor of\ncase counts can have multiple delayed updates of recent temporal slices for\nsome locations or diseases. Recovering such missing or noisy (under-reported)\nelements of the input tensor can be viewed as a generalized tensor completion\nproblem. Existing tensor completion methods usually assume that i) missing\nelements are randomly distributed and ii) noise for each tensor element is\ni.i.d. zero-mean. Both assumptions can be violated for spatio-temporal tensor\ndata. We often observe multiple versions of the input tensor with different\nunder-reporting noise levels. The amount of noise can be time- or\nlocation-dependent as more updates are progressively introduced to the tensor.\nWe model such dynamic data as a multi-version tensor with an extra tensor mode\ncapturing the data updates. We propose a low-rank tensor model to predict the\nupdates over time. We demonstrate that our method can accurately predict the\nground-truth values of many real-world tensors. We obtain up to 27.2% lower\nroot mean-squared-error compared to the best baseline method. Finally, we\nextend our method to track the tensor data over time, leading to significant\ncomputational savings.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 19:55:56 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Qian", "Cheng", ""], ["Kargas", "Nikos", ""], ["Xiao", "Cao", ""], ["Glass", "Lucas", ""], ["Sidiropoulos", "Nicholas", ""], ["Sun", "Jimeng", ""]]}, {"id": "2105.05328", "submitter": "Jack Dunn", "authors": "Jack Dunn, Luca Mingardi, Ying Daisy Zhuo", "title": "Comparing interpretability and explainability for feature selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach for feature selection is to examine the variable importance\nscores for a machine learning model, as a way to understand which features are\nthe most relevant for making predictions. Given the significance of feature\nselection, it is crucial for the calculated importance scores to reflect\nreality. Falsely overestimating the importance of irrelevant features can lead\nto false discoveries, while underestimating importance of relevant features may\nlead us to discard important features, resulting in poor model performance.\nAdditionally, black-box models like XGBoost provide state-of-the art predictive\nperformance, but cannot be easily understood by humans, and thus we rely on\nvariable importance scores or methods for explainability like SHAP to offer\ninsight into their behavior.\n  In this paper, we investigate the performance of variable importance as a\nfeature selection method across various black-box and interpretable machine\nlearning methods. We compare the ability of CART, Optimal Trees, XGBoost and\nSHAP to correctly identify the relevant subset of variables across a number of\nexperiments. The results show that regardless of whether we use the native\nvariable importance method or SHAP, XGBoost fails to clearly distinguish\nbetween relevant and irrelevant features. On the other hand, the interpretable\nmethods are able to correctly and efficiently identify irrelevant features, and\nthus offer significantly better performance for feature selection.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 20:01:23 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Dunn", "Jack", ""], ["Mingardi", "Luca", ""], ["Zhuo", "Ying Daisy", ""]]}, {"id": "2105.05330", "submitter": "Md Kamruzzaman Sarker", "authors": "Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, Pascal Hitzler", "title": "Neuro-Symbolic Artificial Intelligence: Current Trends", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuro-Symbolic Artificial Intelligence -- the combination of symbolic methods\nwith methods that are based on artificial neural networks -- has a\nlong-standing history. In this article, we provide a structured overview of\ncurrent trends, by means of categorizing recent publications from key\nconferences. The article is meant to serve as a convenient starting point for\nresearch on the general topic.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 20:11:57 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 05:08:12 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Sarker", "Md Kamruzzaman", ""], ["Zhou", "Lu", ""], ["Eberhart", "Aaron", ""], ["Hitzler", "Pascal", ""]]}, {"id": "2105.05345", "submitter": "Jacob Carse Mr", "authors": "Jacob Carse, Frank Carey, Stephen McKenna", "title": "Unsupervised Representation Learning from Pathology Images with\n  Multi-directional Contrastive Predictive Coding", "comments": "5 pages, 4 figures, presented at IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital pathology tasks have benefited greatly from modern deep learning\nalgorithms. However, their need for large quantities of annotated data has been\nidentified as a key challenge. This need for data can be countered by using\nunsupervised learning in situations where data are abundant but access to\nannotations is limited. Feature representations learned from unannotated data\nusing contrastive predictive coding (CPC) have been shown to enable classifiers\nto obtain state of the art performance from relatively small amounts of\nannotated computer vision data. We present a modification to the CPC framework\nfor use with digital pathology patches. This is achieved by introducing an\nalternative mask for building the latent context and using a multi-directional\nPixelCNN autoregressor. To demonstrate our proposed method we learn feature\nrepresentations from the Patch Camelyon histology dataset. We show that our\nproposed modification can yield improved deep classification of histology\npatches.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 21:17:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Carse", "Jacob", ""], ["Carey", "Frank", ""], ["McKenna", "Stephen", ""]]}, {"id": "2105.05347", "submitter": "Tom Schaul", "authors": "Tom Schaul, Georg Ostrovski, Iurii Kemaev, Diana Borsa", "title": "Return-based Scaling: Yet Another Normalisation Trick for Deep RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scaling issues are mundane yet irritating for practitioners of reinforcement\nlearning. Error scales vary across domains, tasks, and stages of learning;\nsometimes by many orders of magnitude. This can be detrimental to learning\nspeed and stability, create interference between learning tasks, and\nnecessitate substantial tuning. We revisit this topic for agents based on\ntemporal-difference learning, sketch out some desiderata and investigate\nscenarios where simple fixes fall short. The mechanism we propose requires\nneither tuning, clipping, nor adaptation. We validate its effectiveness and\nrobustness on the suite of Atari games. Our scaling method turns out to be\nparticularly helpful at mitigating interference, when training a shared neural\nnetwork on multiple targets that differ in reward scale or discounting.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 21:31:02 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Schaul", "Tom", ""], ["Ostrovski", "Georg", ""], ["Kemaev", "Iurii", ""], ["Borsa", "Diana", ""]]}, {"id": "2105.05358", "submitter": "Jimeng Shi", "authors": "Jimeng Shi, Cheng-Xian Lin", "title": "Computational Simulation and Analysis of Major Control Parameters of\n  Time-Dependent PV/T Collectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to improve performance of photovoltaic/thermal (or PV/T for\nsimplicity) collectors, this paper firstly validated a previous computational\nthermal model and then introduced an improved computational thermal model to\ninvestigate the effects of the major control parameters on the thermal\nperformance of PV/T collectors, including solar cell temperature, back surface\ntemperature, and outlet water temperature. Besides, a computational electrical\nmodel of PV/T system was also introduced to elaborate the relationship of\nvoltage, current and power of a PV module (MSX60 polycrystalline solar cell)\nused in an experiment in the literature. Simulation results agree with the\nexperimental data very well. The effects of the time-steps from 1 hour to\nminute, which is closed to the real time, were also reported. At last, several\nsuggestions to improve the efficiency of PV/T system were illustrated.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 02:09:19 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Shi", "Jimeng", ""], ["Lin", "Cheng-Xian", ""]]}, {"id": "2105.05361", "submitter": "Philippe Laban", "authors": "Philippe Laban, Andrew Hsi, John Canny, Marti A. Hearst", "title": "The Summary Loop: Learning to Write Abstractive Summaries Without\n  Examples", "comments": "ACL2020, 16 pages, 9 figures", "journal-ref": "Association for Computational Linguistics (2020) 5135-5150", "doi": "10.18653/v1/2020.acl-main.460", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new approach to unsupervised abstractive summarization\nbased on maximizing a combination of coverage and fluency for a given length\nconstraint. It introduces a novel method that encourages the inclusion of key\nterms from the original document into the summary: key terms are masked out of\nthe original document and must be filled in by a coverage model using the\ncurrent generated summary. A novel unsupervised training procedure leverages\nthis coverage model along with a fluency model to generate and score summaries.\nWhen tested on popular news summarization datasets, the method outperforms\nprevious unsupervised methods by more than 2 R-1 points, and approaches results\nof competitive supervised methods. Our model attains higher levels of\nabstraction with copied passages roughly two times shorter than prior work, and\nlearns to compress and merge sentences without supervision.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 23:19:46 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Laban", "Philippe", ""], ["Hsi", "Andrew", ""], ["Canny", "John", ""], ["Hearst", "Marti A.", ""]]}, {"id": "2105.05381", "submitter": "Shahbaz Rezaei", "authors": "Shahbaz Rezaei, Zubair Shafiq, Xin Liu", "title": "Accuracy-Privacy Trade-off in Deep Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep ensemble learning has been shown to improve accuracy by training\nmultiple neural networks and fusing their outputs. Ensemble learning has also\nbeen used to defend against membership inference attacks that undermine\nprivacy. In this paper, we empirically demonstrate a trade-off between these\ntwo goals, namely accuracy and privacy (in terms of membership inference\nattacks), in deep ensembles. Using a wide range of datasets and model\narchitectures, we show that the effectiveness of membership inference attacks\nalso increases when ensembling improves accuracy. To better understand this\ntrade-off, we study the impact of various factors such as prediction confidence\nand agreement between models that constitute the ensemble. Finally, we evaluate\ndefenses against membership inference attacks based on regularization and\ndifferential privacy. We show that while these defenses can mitigate the\neffectiveness of the membership inference attack, they simultaneously degrade\nensemble accuracy. The source code is available at\nhttps://github.com/shrezaei/MI-on-EL.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 00:58:04 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 16:48:18 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Rezaei", "Shahbaz", ""], ["Shafiq", "Zubair", ""], ["Liu", "Xin", ""]]}, {"id": "2105.05400", "submitter": "Jimmy Aronsson", "authors": "Jimmy Aronsson", "title": "Homogeneous vector bundles and $G$-equivariant convolutional neural\n  networks", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.RT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  $G$-equivariant convolutional neural networks (GCNNs) is a geometric deep\nlearning model for data defined on a homogeneous $G$-space $\\mathcal{M}$. GCNNs\nare designed to respect the global symmetry in $\\mathcal{M}$, thereby\nfacilitating learning. In this paper, we analyze GCNNs on homogeneous spaces\n$\\mathcal{M} = G/K$ in the case of unimodular Lie groups $G$ and compact\nsubgroups $K \\leq G$. We demonstrate that homogeneous vector bundles is the\nnatural setting for GCNNs. We also use reproducing kernel Hilbert spaces to\nobtain a precise criterion for expressing $G$-equivariant layers as\nconvolutional layers. This criterion is then rephrased as a bandwidth\ncriterion, leading to even stronger results for some groups.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 02:06:04 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Aronsson", "Jimmy", ""]]}, {"id": "2105.05432", "submitter": "Lai Wei", "authors": "Lai Wei, Ryan McCloy and Jie Bao", "title": "Discrete-time Contraction-based Control of Nonlinear Systems with\n  Parametric Uncertainties using Neural Networks", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Flexible manufacturing in the process industry requires control systems to\nachieve time-varying setpoints (e.g., product specifications) based on market\ndemand. Contraction theory provides a useful framework for\nreference-independent system analysis and tracking control for nonlinear\nsystems. However, determination of the control contraction metrics and control\nlaws can be very difficult for general nonlinear systems. This work develops an\napproach to discrete-time contraction analysis and control using neural\nnetworks. The methodology involves training a neural network to learn a\ncontraction metric and feedback gain. The resulting contraction-based\ncontroller embeds the trained neural network and is capable of achieving\nefficient tracking of time-varying references, with a full range of model\nuncertainty, without the need for controller structure redesign. This is a\nrobust approach that can deal with bounded parametric uncertainties in the\nprocess model, which are commonly encountered in industrial (chemical)\nprocesses. Simulation examples are provided to illustrate the above approach.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 05:07:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wei", "Lai", ""], ["McCloy", "Ryan", ""], ["Bao", "Jie", ""]]}, {"id": "2105.05449", "submitter": "Amir Ahooye Atashin", "authors": "Majid Mohammadi, Amir Ahooye Atashin, Damian A. Tamburri", "title": "An efficient projection neural network for $\\ell_1$-regularized logistic\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\ell_1$ regularization has been used for logistic regression to circumvent\nthe overfitting and use the estimated sparse coefficient for feature selection.\nHowever, the challenge of such a regularization is that the $\\ell_1$ norm is\nnot differentiable, making the standard algorithms for convex optimization not\napplicable to this problem. This paper presents a simple projection neural\nnetwork for $\\ell_1$-regularized logistics regression. In contrast to many\navailable solvers in the literature, the proposed neural network does not\nrequire any extra auxiliary variable nor any smooth approximation, and its\ncomplexity is almost identical to that of the gradient descent for logistic\nregression without $\\ell_1$ regularization, thanks to the projection operator.\nWe also investigate the convergence of the proposed neural network by using the\nLyapunov theory and show that it converges to a solution of the problem with\nany arbitrary initial value. The proposed neural solution significantly\noutperforms state-of-the-art methods with respect to the execution time and is\ncompetitive in terms of accuracy and AUROC.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 06:13:44 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Mohammadi", "Majid", ""], ["Atashin", "Amir Ahooye", ""], ["Tamburri", "Damian A.", ""]]}, {"id": "2105.05458", "submitter": "Xiaolu Wang Mr", "authors": "Xiaolu Wang, Yuen-Man Pun, Anthony Man-Cho So", "title": "Learning Graphs from Smooth Signals under Moment Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inferring the graph structure from a given set of\nsmooth graph signals. The number of perceived graph signals is always finite\nand possibly noisy, thus the statistical properties of the data distribution is\nambiguous. Traditional graph learning models do not take this distributional\nuncertainty into account, thus performance may be sensitive to different sets\nof data. In this paper, we propose a distributionally robust approach to graph\nlearning, which incorporates the first and second moment uncertainty into the\nsmooth graph learning model. Specifically, we cast our graph learning model as\na minimax optimization problem, and further reformulate it as a nonconvex\nminimization problem with linear constraints. In our proposed formulation, we\nfind a theoretical interpretation of the Laplacian regularizer, which is\nadopted in many existing works in an intuitive manner. Although the first\nmoment uncertainty leads to an annoying square root term in the objective\nfunction, we prove that it enjoys the smoothness property with probability 1\nover the entire constraint. We develop a efficient projected gradient descent\n(PGD) method and establish its global iterate convergence to a critical point.\nWe conduct extensive experiments on both synthetic and real data to verify the\neffectiveness of our model and the efficiency of the PGD algorithm. Compared\nwith the state-of-the-art smooth graph learning methods, our approach exhibits\nsuperior and more robust performance across different populations of signals in\nterms of various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 06:47:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wang", "Xiaolu", ""], ["Pun", "Yuen-Man", ""], ["So", "Anthony Man-Cho", ""]]}, {"id": "2105.05473", "submitter": "Chenyang Xi", "authors": "Chenyang Xi, Bo Tang, Jiajun Shen, Xinfu Liu, Feiyu Xiong, Xueying Li", "title": "Interpretable performance analysis towards offline reinforcement\n  learning: A dataset perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline reinforcement learning (RL) has increasingly become the focus of the\nartificial intelligent research due to its wide real-world applications where\nthe collection of data may be difficult, time-consuming, or costly. In this\npaper, we first propose a two-fold taxonomy for existing offline RL algorithms\nfrom the perspective of exploration and exploitation tendency. Secondly, we\nderive the explicit expression of the upper bound of extrapolation error and\nexplore the correlation between the performance of different types of\nalgorithms and the distribution of actions under states. Specifically, we relax\nthe strict assumption on the sufficiently large amount of state-action tuples.\nAccordingly, we provably explain why batch constrained Q-learning (BCQ)\nperforms better than other existing techniques. Thirdly, after identifying the\nweakness of BCQ on dataset of low mean episode returns, we propose a modified\nvariant based on top return selection mechanism, which is proved to be able to\ngain state-of-the-art performance on various datasets. Lastly, we create a\nbenchmark platform on the Atari domain, entitled RL easy go (RLEG), at an\nestimated cost of more than 0.3 million dollars. We make it open-source for\nfair and comprehensive competitions between offline RL algorithms with complete\ndatasets and checkpoints being provided.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 07:17:06 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Xi", "Chenyang", ""], ["Tang", "Bo", ""], ["Shen", "Jiajun", ""], ["Liu", "Xinfu", ""], ["Xiong", "Feiyu", ""], ["Li", "Xueying", ""]]}, {"id": "2105.05482", "submitter": "Wagner Gon\\c{c}alves Pinto", "authors": "Wagner Gon\\c{c}alves Pinto, Antonio Alguacil and Micha\\\"el Bauerheim", "title": "On the reproducibility of fully convolutional neural networks for\n  modeling time-space evolving physical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.flu-dyn", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reproducibility of a deep-learning fully convolutional neural network is\nevaluated by training several times the same network on identical conditions\n(database, hyperparameters, hardware) with non-deterministic Graphics\nProcessings Unit (GPU) operations. The propagation of two-dimensional acoustic\nwaves, typical of time-space evolving physical systems, is studied on both\nrecursive and non-recursive tasks. Significant changes in models properties\n(weights, featured fields) are observed. When tested on various propagation\nbenchmarks, these models systematically returned estimations with a high level\nof deviation, especially for the recurrent analysis which strongly amplifies\nvariability due to the non-determinism. Trainings performed with double\nfloating-point precision provide slightly better estimations and a significant\nreduction of the variability of both the network parameters and its testing\nerror range.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 07:39:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pinto", "Wagner Gon\u00e7alves", ""], ["Alguacil", "Antonio", ""], ["Bauerheim", "Micha\u00ebl", ""]]}, {"id": "2105.05489", "submitter": "Shumao Zhang", "authors": "Shumao Zhang, Pengchuan Zhang, Thomas Y. Hou", "title": "Multiscale Invertible Generative Networks for High-Dimensional Bayesian\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Multiscale Invertible Generative Network (MsIGN) and associated\ntraining algorithm that leverages multiscale structure to solve\nhigh-dimensional Bayesian inference. To address the curse of dimensionality,\nMsIGN exploits the low-dimensional nature of the posterior, and generates\nsamples from coarse to fine scale (low to high dimension) by iteratively\nupsampling and refining samples. MsIGN is trained in a multi-stage manner to\nminimize the Jeffreys divergence, which avoids mode dropping in\nhigh-dimensional cases. On two high-dimensional Bayesian inverse problems, we\nshow superior performance of MsIGN over previous approaches in posterior\napproximation and multiple mode capture. On the natural image synthesis task,\nMsIGN achieves superior performance in bits-per-dimension over baseline models\nand yields great interpret-ability of its neurons in intermediate layers.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 07:51:47 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhang", "Shumao", ""], ["Zhang", "Pengchuan", ""], ["Hou", "Thomas Y.", ""]]}, {"id": "2105.05495", "submitter": "Meenakshi D'Souza", "authors": "Aritra Bhowmick, Meenakshi D'Souza, G. Srinivasa Raghavan", "title": "LipBaB: Computing exact Lipschitz constant of ReLU networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Lipschitz constant of neural networks plays an important role in several\ncontexts of deep learning ranging from robustness certification and\nregularization to stability analysis of systems with neural network\ncontrollers. Obtaining tight bounds of the Lipschitz constant is therefore\nimportant. We introduce LipBaB, a branch and bound framework to compute\ncertified bounds of the local Lipschitz constant of deep neural networks with\nReLU activation functions up to any desired precision. We achieve this by\nbounding the norm of the Jacobians, corresponding to different activation\npatterns of the network caused within the input domain. Our algorithm can\nprovide provably exact computation of the Lipschitz constant for any p-norm.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:06:11 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 06:25:21 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bhowmick", "Aritra", ""], ["D'Souza", "Meenakshi", ""], ["Raghavan", "G. Srinivasa", ""]]}, {"id": "2105.05498", "submitter": "Gyubok Lee", "authors": "Gyubok Lee, Seongjun Yang, Edward Choi", "title": "Improving Lexically Constrained Neural Machine Translation with\n  Source-Conditioned Masked Span Prediction", "comments": "To appear in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate terminology translation is crucial for ensuring the practicality and\nreliability of neural machine translation (NMT) systems. To address this,\nlexically constrained NMT explores various methods to ensure pre-specified\nwords and phrases appear in the translation output. However, in many cases,\nthose methods are studied on general domain corpora, where the terms are mostly\nuni- and bi-grams (>98%). In this paper, we instead tackle a more challenging\nsetup consisting of domain-specific corpora with much longer n-gram and highly\nspecialized terms. Inspired by the recent success of masked span prediction\nmodels, we propose a simple and effective training strategy that achieves\nconsistent improvements on both terminology and sentence-level translation for\nthree domain-specific corpora in two language pairs.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:11:33 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 10:59:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lee", "Gyubok", ""], ["Yang", "Seongjun", ""], ["Choi", "Edward", ""]]}, {"id": "2105.05504", "submitter": "Hyunwook Lee", "authors": "Hyunwook Lee, Cheonbok Park, Seungmin Jin, Hyeshin Chu, Jaegul Choo,\n  Sungahn Ko", "title": "An Empirical Experiment on Deep Learning Models for Predicting Traffic\n  Data", "comments": "6 pages, 3 figures, accepted at 37th IEEE International Conference on\n  Data Engineering (ICDE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To tackle ever-increasing city traffic congestion problems, researchers have\nproposed deep learning models to aid decision-makers in the traffic control\ndomain. Although the proposed models have been remarkably improved in recent\nyears, there are still questions that need to be answered before deploying\nmodels. For example, it is difficult to figure out which models provide\nstate-of-the-art performance, as recently proposed models have often been\nevaluated with different datasets and experiment environments. It is also\ndifficult to determine which models would work when traffic conditions change\nabruptly (e.g., rush hour). In this work, we conduct two experiments to answer\nthe two questions. In the first experiment, we conduct an experiment with the\nstate-of-the-art models and the identical public datasets to compare model\nperformance under a consistent experiment environment. We then extract a set of\ntemporal regions in the datasets, whose speeds change abruptly and use these\nregions to explore model performance with difficult intervals. The experiment\nresults indicate that Graph-WaveNet and GMAN show better performance in\ngeneral. We also find that prediction models tend to have varying performances\nwith data and intervals, which calls for in-depth analysis of models on\ndifficult intervals for real-world deployment.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:28:12 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Lee", "Hyunwook", ""], ["Park", "Cheonbok", ""], ["Jin", "Seungmin", ""], ["Chu", "Hyeshin", ""], ["Choo", "Jaegul", ""], ["Ko", "Sungahn", ""]]}, {"id": "2105.05530", "submitter": "Wenshuo Li", "authors": "Wenshuo Li, Hanting Chen, Mingqiang Huang, Xinghao Chen, Chunjing Xu,\n  Yunhe Wang", "title": "Winograd Algorithm for AdderNet", "comments": "9 pages, accepted by ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adder neural network (AdderNet) is a new kind of deep model that replaces the\noriginal massive multiplications in convolutions by additions while preserving\nthe high performance. Since the hardware complexity of additions is much lower\nthan that of multiplications, the overall energy consumption is thus reduced\nsignificantly. To further optimize the hardware overhead of using AdderNet,\nthis paper studies the winograd algorithm, which is a widely used fast\nalgorithm for accelerating convolution and saving the computational costs.\nUnfortunately, the conventional Winograd algorithm cannot be directly applied\nto AdderNets since the distributive law in multiplication is not valid for the\nl1-norm. Therefore, we replace the element-wise multiplication in the Winograd\nequation by additions and then develop a new set of transform matrixes that can\nenhance the representation ability of output features to maintain the\nperformance. Moreover, we propose the l2-to-l1 training strategy to mitigate\nthe negative impacts caused by formal inconsistency. Experimental results on\nboth FPGA and benchmarks show that the new method can further reduce the energy\nconsumption without affecting the accuracy of the original AdderNet.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:13:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Li", "Wenshuo", ""], ["Chen", "Hanting", ""], ["Huang", "Mingqiang", ""], ["Chen", "Xinghao", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "2105.05539", "submitter": "Robin Thibaut Mr", "authors": "Robin Thibaut, Eric Laloy, Thomas Hermans", "title": "A new framework for experimental design using Bayesian Evidential\n  Learning: the case of wellhead protection area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this contribution, we predict the wellhead protection area (WHPA, target),\nthe shape and extent of which is influenced by the distribution of hydraulic\nconductivity (K), from a small number of tracing experiments (predictor). Our\nfirst objective is to make stochastic predictions of the WHPA within the\nBayesian Evidential Learning (BEL) framework, which aims to find a direct\nrelationship between predictor and target using machine learning. This\nrelationship is learned from a small set of training models (400) sampled from\nthe prior distribution of K. The associated 400 pairs of simulated predictors\nand targets are obtained through forward modelling. Newly collected field data\ncan then be directly used to predict the approximate posterior distribution of\nthe corresponding WHPA. The uncertainty range of the posterior WHPA\ndistribution is affected by the number and position of data sources (injection\nwells). Our second objective is to extend BEL to identify the optimal design of\ndata source locations that minimizes the posterior uncertainty of the WHPA.\nThis can be done explicitly, without averaging or approximating because once\ntrained, the BEL model allows the computation of the posterior uncertainty\ncorresponding to any new input data. We use the Modified Hausdorff Distance and\nthe Structural Similarity index metrics to estimate the posterior uncertainty\nrange of the WHPA. Increasing the number of injection wells effectively reduces\nthe derived posterior WHPA uncertainty. Our approach can also estimate which\ninjection wells are more informative than others, as validated through a k-fold\ncross-validation procedure. Overall, the application of BEL to experimental\ndesign makes it possible to identify the data sources maximizing the\ninformation content of any measurement data.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:40:28 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Thibaut", "Robin", ""], ["Laloy", "Eric", ""], ["Hermans", "Thomas", ""]]}, {"id": "2105.05540", "submitter": "Min Ye", "authors": "Xiangyu Chen and Min Ye", "title": "Cyclically Equivariant Neural Decoders for Cyclic Codes", "comments": "Accepted for long presentation at ICML 2021. Code available at\n  https://github.com/cyclicallyneuraldecoder/CyclicallyEquivariantNeuralDecoders", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural decoders were introduced as a generalization of the classic Belief\nPropagation (BP) decoding algorithms, where the Trellis graph in the BP\nalgorithm is viewed as a neural network, and the weights in the Trellis graph\nare optimized by training the neural network. In this work, we propose a novel\nneural decoder for cyclic codes by exploiting their cyclically invariant\nproperty. More precisely, we impose a shift invariant structure on the weights\nof our neural decoder so that any cyclic shift of inputs results in the same\ncyclic shift of outputs. Extensive simulations with BCH codes and punctured\nReed-Muller (RM) codes show that our new decoder consistently outperforms\nprevious neural decoders when decoding cyclic codes. Finally, we propose a list\ndecoding procedure that can significantly reduce the decoding error probability\nfor BCH codes and punctured RM codes. For certain high-rate codes, the gap\nbetween our list decoder and the Maximum Likelihood decoder is less than\n$0.1$dB. Code available at\nhttps://github.com/cyclicallyneuraldecoder/CyclicallyEquivariantNeuralDecoders\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:41:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Chen", "Xiangyu", ""], ["Ye", "Min", ""]]}, {"id": "2105.05541", "submitter": "Manan Dey", "authors": "Shanya Sharma, Manan Dey and Koustuv Sinha", "title": "Evaluating Gender Bias in Natural Language Inference", "comments": "NeurIPS 2020 Workshop on Dataset Curation and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gender-bias stereotypes have recently raised significant ethical concerns in\nnatural language processing. However, progress in detection and evaluation of\ngender bias in natural language understanding through inference is limited and\nrequires further investigation. In this work, we propose an evaluation\nmethodology to measure these biases by constructing a challenge task that\ninvolves pairing gender-neutral premises against a gender-specific hypothesis.\nWe use our challenge task to investigate state-of-the-art NLI models on the\npresence of gender stereotypes using occupations. Our findings suggest that\nthree models (BERT, RoBERTa, BART) trained on MNLI and SNLI datasets are\nsignificantly prone to gender-induced prediction errors. We also find that\ndebiasing techniques such as augmenting the training dataset to ensure a\ngender-balanced dataset can help reduce such bias in certain cases.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:41:51 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sharma", "Shanya", ""], ["Dey", "Manan", ""], ["Sinha", "Koustuv", ""]]}, {"id": "2105.05553", "submitter": "Guy Hacohen", "authors": "Guy Hacohen and Daphna Weinshall", "title": "Principal Components Bias in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work suggests that convolutional neural networks of different\narchitectures learn to classify images in the same order. To understand this\nphenomenon, we revisit the over-parametrized deep linear network model. Our\nasymptotic analysis, assuming that the hidden layers are wide enough, reveals\nthat the convergence rate of this model's parameters is exponentially faster\nalong directions corresponding to the larger principal components of the data,\nat a rate governed by the singular values. We term this convergence pattern the\nPrincipal Components bias (PC-bias). We show how the PC-bias streamlines the\norder of learning of both linear and non-linear networks, more prominently at\nearlier stages of learning. We then compare our results to the spectral bias,\nshowing that both biases can be seen independently, and affect the order of\nlearning in different ways. Finally, we discuss how the PC-bias may explain\nsome benefits of early stopping and its connection to PCA, and why deep\nnetworks converge more slowly when given random labels.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:08:42 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 07:41:49 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 08:07:59 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Hacohen", "Guy", ""], ["Weinshall", "Daphna", ""]]}, {"id": "2105.05555", "submitter": "Honghao Lin", "authors": "Yu Cheng and Honghao Lin", "title": "Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning Bayesian networks where an\n$\\epsilon$-fraction of the samples are adversarially corrupted. We focus on the\nfully-observable case where the underlying graph structure is known. In this\nwork, we present the first nearly-linear time algorithm for this problem with a\ndimension-independent error guarantee. Previous robust algorithms with\ncomparable error guarantees are slower by at least a factor of $(d/\\epsilon)$,\nwhere $d$ is the number of variables in the Bayesian network and $\\epsilon$ is\nthe fraction of corrupted samples.\n  Our algorithm and analysis are considerably simpler than those in previous\nwork. We achieve this by establishing a direct connection between robust\nlearning of Bayesian networks and robust mean estimation. As a subroutine in\nour algorithm, we develop a robust mean estimation algorithm whose runtime is\nnearly-linear in the number of nonzeros in the input samples, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:11:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cheng", "Yu", ""], ["Lin", "Honghao", ""]]}, {"id": "2105.05559", "submitter": "Hans Weytjens", "authors": "Hans Weytjens and Jochen De Weerdt", "title": "Learning Uncertainty with Artificial Neural Networks for Improved\n  Remaining Time Prediction of Business Processes", "comments": "Accepted for the main conference at the Business Process Management\n  Conferences 2021, 6-10 September 2021, Rome, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural networks will always make a prediction, even when\ncompletely uncertain and regardless of the consequences. This obliviousness of\nuncertainty is a major obstacle towards their adoption in practice. Techniques\nexist, however, to estimate the two major types of uncertainty: model\nuncertainty and observation noise in the data. Bayesian neural networks are\ntheoretically well-founded models that can learn the model uncertainty of their\npredictions. Minor modifications to these models and their loss functions allow\nlearning the observation noise for individual samples as well. This paper is\nthe first to apply these techniques to predictive process monitoring. We found\nthat they contribute towards more accurate predictions and work quickly.\nHowever, their main benefit resides with the uncertainty estimates themselves\nthat allow the separation of higher-quality from lower-quality predictions and\nthe building of confidence intervals. This leads to many interesting\napplications, enables an earlier adoption of prediction systems with smaller\ndatasets and fosters a better cooperation with humans.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:18:57 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Weytjens", "Hans", ""], ["De Weerdt", "Jochen", ""]]}, {"id": "2105.05564", "submitter": "Nikolaos Nomikos Dr.", "authors": "Nikolaos Nomikos, Spyros Zoupanos, Themistoklis Charalambous, Ioannis\n  Krikidis, Athina Petropulu", "title": "A Survey on Reinforcement Learning-Aided Caching in Mobile Edge Networks", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mobile networks are experiencing tremendous increase in data volume and user\ndensity. An efficient technique to alleviate this issue is to bring the data\ncloser to the users by exploiting the caches of edge network nodes, such as\nfixed or mobile access points and even user devices. Meanwhile, the fusion of\nmachine learning and wireless networks offers a viable way for network\noptimization as opposed to traditional optimization approaches which incur high\ncomplexity, or fail to provide optimal solutions. Among the various machine\nlearning categories, reinforcement learning operates in an online and\nautonomous manner without relying on large sets of historical data for\ntraining. In this survey, reinforcement learning-aided mobile edge caching is\npresented, aiming at highlighting the achieved network gains over conventional\ncaching approaches. Taking into account the heterogeneity of sixth generation\n(6G) networks in various wireless settings, such as fixed, vehicular and flying\nnetworks, learning-aided edge caching is presented, departing from traditional\narchitectures. Furthermore, a categorization according to the desirable\nperformance metric, such as spectral, energy and caching efficiency, average\ndelay, and backhaul and fronthaul offloading is provided. Finally, several open\nissues are discussed, targeting to stimulate further interest in this important\nresearch field.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:30:56 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 09:04:15 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 18:57:45 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Nomikos", "Nikolaos", ""], ["Zoupanos", "Spyros", ""], ["Charalambous", "Themistoklis", ""], ["Krikidis", "Ioannis", ""], ["Petropulu", "Athina", ""]]}, {"id": "2105.05566", "submitter": "Casper Gyurik", "authors": "Casper Gyurik, Dyon van Vreumingen, and Vedran Dunjko", "title": "Structural risk minimization for quantum linear classifiers", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum machine learning (QML) stands out as one of the typically highlighted\ncandidates for quantum computing's near-term \"killer application\". In this\ncontext, QML models based on parameterized quantum circuits comprise a family\nof machine learning models that are well suited for implementations on\nnear-term devices and that can potentially harness computational powers beyond\nwhat is efficiently achievable on a classical computer. However, how to best\nuse these models -- e.g., how to control their expressivity to best balance\nbetween training accuracy and generalization performance -- is far from\nunderstood. In this paper we investigate capacity measures of two closely\nrelated QML models called explicit and implicit quantum linear classifiers\n(also called the quantum variational method and quantum kernel estimator) with\nthe objective of identifying new ways to implement structural risk minimization\n-- i.e., how to balance between training accuracy and generalization\nperformance. In particular, we identify that the rank and Frobenius norm of the\nobservables used in the QML model closely control the model's capacity.\nAdditionally, we theoretically investigate the effect that these model\nparameters have on the training accuracy of the QML model. Specifically, we\nshow that there exists datasets that require a high-rank observable for correct\nclassification, and that there exists datasets that can only be classified with\na given margin using an observable of at least a certain Frobenius norm. Our\nresults provide new options for performing structural risk minimization for QML\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:39:55 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Gyurik", "Casper", ""], ["van Vreumingen", "Dyon", ""], ["Dunjko", "Vedran", ""]]}, {"id": "2105.05582", "submitter": "Bertrand Higy", "authors": "Bertrand Higy, Lieke Gelderloos, Afra Alishahi and Grzegorz\n  Chrupa{\\l}a", "title": "Discrete representations in neural models of spoken language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The distributed and continuous representations used by neural networks are at\nodds with representations employed in linguistics, which are typically\nsymbolic. Vector quantization has been proposed as a way to induce discrete\nneural representations that are closer in nature to their linguistic\ncounterparts. However, it is not clear which metrics are the best-suited to\nanalyze such discrete representations. We compare the merits of four commonly\nused metrics in the context of weakly supervised models of spoken language. We\nperform a systematic analysis of the impact of (i) architectural choices, (ii)\nthe learning objective and training dataset, and (iii) the evaluation metric.\nWe find that the different evaluation metrics can give inconsistent results. In\nparticular, we find that the use of minimal pairs of phoneme triples as stimuli\nduring evaluation disadvantages larger embeddings, unlike metrics applied to\ncomplete utterances.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:02:02 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Higy", "Bertrand", ""], ["Gelderloos", "Lieke", ""], ["Alishahi", "Afra", ""], ["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "2105.05596", "submitter": "Ziheng Zhang", "authors": "Zhiyuan Qi, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Yuejia Xiang, Ningyu\n  Zhang, Yefeng Zheng", "title": "Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and\n  Semantic Embedding", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Graph (KG) alignment is to discover the mappings (i.e., equivalent\nentities, relations, and others) between two KGs. The existing methods can be\ndivided into the embedding-based models, and the conventional reasoning and\nlexical matching based systems. The former compute the similarity of entities\nvia their cross-KG embeddings, but they usually rely on an ideal supervised\nlearning setting for good performance and lack appropriate reasoning to avoid\nlogically wrong mappings; while the latter address the reasoning issue but are\npoor at utilizing the KG graph structures and the entity contexts. In this\nstudy, we aim at combining the above two solutions and thus propose an\niterative framework named PRASE which is based on probabilistic reasoning and\nsemantic embedding. It learns the KG embeddings via entity mappings from a\nprobabilistic reasoning system named PARIS, and feeds the resultant entity\nmappings and embeddings back into PARIS for augmentation. The PRASE framework\nis compatible with different embedding-based models, and our experiments on\nmultiple datasets have demonstrated its state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:27:46 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 02:25:23 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 07:51:22 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Qi", "Zhiyuan", ""], ["Zhang", "Ziheng", ""], ["Chen", "Jiaoyan", ""], ["Chen", "Xi", ""], ["Xiang", "Yuejia", ""], ["Zhang", "Ningyu", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2105.05599", "submitter": "Md Sahidullah", "authors": "Shakeel A. Sheikh, Md Sahidullah, Fabrice Hirsch, Slim Ouni", "title": "StutterNet: Stuttering Detection Using Time Delay Neural Network", "comments": "Accepted in EUSIPCO 2021: European Signal Processing Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces StutterNet, a novel deep learning based stuttering\ndetection capable of detecting and identifying various types of disfluencies.\nMost of the existing work in this domain uses automatic speech recognition\n(ASR) combined with language models for stuttering detection. Compared to the\nexisting work, which depends on the ASR module, our method relies solely on the\nacoustic signal. We use a time-delay neural network (TDNN) suitable for\ncapturing contextual aspects of the disfluent utterances. We evaluate our\nsystem on the UCLASS stuttering dataset consisting of more than 100 speakers.\nOur method achieves promising results and outperforms the state-of-the-art\nresidual neural network based method. The number of trainable parameters of the\nproposed method is also substantially less due to the parameter sharing scheme\nof TDNN.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:36:01 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:44:45 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sheikh", "Shakeel A.", ""], ["Sahidullah", "Md", ""], ["Hirsch", "Fabrice", ""], ["Ouni", "Slim", ""]]}, {"id": "2105.05601", "submitter": "DongHyun Choi", "authors": "DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, Dong Ryeol Shin", "title": "OutFlip: Generating Out-of-Domain Samples for Unknown Intent Detection\n  with Natural Language Attack", "comments": "9 pages, 3 figures; to be appear in ACL Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Out-of-domain (OOD) input detection is vital in a task-oriented dialogue\nsystem since the acceptance of unsupported inputs could lead to an incorrect\nresponse of the system. This paper proposes OutFlip, a method to generate\nout-of-domain samples using only in-domain training dataset automatically. A\nwhite-box natural language attack method HotFlip is revised to generate\nout-of-domain samples instead of adversarial examples. Our evaluation results\nshowed that integrating OutFlip-generated out-of-domain samples into the\ntraining dataset could significantly improve an intent classification model's\nout-of-domain detection performance.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:38:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Choi", "DongHyun", ""], ["Shin", "Myeong Cheol", ""], ["Kim", "EungGyun", ""], ["Shin", "Dong Ryeol", ""]]}, {"id": "2105.05605", "submitter": "Ruben Cardoso", "authors": "Ruben Cardoso, Afonso Mendes, Andre Lamurias", "title": "Priberam Labs at the NTCIR-15 SHINRA2020-ML: Classification Task", "comments": "Presented at NTCIR-15 conference (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wikipedia is an online encyclopedia available in 285 languages. It composes\nan extremely relevant Knowledge Base (KB), which could be leveraged by\nautomatic systems for several purposes. However, the structure and organisation\nof such information are not prone to automatic parsing and understanding and it\nis, therefore, necessary to structure this knowledge. The goal of the current\nSHINRA2020-ML task is to leverage Wikipedia pages in order to categorise their\ncorresponding entities across 268 hierarchical categories, belonging to the\nExtended Named Entity (ENE) ontology. In this work, we propose three distinct\nmodels based on the contextualised embeddings yielded by Multilingual BERT. We\nexplore the performances of a linear layer with and without explicit usage of\nthe ontology's hierarchy, and a Gated Recurrent Units (GRU) layer. We also test\nseveral pooling strategies to leverage BERT's embeddings and selection criteria\nbased on the labels' scores. We were able to achieve good performance across a\nlarge variety of languages, including those not seen during the fine-tuning\nprocess (zero-shot languages).\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:49:19 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cardoso", "Ruben", ""], ["Mendes", "Afonso", ""], ["Lamurias", "Andre", ""]]}, {"id": "2105.05612", "submitter": "Damien Teney", "authors": "Damien Teney, Ehsan Abbasnejad, Simon Lucey, Anton van den Hengel", "title": "Evading the Simplicity Bias: Training a Diverse Set of Models Discovers\n  Solutions with Superior OOD Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks trained with SGD were recently shown to rely preferentially\non linearly-predictive features and can ignore complex, equally-predictive\nones. This simplicity bias can explain their lack of robustness out of\ndistribution (OOD). The more complex the task to learn, the more likely it is\nthat statistical artifacts (i.e. selection biases, spurious correlations) are\nsimpler than the mechanisms to learn. We demonstrate that the simplicity bias\ncan be mitigated and OOD generalization improved. We train a set of similar\nmodels to fit the data in different ways using a penalty on the alignment of\ntheir input gradients. We show theoretically and empirically that this induces\nthe learning of more complex predictive patterns. OOD generalization\nfundamentally requires information beyond i.i.d. examples, such as multiple\ntraining environments, counterfactual examples, or other side information. Our\napproach shows that we can defer this requirement to an independent model\nselection stage. We obtain SOTA results in visual recognition on biased data\nand generalization across visual domains. The method - the first to evade the\nsimplicity bias - highlights the need for a better understanding and control of\ninductive biases in deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 12:12:24 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 08:49:35 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Teney", "Damien", ""], ["Abbasnejad", "Ehsan", ""], ["Lucey", "Simon", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2105.05614", "submitter": "Ruben Cardoso", "authors": "Ruben Cardoso, Zita Marinho, Afonso Mendes, Sebasti\\~ao Miranda", "title": "Priberam at MESINESP Multi-label Classification of Medical Texts Task", "comments": "Presented at CLEF2020 conference (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical articles provide current state of the art treatments and diagnostics\nto many medical practitioners and professionals. Existing public databases such\nas MEDLINE contain over 27 million articles, making it difficult to extract\nrelevant content without the use of efficient search engines. Information\nretrieval tools are crucial in order to navigate and provide meaningful\nrecommendations for articles and treatments. Classifying these articles into\nbroader medical topics can improve the retrieval of related articles. The set\nof medical labels considered for the MESINESP task is on the order of several\nthousands of labels (DeCS codes), which falls under the extreme multi-label\nclassification problem. The heterogeneous and highly hierarchical structure of\nmedical topics makes the task of manually classifying articles extremely\nlaborious and costly. It is, therefore, crucial to automate the process of\nclassification. Typical machine learning algorithms become computationally\ndemanding with such a large number of labels and achieving better recall on\nsuch datasets becomes an unsolved problem.\n  This work presents Priberam's participation at the BioASQ task Mesinesp. We\naddress the large multi-label classification problem through the use of four\ndifferent models: a Support Vector Machine (SVM), a customised search engine\n(Priberam Search), a BERT based classifier, and a SVM-rank ensemble of all the\nprevious models. Results demonstrate that all three individual models perform\nwell and the best performance is achieved by their ensemble, granting Priberam\nthe 6th place in the present challenge and making it the 2nd best team.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 12:14:16 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cardoso", "Ruben", ""], ["Marinho", "Zita", ""], ["Mendes", "Afonso", ""], ["Miranda", "Sebasti\u00e3o", ""]]}, {"id": "2105.05622", "submitter": "Aidan Hughes", "authors": "A.J. Hughes, L.A. Bull, P. Gardner, R.J. Barthorpe, N. Dervilis, K.\n  Worden", "title": "On risk-based active learning for structural health monitoring", "comments": "28 pages. 23 figures. Under review, preprint submitted to Mechanical\n  Systems and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A primary motivation for the development and implementation of structural\nhealth monitoring systems, is the prospect of gaining the ability to make\ninformed decisions regarding the operation and maintenance of structures and\ninfrastructure. Unfortunately, descriptive labels for measured data\ncorresponding to health-state information for the structure of interest are\nseldom available prior to the implementation of a monitoring system. This issue\nlimits the applicability of the traditional supervised and unsupervised\napproaches to machine learning in the development of statistical classifiers\nfor decision-supporting SHM systems.\n  The current paper presents a risk-based formulation of active learning, in\nwhich the querying of class-label information is guided by the expected value\nof said information for each incipient data point. When applied to structural\nhealth monitoring, the querying of class labels can be mapped onto the\ninspection of a structure of interest in order to determine its health state.\nIn the current paper, the risk-based active learning process is explained and\nvisualised via a representative numerical example and subsequently applied to\nthe Z24 Bridge benchmark. The results of the case studies indicate that a\ndecision-maker's performance can be improved via the risk-based active learning\nof a statistical classifier, such that the decision process itself is taken\ninto account.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 12:34:03 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Hughes", "A. J.", ""], ["Bull", "L. A.", ""], ["Gardner", "P.", ""], ["Barthorpe", "R. J.", ""], ["Dervilis", "N.", ""], ["Worden", "K.", ""]]}, {"id": "2105.05631", "submitter": "Maysam Behmanesh", "authors": "Maysam Behmanesh, Peyman Adibi, Jocelyn Chanussot, Sayyed Mohammad\n  Saeed Ehsani", "title": "Cross-Modal and Multimodal Data Analysis Based on Functional Mapping of\n  Spectral Descriptors and Manifold Regularization", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal manifold modeling methods extend the spectral geometry-aware data\nanalysis to learning from several related and complementary modalities. Most of\nthese methods work based on two major assumptions: 1) there are the same number\nof homogeneous data samples in each modality, and 2) at least partial\ncorrespondences between modalities are given in advance as prior knowledge.\nThis work proposes two new multimodal modeling methods. The first method\nestablishes a general analyzing framework to deal with the multimodal\ninformation problem for heterogeneous data without any specific prior\nknowledge. For this purpose, first, we identify the localities of each manifold\nby extracting local descriptors via spectral graph wavelet signatures (SGWS).\nThen, we propose a manifold regularization framework based on the functional\nmapping between SGWS descriptors (FMBSD) for finding the pointwise\ncorrespondences. The second method is a manifold regularized multimodal\nclassification based on pointwise correspondences (M$^2$CPC) used for the\nproblem of multiclass classification of multimodal heterogeneous, which the\ncorrespondences between modalities are determined based on the FMBSD method.\nThe experimental results of evaluating the FMBSD method on three common\ncross-modal retrieval datasets and evaluating the (M$^2$CPC) method on three\nbenchmark multimodal multiclass classification datasets indicate their\neffectiveness and superiority over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:00:33 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Behmanesh", "Maysam", ""], ["Adibi", "Peyman", ""], ["Chanussot", "Jocelyn", ""], ["Ehsani", "Sayyed Mohammad Saeed", ""]]}, {"id": "2105.05633", "submitter": "Robin Strudel", "authors": "Robin Strudel, Ricardo Garcia, Ivan Laptev, Cordelia Schmid", "title": "Segmenter: Transformer for Semantic Segmentation", "comments": "Code available at https://github.com/rstrudel/segmenter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation is often ambiguous at the level of individual image\npatches and requires contextual information to reach label consensus. In this\npaper we introduce Segmenter, a transformer model for semantic segmentation. In\ncontrast to convolution based approaches, our approach allows to model global\ncontext already at the first layer and throughout the network. We build on the\nrecent Vision Transformer (ViT) and extend it to semantic segmentation. To do\nso, we rely on the output embeddings corresponding to image patches and obtain\nclass labels from these embeddings with a point-wise linear decoder or a mask\ntransformer decoder. We leverage models pre-trained for image classification\nand show that we can fine-tune them on moderate sized datasets available for\nsemantic segmentation. The linear decoder allows to obtain excellent results\nalready, but the performance can be further improved by a mask transformer\ngenerating class masks. We conduct an extensive ablation study to show the\nimpact of the different parameters, in particular the performance is better for\nlarge models and small patch sizes. Segmenter attains excellent results for\nsemantic segmentation. It outperforms the state of the art on the challenging\nADE20K dataset and performs on-par on Pascal Context and Cityscapes.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:01:44 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Strudel", "Robin", ""], ["Garcia", "Ricardo", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2105.05641", "submitter": "Vamsi Aribandi", "authors": "Vamsi Aribandi, Yi Tay, Donald Metzler", "title": "How Reliable are Model Diagnostics?", "comments": "ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the pursuit of a deeper understanding of a model's behaviour, there is\nrecent impetus for developing suites of probes aimed at diagnosing models\nbeyond simple metrics like accuracy or BLEU. This paper takes a step back and\nasks an important and timely question: how reliable are these diagnostics in\nproviding insight into models and training setups? We critically examine three\nrecent diagnostic tests for pre-trained language models, and find that\nlikelihood-based and representation-based model diagnostics are not yet as\nreliable as previously assumed. Based on our empirical findings, we also\nformulate recommendations for practitioners and researchers.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:20:20 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Aribandi", "Vamsi", ""], ["Tay", "Yi", ""], ["Metzler", "Donald", ""]]}, {"id": "2105.05648", "submitter": "Johan Larsson", "authors": "Johan Larsson", "title": "Look-Ahead Screening Rules for the Lasso", "comments": "EYSM 2021 short paper; 6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso is a popular method to induce shrinkage and sparsity in the\nsolution vector (coefficients) of regression problems, particularly when there\nare many predictors relative to the number of observations. Solving the lasso\nin this high-dimensional setting can, however, be computationally demanding.\nFortunately, this demand can be alleviated via the use of screening rules that\ndiscard predictors prior to fitting the model, leading to a reduced problem to\nbe solved. In this paper, we present a new screening strategy: look-ahead\nscreening. Our method uses safe screening rules to find a range of penalty\nvalues for which a given predictor cannot enter the model, thereby screening\npredictors along the remainder of the path. In experiments we show that these\nlook-ahead screening rules outperform the active warm-start version of the Gap\nSafe rules.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:27:40 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 10:05:19 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Larsson", "Johan", ""]]}, {"id": "2105.05650", "submitter": "Dian Wu", "authors": "Dian Wu, Riccardo Rossi, Giuseppe Carleo", "title": "Unbiased Monte Carlo Cluster Updates with Autoregressive Neural Networks", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient sampling of complex high-dimensional probability densities is a\ncentral task in computational science. Machine Learning techniques based on\nautoregressive neural networks have been recently shown to provide good\napproximations of probability distributions of interest in physics. In this\nwork, we propose a systematic way to remove the intrinsic bias associated with\nthese variational approximations, combining it with Markov-chain Monte Carlo in\nan automatic scheme to efficiently generate cluster updates, which is\nparticularly useful for models for which no efficient cluster update scheme is\nknown. Our approach is based on symmetry-enforced cluster updates building on\nthe neural-network representation of conditional probabilities. We demonstrate\nthat such finite-cluster updates are crucial to circumvent ergodicity problems\nassociated with global neural updates. We test our method for first- and\nsecond-order phase transitions in classical spin systems, proving in particular\nits viability for critical systems, or in the presence of metastable states.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:31:12 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wu", "Dian", ""], ["Rossi", "Riccardo", ""], ["Carleo", "Giuseppe", ""]]}, {"id": "2105.05674", "submitter": "Ismo Horppu", "authors": "Ismo Horppu, Antti Nikander, Elif Buyukcan, Jere M\\\"akiniemi, Amin\n  Sorkhei, Frederick Ayala-G\\'omez", "title": "Automatic Classification of Games using Support Vector Machine", "comments": "7 pages, 7 figures, updated contact information of one author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Game developers benefit from availability of custom game genres when doing\ngame market analysis. This information can help them to spot opportunities in\nmarket and make them more successful in planning a new game. In this paper we\nfind good classifier for predicting category of a game. Prediction is based on\ndescription and title of a game. We use 2443 iOS App Store games as data set to\ngenerate a document-term matrix. To reduce the curse of dimensionality we use\nLatent Semantic Indexing, which, reduces the term dimension to approximately\n1/9. Support Vector Machine supervised learning model is fit to pre-processed\ndata. Model parameters are optimized using grid search and 20-fold cross\nvalidation. Best model yields to 77% mean accuracy or roughly 70% accuracy with\n95% confidence. Developed classifier has been used in-house to assist games\nmarket research.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 14:13:21 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 09:40:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Horppu", "Ismo", ""], ["Nikander", "Antti", ""], ["Buyukcan", "Elif", ""], ["M\u00e4kiniemi", "Jere", ""], ["Sorkhei", "Amin", ""], ["Ayala-G\u00f3mez", "Frederick", ""]]}, {"id": "2105.05682", "submitter": "Ming Jin", "authors": "Ming Jin, Yizhen Zheng, Yuan-Fang Li, Chen Gong, Chuan Zhou, Shirui\n  Pan", "title": "Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph\n  Representation Learning", "comments": "7 pages, 5 figures, 3 tables. Accepted by the 30th International\n  Joint Conference on Artificial Intelligence (IJCAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph representation learning plays a vital role in processing\ngraph-structured data. However, prior arts on graph representation learning\nheavily rely on labeling information. To overcome this problem, inspired by the\nrecent success of graph contrastive learning and Siamese networks in visual\nrepresentation learning, we propose a novel self-supervised approach in this\npaper to learn node representations by enhancing Siamese self-distillation with\nmulti-scale contrastive learning. Specifically, we first generate two augmented\nviews from the input graph based on local and global perspectives. Then, we\nemploy two objectives called cross-view and cross-network contrastiveness to\nmaximize the agreement between node representations across different views and\nnetworks. To demonstrate the effectiveness of our approach, we perform\nempirical experiments on five real-world datasets. Our method not only achieves\nnew state-of-the-art results but also surpasses some semi-supervised\ncounterparts by large margins. Code is made available at\nhttps://github.com/GRAND-Lab/MERIT\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 14:20:13 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 05:01:41 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Jin", "Ming", ""], ["Zheng", "Yizhen", ""], ["Li", "Yuan-Fang", ""], ["Gong", "Chen", ""], ["Zhou", "Chuan", ""], ["Pan", "Shirui", ""]]}, {"id": "2105.05686", "submitter": "Guilherme Rosa", "authors": "Guilherme Moraes Rosa, Ruan Chaves Rodrigues, Roberto Lotufo, Rodrigo\n  Nogueira", "title": "Yes, BM25 is a Strong Baseline for Legal Case Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our single submission to task 1 of COLIEE 2021. Our vanilla BM25\ngot second place, well above the median of submissions. Code is available at\nhttps://github.com/neuralmind-ai/coliee.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 18:33:38 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Rosa", "Guilherme Moraes", ""], ["Rodrigues", "Ruan Chaves", ""], ["Lotufo", "Roberto", ""], ["Nogueira", "Rodrigo", ""]]}, {"id": "2105.05690", "submitter": "Juntao Huang", "authors": "Juntao Huang, Yingda Cheng, Andrew J. Christlieb, Luke F. Roberts", "title": "Machine learning moment closure models for the radiative transfer\n  equation I: directly learning a gradient based closure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we take a data-driven approach and apply machine learning to\nthe moment closure problem for radiative transfer equation in slab geometry.\nInstead of learning the unclosed high order moment, we propose to directly\nlearn the gradient of the high order moment using neural networks. This new\napproach is consistent with the exact closure we derive for the free streaming\nlimit and also provides a natural output normalization. A variety of benchmark\ntests, including the variable scattering problem, the Gaussian source problem\nand the two material problem, show both good accuracy and generalizability of\nour machine learning closure model.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 14:24:20 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Huang", "Juntao", ""], ["Cheng", "Yingda", ""], ["Christlieb", "Andrew J.", ""], ["Roberts", "Luke F.", ""]]}, {"id": "2105.05699", "submitter": "Chris Williams", "authors": "Tijl De Bie, Luc De Raedt, Jos\\'e Hern\\'andez-Orallo, Holger H. Hoos,\n  Padhraic Smyth, Christopher K. I. Williams", "title": "Automating Data Science: Prospects and Challenges", "comments": "19 pages, 3 figures. Accepted for publication (April 2021) in\n  Communications of the ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the complexity of typical data science projects and the associated\ndemand for human expertise, automation has the potential to transform the data\nscience process.\n  Key insights:\n  * Automation in data science aims to facilitate and transform the work of\ndata scientists, not to replace them.\n  * Important parts of data science are already being automated, especially in\nthe modeling stages, where techniques such as automated machine learning\n(AutoML) are gaining traction.\n  * Other aspects are harder to automate, not only because of technological\nchallenges, but because open-ended and context-dependent tasks require human\ninteraction.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 14:34:35 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["De Bie", "Tijl", ""], ["De Raedt", "Luc", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Hoos", "Holger H.", ""], ["Smyth", "Padhraic", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "2105.05716", "submitter": "Adrian Remonda", "authors": "Adrian Remonda, Eduardo Veas, Granit Luzhnica", "title": "Acting upon Imagination: when to trust imagined trajectories in model\n  based reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Model based reinforcement learning (MBRL) uses an imperfect model of the\nworld to imagine trajectories of future states and plan the best actions to\nmaximize a reward function. These trajectories are imperfect and MBRL attempts\nto overcome this by relying on model predictive control (MPC) to continuously\nre-imagine trajectories from scratch. Such re-generation of imagined\ntrajectories carries the major computational cost and increasing complexity in\ntasks with longer receding horizon. This paper aims to investigate how far in\nthe future the imagined trajectories can be relied upon while still maintaining\nacceptable reward. Firstly, an error analysis is presented for systematic\nskipping recalculations for varying number of consecutive steps.% in several\nchallenging benchmark control tasks. Secondly, we propose two methods offering\nwhen to trust and act upon imagined trajectories, looking at recent errors with\nrespect to expectations, or comparing the confidence in an action imagined\nagainst its execution. Thirdly, we evaluate the effects of acting upon\nimagination while training the model of the world. Results show that acting\nupon imagination can reduce calculations by at least 20% and up to 80%,\ndepending on the environment, while retaining acceptable reward.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:04:07 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 10:26:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Remonda", "Adrian", ""], ["Veas", "Eduardo", ""], ["Luzhnica", "Granit", ""]]}, {"id": "2105.05717", "submitter": "Lunchen Xie", "authors": "Lunchen Xie, Jiaqi Liu, Songtao Lu, Tsung-hui Chang, Qingjiang Shi", "title": "An Efficient Learning Framework For Federated XGBoost Using Secret\n  Sharing And Distributed Optimization", "comments": "24 pages, Special issue of ACM Transactions on Intelligent Systems\n  and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  XGBoost is one of the most widely used machine learning models in the\nindustry due to its superior learning accuracy and efficiency. Targeting at\ndata isolation issues in the big data problems, it is crucial to deploy a\nsecure and efficient federated XGBoost (FedXGB) model. Existing FedXGB models\neither have data leakage issues or are only applicable to the two-party setting\nwith heavy communication and computation overheads. In this paper, a lossless\nmulti-party federated XGB learning framework is proposed with a security\nguarantee, which reshapes the XGBoost's split criterion calculation process\nunder a secret sharing setting and solves the leaf weight calculation problem\nby leveraging distributed optimization. Remarkably, a thorough analysis of\nmodel security is provided as well, and multiple numerical results showcase the\nsuperiority of the proposed FedXGB compared with the state-of-the-art models on\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:04:18 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Xie", "Lunchen", ""], ["Liu", "Jiaqi", ""], ["Lu", "Songtao", ""], ["Chang", "Tsung-hui", ""], ["Shi", "Qingjiang", ""]]}, {"id": "2105.05720", "submitter": "Abhinav Jangda", "authors": "Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet,\n  Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi", "title": "CoCoNet: Co-Optimizing Computation and Communication for Distributed\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning workloads run on distributed hardware and are difficult\nto optimize -- data, model, and pipeline parallelism require a developer to\nthoughtfully restructure their workload around optimized computation and\ncommunication kernels in libraries such as cuBLAS and NCCL. The logical\nseparation between computation and communication leaves performance on the\ntable with missed optimization opportunities across abstraction boundaries. To\nexplore these opportunities, this paper presents CoCoNet, which consists of a\ncompute language to express programs with both computation and communication, a\nscheduling language to apply transformations on such programs, and a compiler\nto generate high performance kernels. Providing both computation and\ncommunication as first class constructs enables new optimizations, such as\noverlapping or fusion of communication with computation. CoCoNet allowed us to\noptimize several data, model and pipeline parallel workloads in existing deep\nlearning systems with very few lines of code. We show significant improvements\nafter integrating novel CoCoNet generated kernels.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:13:43 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 01:04:11 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Jangda", "Abhinav", ""], ["Huang", "Jun", ""], ["Liu", "Guodong", ""], ["Sabet", "Amir Hossein Nodehi", ""], ["Maleki", "Saeed", ""], ["Miao", "Youshan", ""], ["Musuvathi", "Madanlal", ""], ["Mytkowicz", "Todd", ""], ["Sarikivi", "Olli", ""]]}, {"id": "2105.05728", "submitter": "Matthias H\\\"user", "authors": "Matthias H\\\"user, Martin Faltys, Xinrui Lyu, Chris Barber, Stephanie\n  L. Hyland, Tobias M. Merz, Gunnar R\\\"atsch", "title": "Early prediction of respiratory failure in the intensive care unit", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of respiratory failure is common among patients in intensive\ncare units (ICU). Large data quantities from ICU patient monitoring systems\nmake timely and comprehensive analysis by clinicians difficult but are ideal\nfor automatic processing by machine learning algorithms. Early prediction of\nrespiratory system failure could alert clinicians to patients at risk of\nrespiratory failure and allow for early patient reassessment and treatment\nadjustment. We propose an early warning system that predicts moderate/severe\nrespiratory failure up to 8 hours in advance. Our system was trained on\nHiRID-II, a data-set containing more than 60,000 admissions to a tertiary care\nICU. An alarm is typically triggered several hours before the beginning of\nrespiratory failure. Our system outperforms a clinical baseline mimicking\ntraditional clinical decision-making based on pulse-oximetric oxygen saturation\nand the fraction of inspired oxygen. To provide model introspection and\ndiagnostics, we developed an easy-to-use web browser-based system to explore\nmodel input data and predictions visually.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:20:09 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["H\u00fcser", "Matthias", ""], ["Faltys", "Martin", ""], ["Lyu", "Xinrui", ""], ["Barber", "Chris", ""], ["Hyland", "Stephanie L.", ""], ["Merz", "Tobias M.", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "2105.05733", "submitter": "Mariano Beguerisse-D\\'iaz", "authors": "Mariano Beguerisse-D\\'iaz, Dimitrios Korkinof, Till Hoffmann", "title": "Thematic recommendations on knowledge graphs using multilayer networks", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to generate and evaluate thematic recommendations\nbased on multilayer network representations of knowledge graphs (KGs). In this\nrepresentation, each layer encodes a different type of relationship in the KG,\nand directed interlayer couplings connect the same entity in different roles.\nThe relative importance of different types of connections is captured by an\nintuitive salience matrix that can be estimated from data, tuned to incorporate\ndomain knowledge, address different use cases, or respect business logic.\n  We apply an adaptation of the personalised PageRank algorithm to multilayer\nmodels of KGs to generate item-item recommendations. These recommendations\nreflect the knowledge we hold about the content and are suitable for thematic\nand/or cold-start recommendation settings. Evaluating thematic recommendations\nfrom user data presents unique challenges that we address by developing a\nmethod to evaluate recommendations relying on user-item ratings, yet respecting\ntheir thematic nature. We also show that the salience matrix can be estimated\nfrom user data. We demonstrate the utility of our methods by significantly\nimproving consumption metrics in an AB test where collaborative filtering\ndelivered subpar performance. We also apply our approach to movie\nrecommendation using publicly-available data to ensure the reproducibility of\nour results. We demonstrate that our approach outperforms existing thematic\nrecommendation methods and is even competitive with collaborative filtering\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:30:21 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Beguerisse-D\u00edaz", "Mariano", ""], ["Korkinof", "Dimitrios", ""], ["Hoffmann", "Till", ""]]}, {"id": "2105.05734", "submitter": "Julian Matschinske", "authors": "Julian Matschinske, Julian Sp\\\"ath, Reza Nasirigerdeh, Reihaneh\n  Torkzadehmahani, Anne Hartebrodt, Bal\\'azs Orb\\'an, S\\'andor Fej\\'er, Olga\n  Zolotareva, Mohammad Bakhtiari, B\\'ela Bihari, Marcus Bloice, Nina C Donner,\n  Walid Fdhila, Tobias Frisch, Anne-Christin Hauschild, Dominik Heider, Andreas\n  Holzinger, Walter H\\\"otzendorfer, Jan Hospes, Tim Kacprowski, Markus\n  Kastelitz, Markus List, Rudolf Mayer, M\\'onika Moga, Heimo M\\\"uller,\n  Anastasia Pustozerova, Richard R\\\"ottger, Anna Saranti, Harald HHW Schmidt,\n  Christof Tschohl, Nina K Wenke, Jan Baumbach", "title": "The FeatureCloud AI Store for Federated Learning in Biomedicine and\n  Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning (ML) and Artificial Intelligence (AI) have shown promising\nresults in many areas and are driven by the increasing amount of available\ndata. However, this data is often distributed across different institutions and\ncannot be shared due to privacy concerns. Privacy-preserving methods, such as\nFederated Learning (FL), allow for training ML models without sharing sensitive\ndata, but their implementation is time-consuming and requires advanced\nprogramming skills. Here, we present the FeatureCloud AI Store for FL as an\nall-in-one platform for biomedical research and other applications. It removes\nlarge parts of this complexity for developers and end-users by providing an\nextensible AI Store with a collection of ready-to-use apps. We show that the\nfederated apps produce similar results to centralized ML, scale well for a\ntypical number of collaborators and can be combined with Secure Multiparty\nComputation (SMPC), thereby making FL algorithms safely and easily applicable\nin biomedical and clinical environments.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:31:46 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Matschinske", "Julian", ""], ["Sp\u00e4th", "Julian", ""], ["Nasirigerdeh", "Reza", ""], ["Torkzadehmahani", "Reihaneh", ""], ["Hartebrodt", "Anne", ""], ["Orb\u00e1n", "Bal\u00e1zs", ""], ["Fej\u00e9r", "S\u00e1ndor", ""], ["Zolotareva", "Olga", ""], ["Bakhtiari", "Mohammad", ""], ["Bihari", "B\u00e9la", ""], ["Bloice", "Marcus", ""], ["Donner", "Nina C", ""], ["Fdhila", "Walid", ""], ["Frisch", "Tobias", ""], ["Hauschild", "Anne-Christin", ""], ["Heider", "Dominik", ""], ["Holzinger", "Andreas", ""], ["H\u00f6tzendorfer", "Walter", ""], ["Hospes", "Jan", ""], ["Kacprowski", "Tim", ""], ["Kastelitz", "Markus", ""], ["List", "Markus", ""], ["Mayer", "Rudolf", ""], ["Moga", "M\u00f3nika", ""], ["M\u00fcller", "Heimo", ""], ["Pustozerova", "Anastasia", ""], ["R\u00f6ttger", "Richard", ""], ["Saranti", "Anna", ""], ["Schmidt", "Harald HHW", ""], ["Tschohl", "Christof", ""], ["Wenke", "Nina K", ""], ["Baumbach", "Jan", ""]]}, {"id": "2105.05735", "submitter": "Sangwoong Yoon", "authors": "Sangwoong Yoon, Yung-Kyun Noh, Frank Chongwoo Park", "title": "Autoencoding Under Normalization Constraints", "comments": "Accepted to ICML 2021. The code is released in\n  https://github.com/swyoon/normalized-autoencoders . Any updates regarding the\n  work, e.g. the release of the code, will be broadcasted through the mailing\n  list, see\n  https://mailchi.mp/32e7ca8a0a85/autoencoding-under-normalization-constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Likelihood is a standard estimate for outlier detection. The specific role of\nthe normalization constraint is to ensure that the out-of-distribution (OOD)\nregime has a small likelihood when samples are learned using maximum\nlikelihood. Because autoencoders do not possess such a process of\nnormalization, they often fail to recognize outliers even when they are\nobviously OOD. We propose the Normalized Autoencoder (NAE), a normalized\nprobabilistic model constructed from an autoencoder. The probability density of\nNAE is defined using the reconstruction error of an autoencoder, which is\ndifferently defined in the conventional energy-based model. In our model,\nnormalization is enforced by suppressing the reconstruction of negative\nsamples, significantly improving the outlier detection performance. Our\nexperimental results confirm the efficacy of NAE, both in detecting outliers\nand in generating in-distribution samples.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:36:48 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 20:42:46 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yoon", "Sangwoong", ""], ["Noh", "Yung-Kyun", ""], ["Park", "Frank Chongwoo", ""]]}, {"id": "2105.05736", "submitter": "Ankit Singh Rawat", "authors": "Ankit Singh Rawat, Aditya Krishna Menon, Wittawat Jitkrittum, Sadeep\n  Jayasumana, Felix X. Yu, Sashank Reddi, Sanjiv Kumar", "title": "Disentangling Sampling and Labeling Bias for Learning in Large-Output\n  Spaces", "comments": "To appear in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Negative sampling schemes enable efficient training given a large number of\nclasses, by offering a means to approximate a computationally expensive loss\nfunction that takes all labels into account. In this paper, we present a new\nconnection between these schemes and loss modification techniques for\ncountering label imbalance. We show that different negative sampling schemes\nimplicitly trade-off performance on dominant versus rare labels. Further, we\nprovide a unified means to explicitly tackle both sampling bias, arising from\nworking with a subset of all labels, and labeling bias, which is inherent to\nthe data due to label imbalance. We empirically verify our findings on\nlong-tail classification and retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:40:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Rawat", "Ankit Singh", ""], ["Menon", "Aditya Krishna", ""], ["Jitkrittum", "Wittawat", ""], ["Jayasumana", "Sadeep", ""], ["Yu", "Felix X.", ""], ["Reddi", "Sashank", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "2105.05737", "submitter": "Zili Zhou", "authors": "Zili Zhou, Marco Valentino, Donal Landers, Andre Freitas", "title": "Encoding Explanatory Knowledge for Zero-shot Science Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge\nTransfer), a novel method for the automatic transfer of explanatory knowledge\nthrough neural encoding mechanisms. We demonstrate that N-XKT is able to\nimprove accuracy and generalization on science Question Answering (QA).\nSpecifically, by leveraging facts from background explanatory knowledge\ncorpora, the N-XKT model shows a clear improvement on zero-shot QA.\nFurthermore, we show that N-XKT can be fine-tuned on a target QA dataset,\nenabling faster convergence and more accurate results. A systematic analysis is\nconducted to quantitatively analyze the performance of the N-XKT model and the\nimpact of different categories of knowledge on the zero-shot generalization\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:42:50 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 20:10:09 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhou", "Zili", ""], ["Valentino", "Marco", ""], ["Landers", "Donal", ""], ["Freitas", "Andre", ""]]}, {"id": "2105.05757", "submitter": "Thomas Goerttler", "authors": "Thomas Goerttler and Klaus Obermayer", "title": "Exploring the Similarity of Representations in Model-Agnostic\n  Meta-Learning", "comments": "Learning to Learn workshop at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In past years model-agnostic meta-learning (MAML) has been one of the most\npromising approaches in meta-learning. It can be applied to different kinds of\nproblems, e.g., reinforcement learning, but also shows good results on few-shot\nlearning tasks. Besides their tremendous success in these tasks, it has still\nnot been fully revealed yet, why it works so well. Recent work proposes that\nMAML rather reuses features than rapidly learns. In this paper, we want to\ninspire a deeper understanding of this question by analyzing MAML's\nrepresentation. We apply representation similarity analysis (RSA), a\nwell-established method in neuroscience, to the few-shot learning instantiation\nof MAML. Although some part of our analysis supports their general results that\nfeature reuse is predominant, we also reveal arguments against their\nconclusion. The similarity-increase of layers closer to the input layers arises\nfrom the learning task itself and not from the model. In addition, the\nrepresentations after inner gradient steps make a broader change to the\nrepresentation than the changes during meta-training.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 16:20:40 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Goerttler", "Thomas", ""], ["Obermayer", "Klaus", ""]]}, {"id": "2105.05758", "submitter": "Mohammadsadegh Saberian", "authors": "M.Sadegh Saberian, Kathleen P. Moriarty, Andrea D. Olmstead, Ivan R.\n  Nabi, Fran\\c{c}ois Jean, Maxwell W. Libbrecht, Ghassan Hamarneh", "title": "DEEMD: Drug Efficacy Estimation against SARS-CoV-2 based on cell\n  Morphology with Deep multiple instance learning", "comments": "Supplementary material is appended to the end of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Drug repurposing can accelerate the identification of effective compounds for\nclinical use against SARS-CoV-2, with the advantage of pre-existing clinical\nsafety data and an established supply chain. RNA viruses such as SARS-CoV-2\nmanipulate cellular pathways and induce reorganization of subcellular\nstructures to support their life cycle. These morphological changes can be\nquantified using bioimaging techniques. In this work, we developed DEEMD: a\ncomputational pipeline using deep neural network models within a multiple\ninstance learning (MIL) framework, to identify putative treatments effective\nagainst SARS-CoV-2 based on morphological analysis of the publicly available\nRxRx19a dataset. This dataset consists of fluorescence microscopy images of\nSARS-CoV-2 non-infected cells and infected cells, with and without drug\ntreatment. DEEMD first extracts discriminative morphological features to\ngenerate cell morphological profiles from the non-infected and infected cells.\nThese morphological profiles are then used in a statistical model to estimate\nthe applied treatment efficacy on infected cells based on similarities to\nnon-infected cells. DEEMD is capable of localizing infected cells via weak\nsupervision without any expensive pixel-level annotations. DEEMD identifies\nknown SARS-CoV-2 inhibitors, such as Remdesivir and Aloxistatin, supporting the\nvalidity of our approach. DEEMD is scalable to process and screen thousands of\ntreatments in parallel and can be explored for other emerging viruses and\ndatasets to rapidly identify candidate antiviral treatments in the future.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:38:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Saberian", "M. Sadegh", ""], ["Moriarty", "Kathleen P.", ""], ["Olmstead", "Andrea D.", ""], ["Nabi", "Ivan R.", ""], ["Jean", "Fran\u00e7ois", ""], ["Libbrecht", "Maxwell W.", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "2105.05790", "submitter": "Caleb Belth", "authors": "Caleb Belth, Sarah Payne, Deniz Beser, Jordan Kodner, Charles Yang", "title": "The Greedy and Recursive Search for Morphological Productivity", "comments": "CogSci 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As children acquire the knowledge of their language's morphology, they\ninvariably discover the productive processes that can generalize to new words.\nMorphological learning is made challenging by the fact that even fully\nproductive rules have exceptions, as in the well-known case of English past\ntense verbs, which features the -ed rule against the irregular verbs. The\nTolerance Principle is a recent proposal that provides a precise threshold of\nexceptions that a productive rule can withstand. Its empirical application so\nfar, however, requires the researcher to fully specify rules defined over a set\nof words. We propose a greedy search model that automatically hypothesizes\nrules and evaluates their productivity over a vocabulary. When the search for\nbroader productivity fails, the model recursively subdivides the vocabulary and\ncontinues the search for productivity over narrower rules. Trained on\npsychologically realistic data from child-directed input, our model displays\ndevelopmental patterns observed in child morphology acquisition, including the\nnotoriously complex case of German noun pluralization. It also produces\nresponses to nonce words that, despite receiving only a fraction of the\ntraining data, are more similar to those of human subjects than current neural\nnetwork models' responses are.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:02:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Belth", "Caleb", ""], ["Payne", "Sarah", ""], ["Beser", "Deniz", ""], ["Kodner", "Jordan", ""], ["Yang", "Charles", ""]]}, {"id": "2105.05791", "submitter": "Ryoto Ishizuka", "authors": "Ryoto Ishizuka, Ryo Nishikimi, Kazuyoshi Yoshii", "title": "Global Structure-Aware Drum Transcription Based on Self-Attention\n  Mechanisms", "comments": "Submitted to Signals (ISSN 2624-6120)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an automatic drum transcription (ADT) method that\ndirectly estimates a tatum-level drum score from a music signal, in contrast to\nmost conventional ADT methods that estimate the frame-level onset probabilities\nof drums. To estimate a tatum-level score, we propose a deep transcription\nmodel that consists of a frame-level encoder for extracting the latent features\nfrom a music signal and a tatum-level decoder for estimating a drum score from\nthe latent features pooled at the tatum level. To capture the global repetitive\nstructure of drum scores, which is difficult to learn with a recurrent neural\nnetwork (RNN), we introduce a self-attention mechanism with tatum-synchronous\npositional encoding into the decoder. To mitigate the difficulty of training\nthe self-attention-based model from an insufficient amount of paired data and\nimprove the musical naturalness of the estimated scores, we propose a\nregularized training method that uses a global structure-aware masked language\n(score) model with a self-attention mechanism pretrained from an extensive\ncollection of drum scores. Experimental results showed that the proposed\nregularized model outperformed the conventional RNN-based model in terms of the\ntatum-level error rate and the frame-level F-measure, even when only a limited\namount of paired data was available so that the non-regularized model\nunderperformed the RNN-based model.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:04:16 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ishizuka", "Ryoto", ""], ["Nishikimi", "Ryo", ""], ["Yoshii", "Kazuyoshi", ""]]}, {"id": "2105.05806", "submitter": "Romain Camilleri", "authors": "Romain Camilleri and Julian Katz-Samuels and Kevin Jamieson", "title": "High-Dimensional Experimental Design and Kernel Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years methods from optimal linear experimental design have been\nleveraged to obtain state of the art results for linear bandits. A design\nreturned from an objective such as $G$-optimal design is actually a probability\ndistribution over a pool of potential measurement vectors. Consequently, one\nnuisance of the approach is the task of converting this continuous probability\ndistribution into a discrete assignment of $N$ measurements. While\nsophisticated rounding techniques have been proposed, in $d$ dimensions they\nrequire $N$ to be at least $d$, $d \\log(\\log(d))$, or $d^2$ based on the\nsub-optimality of the solution. In this paper we are interested in settings\nwhere $N$ may be much less than $d$, such as in experimental design in an RKHS\nwhere $d$ may be effectively infinite. In this work, we propose a rounding\nprocedure that frees $N$ of any dependence on the dimension $d$, while\nachieving nearly the same performance guarantees of existing rounding\nprocedures. We evaluate the procedure against a baseline that projects the\nproblem to a lower dimensional space and performs rounding which requires $N$\nto just be at least a notion of the effective dimension. We also leverage our\nnew approach in a new algorithm for kernelized bandits to obtain state of the\nart results for regret minimization and pure exploration. An advantage of our\napproach over existing UCB-like approaches is that our kernel bandit algorithms\nare also robust to model misspecification.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:10:56 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Camilleri", "Romain", ""], ["Katz-Samuels", "Julian", ""], ["Jamieson", "Kevin", ""]]}, {"id": "2105.05817", "submitter": "Feng Wang", "authors": "Feng Wang, M. Cenk Gursoy, and Senem Velipasalar", "title": "Adversarial Reinforcement Learning in Dynamic Channel Access and Power\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) has recently been used to perform efficient\nresource allocation in wireless communications. In this paper, the\nvulnerabilities of such DRL agents to adversarial attacks is studied. In\nparticular, we consider multiple DRL agents that perform both dynamic channel\naccess and power control in wireless interference channels. For these victim\nDRL agents, we design a jammer, which is also a DRL agent. We propose an\nadversarial jamming attack scheme that utilizes a listening phase and\nsignificantly degrades the users' sum rate. Subsequently, we develop an\nensemble policy defense strategy against such a jamming attacker by reloading\nmodels (saved during retraining) that have minimum transition correlation.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:27:21 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wang", "Feng", ""], ["Gursoy", "M. Cenk", ""], ["Velipasalar", "Senem", ""]]}, {"id": "2105.05821", "submitter": "Lingda Li", "authors": "Lingda Li, Santosh Pandey, Thomas Flynn, Hang Liu, Noel Wheeler,\n  Adolfy Hoisie", "title": "SimNet: Computer Architecture Simulation using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While cycle-accurate simulators are essential tools for architecture\nresearch, design, and development, their practicality is limited by an\nextremely long time-to-solution for realistic problems under investigation.\nThis work describes a concerted effort, where machine learning (ML) is used to\naccelerate discrete-event simulation. First, an ML-based instruction latency\nprediction framework that accounts for both static instruction/architecture\nproperties and dynamic execution context is constructed. Then, a\nGPU-accelerated parallel simulator is implemented based on the proposed\ninstruction latency predictor, and its simulation accuracy and throughput are\nvalidated and evaluated against a state-of-the-art simulator. Leveraging modern\nGPUs, the ML-based simulator outperforms traditional simulators significantly.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:31:52 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Li", "Lingda", ""], ["Pandey", "Santosh", ""], ["Flynn", "Thomas", ""], ["Liu", "Hang", ""], ["Wheeler", "Noel", ""], ["Hoisie", "Adolfy", ""]]}, {"id": "2105.05827", "submitter": "Omer Demirel", "authors": "Omer Burak Demirel, Burhaneddin Yaman, Logan Dowdle, Steen Moeller,\n  Luca Vizioli, Essa Yacoub, John Strupp, Cheryl A. Olman, K\\^amil U\\u{g}urbil\n  and Mehmet Ak\\c{c}akaya", "title": "20-fold Accelerated 7T fMRI Using Referenceless Self-Supervised Deep\n  Learning Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High spatial and temporal resolution across the whole brain is essential to\naccurately resolve neural activities in fMRI. Therefore, accelerated imaging\ntechniques target improved coverage with high spatio-temporal resolution.\nSimultaneous multi-slice (SMS) imaging combined with in-plane acceleration are\nused in large studies that involve ultrahigh field fMRI, such as the Human\nConnectome Project. However, for even higher acceleration rates, these methods\ncannot be reliably utilized due to aliasing and noise artifacts. Deep learning\n(DL) reconstruction techniques have recently gained substantial interest for\nimproving highly-accelerated MRI. Supervised learning of DL reconstructions\ngenerally requires fully-sampled training datasets, which is not available for\nhigh-resolution fMRI studies. To tackle this challenge, self-supervised\nlearning has been proposed for training of DL reconstruction with only\nundersampled datasets, showing similar performance to supervised learning. In\nthis study, we utilize a self-supervised physics-guided DL reconstruction on a\n5-fold SMS and 4-fold in-plane accelerated 7T fMRI data. Our results show that\nour self-supervised DL reconstruction produce high-quality images at this\n20-fold acceleration, substantially improving on existing methods, while\nshowing similar functional precision and temporal effects in the subsequent\nanalysis compared to a standard 10-fold accelerated acquisition.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:39:16 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Demirel", "Omer Burak", ""], ["Yaman", "Burhaneddin", ""], ["Dowdle", "Logan", ""], ["Moeller", "Steen", ""], ["Vizioli", "Luca", ""], ["Yacoub", "Essa", ""], ["Strupp", "John", ""], ["Olman", "Cheryl A.", ""], ["U\u011furbil", "K\u00e2mil", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2105.05837", "submitter": "Elijah Cole", "authors": "Elijah Cole, Xuan Yang, Kimberly Wilber, Oisin Mac Aodha, Serge\n  Belongie", "title": "When Does Contrastive Visual Representation Learning Work?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent self-supervised representation learning techniques have largely closed\nthe gap between supervised and unsupervised learning on ImageNet\nclassification. While the particulars of pretraining on ImageNet are now\nrelatively well understood, the field still lacks widely accepted best\npractices for replicating this success on other datasets. As a first step in\nthis direction, we study contrastive self-supervised learning on four diverse\nlarge-scale datasets. By looking through the lenses of data quantity, data\ndomain, data quality, and task granularity, we provide new insights into the\nnecessary conditions for successful self-supervised learning. Our key findings\ninclude observations such as: (i) the benefit of additional pretraining data\nbeyond 500k images is modest, (ii) adding pretraining images from another\ndomain does not lead to more general representations, (iii) corrupted\npretraining images have a disparate impact on supervised and self-supervised\npretraining, and (iv) contrastive learning lags far behind supervised learning\non fine-grained visual classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:52:42 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cole", "Elijah", ""], ["Yang", "Xuan", ""], ["Wilber", "Kimberly", ""], ["Mac Aodha", "Oisin", ""], ["Belongie", "Serge", ""]]}, {"id": "2105.05842", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Lester Mackey", "title": "Kernel Thinning", "comments": "Accepted for presentation as an extended abstract at the Conference\n  on Learning Theory (COLT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error in\nthe associated reproducing kernel Hilbert space. With high probability, the\nmaximum discrepancy in integration error is\n$\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported\n$\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log\nn})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an\nequal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$\nintegration error. Our sub-exponential guarantees resemble the classical\nquasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply\nto general distributions on $\\mathbb{R}^d$ and a wide range of common kernels.\nWe use our results to derive explicit non-asymptotic maximum mean discrepancy\nbounds for Gaussian, Mat\\'ern, and B-spline kernels and present two vignettes\nillustrating the practical benefits of kernel thinning over i.i.d. sampling and\nstandard Markov chain Monte Carlo thinning.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:56:42 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 17:59:23 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 20:57:55 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Mackey", "Lester", ""]]}, {"id": "2105.05847", "submitter": "Vadim Sushko", "authors": "Vadim Sushko, Juergen Gall, Anna Khoreva", "title": "Learning to Generate Novel Scene Compositions from Single Images and\n  Videos", "comments": "The AI for Content Creation (AICC) workshop at CVPR 2021. The full\n  8-page version of this submission is available at arXiv:2103.13389", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training GANs in low-data regimes remains a challenge, as overfitting often\nleads to memorization or training divergence. In this work, we introduce\nOne-Shot GAN that can learn to generate samples from a training set as little\nas one image or one video. We propose a two-branch discriminator, with content\nand layout branches designed to judge the internal content separately from the\nscene layout realism. This allows synthesis of visually plausible, novel\ncompositions of a scene, with varying content and layout, while preserving the\ncontext of the original sample. Compared to previous single-image GAN models,\nOne-Shot GAN achieves higher diversity and quality of synthesis. It is also not\nrestricted to the single image setting, successfully learning in the introduced\nsetting of a single video.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:59:45 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sushko", "Vadim", ""], ["Gall", "Juergen", ""], ["Khoreva", "Anna", ""]]}, {"id": "2105.05881", "submitter": "Faez Ahmed", "authors": "Faez Ahmed, Yaxin Cui, Yan Fu, Wei Chen", "title": "A Graph Neural Network Approach for Product Relationship Prediction", "comments": "Paper accepted in ASME IDETC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks have revolutionized many machine learning tasks in\nrecent years, ranging from drug discovery, recommendation systems, image\nclassification, social network analysis to natural language understanding. This\npaper shows their efficacy in modeling relationships between products and\nmaking predictions for unseen product networks. By representing products as\nnodes and their relationships as edges of a graph, we show how an inductive\ngraph neural network approach, named GraphSAGE, can efficiently learn\ncontinuous representations for nodes and edges. These representations also\ncapture product feature information such as price, brand, or engineering\nattributes. They are combined with a classification model for predicting the\nexistence of the relationship between products. Using a case study of the\nChinese car market, we find that our method yields double the prediction\nperformance compared to an Exponential Random Graph Model-based method for\npredicting the co-consideration relationship between cars. While a vanilla\nGraphSAGE requires a partial network to make predictions, we introduce an\n`adjacency prediction model' to circumvent this limitation. This enables us to\npredict product relationships when no neighborhood information is known.\nFinally, we demonstrate how a permutation-based interpretability analysis can\nprovide insights on how design attributes impact the predictions of\nrelationships between products. This work provides a systematic method to\npredict the relationships between products in many different markets.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:18:38 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ahmed", "Faez", ""], ["Cui", "Yaxin", ""], ["Fu", "Yan", ""], ["Chen", "Wei", ""]]}, {"id": "2105.05883", "submitter": "Yann Fraboni", "authors": "Yann Fraboni, Richard Vidal, Laetitia Kameni, Marco Lorenzi", "title": "Clustered Sampling: Low-Variance and Improved Representativity for\n  Clients Selection in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of optimizing communications between server\nand clients in federated learning (FL). Current sampling approaches in FL are\neither biased, or non optimal in terms of server-clients communications and\ntraining stability. To overcome this issue, we introduce \\textit{clustered\nsampling} for clients selection. We prove that clustered sampling leads to\nbetter clients representatitivity and to reduced variance of the clients\nstochastic aggregation weights in FL. Compatibly with our theory, we provide\ntwo different clustering approaches enabling clients aggregation based on 1)\nsample size, and 2) models similarity. Through a series of experiments in\nnon-iid and unbalanced scenarios, we demonstrate that model aggregation through\nclustered sampling consistently leads to better training convergence and\nvariability when compared to standard sampling approaches. Our approach does\nnot require any additional operation on the clients side, and can be seamlessly\nintegrated in standard FL implementations. Finally, clustered sampling is\ncompatible with existing methods and technologies for privacy enhancement, and\nfor communication reduction through model compression.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:19:20 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 12:50:59 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Fraboni", "Yann", ""], ["Vidal", "Richard", ""], ["Kameni", "Laetitia", ""], ["Lorenzi", "Marco", ""]]}, {"id": "2105.05885", "submitter": "Divya Koyyalagunta", "authors": "Divya Koyyalagunta, Anna Sun, Rachel Lea Draelos, Cynthia Rudin", "title": "Playing Codenames with Language Graphs and Word Embeddings", "comments": "Divya Koyyalagunta and Anna Sun contributed equally to this work.\n  This is an arXiv version of the paper that has been accepted for publication\n  in the Journal of Artificial Intelligence Research (JAIR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although board games and video games have been studied for decades in\nartificial intelligence research, challenging word games remain relatively\nunexplored. Word games are not as constrained as games like chess or poker.\nInstead, word game strategy is defined by the players' understanding of the way\nwords relate to each other. The word game Codenames provides a unique\nopportunity to investigate common sense understanding of relationships between\nwords, an important open challenge. We propose an algorithm that can generate\nCodenames clues from the language graph BabelNet or from any of several\nembedding methods - word2vec, GloVe, fastText or BERT. We introduce a new\nscoring function that measures the quality of clues, and we propose a weighting\nterm called DETECT that incorporates dictionary-based word representations and\ndocument frequency to improve clue selection. We develop BabelNet-Word\nSelection Framework (BabelNet-WSF) to improve BabelNet clue quality and\novercome the computational barriers that previously prevented leveraging\nlanguage graphs for Codenames. Extensive experiments with human evaluators\ndemonstrate that our proposed innovations yield state-of-the-art performance,\nwith up to 102.8% improvement in precision@2 in some cases. Overall, this work\nadvances the formal study of word games and approaches for common sense\nlanguage understanding.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:23:03 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Koyyalagunta", "Divya", ""], ["Sun", "Anna", ""], ["Draelos", "Rachel Lea", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2105.05891", "submitter": "Kimmo K\\\"arkk\\\"ainen", "authors": "Kimmo K\\\"arkk\\\"ainen, Shayan Fazeli, Majid Sarrafzadeh", "title": "Unsupervised Acute Intracranial Hemorrhage Segmentation with Mixture\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intracranial hemorrhage occurs when blood vessels rupture or leak within the\nbrain tissue or elsewhere inside the skull. It can be caused by physical trauma\nor by various medical conditions and in many cases leads to death. The\ntreatment must be started as soon as possible, and therefore the hemorrhage\nshould be diagnosed accurately and quickly. The diagnosis is usually performed\nby a radiologist who analyses a Computed Tomography (CT) scan containing a\nlarge number of cross-sectional images throughout the brain. Analysing each\nimage manually can be very time-consuming, but automated techniques can help\nspeed up the process. While much of the recent research has focused on solving\nthis problem by using supervised machine learning algorithms,\npublicly-available training data remains scarce due to privacy concerns. This\nproblem can be alleviated by unsupervised algorithms. In this paper, we propose\na fully-unsupervised algorithm which is based on the mixture models. Our\nalgorithm utilizes the fact that the properties of hemorrhage and healthy\ntissues follow different distributions, and therefore an appropriate\nformulation of these distributions allows us to separate them through an\nExpectation-Maximization process. In addition, our algorithm is able to\nadaptively determine the number of clusters such that all the hemorrhage\nregions can be found without including noisy voxels. We demonstrate the results\nof our algorithm on publicly-available datasets that contain all different\nhemorrhage types in various sizes and intensities, and our results are compared\nto earlier unsupervised and supervised algorithms. The results show that our\nalgorithm can outperform the other algorithms with most hemorrhage types.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:26:00 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["K\u00e4rkk\u00e4inen", "Kimmo", ""], ["Fazeli", "Shayan", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "2105.05893", "submitter": "Hrushikesh Mhaskar", "authors": "H.N. Mhaskar, S.V. Pereverzyev, M.D. van der Walt", "title": "A function approximation approach to the prediction of blood glucose\n  levels", "comments": "arXiv admin note: text overlap with arXiv:1707.05828", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of real time prediction of blood glucose (BG) levels based on the\nreadings from a continuous glucose monitoring (CGM) device is a problem of\ngreat importance in diabetes care, and therefore, has attracted a lot of\nresearch in recent years, especially based on machine learning. An accurate\nprediction with a 30, 60, or 90 minute prediction horizon has the potential of\nsaving millions of dollars in emergency care costs. In this paper, we treat the\nproblem as one of function approximation, where the value of the BG level at\ntime $t+h$ (where $h$ the prediction horizon) is considered to be an unknown\nfunction of $d$ readings prior to the time $t$. This unknown function may be\nsupported in particular on some unknown submanifold of the $d$-dimensional\nEuclidean space. While manifold learning is classically done in a\nsemi-supervised setting, where the entire data has to be known in advance, we\nuse recent ideas to achieve an accurate function approximation in a supervised\nsetting; i.e., construct a model for the target function. We use the\nstate-of-the-art clinically relevant PRED-EGA grid to evaluate our results, and\ndemonstrate that for a real life dataset, our method performs better than a\nstandard deep network, especially in hypoglycemic and hyperglycemic regimes.\nOne noteworthy aspect of this work is that the training data and test data may\ncome from different distributions.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:28:16 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 04:51:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Mhaskar", "H. N.", ""], ["Pereverzyev", "S. V.", ""], ["van der Walt", "M. D.", ""]]}, {"id": "2105.05894", "submitter": "Dania Humaidan", "authors": "Dania Humaidan, Sebastian Otte, Christian Gumbsch, Charley Wu, Martin\n  V. Butz", "title": "Latent Event-Predictive Encodings through Counterfactual Regularization", "comments": "Accepted at CogSci2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical challenge for any intelligent system is to infer structure from\ncontinuous data streams. Theories of event-predictive cognition suggest that\nthe brain segments sensorimotor information into compact event encodings, which\nare used to anticipate and interpret environmental dynamics. Here, we introduce\na SUrprise-GAted Recurrent neural network (SUGAR) using a novel form of\ncounterfactual regularization. We test the model on a hierarchical sequence\nprediction task, where sequences are generated by alternating hidden graph\nstructures. Our model learns to both compress the temporal dynamics of the task\ninto latent event-predictive encodings and anticipate event transitions at the\nright moments, given noisy hidden signals about them. The addition of the\ncounterfactual regularization term ensures fluid transitions from one latent\ncode to the next, whereby the resulting latent codes exhibit compositional\nproperties. The implemented mechanisms offer a host of useful applications in\nother domains, including hierarchical reasoning, planning, and decision making.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:30:09 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Humaidan", "Dania", ""], ["Otte", "Sebastian", ""], ["Gumbsch", "Christian", ""], ["Wu", "Charley", ""], ["Butz", "Martin V.", ""]]}, {"id": "2105.05911", "submitter": "Christopher Morris", "authors": "Christopher Morris, Matthias Fey, Nils M. Kriege", "title": "The Power of the Weisfeiler-Leman Algorithm for Machine Learning with\n  Graphs", "comments": "Accepted at IJCAI 2021 (survey track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, algorithms and neural architectures based on the\nWeisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism\nproblem, emerged as a powerful tool for (supervised) machine learning with\ngraphs and relational data. Here, we give a comprehensive overview of the\nalgorithm's use in a machine learning setting. We discuss the theoretical\nbackground, show how to use it for supervised graph- and node classification,\ndiscuss recent extensions, and its connection to neural architectures.\nMoreover, we give an overview of current applications and future directions to\nstimulate research.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:05:18 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 05:04:15 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Morris", "Christopher", ""], ["Fey", "Matthias", ""], ["Kriege", "Nils M.", ""]]}, {"id": "2105.05912", "submitter": "Ahmad Rashid", "authors": "Ahmad Rashid, Vasileios Lioutas and Mehdi Rezagholizadeh", "title": "MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of large pre-trained language models has given rise to rapid\nprogress in the field of Natural Language Processing (NLP). While the\nperformance of these models on standard benchmarks has scaled with size,\ncompression techniques such as knowledge distillation have been key in making\nthem practical. We present, MATE-KD, a novel text-based adversarial training\nalgorithm which improves the performance of knowledge distillation. MATE-KD\nfirst trains a masked language model based generator to perturb text by\nmaximizing the divergence between teacher and student logits. Then using\nknowledge distillation a student is trained on both the original and the\nperturbed training samples. We evaluate our algorithm, using BERT-based models,\non the GLUE benchmark and demonstrate that MATE-KD outperforms competitive\nadversarial learning and data augmentation baselines. On the GLUE test set our\n6 layer RoBERTa based model outperforms BERT-Large.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:11:34 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Rashid", "Ahmad", ""], ["Lioutas", "Vasileios", ""], ["Rezagholizadeh", "Mehdi", ""]]}, {"id": "2105.05915", "submitter": "Boxiang Liu", "authors": "Boxiang Liu, Jiaji Huang, Xingyu Cai, Kenneth Church", "title": "Better than BERT but Worse than Baseline", "comments": "6 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper compares BERT-SQuAD and Ab3P on the Abbreviation Definition\nIdentification (ADI) task. ADI inputs a text and outputs short forms\n(abbreviations/acronyms) and long forms (expansions). BERT with reranking\nimproves over BERT without reranking but fails to reach the Ab3P rule-based\nbaseline. What is BERT missing? Reranking introduces two new features:\ncharmatch and freq. The first feature identifies opportunities to take\nadvantage of character constraints in acronyms and the second feature\nidentifies opportunities to take advantage of frequency constraints across\ndocuments.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:18:26 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Liu", "Boxiang", ""], ["Huang", "Jiaji", ""], ["Cai", "Xingyu", ""], ["Church", "Kenneth", ""]]}, {"id": "2105.05916", "submitter": "Huan Wang", "authors": "Huan Wang, Can Qin, Yue Bai, Yun Fu", "title": "Dynamical Isometry: The Missing Ingredient for Neural Network Pruning", "comments": "8 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works [40, 24] observed an interesting phenomenon in neural\nnetwork pruning: A larger finetuning learning rate can improve the final\nperformance significantly. Unfortunately, the reason behind it remains elusive\nup to date. This paper is meant to explain it through the lens of dynamical\nisometry [42]. Specifically, we examine neural network pruning from an unusual\nperspective: pruning as initialization for finetuning, and ask whether the\ninherited weights serve as a good initialization for the finetuning? The\ninsights from dynamical isometry suggest a negative answer. Despite its\ncritical role, this issue has not been well-recognized by the community so far.\nIn this paper, we will show the understanding of this problem is very important\n-- on top of explaining the aforementioned mystery about the larger finetuning\nrate, it also unveils the mystery about the value of pruning [5, 30]. Besides a\nclearer theoretical understanding of pruning, resolving the problem can also\nbring us considerable performance benefits in practice.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:20:09 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Huan", ""], ["Qin", "Can", ""], ["Bai", "Yue", ""], ["Fu", "Yun", ""]]}, {"id": "2105.05920", "submitter": "Yang Gao", "authors": "Bhargav Pulugundla, Yang Gao, Brian King, Gokce Keskin, Harish\n  Mallidi, Minhua Wu, Jasha Droppo, Roland Maas", "title": "Attention-based Neural Beamforming Layers for Multi-channel Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based beamformers have recently been shown to be effective for\nmulti-channel speech recognition. However, they are less capable at capturing\nlocal information. In this work, we propose a 2D Conv-Attention module which\ncombines convolution neural networks with attention for beamforming. We apply\nself- and cross-attention to explicitly model the correlations within and\nbetween the input channels. The end-to-end 2D Conv-Attention model is compared\nwith a multi-head self-attention and superdirective-based neural beamformers.\nWe train and evaluate on an in-house multi-channel dataset. The results show a\nrelative improvement of 3.8% in WER by the proposed model over the baseline\nneural beamformer.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:32:24 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 21:23:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Pulugundla", "Bhargav", ""], ["Gao", "Yang", ""], ["King", "Brian", ""], ["Keskin", "Gokce", ""], ["Mallidi", "Harish", ""], ["Wu", "Minhua", ""], ["Droppo", "Jasha", ""], ["Maas", "Roland", ""]]}, {"id": "2105.05932", "submitter": "Marcus Carpenter", "authors": "Marcus Carpenter, Chunbo Luo, Xiao-Si Wang", "title": "The effects of regularisation on RNN models for time series forecasting:\n  Covid-19 as an example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many research papers that propose models to predict the course of the\nCOVID-19 pandemic either use handcrafted statistical models or large neural\nnetworks. Even though large neural networks are more powerful than simpler\nstatistical models, they are especially hard to train on small datasets. This\npaper not only presents a model with grater flexibility than the other proposed\nneural networks, but also presents a model that is effective on smaller\ndatasets. To improve performance on small data, six regularisation methods were\ntested. The results show that the GRU combined with 20% Dropout achieved the\nlowest RMSE scores. The main finding was that models with less access to data\nrelied more on the regulariser. Applying Dropout to a GRU model trained on only\n28 days of data reduced the RMSE by 23%.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 10:50:57 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Carpenter", "Marcus", ""], ["Luo", "Chunbo", ""], ["Wang", "Xiao-Si", ""]]}, {"id": "2105.05944", "submitter": "Hsiang-Yun Chien", "authors": "Hsiang-Yun Sherry Chien, Javier S. Turek, Nicole Beckage, Vy A. Vo,\n  Christopher J. Honey, Ted L. Willke", "title": "Slower is Better: Revisiting the Forgetting Mechanism in LSTM for Slower\n  Information Decay", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential information contains short- to long-range dependencies; however,\nlearning long-timescale information has been a challenge for recurrent neural\nnetworks. Despite improvements in long short-term memory networks (LSTMs), the\nforgetting mechanism results in the exponential decay of information, limiting\ntheir capacity to capture long-timescale information. Here, we propose a power\nlaw forget gate, which instead learns to forget information along a slower\npower law decay function. Specifically, the new gate learns to control the\npower law decay factor, p, allowing the network to adjust the information decay\nrate according to task demands. Our experiments show that an LSTM with power\nlaw forget gates (pLSTM) can effectively capture long-range dependencies beyond\nhundreds of elements on image classification, language modeling, and\ncategorization tasks, improving performance over the vanilla LSTM. We also\ninspected the revised forget gate by varying the initialization of p, setting p\nto a fixed value, and ablating cells in the pLSTM network. The results show\nthat the information decay can be controlled by the learnable decay factor p,\nwhich allows pLSTM to achieve its superior performance. Altogether, we found\nthat LSTM with the proposed forget gate can learn long-term dependencies,\noutperforming other recurrent networks in multiple domains; such gating\nmechanism can be integrated into other architectures for improving the learning\nof long timescale information in recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 20:21:16 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chien", "Hsiang-Yun Sherry", ""], ["Turek", "Javier S.", ""], ["Beckage", "Nicole", ""], ["Vo", "Vy A.", ""], ["Honey", "Christopher J.", ""], ["Willke", "Ted L.", ""]]}, {"id": "2105.05947", "submitter": "Ryan Cory-Wright", "authors": "Dimitris Bertsimas, Ryan Cory-Wright, Jean Pauphilet", "title": "A new perspective on low-rank optimization", "comments": "Submitted to Mathematical Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key question in many low-rank problems throughout optimization, machine\nlearning, and statistics is to characterize the convex hulls of simple low-rank\nsets and judiciously apply these convex hulls to obtain strong yet\ncomputationally tractable convex relaxations. We invoke the matrix perspective\nfunction - the matrix analog of the perspective function-and characterize\nexplicitly the convex hull of epigraphs of convex quadratic, matrix\nexponential, and matrix power functions under low-rank constraints. Further, we\nexploit these characterizations to develop strong relaxations for a variety of\nlow-rank problems including reduced rank regression, non-negative matrix\nfactorization, and factor analysis. We establish that these relaxations can be\nmodeled via semidefinite and matrix power cone constraints, and thus optimized\nover tractably. The proposed approach parallels and generalizes the perspective\nreformulation technique in mixed-integer optimization, and leads to new\nrelaxations for a broad class of problems.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 20:22:21 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Cory-Wright", "Ryan", ""], ["Pauphilet", "Jean", ""]]}, {"id": "2105.05953", "submitter": "Babak Barazandeh", "authors": "Babak Barazandeh, Ali Ghafelebashi, Meisam Razaviyayn, Ram Sriharsha", "title": "Efficient Algorithms for Estimating the Parameters of Mixed Linear\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed linear regression (MLR) model is among the most exemplary statistical\ntools for modeling non-linear distributions using a mixture of linear models.\nWhen the additive noise in MLR model is Gaussian, Expectation-Maximization (EM)\nalgorithm is a widely-used algorithm for maximum likelihood estimation of MLR\nparameters. However, when noise is non-Gaussian, the steps of EM algorithm may\nnot have closed-form update rules, which makes EM algorithm impractical. In\nthis work, we study the maximum likelihood estimation of the parameters of MLR\nmodel when the additive noise has non-Gaussian distribution. In particular, we\nconsider the case that noise has Laplacian distribution and we first show that\nunlike the the Gaussian case, the resulting sub-problems of EM algorithm in\nthis case does not have closed-form update rule, thus preventing us from using\nEM in this case. To overcome this issue, we propose a new algorithm based on\ncombining the alternating direction method of multipliers (ADMM) with EM\nalgorithm idea. Our numerical experiments show that our method outperforms the\nEM algorithm in statistical accuracy and computational time in non-Gaussian\nnoise case.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 20:29:03 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Barazandeh", "Babak", ""], ["Ghafelebashi", "Ali", ""], ["Razaviyayn", "Meisam", ""], ["Sriharsha", "Ram", ""]]}, {"id": "2105.05957", "submitter": "Robert Barton", "authors": "Robert A. Barton, Tal Neiman, Changhe Yuan", "title": "Graph Neural Networks for Inconsistent Cluster Detection in Incremental\n  Entity Resolution", "comments": "Presented at the Knowledge Management for E-Commerce Workshop at\n  WWW'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online stores often utilize product relationships such as bundles and\nsubstitutes to improve their catalog quality and guide customers through myriad\nchoices. Entity resolution using pairwise product matching models offers a\nmeans of inferring relationships between products. In mature data repositories,\nthe relationships may be mostly correct but require incremental improvements\nowing to errors in the original data or in the entity resolution system. It is\ncritical to devise incremental entity resolution (IER) approaches for improving\nthe health of relationships. However, most existing research on IER focuses on\nthe addition of new products or information into existing relationships.\nRelatively little research has been done for detecting low quality within\ncurrent relationships.\n  This paper proposes a novel method for identifying inconsistent clusters\n(IC), existing groups of related products that do not belong together. We\npropose to treat the identification of inconsistent clusters as a supervised\nlearning task which predicts whether a graph of products with similarities as\nweighted edges should be partitioned into multiple clusters. In this case, the\nproblem becomes a classification task on weighted graphs and represents an\ninteresting application area for modern tools such as Graph Neural Networks\n(GNNs). We demonstrate that existing Message Passing neural networks perform\nwell at this task, exceeding traditional graph processing techniques. We also\ndevelop a novel message aggregation scheme for Message Passing Neural Networks\nthat further improves the performance of GNNs on this task. We apply the model\nto synthetic datasets, a public benchmark dataset, and an internal application.\nOur results demonstrate the value of graph classification in IER and the\nability of graph neural networks to develop useful representations for graph\npartitioning.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 20:39:22 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Barton", "Robert A.", ""], ["Neiman", "Tal", ""], ["Yuan", "Changhe", ""]]}, {"id": "2105.05975", "submitter": "B{\\l}a\\.zej Dolicki", "authors": "B{\\l}a\\.zej Dolicki and Gerasimos Spanakis", "title": "Analysing The Impact Of Linguistic Features On Cross-Lingual Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing amount of evidence that in cases with little or no\ndata in a target language, training on a different language can yield\nsurprisingly good results. However, currently there are no established\nguidelines for choosing the training (source) language. In attempt to solve\nthis issue we thoroughly analyze a state-of-the-art multilingual model and try\nto determine what impacts good transfer between languages. As opposed to the\nmajority of multilingual NLP literature, we don't only train on English, but on\na group of almost 30 languages. We show that looking at particular syntactic\nfeatures is 2-4 times more helpful in predicting the performance than an\naggregated syntactic similarity. We find out that the importance of syntactic\nfeatures strongly differs depending on the downstream task - no single feature\nis a good performance predictor for all NLP tasks. As a result, one should not\nexpect that for a target language $L_1$ there is a single language $L_2$ that\nis the best choice for any NLP task (for instance, for Bulgarian, the best\nsource language is French on POS tagging, Russian on NER and Thai on NLI). We\ndiscuss the most important linguistic features affecting the transfer quality\nusing statistical and machine learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:22:58 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Dolicki", "B\u0142a\u017cej", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "2105.05977", "submitter": "Alex Kuznetsov", "authors": "Alex Kuznetsov, Hector Urdiales", "title": "Spelling Correction with Denoising Transformer", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method of performing spelling correction on short input\nstrings, such as search queries or individual words. At its core lies a\nprocedure for generating artificial typos which closely follow the error\npatterns manifested by humans. This procedure is used to train the production\nspelling correction model based on a transformer architecture. This model is\ncurrently served in the HubSpot product search. We show that our approach to\ntypo generation is superior to the widespread practice of adding noise, which\nignores human patterns. We also demonstrate how our approach may be extended to\nresource-scarce settings and train spelling correction models for Arabic,\nGreek, Russian, and Setswana languages, without using any labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:35:18 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Kuznetsov", "Alex", ""], ["Urdiales", "Hector", ""]]}, {"id": "2105.05983", "submitter": "Lucas Tsutsui Da Silva", "authors": "Lucas Tsutsui da Silva, Vinicius M. A. Souza, Gustavo E. A. P. A.\n  Batista", "title": "An Open-Source Tool for Classification Models in Resource-Constrained\n  Hardware", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications that need to sense, measure, and gather real-time information\nfrom the environment frequently face three main restrictions: power\nconsumption, cost, and lack of infrastructure. Most of the challenges imposed\nby these limitations can be better addressed by embedding Machine Learning (ML)\nclassifiers in the hardware that senses the environment, creating smart sensors\nable to interpret the low-level data stream. However, for this approach to be\ncost-effective, we need highly efficient classifiers suitable to execute in\nunresourceful hardware, such as low-power microcontrollers. In this paper, we\npresent an open-source tool named EmbML - Embedded Machine Learning that\nimplements a pipeline to develop classifiers for resource-constrained hardware.\nWe describe its implementation details and provide a comprehensive analysis of\nits classifiers considering accuracy, classification time, and memory usage.\nMoreover, we compare the performance of its classifiers with classifiers\nproduced by related tools to demonstrate that our tool provides a diverse set\nof classification algorithms that are both compact and accurate. Finally, we\nvalidate EmbML classifiers in a practical application of a smart sensor and\ntrap for disease vector mosquitoes.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:51:02 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["da Silva", "Lucas Tsutsui", ""], ["Souza", "Vinicius M. A.", ""], ["Batista", "Gustavo E. A. P. A.", ""]]}, {"id": "2105.05985", "submitter": "Xintong Yang", "authors": "Xintong Yang, Ze Ji, Jing Wu, Yu-Kun Lai", "title": "An Open-Source Multi-Goal Reinforcement Learning Environment for Robotic\n  Manipulation with Pybullet", "comments": "Submitted to Taros 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work re-implements the OpenAI Gym multi-goal robotic manipulation\nenvironment, originally based on the commercial Mujoco engine, onto the\nopen-source Pybullet engine. By comparing the performances of the Hindsight\nExperience Replay-aided Deep Deterministic Policy Gradient agent on both\nenvironments, we demonstrate our successful re-implementation of the original\nenvironment. Besides, we provide users with new APIs to access a joint control\nmode, image observations and goals with customisable camera and a built-in\non-hand camera. We further design a set of multi-step, multi-goal, long-horizon\nand sparse reward robotic manipulation tasks, aiming to inspire new\ngoal-conditioned reinforcement learning algorithms for such challenges. We use\na simple, human-prior-based curriculum learning method to benchmark the\nmulti-step manipulation tasks. Discussions about future research opportunities\nregarding this kind of tasks are also provided.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:58:57 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Yang", "Xintong", ""], ["Ji", "Ze", ""], ["Wu", "Jing", ""], ["Lai", "Yu-Kun", ""]]}, {"id": "2105.05991", "submitter": "Gareth Aye", "authors": "Wen Zhou, Seohyun Kim, Vijayaraghavan Murali, Gareth Ari Aye", "title": "Improving Code Autocompletion with Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software language models have achieved promising results predicting code\ncompletion usages, and several industry studies have described successful IDE\nintegrations. Recently, accuracy in autocompletion prediction improved 12.8%\nfrom training on a real-world dataset collected from programmers' IDE activity.\nBut what if limited examples of IDE autocompletion in the target programming\nlanguage are available for model training? In this paper, we investigate the\nefficacy of pretraining autocompletion models on non-IDE, non-autocompletion,\nand different-language example code sequences. We find that these unsupervised\npretrainings improve model accuracy by over 50% on very small fine-tuning\ndatasets and over 10% on 50k labeled examples. We confirm the real-world impact\nof these pretrainings in an online setting through A/B testing on thousands of\nIDE autocompletion users, finding that pretraining is responsible for increases\nof up to 6.63% autocompletion usage.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 22:22:11 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhou", "Wen", ""], ["Kim", "Seohyun", ""], ["Murali", "Vijayaraghavan", ""], ["Aye", "Gareth Ari", ""]]}, {"id": "2105.05996", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Marcos Zampieri", "title": "Multilingual Offensive Language Identification for Low-resource\n  Languages", "comments": "Accepted to ACM Transactions on Asian and Low-Resource Language\n  Information Processing (TALLIP). This is an extended version of a paper\n  accepted to EMNLP. arXiv admin note: substantial text overlap with\n  arXiv:2010.05324", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of available\nEnglish datasets by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in low-resource languages. We project\npredictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,\nSpanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in\nTRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in\nOffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513\nF1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our\napproach compares favourably to the best systems submitted to recent shared\ntasks on these three languages. Additionally, we report competitive performance\non Arabic, and Turkish using the training and development sets of OffensEval\n2020 shared task. The results for all languages confirm the robustness of\ncross-lingual contextual embeddings and transfer learning for this task.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 22:50:16 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 15:39:26 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 11:20:49 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2105.06002", "submitter": "Robert Cohen", "authors": "Robert A. Cohen, Hyomin Choi, Ivan V. Baji\\'c", "title": "Lightweight compression of neural network feature tensors for\n  collaborative intelligence", "comments": "Accepted for publication in IEEE ICME 2020", "journal-ref": "2020 IEEE International Conference on Multimedia and Expo (ICME)", "doi": "10.1109/ICME46284.2020.9102797", "report-no": null, "categories": "cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a relatively low-complexity device such as a mobile phone\nor edge device, and the remainder of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to code the activations\nof a split DNN layer, while having a low complexity suitable for edge devices\nand not requiring any retraining. We also present a modified\nentropy-constrained quantizer design algorithm optimized for clipped\nactivations. When applied to popular object-detection and classification DNNs,\nwe were able to compress the 32-bit floating point activations down to 0.6 to\n0.8 bits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding a\nlayer's activations in split neural networks for edge/cloud applications.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 23:41:35 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Cohen", "Robert A.", ""], ["Choi", "Hyomin", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2105.06015", "submitter": "Yi Xu", "authors": "Yi Xu, Qi Qian, Hao Li, Rong Jin", "title": "Why Does Multi-Epoch Training Help?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic gradient descent (SGD) has become the most attractive optimization\nmethod in training large-scale deep neural networks due to its simplicity, low\ncomputational cost in each updating step, and good performance. Standard excess\nrisk bounds show that SGD only needs to take one pass over the training data\nand more passes could not help to improve the performance. Empirically, it has\nbeen observed that SGD taking more than one pass over the training data\n(multi-pass SGD) has much better excess risk bound performance than the SGD\nonly taking one pass over the training data (one-pass SGD). However, it is not\nvery clear that how to explain this phenomenon in theory. In this paper, we\nprovide some theoretical evidences for explaining why multiple passes over the\ntraining data can help improve performance under certain circumstance.\nSpecifically, we consider smooth risk minimization problems whose objective\nfunction is non-convex least squared loss. Under Polyak-Lojasiewicz (PL)\ncondition, we establish faster convergence rate of excess risk bound for\nmulti-pass SGD than that for one-pass SGD.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 00:52:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Xu", "Yi", ""], ["Qian", "Qi", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "2105.06018", "submitter": "Bin Liu", "authors": "Bin Liu", "title": "Robust Dynamic Multi-Modal Data Fusion: A Model Uncertainty Perspective", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper is concerned with multi-modal data fusion (MMDF) under unexpected\nmodality failures in nonlinear non-Gaussian dynamic processes. An efficient\nframework to tackle this problem is proposed. In particular, a notion termed\nmodality \"\\emph{usefulness}\", which takes a value of 1 or 0, is used for\nindicating whether the observation of this modality is useful or not. For $n$\nmodalities involved, $2^n$ combinations of their \"\\emph{usefulness}\" values\nexist. Each combination defines one hypothetical model of the true data\ngenerative process. Then the problem of concern is formalized as a task of\nnonlinear non-Gaussian state filtering under model uncertainty, which is\naddressed by a dynamic model averaging (DMA) based particle filter (PF)\nalgorithm. This DMA algorithm employs $2^n$ models, while all models share the\nsame state-transition function and a unique set of particle values. That makes\nthe computational complexity of this algorithm only slightly larger than a\nsingle model based PF algorithm, especially for scenarios in which $n$ is\nsmall. Experimental results show that the proposed solution outperforms\nremarkably state-of-the-art methods. Code and data are available at\nhttps://github.com/robinlau1981/fusion.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:08:34 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 13:15:36 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 01:42:41 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Liu", "Bin", ""]]}, {"id": "2105.06020", "submitter": "Ruiqi Zhong", "authors": "Ruiqi Zhong, Dhruba Ghosh, Dan Klein, Jacob Steinhardt", "title": "Are Larger Pretrained Language Models Uniformly Better? Comparing\n  Performance at the Instance Level", "comments": "ACL 2021 Findings. Code and data:\n  https://github.com/ruiqi-zhong/acl2021-instance-level", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Larger language models have higher accuracy on average, but are they better\non every single instance (datapoint)? Some work suggests larger models have\nhigher out-of-distribution robustness, while other work suggests they have\nlower accuracy on rare subgroups. To understand these differences, we\ninvestigate these models at the level of individual instances. However, one\nmajor challenge is that individual predictions are highly sensitive to noise in\nthe randomness in training. We develop statistically rigorous methods to\naddress this, and after accounting for pretraining and finetuning noise, we\nfind that our BERT-Large is worse than BERT-Mini on at least 1-4% of instances\nacross MNLI, SST-2, and QQP, compared to the overall accuracy improvement of\n2-10%. We also find that finetuning noise increases with model size and that\ninstance-level accuracy has momentum: improvement from BERT-Mini to BERT-Medium\ncorrelates with improvement from BERT-Medium to BERT-Large. Our findings\nsuggest that instance-level predictions provide a rich source of information;\nwe therefore, recommend that researchers supplement model weights with model\npredictions.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:10:51 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhong", "Ruiqi", ""], ["Ghosh", "Dhruba", ""], ["Klein", "Dan", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2105.06022", "submitter": "Chenjia Bai", "authors": "Chenjia Bai, Lingxiao Wang, Lei Han, Jianye Hao, Animesh Garg, Peng\n  Liu, Zhaoran Wang", "title": "Principled Exploration via Optimistic Bootstrapping and Backward\n  Induction", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One principled approach for provably efficient exploration is incorporating\nthe upper confidence bound (UCB) into the value function as a bonus. However,\nUCB is specified to deal with linear and tabular settings and is incompatible\nwith Deep Reinforcement Learning (DRL). In this paper, we propose a principled\nexploration method for DRL through Optimistic Bootstrapping and Backward\nInduction (OB2I). OB2I constructs a general-purpose UCB-bonus through\nnon-parametric bootstrap in DRL. The UCB-bonus estimates the epistemic\nuncertainty of state-action pairs for optimistic exploration. We build\ntheoretical connections between the proposed UCB-bonus and the LSVI-UCB in a\nlinear setting. We propagate future uncertainty in a time-consistent manner\nthrough episodic backward update, which exploits the theoretical advantage and\nempirically improves the sample-efficiency. Our experiments in the MNIST maze\nand Atari suite suggest that OB2I outperforms several state-of-the-art\nexploration approaches.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:15:44 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 00:22:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bai", "Chenjia", ""], ["Wang", "Lingxiao", ""], ["Han", "Lei", ""], ["Hao", "Jianye", ""], ["Garg", "Animesh", ""], ["Liu", "Peng", ""], ["Wang", "Zhaoran", ""]]}, {"id": "2105.06025", "submitter": "Von Ralph Dane Herbuela", "authors": "Von Ralph Dane Marquez Herbuela, Tomonori Karita, Yoshiya Furukawa,\n  Yoshinori Wada, Yoshihiro Yagi, Shuichiro Senba, Eiko Onishi, Tatsuo Saeki", "title": "Machine-learning-based investigation on classifying binary and\n  multiclass behavior outcomes of children with PIMD/SMID", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the importance of weather parameters and location information to\nbetter understand the context of the communication of children with profound\nintellectual and multiple disabilities (PIMD) or severe motor and intellectual\ndisorders (SMID) has been proposed. However, an investigation on whether these\ndata can be used to classify their behavior for system optimization aimed for\npredicting their behavior for independent communication and mobility has not\nbeen done. Thus, this study investigates whether recalibrating the datasets\nincluding either minor or major behavior categories or both, combining location\nand weather data and feature selection method training (Boruta) would allow\nmore accurate classification of behavior discriminated to binary and multiclass\nclassification outcomes using eXtreme Gradient Boosting (XGB), support vector\nmachine (SVM), random forest (RF), and neural network (NN) classifiers.\nMultiple single-subject face-to-face and video-recorded sessions were conducted\namong 20 purposively sampled 8 to 10 -year old children diagnosed with\nPIMD/SMID or severe or profound intellectual disabilities and their caregivers.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:22:42 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Herbuela", "Von Ralph Dane Marquez", ""], ["Karita", "Tomonori", ""], ["Furukawa", "Yoshiya", ""], ["Wada", "Yoshinori", ""], ["Yagi", "Yoshihiro", ""], ["Senba", "Shuichiro", ""], ["Onishi", "Eiko", ""], ["Saeki", "Tatsuo", ""]]}, {"id": "2105.06029", "submitter": "Ming Yin", "authors": "Ming Yin, Yu-Xiang Wang", "title": "Optimal Uniform OPE and Model-based Offline Reinforcement Learning in\n  Time-Homogeneous, Reward-Free and Task-Agnostic Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the statistical limits of uniform convergence for offline\npolicy evaluation (OPE) problems with model-based methods (for episodic MDP)\nand provides a unified framework towards optimal learning for several\nwell-motivated offline tasks. Uniform OPE\n$\\sup_\\Pi|Q^\\pi-\\hat{Q}^\\pi|<\\epsilon$ is a stronger measure than the\npoint-wise OPE and ensures offline learning when $\\Pi$ contains all policies\n(the global class). In this paper, we establish an $\\Omega(H^2\nS/d_m\\epsilon^2)$ lower bound (over model-based family) for the global uniform\nOPE and our main result establishes an upper bound of\n$\\tilde{O}(H^2/d_m\\epsilon^2)$ for the \\emph{local} uniform convergence that\napplies to all \\emph{near-empirically optimal} policies for the MDPs with\n\\emph{stationary} transition. Here $d_m$ is the minimal marginal state-action\nprobability. Critically, the highlight in achieving the optimal rate\n$\\tilde{O}(H^2/d_m\\epsilon^2)$ is our design of \\emph{singleton absorbing MDP},\nwhich is a new sharp analysis tool that works with the model-based approach. We\ngeneralize such a model-based framework to the new settings: offline\ntask-agnostic and the offline reward-free with optimal complexity\n$\\tilde{O}(H^2\\log(K)/d_m\\epsilon^2)$ ($K$ is the number of tasks) and\n$\\tilde{O}(H^2S/d_m\\epsilon^2)$ respectively. These results provide a unified\nsolution for simultaneously solving different offline RL problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:36:34 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 04:32:16 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 07:01:53 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yin", "Ming", ""], ["Wang", "Yu-Xiang", ""]]}, {"id": "2105.06031", "submitter": "Yifeng Fan", "authors": "Yifeng Fan, Yuehaw Khoo and Zhizhen Zhao", "title": "Joint Community Detection and Rotational Synchronization via\n  Semidefinite Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the presence of heterogeneous data, where randomly rotated objects fall\ninto multiple underlying categories, it is challenging to simultaneously\nclassify them into clusters and synchronize them based on pairwise relations.\nThis gives rise to the joint problem of community detection and\nsynchronization. We propose a series of semidefinite relaxations, and prove\ntheir exact recovery when extending the celebrated stochastic block model to\nthis new setting where both rotations and cluster identities are to be\ndetermined. Numerical experiments demonstrate the efficacy of our proposed\nalgorithms and confirm our theoretical result which indicates a sharp phase\ntransition for exact recovery.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:40:20 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Fan", "Yifeng", ""], ["Khoo", "Yuehaw", ""], ["Zhao", "Zhizhen", ""]]}, {"id": "2105.06035", "submitter": "Yongchao Liu", "authors": "Qinkai Zheng, Houyi Li, Peng Zhang, Zhixiong Yang, Guowei Zhang,\n  Xintan Zeng, Yongchao Liu", "title": "GIPA: General Information Propagation Algorithm for Graph Learning", "comments": "4 pages, 1 figure, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have been popularly used in analyzing\ngraph-structured data, showing promising results in various applications such\nas node classification, link prediction and network recommendation. In this\npaper, we present a new graph attention neural network, namely GIPA, for\nattributed graph data learning. GIPA consists of three key components:\nattention, feature propagation and aggregation. Specifically, the attention\ncomponent introduces a new multi-layer perceptron based multi-head to generate\nbetter non-linear feature mapping and representation than conventional\nimplementations such as dot-product. The propagation component considers not\nonly node features but also edge features, which differs from existing GNNs\nthat merely consider node features. The aggregation component uses a residual\nconnection to generate the final embedding. We evaluate the performance of GIPA\nusing the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The\nexperimental results reveal that GIPA can beat the state-of-the-art models in\nterms of prediction accuracy, e.g., GIPA achieves an average ROC-AUC of\n$0.8700\\pm 0.0010$ and outperforms all the previous methods listed in the\nogbn-proteins leaderboard.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:50:43 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zheng", "Qinkai", ""], ["Li", "Houyi", ""], ["Zhang", "Peng", ""], ["Yang", "Zhixiong", ""], ["Zhang", "Guowei", ""], ["Zeng", "Xintan", ""], ["Liu", "Yongchao", ""]]}, {"id": "2105.06047", "submitter": "Rahul Duggal", "authors": "Rahul Duggal, Hao Zhou, Shuo Yang, Yuanjun Xiong, Wei Xia, Zhuowen Tu,\n  Stefano Soatto", "title": "Compatibility-aware Heterogeneous Visual Search", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of visual search under resource constraints. Existing\nsystems use the same embedding model to compute representations (embeddings)\nfor the query and gallery images. Such systems inherently face a hard\naccuracy-efficiency trade-off: the embedding model needs to be large enough to\nensure high accuracy, yet small enough to enable query-embedding computation on\nresource-constrained platforms. This trade-off could be mitigated if gallery\nembeddings are generated from a large model and query embeddings are extracted\nusing a compact model. The key to building such a system is to ensure\nrepresentation compatibility between the query and gallery models. In this\npaper, we address two forms of compatibility: One enforced by modifying the\nparameters of each model that computes the embeddings. The other by modifying\nthe architectures that compute the embeddings, leading to compatibility-aware\nneural architecture search (CMP-NAS). We test CMP-NAS on challenging retrieval\ntasks for fashion images (DeepFashion2), and face images (IJB-C). Compared to\nordinary (homogeneous) visual search using the largest embedding model\n(paragon), CMP-NAS achieves 80-fold and 23-fold cost reduction while\nmaintaining accuracy within 0.3% and 1.6% of the paragon on DeepFashion2 and\nIJB-C respectively.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 02:30:50 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Duggal", "Rahul", ""], ["Zhou", "Hao", ""], ["Yang", "Shuo", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""], ["Tu", "Zhuowen", ""], ["Soatto", "Stefano", ""]]}, {"id": "2105.06052", "submitter": "Zidu Wang", "authors": "Zidu Wang, Xuexin Liu, Long Huang, Yunqing Chen, Yufei Zhang, Zhikang\n  Lin, Rui Wang", "title": "Model Pruning Based on Quantified Similarity of Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A high-accuracy CNN is often accompanied by huge parameters, which are\nusually stored in the high-dimensional tensors. However, there are few methods\ncan figure out the redundant information of the parameters stored in the\nhigh-dimensional tensors, which leads to the lack of theoretical guidance for\nthe compression of CNNs. In this paper, we propose a novel theory to find\nredundant information in three dimensional tensors, namely Quantified\nSimilarity of Feature Maps (QSFM), and use this theory to prune convolutional\nneural networks to enhance the inference speed. Our method belongs to filter\npruning, which can be implemented without using any special libraries. We\nperform our method not only on common convolution layers but also on special\nconvolution layers, such as depthwise separable convolution layers. The\nexperiments prove that QSFM can find the redundant information in the neural\nnetwork effectively. Without any fine-tuning operation, QSFM can compress\nResNet-56 on CIFAR-10 significantly (48.27% FLOPs and 57.90% parameters\nreduction) with only a loss of 0.54% in the top-1 accuracy. QSFM also prunes\nResNet-56, VGG-16 and MobileNetV2 with fine-tuning operation, which also shows\nexcellent results.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 02:57:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Zidu", ""], ["Liu", "Xuexin", ""], ["Huang", "Long", ""], ["Chen", "Yunqing", ""], ["Zhang", "Yufei", ""], ["Lin", "Zhikang", ""], ["Wang", "Rui", ""]]}, {"id": "2105.06060", "submitter": "Hoormazd Rezaei", "authors": "Sina Jandaghi Semnani, Hoormazd Rezaei", "title": "House Price Prediction using Satellite Imagery", "comments": "Stanford CS230 Deep Learning, Winter 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how using satellite images can improve the accuracy of\nhousing price estimation models. Using Los Angeles County's property assessment\ndataset, by transferring learning from an Inception-v3 model pretrained on\nImageNet, we could achieve an improvement of ~10% in R-squared score compared\nto two baseline models that only use non-image features of the house.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 03:25:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Semnani", "Sina Jandaghi", ""], ["Rezaei", "Hoormazd", ""]]}, {"id": "2105.06072", "submitter": "Jincheng Mei", "authors": "Jincheng Mei, Yue Gao, Bo Dai, Csaba Szepesvari, Dale Schuurmans", "title": "Leveraging Non-uniformity in First-order Non-convex Optimization", "comments": "48 pages, 10 figures. Accepted at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical global convergence results for first-order methods rely on uniform\nsmoothness and the \\L{}ojasiewicz inequality. Motivated by properties of\nobjective functions that arise in machine learning, we propose a non-uniform\nrefinement of these notions, leading to \\emph{Non-uniform Smoothness} (NS) and\n\\emph{Non-uniform \\L{}ojasiewicz inequality} (N\\L{}). The new definitions\ninspire new geometry-aware first-order methods that are able to converge to\nglobal optimality faster than the classical $\\Omega(1/t^2)$ lower bounds. To\nillustrate the power of these geometry-aware methods and their corresponding\nnon-uniform analysis, we consider two important problems in machine learning:\npolicy gradient optimization in reinforcement learning (PG), and generalized\nlinear model training in supervised learning (GLM). For PG, we find that\nnormalizing the gradient ascent method can accelerate convergence to\n$O(e^{-t})$ while incurring less overhead than existing algorithms. For GLM, we\nshow that geometry-aware normalized gradient descent can also achieve a linear\nconvergence rate, which significantly improves the best known results. We\nadditionally show that the proposed geometry-aware descent methods escape\nlandscape plateaus faster than standard gradient descent. Experimental results\nare used to illustrate and complement the theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 04:23:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Mei", "Jincheng", ""], ["Gao", "Yue", ""], ["Dai", "Bo", ""], ["Szepesvari", "Csaba", ""], ["Schuurmans", "Dale", ""]]}, {"id": "2105.06073", "submitter": "Johannes Royset", "authors": "Johannes O. Royset", "title": "Good and Bad Optimization Models: Insights from Rockafellians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic requirement for a mathematical model is often that its solution\n(output) shouldn't change much if the model's parameters (input) are perturbed.\nThis is important because the exact values of parameters may not be known and\none would like to avoid being mislead by an output obtained using incorrect\nvalues. Thus, it's rarely enough to address an application by formulating a\nmodel, solving the resulting optimization problem and presenting the solution\nas the answer. One would need to confirm that the model is suitable, i.e.,\n\"good,\" and this can, at least in part, be achieved by considering a family of\noptimization problems constructed by perturbing parameters of concern. The\nresulting sensitivity analysis uncovers troubling situations with unstable\nsolutions, which we referred to as \"bad\" models, and indicates better model\nformulations. Embedding an actual problem of interest within a family of\nproblems is also a primary path to optimality conditions as well as\ncomputationally attractive, alternative problems, which under ideal\ncircumstances, and when properly tuned, may even furnish the minimum value of\nthe actual problem. The tuning of these alternative problems turns out to be\nintimately tied to finding multipliers in optimality conditions and thus\nemerges as a main component of several optimization algorithms. In fact, the\ntuning amounts to solving certain dual optimization problems. In this tutorial,\nwe'll discuss the opportunities and insights afforded by this broad\nperspective.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 04:31:42 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 12:12:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Royset", "Johannes O.", ""]]}, {"id": "2105.06084", "submitter": "Hung Tuan Nguyen Dr.", "authors": "Thanh-Nghia Truong, Hung Tuan Nguyen, Cuong Tuan Nguyen and Masaki\n  Nakagawa", "title": "Learning symbol relation tree for online mathematical expression\n  recognition", "comments": "13 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for recognizing online handwritten mathematical\nexpressions (OnHME) by building a symbol relation tree (SRT) directly from a\nsequence of strokes. A bidirectional recurrent neural network learns from\nmultiple derived paths of SRT to predict both symbols and spatial relations\nbetween symbols using global context. The recognition system has two parts: a\ntemporal classifier and a tree connector. The temporal classifier produces an\nSRT by recognizing an OnHME pattern. The tree connector splits the SRT into\nseveral sub-SRTs. The final SRT is formed by looking up the best combination\namong those sub-SRTs. Besides, we adopt a tree sorting method to deal with\nvarious stroke orders. Recognition experiments indicate that the proposed OnHME\nrecognition system is competitive to other methods. The recognition system\nachieves 44.12% and 41.76% expression recognition rates on the Competition on\nRecognition of Online Handwritten Mathematical Expressions (CROHME) 2014 and\n2016 testing sets.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 05:18:17 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Truong", "Thanh-Nghia", ""], ["Nguyen", "Hung Tuan", ""], ["Nguyen", "Cuong Tuan", ""], ["Nakagawa", "Masaki", ""]]}, {"id": "2105.06126", "submitter": "Quoc Phong Nguyen", "authors": "Quoc Phong Nguyen and Zhongxiang Dai and Bryan Kian Hsiang Low and\n  Patrick Jaillet", "title": "Value-at-Risk Optimization with Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value-at-risk (VaR) is an established measure to assess risks in critical\nreal-world applications with random environmental factors. This paper presents\na novel VaR upper confidence bound (V-UCB) algorithm for maximizing the VaR of\na black-box objective function with the first no-regret guarantee. To realize\nthis, we first derive a confidence bound of VaR and then prove the existence of\nvalues of the environmental random variable (to be selected to achieve no\nregret) such that the confidence bound of VaR lies within that of the objective\nfunction evaluated at such values. Our V-UCB algorithm empirically demonstrates\nstate-of-the-art performance in optimizing synthetic benchmark functions, a\nportfolio optimization problem, and a simulated robot task.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:00:22 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Nguyen", "Quoc Phong", ""], ["Dai", "Zhongxiang", ""], ["Low", "Bryan Kian Hsiang", ""], ["Jaillet", "Patrick", ""]]}, {"id": "2105.06129", "submitter": "Aaditya Singh", "authors": "Aaditya Singh, Shreeshail Hingane, Xinyu Gong, Zhangyang Wang", "title": "SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance\n  Normalization", "comments": "Accepted at ICME 2021, 5 Pages + 1 Page (references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artistic style transfer aims to transfer the style characteristics of one\nimage onto another image while retaining its content. Existing approaches\ncommonly leverage various normalization techniques, although these face\nlimitations in adequately transferring diverse textures to different spatial\nlocations. Self-Attention-based approaches have tackled this issue with partial\nsuccess but suffer from unwanted artifacts. Motivated by these observations,\nthis paper aims to combine the best of both worlds: self-attention and\nnormalization. That yields a new plug-and-play module that we name\nSelf-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially\na spatially adaptive normalization module whose parameters are inferred through\nattention on the content and style image. We demonstrate that plugging SAFIN\ninto the base network of another state-of-the-art method results in enhanced\nstylization. We also develop a novel base network composed of Wavelet Transform\nfor multi-scale style transfer, which when combined with SAFIN, produces\nvisually appealing results with lesser unwanted textures.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:01:01 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 05:44:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Singh", "Aaditya", ""], ["Hingane", "Shreeshail", ""], ["Gong", "Xinyu", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2105.06136", "submitter": "Hui Wang", "authors": "Hui Wang and Mike Preuss and Aske Plaat", "title": "Adaptive Warm-Start MCTS in AlphaZero-like Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  AlphaZero has achieved impressive performance in deep reinforcement learning\nby utilizing an architecture that combines search and training of a neural\nnetwork in self-play. Many researchers are looking for ways to reproduce and\nimprove results for other games/tasks. However, the architecture is designed to\nlearn from scratch, tabula rasa, accepting a cold-start problem in self-play.\nRecently, a warm-start enhancement method for Monte Carlo Tree Search was\nproposed to improve the self-play starting phase. It employs a fixed parameter\n$I^\\prime$ to control the warm-start length. Improved performance was reported\nin small board games. In this paper we present results with an adaptive switch\nmethod. Experiments show that our approach works better than the fixed\n$I^\\prime$, especially for \"deep,\" tactical, games (Othello and Connect Four).\nWe conjecture that the adaptive value for $I^\\prime$ is also influenced by the\nsize of the game, and that on average $I^\\prime$ will increase with game size.\nWe conclude that AlphaZero-like deep reinforcement learning benefits from\nadaptive rollout based warm-start, as Rapid Action Value Estimate did for\nrollout-based reinforcement learning 15 years ago.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:24:51 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Hui", ""], ["Preuss", "Mike", ""], ["Plaat", "Aske", ""]]}, {"id": "2105.06141", "submitter": "Matteo Chieregato", "authors": "Matteo Chieregato, Fabio Frangiamore, Mauro Morassi, Claudia Baresi,\n  Stefania Nici, Chiara Bassetti, Claudio Bn\\`a and Marco Galelli", "title": "A hybrid machine learning/deep learning COVID-19 severity predictive\n  model from CT images and clinical data", "comments": "16 pages, 10 figures, 2 supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  COVID-19 clinical presentation and prognosis are highly variable, ranging\nfrom asymptomatic and paucisymptomatic cases to acute respiratory distress\nsyndrome and multi-organ involvement. We developed a hybrid machine\nlearning/deep learning model to classify patients in two outcome categories,\nnon-ICU and ICU (intensive care admission or death), using 558 patients\nadmitted in a northern Italy hospital in February/May of 2020. A fully 3D\npatient-level CNN classifier on baseline CT images is used as feature\nextractor. Features extracted, alongside with laboratory and clinical data, are\nfed for selection in a Boruta algorithm with SHAP game theoretical values. A\nclassifier is built on the reduced feature space using CatBoost gradient\nboosting algorithm and reaching a probabilistic AUC of 0.949 on holdout test\nset. The model aims to provide clinical decision support to medical doctors,\nwith the probability score of belonging to an outcome class and with case-based\nSHAP interpretation of features importance.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:39:56 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chieregato", "Matteo", ""], ["Frangiamore", "Fabio", ""], ["Morassi", "Mauro", ""], ["Baresi", "Claudia", ""], ["Nici", "Stefania", ""], ["Bassetti", "Chiara", ""], ["Bn\u00e0", "Claudio", ""], ["Galelli", "Marco", ""]]}, {"id": "2105.06165", "submitter": "Dorjan Hitaj", "authors": "Giulio Pagnotta, Dorjan Hitaj, Fabio De Gaspari, Luigi V. Mancini", "title": "PassFlow: Guessing Passwords with Generative Flows", "comments": "19 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in generative machine learning models rekindled research\ninterest in the area of password guessing. Data-driven password guessing\napproaches based on GANs, language models and deep latent variable models show\nimpressive generalization performance and offer compelling properties for the\ntask of password guessing. In this paper, we propose a flow-based generative\nmodel approach to password guessing. Flow-based models allow for precise\nlog-likelihood computation and optimization, which enables exact latent\nvariable inference. Additionally, flow-based models provide meaningful latent\nspace representation, which enables operations such as exploration of specific\nsubspaces of the latent space and interpolation. We demonstrate the\napplicability of generative flows to the context of password guessing,\ndeparting from previous applications of flow networks which are mainly limited\nto the continuous space of image generation. We show that the above-mentioned\nproperties allow flow-based models to outperform deep latent variable model\napproaches and remain competitive with state-of-the-art GANs in the password\nguessing task, while using a training set that is orders of magnitudes smaller\nthan that of previous art. Furthermore, a qualitative analysis of the generated\nsamples shows that flow-based networks are able to accurately model the\noriginal passwords distribution, with even non-matched samples closely\nresembling human-like passwords.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 09:50:36 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pagnotta", "Giulio", ""], ["Hitaj", "Dorjan", ""], ["De Gaspari", "Fabio", ""], ["Mancini", "Luigi V.", ""]]}, {"id": "2105.06168", "submitter": "Mansura Habiba Miss", "authors": "Mehrdad Maleki and Mansura Habiba and Barak A. Pearlmutter", "title": "HeunNet: Extending ResNet using Heun's Methods", "comments": "Irish Signals & Systems Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an analogy between the ResNet (Residual Network) architecture for\ndeep neural networks and an Euler solver for an ODE. The transformation\nperformed by each layer resembles an Euler step in solving an ODE. We consider\nthe Heun Method, which involves a single predictor-corrector cycle, and\ncomplete the analogy, building a predictor-corrector variant of ResNet, which\nwe call a HeunNet. Just as Heun's method is more accurate than Euler's,\nexperiments show that HeunNet achieves high accuracy with low computational\n(both training and test) time compared to both vanilla recurrent neural\nnetworks and other ResNet variants.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 09:55:26 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 14:50:53 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Maleki", "Mehrdad", ""], ["Habiba", "Mansura", ""], ["Pearlmutter", "Barak A.", ""]]}, {"id": "2105.06178", "submitter": "Tarek Allam", "authors": "Tarek Allam Jr., Jason D. McEwen", "title": "Paying Attention to Astronomical Transients: Photometric Classification\n  with the Time-Series Transformer", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Future surveys such as the Legacy Survey of Space and Time (LSST) of the Vera\nC. Rubin Observatory will observe an order of magnitude more astrophysical\ntransient events than any previous survey before. With this deluge of\nphotometric data, it will be impossible for all such events to be classified by\nhumans alone. Recent efforts have sought to leverage machine learning methods\nto tackle the challenge of astronomical transient classification, with ever\nimproving success. Transformers are a recently developed deep learning\narchitecture, first proposed for natural language processing, that have shown a\ngreat deal of recent success. In this work we develop a new transformer\narchitecture, which uses multi-head self attention at its core, for general\nmulti-variate time-series data. Furthermore, the proposed time-series\ntransformer architecture supports the inclusion of an arbitrary number of\nadditional features, while also offering interpretability. We apply the\ntime-series transformer to the task of photometric classification, minimising\nthe reliance of expert domain knowledge for feature selection, while achieving\nresults comparable to state-of-the-art photometric classification methods. We\nachieve a weighted logarithmic-loss of 0.507 on imbalanced data in a\nrepresentative setting using data from the Photometric LSST Astronomical\nTime-Series Classification Challenge (PLAsTiCC). Moreover, we achieve a\nmicro-averaged receiver operating characteristic area under curve of 0.98 and\nmicro-averaged precision-recall area under curve of 0.87.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 10:16:13 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Allam", "Tarek", "Jr."], ["McEwen", "Jason D.", ""]]}, {"id": "2105.06183", "submitter": "Luca Mocerino", "authors": "Luca Mocerino, Roberto G. Rizzo, Valentino Peluso, Andrea Calimera,\n  Enrico Macii", "title": "Adaptive Test-Time Augmentation for Low-Power CPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (ConvNets) are trained offline using the few\navailable data and may therefore suffer from substantial accuracy loss when\nported on the field, where unseen input patterns received under unpredictable\nexternal conditions can mislead the model. Test-Time Augmentation (TTA)\ntechniques aim to alleviate such common side effect at inference-time, first\nrunning multiple feed-forward passes on a set of altered versions of the same\ninput sample, and then computing the main outcome through a consensus of the\naggregated predictions. Unfortunately, the implementation of TTA on embedded\nCPUs introduces latency penalties that limit its adoption on edge applications.\nTo tackle this issue, we propose AdapTTA, an adaptive implementation of TTA\nthat controls the number of feed-forward passes dynamically, depending on the\ncomplexity of the input. Experimental results on state-of-the-art ConvNets for\nimage classification deployed on a commercial ARM Cortex-A CPU demonstrate\nAdapTTA reaches remarkable latency savings, from 1.49X to 2.21X, and hence a\nhigher frame rate compared to static TTA, still preserving the same accuracy\ngain.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 10:50:13 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Mocerino", "Luca", ""], ["Rizzo", "Roberto G.", ""], ["Peluso", "Valentino", ""], ["Calimera", "Andrea", ""], ["Macii", "Enrico", ""]]}, {"id": "2105.06209", "submitter": "Yingzhe He", "authors": "Yingzhe He, Guozhu Meng, Kai Chen, Jinwen He, Xingbo Hu", "title": "DeepObliviate: A Powerful Charm for Erasing Data Residual Memory in Deep\n  Neural Networks", "comments": "16 pages, 10 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine unlearning has great significance in guaranteeing model security and\nprotecting user privacy. Additionally, many legal provisions clearly stipulate\nthat users have the right to demand model providers to delete their own data\nfrom training set, that is, the right to be forgotten. The naive way of\nunlearning data is to retrain the model without it from scratch, which becomes\nextremely time and resource consuming at the modern scale of deep neural\nnetworks. Other unlearning approaches by refactoring model or training data\nstruggle to gain a balance between overhead and model usability.\n  In this paper, we propose an approach, dubbed as DeepObliviate, to implement\nmachine unlearning efficiently, without modifying the normal training mode. Our\napproach improves the original training process by storing intermediate models\non the hard disk. Given a data point to unlearn, we first quantify its temporal\nresidual memory left in stored models. The influenced models will be retrained\nand we decide when to terminate the retraining based on the trend of residual\nmemory on-the-fly. Last, we stitch an unlearned model by combining the\nretrained models and uninfluenced models. We extensively evaluate our approach\non five datasets and deep learning models. Compared to the method of retraining\nfrom scratch, our approach can achieve 99.0%, 95.0%, 91.9%, 96.7%, 74.1%\naccuracy rates and 66.7$\\times$, 75.0$\\times$, 33.3$\\times$, 29.4$\\times$,\n13.7$\\times$ speedups on the MNIST, SVHN, CIFAR-10, Purchase, and ImageNet\ndatasets, respectively. Compared to the state-of-the-art unlearning approach,\nwe improve 5.8% accuracy, 32.5$\\times$ prediction speedup, and reach a\ncomparable retrain speedup under identical settings on average on these\ndatasets. Additionally, DeepObliviate can also pass the backdoor-based\nunlearning verification.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:02:04 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["He", "Yingzhe", ""], ["Meng", "Guozhu", ""], ["Chen", "Kai", ""], ["He", "Jinwen", ""], ["Hu", "Xingbo", ""]]}, {"id": "2105.06211", "submitter": "Kartheek Kumar Reddy Nareddy", "authors": "Kartheek Kumar Reddy Nareddy, Mani Madhoolika Bulusu, Praveen Kumar\n  Pokala, Chandra Sekhar Seelamantula", "title": "Quantized Proximal Averaging Network for Analysis Sparse Coding", "comments": "8 pages + references, 7 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We solve the analysis sparse coding problem considering a combination of\nconvex and non-convex sparsity promoting penalties. The multi-penalty\nformulation results in an iterative algorithm involving proximal-averaging. We\nthen unfold the iterative algorithm into a trainable network that facilitates\nlearning the sparsity prior. We also consider quantization of the network\nweights. Quantization makes neural networks efficient both in terms of memory\nand computation during inference, and also renders them compatible for\nlow-precision hardware deployment. Our learning algorithm is based on a variant\nof the ADAM optimizer in which the quantizer is part of the forward pass and\nthe gradients of the loss function are evaluated corresponding to the quantized\nweights while doing a book-keeping of the high-precision weights. We\ndemonstrate applications to compressed image recovery and magnetic resonance\nimage reconstruction. The proposed approach offers superior reconstruction\naccuracy and quality than state-of-the-art unfolding techniques and the\nperformance degradation is minimal even when the weights are subjected to\nextreme quantization.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:05:35 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Nareddy", "Kartheek Kumar Reddy", ""], ["Bulusu", "Mani Madhoolika", ""], ["Pokala", "Praveen Kumar", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "2105.06228", "submitter": "Zhiwei Xu", "authors": "Zhiwei Xu, Yunpeng Bai, Dapeng Li, Bin Zhang, Guoliang Fan", "title": "SIDE: I Infer the State I Want to Learn", "comments": "8 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the solutions to the Dec-POMDP problem, the value decomposition\nmethod has achieved good results recently. However, most value decomposition\nmethods require the global state during training, but this is not feasible in\nsome scenarios where the global state cannot be obtained. Therefore, we propose\na novel value decomposition framework, named State Inference for value\nDEcomposition (SIDE), which eliminates the need to know the true state by\nsimultaneously seeking solutions to the two problems of optimal control and\nstate inference. SIDE can be extended to any value decomposition method, as\nwell as other types of multi-agent algorithms in the case of Dec-POMDP. Based\non the performance results of different algorithms in Starcraft II\nmicromanagement tasks, we verified that SIDE can construct the current state\nthat contributes to the reinforcement learning process based on past local\nobservations.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:26:02 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Xu", "Zhiwei", ""], ["Bai", "Yunpeng", ""], ["Li", "Dapeng", ""], ["Zhang", "Bin", ""], ["Fan", "Guoliang", ""]]}, {"id": "2105.06238", "submitter": "Reza Azad", "authors": "Afshin Bozorgpour, Reza Azad, Eman Showkatian, Alaa Sulaiman", "title": "Multi-scale Regional Attention Deeplab3+: Multiple Myeloma Plasma Cells\n  Segmentation in Microscopic Images", "comments": "10 pages, 5 figures, presented at ISBI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple myeloma cancer is a type of blood cancer that happens when the\ngrowth of abnormal plasma cells becomes out of control in the bone marrow.\nThere are various ways to diagnose multiple myeloma in bone marrow such as\ncomplete blood count test (CBC) or counting myeloma plasma cell in aspirate\nslide images using manual visualization or through image processing technique.\nIn this work, an automatic deep learning method for the detection and\nsegmentation of multiple myeloma plasma cell have been explored. To this end, a\ntwo-stage deep learning method is designed. In the first stage, the nucleus\ndetection network is utilized to extract each instance of a cell of interest.\nThe extracted instance is then fed to the multi-scale function to generate a\nmulti-scale representation. The objective of the multi-scale function is to\ncapture the shape variation and reduce the effect of object scale on the\ncytoplasm segmentation network. The generated scales are then fed into a\npyramid of cytoplasm networks to learn the segmentation map in various scales.\nOn top of the cytoplasm segmentation network, we included a scale aggregation\nfunction to refine and generate a final prediction. The proposed approach has\nbeen evaluated on the SegPC2021 grand-challenge and ranked second on the final\ntest phase among all teams.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:42:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Bozorgpour", "Afshin", ""], ["Azad", "Reza", ""], ["Showkatian", "Eman", ""], ["Sulaiman", "Alaa", ""]]}, {"id": "2105.06241", "submitter": "David Heckerman", "authors": "David Heckerman and Dan Geiger", "title": "Likelihoods and Parameter Priors for Bayesian Networks", "comments": "This version has improved pointers to the literature", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop simple methods for constructing likelihoods and parameter priors\nfor learning about the parameters and structure of a Bayesian network. In\nparticular, we introduce several assumptions that permit the construction of\nlikelihoods and parameter priors for a large number of Bayesian-network\nstructures from a small set of assessments. The most notable assumption is that\nof likelihood equivalence, which says that data can not help to discriminate\nnetwork structures that encode the same assertions of conditional independence.\nWe describe the constructions that follow from these assumptions, and also\npresent a method for directly computing the marginal likelihood of a random\nsample with no missing observations. Also, we show how these assumptions lead\nto a general framework for characterizing parameter priors of multivariate\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:45:44 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 19:42:53 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Heckerman", "David", ""], ["Geiger", "Dan", ""]]}, {"id": "2105.06246", "submitter": "Carl Ringqvist Mr", "authors": "Adam Lindhe, Carl Ringqvist and Henrik Hult", "title": "Variational Auto Encoder Gradient Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering using deep neural network models have been extensively studied in\nrecent years. Among the most popular frameworks are the VAE and GAN frameworks,\nwhich learns latent feature representations of data through encoder / decoder\nneural net structures. This is a suitable base for clustering tasks, as the\nlatent space often seems to effectively capture the inherent essence of data,\nsimplifying its manifold and reducing noise. In this article, the VAE framework\nis used to investigate how probability function gradient ascent over data\npoints can be used to process data in order to achieve better clustering.\nImprovements in classification is observed comparing with unprocessed data,\nalthough state of the art results are not obtained. Processing data with\ngradient descent however results in more distinct cluster separation, making it\nsimpler to investigate suitable hyper parameter settings such as the number of\nclusters. We propose a simple yet effective method for investigating suitable\nnumber of clusters for data, based on the DBSCAN clustering algorithm, and\ndemonstrate that cluster number determination is facilitated with gradient\nprocessing. As an additional curiosity, we find that our baseline model used\nfor comparison; a GMM on a t-SNE latent space for a VAE structure with weight\none on reconstruction during training (autoencoder), yield state of the art\nresults on the MNIST data, to our knowledge not beaten by any other existing\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:00:36 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Lindhe", "Adam", ""], ["Ringqvist", "Carl", ""], ["Hult", "Henrik", ""]]}, {"id": "2105.06250", "submitter": "Yao Chen", "authors": "Yao Chen, Cole Hawkins, Kaiqi Zhang, Zheng Zhang, Cong Hao", "title": "3U-EdgeAI: Ultra-Low Memory Training, Ultra-Low BitwidthQuantization,\n  and Ultra-Low Latency Acceleration", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The deep neural network (DNN) based AI applications on the edge require both\nlow-cost computing platforms and high-quality services. However, the limited\nmemory, computing resources, and power budget of the edge devices constrain the\neffectiveness of the DNN algorithms. Developing edge-oriented AI algorithms and\nimplementations (e.g., accelerators) is challenging. In this paper, we\nsummarize our recent efforts for efficient on-device AI development from three\naspects, including both training and inference. First, we present on-device\ntraining with ultra-low memory usage. We propose a novel rank-adaptive\ntensor-based tensorized neural network model, which offers orders-of-magnitude\nmemory reduction during training. Second, we introduce an ultra-low bitwidth\nquantization method for DNN model compression, achieving the state-of-the-art\naccuracy under the same compression ratio. Third, we introduce an ultra-low\nlatency DNN accelerator design, practicing the software/hardware co-design\nmethodology. This paper emphasizes the importance and efficacy of training,\nquantization and accelerator design, and calls for more research breakthroughs\nin the area for AI on the edge.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:22:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chen", "Yao", ""], ["Hawkins", "Cole", ""], ["Zhang", "Kaiqi", ""], ["Zhang", "Zheng", ""], ["Hao", "Cong", ""]]}, {"id": "2105.06251", "submitter": "Eike Stadtl\\\"ander", "authors": "Eike Stadtl\\\"ander, Tam\\'as Horv\\'ath, Stefan Wrobel", "title": "Learning Weakly Convex Sets in Metric Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of weak convexity in metric spaces, a generalization\nof ordinary convexity commonly used in machine learning. It is shown that\nweakly convex sets can be characterized by a closure operator and have a unique\ndecomposition into a set of pairwise disjoint connected blocks. We give two\ngeneric efficient algorithms, an extensional and an intensional one for\nlearning weakly convex concepts and study their formal properties. Our\nexperimental results concerning vertex classification clearly demonstrate the\nexcellent predictive performance of the extensional algorithm. Two non-trivial\napplications of the intensional algorithm to polynomial PAC-learnability are\npresented. The first one deals with learning $k$-convex Boolean functions,\nwhich are already known to be efficiently PAC-learnable. It is shown how to\nderive this positive result in a fairly easy way by the generic intensional\nalgorithm. The second one is concerned with the Euclidean space equipped with\nthe Manhattan distance. For this metric space, weakly convex sets are a union\nof pairwise disjoint axis-aligned hyperrectangles. We show that a weakly convex\nset that is consistent with a set of examples and contains a minimum number of\nhyperrectangles can be found in polynomial time. In contrast, this problem is\nknown to be NP-complete if the hyperrectangles may be overlapping.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 23:00:02 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Stadtl\u00e4nder", "Eike", ""], ["Horv\u00e1th", "Tam\u00e1s", ""], ["Wrobel", "Stefan", ""]]}, {"id": "2105.06253", "submitter": "Khin Chit", "authors": "Khin Me Me Chit, Laet Laet Lin", "title": "Exploring CTC Based End-to-End Techniques for Myanmar Speech Recognition", "comments": "This is a preprint of the chapter: Chit K.M.M., Lin L.L., Exploring\n  CTC Based End-To-End Techniques for Myanmar Speech Recognition, published in\n  Advances in Intelligent Systems and Computing, vol 1324, edited by Vasant P.,\n  Zelinka I., Weber GW., 2021, Springer, Cham reproduced with permission of\n  Springer. The final authenticated version is available at\n  https://doi.org/10.1007/978-3-030-68154-8_87", "journal-ref": "Advances in Intelligent Systems and Computing, vol 1324. Springer,\n  Cham (2021)", "doi": "10.1007/978-3-030-68154-8_87", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore a Connectionist Temporal Classification (CTC) based\nend-to-end Automatic Speech Recognition (ASR) model for the Myanmar language. A\nseries of experiments is presented on the topology of the model in which the\nconvolutional layers are added and dropped, different depths of bidirectional\nlong short-term memory (BLSTM) layers are used and different label encoding\nmethods are investigated. The experiments are carried out in low-resource\nscenarios using our recorded Myanmar speech corpus of nearly 26 hours. The best\nmodel achieves character error rate (CER) of 4.72% and syllable error rate\n(SER) of 12.38% on the test set.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:58:51 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 05:29:56 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Chit", "Khin Me Me", ""], ["Lin", "Laet Laet", ""]]}, {"id": "2105.06255", "submitter": "Anupam Khan", "authors": "Anupam Khan, Soumya K. Ghosh", "title": "Machine Assistance for Credit Card Approval? Random Wheel can Recommend\n  and Explain", "comments": "14 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Approval of credit card application is one of the censorious business\ndecision the bankers are usually taking regularly. The growing number of new\ncard applications and the enormous outstanding amount of credit card bills\nduring the recent pandemic make this even more challenging nowadays. Some of\nthe previous studies suggest the usage of machine intelligence for automating\nthe approval process to mitigate this challenge. However, the effectiveness of\nsuch automation may depend on the richness of the training dataset and model\nefficiency. We have recently developed a novel classifier named random wheel\nwhich provides a more interpretable output. In this work, we have used an\nenhanced version of random wheel to facilitate a trustworthy recommendation for\ncredit card approval process. It not only produces more accurate and precise\nrecommendation but also provides an interpretable confidence measure. Besides,\nit explains the machine recommendation for each credit card application as\nwell. The availability of recommendation confidence and explanation could bring\nmore trust in the machine provided intelligence which in turn can enhance the\nefficiency of the credit card approval process.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:41:42 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Khan", "Anupam", ""], ["Ghosh", "Soumya K.", ""]]}, {"id": "2105.06256", "submitter": "Kang Wei", "authors": "Chuan Ma, Jun Li, Ming Ding, Kang Wei, Wen Chen and H. Vincent Poor", "title": "Federated Learning with Unreliable Clients: Performance Analysis and\n  Mechanism Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the low communication costs and privacy-promoting capabilities,\nFederated Learning (FL) has become a promising tool for training effective\nmachine learning models among distributed clients. However, with the\ndistributed architecture, low quality models could be uploaded to the\naggregator server by unreliable clients, leading to a degradation or even a\ncollapse of training. In this paper, we model these unreliable behaviors of\nclients and propose a defensive mechanism to mitigate such a security risk.\nSpecifically, we first investigate the impact on the models caused by\nunreliable clients by deriving a convergence upper bound on the loss function\nbased on the gradient descent updates. Our theoretical bounds reveal that with\na fixed amount of total computational resources, there exists an optimal number\nof local training iterations in terms of convergence performance. We further\ndesign a novel defensive mechanism, named deep neural network based secure\naggregation (DeepSA). Our experimental results validate our theoretical\nanalysis. In addition, the effectiveness of DeepSA is verified by comparing\nwith other state-of-the-art defensive mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:02:27 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ma", "Chuan", ""], ["Li", "Jun", ""], ["Ding", "Ming", ""], ["Wei", "Kang", ""], ["Chen", "Wen", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2105.06266", "submitter": "Yuhao Zhou", "authors": "Yuhao Zhou, Xihua Li, Yunbo Cao, Xuemin Zhao, Qing Ye and Jiancheng Lv", "title": "LANA: Towards Personalized Deep Knowledge Tracing Through\n  Distinguishable Interactive Sequences", "comments": "EDM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In educational applications, Knowledge Tracing (KT), the problem of\naccurately predicting students' responses to future questions by summarizing\ntheir knowledge states, has been widely studied for decades as it is considered\na fundamental task towards adaptive online learning. Among all the proposed KT\nmethods, Deep Knowledge Tracing (DKT) and its variants are by far the most\neffective ones due to the high flexibility of the neural network. However, DKT\noften ignores the inherent differences between students (e.g. memory skills,\nreasoning skills, ...), averaging the performances of all students, leading to\nthe lack of personalization, and therefore was considered insufficient for\nadaptive learning. To alleviate this problem, in this paper, we proposed\nLeveled Attentive KNowledge TrAcing (LANA), which firstly uses a novel\nstudent-related features extractor (SRFE) to distill students' unique inherent\nproperties from their respective interactive sequences. Secondly, the pivot\nmodule was utilized to dynamically reconstruct the decoder of the neural\nnetwork on attention of the extracted features, successfully distinguishing the\nperformance between students over time. Moreover, inspired by Item Response\nTheory (IRT), the interpretable Rasch model was used to cluster students by\ntheir ability levels, and thereby utilizing leveled learning to assign\ndifferent encoders to different groups of students. With pivot module\nreconstructed the decoder for individual students and leveled learning\nspecialized encoders for groups, personalized DKT was achieved. Extensive\nexperiments conducted on two real-world large-scale datasets demonstrated that\nour proposed LANA improves the AUC score by at least 1.00% (i.e. EdNet 1.46%\nand RAIEd2020 1.00%), substantially surpassing the other State-Of-The-Art KT\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 02:57:42 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhou", "Yuhao", ""], ["Li", "Xihua", ""], ["Cao", "Yunbo", ""], ["Zhao", "Xuemin", ""], ["Ye", "Qing", ""], ["Lv", "Jiancheng", ""]]}, {"id": "2105.06270", "submitter": "Chen-Chen Fan", "authors": "Chen-Chen Fan, Haiqun Xie, Liang Peng, Hongjun Yang, Zhen-Liang Ni,\n  Guan'an Wang, Yan-Jie Zhou, Sheng Chen, Zhijie Fang, Shuyun Huang, Zeng-Guang\n  Hou", "title": "Group Feature Learning and Domain Adversarial Neural Network for aMCI\n  Diagnosis System Based on EEG", "comments": "This paper has been accepted by 2021 International Conference on\n  Robotics and Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical diagnostic robot systems have been paid more and more attention due\nto its objectivity and accuracy. The diagnosis of mild cognitive impairment\n(MCI) is considered an effective means to prevent Alzheimer's disease (AD).\nDoctors diagnose MCI based on various clinical examinations, which are\nexpensive and the diagnosis results rely on the knowledge of doctors.\nTherefore, it is necessary to develop a robot diagnostic system to eliminate\nthe influence of human factors and obtain a higher accuracy rate. In this\npaper, we propose a novel Group Feature Domain Adversarial Neural Network\n(GF-DANN) for amnestic MCI (aMCI) diagnosis, which involves two important\nmodules. A Group Feature Extraction (GFE) module is proposed to reduce\nindividual differences by learning group-level features through adversarial\nlearning. A Dual Branch Domain Adaptation (DBDA) module is carefully designed\nto reduce the distribution difference between the source and target domain in a\ndomain adaption way. On three types of data set, GF-DANN achieves the best\naccuracy compared with classic machine learning and deep learning methods. On\nthe DMS data set, GF-DANN has obtained an accuracy rate of 89.47%, and the\nsensitivity and specificity are 90% and 89%. In addition, by comparing three\nEEG data collection paradigms, our results demonstrate that the DMS paradigm\nhas the potential to build an aMCI diagnose robot system.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:08:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Fan", "Chen-Chen", ""], ["Xie", "Haiqun", ""], ["Peng", "Liang", ""], ["Yang", "Hongjun", ""], ["Ni", "Zhen-Liang", ""], ["Wang", "Guan'an", ""], ["Zhou", "Yan-Jie", ""], ["Chen", "Sheng", ""], ["Fang", "Zhijie", ""], ["Huang", "Shuyun", ""], ["Hou", "Zeng-Guang", ""]]}, {"id": "2105.06275", "submitter": "Nicol\\`o Felicioni", "authors": "Nicol\\`o Felicioni, Maurizio Ferrari Dacrema, Paolo Cremonesi", "title": "A Methodology for the Offline Evaluation of Recommender Systems in a\n  User Interface with Multiple Carousels", "comments": null, "journal-ref": "Adjunct Proceedings of the 29th ACM Conference on User Modeling,\n  Adaptation and Personalization (UMAP '21 Adjunct), June 21--25, 2021,\n  Utrecht, Netherlands", "doi": "10.1145/3450614.3461680", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many video-on-demand and music streaming services provide the user with a\npage consisting of several recommendation lists, i.e. widgets or swipeable\ncarousels, each built with a specific criterion (e.g. most recent, TV series,\netc.). Finding efficient strategies to select which carousels to display is an\nactive research topic of great industrial interest. In this setting, the\noverall quality of the recommendations of a new algorithm cannot be assessed by\nmeasuring solely its individual recommendation quality. Rather, it should be\nevaluated in a context where other recommendation lists are already available,\nto account for how they complement each other. This is not considered by\ntraditional offline evaluation protocols. Hence, we propose an offline\nevaluation protocol for a carousel setting in which the recommendation quality\nof a model is measured by how much it improves upon that of an already\navailable set of carousels. We report experiments on publicly available\ndatasets on the movie domain and notice that under a carousel setting the\nranking of the algorithms change. In particular, when a SLIM carousel is\navailable, matrix factorization models tend to be preferred, while item-based\nmodels are penalized. We also propose to extend ranking metrics to the\ntwo-dimensional carousel layout in order to account for a known position bias,\ni.e. users will not explore the lists sequentially, but rather concentrate on\nthe top-left corner of the screen.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 13:14:59 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Felicioni", "Nicol\u00f2", ""], ["Dacrema", "Maurizio Ferrari", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "2105.06285", "submitter": "Thomas Elliott", "authors": "Thomas J. Elliott", "title": "Memory compression and thermal efficiency of quantum implementations of\n  non-deterministic hidden Markov models", "comments": "10 pages, 1 figure, 1 table", "journal-ref": "Physical Review A, 103, 052615 (2021)", "doi": "10.1103/PhysRevA.103.052615", "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.IT cs.LG math.IT nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic modelling is an essential component of the quantitative sciences,\nwith hidden Markov models (HMMs) often playing a central role. Concurrently,\nthe rise of quantum technologies promises a host of advantages in computational\nproblems, typically in terms of the scaling of requisite resources such as time\nand memory. HMMs are no exception to this, with recent results highlighting\nquantum implementations of deterministic HMMs exhibiting superior memory and\nthermal efficiency relative to their classical counterparts. In many contexts\nhowever, non-deterministic HMMs are viable alternatives; compared to them the\nadvantages of current quantum implementations do not always hold. Here, we\nprovide a systematic prescription for constructing quantum implementations of\nnon-deterministic HMMs that re-establish the quantum advantages against this\nbroader class. Crucially, we show that whenever the classical implementation\nsuffers from thermal dissipation due to its need to process information in a\ntime-local manner, our quantum implementations will both mitigate some of this\ndissipation, and achieve an advantage in memory compression.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 13:32:25 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 11:26:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Elliott", "Thomas J.", ""]]}, {"id": "2105.06288", "submitter": "Geethu Joseph", "authors": "Geethu Joseph, Chen Zhong, M. Cenk Gursoy, Senem Velipasalar, Pramod\n  K. Varshney", "title": "Anomaly Detection via Controlled Sensing and Deep Active Inference", "comments": "6 pages,9 figures", "journal-ref": "Globecom 2020", "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the anomaly detection problem where the objective\nis to find the anomalous processes among a given set of processes. To this end,\nthe decision-making agent probes a subset of processes at every time instant\nand obtains a potentially erroneous estimate of the binary variable which\nindicates whether or not the corresponding process is anomalous. The agent\ncontinues to probe the processes until it obtains a sufficient number of\nmeasurements to reliably identify the anomalous processes. In this context, we\ndevelop a sequential selection algorithm that decides which processes to be\nprobed at every instant to detect the anomalies with an accuracy exceeding a\ndesired value while minimizing the delay in making the decision and the total\nnumber of measurements taken. Our algorithm is based on active inference which\nis a general framework to make sequential decisions in order to maximize the\nnotion of free energy. We define the free energy using the objectives of the\nselection policy and implement the active inference framework using a deep\nneural network approximation. Using numerical experiments, we compare our\nalgorithm with the state-of-the-art method based on deep actor-critic\nreinforcement learning and demonstrate the superior performance of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:54:02 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Joseph", "Geethu", ""], ["Zhong", "Chen", ""], ["Gursoy", "M. Cenk", ""], ["Velipasalar", "Senem", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "2105.06289", "submitter": "Geethu Joseph", "authors": "Geethu Joseph, M. Cenk Gursoy, Pramod K. Varshney", "title": "A Scalable Algorithm for Anomaly Detection via Learning-Based Controlled\n  Sensing", "comments": "6 pages, 8 figures", "journal-ref": "ICC 2021", "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of sequentially selecting and observing processes from\na given set to find the anomalies among them. The decision-maker observes one\nprocess at a time and obtains a noisy binary indicator of whether or not the\ncorresponding process is anomalous. In this setting, we develop an anomaly\ndetection algorithm that chooses the process to be observed at a given time\ninstant, decides when to stop taking observations, and makes a decision\nregarding the anomalous processes. The objective of the detection algorithm is\nto arrive at a decision with an accuracy exceeding a desired value while\nminimizing the delay in decision making. Our algorithm relies on a Markov\ndecision process defined using the marginal probability of each process being\nnormal or anomalous, conditioned on the observations. We implement the\ndetection algorithm using the deep actor-critic reinforcement learning\nframework. Unlike prior work on this topic that has exponential complexity in\nthe number of processes, our algorithm has computational and memory\nrequirements that are both polynomial in the number of processes. We\ndemonstrate the efficacy of our algorithm using numerical experiments by\ncomparing it with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:46:01 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Joseph", "Geethu", ""], ["Gursoy", "M. Cenk", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "2105.06292", "submitter": "Eric Postma", "authors": "Koko Visser and Bas Bosma and Eric Postma", "title": "A one-armed CNN for exoplanet detection from light curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Genesis, a one-armed simplified Convolutional Neural Network\n(CNN)for exoplanet detection, and compare it to the more complex, two-armed CNN\ncalled Astronet. Furthermore, we examine how Monte Carlo cross-validation\naffects the estimation of the exoplanet detection performance. Finally, we\nincrease the input resolution twofold to assess its effect on performance. The\nexperiments reveal that (i)the reduced complexity of Genesis, i.e., a more than\n95% reduction in the number of free parameters, incurs a small performance cost\nof about 0.5% compared to Astronet, (ii) Monte Carlo cross-validation provides\na more realistic performance estimate that is almost 0.7% below the original\nestimate, and (iii) the twofold increase in input resolution decreases the\naverage performance by about 0.5%. We conclude by arguing that further\nexploration of shallower CNN architectures may be beneficial in order to\nimprove the generalizability of CNN-based exoplanet detection across surveys.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 14:00:22 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Visser", "Koko", ""], ["Bosma", "Bas", ""], ["Postma", "Eric", ""]]}, {"id": "2105.06295", "submitter": "Albara Ramli", "authors": "Albara Ah Ramli, Huanle Zhang, Jiahui Hou, Rex Liu, Xin Liu, Alina\n  Nicorici, Daniel Aranki, Corey Owens, Poonam Prasad, Craig McDonald, Erik\n  Henricson", "title": "Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a\n  Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning\n  Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differences in gait patterns of children with Duchenne muscular dystrophy\n(DMD) and typically developing (TD) peers are visible to the eye, but\nquantification of those differences outside of the gait laboratory has been\nelusive. We measured vertical, mediolateral, and anteroposterior acceleration\nusing a waist-worn iPhone accelerometer during ambulation across a typical\nrange of velocities. Six TD and six DMD children from 3-15 years of age\nunderwent seven walking/running tasks, including five 25m walk/run tests at a\nslow walk to running speeds, a 6-minute walk test (6MWT), and a\n100-meter-run/walk (100MRW). We extracted temporospatial clinical gait features\n(CFs) and applied multiple Artificial Intelligence (AI) tools to differentiate\nbetween DMD and TD control children using extracted features and raw data.\nExtracted CFs showed reduced step length and a greater mediolateral component\nof total power (TP) consistent with shorter strides and Trendelenberg-like gait\ncommonly observed in DMD. AI methods using CFs and raw data varied\nineffectiveness at differentiating between DMD and TD controls at different\nspeeds, with an accuracy of some methods exceeding 91%. We demonstrate that by\nusing AI tools with accelerometer data from a consumer-level smartphone, we can\nidentify DMD gait disturbance in toddlers to early teens.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 07:06:57 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ramli", "Albara Ah", ""], ["Zhang", "Huanle", ""], ["Hou", "Jiahui", ""], ["Liu", "Rex", ""], ["Liu", "Xin", ""], ["Nicorici", "Alina", ""], ["Aranki", "Daniel", ""], ["Owens", "Corey", ""], ["Prasad", "Poonam", ""], ["McDonald", "Craig", ""], ["Henricson", "Erik", ""]]}, {"id": "2105.06314", "submitter": "Ismini Psychoula", "authors": "Ismini Psychoula, Andreas Gutmann, Pradip Mainali, S. H. Lee, Paul\n  Dunphy, Fabien A. P. Petitcolas", "title": "Explainable Machine Learning for Fraud Detection", "comments": "To be published in IEEE Computer Special Issue on Explainable AI and\n  Machine Learning, 12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of machine learning to support the processing of large\ndatasets holds promise in many industries, including financial services.\nHowever, practical issues for the full adoption of machine learning remain with\nthe focus being on understanding and being able to explain the decisions and\npredictions made by complex models. In this paper, we explore explainability\nmethods in the domain of real-time fraud detection by investigating the\nselection of appropriate background datasets and runtime trade-offs on both\nsupervised and unsupervised models.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:12:02 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Psychoula", "Ismini", ""], ["Gutmann", "Andreas", ""], ["Mainali", "Pradip", ""], ["Lee", "S. H.", ""], ["Dunphy", "Paul", ""], ["Petitcolas", "Fabien A. P.", ""]]}, {"id": "2105.06323", "submitter": "Dongha Lee", "authors": "Dongha Lee, SeongKu Kang, Hyunjun Ju, Chanyoung Park, Hwanjo Yu", "title": "Bootstrapping User and Item Representations for One-Class Collaborative\n  Filtering", "comments": "SIGIR 2021. 9 pages + references (1 page). 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of one-class collaborative filtering (OCCF) is to identify the\nuser-item pairs that are positively-related but have not been interacted yet,\nwhere only a small portion of positive user-item interactions (e.g., users'\nimplicit feedback) are observed. For discriminative modeling between positive\nand negative interactions, most previous work relied on negative sampling to\nsome extent, which refers to considering unobserved user-item pairs as\nnegative, as actual negative ones are unknown. However, the negative sampling\nscheme has critical limitations because it may choose \"positive but unobserved\"\npairs as negative. This paper proposes a novel OCCF framework, named as BUIR,\nwhich does not require negative sampling. To make the representations of\npositively-related users and items similar to each other while avoiding a\ncollapsed solution, BUIR adopts two distinct encoder networks that learn from\neach other; the first encoder is trained to predict the output of the second\nencoder as its target, while the second encoder provides the consistent targets\nby slowly approximating the first encoder. In addition, BUIR effectively\nalleviates the data sparsity issue of OCCF, by applying stochastic data\naugmentation to encoder inputs. Based on the neighborhood information of users\nand items, BUIR randomly generates the augmented views of each positive\ninteraction each time it encodes, then further trains the model by this\nself-supervision. Our extensive experiments demonstrate that BUIR consistently\nand significantly outperforms all baseline methods by a large margin especially\nfor much sparse datasets in which any assumptions about negative interactions\nare less valid.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:24:13 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Lee", "Dongha", ""], ["Kang", "SeongKu", ""], ["Ju", "Hyunjun", ""], ["Park", "Chanyoung", ""], ["Yu", "Hwanjo", ""]]}, {"id": "2105.06331", "submitter": "Matthias Werner Mr.", "authors": "Matthias Werner, Andrej Junginger, Philipp Hennig, Georg Martius", "title": "Informed Equation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distilling data into compact and interpretable analytic equations is one of\nthe goals of science. Instead, contemporary supervised machine learning methods\nmostly produce unstructured and dense maps from input to output. Particularly\nin deep learning, this property is owed to the generic nature of simple\nstandard link functions. To learn equations rather than maps, standard\nnon-linearities can be replaced with structured building blocks of atomic\nfunctions. However, without strong priors on sparsity and structure,\nrepresentational complexity and numerical conditioning limit this direct\napproach. To scale to realistic settings in science and engineering, we propose\nan informed equation learning system. It provides a way to incorporate expert\nknowledge about what are permitted or prohibited equation components, as well\nas a domain-dependent structured sparsity prior. Our system then utilizes a\nrobust method to learn equations with atomic functions exhibiting\nsingularities, as e.g. logarithm and division. We demonstrate several\nartificial and real-world experiments from the engineering domain, in which our\nsystem learns interpretable models of high predictive power.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:37:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Werner", "Matthias", ""], ["Junginger", "Andrej", ""], ["Hennig", "Philipp", ""], ["Martius", "Georg", ""]]}, {"id": "2105.06337", "submitter": "Mikhail Kudinov", "authors": "Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail\n  Kudinov", "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, denoising diffusion probabilistic models and generative score\nmatching have shown high potential in modelling complex data distributions\nwhile stochastic calculus has provided a unified point of view on these\ntechniques allowing for flexible inference schemes. In this paper we introduce\nGrad-TTS, a novel text-to-speech model with score-based decoder producing\nmel-spectrograms by gradually transforming noise predicted by encoder and\naligned with text input by means of Monotonic Alignment Search. The framework\nof stochastic differential equations helps us to generalize conventional\ndiffusion probabilistic models to the case of reconstructing data from noise\nwith different parameters and allows to make this reconstruction flexible by\nexplicitly controlling trade-off between sound quality and inference speed.\nSubjective human evaluation shows that Grad-TTS is competitive with\nstate-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We\nwill make the code publicly available shortly.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:47:44 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Popov", "Vadim", ""], ["Vovk", "Ivan", ""], ["Gogoryan", "Vladimir", ""], ["Sadekova", "Tasnima", ""], ["Kudinov", "Mikhail", ""]]}, {"id": "2105.06339", "submitter": "Shoujin Wang", "authors": "Shoujin Wang, Liang Hu, Yan Wang, Xiangnan He, Quan Z. Sheng, Mehmet\n  A. Orgun, Longbing Cao, Francesco Ricci, Philip S. Yu", "title": "Graph Learning based Recommender Systems: A Review", "comments": "Accepted by IJCAI 2021 Survey Track, copyright is owned to IJCAI. The\n  first systematic survey on graph learning based recommender systems. arXiv\n  admin note: text overlap with arXiv:2004.11718", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed the fast development of the emerging topic of\nGraph Learning based Recommender Systems (GLRS). GLRS employ advanced graph\nlearning approaches to model users' preferences and intentions as well as\nitems' characteristics for recommendations. Differently from other RS\napproaches, including content-based filtering and collaborative filtering, GLRS\nare built on graphs where the important objects, e.g., users, items, and\nattributes, are either explicitly or implicitly connected. With the rapid\ndevelopment of graph learning techniques, exploring and exploiting homogeneous\nor heterogeneous relations in graphs are a promising direction for building\nmore effective RS. In this paper, we provide a systematic review of GLRS, by\ndiscussing how they extract important knowledge from graph-based\nrepresentations to improve the accuracy, reliability and explainability of the\nrecommendations. First, we characterize and formalize GLRS, and then summarize\nand categorize the key challenges and main progress in this novel research\narea. Finally, we share some new research directions in this vibrant area.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:50:45 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Shoujin", ""], ["Hu", "Liang", ""], ["Wang", "Yan", ""], ["He", "Xiangnan", ""], ["Sheng", "Quan Z.", ""], ["Orgun", "Mehmet A.", ""], ["Cao", "Longbing", ""], ["Ricci", "Francesco", ""], ["Yu", "Philip S.", ""]]}, {"id": "2105.06345", "submitter": "Elisa Ferrari", "authors": "Elisa Ferrari, Davide Bacciu", "title": "Addressing Fairness, Bias and Class Imbalance in Machine Learning: the\n  FBI-loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Resilience to class imbalance and confounding biases, together with the\nassurance of fairness guarantees are highly desirable properties of autonomous\ndecision-making systems with real-life impact. Many different targeted\nsolutions have been proposed to address separately these three problems,\nhowever a unifying perspective seems to be missing. With this work, we provide\na general formalization, showing that they are different expressions of\nunbalance. Following this intuition, we formulate a unified loss correction to\naddress issues related to Fairness, Biases and Imbalances (FBI-loss). The\ncorrection capabilities of the proposed approach are assessed on three\nreal-world benchmarks, each associated to one of the issues under\nconsideration, and on a family of synthetic data in order to better investigate\nthe effectiveness of our loss on tasks with different complexities. The\nempirical results highlight that the flexible formulation of the FBI-loss leads\nalso to competitive performances with respect to literature solutions\nspecialised for the single problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:01:14 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ferrari", "Elisa", ""], ["Bacciu", "Davide", ""]]}, {"id": "2105.06347", "submitter": "Sela Fried", "authors": "Sela Fried and Geoffrey Wolfer", "title": "Identity testing of reversible Markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identity testing of Markov chains based on a\nsingle trajectory of observations under the distance notion introduced by\nDaskalakis et al. [2018a] and further analyzed by Cherapanamjeri and Bartlett\n[2019]. Both works made the restrictive assumption that the Markov chains under\nconsideration are symmetric. In this work we relax the symmetry assumption to\nthe more natural assumption of reversibility, still assuming that both the\nreference and the unknown Markov chains share the same stationary distribution.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:03:27 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Fried", "Sela", ""], ["Wolfer", "Geoffrey", ""]]}, {"id": "2105.06350", "submitter": "Menghui Zhu", "authors": "Menghui Zhu, Minghuan Liu, Jian Shen, Zhicheng Zhang, Sheng Chen,\n  Weinan Zhang, Deheng Ye, Yong Yu, Qiang Fu, Wei Yang", "title": "MapGo: Model-Assisted Policy Optimization for Goal-Oriented Tasks", "comments": "Paper accepted at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Goal-oriented Reinforcement learning, relabeling the raw goals in past\nexperience to provide agents with hindsight ability is a major solution to the\nreward sparsity problem. In this paper, to enhance the diversity of relabeled\ngoals, we develop FGI (Foresight Goal Inference), a new relabeling strategy\nthat relabels the goals by looking into the future with a learned dynamics\nmodel. Besides, to improve sample efficiency, we propose to use the dynamics\nmodel to generate simulated trajectories for policy training. By integrating\nthese two improvements, we introduce the MapGo framework (Model-Assisted Policy\nOptimization for Goal-oriented tasks). In our experiments, we first show the\neffectiveness of the FGI strategy compared with the hindsight one, and then\nshow that the MapGo framework achieves higher sample efficiency when compared\nto model-free baselines on a set of complicated tasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:07:23 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhu", "Menghui", ""], ["Liu", "Minghuan", ""], ["Shen", "Jian", ""], ["Zhang", "Zhicheng", ""], ["Chen", "Sheng", ""], ["Zhang", "Weinan", ""], ["Ye", "Deheng", ""], ["Yu", "Yong", ""], ["Fu", "Qiang", ""], ["Yang", "Wei", ""]]}, {"id": "2105.06351", "submitter": "Hancheng Min", "authors": "Hancheng Min, Salma Tarmoun, Rene Vidal, Enrique Mallada", "title": "On the Explicit Role of Initialization on the Convergence and Implicit\n  Bias of Overparametrized Linear Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks trained via gradient descent with random initialization and\nwithout any regularization enjoy good generalization performance in practice\ndespite being highly overparametrized. A promising direction to explain this\nphenomenon is to study how initialization and overparametrization affect\nconvergence and implicit bias of training algorithms. In this paper, we present\na novel analysis of single-hidden-layer linear networks trained under gradient\nflow, which connects initialization, optimization, and overparametrization.\nFirstly, we show that the squared loss converges exponentially to its optimum\nat a rate that depends on the level of imbalance of the initialization.\nSecondly, we show that proper initialization constrains the dynamics of the\nnetwork parameters to lie within an invariant set. In turn, minimizing the loss\nover this set leads to the min-norm solution. Finally, we show that large\nhidden layer width, together with (properly scaled) random initialization,\nensures proximity to such an invariant set during training, allowing us to\nderive a novel non-asymptotic upper-bound on the distance between the trained\nnetwork and the min-norm solution.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:13:51 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Min", "Hancheng", ""], ["Tarmoun", "Salma", ""], ["Vidal", "Rene", ""], ["Mallada", "Enrique", ""]]}, {"id": "2105.06355", "submitter": "Ay\\c{s}eg\\\"ul \\\"Ozkaya Eren", "authors": "Ay\\c{s}eg\\\"ul \\\"Ozkaya Eren and Mustafa Sert", "title": "Audio Captioning with Composition of Acoustic and Semantic Information", "comments": "Accepted for publication in International Journal of Semantic\n  Computing. arXiv admin note: substantial text overlap with arXiv:2006.03391", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating audio captions is a new research area that combines audio and\nnatural language processing to create meaningful textual descriptions for audio\nclips. To address this problem, previous studies mostly use the encoder-decoder\nbased models without considering semantic information. To fill this gap, we\npresent a novel encoder-decoder architecture using bi-directional Gated\nRecurrent Units (BiGRU) with audio and semantic embeddings. We extract semantic\nembedding by obtaining subjects and verbs from the audio clip captions and\ncombine these embedding with audio embedding to feed the BiGRU-based\nencoder-decoder model. To enable semantic embeddings for the test audios, we\nintroduce a Multilayer Perceptron classifier to predict the semantic embeddings\nof those clips. We also present exhaustive experiments to show the efficiency\nof different features and datasets for our proposed model the audio captioning\ntask. To extract audio features, we use the log Mel energy features, VGGish\nembeddings, and a pretrained audio neural network (PANN) embeddings. Extensive\nexperiments on two audio captioning datasets Clotho and AudioCaps show that our\nproposed model outperforms state-of-the-art audio captioning models across\ndifferent evaluation metrics and using the semantic information improves the\ncaptioning performance. Keywords: Audio captioning; PANNs; VGGish; GRU; BiGRU.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:30:14 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Eren", "Ay\u015feg\u00fcl \u00d6zkaya", ""], ["Sert", "Mustafa", ""]]}, {"id": "2105.06363", "submitter": "Ye Lu", "authors": "Lei Zhang (1 and 3), Ye Lu (2), Shaoqiang Tang (1) and Wing Kam Liu\n  (2) ((1) Peking University, Beijing, China, (2) Northwestern University,\n  Evanston, USA, (3) Visiting student, Department of Mechanical Engineering,\n  Northwestern University)", "title": "HiDeNN-PGD: reduced-order hierarchical deep learning neural networks", "comments": "35 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a proper generalized decomposition (PGD) based\nreduced-order model of hierarchical deep-learning neural networks (HiDeNN). The\nproposed HiDeNN-PGD method keeps both advantages of HiDeNN and PGD methods. The\nautomatic mesh adaptivity makes the HiDeNN-PGD more accurate than the finite\nelement method (FEM) and conventional PGD, using a fraction of the FEM degrees\nof freedom. The accuracy and convergence of the method have been studied\ntheoretically and numerically, with a comparison to different methods,\nincluding FEM, PGD, HiDeNN and Deep Neural Networks. In addition, we\ntheoretically showed that the PGD converges to FEM at increasing modes, and the\nPGD error is a direct sum of the FEM error and the mode reduction error. The\nproposed HiDeNN-PGD performs high accuracy with orders of magnitude fewer\ndegrees of freedom, which shows a high potential to achieve fast computations\nwith a high level of accuracy for large-size engineering problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:42:59 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhang", "Lei", "", "1 and 3"], ["Lu", "Ye", ""], ["Tang", "Shaoqiang", ""], ["Liu", "Wing Kam", ""]]}, {"id": "2105.06369", "submitter": "Xiaofang Wang", "authors": "Xiaofang Wang, Shengcao Cao, Mengtian Li, Kris M. Kitani", "title": "Neighborhood-Aware Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing neural architecture search (NAS) methods often return an\narchitecture with good search performance but generalizes poorly to the test\nsetting. To achieve better generalization, we propose a novel\nneighborhood-aware NAS formulation to identify flat-minima architectures in the\nsearch space, with the assumption that flat minima generalize better than sharp\nminima. The phrase \"flat-minima architecture\" refers to architectures whose\nperformance is stable under small perturbations in the architecture (e.g.,\nreplacing a convolution with a skip connection). Our formulation takes the\n\"flatness\" of an architecture into account by aggregating the performance over\nthe neighborhood of this architecture. We demonstrate a principled way to apply\nour formulation to existing search algorithms, including sampling-based\nalgorithms and gradient-based algorithms. To facilitate the application to\ngradient-based algorithms, we also propose a differentiable representation for\nthe neighborhood of architectures. Based on our formulation, we propose\nneighborhood-aware random search (NA-RS) and neighborhood-aware differentiable\narchitecture search (NA-DARTS). Notably, by simply augmenting DARTS with our\nformulation, NA-DARTS finds architectures that perform better or on par with\nthose found by state-of-the-art NAS methods on established benchmarks,\nincluding CIFAR-10, CIFAR-100 and ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:56:52 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Xiaofang", ""], ["Cao", "Shengcao", ""], ["Li", "Mengtian", ""], ["Kitani", "Kris M.", ""]]}, {"id": "2105.06370", "submitter": "Iain Barclay", "authors": "Iain Barclay, Alun Preece, Ian Taylor, Swapna K. Radha, Jarek\n  Nabrzyski", "title": "Providing Assurance and Scrutability on Shared Data and Machine Learning\n  Models with Verifiable Credentials", "comments": "This is the submitted, pre-peer reviewed version of this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adopting shared data resources requires scientists to place trust in the\noriginators of the data. When shared data is later used in the development of\nartificial intelligence (AI) systems or machine learning (ML) models, the trust\nlineage extends to the users of the system, typically practitioners in fields\nsuch as healthcare and finance. Practitioners rely on AI developers to have\nused relevant, trustworthy data, but may have limited insight and recourse.\nThis paper introduces a software architecture and implementation of a system\nbased on design patterns from the field of self-sovereign identity. Scientists\ncan issue signed credentials attesting to qualities of their data resources.\nData contributions to ML models are recorded in a bill of materials (BOM),\nwhich is stored with the model as a verifiable credential. The BOM provides a\ntraceable record of the supply chain for an AI system, which facilitates\non-going scrutiny of the qualities of the contributing components. The verified\nBOM, and its linkage to certified data qualities, is used in the AI Scrutineer,\na web-based tool designed to offer practitioners insight into ML model\nconstituents and highlight any problems with adopted datasets, should they be\nfound to have biased data or be otherwise discredited.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:58:05 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Barclay", "Iain", ""], ["Preece", "Alun", ""], ["Taylor", "Ian", ""], ["Radha", "Swapna K.", ""], ["Nabrzyski", "Jarek", ""]]}, {"id": "2105.06371", "submitter": "Chinmay Hegde", "authors": "Viraj Shah, Rakib Hyder, M. Salman Asif, Chinmay Hegde", "title": "Provably Convergent Algorithms for Solving Inverse Problems Using\n  Generative Models", "comments": "arXiv admin note: text overlap with arXiv:1810.03587,\n  arXiv:1802.08406", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The traditional approach of hand-crafting priors (such as sparsity) for\nsolving inverse problems is slowly being replaced by the use of richer learned\npriors (such as those modeled by deep generative networks). In this work, we\nstudy the algorithmic aspects of such a learning-based approach from a\ntheoretical perspective. For certain generative network architectures, we\nestablish a simple non-convex algorithmic approach that (a) theoretically\nenjoys linear convergence guarantees for certain linear and nonlinear inverse\nproblems, and (b) empirically improves upon conventional techniques such as\nback-propagation. We support our claims with the experimental results for\nsolving various inverse problems. We also propose an extension of our approach\nthat can handle model mismatch (i.e., situations where the generative network\nprior is not exactly applicable). Together, our contributions serve as building\nblocks towards a principled use of generative models in inverse problems with\nmore complete algorithmic understanding.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:58:27 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Shah", "Viraj", ""], ["Hyder", "Rakib", ""], ["Asif", "M. Salman", ""], ["Hegde", "Chinmay", ""]]}, {"id": "2105.06381", "submitter": "Yongxin Liu", "authors": "Yongxin Liu, Jian Wang, Jianqiang Li, Shuteng Niu, Houbing Song", "title": "Class-Incremental Learning for Wireless Device Identification in IoT", "comments": "Accepted for publication by IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning (DL) has been utilized pervasively in the Internet of Things\n(IoT). One typical application of DL in IoT is device identification from\nwireless signals, namely Non-cryptographic Device Identification (NDI).\nHowever, learning components in NDI systems have to evolve to adapt to\noperational variations, such a paradigm is termed as Incremental Learning (IL).\nVarious IL algorithms have been proposed and many of them require dedicated\nspace to store the increasing amount of historical data, and therefore, they\nare not suitable for IoT or mobile applications. However, conventional IL\nschemes can not provide satisfying performance when historical data are not\navailable. In this paper, we address the IL problem in NDI from a new\nperspective, firstly, we provide a new metric to measure the degree of\ntopological maturity of DNN models from the degree of conflict of\nclass-specific fingerprints. We discover that an important cause for\nperformance degradation in IL enabled NDI is owing to the conflict of devices'\nfingerprints. Second, we also show that the conventional IL schemes can lead to\nlow topological maturity of DNN models in NDI systems. Thirdly, we propose a\nnew Channel Separation Enabled Incremental Learning (CSIL) scheme without using\nhistorical data, in which our strategy can automatically separate devices'\nfingerprints in different learning stages and avoid potential conflict.\nFinally, We evaluated the effectiveness of the proposed framework using real\ndata from ADS-B (Automatic Dependent Surveillance-Broadcast), an application of\nIoT in aviation. The proposed framework has the potential to be applied to\naccurate identification of IoT devices in a variety of IoT applications and\nservices. Data and code available at IEEE Dataport (DOI: 10.21227/1bxc-ke87)\nand \\url{https://github.com/pcwhy/CSIL}}\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:11:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Liu", "Yongxin", ""], ["Wang", "Jian", ""], ["Li", "Jianqiang", ""], ["Niu", "Shuteng", ""], ["Song", "Houbing", ""]]}, {"id": "2105.06386", "submitter": "Brian Hutchinson", "authors": "Alexis Ayala, Christopher Drazic, Brian Hutchinson, Ben Kravitz,\n  Claudia Tebaldi", "title": "Loosely Conditioned Emulation of Global Climate Models With Generative\n  Adversarial Networks", "comments": "Presented at NeurIPS 2020 Workshop Tackling Climate Change with\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.LG cs.NE physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate models encapsulate our best understanding of the Earth system,\nallowing research to be conducted on its future under alternative assumptions\nof how human-driven climate forces are going to evolve. An important\napplication of climate models is to provide metrics of mean and extreme climate\nchanges, particularly under these alternative future scenarios, as these\nquantities drive the impacts of climate on society and natural systems. Because\nof the need to explore a wide range of alternative scenarios and other sources\nof uncertainties in a computationally efficient manner, climate models can only\ntake us so far, as they require significant computational resources, especially\nwhen attempting to characterize extreme events, which are rare and thus demand\nlong and numerous simulations in order to accurately represent their changing\nstatistics. Here we use deep learning in a proof of concept that lays the\nfoundation for emulating global climate model output for different scenarios.\nWe train two \"loosely conditioned\" Generative Adversarial Networks (GANs) that\nemulate daily precipitation output from a fully coupled Earth system model: one\nGAN modeling Fall-Winter behavior and the other Spring-Summer. Our GANs are\ntrained to produce spatiotemporal samples: 32 days of precipitation over a\n64x128 regular grid discretizing the globe. We evaluate the generator with a\nset of related performance metrics based upon KL divergence, and find the\ngenerated samples to be nearly as well matched to the test data as the\nvalidation data is to test. We also find the generated samples to accurately\nestimate the mean number of dry days and mean longest dry spell in the 32 day\nsamples. Our trained GANs can rapidly generate numerous realizations at a\nvastly reduced computational expense, compared to large ensembles of climate\nmodels, which greatly aids in estimating the statistics of extreme events.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 02:10:08 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ayala", "Alexis", ""], ["Drazic", "Christopher", ""], ["Hutchinson", "Brian", ""], ["Kravitz", "Ben", ""], ["Tebaldi", "Claudia", ""]]}, {"id": "2105.06399", "submitter": "Ali Jazayeri", "authors": "Ali Jazayeri and Christopher C. Yang", "title": "Frequent Pattern Mining in Continuous-time Temporal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are used as highly expressive tools in different disciplines. In\nrecent years, the analysis and mining of temporal networks have attracted\nsubstantial attention. Frequent pattern mining is considered an essential task\nin the network science literature. In addition to the numerous applications,\nthe investigation of frequent pattern mining in networks directly impacts other\nanalytical approaches, such as clustering, quasi-clique and clique mining, and\nlink prediction. In nearly all the algorithms proposed for frequent pattern\nmining in temporal networks, the networks are represented as sequences of\nstatic networks. Then, the inter- or intra-network patterns are mined. This\ntype of representation imposes a computation-expressiveness trade-off to the\nmining problem. In this paper, we propose a novel representation that can\npreserve the temporal aspects of the network losslessly. Then, we introduce the\nconcept of constrained interval graphs (CIGs). Next, we develop a series of\nalgorithms for mining the complete set of frequent temporal patterns in a\ntemporal network data set. We also consider four different definitions of\nisomorphism to allow noise tolerance in temporal data collection. Implementing\nthe algorithm for three real-world data sets proves the practicality of the\nproposed algorithm and its capability to discover unknown patterns in various\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 02:47:24 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Jazayeri", "Ali", ""], ["Yang", "Christopher C.", ""]]}, {"id": "2105.06400", "submitter": "Harsh Desai", "authors": "Harsh Desai, Pratik Kayal, Mayank Singh", "title": "TabLeX: A Benchmark Dataset for Structure and Content Information\n  Extraction from Scientific Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information Extraction (IE) from the tables present in scientific articles is\nchallenging due to complicated tabular representations and complex embedded\ntext. This paper presents TabLeX, a large-scale benchmark dataset comprising\ntable images generated from scientific articles. TabLeX consists of two\nsubsets, one for table structure extraction and the other for table content\nextraction. Each table image is accompanied by its corresponding LATEX source\ncode. To facilitate the development of robust table IE tools, TabLeX contains\nimages in different aspect ratios and in a variety of fonts. Our analysis sheds\nlight on the shortcomings of current state-of-the-art table extraction models\nand shows that they fail on even simple table images. Towards the end, we\nexperiment with a transformer-based existing baseline to report performance\nscores. In contrast to the static benchmarks, we plan to augment this dataset\nwith more complex and diverse tables at regular intervals.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 05:13:38 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Desai", "Harsh", ""], ["Kayal", "Pratik", ""], ["Singh", "Mayank", ""]]}, {"id": "2105.06409", "submitter": "Fred Bertsch", "authors": "Trung Le, Ryan Poplin, Fred Bertsch, Andeep Singh Toor, Margaret L. Oh", "title": "SyntheticFur dataset for neural rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new dataset called SyntheticFur built specifically for machine\nlearning training. The dataset consists of ray traced synthetic fur renders\nwith corresponding rasterized input buffers and simulation data files. We\nprocedurally generated approximately 140,000 images and 15 simulations with\nHoudini. The images consist of fur groomed with different skin primitives and\nmove with various motions in a predefined set of lighting environments. We also\ndemonstrated how the dataset could be used with neural rendering to\nsignificantly improve fur graphics using inexpensive input buffers by training\na conditional generative adversarial network with perceptual loss. We hope the\navailability of such high fidelity fur renders will encourage new advances with\nneural rendering for a variety of applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:31:15 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Le", "Trung", ""], ["Poplin", "Ryan", ""], ["Bertsch", "Fred", ""], ["Toor", "Andeep Singh", ""], ["Oh", "Margaret L.", ""]]}, {"id": "2105.06411", "submitter": "Edward Johns", "authors": "Edward Johns", "title": "Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single\n  Demonstration", "comments": "Published at ICRA 2021. Webpage and video:\n  https://www.robot-learning.uk/coarse-to-fine-imitation-learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a simple new method for visual imitation learning, which allows\na novel robot manipulation task to be learned from a single human\ndemonstration, without requiring any prior knowledge of the object being\ninteracted with. Our method models imitation learning as a state estimation\nproblem, with the state defined as the end-effector's pose at the point where\nobject interaction begins, as observed from the demonstration. By then\nmodelling a manipulation task as a coarse, approach trajectory followed by a\nfine, interaction trajectory, this state estimator can be trained in a\nself-supervised manner, by automatically moving the end-effector's camera\naround the object. At test time, the end-effector moves to the estimated state\nthrough a linear path, at which point the original demonstration's end-effector\nvelocities are simply replayed. This enables convenient acquisition of a\ncomplex interaction trajectory, without actually needing to explicitly learn a\npolicy. Real-world experiments on 8 everyday tasks show that our method can\nlearn a diverse range of skills from a single human demonstration, whilst also\nyielding a stable and interpretable controller.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:36:55 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 17:58:27 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Johns", "Edward", ""]]}, {"id": "2105.06413", "submitter": "Sarthak Pati", "authors": "G Anthony Reina, Alexey Gruzdev, Patrick Foley, Olga Perepelkina,\n  Mansi Sharma, Igor Davidyuk, Ilya Trushkin, Maksim Radionov, Aleksandr\n  Mokrov, Dmitry Agapov, Jason Martin, Brandon Edwards, Micah J. Sheller,\n  Sarthak Pati, Prakash Narayana Moorthy, Shih-han Wang, Prashant Shah,\n  Spyridon Bakas", "title": "OpenFL: An open-source framework for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a computational paradigm that enables\norganizations to collaborate on machine learning (ML) projects without sharing\nsensitive data, such as, patient records, financial data, or classified\nsecrets. Open Federated Learning (OpenFL https://github.com/intel/openfl) is an\nopen-source framework for training ML algorithms using the data-private\ncollaborative learning paradigm of FL. OpenFL works with training pipelines\nbuilt with both TensorFlow and PyTorch, and can be easily extended to other ML\nand deep learning frameworks. Here, we summarize the motivation and development\ncharacteristics of OpenFL, with the intention of facilitating its application\nto existing ML model training in a production environment. Finally, we describe\nthe first use of the OpenFL framework to train consensus ML models in a\nconsortium of international healthcare organizations, as well as how it\nfacilitates the first computational competition on FL.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:40:19 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Reina", "G Anthony", ""], ["Gruzdev", "Alexey", ""], ["Foley", "Patrick", ""], ["Perepelkina", "Olga", ""], ["Sharma", "Mansi", ""], ["Davidyuk", "Igor", ""], ["Trushkin", "Ilya", ""], ["Radionov", "Maksim", ""], ["Mokrov", "Aleksandr", ""], ["Agapov", "Dmitry", ""], ["Martin", "Jason", ""], ["Edwards", "Brandon", ""], ["Sheller", "Micah J.", ""], ["Pati", "Sarthak", ""], ["Moorthy", "Prakash Narayana", ""], ["Wang", "Shih-han", ""], ["Shah", "Prashant", ""], ["Bakas", "Spyridon", ""]]}, {"id": "2105.06421", "submitter": "Mahdi Pourmirzaei", "authors": "Mahdi Pourmirzaei, Farzaneh Esmaili, Gholam Ali Montazer", "title": "Using Self-Supervised Co-Training to Improve Facial Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, at first, the impact of ImageNet pre-training on Facial\nExpression Recognition (FER) was tested under different augmentation levels. It\ncould be seen from the results that training from scratch could reach better\nperformance compared to ImageNet fine-tuning at stronger augmentation levels.\nAfter that, a framework was proposed for standard Supervised Learning (SL),\ncalled Hybrid Learning (HL) which used Self-Supervised co-training with SL in\nMulti-Task Learning (MTL) manner. Leveraging Self-Supervised Learning (SSL)\ncould gain additional information from input data like spatial information from\nfaces which helped the main SL task. It is been investigated how this method\ncould be used for FER problems with self-supervised pre-tasks such as Jigsaw\npuzzling and in-painting. The supervised head (SH) was helped by these two\nmethods to lower the error rate under different augmentations and low data\nregime in the same training settings. The state-of-the-art was reached on\nAffectNet via two completely different HL methods, without utilizing additional\ndatasets. Moreover, HL's effect was shown on two different facial-related\nproblem, head poses estimation and gender recognition, which concluded to\nreduce in error rate by up to 9% and 1% respectively. Also, we saw that the HL\nmethods prevented the model from reaching overfitting.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:56:36 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pourmirzaei", "Mahdi", ""], ["Esmaili", "Farzaneh", ""], ["Montazer", "Gholam Ali", ""]]}, {"id": "2105.06422", "submitter": "Maggie Makar", "authors": "Maggie Makar, Ben Packer, Dan Moldovan, Davis Blalock, Yoni Halpern,\n  Alexander D'Amour", "title": "Causally-motivated Shortcut Removal Using Auxiliary Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness to certain forms of distribution shift is a key concern in many ML\napplications. Often, robustness can be formulated as enforcing invariances to\nparticular interventions on the data generating process. Here, we study a\nflexible, causally-motivated approach to enforcing such invariances, paying\nspecial attention to shortcut learning, where a robust predictor can achieve\noptimal i.i.d generalization in principle, but instead it relies on spurious\ncorrelations or shortcuts in practice. Our approach uses auxiliary labels,\ntypically available at training time, to enforce conditional independences\nbetween the latent factors that determine these labels. We show both\ntheoretically and empirically that causally-motivated regularization schemes\n(a) lead to more robust estimators that generalize well under distribution\nshift, and (b) have better finite sample efficiency compared to usual\nregularization schemes, even in the absence of distribution shifts. Our\nanalysis highlights important theoretical properties of training techniques\ncommonly used in causal inference, fairness, and disentanglement literature.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:58:45 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 21:31:05 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Makar", "Maggie", ""], ["Packer", "Ben", ""], ["Moldovan", "Dan", ""], ["Blalock", "Davis", ""], ["Halpern", "Yoni", ""], ["D'Amour", "Alexander", ""]]}, {"id": "2105.06423", "submitter": "Wenqi Shao", "authors": "Wenqi Shao, Hang Yu, Zhaoyang Zhang, Hang Xu, Zhenguo Li, Ping Luo", "title": "BWCP: Probabilistic Learning-to-Prune Channels for ConvNets via Batch\n  Whitening", "comments": "19 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a probabilistic channel pruning method to accelerate\nConvolutional Neural Networks (CNNs). Previous pruning methods often zero out\nunimportant channels in training in a deterministic manner, which reduces CNN's\nlearning capacity and results in suboptimal performance. To address this\nproblem, we develop a probability-based pruning algorithm, called batch\nwhitening channel pruning (BWCP), which can stochastically discard unimportant\nchannels by modeling the probability of a channel being activated. BWCP has\nseveral merits. (1) It simultaneously trains and prunes CNNs from scratch in a\nprobabilistic way, exploring larger network space than deterministic methods.\n(2) BWCP is empowered by the proposed batch whitening tool, which is able to\nempirically and theoretically increase the activation probability of useful\nchannels while keeping unimportant channels unchanged without adding any extra\nparameters and computational cost in inference. (3) Extensive experiments on\nCIFAR-10, CIFAR-100, and ImageNet with various network architectures show that\nBWCP outperforms its counterparts by achieving better accuracy given limited\ncomputational budgets. For example, ResNet50 pruned by BWCP has only 0.70\\%\nTop-1 accuracy drop on ImageNet, while reducing 43.1\\% FLOPs of the plain\nResNet50.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:00:05 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Shao", "Wenqi", ""], ["Yu", "Hang", ""], ["Zhang", "Zhaoyang", ""], ["Xu", "Hang", ""], ["Li", "Zhenguo", ""], ["Luo", "Ping", ""]]}, {"id": "2105.06438", "submitter": "David Betancourt", "authors": "David Betancourt and Rafi Muhanna", "title": "Interval Deep Learning for Uncertainty Quantification in Safety\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep neural networks (DNNs) are becoming more prevalent in important\nsafety-critical applications, where reliability in the prediction is paramount.\nDespite their exceptional prediction capabilities, current DNNs do not have an\nimplicit mechanism to quantify and propagate significant input data uncertainty\n-- which is common in safety-critical applications. In many cases, this\nuncertainty is epistemic and can arise from multiple sources, such as lack of\nknowledge about the data generating process, imprecision, ignorance, and poor\nunderstanding of physics phenomena. Recent approaches have focused on\nquantifying parameter uncertainty, but approaches to end-to-end training of\nDNNs with epistemic input data uncertainty are more limited and largely\nproblem-specific. In this work, we present a DNN optimized with gradient-based\nmethods capable to quantify input and parameter uncertainty by means of\ninterval analysis, which we call Deep Interval Neural Network (DINN). We\nperform experiments on an air pollution dataset with sensor uncertainty and\nshow that the DINN can produce accurate bounded estimates from uncertain input\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:21:33 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Betancourt", "David", ""], ["Muhanna", "Rafi", ""]]}, {"id": "2105.06442", "submitter": "Hemank Lamba", "authors": "Hemank Lamba and Kit T. Rodolfa and Rayid Ghani", "title": "An Empirical Comparison of Bias Reduction Methods on Real-World Problems\n  in High-Stakes Policy Settings", "comments": "17 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of machine learning (ML) to high-stakes policy settings -- such\nas education, criminal justice, healthcare, and social service delivery -- have\ngrown rapidly in recent years, sparking important conversations about how to\nensure fair outcomes from these systems. The machine learning research\ncommunity has responded to this challenge with a wide array of proposed\nfairness-enhancing strategies for ML models, but despite the large number of\nmethods that have been developed, little empirical work exists evaluating these\nmethods in real-world settings. Here, we seek to fill this research gap by\ninvestigating the performance of several methods that operate at different\npoints in the ML pipeline across four real-world public policy and social good\nproblems. Across these problems, we find a wide degree of variability and\ninconsistency in the ability of many of these methods to improve model\nfairness, but post-processing by choosing group-specific score thresholds\nconsistently removes disparities, with important implications for both the ML\nresearch community and practitioners deploying machine learning to inform\nconsequential policy decisions.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:33:28 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Lamba", "Hemank", ""], ["Rodolfa", "Kit T.", ""], ["Ghani", "Rayid", ""]]}, {"id": "2105.06449", "submitter": "Christian K\\\"astner", "authors": "Christian K\\\"astner, Eunsuk Kang, Sven Apel", "title": "Feature Interactions on Steroids: On the Composition of ML Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of specifications is a key difference between traditional software\nengineering and machine learning. We discuss how it drastically impacts how we\nthink about divide-and-conquer approaches to system design, and how it impacts\nreuse, testing and debugging activities. Traditionally, specifications provide\na cornerstone for compositional reasoning and for the divide-and-conquer\nstrategy of how we build large and complex systems from components, but those\nare hard to come by for machine-learned components. While the lack of\nspecification seems like a fundamental new problem at first sight, in fact\nsoftware engineers routinely deal with iffy specifications in practice: we face\nweak specifications, wrong specifications, and unanticipated interactions among\ncomponents and their specifications. Machine learning may push us further, but\nthe problems are not fundamentally new. Rethinking machine-learning model\ncomposition from the perspective of the feature interaction problem, we may\neven teach us a thing or two on how to move forward, including the importance\nof integration testing, of requirements engineering, and of design.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:46:29 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["K\u00e4stner", "Christian", ""], ["Kang", "Eunsuk", ""], ["Apel", "Sven", ""]]}, {"id": "2105.06456", "submitter": "Radu Tudor Ionescu", "authors": "Ana-Cristina Rogoz, Mihaela Gaman, Radu Tudor Ionescu", "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a corpus for satire detection in Romanian news. We\ngathered 55,608 public news articles from multiple real and satirical news\nsources, composing one of the largest corpora for satire detection regardless\nof language and the only one for the Romanian language. We provide an official\nsplit of the text samples, such that training news articles belong to different\nsources than test news articles, thus ensuring that models do not achieve high\nperformance simply due to overfitting. We conduct experiments with two\nstate-of-the-art deep neural models, resulting in a set of strong baselines for\nour novel corpus. Our results show that the machine-level accuracy for satire\ndetection in Romanian is quite low (under 73% on the test set) compared to the\nhuman-level accuracy (87%), leaving enough room for improvement in future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:54:37 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 05:24:26 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 16:35:45 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Rogoz", "Ana-Cristina", ""], ["Gaman", "Mihaela", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2105.06461", "submitter": "Zhongzheng Ren", "authors": "Zhongzheng Ren, Ishan Misra, Alexander G. Schwing, and Rohit Girdhar", "title": "3D Spatial Recognition without Spatially Labeled 3D", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce WyPR, a Weakly-supervised framework for Point cloud Recognition,\nrequiring only scene-level class tags as supervision. WyPR jointly addresses\nthree core 3D recognition tasks: point-level semantic segmentation, 3D proposal\ngeneration, and 3D object detection, coupling their predictions through self\nand cross-task consistency losses. We show that in conjunction with standard\nmultiple-instance learning objectives, WyPR can detect and segment objects in\npoint cloud data without access to any spatial labels at training time. We\ndemonstrate its efficacy using the ScanNet and S3DIS datasets, outperforming\nprior state of the art on weakly-supervised segmentation by more than 6% mIoU.\nIn addition, we set up the first benchmark for weakly-supervised 3D object\ndetection on both datasets, where WyPR outperforms standard approaches and\nestablishes strong baselines for future work.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:58:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ren", "Zhongzheng", ""], ["Misra", "Ishan", ""], ["Schwing", "Alexander G.", ""], ["Girdhar", "Rohit", ""]]}, {"id": "2105.06464", "submitter": "Zhiding Yu", "authors": "Shiyi Lan, Zhiding Yu, Christopher Choy, Subhashree Radhakrishnan,\n  Guilin Liu, Yuke Zhu, Larry S. Davis, Anima Anandkumar", "title": "DiscoBox: Weakly Supervised Instance Segmentation and Semantic\n  Correspondence from Box Supervision", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DiscoBox, a novel framework that jointly learns instance\nsegmentation and semantic correspondence using bounding box supervision.\nSpecifically, we propose a self-ensembling framework where instance\nsegmentation and semantic correspondence are jointly guided by a structured\nteacher in addition to the bounding box supervision. The teacher is a\nstructured energy model incorporating a pairwise potential and a cross-image\npotential to model the pairwise pixel relationships both within and across the\nboxes. Minimizing the teacher energy simultaneously yields refined object masks\nand dense correspondences between intra-class objects, which are taken as\npseudo-labels to supervise the task network and provide positive/negative\ncorrespondence pairs for dense constrastive learning. We show a symbiotic\nrelationship where the two tasks mutually benefit from each other. Our best\nmodel achieves 37.9% AP on COCO instance segmentation, surpassing prior weakly\nsupervised methods and is competitive to supervised methods. We also obtain\nstate of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with\nreal-time inference.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:59:41 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 23:19:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lan", "Shiyi", ""], ["Yu", "Zhiding", ""], ["Choy", "Christopher", ""], ["Radhakrishnan", "Subhashree", ""], ["Liu", "Guilin", ""], ["Zhu", "Yuke", ""], ["Davis", "Larry S.", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2105.06466", "submitter": "Steven Liu", "authors": "Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu,\n  Bryan Russell", "title": "Editing Conditional Radiance Fields", "comments": "Code: https://github.com/stevliu/editnerf Website:\n  http://editnerf.csail.mit.edu/, v2 updated figure 8 and included additional\n  details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A neural radiance field (NeRF) is a scene model supporting high-quality view\nsynthesis, optimized per scene. In this paper, we explore enabling user editing\nof a category-level NeRF - also known as a conditional radiance field - trained\non a shape category. Specifically, we introduce a method for propagating coarse\n2D user scribbles to the 3D space, to modify the color or shape of a local\nregion. First, we propose a conditional radiance field that incorporates new\nmodular network components, including a shape branch that is shared across\nobject instances. Observing multiple instances of the same category, our model\nlearns underlying part semantics without any supervision, thereby allowing the\npropagation of coarse 2D user scribbles to the entire 3D region (e.g., chair\nseat). Next, we propose a hybrid network update strategy that targets specific\nnetwork components, which balances efficiency and accuracy. During user\ninteraction, we formulate an optimization problem that both satisfies the\nuser's constraints and preserves the original object structure. We demonstrate\nour approach on various editing tasks over three shape datasets and show that\nit outperforms prior neural editing approaches. Finally, we edit the appearance\nand shape of a real photograph and show that the edit propagates to\nextrapolated novel views.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:59:48 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 16:30:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Liu", "Steven", ""], ["Zhang", "Xiuming", ""], ["Zhang", "Zhoutong", ""], ["Zhang", "Richard", ""], ["Zhu", "Jun-Yan", ""], ["Russell", "Bryan", ""]]}, {"id": "2105.06479", "submitter": "Eliu Huerta", "authors": "E. A. Huerta and Zhizhen Zhao", "title": "Advances in Machine and Deep Learning for Modeling and Real-time\n  Detection of Multi-Messenger Sources", "comments": "30 pages, 11 figures. Invited chapter to be published in \"Handbook of\n  Gravitational Wave Astronomy\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.HE cs.AI cs.LG gr-qc", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We live in momentous times. The science community is empowered with an\narsenal of cosmic messengers to study the Universe in unprecedented detail.\nGravitational waves, electromagnetic waves, neutrinos and cosmic rays cover a\nwide range of wavelengths and time scales. Combining and processing these\ndatasets that vary in volume, speed and dimensionality requires new modes of\ninstrument coordination, funding and international collaboration with a\nspecialized human and technological infrastructure. In tandem with the advent\nof large-scale scientific facilities, the last decade has experienced an\nunprecedented transformation in computing and signal processing algorithms. The\ncombination of graphics processing units, deep learning, and the availability\nof open source, high-quality datasets, have powered the rise of artificial\nintelligence. This digital revolution now powers a multi-billion dollar\nindustry, with far-reaching implications in technology and society. In this\nchapter we describe pioneering efforts to adapt artificial intelligence\nalgorithms to address computational grand challenges in Multi-Messenger\nAstrophysics. We review the rapid evolution of these disruptive algorithms,\nfrom the first class of algorithms introduced in early 2017, to the\nsophisticated algorithms that now incorporate domain expertise in their\narchitectural design and optimization schemes. We discuss the importance of\nscientific visualization and extreme-scale computing in reducing\ntime-to-insight and obtaining new knowledge from the interplay between models\nand data.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:00:02 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Huerta", "E. A.", ""], ["Zhao", "Zhizhen", ""]]}, {"id": "2105.06499", "submitter": "Julian Katz-Samuels", "authors": "Julian Katz-Samuels, Jifan Zhang, Lalit Jain, Kevin Jamieson", "title": "Improved Algorithms for Agnostic Pool-based Active Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider active learning for binary classification in the agnostic\npool-based setting. The vast majority of works in active learning in the\nagnostic setting are inspired by the CAL algorithm where each query is\nuniformly sampled from the disagreement region of the current version space.\nThe sample complexity of such algorithms is described by a quantity known as\nthe disagreement coefficient which captures both the geometry of the hypothesis\nspace as well as the underlying probability space. To date, the disagreement\ncoefficient has been justified by minimax lower bounds only, leaving the door\nopen for superior instance dependent sample complexities. In this work we\npropose an algorithm that, in contrast to uniform sampling over the\ndisagreement region, solves an experimental design problem to determine a\ndistribution over examples from which to request labels. We show that the new\napproach achieves sample complexity bounds that are never worse than the best\ndisagreement coefficient-based bounds, but in specific cases can be\ndramatically smaller. From a practical perspective, the proposed algorithm\nrequires no hyperparameters to tune (e.g., to control the aggressiveness of\nsampling), and is computationally efficient by means of assuming access to an\nempirical risk minimization oracle (without any constraints). Empirically, we\ndemonstrate that our algorithm is superior to state of the art agnostic active\nlearning algorithms on image classification datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:24:30 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Katz-Samuels", "Julian", ""], ["Zhang", "Jifan", ""], ["Jain", "Lalit", ""], ["Jamieson", "Kevin", ""]]}, {"id": "2105.06506", "submitter": "Joon Sik Kim", "authors": "Joon Sik Kim, Gregory Plumb, Ameet Talwalkar", "title": "Sanity Simulations for Saliency Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Saliency methods are a popular class of feature attribution tools that aim to\ncapture a model's predictive reasoning by identifying \"important\" pixels in an\ninput image. However, the development and adoption of saliency methods are\ncurrently hindered by the lack of access to underlying model reasoning, which\nprevents accurate method evaluation. In this work, we design a synthetic\nevaluation framework, SMERF, that allows us to perform ground-truth-based\nevaluation of saliency methods while controlling the underlying complexity of\nmodel reasoning. Experimental evaluations via SMERF reveal significant\nlimitations in existing saliency methods, especially given the relative\nsimplicity of SMERF's synthetic evaluation tasks. Moreover, the SMERF\nbenchmarking suite represents a useful tool in the development of new saliency\nmethods to potentially overcome these limitations.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:40:57 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 13:53:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kim", "Joon Sik", ""], ["Plumb", "Gregory", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "2105.06508", "submitter": "Shailesh Arya", "authors": "Shailesh Arya", "title": "Internet of Things (IoT) Based Video Analytics: a use case of Smart\n  Doorbell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vision of the internet of things (IoT) is a reality now. IoT devices are\ngetting cheaper, smaller. They are becoming more and more computationally and\nenergy-efficient. The global market of IoT-based video analytics has seen\nsignificant growth in recent years and it is expected to be a growing market\nsegment. For any IoT-based video analytics application, few key points\nrequired, such as cost-effectiveness, widespread use, flexible design, accurate\nscene detection, reusability of the framework. Video-based smart doorbell\nsystem is one such application domain for video analytics where many commercial\nofferings are available in the consumer market. However, such existing\nofferings are costly, monolithic, and proprietary. Also, there will be a\ntrade-off between accuracy and portability. To address the foreseen problems,\nI'm proposing a distributed framework for video analytics with a use case of a\nsmart doorbell system. The proposed framework uses AWS cloud services as a base\nplatform and to meet the price affordability constraint, the system was\nimplemented on affordable Raspberry Pi. The smart doorbell will be able to\nrecognize the known/unknown person with at most accuracy. The smart doorbell\nsystem is also having additional detection functionalities such as harmful\nweapon detection, noteworthy vehicle detection, animal/pet detection. An iOS\napplication is specifically developed for this implementation which can receive\nthe notification from the smart doorbell in real-time. Finally, the paper also\nmentions the classical approaches for video analytics, their feasibility in\nimplementing with this use-case, and comparative analysis in terms of accuracy\nand time required to detect an object in the frame is carried out. Results\nconclude that AWS cloud-based approach is worthy for this smart doorbell use\ncase.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:48:48 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Arya", "Shailesh", ""]]}, {"id": "2105.06512", "submitter": "Lorena Qendro", "authors": "Lorena Qendro, Sangwon Ha, Ren\\'e de Jong, Partha Maji", "title": "Stochastic-Shield: A Probabilistic Approach Towards Training-Free\n  Adversarial Defense in Quantized CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantized neural networks (NN) are the common standard to efficiently deploy\ndeep learning models on tiny hardware platforms. However, we notice that\nquantized NNs are as vulnerable to adversarial attacks as the full-precision\nmodels. With the proliferation of neural networks on small devices that we\ncarry or surround us, there is a need for efficient models without sacrificing\ntrust in the prediction in presence of malign perturbations. Current mitigation\napproaches often need adversarial training or are bypassed when the strength of\nadversarial examples is increased.\n  In this work, we investigate how a probabilistic framework would assist in\novercoming the aforementioned limitations for quantized deep learning models.\nWe explore Stochastic-Shield: a flexible defense mechanism that leverages input\nfiltering and a probabilistic deep learning approach materialized via Monte\nCarlo Dropout. We show that it is possible to jointly achieve efficiency and\nrobustness by accurately enabling each module without the burden of\nre-retraining or ad hoc fine-tuning.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:59:15 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Qendro", "Lorena", ""], ["Ha", "Sangwon", ""], ["de Jong", "Ren\u00e9", ""], ["Maji", "Partha", ""]]}, {"id": "2105.06514", "submitter": "Bansidhar Mangalwedhekar", "authors": "Bansidhar Mangalwedhekar", "title": "Distilling BERT for low complexity network training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies the efficiency of transferring BERT learnings to low\ncomplexity models like BiLSTM, BiLSTM with attention and shallow CNNs using\nsentiment analysis on SST-2 dataset. It also compares the complexity of\ninference of the BERT model with these lower complexity models and underlines\nthe importance of these techniques in enabling high performance NLP models on\nedge devices like mobiles, tablets and MCU development boards like Raspberry Pi\netc. and enabling exciting new applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 19:09:22 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Mangalwedhekar", "Bansidhar", ""]]}, {"id": "2105.06517", "submitter": "Arash MohammadHasani", "authors": "Arash Mohammadhasani, Hamed Mehrivash, Alan Lynch, Zhan Shu", "title": "Reinforcement Learning Based Safe Decision Making for Highway Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we develop a safe decision-making method for self-driving cars\nin a multi-lane, single-agent setting. The proposed approach utilizes deep\nreinforcement learning (RL) to achieve a high-level policy for safe tactical\ndecision-making. We address two major challenges that arise solely in\nautonomous navigation. First, the proposed algorithm ensures that collisions\nnever happen, and therefore accelerate the learning process. Second, the\nproposed algorithm takes into account the unobservable states in the\nenvironment. These states appear mainly due to the unpredictable behavior of\nother agents, such as cars, and pedestrians, and make the Markov Decision\nProcess (MDP) problematic when dealing with autonomous navigation. Simulations\nfrom a well-known self-driving car simulator demonstrate the applicability of\nthe proposed method\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 19:17:30 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Mohammadhasani", "Arash", ""], ["Mehrivash", "Hamed", ""], ["Lynch", "Alan", ""], ["Shu", "Zhan", ""]]}, {"id": "2105.06535", "submitter": "Dushyant Sahoo", "authors": "Dushyant Sahoo, Christos Davatzikos", "title": "Learning Robust Hierarchical Patterns of Human Brain across Many fMRI\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Resting-state fMRI has been shown to provide surrogate biomarkers for the\nanalysis of various diseases. In addition, fMRI data helps in understanding the\nbrain's functional working during resting state and task-induced activity. To\nimprove the statistical power of biomarkers and the understanding mechanism of\nthe brain, pooling of multi-center studies has become increasingly popular. But\npooling the data from multiple sites introduces variations due to hardware,\nsoftware, and environment. In this paper, we look at the estimation problem of\nhierarchical Sparsity Connectivity Patterns (hSCPs) in fMRI data acquired on\nmultiple sites. We introduce a simple yet effective matrix factorization based\nformulation to reduce site-related effects while preserving biologically\nrelevant variations. We leverage adversarial learning in the unsupervised\nregime to improve the reproducibility of the components. Experiments on\nsimulated datasets display that the proposed method can estimate components\nwith improved accuracy and reproducibility. We also demonstrate the improved\nreproducibility of the components while preserving age-related variation on a\nreal dataset compiled from multiple sites.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 20:10:00 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Sahoo", "Dushyant", ""], ["Davatzikos", "Christos", ""]]}, {"id": "2105.06543", "submitter": "Wei Xie", "authors": "Hua Zheng, Wei Xie, Ilya O. Ryzhov, Dongming Xie", "title": "Policy Optimization in Bayesian Network Hybrid Models of\n  Biomanufacturing Processes", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biopharmaceutical manufacturing is a rapidly growing industry with impact in\nvirtually all branches of medicine. Biomanufacturing processes require close\nmonitoring and control, in the presence of complex bioprocess dynamics with\nmany interdependent factors, as well as extremely limited data due to the high\ncost and long duration of experiments. We develop a novel model-based\nreinforcement learning framework that can achieve human-level control in\nlow-data environments. The model uses a probabilistic knowledge graph to\ncapture causal interdependencies between factors in the underlying stochastic\ndecision process, leveraging information from existing kinetic models from\ndifferent unit operations while incorporating real-world experimental data. We\nthen present a computationally efficient, provably convergent stochastic\ngradient method for policy optimization. Validation is conducted on a realistic\napplication with a multi-dimensional, continuous state variable.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 20:39:02 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zheng", "Hua", ""], ["Xie", "Wei", ""], ["Ryzhov", "Ilya O.", ""], ["Xie", "Dongming", ""]]}, {"id": "2105.06544", "submitter": "Chuanlong Li", "authors": "Chuanlong Li", "title": "Stroke Lesion Segmentation with Visual Cortex Anatomy Alike Neural Nets", "comments": "Update the segmentation examples figure (Fig. 7); Add the\n  implementation link; Language related issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cerebrovascular accident, or commonly known as stroke, is an acute disease\nwith extreme impact on patients and healthcare systems and is the second\nlargest cause of death worldwide. Fast and precise stroke lesion detection and\nlocation is an extreme important process with regards to stroke diagnosis,\ntreatment, and prognosis. Except from the manual segmentation approach, machine\nlearning based segmentation methods are the most promising ones when\nconsidering efficiency and accuracy, and convolutional neural network based\nmodels are the first of its kind. However, most of these neural network models\ndo not really align with the brain anatomical structures. Intuitively, this\nwork presents a more brain alike model which mimics the anatomical structure of\nthe human visual cortex. Through the preliminary experiments on the stroke\nlesion segmentation task, the proposed model is found to be able to perform\nequally well or better to the de-facto standard U-Net. Part of the\nimplementation will be made available at https://github.com/DarkoBomer/VCA-Net.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 20:39:29 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 07:01:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Li", "Chuanlong", ""]]}, {"id": "2105.06548", "submitter": "Angela Fan", "authors": "Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur\n  Szlam, Jason Weston, Angela Fan", "title": "Not All Memories are Created Equal: Learning to Forget by Expiring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have shown promising results in sequence modeling tasks\nthat require long-term memory. Recent work investigated mechanisms to reduce\nthe computational cost of preserving and storing memories. However, not all\ncontent in the past is equally important to remember. We propose Expire-Span, a\nmethod that learns to retain the most important information and expire the\nirrelevant information. This forgetting of memories enables Transformers to\nscale to attend over tens of thousands of previous timesteps efficiently, as\nnot all states from previous timesteps are preserved. We demonstrate that\nExpire-Span can help models identify and retain critical information and show\nit can achieve strong performance on reinforcement learning tasks specifically\ndesigned to challenge this functionality. Next, we show that Expire-Span can\nscale to memories that are tens of thousands in size, setting a new state of\nthe art on incredibly long context tasks such as character-level language\nmodeling and a frame-by-frame moving objects task. Finally, we analyze the\nefficiency of Expire-Span compared to existing approaches and demonstrate that\nit trains faster and uses less memory.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 20:50:13 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 15:37:24 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Ju", "Da", ""], ["Poff", "Spencer", ""], ["Roller", "Stephen", ""], ["Szlam", "Arthur", ""], ["Weston", "Jason", ""], ["Fan", "Angela", ""]]}, {"id": "2105.06558", "submitter": "Nengfeng Zhou", "authors": "Nengfeng Zhou, Zach Zhang, Vijayan N. Nair, Harsh Singhal, Jie Chen,\n  and Agus Sudjianto", "title": "Bias, Fairness, and Accountability with AI and ML Algorithms", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of AI and ML algorithms has led to opportunities as well as\nchallenges. In this paper, we provide an overview of bias and fairness issues\nthat arise with the use of ML algorithms. We describe the types and sources of\ndata bias, and discuss the nature of algorithmic unfairness. This is followed\nby a review of fairness metrics in the literature, discussion of their\nlimitations, and a description of de-biasing (or mitigation) techniques in the\nmodel life cycle.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 21:12:04 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhou", "Nengfeng", ""], ["Zhang", "Zach", ""], ["Nair", "Vijayan N.", ""], ["Singhal", "Harsh", ""], ["Chen", "Jie", ""], ["Sudjianto", "Agus", ""]]}, {"id": "2105.06569", "submitter": "Siddhartha Satpathi Mr", "authors": "Siddhartha Satpathi and R Srikant", "title": "The Dynamics of Gradient Descent for Overparametrized Neural Networks", "comments": "24 pages, published in parts at L4DC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dynamics of gradient descent (GD) in overparameterized single\nhidden layer neural networks with a squared loss function. Recently, it has\nbeen shown that, under some conditions, the parameter values obtained using GD\nachieve zero training error and generalize well if the initial conditions are\nchosen appropriately. Here, through a Lyapunov analysis, we show that the\ndynamics of neural network weights under GD converge to a point which is close\nto the minimum norm solution subject to the condition that there is no training\nerror when using the linear approximation to the neural network. To illustrate\nthe application of this result, we show that the GD converges to a prediction\nfunction that generalizes well, thereby providing an alternative proof of the\ngeneralization results in Arora et al. (2019).\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 22:20:30 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Satpathi", "Siddhartha", ""], ["Srikant", "R", ""]]}, {"id": "2105.06577", "submitter": "Yingnan Cui", "authors": "Anuradha M. Annaswamy, Anubhav Guha, Yingnan Cui, Joseph E. Gaudio,\n  Jos\\'e M. Moreu", "title": "Online Algorithms and Policies Using Adaptive and Machine Learning\n  Approaches", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of real-time control and learning in dynamic\nsystems subjected to uncertainties. Adaptive approaches are proposed to address\nthe problem, which are combined to with methods and tools in Reinforcement\nLearning (RL) and Machine Learning (ML). Algorithms are proposed in\ncontinuous-time that combine adaptive approaches with RL leading to online\ncontrol policies that guarantee stable behavior in the presence of parametric\nuncertainties that occur in real-time. Algorithms are proposed in discrete-time\nthat combine adaptive approaches proposed for parameter and output estimation\nand ML approaches proposed for accelerated performance that guarantee stable\nestimation even in the presence of time-varying regressors, and for accelerated\nlearning of the parameters with persistent excitation. Numerical validations of\nall algorithms are carried out using a quadrotor landing task on a moving\nplatform and benchmark problems in ML. All results clearly point out the\nadvantage of adaptive approaches for real-time control and learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 22:51:25 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 03:19:02 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Annaswamy", "Anuradha M.", ""], ["Guha", "Anubhav", ""], ["Cui", "Yingnan", ""], ["Gaudio", "Joseph E.", ""], ["Moreu", "Jos\u00e9 M.", ""]]}, {"id": "2105.06587", "submitter": "Tomas Geffner", "authors": "Tomas Geffner and Justin Domke", "title": "Empirical Evaluation of Biased Methods for Alpha Divergence Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we empirically evaluate biased methods for alpha-divergence\nminimization. In particular, we focus on how the bias affects the final\nsolutions found, and how this depends on the dimensionality of the problem. We\nfind that (i) solutions returned by these methods appear to be strongly biased\ntowards minimizers of the traditional \"exclusive\" KL-divergence, KL(q||p), and\n(ii) in high dimensions, an impractically large amount of computation is needed\nto mitigate this bias and obtain solutions that actually minimize the\nalpha-divergence of interest.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 23:16:54 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Geffner", "Tomas", ""], ["Domke", "Justin", ""]]}, {"id": "2105.06591", "submitter": "Subhabrata Majumdar", "authors": "Noemi Derzsy, Subhabrata Majumdar, Rajat Malik", "title": "An Interpretable Graph-based Mapping of Trustworthy Machine Learning\n  Research", "comments": "Accepted in CompleNet-2021 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in ensuring machine learning (ML) frameworks\nbehave in a socially responsible manner and are deemed trustworthy. Although\nconsiderable progress has been made in the field of Trustworthy ML (TwML) in\nthe recent past, much of the current characterization of this progress is\nqualitative. Consequently, decisions about how to address issues of\ntrustworthiness and future research goals are often left to the interested\nresearcher. In this paper, we present the first quantitative approach to\ncharacterize the comprehension of TwML research. We build a co-occurrence\nnetwork of words using a web-scraped corpus of more than 7,000 peer-reviewed\nrecent ML papers -- consisting of papers both related and unrelated to TwML. We\nuse community detection to obtain semantic clusters of words in this network\nthat can infer relative positions of TwML topics. We propose an innovative\nfingerprinting algorithm to obtain probabilistic similarity scores for\nindividual words, then combine them to give a paper-level relevance score. The\noutcomes of our analysis inform a number of interesting insights on advancing\nthe field of TwML research.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 23:25:07 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Derzsy", "Noemi", ""], ["Majumdar", "Subhabrata", ""], ["Malik", "Rajat", ""]]}, {"id": "2105.06593", "submitter": "Woodrow Wang", "authors": "Woodrow Z. Wang, Mark Beliaev, Erdem B{\\i}y{\\i}k, Daniel A. Lazar,\n  Ramtin Pedarsani, Dorsa Sadigh", "title": "Emergent Prosociality in Multi-Agent Games Through Gifting", "comments": "9 pages, 6 figures, IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination is often critical to forming prosocial behaviors -- behaviors\nthat increase the overall sum of rewards received by all agents in a\nmulti-agent game. However, state of the art reinforcement learning algorithms\noften suffer from converging to socially less desirable equilibria when\nmultiple equilibria exist. Previous works address this challenge with explicit\nreward shaping, which requires the strong assumption that agents can be forced\nto be prosocial. We propose using a less restrictive peer-rewarding mechanism,\ngifting, that guides the agents toward more socially desirable equilibria while\nallowing agents to remain selfish and decentralized. Gifting allows each agent\nto give some of their reward to other agents. We employ a theoretical framework\nthat captures the benefit of gifting in converging to the prosocial equilibrium\nby characterizing the equilibria's basins of attraction in a dynamical system.\nWith gifting, we demonstrate increased convergence of high risk, general-sum\ncoordination games to the prosocial equilibrium both via numerical analysis and\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 23:28:30 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wang", "Woodrow Z.", ""], ["Beliaev", "Mark", ""], ["B\u0131y\u0131k", "Erdem", ""], ["Lazar", "Daniel A.", ""], ["Pedarsani", "Ramtin", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2105.06598", "submitter": "Vineet Garg", "authors": "Vineet Garg, Wonil Chang, Siddharth Sigtia, Saurabh Adya, Pramod\n  Simha, Pranay Dighe, Chandra Dhir", "title": "Streaming Transformer for Hardware Efficient Voice Trigger Detection and\n  False Trigger Mitigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified and hardware efficient architecture for two stage voice\ntrigger detection (VTD) and false trigger mitigation (FTM) tasks. Two stage VTD\nsystems of voice assistants can get falsely activated to audio segments\nacoustically similar to the trigger phrase of interest. FTM systems cancel such\nactivations by using post trigger audio context. Traditional FTM systems rely\non automatic speech recognition lattices which are computationally expensive to\nobtain on device. We propose a streaming transformer (TF) encoder architecture,\nwhich progressively processes incoming audio chunks and maintains audio context\nto perform both VTD and FTM tasks using only acoustic features. The proposed\njoint model yields an average 18% relative reduction in false reject rate (FRR)\nfor the VTD task at a given false alarm rate. Moreover, our model suppresses\n95% of the false triggers with an additional one second of post-trigger audio.\nFinally, on-device measurements show 32% reduction in runtime memory and 56%\nreduction in inference time compared to non-streaming version of the model.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 00:41:42 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Garg", "Vineet", ""], ["Chang", "Wonil", ""], ["Sigtia", "Siddharth", ""], ["Adya", "Saurabh", ""], ["Simha", "Pramod", ""], ["Dighe", "Pranay", ""], ["Dhir", "Chandra", ""]]}, {"id": "2105.06604", "submitter": "Zachary Pardos", "authors": "Weijie Jiang, Zachary A. Pardos", "title": "Towards Equity and Algorithmic Fairness in Student Grade Prediction", "comments": "Accepted to the 2021 AAAI/ACM Conference on AI, Ethics, and Society\n  (AIES '21)", "journal-ref": null, "doi": "10.1145/3461702.3462623", "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Equity of educational outcome and fairness of AI with respect to race have\nbeen topics of increasing importance in education. In this work, we address\nboth with empirical evaluations of grade prediction in higher education, an\nimportant task to improve curriculum design, plan interventions for academic\nsupport, and offer course guidance to students. With fairness as the aim, we\ntrial several strategies for both label and instance balancing to attempt to\nminimize differences in algorithm performance with respect to race. We find\nthat an adversarial learning approach, combined with grade label balancing,\nachieved by far the fairest results. With equity of educational outcome as the\naim, we trial strategies for boosting predictive performance on historically\nunderserved groups and find success in sampling those groups in inverse\nproportion to their historic outcomes. With AI-infused technology supports\nincreasingly prevalent on campuses, our methodologies fill a need for\nframeworks to consider performance trade-offs with respect to sensitive student\nattributes and allow institutions to instrument their AI resources in ways that\nare attentive to equity and fairness.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 01:12:01 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Jiang", "Weijie", ""], ["Pardos", "Zachary A.", ""]]}, {"id": "2105.06618", "submitter": "Mahdi Abolghasemi", "authors": "Mahdi Abolghasemi, Babak Abbasi, Toktam Babaei, Zahra HosseiniFard", "title": "How to effectively use machine learning models to predict the solutions\n  for optimization problems: lessons from loss function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Using machine learning in solving constraint optimization and combinatorial\nproblems is becoming an active research area in both computer science and\noperations research communities. This paper aims to predict a good solution for\nconstraint optimization problems using advanced machine learning techniques. It\nextends the work of \\cite{abbasi2020predicting} to use machine learning models\nfor predicting the solution of large-scaled stochastic optimization models by\nexamining more advanced algorithms and various costs associated with the\npredicted values of decision variables. It also investigates the importance of\nloss function and error criterion in machine learning models where they are\nused for predicting solutions of optimization problems. We use a blood\ntransshipment problem as the case study. The results for the case study show\nthat LightGBM provides promising solutions and outperforms other machine\nlearning models used by \\cite{abbasi2020predicting} specially when mean\nabsolute deviation criterion is used.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 02:14:00 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Abolghasemi", "Mahdi", ""], ["Abbasi", "Babak", ""], ["Babaei", "Toktam", ""], ["HosseiniFard", "Zahra", ""]]}, {"id": "2105.06631", "submitter": "Xiaoqiang Wang", "authors": "Xiaoqiang Wang, Yali Du, Shengyu Zhu, Liangjun Ke, Zhitang Chen,\n  Jianye Hao and Jun Wang", "title": "Ordering-Based Causal Discovery with Reinforcement Learning", "comments": "Accepted to IJCAI'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a long-standing question to discover causal relations among a set of\nvariables in many empirical sciences. Recently, Reinforcement Learning (RL) has\nachieved promising results in causal discovery from observational data.\nHowever, searching the space of directed graphs and enforcing acyclicity by\nimplicit penalties tend to be inefficient and restrict the existing RL-based\nmethod to small scale problems. In this work, we propose a novel RL-based\napproach for causal discovery, by incorporating RL into the ordering-based\nparadigm. Specifically, we formulate the ordering search problem as a\nmulti-step Markov decision process, implement the ordering generating process\nwith an encoder-decoder architecture, and finally use RL to optimize the\nproposed model based on the reward mechanisms designed for~each ordering. A\ngenerated ordering would then be processed using variable selection to obtain\nthe final causal graph. We analyze the consistency and computational complexity\nof the proposed method, and empirically show that a pretrained model can be\nexploited to accelerate training. Experimental results on both synthetic and\nreal data sets shows that the proposed method achieves a much improved\nperformance over existing RL-based method.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 03:49:59 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 02:33:16 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Xiaoqiang", ""], ["Du", "Yali", ""], ["Zhu", "Shengyu", ""], ["Ke", "Liangjun", ""], ["Chen", "Zhitang", ""], ["Hao", "Jianye", ""], ["Wang", "Jun", ""]]}, {"id": "2105.06637", "submitter": "Nan Gao", "authors": "Nan Gao, Max Marschall, Jane Burry, Simon Watkins, Flora D. Salim", "title": "Understanding occupants' behaviour, engagement, emotion, and comfort\n  indoors with heterogeneous sensors and wearables", "comments": "This paper introduces In-Gauge and En-Gage datasets. The link for the\n  datasets:\n  https://rmit.figshare.com/articles/dataset/In-Gauge_and_En-Gage_Datasets/14578908", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We conducted a field study at a K-12 private school in the suburbs of\nMelbourne, Australia. The data capture contained two elements: First, a 5-month\nlongitudinal field study In-Gauge using two outdoor weather stations, as well\nas indoor weather stations in 17 classrooms and temperature sensors on the\nvents of occupant-controlled room air-conditioners; these were collated into\nindividual datasets for each classroom at a 5-minute logging frequency,\nincluding additional data on occupant presence. The dataset was used to derive\npredictive models of how occupants operate room air-conditioning units. Second,\nwe tracked 23 students and 6 teachers in a 4-week cross-sectional study\nEn-Gage, using wearable sensors to log physiological data, as well as daily\nsurveys to query the occupants' thermal comfort, learning engagement, emotions\nand seating behaviours. This is the first publicly available dataset studying\nthe daily behaviours and engagement of high school students using heterogeneous\nmethods. The combined data could be used to analyse the relationships between\nindoor climates and mental states of school students.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 04:17:24 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gao", "Nan", ""], ["Marschall", "Max", ""], ["Burry", "Jane", ""], ["Watkins", "Simon", ""], ["Salim", "Flora D.", ""]]}, {"id": "2105.06640", "submitter": "Alexander Wong", "authors": "Maya Pavlova, Naomi Terhljan, Audrey G. Chung, Andy Zhao, Siddharth\n  Surana, Hossein Aboutalebi, Hayden Gunraj, Ali Sabri, Amer Alaref, and\n  Alexander Wong", "title": "COVID-Net CXR-2: An Enhanced Deep Convolutional Neural Network Design\n  for Detection of COVID-19 Cases from Chest X-ray Images", "comments": "12 pages. arXiv admin note: text overlap with arXiv:2105.00256", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the COVID-19 pandemic continues to devastate globally, the use of chest\nX-ray (CXR) imaging as a complimentary screening strategy to RT-PCR testing\ncontinues to grow given its routine clinical use for respiratory complaint. As\npart of the COVID-Net open source initiative, we introduce COVID-Net CXR-2, an\nenhanced deep convolutional neural network design for COVID-19 detection from\nCXR images built using a greater quantity and diversity of patients than the\noriginal COVID-Net. To facilitate this, we also introduce a new benchmark\ndataset composed of 19,203 CXR images from a multinational cohort of 16,656\npatients from at least 51 countries, making it the largest, most diverse\nCOVID-19 CXR dataset in open access form. The COVID-Net CXR-2 network achieves\nsensitivity and positive predictive value of 95.5%/97.0%, respectively, and was\naudited in a transparent and responsible manner. Explainability-driven\nperformance validation was used during auditing to gain deeper insights in its\ndecision-making behaviour and to ensure clinically relevant factors are\nleveraged for improving trust in its usage. Radiologist validation was also\nconducted, where select cases were reviewed and reported on by two\nboard-certified radiologists with over 10 and 19 years of experience,\nrespectively, and showed that the critical factors leveraged by COVID-Net CXR-2\nare consistent with radiologist interpretations. While not a production-ready\nsolution, we hope the open-source, open-access release of COVID-Net CXR-2 and\nthe respective CXR benchmark dataset will encourage researchers, clinical\nscientists, and citizen scientists to accelerate advancements and innovations\nin the fight against the pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 04:29:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Pavlova", "Maya", ""], ["Terhljan", "Naomi", ""], ["Chung", "Audrey G.", ""], ["Zhao", "Andy", ""], ["Surana", "Siddharth", ""], ["Aboutalebi", "Hossein", ""], ["Gunraj", "Hayden", ""], ["Sabri", "Ali", ""], ["Alaref", "Amer", ""], ["Wong", "Alexander", ""]]}, {"id": "2105.06643", "submitter": "Rakshitha Godahewa", "authors": "Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob J.\n  Hyndman, Pablo Montero-Manso", "title": "Monash Time Series Forecasting Archive", "comments": "33 pages, 3 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many businesses and industries nowadays rely on large quantities of time\nseries data making time series forecasting an important research area. Global\nforecasting models that are trained across sets of time series have shown a\nhuge potential in providing accurate forecasts compared with the traditional\nunivariate forecasting models that work on isolated series. However, there are\ncurrently no comprehensive time series archives for forecasting that contain\ndatasets of time series from similar sources available for the research\ncommunity to evaluate the performance of new global forecasting algorithms over\na wide variety of datasets. In this paper, we present such a comprehensive time\nseries forecasting archive containing 20 publicly available time series\ndatasets from varied domains, with different characteristics in terms of\nfrequency, series lengths, and inclusion of missing values. We also\ncharacterise the datasets, and identify similarities and differences among\nthem, by conducting a feature analysis. Furthermore, we present the performance\nof a set of standard baseline forecasting methods over all datasets across\neight error metrics, for the benefit of researchers using the archive to\nbenchmark their forecasting algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 04:49:58 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Godahewa", "Rakshitha", ""], ["Bergmeir", "Christoph", ""], ["Webb", "Geoffrey I.", ""], ["Hyndman", "Rob J.", ""], ["Montero-Manso", "Pablo", ""]]}, {"id": "2105.06649", "submitter": "Cangning Fan", "authors": "Cangning Fan, Fangyi Zhang, Peng Liu, Xiuyu Sun, Hao Li, Ting Xiao,\n  Wei Zhao, Xianglong Tang", "title": "Importance Weighted Adversarial Discriminative Transfer for Anomaly\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous transfer methods for anomaly detection generally assume the\navailability of labeled data in source or target domains. However, such an\nassumption is not valid in most real applications where large-scale labeled\ndata are too expensive. Therefore, this paper proposes an importance weighted\nadversarial autoencoder-based method to transfer anomaly detection knowledge in\nan unsupervised manner, particularly for a rarely studied scenario where a\ntarget domain has no labeled normal/abnormal data while only normal data from a\nrelated source domain exist. Specifically, the method learns to align the\ndistributions of normal data in both source and target domains, but leave the\ndistribution of abnormal data in the target domain unchanged. In this way, an\nobvious gap can be produced between the distributions of normal and abnormal\ndata in the target domain, therefore enabling the anomaly detection in the\ndomain. Extensive experiments on multiple synthetic datasets and the UCSD\nbenchmark demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 05:21:17 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 07:23:54 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 07:00:19 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Fan", "Cangning", ""], ["Zhang", "Fangyi", ""], ["Liu", "Peng", ""], ["Sun", "Xiuyu", ""], ["Li", "Hao", ""], ["Xiao", "Ting", ""], ["Zhao", "Wei", ""], ["Tang", "Xianglong", ""]]}, {"id": "2105.06660", "submitter": "Kei Akuzawa", "authors": "Kei Akuzawa, Yusuke Iwasawa, Yutaka Matsuo", "title": "Estimating Disentangled Belief about Hidden State and Hidden Task for\n  Meta-RL", "comments": "accepted to L4DC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is considerable interest in designing meta-reinforcement learning\n(meta-RL) algorithms, which enable autonomous agents to adapt new tasks from\nsmall amount of experience. In meta-RL, the specification (such as reward\nfunction) of current task is hidden from the agent. In addition, states are\nhidden within each task owing to sensor noise or limitations in realistic\nenvironments. Therefore, the meta-RL agent faces the challenge of specifying\nboth the hidden task and states based on small amount of experience. To address\nthis, we propose estimating disentangled belief about task and states,\nleveraging an inductive bias that the task and states can be regarded as global\nand local features of each task. Specifically, we train a hierarchical\nstate-space model (HSSM) parameterized by deep neural networks as an\nenvironment model, whose global and local latent variables correspond to task\nand states, respectively. Because the HSSM does not allow analytical\ncomputation of posterior distribution, i.e., belief, we employ amortized\ninference to approximate it. After the belief is obtained, we can augment\nobservations of a model-free policy with the belief to efficiently train the\npolicy. Moreover, because task and state information are factorized and\ninterpretable, the downstream policy training is facilitated compared with the\nprior methods that did not consider the hierarchical nature. Empirical\nvalidations on a GridWorld environment confirm that the HSSM can separate the\nhidden task and states information. Then, we compare the meta-RL agent with the\nHSSM to prior meta-RL methods in MuJoCo environments, and confirm that our\nagent requires less training data and reaches higher final performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 06:11:36 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Akuzawa", "Kei", ""], ["Iwasawa", "Yusuke", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "2105.06677", "submitter": "Sebastian Palacio", "authors": "Sebastian Palacio, Adriano Lucieri, Mohsin Munir, J\\\"orn Hees, Sheraz\n  Ahmed, Andreas Dengel", "title": "XAI Handbook: Towards a Unified Framework for Explainable AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The field of explainable AI (XAI) has quickly become a thriving and prolific\ncommunity. However, a silent, recurrent and acknowledged issue in this area is\nthe lack of consensus regarding its terminology. In particular, each new\ncontribution seems to rely on its own (and often intuitive) version of terms\nlike \"explanation\" and \"interpretation\". Such disarray encumbers the\nconsolidation of advances in the field towards the fulfillment of scientific\nand regulatory demands e.g., when comparing methods or establishing their\ncompliance with respect to biases and fairness constraints. We propose a\ntheoretical framework that not only provides concrete definitions for these\nterms, but it also outlines all steps necessary to produce explanations and\ninterpretations. The framework also allows for existing contributions to be\nre-contextualized such that their scope can be measured, thus making them\ncomparable to other methods. We show that this framework is compliant with\ndesiderata on explanations, on interpretability and on evaluation metrics. We\npresent a use-case showing how the framework can be used to compare LIME, SHAP\nand MDNet, establishing their advantages and shortcomings. Finally, we discuss\nrelevant trends in XAI as well as recommendations for future work, all from the\nstandpoint of our framework.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 07:28:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Palacio", "Sebastian", ""], ["Lucieri", "Adriano", ""], ["Munir", "Mohsin", ""], ["Hees", "J\u00f6rn", ""], ["Ahmed", "Sheraz", ""], ["Dengel", "Andreas", ""]]}, {"id": "2105.06696", "submitter": "Qi Kuang", "authors": "Fan Zhou, Zhoufan Zhu, Qi Kuang, Liwen Zhang", "title": "Non-decreasing Quantile Function Network with Efficient Exploration for\n  Distributional Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although distributional reinforcement learning (DRL) has been widely examined\nin the past few years, there are two open questions people are still trying to\naddress. One is how to ensure the validity of the learned quantile function,\nthe other is how to efficiently utilize the distribution information. This\npaper attempts to provide some new perspectives to encourage the future\nin-depth studies in these two fields. We first propose a non-decreasing\nquantile function network (NDQFN) to guarantee the monotonicity of the obtained\nquantile estimates and then design a general exploration framework called\ndistributional prediction error (DPE) for DRL which utilizes the entire\ndistribution of the quantile function. In this paper, we not only discuss the\ntheoretical necessity of our method but also show the performance gain it\nachieves in practice by comparing with some competitors on Atari 2600 Games\nespecially in some hard-explored games.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:12:51 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhou", "Fan", ""], ["Zhu", "Zhoufan", ""], ["Kuang", "Qi", ""], ["Zhang", "Liwen", ""]]}, {"id": "2105.06697", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang, Keyou You, Lihua Xie", "title": "Innovation Compression for Communication-efficient Distributed\n  Optimization with Linear Convergence", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.SY eess.SP eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Information compression is essential to reduce communication cost in\ndistributed optimization over peer-to-peer networks. This paper proposes a\ncommunication-efficient linearly convergent distributed (COLD) algorithm to\nsolve strongly convex optimization problems. By compressing innovation vectors,\nwhich are the differences between decision vectors and their estimates, COLD is\nable to achieve linear convergence for a class of $\\delta$-contracted\ncompressors. We explicitly quantify how the compression affects the convergence\nrate and show that COLD matches the same rate of its uncompressed version. To\naccommodate a wider class of compressors that includes the binary quantizer, we\nfurther design a novel dynamical scaling mechanism and obtain the linearly\nconvergent Dyna-COLD. Importantly, our results strictly improve existing\nresults for the quantized consensus problem. Numerical experiments demonstrate\nthe advantages of both algorithms under different compressors.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:15:18 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhang", "Jiaqi", ""], ["You", "Keyou", ""], ["Xie", "Lihua", ""]]}, {"id": "2105.06709", "submitter": "Guofeng Lv", "authors": "Guofeng Lv, Zhiqiang Hu, Yanguang Bi, Shaoting Zhang", "title": "Learning Unknown from Correlations: Graph Neural Network for\n  Inter-novel-protein Interaction Prediction", "comments": "10 pages(3 pages appendix), 2 figures, Accepted by Conference\n  IJCAI2021, which is its extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of multi-type Protein-Protein Interaction (PPI) is fundamental for\nunderstanding biological processes from a systematic perspective and revealing\ndisease mechanisms. Existing methods suffer from significant performance\ndegradation when tested in unseen dataset. In this paper, we investigate the\nproblem and find that it is mainly attributed to the poor performance for\ninter-novel-protein interaction prediction. However, current evaluations\noverlook the inter-novel-protein interactions, and thus fail to give an\ninstructive assessment. As a result, we propose to address the problem from\nboth the evaluation and the methodology. Firstly, we design a new evaluation\nframework that fully respects the inter-novel-protein interactions and gives\nconsistent assessment across datasets. Secondly, we argue that correlations\nbetween proteins must provide useful information for analysis of novel\nproteins, and based on this, we propose a graph neural network based method\n(GNN-PPI) for better inter-novel-protein interaction prediction. Experimental\nresults on real-world datasets of different scales demonstrate that GNN-PPI\nsignificantly outperforms state-of-the-art PPI prediction methods, especially\nfor the inter-novel-protein interaction prediction.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:42:55 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:16:37 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 04:27:33 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lv", "Guofeng", ""], ["Hu", "Zhiqiang", ""], ["Bi", "Yanguang", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2105.06715", "submitter": "Xiaolong Fan", "authors": "Xiaolong Fan, Maoguo Gong, Yue Wu, Hao Li", "title": "Maximizing Mutual Information Across Feature and Topology Views for\n  Learning Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, maximizing mutual information has emerged as a powerful method for\nunsupervised graph representation learning. The existing methods are typically\neffective to capture information from the topology view but ignore the feature\nview. To circumvent this issue, we propose a novel approach by exploiting\nmutual information maximization across feature and topology views.\nSpecifically, we first utilize a multi-view representation learning module to\nbetter capture both local and global information content across feature and\ntopology views on graphs. To model the information shared by the feature and\ntopology spaces, we then develop a common representation learning module using\nmutual information maximization and reconstruction loss minimization. To\nexplicitly encourage diversity between graph representations from the same\nview, we also introduce a disagreement regularization to enlarge the distance\nbetween representations from the same view. Experiments on synthetic and\nreal-world datasets demonstrate the effectiveness of integrating feature and\ntopology views. In particular, compared with the previous supervised methods,\nour proposed method can achieve comparable or even better performance under the\nunsupervised representation and linear evaluation protocol.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:49:40 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Fan", "Xiaolong", ""], ["Gong", "Maoguo", ""], ["Wu", "Yue", ""], ["Li", "Hao", ""]]}, {"id": "2105.06724", "submitter": "Rwiddhi Chakraborty", "authors": "Rwiddhi Chakraborty and Shubhayu Das", "title": "RC2020 Report: Learning De-biased Representations with Biased\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": "3742554", "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As part of the ML Reproducibility Challenge 2020, we investigated the ICML\n2020 paper \"Learning De-biased Representations with Biased Representations\" by\nBahng et al., where the authors formalize and attempt to tackle the so called\n\"cross bias generalization\" problem with a new approach they introduce called\nReBias. This report contains results of our attempts at reproducing the work in\nthe application area of Image Recognition, specifically on the datasets biased\nMNIST and ImageNet. We compare ReBias with other methods - Vanilla, Biased,\nRUBi (as implemented by the authors), and conclude with a discussion concerning\nthe validity of the claims made by the paper. We were able to reproduce results\nreported for the biased MNIST dataset to within 1% of the original values\nreported in the paper. Like the authors, we report results averaged over 3\nruns. However, in a later section, we provide some additional results that\nappear to weaken the central claim of the paper with regards to the biased\nMNIST dataset. We were not able to reproduce results for ImageNet as in the\noriginal paper, but based on communication with the authors, provide a\ndiscussion as to the reasons for the same. This work attempts to be useful to\nother researchers aiming to use ReBias for their own research purposes,\nadvising on certain possible pitfalls that may be encountered in the process.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 09:14:50 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Chakraborty", "Rwiddhi", ""], ["Das", "Shubhayu", ""]]}, {"id": "2105.06725", "submitter": "Zhihao Wen", "authors": "Zhihao Wen, Yuan Fang, Zemin Liu", "title": "Meta-Inductive Node Classification across Graphs", "comments": null, "journal-ref": null, "doi": "10.1145/3404835.3462915", "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised node classification on graphs is an important research\nproblem, with many real-world applications in information retrieval such as\ncontent classification on a social network and query intent classification on\nan e-commerce query graph. While traditional approaches are largely\ntransductive, recent graph neural networks (GNNs) integrate node features with\nnetwork structures, thus enabling inductive node classification models that can\nbe applied to new nodes or even new graphs in the same feature space. However,\ninter-graph differences still exist across graphs within the same domain. Thus,\ntraining just one global model (e.g., a state-of-the-art GNN) to handle all new\ngraphs, whilst ignoring the inter-graph differences, can lead to suboptimal\nperformance.\n  In this paper, we study the problem of inductive node classification across\ngraphs. Unlike existing one-model-fits-all approaches, we propose a novel\nmeta-inductive framework called MI-GNN to customize the inductive model to each\ngraph under a meta-learning paradigm. That is, MI-GNN does not directly learn\nan inductive model; it learns the general knowledge of how to train a model for\nsemi-supervised node classification on new graphs. To cope with the differences\nacross graphs, MI-GNN employs a dual adaptation mechanism at both the graph and\ntask levels. More specifically, we learn a graph prior to adapt for the\ngraph-level differences, and a task prior to adapt for the task-level\ndifferences conditioned on a graph. Extensive experiments on five real-world\ngraph collections demonstrate the effectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 09:16:28 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wen", "Zhihao", ""], ["Fang", "Yuan", ""], ["Liu", "Zemin", ""]]}, {"id": "2105.06741", "submitter": "Amina Boubendir", "authors": "Jose Jurandir Alves Esteves, Amina Boubendir, Fabrice Guillemin, and\n  Pierre Sens", "title": "A Heuristically Assisted Deep Reinforcement Learning Approach for\n  Network Slice Placement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Slice placement with the problem of allocation of resources from a\nvirtualized substrate network is an optimization problem which can be\nformulated as a multiobjective Integer Linear Programming (ILP) problem.\nHowever, to cope with the complexity of such a continuous task and seeking for\noptimality and automation, the use of Machine Learning (ML) techniques appear\nas a promising approach. We introduce a hybrid placement solution based on Deep\nReinforcement Learning (DRL) and a dedicated optimization heuristic based on\nthe Power of Two Choices principle. The DRL algorithm uses the so-called\nAsynchronous Advantage Actor Critic (A3C) algorithm for fast learning, and\nGraph Convolutional Networks (GCN) to automate feature extraction from the\nphysical substrate network. The proposed Heuristically-Assisted DRL (HA-DRL)\nallows to accelerate the learning process and gain in resource usage when\ncompared against other state-of-the-art approaches as the evaluation results\nevidence.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:04:17 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Esteves", "Jose Jurandir Alves", ""], ["Boubendir", "Amina", ""], ["Guillemin", "Fabrice", ""], ["Sens", "Pierre", ""]]}, {"id": "2105.06746", "submitter": "Christian Venner{\\o}d", "authors": "Adrian Kj{\\ae}rran and Christian Bakke Venner{\\o}d and Erling Stray\n  Bugge", "title": "Facial Age Estimation using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is a part of a student project in Machine Learning at the\nNorwegian University of Science and Technology. In this paper, a deep\nconvolutional neural network with five convolutional layers and three\nfully-connected layers is presented to estimate the ages of individuals based\non images. The model is in its entirety trained from scratch, where a\ncombination of three different datasets is used as training data. These\ndatasets are the APPA dataset, UTK dataset, and the IMDB dataset. The images\nwere preprocessed using a proprietary face-recognition software. Our model is\nevaluated on both a held-out test set, and on the Adience benchmark. On the\ntest set, our model achieves a categorical accuracy of 52%. On the Adience\nbenchmark, our model proves inferior compared with other leading models, with\nan exact accuray of 30%, and an one-off accuracy of 46%. Furthermore, a script\nwas created, allowing users to estimate their age directly using their web\ncamera. The script, alongside all other code, is located in our GitHub\nrepository: AgeNet.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:09:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Kj\u00e6rran", "Adrian", ""], ["Venner\u00f8d", "Christian Bakke", ""], ["Bugge", "Erling Stray", ""]]}, {"id": "2105.06756", "submitter": "Christian Venner{\\o}d", "authors": "Christian Bakke Venner{\\o}d and Adrian Kj{\\ae}rran and Erling Stray\n  Bugge", "title": "Long Short-term Memory RNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is based on a machine learning project at the Norwegian University\nof Science and Technology, fall 2020. The project was initiated with a\nliterature review on the latest developments within time-series forecasting\nmethods in the scientific community over the past five years. The paper\nsummarizes the essential aspects of this research. Furthermore, in this paper,\nwe introduce an LSTM cell's architecture, and explain how different components\ngo together to alter the cell's memory and predict the output. Also, the paper\nprovides the necessary formulas and foundations to calculate a forward\niteration through an LSTM. Then, the paper refers to some practical\napplications and research that emphasize the strength and weaknesses of LSTMs,\nshown within the time-series domain and the natural language processing (NLP)\ndomain. Finally, alternative statistical methods for time series predictions\nare highlighted, where the paper outline ARIMA and exponential smoothing.\nNevertheless, as LSTMs can be viewed as a complex architecture, the paper\nassumes that the reader has some knowledge of essential machine learning\naspects, such as the multi-layer perceptron, activation functions, overfitting,\nbackpropagation, bias, over- and underfitting, and more.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:34:14 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Venner\u00f8d", "Christian Bakke", ""], ["Kj\u00e6rran", "Adrian", ""], ["Bugge", "Erling Stray", ""]]}, {"id": "2105.06758", "submitter": "Cor Steging", "authors": "Cor Steging, Silja Renooij, Bart Verheij", "title": "Discovering the Rationale of Decisions: Experiments on Aligning Learning\n  and Reasoning", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In AI and law, systems that are designed for decision support should be\nexplainable when pursuing justice. In order for these systems to be fair and\nresponsible, they should make correct decisions and make them using a sound and\ntransparent rationale. In this paper, we introduce a knowledge-driven method\nfor model-agnostic rationale evaluation using dedicated test cases, similar to\nunit-testing in professional software development. We apply this new method in\na set of machine learning experiments aimed at extracting known knowledge\nstructures from artificial datasets from fictional and non-fictional legal\nsettings. We show that our method allows us to analyze the rationale of\nblack-box machine learning systems by assessing which rationale elements are\nlearned or not. Furthermore, we show that the rationale can be adjusted using\ntailor-made training data based on the results of the rationale evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:37:03 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Steging", "Cor", ""], ["Renooij", "Silja", ""], ["Verheij", "Bart", ""]]}, {"id": "2105.06782", "submitter": "Alexey Ignatiev", "authors": "Alexey Ignatiev and Joao Marques-Silva", "title": "SAT-Based Rigorous Explanations for Decision Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision lists (DLs) find a wide range of uses for classification problems in\nMachine Learning (ML), being implemented in a number of ML frameworks. DLs are\noften perceived as interpretable. However, building on recent results for\ndecision trees (DTs), we argue that interpretability is an elusive goal for\nsome DLs. As a result, for some uses of DLs, it will be important to compute\n(rigorous) explanations. Unfortunately, and in clear contrast with the case of\nDTs, this paper shows that computing explanations for DLs is computationally\nhard. Motivated by this result, the paper proposes propositional encodings for\ncomputing abductive explanations (AXps) and contrastive explanations (CXps) of\nDLs. Furthermore, the paper investigates the practical efficiency of a\nMARCO-like approach for enumerating explanations. The experimental results\ndemonstrate that, for DLs used in practical settings, the use of SAT oracles\noffers a very efficient solution, and that complete enumeration of explanations\nis most often feasible.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 12:06:12 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ignatiev", "Alexey", ""], ["Marques-Silva", "Joao", ""]]}, {"id": "2105.06784", "submitter": "Alessandro Ronca", "authors": "Alessandro Ronca and Giuseppe De Giacomo", "title": "Efficient PAC Reinforcement Learning in Regular Decision Processes", "comments": "Extended version of a paper accepted for publication at IJCAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently regular decision processes have been proposed as a well-behaved form\nof non-Markov decision process. Regular decision processes are characterised by\na transition function and a reward function that depend on the whole history,\nthough regularly (as in regular languages). In practice both the transition and\nthe reward functions can be seen as finite transducers. We study reinforcement\nlearning in regular decision processes. Our main contribution is to show that a\nnear-optimal policy can be PAC-learned in polynomial time in a set of\nparameters that describe the underlying decision process. We argue that the\nidentified set of parameters is minimal and it reasonably captures the\ndifficulty of a regular decision process.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 12:08:46 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 10:25:08 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ronca", "Alessandro", ""], ["De Giacomo", "Giuseppe", ""]]}, {"id": "2105.06785", "submitter": "Amin Totounferoush", "authors": "Amin Totounferoush, Axel Schumacher and Miriam Schulte", "title": "Partitioned Deep Learning of Fluid-Structure Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a partitioned neural network-based framework for learning of\nfluid-structure interaction (FSI) problems. We decompose the simulation domain\ninto two smaller sub-domains, i.e., fluid and solid domains, and incorporate an\nindependent neural network for each. A library is used to couple the two\nnetworks which takes care of boundary data communication, data mapping and\nequation coupling. Simulation data are used for training of the both neural\nnetworks. We use a combination of convolutional and recurrent neural networks\n(CNN and RNN) to account for both spatial and temporal connectivity. A\nquasi-Newton method is used to accelerate the FSI coupling convergence. We\nobserve a very good agreement between the results of the presented framework\nand the classical numerical methods for simulation of 1d fluid flow inside an\nelastic tube. This work is a preliminary step for using neural networks to\nspeed-up the FSI coupling convergence by providing an accurate initial guess in\neach time step for classical numerical solvers\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 12:09:03 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Totounferoush", "Amin", ""], ["Schumacher", "Axel", ""], ["Schulte", "Miriam", ""]]}, {"id": "2105.06791", "submitter": "Matthew Watson", "authors": "Matthew Watson (1), Bashar Awwad Shiekh Hasan (1), Noura Al Moubayed\n  (1) ((1) Durham University, Durham, UK)", "title": "Agree to Disagree: When Deep Learning Models With Identical\n  Architectures Produce Distinct Explanations", "comments": "9 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning of neural networks has progressively become more prominent in\nhealthcare with models reaching, or even surpassing, expert accuracy levels.\nHowever, these success stories are tainted by concerning reports on the lack of\nmodel transparency and bias against some medical conditions or patients'\nsub-groups. Explainable methods are considered the gateway to alleviate many of\nthese concerns. In this study we demonstrate that the generated explanations\nare volatile to changes in model training that are perpendicular to the\nclassification task and model structure. This raises further questions about\ntrust in deep learning models for healthcare. Mainly, whether the models\ncapture underlying causal links in the data or just rely on spurious\ncorrelations that are made visible via explanation methods. We demonstrate that\nthe output of explainability methods on deep neural networks can vary\nsignificantly by changes of hyper-parameters, such as the random seed or how\nthe training set is shuffled. We introduce a measure of explanation consistency\nwhich we use to highlight the identified problems on the MIMIC-CXR dataset. We\nfind explanations of identical models but with different training setups have a\nlow consistency: $\\approx$ 33% on average. On the contrary, kernel methods are\nrobust against any orthogonal changes, with explanation consistency at 94%. We\nconclude that current trends in model explanation are not sufficient to\nmitigate the risks of deploying models in real life healthcare applications.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 12:16:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Watson", "Matthew", "", "Durham University, Durham, UK"], ["Hasan", "Bashar Awwad Shiekh", "", "Durham University, Durham, UK"], ["Moubayed", "Noura Al", "", "Durham University, Durham, UK"]]}, {"id": "2105.06811", "submitter": "Gianluca Truda", "authors": "Gianluca Truda", "title": "Quantified Sleep: Machine learning techniques for observational n-of-1\n  studies", "comments": "Source code: https://github.com/gianlucatruda/quantified-sleep", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper applies statistical learning techniques to an observational\nQuantified-Self (QS) study to build a descriptive model of sleep quality. A\ntotal of 472 days of my sleep data was collected with an Oura ring and combined\nwith lifestyle, environmental, and psychological data. Such n-of-1 QS projects\npose a number of challenges: heterogeneous data sources; missing values; high\ndimensionality; dynamic feedback loops; human biases. This paper directly\naddresses these challenges with an end-to-end QS pipeline that produces robust\ndescriptive models. Sleep quality is one of the most difficult modelling\ntargets in QS research, due to high noise and a large number of\nweakly-contributing factors. Sleep quality was selected so that approaches from\nthis paper would generalise to most other n-of-1 QS projects. Techniques are\npresented for combining and engineering features for the different classes of\ndata types, sample frequencies, and schema - including event logs, weather, and\ngeo-spatial data. Statistical analyses for outliers, normality,\n(auto)correlation, stationarity, and missing data are detailed, along with a\nproposed method for hierarchical clustering to identify correlated groups of\nfeatures. The missing data was overcome using a combination of knowledge-based\nand statistical techniques, including several multivariate imputation\nalgorithms. \"Markov unfolding\" is presented for collapsing the time series into\na collection of independent observations, whilst incorporating historical\ninformation. The final model was interpreted in two ways: by inspecting the\ninternal $\\beta$-parameters, and using the SHAP framework. These two\ninterpretation techniques were combined to produce a list of the 16\nmost-predictive features, demonstrating that an observational study can greatly\nnarrow down the number of features that need to be considered when designing\ninterventional QS studies.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:13:17 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Truda", "Gianluca", ""]]}, {"id": "2105.06813", "submitter": "Guilherme Moraes Rosa", "authors": "Guilherme Moraes Rosa, Luiz Henrique Bonifacio, Leandro Rodrigues de\n  Souza, Roberto Lotufo and Rodrigo Nogueira", "title": "A cost-benefit analysis of cross-lingual transfer methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective method for cross-lingual transfer is to fine-tune a bilingual or\nmultilingual model on a supervised dataset in one language and evaluating it on\nanother language in a zero-shot manner. Translating examples at training time\nor inference time are also viable alternatives. However, there are costs\nassociated with these methods that are rarely addressed in the literature. In\nthis work, we analyze cross-lingual methods in terms of their effectiveness\n(e.g., accuracy), development and deployment costs, as well as their latencies\nat inference time. Our experiments on three tasks indicate that the best\ncross-lingual method is highly task-dependent. Finally, by combining zero-shot\nand translation methods, we achieve the state-of-the-art in two of the three\ndatasets used in this work. Based on these results, we question the need for\nmanually labeled training data in a target language. Code, models and\ntranslated datasets are available at\nhttps://github.com/unicamp-dl/cross-lingual-analysis\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:21:12 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 14:09:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Rosa", "Guilherme Moraes", ""], ["Bonifacio", "Luiz Henrique", ""], ["de Souza", "Leandro Rodrigues", ""], ["Lotufo", "Roberto", ""], ["Nogueira", "Rodrigo", ""]]}, {"id": "2105.06825", "submitter": "Pablo Gil Dr.", "authors": "Victor De Gea and Santiago T. Puente and Pablo Gil", "title": "Domestic waste detection and grasping points for robotic picking up", "comments": "2 pages, 3 figures, accepted as poster for presentation in ICRA 2021\n  Workshop: Emerging paradigms for robotic manipulation: from the lab to the\n  productive world", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents an AI system applied to location and robotic grasping.\nExperimental setup is based on a parameter study to train a deep-learning\nnetwork based on Mask-RCNN to perform waste location in indoor and outdoor\nenvironment, using five different classes and generating a new waste dataset.\nInitially the AI system obtain the RGBD data of the environment, followed by\nthe detection of objects using the neural network. Later, the 3D object shape\nis computed using the network result and the depth channel. Finally, the shape\nis used to compute grasping for a robot arm with a two-finger gripper. The\nobjective is to classify the waste in groups to improve a recycling strategy.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:37:33 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["De Gea", "Victor", ""], ["Puente", "Santiago T.", ""], ["Gil", "Pablo", ""]]}, {"id": "2105.06834", "submitter": "Dean Foster", "authors": "Dean P. Foster and Robert A. Stine", "title": "Threshold Martingales and the Evolution of Forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a martingale that characterizes two properties of\nevolving forecast distributions. Ideal forecasts of a future event behave as\nmartingales, sequen- tially updating the forecast to leverage the available\ninformation as the future event approaches. The threshold martingale introduced\nhere measures the proportion of the forecast distribution lying below a\nthreshold. In addition to being calibrated, a threshold martingale has\nquadratic variation that accumulates to a total determined by a quantile of the\ninitial forecast distribution. Deviations from calibration or to- tal\nvolatility signal problems in the underlying model. Calibration adjustments are\nwell-known, and we augment these by introducing a martingale filter that\nimproves volatility while guaranteeing smaller mean squared error. Thus,\npost-processing can rectify problems with calibration and volatility without\nrevisiting the original forecast- ing model. We apply threshold martingales\nfirst to forecasts from simulated models and then to models that predict the\nwinner in professional basketball games.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:49:55 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Foster", "Dean P.", ""], ["Stine", "Robert A.", ""]]}, {"id": "2105.06844", "submitter": "Bernd Accou", "authors": "Bernd Accou, Mohammad Jalilpour Monesi, Hugo Van hamme and Tom\n  Francart", "title": "Predicting speech intelligibility from EEG using a dilated convolutional\n  network", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Objective: Currently, only behavioral speech understanding tests are\navailable, which require active participation of the person. As this is\ninfeasible for certain populations, an objective measure of speech\nintelligibility is required. Recently, brain imaging data has been used to\nestablish a relationship between stimulus and brain response. Linear models\nhave been successfully linked to speech intelligibility but require per-subject\ntraining. We present a deep-learning-based model incorporating dilated\nconvolutions that can be used to predict speech intelligibility without\nsubject-specific (re)training. Methods: We evaluated the performance of the\nmodel as a function of input segment length, EEG frequency band and receptive\nfield size while comparing it to a baseline model. Next, we evaluated\nperformance on held-out data and finetuning. Finally, we established a link\nbetween the accuracy of our model and the state-of-the-art behavioral MATRIX\ntest. Results: The model significantly outperformed the baseline for every\ninput segment length (p$\\leq10^{-9}$), for all EEG frequency bands except the\ntheta band (p$\\leq0.001$) and for receptive field sizes larger than 125 ms\n(p$\\leq0.05$). Additionally, finetuning significantly increased the accuracy\n(p$\\leq0.05$) on a held-out dataset. Finally, a significant correlation\n(r=0.59, p=0.0154) was found between the speech reception threshold estimated\nusing the behavioral MATRIX test and our objective method. Conclusion: Our\nproposed dilated convolutional model can be used as a proxy for speech\nintelligibility. Significance: Our method is the first to predict the speech\nreception threshold from EEG for unseen subjects, contributing to objective\nmeasures of speech intelligibility.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:12:52 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 11:20:56 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Accou", "Bernd", ""], ["Monesi", "Mohammad Jalilpour", ""], ["Van hamme", "Hugo", ""], ["Francart", "Tom", ""]]}, {"id": "2105.06849", "submitter": "Ido Ben-Shaul", "authors": "Ido Ben-Shaul and Shai Dekel", "title": "Sparsity-Probe: Analysis tool for Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a probe for the analysis of deep learning architectures that is\nbased on machine learning and approximation theoretical principles. Given a\ndeep learning architecture and a training set, during or after training, the\nSparsity Probe allows to analyze the performance of intermediate layers by\nquantifying the geometrical features of representations of the training set. We\nshow how the Sparsity Probe enables measuring the contribution of adding depth\nto a given architecture, to detect under-performing layers, etc., all this\nwithout any auxiliary test data set.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:24:20 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ben-Shaul", "Ido", ""], ["Dekel", "Shai", ""]]}, {"id": "2105.06868", "submitter": "Vincent Fortuin", "authors": "Vincent Fortuin", "title": "Priors in Bayesian Deep Learning: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the choice of prior is one of the most critical parts of the Bayesian\ninference workflow, recent Bayesian deep learning models have often fallen back\non vague priors, such as standard Gaussians. In this review, we highlight the\nimportance of prior choices for Bayesian deep learning and present an overview\nof different priors that have been proposed for (deep) Gaussian processes,\nvariational autoencoders, and Bayesian neural networks. We also outline\ndifferent methods of learning priors for these models from data. We hope to\nmotivate practitioners in Bayesian deep learning to think more carefully about\nthe prior specification for their models and to provide them with some\ninspiration in this regard.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:53:30 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 17:48:55 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Fortuin", "Vincent", ""]]}, {"id": "2105.06869", "submitter": "Ali Reza Ghavamipour", "authors": "Ali Reza Ghavamipour, Fatih Turkmen, Xiaoqian Jian", "title": "Privacy-preserving Logistic Regression with Secret Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression (LR) is a widely used classification method for modeling\nbinary outcomes in many medical data classification tasks. Research that\ncollects and combines datasets from various data custodians and jurisdictions\ncan excessively benefit from the increased statistical power to support their\nanalyzing goals. However, combining data from these various sources creates\nsignificant privacy concerns that need to be addressed. In this paper, we\nproposed secret sharing-based privacy-preserving logistic regression protocols\nusing the Newton-Raphson method. Our proposed approaches are based on secure\nMulti-Party Computation (MPC) with different security settings to analyze data\nowned by several data holders. We conducted experiments on both synthetic data\nand real-world datasets and compared the efficiency and accuracy of them with\nthose of an ordinary logistic regression model. Experimental results\ndemonstrate that the proposed protocols are highly efficient and accurate. This\nstudy introduces iterative algorithms to simplify the federated training a\nlogistic regression model in a privacy-preserving manner. Our implementation\nresults show that our improved method can handle large datasets used in\nsecurely training a logistic regression from multiple sources.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:53:50 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ghavamipour", "Ali Reza", ""], ["Turkmen", "Fatih", ""], ["Jian", "Xiaoqian", ""]]}, {"id": "2105.06887", "submitter": "Qing Ma", "authors": "Qing Ma, Jae Chul Koh, WonSook Lee", "title": "A Frequency Domain Constraint for Synthetic X-ray Image Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic X-ray images can be helpful for image guiding systems and VR\nsimulations. However, it is difficult to produce high-quality arbitrary view\nsynthetic X-ray images in real-time due to limited CT scanning resolution, high\ncomputation resource demand or algorithm complexity. Our goal is to generate\nhigh-resolution synthetic X-ray images in real-time by upsampling\nlow-resolution im-ages. Reference-based Super Resolution (RefSR) has been well\nstudied in recent years and has been proven to be more powerful than\ntraditional Single Image Su-per-Resolution (SISR). RefSR can produce fine\ndetails by utilizing the reference image but it still inevitably generates some\nartifacts and noise. In this paper, we propose texture transformer\nsuper-resolution with frequency domain (TTSR-FD). We introduce frequency domain\nloss as a constraint to further improve the quality of the RefSR results with\nfine details and without obvious artifacts. This makes a real-time synthetic\nX-ray image-guided procedure VR simulation system possible. To the best of our\nknowledge, this is the first paper utilizing the frequency domain as part of\nthe loss functions in the field of super-resolution. We evaluated TTSR-FD on\nour synthetic X-ray image dataset and achieved state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:17:27 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ma", "Qing", ""], ["Koh", "Jae Chul", ""], ["Lee", "WonSook", ""]]}, {"id": "2105.06899", "submitter": "H{\\aa}rek Haugerud", "authors": "Eirik Molde B{\\aa}rli, Anis Yazidi, Enrique Herrera Viedma, H{\\aa}rek\n  Haugerud", "title": "DoS and DDoS Mitigation Using Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DoS and DDoS attacks have been growing in size and number over the last\ndecade and existing solutions to mitigate these attacks are in general\ninefficient. Compared to other types of malicious cyber attacks, DoS and DDoS\nattacks are particularly more challenging to combat. With their ability to mask\nthemselves as legitimate traffic, developing methods to detect these types of\nattacks on a packet or flow level, has proven to be a difficult task. In this\npaper, we explore the potential of Variational Autoencoders to serve as a\ncomponent within an intelligent security solution that differentiates between\nnormal and malicious traffic. Two methods based on the ability of Variational\nAutoencoders to learn latent representations from network traffic flows are\nproposed. The first method resorts to a classifier based on the latent\nencodings obtained from Variational Autoencoders learned from traffic traces.\nThe second method is rather an anomaly detection method where the Variational\nAutoencoder is used to learn the abstract feature representations of\nexclusively legitimate traffic. Then anomalies are filtered out by relying on\nthe reconstruction loss of the Variational Autoencoder.\n  Both of the proposed methods have been thoroughly tested on two separate\ndatasets with a similar feature space. The results show that both methods are\npromising, with a slight superiority of the classifier based method over the\nanomaly based one.\n  %that the first method is able to successfully detect individual traffic\nflows with high precision on the training and validation data, slightly less\nsuccessfully on the test data. For the second method, the Variational\nAutoencoder will require further adjustments to be able to sufficiently filter\nout anomalies from network traffic flows.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:38:40 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["B\u00e5rli", "Eirik Molde", ""], ["Yazidi", "Anis", ""], ["Viedma", "Enrique Herrera", ""], ["Haugerud", "H\u00e5rek", ""]]}, {"id": "2105.06903", "submitter": "Weipeng Huang", "authors": "Weipeng Huang, Tin Lok James Ng, Nishma Laitonjam, Neil J. Hurley", "title": "Posterior Regularisation on Bayesian Hierarchical Mixture Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a recent inferential framework, named posterior regularisation, on\nthe Bayesian hierarchical mixture clustering (BHMC) model. This framework\nfacilitates a simple way to impose extra constraints on a Bayesian model to\novercome some weakness of the original model. It narrows the search space of\nthe parameters of the Bayesian model through a formalism that imposes certain\nconstraints on the features of the found solutions. In this paper, in order to\nenhance the separation of clusters, we apply posterior regularisation to impose\nmax-margin constraints on the nodes at every level of the hierarchy. This paper\nshows how the framework integrates with BHMC and achieves the expected\nimprovements over the original Bayesian model.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:41:15 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 17:03:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Huang", "Weipeng", ""], ["Ng", "Tin Lok James", ""], ["Laitonjam", "Nishma", ""], ["Hurley", "Neil J.", ""]]}, {"id": "2105.06907", "submitter": "Kiana Farhadyar", "authors": "Kiana Farhadyar, Federico Bonofiglio, Daniela Zoeller and Harald\n  Binder", "title": "Adapting deep generative approaches for getting synthetic data with\n  realistic marginal distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data generation is of great interest in diverse applications, such\nas for privacy protection. Deep generative models, such as variational\nautoencoders (VAEs), are a popular approach for creating such synthetic\ndatasets from original data. Despite the success of VAEs, there are limitations\nwhen it comes to the bimodal and skewed marginal distributions. These deviate\nfrom the unimodal symmetric distributions that are encouraged by the normality\nassumption typically used for the latent representations in VAEs. While there\nare extensions that assume other distributions for the latent space, this does\nnot generally increase flexibility for data with many different distributions.\nTherefore, we propose a novel method, pre-transformation variational\nautoencoders (PTVAEs), to specifically address bimodal and skewed data, by\nemploying pre-transformations at the level of original variables. Two types of\ntransformations are used to bring the data close to a normal distribution by a\nseparate parameter optimization for each variable in a dataset. We compare the\nperformance of our method with other state-of-the-art methods for synthetic\ndata generation. In addition to the visual comparison, we use a utility\nmeasurement for a quantitative evaluation. The results show that the PTVAE\napproach can outperform others in both bimodal and skewed data generation.\nFurthermore, the simplicity of the approach makes it usable in combination with\nother extensions of VAE.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:47:20 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Farhadyar", "Kiana", ""], ["Bonofiglio", "Federico", ""], ["Zoeller", "Daniela", ""], ["Binder", "Harald", ""]]}, {"id": "2105.06923", "submitter": "Wei Lu", "authors": "John Moon, Wei D. Lu (University of Michigan)", "title": "Hierarchical Architectures in Reservoir Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reservoir computing (RC) offers efficient temporal data processing with a low\ntraining cost by separating recurrent neural networks into a fixed network with\nrecurrent connections and a trainable linear network. The quality of the fixed\nnetwork, called reservoir, is the most important factor that determines the\nperformance of the RC system. In this paper, we investigate the influence of\nthe hierarchical reservoir structure on the properties of the reservoir and the\nperformance of the RC system. Analogous to deep neural networks, stacking\nsub-reservoirs in series is an efficient way to enhance the nonlinearity of\ndata transformation to high-dimensional space and expand the diversity of\ntemporal information captured by the reservoir. These deep reservoir systems\noffer better performance when compared to simply increasing the size of the\nreservoir or the number of sub-reservoirs. Low frequency components are mainly\ncaptured by the sub-reservoirs in later stage of the deep reservoir structure,\nsimilar to observations that more abstract information can be extracted by\nlayers in the late stage of deep neural networks. When the total size of the\nreservoir is fixed, tradeoff between the number of sub-reservoirs and the size\nof each sub-reservoir needs to be carefully considered, due to the degraded\nability of individual sub-reservoirs at small sizes. Improved performance of\nthe deep reservoir structure alleviates the difficulty of implementing the RC\nsystem on hardware systems.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 16:11:35 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Moon", "John", "", "University of Michigan"], ["Lu", "Wei D.", "", "University of Michigan"]]}, {"id": "2105.06934", "submitter": "Siva Shanmugam", "authors": "Siva Shanmugam, Sheetal Kalyani", "title": "Deep learned SVT: Unrolling singular value thresholding to obtain better\n  MSE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Affine rank minimization problem is the generalized version of low rank\nmatrix completion problem where linear combinations of the entries of a low\nrank matrix are observed and the matrix is estimated from these measurements.\nWe propose a trainable deep neural network by unrolling a popular iterative\nalgorithm called the singular value thresholding (SVT) algorithm to perform\nthis generalized matrix completion which we call Learned SVT (LSVT). We show\nthat our proposed LSVT with fixed layers (say T) reconstructs the matrix with\nlesser mean squared error (MSE) compared with that incurred by SVT with fixed\n(same T) number of iterations and our method is much more robust to the\nparameters which need to be carefully chosen in SVT algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 16:23:29 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Shanmugam", "Siva", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "2105.06956", "submitter": "Sukriti Verma", "authors": "Sukriti Verma, Nikaash Puri, Piyush Gupta, Balaji Krishnamurthy", "title": "Information-theoretic Evolution of Model Agnostic Global Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining the behavior of black box machine learning models through human\ninterpretable rules is an important research area. Recent work has focused on\nexplaining model behavior locally i.e. for specific predictions as well as\nglobally across the fields of vision, natural language, reinforcement learning\nand data science. We present a novel model-agnostic approach that derives rules\nto globally explain the behavior of classification models trained on numerical\nand/or categorical data. Our approach builds on top of existing local model\nexplanation methods to extract conditions important for explaining model\nbehavior for specific instances followed by an evolutionary algorithm that\noptimizes an information theory based fitness function to construct rules that\nexplain global model behavior. We show how our approach outperforms existing\napproaches on a variety of datasets. Further, we introduce a parameter to\nevaluate the quality of interpretation under the scenario of distributional\nshift. This parameter evaluates how well the interpretation can predict model\nbehavior for previously unseen data distributions. We show how existing\napproaches for interpreting models globally lack distributional robustness.\nFinally, we show how the quality of the interpretation can be improved under\nthe scenario of distributional shift by adding out of distribution samples to\nthe dataset used to learn the interpretation and thereby, increase robustness.\nAll of the datasets used in our paper are open and publicly available. Our\napproach has been deployed in a leading digital marketing suite of products.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 16:52:16 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Verma", "Sukriti", ""], ["Puri", "Nikaash", ""], ["Gupta", "Piyush", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "2105.06958", "submitter": "Reza Sameni", "authors": "Reza Sameni, Christian Jutten", "title": "A Hypothesis Testing Approach to Nonstationary Source Separation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of nonstationary signals from blind and semi-blind\nmultivariate observations is a recurrent problem. Numerous algorithms have been\ndeveloped for this problem, which are based on the exact or approximate joint\ndiagonalization of second or higher order cumulant matrices/tensors of\nmultichannel data. While a great body of research has been dedicated to joint\ndiagonalization algorithms, the selection of the diagonalized matrix/tensor set\nremains highly problem-specific. Herein, various methods for nonstationarity\nidentification are reviewed and a new general framework based on hypothesis\ntesting is proposed, which results in a classification/clustering perspective\nto semi-blind source separation of nonstationary components. The proposed\nmethod is applied to noninvasive fetal ECG extraction, as case study.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 16:58:55 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 17:22:56 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sameni", "Reza", ""], ["Jutten", "Christian", ""]]}, {"id": "2105.06960", "submitter": "Ming Liang Ang", "authors": "Ming Liang Ang, Eloise Y. Y. Lim, Joel Q. L. Chang", "title": "Thompson Sampling for Gaussian Entropic Risk Bandits", "comments": "arXiv admin note: text overlap with arXiv:2011.08046", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The multi-armed bandit (MAB) problem is a ubiquitous decision-making problem\nthat exemplifies exploration-exploitation tradeoff. Standard formulations\nexclude risk in decision making. Risknotably complicates the basic\nreward-maximising objectives, in part because there is no universally agreed\ndefinition of it. In this paper, we consider an entropic risk (ER) measure and\nexplore the performance of a Thompson sampling-based algorithm ERTS under this\nrisk measure by providing regret bounds for ERTS and corresponding instance\ndependent lower bounds.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:01:02 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ang", "Ming Liang", ""], ["Lim", "Eloise Y. Y.", ""], ["Chang", "Joel Q. L.", ""]]}, {"id": "2105.06964", "submitter": "Vincent Fortuin", "authors": "Vincent Fortuin, Adri\\`a Garriga-Alonso, Mark van der Wilk, Laurence\n  Aitchison", "title": "BNNpriors: A library for Bayesian neural network inference with\n  different prior distributions", "comments": "Accepted for publication at Software Impacts", "journal-ref": null, "doi": "10.1016/j.simpa.2021.100079", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian neural networks have shown great promise in many applications where\ncalibrated uncertainty estimates are crucial and can often also lead to a\nhigher predictive performance. However, it remains challenging to choose a good\nprior distribution over their weights. While isotropic Gaussian priors are\noften chosen in practice due to their simplicity, they do not reflect our true\nprior beliefs well and can lead to suboptimal performance. Our new library,\nBNNpriors, enables state-of-the-art Markov Chain Monte Carlo inference on\nBayesian neural networks with a wide range of predefined priors, including\nheavy-tailed ones, hierarchical ones, and mixture priors. Moreover, it follows\na modular approach that eases the design and implementation of new custom\npriors. It has facilitated foundational discoveries on the nature of the cold\nposterior effect in Bayesian neural networks and will hopefully catalyze future\nresearch as well as practical applications in this area.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:11:04 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Fortuin", "Vincent", ""], ["Garriga-Alonso", "Adri\u00e0", ""], ["van der Wilk", "Mark", ""], ["Aitchison", "Laurence", ""]]}, {"id": "2105.06977", "submitter": "Kayo Yin", "authors": "Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, Andr\\'e\n  F. T. Martins, Graham Neubig", "title": "Do Context-Aware Translation Models Pay the Right Attention?", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware machine translation models are designed to leverage contextual\ninformation, but often fail to do so. As a result, they inaccurately\ndisambiguate pronouns and polysemous words that require context for resolution.\nIn this paper, we ask several questions: What contexts do human translators use\nto resolve ambiguous words? Are models paying large amounts of attention to the\nsame context? What if we explicitly train them to do so? To answer these\nquestions, we introduce SCAT (Supporting Context for Ambiguous Translations), a\nnew English-French dataset comprising supporting context words for 14K\ntranslations that professional translators found useful for pronoun\ndisambiguation. Using SCAT, we perform an in-depth analysis of the context used\nto disambiguate, examining positional and lexical characteristics of the\nsupporting words. Furthermore, we measure the degree of alignment between the\nmodel's attention scores and the supporting context from SCAT, and apply a\nguided attention strategy to encourage agreement between the two.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:32:24 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 16:26:23 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Yin", "Kayo", ""], ["Fernandes", "Patrick", ""], ["Pruthi", "Danish", ""], ["Chaudhary", "Aditi", ""], ["Martins", "Andr\u00e9 F. T.", ""], ["Neubig", "Graham", ""]]}, {"id": "2105.06987", "submitter": "Andrey Malinin Dr.", "authors": "Max Ryabinin, Andrey Malinin, Mark Gales", "title": "Scaling Ensemble Distribution Distillation to Many Classes with Proxy\n  Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of machine learning models yield improved system performance as\nwell as robust and interpretable uncertainty estimates; however, their\ninference costs may often be prohibitively high. \\emph{Ensemble Distribution\nDistillation} is an approach that allows a single model to efficiently capture\nboth the predictive performance and uncertainty estimates of an ensemble. For\nclassification, this is achieved by training a Dirichlet distribution over the\nensemble members' output distributions via the maximum likelihood criterion.\nAlthough theoretically principled, this criterion exhibits poor convergence\nwhen applied to large-scale tasks where the number of classes is very high. In\nour work, we analyze this effect and show that the Dirichlet log-likelihood\ncriterion classes with low probability induce larger gradients than\nhigh-probability classes. This forces the model to focus on the distribution of\nthe ensemble tail-class probabilities. We propose a new training objective that\nminimizes the reverse KL-divergence to a \\emph{Proxy-Dirichlet} target derived\nfrom the ensemble. This loss resolves the gradient issues of Ensemble\nDistribution Distillation, as we demonstrate both theoretically and empirically\non the ImageNet and WMT17 En-De datasets containing 1000 and 40,000 classes,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:50:14 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ryabinin", "Max", ""], ["Malinin", "Andrey", ""], ["Gales", "Mark", ""]]}, {"id": "2105.06998", "submitter": "Elisa Ferrari", "authors": "Elisa Ferrari, Luna Gargani, Greta Barbieri, Lorenzo Ghiadoni,\n  Francesco Faita, Davide Bacciu", "title": "A causal learning framework for the analysis and interpretation of\n  COVID-19 clinical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a workflow for clinical data analysis that relies on Bayesian\nStructure Learning (BSL), an unsupervised learning approach, robust to noise\nand biases, that allows to incorporate prior medical knowledge into the\nlearning process and that provides explainable results in the form of a graph\nshowing the causal connections among the analyzed features. The workflow\nconsists in a multi-step approach that goes from identifying the main causes of\npatient's outcome through BSL, to the realization of a tool suitable for\nclinical practice, based on a Binary Decision Tree (BDT), to recognize patients\nat high-risk with information available already at hospital admission time. We\nevaluate our approach on a feature-rich COVID-19 dataset, showing that the\nproposed framework provides a schematic overview of the multi-factorial\nprocesses that jointly contribute to the outcome. We discuss how these\ncomputational findings are confirmed by current understanding of the COVID-19\npathogenesis. Further, our approach yields to a highly interpretable tool\ncorrectly predicting the outcome of 85% of subjects based exclusively on 3\nfeatures: age, a previous history of chronic obstructive pulmonary disease and\nthe PaO2/FiO2 ratio at the time of arrival to the hospital. The inclusion of\nadditional information from 4 routine blood tests (Creatinine, Glucose, pO2 and\nSodium) increases predictive accuracy to 94.5%.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:58:18 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ferrari", "Elisa", ""], ["Gargani", "Luna", ""], ["Barbieri", "Greta", ""], ["Ghiadoni", "Lorenzo", ""], ["Faita", "Francesco", ""], ["Bacciu", "Davide", ""]]}, {"id": "2105.07024", "submitter": "Kelsey Maass", "authors": "Kelsey Maass and Aleksandr Aravkin and Minsun Kim", "title": "A feasibility study of a hyperparameter tuning approach to automated\n  inverse planning in radiotherapy", "comments": "27 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radiotherapy inverse planning requires treatment planners to modify multiple\nparameters in the objective function to produce clinically acceptable plans.\nDue to manual steps in this process, plan quality can vary widely depending on\nplanning time available and planner's skills. The purpose of this study is to\nautomate the inverse planning process to reduce active planning time while\nmaintaining plan quality. We propose a hyperparameter tuning approach for\nautomated inverse planning, where a treatment plan utility is maximized with\nrespect to the limit dose parameters and weights of each organ-at-risk (OAR)\nobjective. Using 6 patient cases, we investigated the impact of the choice of\ndose parameters, random and Bayesian search methods, and utility function form\non planning time and plan quality. For given parameters, the plan was optimized\nin RayStation, using the scripting interface to obtain the dose distributions\ndeliverable. We normalized all plans to have the same target coverage and\ncompared the OAR dose metrics in the automatically generated plans with those\nin the manually generated clinical plans. Using 100 samples was found to\nproduce satisfactory plan quality, and the average planning time was 2.3 hours.\nThe OAR doses in the automatically generated plans were lower than the clinical\nplans by up to 76.8%. When the OAR doses were larger than the clinical plans,\nthey were still between 0.57% above and 98.9% below the limit doses, indicating\nthey are clinically acceptable. For a challenging case, a dimensionality\nreduction strategy produced a 92.9% higher utility using only 38.5% of the time\nneeded to optimize over the original problem. This study demonstrates our\nhyperparameter tuning framework for automated inverse planning can\nsignificantly reduce the treatment planner's planning time with plan quality\nthat is similar to or better than manually generated plans.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:37:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Maass", "Kelsey", ""], ["Aravkin", "Aleksandr", ""], ["Kim", "Minsun", ""]]}, {"id": "2105.07026", "submitter": "Amin Asadi", "authors": "Amin Asadi, Sarah Nurre Pinkley", "title": "A Monotone Approximate Dynamic Programming Approach for the Stochastic\n  Scheduling, Allocation, and Inventory Replenishment Problem: Applications to\n  Drone and Electric Vehicle Battery Swap Stations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing interest in using electric vehicles (EVs) and drones for\nmany applications. However, battery-oriented issues, including range anxiety\nand battery degradation, impede adoption. Battery swap stations are one\nalternative to reduce these concerns that allow the swap of depleted for full\nbatteries in minutes. We consider the problem of deriving actions at a battery\nswap station when explicitly considering the uncertain arrival of swap demand,\nbattery degradation, and replacement. We model the operations at a battery swap\nstation using a finite horizon Markov Decision Process model for the stochastic\nscheduling, allocation, and inventory replenishment problem (SAIRP), which\ndetermines when and how many batteries are charged, discharged, and replaced\nover time. We present theoretical proofs for the monotonicity of the value\nfunction and monotone structure of an optimal policy for special SAIRP cases.\nDue to the curses of dimensionality, we develop a new monotone approximate\ndynamic programming (ADP) method, which intelligently initializes a value\nfunction approximation using regression. In computational tests, we demonstrate\nthe superior performance of the new regression-based monotone ADP method as\ncompared to exact methods and other monotone ADP methods. Further, with the\ntests, we deduce policy insights for drone swap stations.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:39:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Asadi", "Amin", ""], ["Pinkley", "Sarah Nurre", ""]]}, {"id": "2105.07029", "submitter": "Eleni Triantafillou", "authors": "Eleni Triantafillou, Hugo Larochelle, Richard Zemel and Vincent\n  Dumoulin", "title": "Learning a Universal Template for Few-shot Dataset Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot dataset generalization is a challenging variant of the well-studied\nfew-shot classification problem where a diverse training set of several\ndatasets is given, for the purpose of training an adaptable model that can then\nlearn classes from new datasets using only a few examples. To this end, we\npropose to utilize the diverse training set to construct a universal template:\na partial model that can define a wide array of dataset-specialized models, by\nplugging in appropriate components. For each new few-shot classification\nproblem, our approach therefore only requires inferring a small number of\nparameters to insert into the universal template. We design a separate network\nthat produces an initialization of those parameters for each given task, and we\nthen fine-tune its proposed initialization via a few steps of gradient descent.\nOur approach is more parameter-efficient, scalable and adaptable compared to\nprevious methods, and achieves the state-of-the-art on the challenging\nMeta-Dataset benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:46:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 15:31:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Triantafillou", "Eleni", ""], ["Larochelle", "Hugo", ""], ["Zemel", "Richard", ""], ["Dumoulin", "Vincent", ""]]}, {"id": "2105.07033", "submitter": "Mohammad Nokhbeh Zaeem", "authors": "Mohammad Nokhbeh Zaeem and Majid Komeili", "title": "Cause and Effect: Concept-based Explanation of Neural Networks", "comments": "14 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scenarios, human decisions are explained based on some high-level\nconcepts. In this work, we take a step in the interpretability of neural\nnetworks by examining their internal representation or neuron's activations\nagainst concepts. A concept is characterized by a set of samples that have\nspecific features in common. We propose a framework to check the existence of a\ncausal relationship between a concept (or its negation) and task classes. While\nthe previous methods focus on the importance of a concept to a task class, we\ngo further and introduce four measures to quantitatively determine the order of\ncausality. Through experiments, we demonstrate the effectiveness of the\nproposed method in explaining the relationship between a concept and the\npredictive behaviour of a neural network.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:54:17 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zaeem", "Mohammad Nokhbeh", ""], ["Komeili", "Majid", ""]]}, {"id": "2105.07043", "submitter": "Bob de Ruiter", "authors": "Bob de Ruiter", "title": "Post-processing Multi-Model Medium-Term Precipitation Forecasts Using\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.ao-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The goal of this study was to improve the post-processing of precipitation\nforecasts using convolutional neural networks (CNNs). Instead of\npost-processing forecasts on a per-pixel basis, as is usually done when\nemploying machine learning in meteorological post-processing, input forecast\nimages were combined and transformed into probabilistic output forecast images\nusing fully convolutional neural networks. CNNs did not outperform regularized\nlogistic regression. Additionally, an ablation analysis was performed.\nCombining input forecasts from a global low-resolution weather model and a\nregional high-resolution weather model improved performance over either one.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 19:30:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["de Ruiter", "Bob", ""]]}, {"id": "2105.07059", "submitter": "Chenyu You", "authors": "Chenyu You, Ruihan Zhao, Lawrence Staib, James S. Duncan", "title": "Momentum Contrastive Voxel-wise Representation Learning for\n  Semi-supervised Volumetric Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, manually annotating\nmedical data is often laborious, and most existing learning-based approaches\nfail to accurately delineate object boundaries without effective geometric\nconstraints. Contrastive learning, a sub-area of self-supervised learning, has\nrecently been noted as a promising direction in multiple application fields. In\nthis work, we present a novel Contrastive Voxel-wise Representation Learning\n(CVRL) method with geometric constraints to learn global-local visual\nrepresentations for volumetric medical image segmentation with limited\nannotations. Our framework can effectively learn global and local features by\ncapturing 3D spatial context and rich anatomical information. Specifically, we\nintroduce a voxel-to-volume contrastive algorithm to learn global information\nfrom 3D images, and propose to perform local voxel-to-voxel contrast to\nexplicitly make use of local cues in the embedding space. Moreover, we\nintegrate an elastic interaction-based active contour model as a geometric\nregularization term to enable fast and reliable object delineations in an\nend-to-end learning manner. Results on the Atrial Segmentation Challenge\ndataset demonstrate superiority of our proposed scheme, especially in a setting\nwith a very limited number of annotated data.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 20:27:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["You", "Chenyu", ""], ["Zhao", "Ruihan", ""], ["Staib", "Lawrence", ""], ["Duncan", "James S.", ""]]}, {"id": "2105.07062", "submitter": "Nicol\\`o Felicioni", "authors": "Nicol\\`o Felicioni, Maurizio Ferrari Dacrema, Paolo Cremonesi", "title": "Measuring the User Satisfaction in a Recommendation Interface with\n  Multiple Carousels", "comments": null, "journal-ref": "ACM International Conference on Interactive Media Experiences (IMX\n  '21), June 21--23, 2021, Virtual Event, NY, USA", "doi": "10.1145/3452918.3465493", "report-no": null, "categories": "cs.IR cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for video-on-demand and music streaming services to adopt a user\ninterface composed of several recommendation lists, i.e. widgets or swipeable\ncarousels, each generated according to a specific criterion or algorithm (e.g.\nmost recent, top popular, recommended for you, editors' choice, etc.).\nSelecting the appropriate combination of carousel has significant impact on\nuser satisfaction. A crucial aspect of this user interface is that to measure\nthe relevance a new carousel for the user it is not sufficient to account\nsolely for its individual quality. Instead, it should be considered that other\ncarousels will already be present in the interface. This is not considered by\ntraditional evaluation protocols for recommenders systems, in which each\ncarousel is evaluated in isolation, regardless of (i) which other carousels are\ndisplayed to the user and (ii) the relative position of the carousel with\nrespect to other carousels. Hence, we propose a two-dimensional evaluation\nprotocol for a carousel setting that will measure the quality of a\nrecommendation carousel based on how much it improves upon the quality of an\nalready available set of carousels. Our evaluation protocol takes into account\nalso the position bias, i.e. users do not explore the carousels sequentially,\nbut rather concentrate on the top-left corner of the screen.\n  We report experiments on the movie domain and notice that under a carousel\nsetting the definition of which criteria has to be preferred to generate a list\nof recommended items changes with respect to what is commonly understood.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 20:33:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Felicioni", "Nicol\u00f2", ""], ["Dacrema", "Maurizio Ferrari", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "2105.07065", "submitter": "Nicholas Ichien", "authors": "Nicholas Ichien, Qing Liu, Shuhao Fu, Keith J. Holyoak, Alan Yuille,\n  Hongjing Lu", "title": "Visual analogy: Deep learning versus compositional models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Is analogical reasoning a task that must be learned to solve from scratch by\napplying deep learning models to massive numbers of reasoning problems? Or are\nanalogies solved by computing similarities between structured representations\nof analogs? We address this question by comparing human performance on visual\nanalogies created using images of familiar three-dimensional objects (cars and\ntheir subregions) with the performance of alternative computational models.\nHuman reasoners achieved above-chance accuracy for all problem types, but made\nmore errors in several conditions (e.g., when relevant subregions were\noccluded). We compared human performance to that of two recent deep learning\nmodels (Siamese Network and Relation Network) directly trained to solve these\nanalogy problems, as well as to that of a compositional model that assesses\nrelational similarity between part-based representations. The compositional\nmodel based on part representations, but not the deep learning models,\ngenerated qualitative performance similar to that of human reasoners.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 20:56:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ichien", "Nicholas", ""], ["Liu", "Qing", ""], ["Fu", "Shuhao", ""], ["Holyoak", "Keith J.", ""], ["Yuille", "Alan", ""], ["Lu", "Hongjing", ""]]}, {"id": "2105.07066", "submitter": "Hongda Wu", "authors": "Hongda Wu, Ping Wang", "title": "Node Selection Toward Faster Convergence for Federated Learning on\n  Non-IID Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated Learning (FL) is a distributed learning paradigm that enables a\nlarge number of resource-limited nodes to collaboratively train a model without\ndata sharing. The non-independent-and-identically-distributed (non-i.i.d.) data\nsamples invoke discrepancy between global and local objectives, making the FL\nmodel slow to converge. In this paper, we proposed Optimal Aggregation\nalgorithm for better aggregation, which finds out the optimal subset of local\nupdates of participating nodes in each global round, by identifying and\nexcluding the adverse local updates via checking the relationship between the\nlocal gradient and the global gradient. Then, we proposed a Probabilistic Node\nSelection framework (FedPNS) to dynamically change the probability for each\nnode to be selected based on the output of Optimal Aggregation. FedPNS can\npreferentially select nodes that propel faster model convergence. The\nunbiasedness of the proposed FedPNS design is illustrated and the convergence\nrate improvement of FedPNS over the commonly adopted Federated Averaging\n(FedAvg) algorithm is analyzed theoretically. Experimental results demonstrate\nthe effectiveness of FedPNS in accelerating the FL convergence rate, as\ncompared to FedAvg with random node selection.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 20:56:09 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wu", "Hongda", ""], ["Wang", "Ping", ""]]}, {"id": "2105.07078", "submitter": "Siyue Wang", "authors": "Siyue Wang, Xiao Wang, Pin-Yu Chen, Pu Zhao and Xue Lin", "title": "High-Robustness, Low-Transferability Fingerprinting of Neural Networks", "comments": "ICLR 2021 Workshop on Security and Safety in Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes Characteristic Examples for effectively fingerprinting\ndeep neural networks, featuring high-robustness to the base model against model\npruning as well as low-transferability to unassociated models. This is the\nfirst work taking both robustness and transferability into consideration for\ngenerating realistic fingerprints, whereas current methods lack practical\nassumptions and may incur large false positive rates. To achieve better\ntrade-off between robustness and transferability, we propose three kinds of\ncharacteristic examples: vanilla C-examples, RC-examples, and LTRC-example, to\nderive fingerprints from the original base model. To fairly characterize the\ntrade-off between robustness and transferability, we propose Uniqueness Score,\na comprehensive metric that measures the difference between robustness and\ntransferability, which also serves as an indicator to the false alarm problem.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 21:48:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Siyue", ""], ["Wang", "Xiao", ""], ["Chen", "Pin-Yu", ""], ["Zhao", "Pu", ""], ["Lin", "Xue", ""]]}, {"id": "2105.07082", "submitter": "Zehao Dong", "authors": "Zehao Dong, Heming Zhang, Yixin Chen, Fuhai Li", "title": "Interpretable Drug Synergy Prediction with Graph Neural Networks for\n  Human-AI Collaboration in Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate molecular mechanisms of resistant or sensitive response of\ncancer drug combination therapies in an inductive and interpretable manner.\nThough deep learning algorithms are widely used in the drug synergy prediction\nproblem, it is still an open problem to formulate the prediction model with\nbiological meaning to investigate the mysterious mechanisms of synergy (MoS)\nfor the human-AI collaboration in healthcare systems. To address the\nchallenges, we propose a deep graph neural network, IDSP (Interpretable Deep\nSignaling Pathways), to incorporate the gene-gene as well as gene-drug\nregulatory relationships in synergic drug combination predictions. IDSP\nautomatically learns weights of edges based on the gene and drug node\nrelations, i.e., signaling interactions, by a multi-layer perceptron (MLP) and\naggregates information in an inductive manner. The proposed architecture\ngenerates interpretable drug synergy prediction by detecting important\nsignaling interactions, and can be implemented when the underlying molecular\nmechanism encounters unseen genes or signaling pathways. We test IDWSP on\nsignaling networks formulated by genes from 46 core cancer signaling pathways\nand drug combinations from NCI ALMANAC drug combination screening data. The\nexperimental results demonstrated that 1) IDSP can learn from the underlying\nmolecular mechanism to make prediction without additional drug chemical\ninformation while achieving highly comparable performance with current\nstate-of-art methods; 2) IDSP show superior generality and flexibility to\nimplement the synergy prediction task on both transductive tasks and inductive\ntasks. 3) IDSP can generate interpretable results by detecting different\nsalient signaling patterns (i.e. MoS) for different cell lines.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 22:20:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dong", "Zehao", ""], ["Zhang", "Heming", ""], ["Chen", "Yixin", ""], ["Li", "Fuhai", ""]]}, {"id": "2105.07091", "submitter": "Sydney Katz", "authors": "Sydney M. Katz, Anthony L. Corso, Christopher A. Strong, Mykel J.\n  Kochenderfer", "title": "Verification of Image-based Neural Network Controllers Using Generative\n  Models", "comments": "10 pages, 12 figures, presented at the 2021 AIAA Digital Avionics\n  Systems Conference (DASC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are often used to process information from image-based\nsensors to produce control actions. While they are effective for this task, the\ncomplex nature of neural networks makes their output difficult to verify and\npredict, limiting their use in safety-critical systems. For this reason, recent\nwork has focused on combining techniques in formal methods and reachability\nanalysis to obtain guarantees on the closed-loop performance of neural network\ncontrollers. However, these techniques do not scale to the high-dimensional and\ncomplicated input space of image-based neural network controllers. In this\nwork, we propose a method to address these challenges by training a generative\nadversarial network (GAN) to map states to plausible input images. By\nconcatenating the generator network with the control network, we obtain a\nnetwork with a low-dimensional input space. This insight allows us to use\nexisting closed-loop verification tools to obtain formal guarantees on the\nperformance of image-based controllers. We apply our approach to provide safety\nguarantees for an image-based neural network controller for an autonomous\naircraft taxi problem. We guarantee that the controller will keep the aircraft\non the runway and guide the aircraft towards the center of the runway. The\nguarantees we provide are with respect to the set of input images modeled by\nour generator network, so we provide a recall metric to evaluate how well the\ngenerator captures the space of plausible images.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 23:18:05 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Katz", "Sydney M.", ""], ["Corso", "Anthony L.", ""], ["Strong", "Christopher A.", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2105.07099", "submitter": "Seyed Omid Davoudi", "authors": "Omid Davoodi, Majid Komeili", "title": "Feature-Based Interpretable Reinforcement Learning based on\n  State-Transition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing concerns regarding the operational usage of AI models in the\nreal-world has caused a surge of interest in explaining AI models' decisions to\nhumans. Reinforcement Learning is not an exception in this regard. In this\nwork, we propose a method for offering local explanations on risk in\nreinforcement learning. Our method only requires a log of previous interactions\nbetween the agent and the environment to create a state-transition model. It is\ndesigned to work on RL environments with either continuous or discrete state\nand action spaces. After creating the model, actions of any agent can be\nexplained in terms of the features most influential in increasing or decreasing\nrisk or any other desirable objective function in the locality of the agent.\nThrough experiments, we demonstrate the effectiveness of the proposed method in\nproviding such explanations.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 23:43:11 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Davoodi", "Omid", ""], ["Komeili", "Majid", ""]]}, {"id": "2105.07102", "submitter": "Robert Cohen", "authors": "Robert A. Cohen, Hyomin Choi, Ivan V. Baji\\'c", "title": "Lightweight Compression of Intermediate Neural Network Features for\n  Collaborative Intelligence", "comments": "Accepted for publication in IEEE Open Journal of Circuits and Systems", "journal-ref": "IEEE Open Journal of Circuits and Systems, vol. 2, 13 May 2021,\n  pp. 350-362", "doi": "10.1109/OJCAS.2021.3072884", "report-no": null, "categories": "cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 00:10:12 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Cohen", "Robert A.", ""], ["Choi", "Hyomin", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2105.07107", "submitter": "Sushil Thapa", "authors": "Sunil Thulasidasan, Sushil Thapa, Sayera Dhaubhadel, Gopinath\n  Chennupati, Tanmoy Bhattacharya, Jeff Bilmes", "title": "An Effective Baseline for Robustness to Distributional Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Refraining from confidently predicting when faced with categories of inputs\ndifferent from those seen during training is an important requirement for the\nsafe deployment of deep learning systems. While simple to state, this has been\na particularly challenging problem in deep learning, where models often end up\nmaking overconfident predictions in such situations. In this work we present a\nsimple, but highly effective approach to deal with out-of-distribution\ndetection that uses the principle of abstention: when encountering a sample\nfrom an unseen class, the desired behavior is to abstain from predicting. Our\napproach uses a network with an extra abstention class and is trained on a\ndataset that is augmented with an uncurated set that consists of a large number\nof out-of-distribution (OoD) samples that are assigned the label of the\nabstention class; the model is then trained to learn an effective discriminator\nbetween in and out-of-distribution samples. We compare this relatively simple\napproach against a wide variety of more complex methods that have been proposed\nboth for out-of-distribution detection as well as uncertainty modeling in deep\nlearning, and empirically demonstrate its effectiveness on a wide variety of of\nbenchmarks and deep architectures for image recognition and text\nclassification, often outperforming existing approaches by significant margins.\nGiven the simplicity and effectiveness of this method, we propose that this\napproach be used as a new additional baseline for future work in this domain.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 00:46:11 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Thulasidasan", "Sunil", ""], ["Thapa", "Sushil", ""], ["Dhaubhadel", "Sayera", ""], ["Chennupati", "Gopinath", ""], ["Bhattacharya", "Tanmoy", ""], ["Bilmes", "Jeff", ""]]}, {"id": "2105.07111", "submitter": "Zahra Dasht Bozorgi", "authors": "Zahra Dasht Bozorgi, Irene Teinemaa, Marlon Dumas, Marcello La Rosa", "title": "Prescriptive Process Monitoring for Cost-Aware Cycle Time Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reducing cycle time is a recurrent concern in the field of business process\nmanagement. Depending on the process, various interventions may be triggered to\nreduce the cycle time of a case, for example, using a faster shipping service\nin an order-to-delivery process or giving a phone call to a customer to obtain\nmissing information rather than waiting passively. Each of these interventions\ncomes with a cost. This paper tackles the problem of determining if and when to\ntrigger a time-reducing intervention in a way that maximizes the total net\ngain. The paper proposes a prescriptive process monitoring method that uses\northogonal random forest models to estimate the causal effect of triggering a\ntime-reducing intervention for each ongoing case of a process. Based on this\ncausal effect estimate, the method triggers interventions according to a\nuser-defined policy. The method is evaluated on two real-life logs.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 01:19:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bozorgi", "Zahra Dasht", ""], ["Teinemaa", "Irene", ""], ["Dumas", "Marlon", ""], ["La Rosa", "Marcello", ""]]}, {"id": "2105.07113", "submitter": "Christian Mejia-Escobar", "authors": "Christian Mejia-Escobar, Miguel Cazorla, Ester Martinez-Martin", "title": "A Large Visual, Qualitative and Quantitative Dataset of Web Pages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The World Wide Web is not only one of the most important platforms of\ncommunication and information at present, but also an area of growing interest\nfor scientific research. This motivates a lot of work and projects that require\nlarge amounts of data. However, there is no dataset that integrates the\nparameters and visual appearance of Web pages, because its collection is a\ncostly task in terms of time and effort. With the support of various computer\ntools and programming scripts, we have created a large dataset of 49,438 Web\npages. It consists of visual, textual and numerical data types, includes all\ncountries worldwide, and considers a broad range of topics such as art,\nentertainment, economy, business, education, government, news, media, science,\nand environment, covering different cultural characteristics and varied design\npreferences. In this paper, we describe the process of collecting, debugging\nand publishing the final product, which is freely available. To demonstrate the\nusefulness of our dataset, we expose a binary classification model for\ndetecting error Web pages, and a multi-class Web subject-based categorization,\nboth problems using convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 01:31:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Mejia-Escobar", "Christian", ""], ["Cazorla", "Miguel", ""], ["Martinez-Martin", "Ester", ""]]}, {"id": "2105.07142", "submitter": "Sagnik Majumder", "authors": "Sagnik Majumder, Ziad Al-Halah, Kristen Grauman", "title": "Move2Hear: Active Audio-Visual Source Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the active audio-visual source separation problem, where an\nagent must move intelligently in order to better isolate the sounds coming from\nan object of interest in its environment. The agent hears multiple audio\nsources simultaneously (e.g., a person speaking down the hall in a noisy\nhousehold) and must use its eyes and ears to automatically separate out the\nsounds originating from the target object within a limited time budget. Towards\nthis goal, we introduce a reinforcement learning approach that trains movement\npolicies controlling the agent's camera and microphone placement over time,\nguided by the improvement in predicted audio separation quality. We demonstrate\nour approach in scenarios motivated by both augmented reality (system is\nalready co-located with the target object) and mobile robotics (agent begins\narbitrarily far from the target object). Using state-of-the-art realistic\naudio-visual simulations in 3D environments, we demonstrate our model's ability\nto find minimal movement sequences with maximal payoff for audio source\nseparation. Project: http://vision.cs.utexas.edu/projects/move2hear.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:58:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Majumder", "Sagnik", ""], ["Al-Halah", "Ziad", ""], ["Grauman", "Kristen", ""]]}, {"id": "2105.07168", "submitter": "Masayoshi Mase", "authors": "Masayoshi Mase, Art B. Owen, Benjamin B. Seiler", "title": "Cohort Shapley value for algorithmic fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cohort Shapley value is a model-free method of variable importance grounded\nin game theory that does not use any unobserved and potentially impossible\nfeature combinations. We use it to evaluate algorithmic fairness, using the\nwell known COMPAS recidivism data as our example. This approach allows one to\nidentify for each individual in a data set the extent to which they were\nadversely or beneficially affected by their value of a protected attribute such\nas their race. The method can do this even if race was not one of the original\npredictors and even if it does not have access to a proprietary algorithm that\nhas made the predictions. The grounding in game theory lets us define aggregate\nvariable importance for a data set consistently with its per subject\ndefinitions. We can investigate variable importance for multiple quantities of\ninterest in the fairness literature including false positive predictions.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 08:02:18 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Mase", "Masayoshi", ""], ["Owen", "Art B.", ""], ["Seiler", "Benjamin B.", ""]]}, {"id": "2105.07179", "submitter": "Hanfeng Zhai", "authors": "Hanfeng Zhai, Guohui Hu", "title": "BubbleNet: Inferring micro-bubble dynamics with semi-physics-informed\n  deep learning", "comments": "Edit and reconstruct the paper; remade some figures; & reanalysis\n  some results", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Micro-bubbles and bubbly flows are widely observed and applied in chemical\nengineering, medicine, involves deformation, rupture, and collision of bubbles,\nphase mixture, etc. We study bubble dynamics by setting up two numerical\nsimulation cases: bubbly flow with a single bubble and multiple bubbles, both\nconfined in the microchannel, with parameters corresponding to their medical\nbackgrounds. Both the cases have their medical background applications.\nMultiphase flow simulation requires high computation accuracy due to possible\ncomponent losses that may be caused by sparse meshing during the computation.\nHence, data-driven methods can be adopted as an useful tool. Based on\nphysics-informed neural networks (PINNs), we propose a novel deep learning\nframework BubbleNet, which entails three main parts: deep neural networks (DNN)\nwith sub nets for predicting different physics fields; the\nsemi-physics-informed part, with only the fluid continuum condition and the\npressure Poisson equation $\\mathcal{P}$ encoded within; the time discretized\nnormalizer (TDN), an algorithm to normalize field data per time step before\ntraining. We apply the traditional DNN and our BubbleNet to train the coarsened\nsimulation data and predict the physics fields of both the two bubbly flow\ncases. The BubbleNets are trained for both with and without $\\mathcal{P}$, from\nwhich we conclude that the 'physics-informed' part can serve as inner\nsupervision. Results indicate our framework can predict the physics fields more\naccurately, estimating the prediction absolute errors. Our deep learning\npredictions outperform traditional numerical methods computed with similar data\ndensity meshing. The proposed network can potentially be applied to many other\nengineering fields.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 09:17:56 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 10:05:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhai", "Hanfeng", ""], ["Hu", "Guohui", ""]]}, {"id": "2105.07181", "submitter": "Xinyu Peng", "authors": "Xinyu Peng, Jiawei Zhang, Fei-Yue Wang and Li Li", "title": "Drill the Cork of Information Bottleneck by Inputting the Most Important\n  Data", "comments": "11 pages, to be published in IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has become the most powerful machine learning tool in the last\ndecade. However, how to efficiently train deep neural networks remains to be\nthoroughly solved. The widely used minibatch stochastic gradient descent (SGD)\nstill needs to be accelerated. As a promising tool to better understand the\nlearning dynamic of minibatch SGD, the information bottleneck (IB) theory\nclaims that the optimization process consists of an initial fitting phase and\nthe following compression phase. Based on this principle, we further study\ntypicality sampling, an efficient data selection method, and propose a new\nexplanation of how it helps accelerate the training process of the deep\nnetworks. We show that the fitting phase depicted in the IB theory will be\nboosted with a high signal-to-noise ratio of gradient approximation if the\ntypicality sampling is appropriately adopted. Furthermore, this finding also\nimplies that the prior information of the training set is critical to the\noptimization process and the better use of the most important data can help the\ninformation flow through the bottleneck faster. Both theoretical analysis and\nexperimental results on synthetic and real-world datasets demonstrate our\nconclusions.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 09:20:36 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Peng", "Xinyu", ""], ["Zhang", "Jiawei", ""], ["Wang", "Fei-Yue", ""], ["Li", "Li", ""]]}, {"id": "2105.07190", "submitter": "Gesina Schwalbe", "authors": "Gesina Schwalbe, Bettina Finzel", "title": "XAI Method Properties: A (Meta-)study", "comments": "37 pages, 2 figures, submitted to Data Mining and Knowledge Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the meantime, a wide variety of terminologies, motivations, approaches and\nevaluation criteria have been developed within the scope of research on\nexplainable artificial intelligence (XAI). Many taxonomies can be found in the\nliterature, each with a different focus, but also showing many points of\noverlap. In this paper, we summarize the most cited and current taxonomies in a\nmeta-analysis in order to highlight the essential aspects of the\nstate-of-the-art in XAI. We also present and add terminologies as well as\nconcepts from a large number of survey articles on the topic. Last but not\nleast, we illustrate concepts from the higher-level taxonomy with more than 50\nexample methods, which we categorize accordingly, thus providing a wide-ranging\noverview of aspects of XAI and paving the way for use case-appropriate as well\nas context-specific subsequent research.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 09:52:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Schwalbe", "Gesina", ""], ["Finzel", "Bettina", ""]]}, {"id": "2105.07194", "submitter": "Cheng Qiu", "authors": "Cheng Qiu, Yuzi Han, Logesh Shanmugam, Fengyang Jiang, Zhidong Guan,\n  Shanyi Du, Jinglei Yang", "title": "An even-load-distribution design for composite bolted joints using a\n  novel circuit model and artificial neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.app-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the brittle feature of carbon fiber reinforced plastic laminates,\nmechanical multi-joint within these composite components show uneven load\ndistribution for each bolt, which weaken the strength advantage of composite\nlaminates. In order to reduce this defect and achieve the goal of even load\ndistribution in mechanical joints, we propose a machine learning-based\nframework as an optimization method. Since that the friction effect has been\nproven to be a significant factor in determining bolt load distribution, our\nframework aims at providing optimal parameters including bolt-hole clearances\nand tightening torques for a minimum unevenness of bolt load. A novel circuit\nmodel is established to generate data samples for the training of artificial\nnetworks at a relatively low computational cost. A database for all the\npossible inputs in the design space is built through the machine learning\nmodel. The optimal dataset of clearances and torques provided by the database\nis validated by both the finite element method, circuit model, and an\nexperimental measurement based on the linear superposition principle, which\nshows the effectiveness of this general framework for the optimization problem.\nThen, our machine learning model is further compared and worked in\ncollaboration with commonly used optimization algorithms, which shows the\npotential of greatly increasing computational efficiency for the inverse design\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 10:10:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Qiu", "Cheng", ""], ["Han", "Yuzi", ""], ["Shanmugam", "Logesh", ""], ["Jiang", "Fengyang", ""], ["Guan", "Zhidong", ""], ["Du", "Shanyi", ""], ["Yang", "Jinglei", ""]]}, {"id": "2105.07196", "submitter": "Jitkomut Songsiri", "authors": "Parinthorn Manomaisaowapak and Jitkomut Songsiri", "title": "Joint learning of multiple Granger causal networks via non-convex\n  regularizations: Inference of group-level brain connectivity", "comments": "23 pages, 9 figures, 3 tables, Comments: Title changed, Math\n  expression corrected in the formulation and algorithm section, More\n  references and discussions are added to explain difficulties in the\n  convergence analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper considers joint learning of multiple sparse Granger graphical\nmodels to discover underlying common and differential Granger causality (GC)\nstructures across multiple time series. This can be applied to drawing\ngroup-level brain connectivity inferences from a homogeneous group of subjects\nor discovering network differences among groups of signals collected under\nheterogeneous conditions. By recognizing that the GC of a single multivariate\ntime series can be characterized by common zeros of vector autoregressive (VAR)\nlag coefficients, a group sparse prior is included in joint regularized\nleast-squares estimations of multiple VAR models. Group-norm regularizations\nbased on group- and fused-lasso penalties encourage a decomposition of multiple\nnetworks into a common GC structure, with other remaining parts defined in\nindividual-specific networks. Prior information about sparseness and sparsity\npatterns of desired GC networks are incorporated as relative weights, while a\nnon-convex group norm in the penalty is proposed to enhance the accuracy of\nnetwork estimation in low-sample settings. Extensive numerical results on\nsimulations illustrated our method's improvements over existing sparse\nestimation approaches on GC network sparsity recovery. Our methods were also\napplied to available resting-state fMRI time series from the ADHD-200 data sets\nto learn the differences of causality mechanisms, called effective brain\nconnectivity, between adolescents with ADHD and typically developing children.\nOur analysis revealed that parts of the causality differences between the two\ngroups often resided in the orbitofrontal region and areas associated with the\nlimbic system, which agreed with clinical findings and data-driven results in\nprevious studies.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 10:29:02 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 03:19:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Manomaisaowapak", "Parinthorn", ""], ["Songsiri", "Jitkomut", ""]]}, {"id": "2105.07205", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Xuancheng Ren, Zhiyuan Zhang, Xu Sun, Yuexian Zou", "title": "Rethinking Skip Connection with Layer Normalization in Transformers and\n  ResNets", "comments": "Accepted by COLING2020 (The 28th International Conference on\n  Computational Linguistics (COLING 2020))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skip connection, is a widely-used technique to improve the performance and\nthe convergence of deep neural networks, which is believed to relieve the\ndifficulty in optimization due to non-linearity by propagating a linear\ncomponent through the neural network layers. However, from another point of\nview, it can also be seen as a modulating mechanism between the input and the\noutput, with the input scaled by a pre-defined value one. In this work, we\ninvestigate how the scale factors in the effectiveness of the skip connection\nand reveal that a trivial adjustment of the scale will lead to spurious\ngradient exploding or vanishing in line with the deepness of the models, which\ncould be addressed by normalization, in particular, layer normalization, which\ninduces consistent improvements over the plain skip connection. Inspired by the\nfindings, we further propose to adaptively adjust the scale of the input by\nrecursively applying skip connection with layer normalization, which promotes\nthe performance substantially and generalizes well across diverse tasks\nincluding both machine translation and image classification datasets.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 11:44:49 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Liu", "Fenglin", ""], ["Ren", "Xuancheng", ""], ["Zhang", "Zhiyuan", ""], ["Sun", "Xu", ""], ["Zou", "Yuexian", ""]]}, {"id": "2105.07222", "submitter": "Zhiyi Zhang", "authors": "Zhang Zhiyi, Liu Ziyin", "title": "On the Distributional Properties of Adaptive Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive gradient methods have achieved remarkable success in training deep\nneural networks on a wide variety of tasks. However, not much is known about\nthe mathematical and statistical properties of this family of methods. This\nwork aims at providing a series of theoretical analyses of its statistical\nproperties justified by experiments. In particular, we show that when the\nunderlying gradient obeys a normal distribution, the variance of the magnitude\nof the \\textit{update} is an increasing and bounded function of time and does\nnot diverge. This work suggests that the divergence of variance is not the\ncause of the need for warm up of the Adam optimizer, contrary to what is\nbelieved in the current literature.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 13:45:16 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhiyi", "Zhang", ""], ["Ziyin", "Liu", ""]]}, {"id": "2105.07228", "submitter": "Tizian Wenzel", "authors": "Tizian Wenzel, Gabriele Santin, Bernard Haasdonk", "title": "Universality and Optimality of Structured Deep Kernel Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel based methods yield approximation models that are flexible, efficient\nand powerful. In particular, they utilize fixed feature maps of the data, being\noften associated to strong analytical results that prove their accuracy. On the\nother hand, the recent success of machine learning methods has been driven by\ndeep neural networks (NNs). They achieve a significant accuracy on very\nhigh-dimensional data, in that they are able to learn also efficient data\nrepresentations or data-based feature maps. In this paper, we leverage a recent\ndeep kernel representer theorem to connect the two approaches and understand\ntheir interplay. In particular, we show that the use of special types of\nkernels yield models reminiscent of neural networks that are founded in the\nsame theoretical framework of classical kernel methods, while enjoying many\ncomputational properties of deep neural networks. Especially the introduced\nStructured Deep Kernel Networks (SDKNs) can be viewed as neural networks with\noptimizable activation functions obeying a representer theorem. Analytic\nproperties show their universal approximation properties in different\nasymptotic regimes of unbounded number of centers, width and depth. Especially\nin the case of unbounded depth, the constructions is asymptotically better than\ncorresponding constructions for ReLU neural networks, which is made possible by\nthe flexibility of kernel approximation\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 14:10:35 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wenzel", "Tizian", ""], ["Santin", "Gabriele", ""], ["Haasdonk", "Bernard", ""]]}, {"id": "2105.07229", "submitter": "Amr Alanwar", "authors": "Amr Alanwar, Anne Koch, Frank Allg\\\"ower, Karl Henrik Johansson", "title": "Data-Driven Reachability Analysis from Noisy Data", "comments": "Submitted to Transactions on Automatic Control. arXiv admin note:\n  text overlap with arXiv:2011.08472", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing reachable sets directly from noisy data\nwithout a given system model. Several reachability algorithms are presented,\nand their accuracy is shown to depend on the underlying system generating the\ndata. First, an algorithm for computing over-approximated reachable sets based\non matrix zonotopes is proposed for linear systems. Constrained matrix\nzonotopes are introduced to provide less conservative reachable sets at the\ncost of increased computational expenses and utilized to incorporate prior\nknowledge about the unknown system model. Then we extend the approach to\npolynomial systems and under the assumption of Lipschitz continuity to\nnonlinear systems. Theoretical guarantees are given for these algorithms in\nthat they give a proper over-approximative reachable set containing the true\nreachable set. Multiple numerical examples show the applicability of the\nintroduced algorithms, and accuracy comparisons are made between algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 14:11:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Alanwar", "Amr", ""], ["Koch", "Anne", ""], ["Allg\u00f6wer", "Frank", ""], ["Johansson", "Karl Henrik", ""]]}, {"id": "2105.07231", "submitter": "Christopher Zach", "authors": "Christopher Zach", "title": "Bilevel Programming and Deep Learning: A Unifying View on Inference\n  Learning Methods", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work we unify a number of inference learning methods, that are\nproposed in the literature as alternative training algorithms to the ones based\non regular error back-propagation. These inference learning methods were\ndeveloped with very diverse motivations, mainly aiming to enhance the\nbiological plausibility of deep neural networks and to improve the intrinsic\nparallelism of training methods. We show that these superficially very\ndifferent methods can all be obtained by successively applying a particular\nreformulation of bilevel optimization programs. As a by-product it becomes also\nevident that all considered inference learning methods include back-propagation\nas a special case, and therefore at least approximate error back-propagation in\ntypical settings. Finally, we propose Fenchel back-propagation, that replaces\nthe propagation of infinitesimal corrections performed in standard\nback-propagation with finite targets as the learning signal. Fenchel\nback-propagation can therefore be seen as an instance of learning via explicit\ntarget propagation.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 14:19:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zach", "Christopher", ""]]}, {"id": "2105.07246", "submitter": "Minkai Xu", "authors": "Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael\n  Gomez-Bombarelli, Jian Tang", "title": "An End-to-End Framework for Molecular Conformation Generation via\n  Bilevel Programming", "comments": "Accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting molecular conformations (or 3D structures) from molecular graphs\nis a fundamental problem in many applications. Most existing approaches are\nusually divided into two steps by first predicting the distances between atoms\nand then generating a 3D structure through optimizing a distance geometry\nproblem. However, the distances predicted with such two-stage approaches may\nnot be able to consistently preserve the geometry of local atomic\nneighborhoods, making the generated structures unsatisfying. In this paper, we\npropose an end-to-end solution for molecular conformation prediction called\nConfVAE based on the conditional variational autoencoder framework.\nSpecifically, the molecular graph is first encoded in a latent space, and then\nthe 3D structures are generated by solving a principled bilevel optimization\nprogram. Extensive experiments on several benchmark data sets prove the\neffectiveness of our proposed approach over existing state-of-the-art\napproaches. Code is available at\n\\url{https://github.com/MinkaiXu/ConfVAE-ICML21}.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 15:22:29 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 13:01:35 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Xu", "Minkai", ""], ["Wang", "Wujie", ""], ["Luo", "Shitong", ""], ["Shi", "Chence", ""], ["Bengio", "Yoshua", ""], ["Gomez-Bombarelli", "Rafael", ""], ["Tang", "Jian", ""]]}, {"id": "2105.07253", "submitter": "Zhenghai Xue", "authors": "Zhenghai Xue, Xu-Hui Liu, Jing-Cheng Pang, Shengyi Jiang, Feng Xu,\n  Yang Yu", "title": "Regret Minimization Experience Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, experience replay stores past samples for further\nreuse. Prioritized sampling is a promising technique to better utilize these\nsamples. Previous criteria of prioritization include TD error, recentness and\ncorrective feedback, which are mostly heuristically designed. In this work, we\nstart from the regret minimization objective, and obtain an optimal\nprioritization strategy for Bellman update that can directly maximize the\nreturn of the policy. The theory suggests that data with higher hindsight TD\nerror, better on-policiness and more accurate Q value should be assigned with\nhigher weights during sampling. Thus most previous criteria only consider this\nstrategy partially. We not only provide theoretical justifications for previous\ncriteria, but also propose two new methods to compute the prioritization\nweight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT\nexploits the temporal ordering of states. Both methods outperform previous\nprioritized sampling algorithms in challenging RL benchmarks, including MuJoCo,\nAtari and Meta-World.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 16:08:45 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 01:34:37 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xue", "Zhenghai", ""], ["Liu", "Xu-Hui", ""], ["Pang", "Jing-Cheng", ""], ["Jiang", "Shengyi", ""], ["Xu", "Feng", ""], ["Yu", "Yang", ""]]}, {"id": "2105.07263", "submitter": "Nicholas Andrews", "authors": "Aleem Khan, Elizabeth Fleming, Noah Schofield, Marcus Bishop, Nicholas\n  Andrews", "title": "A Deep Metric Learning Approach to Account Linking", "comments": "13 pages; to be published in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of linking social media accounts that belong to the same\nauthor in an automated fashion on the basis of the content and metadata of\ntheir corresponding document streams. We focus on learning an embedding that\nmaps variable-sized samples of user activity -- ranging from single posts to\nentire months of activity -- to a vector space, where samples by the same\nauthor map to nearby points. The approach does not require human-annotated data\nfor training purposes, which allows us to leverage large amounts of social\nmedia content. The proposed model outperforms several competitive baselines\nunder a novel evaluation framework modeled after established recognition\nbenchmarks in other domains. Our method achieves high linking accuracy, even\nwith small samples from accounts not seen at training time, a prerequisite for\npractical applications of the proposed linking framework.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 17:06:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Khan", "Aleem", ""], ["Fleming", "Elizabeth", ""], ["Schofield", "Noah", ""], ["Bishop", "Marcus", ""], ["Andrews", "Nicholas", ""]]}, {"id": "2105.07264", "submitter": "Rajat Talak", "authors": "Rajat Talak, Siyi Hu, Lisa Peng, and Luca Carlone", "title": "Neural Trees for Learning on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) have emerged as a flexible and powerful approach\nfor learning over graphs. Despite this success, existing GNNs are constrained\nby their local message-passing architecture and are provably limited in their\nexpressive power. In this work, we propose a new GNN architecture -- the Neural\nTree. The neural tree architecture does not perform message passing on the\ninput graph but on a tree-structured graph, called the H-tree, that is\nconstructed from the input graph. Nodes in the H-tree correspond to subgraphs\nin the input graph, and they are reorganized in a hierarchical manner such that\na parent-node of a node in the H-tree always corresponds to a larger subgraph\nin the input graph. We show that the neural tree architecture can approximate\nany smooth probability distribution function over an undirected graph, as well\nas emulate the junction tree algorithm. We also prove that the number of\nparameters needed to achieve an $\\epsilon$-approximation of the distribution\nfunction is exponential in the treewidth of the input graph, but linear in its\nsize. We apply the neural tree to semi-supervised node classification in 3D\nscene graphs, and show that these theoretical properties translate into\nsignificant gains in prediction accuracy, over the more traditional GNN\narchitectures.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 17:08:20 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Talak", "Rajat", ""], ["Hu", "Siyi", ""], ["Peng", "Lisa", ""], ["Carlone", "Luca", ""]]}, {"id": "2105.07283", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Calibrating sufficiently", "comments": "26 pages, 2 figures, appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When probabilistic classifiers are trained and calibrated, the so-called\ngrouping loss component of the calibration loss can easily be overlooked.\nGrouping loss refers to the gap between observable information and information\nactually exploited in the calibration exercise. We investigate the relation\nbetween grouping loss and the concept of sufficiency, identifying\ncomonotonicity as a useful criterion for sufficiency. We revisit the probing\nreduction approach of Langford & Zadrozny (2005) and find that it produces an\nestimator of probabilistic classifiers that reduces grouping loss. Finally, we\ndiscuss Brier curves as tools to support training and 'sufficient' calibration\nof probabilistic classifiers.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 19:48:28 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 18:36:00 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 13:02:08 GMT"}, {"version": "v4", "created": "Sun, 25 Jul 2021 12:41:41 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "2105.07291", "submitter": "Jonathan Lacotte", "authors": "Jonathan Lacotte, Yifei Wang, Mert Pilanci", "title": "Adaptive Newton Sketch: Linear-time Optimization with Quadratic\n  Convergence and Effective Hessian Dimensionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized algorithm with quadratic convergence rate for convex\noptimization problems with a self-concordant, composite, strongly convex\nobjective function. Our method is based on performing an approximate Newton\nstep using a random projection of the Hessian. Our first contribution is to\nshow that, at each iteration, the embedding dimension (or sketch size) can be\nas small as the effective dimension of the Hessian matrix. Leveraging this\nnovel fundamental result, we design an algorithm with a sketch size\nproportional to the effective dimension and which exhibits a quadratic rate of\nconvergence. This result dramatically improves on the classical\nlinear-quadratic convergence rates of state-of-the-art sub-sampled Newton\nmethods. However, in most practical cases, the effective dimension is not known\nbeforehand, and this raises the question of how to pick a sketch size as small\nas the effective dimension while preserving a quadratic convergence rate. Our\nsecond and main contribution is thus to propose an adaptive sketch size\nalgorithm with quadratic convergence rate and which does not require prior\nknowledge or estimation of the effective dimension: at each iteration, it\nstarts with a small sketch size, and increases it until quadratic progress is\nachieved. Importantly, we show that the embedding dimension remains\nproportional to the effective dimension throughout the entire path and that our\nmethod achieves state-of-the-art computational complexity for solving convex\noptimization programs with a strongly convex component.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 20:24:26 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Lacotte", "Jonathan", ""], ["Wang", "Yifei", ""], ["Pilanci", "Mert", ""]]}, {"id": "2105.07308", "submitter": "Alexander Ororbia", "authors": "Alexander Ororbia, M. A. Kelly", "title": "Towards a Predictive Processing Implementation of the Common Model of\n  Cognition", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a cognitive architecture that is built from\npowerful yet simple neural models. Specifically, we describe an implementation\nof the common model of cognition grounded in neural generative coding and\nholographic associative memory. The proposed system creates the groundwork for\ndeveloping agents that learn continually from diverse tasks as well as model\nhuman performance at larger scales than what is possible with existant\ncognitive architectures.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 22:55:23 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 21:14:26 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ororbia", "Alexander", ""], ["Kelly", "M. A.", ""]]}, {"id": "2105.07310", "submitter": "Ting-Jui Chang", "authors": "Ting-Jui Chang and Shahin Shahrampour", "title": "Regret Analysis of Distributed Online LQR Control for Unknown LTI\n  Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.13749", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning has recently opened avenues for rethinking classical optimal\ncontrol beyond time-invariant cost metrics, and online controllers are designed\nwhen the performance criteria changes adversarially over time. Inspired by this\nline of research, we study the distributed online linear quadratic regulator\n(LQR) problem for linear time-invariant (LTI) systems with unknown dynamics.\nConsider a multi-agent network where each agent is modeled as a LTI system. The\nLTI systems are associated with time-varying quadratic costs that are revealed\nsequentially. The goal of the network is to collectively (i) estimate the\nunknown dynamics and (ii) compute local control sequences competitive to that\nof the best centralized policy in hindsight that minimizes the sum of costs for\nall agents. This problem is formulated as a {\\it regret} minimization. We\npropose a distributed variant of the online LQR algorithm where each agent\ncomputes its system estimate during an exploration stage. The agent then\napplies distributed online gradient descent on a semi-definite programming\n(SDP) whose feasible set is based on the agent's system estimate. We prove that\nthe regret bound of our proposed algorithm scales $\\tilde{O}(T^{2/3})$,\nimplying the consensus of the network over time. We also provide simulation\nresults verifying our theoretical guarantee.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 23:02:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chang", "Ting-Jui", ""], ["Shahrampour", "Shahin", ""]]}, {"id": "2105.07322", "submitter": "Max Ehrlich", "authors": "Arthita Ghosh, Max Ehrlich, Larry Davis, Rama Chellappa", "title": "Unsupervised Super-Resolution of Satellite Imagery for High Fidelity\n  Material Label Transfer", "comments": "Published in the proceedings of the 2019 IEEE International\n  Geoscience and Remote Sensing Symposium", "journal-ref": "IGARSS (2019), 5144-5147", "doi": "10.1109/IGARSS.2019.8900639", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban material recognition in remote sensing imagery is a highly relevant,\nyet extremely challenging problem due to the difficulty of obtaining human\nannotations, especially on low resolution satellite images. To this end, we\npropose an unsupervised domain adaptation based approach using adversarial\nlearning. We aim to harvest information from smaller quantities of high\nresolution data (source domain) and utilize the same to super-resolve low\nresolution imagery (target domain). This can potentially aid in semantic as\nwell as material label transfer from a richly annotated source to a target\ndomain.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 00:57:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ghosh", "Arthita", ""], ["Ehrlich", "Max", ""], ["Davis", "Larry", ""], ["Chellappa", "Rama", ""]]}, {"id": "2105.07331", "submitter": "Haichao Yu", "authors": "Haichao Yu, Linjie Yang, Humphrey Shi", "title": "Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain\n  Calibration for Network Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Post-training quantization methods use a set of calibration data to compute\nquantization ranges for network parameters and activations. The calibration\ndata usually comes from the training dataset which could be inaccessible due to\nsensitivity of the data. In this work, we want to study such a problem: can we\nuse out-of-domain data to calibrate the trained networks without knowledge of\nthe original dataset? Specifically, we go beyond the domain of natural images\nto include drastically different domains such as X-ray images, satellite images\nand ultrasound images. We find cross-domain calibration leads to surprisingly\nstable performance of quantized models on 10 tasks in different image domains\nwith 13 different calibration datasets. We also find that the performance of\nquantized models is correlated with the similarity of the Gram matrices between\nthe source and calibration domains, which can be used as a criterion to choose\ncalibration set for better performance. We believe our research opens the door\nto borrow cross-domain knowledge for network quantization and compression.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 02:07:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yu", "Haichao", ""], ["Yang", "Linjie", ""], ["Shi", "Humphrey", ""]]}, {"id": "2105.07334", "submitter": "Kenneth Co", "authors": "Kenneth T. Co, Luis Mu\\~noz-Gonz\\'alez, Leslie Kanthan, Emil C. Lupu", "title": "Real-time Detection of Practical Universal Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Universal Adversarial Perturbations (UAPs) are a prominent class of\nadversarial examples that exploit the systemic vulnerabilities and enable\nphysically realizable and robust attacks against Deep Neural Networks (DNNs).\nUAPs generalize across many different inputs; this leads to realistic and\neffective attacks that can be applied at scale. In this paper we propose\nHyperNeuron, an efficient and scalable algorithm that allows for the real-time\ndetection of UAPs by identifying suspicious neuron hyper-activations. Our\nresults show the effectiveness of HyperNeuron on multiple tasks (image\nclassification, object detection), against a wide variety of universal attacks,\nand in realistic scenarios, like perceptual ad-blocking and adversarial\npatches. HyperNeuron is able to simultaneously detect both adversarial mask and\npatch UAPs with comparable or better performance than existing UAP defenses\nwhilst introducing a significantly reduced latency of only 0.86 milliseconds\nper image. This suggests that many realistic and practical universal attacks\ncan be reliably mitigated in real-time, which shows promise for the robust\ndeployment of machine learning systems.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 03:01:29 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 23:33:20 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Co", "Kenneth T.", ""], ["Mu\u00f1oz-Gonz\u00e1lez", "Luis", ""], ["Kanthan", "Leslie", ""], ["Lupu", "Emil C.", ""]]}, {"id": "2105.07338", "submitter": "Ming-Kun Xie", "authors": "Ming-Kun Xie and Sheng-Jun Huang", "title": "CCMN: A General Framework for Learning with Class-Conditional\n  Multi-Label Noise", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class-conditional noise commonly exists in machine learning tasks, where the\nclass label is corrupted with a probability depending on its ground-truth. Many\nresearch efforts have been made to improve the model robustness against the\nclass-conditional noise. However, they typically focus on the single label case\nby assuming that only one label is corrupted. In real applications, an instance\nis usually associated with multiple labels, which could be corrupted\nsimultaneously with their respective conditional probabilities. In this paper,\nwe formalize this problem as a general framework of learning with\nClass-Conditional Multi-label Noise (CCMN for short). We establish two unbiased\nestimators with error bounds for solving the CCMN problems, and further prove\nthat they are consistent with commonly used multi-label loss functions.\nFinally, a new method for partial multi-label learning is implemented with\nunbiased estimator under the CCMN framework. Empirical studies on multiple\ndatasets and various evaluation metrics validate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 03:24:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Xie", "Ming-Kun", ""], ["Huang", "Sheng-Jun", ""]]}, {"id": "2105.07342", "submitter": "Lirong Wu", "authors": "Lirong Wu, Haitao Lin, Zhangyang Gao, Cheng Tan, Stan.Z.Li", "title": "Self-supervised on Graphs: Contrastive, Generative,or Predictive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning on graphs has recently achieved remarkable success on a variety\nof tasks while such success relies heavily on the massive and carefully labeled\ndata. However, precise annotations are generally very expensive and\ntime-consuming. To address this problem, self-supervised learning (SSL) is\nemerging as a new paradigm for extracting informative knowledge through\nwell-designed pretext tasks without relying on manual labels. In this survey,\nwe extend the concept of SSL, which first emerged in the fields of computer\nvision and natural language processing, to present a timely and comprehensive\nreview of the existing SSL techniques for graph data. Specifically, we divide\nexisting graph SSL methods into three categories: contrastive, generative, and\npredictive. More importantly, unlike many other surveys that only provide a\nhigh-level description of published research, we present an additional\nmathematical summary of the existing works in a unified framework. Furthermore,\nto facilitate methodological development and empirical comparisons, we also\nsummarize the commonly used datasets, evaluation metrics, downstream tasks, and\nopen-source implementations of various algorithms. Finally, we discuss the\ntechnical challenges and potential future directions for improving graph\nself-supervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 03:30:03 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 14:03:20 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 08:49:48 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wu", "Lirong", ""], ["Lin", "Haitao", ""], ["Gao", "Zhangyang", ""], ["Tan", "Cheng", ""], ["Li", "Stan. Z.", ""]]}, {"id": "2105.07346", "submitter": "Ziyu Ye", "authors": "Ziyu Ye, Yuxin Chen and Haitao Zheng", "title": "Understanding the Effect of Bias in Deep Anomaly Detection", "comments": "Accepted at IJCAI '21. Codes available on\n  github.com/ZIYU-DEEP/Understanding-Bias-in-Deep-Anomaly-Detection-PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection presents a unique challenge in machine learning, due to the\nscarcity of labeled anomaly data. Recent work attempts to mitigate such\nproblems by augmenting training of deep anomaly detection models with\nadditional labeled anomaly samples. However, the labeled data often does not\nalign with the target distribution and introduces harmful bias to the trained\nmodel. In this paper, we aim to understand the effect of a biased anomaly set\non anomaly detection. Concretely, we view anomaly detection as a supervised\nlearning task where the objective is to optimize the recall at a given false\npositive rate. We formally study the relative scoring bias of an anomaly\ndetector, defined as the difference in performance with respect to a baseline\nanomaly detector. We establish the first finite sample rates for estimating the\nrelative scoring bias for deep anomaly detection, and empirically validate our\ntheoretical results on both synthetic and real-world datasets. We also provide\nan extensive empirical study on how a biased training anomaly set affects the\nanomaly score function and therefore the detection performance on different\nanomaly classes. Our study demonstrates scenarios in which the biased anomaly\nset can be useful or problematic, and provides a solid benchmark for future\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 03:55:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ye", "Ziyu", ""], ["Chen", "Yuxin", ""], ["Zheng", "Haitao", ""]]}, {"id": "2105.07350", "submitter": "Zicheng Zhang", "authors": "ZiCheng Zhang, CongYing Han, TianDe Guo", "title": "ExSinGAN: Learning an Explainable Generative Model from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating images from a single sample, as a newly developing branch of image\nsynthesis, has attracted extensive attention. In this paper, we formulate this\nproblem as sampling from the conditional distribution of a single image, and\npropose a hierarchical framework that simplifies the learning of the intricate\nconditional distributions through the successive learning of the distributions\nabout structure, semantics and texture, making the process of learning and\ngeneration comprehensible. On this basis, we design ExSinGAN composed of three\ncascaded GANs for learning an explainable generative model from a given image,\nwhere the cascaded GANs model the distributions about structure, semantics and\ntexture successively. ExSinGAN is learned not only from the internal patches of\nthe given image as the previous works did, but also from the external prior\nobtained by the GAN inversion technique. Benefiting from the appropriate\ncombination of internal and external information, ExSinGAN has a more powerful\ncapability of generation and competitive generalization ability for the image\nmanipulation tasks compared with prior works.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 04:38:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "ZiCheng", ""], ["Han", "CongYing", ""], ["Guo", "TianDe", ""]]}, {"id": "2105.07364", "submitter": "Yu Shen", "authors": "Yu Shen, Sijie Zhu, Taojiannan Yang, Chen Chen, Delu Pan, Jianyu Chen,\n  Liang Xiao, Qian Du", "title": "BDANet: Multiscale Convolutional Neural Network with Cross-directional\n  Attention for Building Damage Assessment from Satellite Images", "comments": "arXiv admin note: text overlap with arXiv:2010.14014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and effective responses are required when a natural disaster (e.g.,\nearthquake, hurricane, etc.) strikes. Building damage assessment from satellite\nimagery is critical before relief effort is deployed. With a pair of pre- and\npost-disaster satellite images, building damage assessment aims at predicting\nthe extent of damage to buildings. With the powerful ability of feature\nrepresentation, deep neural networks have been successfully applied to building\ndamage assessment. Most existing works simply concatenate pre- and\npost-disaster images as input of a deep neural network without considering\ntheir correlations. In this paper, we propose a novel two-stage convolutional\nneural network for Building Damage Assessment, called BDANet. In the first\nstage, a U-Net is used to extract the locations of buildings. Then the network\nweights from the first stage are shared in the second stage for building damage\nassessment. In the second stage, a two-branch multi-scale U-Net is employed as\nbackbone, where pre- and post-disaster images are fed into the network\nseparately. A cross-directional attention module is proposed to explore the\ncorrelations between pre- and post-disaster images. Moreover, CutMix data\naugmentation is exploited to tackle the challenge of difficult classes. The\nproposed method achieves state-of-the-art performance on a large-scale dataset\n-- xBD. The code is available at\nhttps://github.com/ShaneShen/BDANet-Building-Damage-Assessment.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 06:13:28 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shen", "Yu", ""], ["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""], ["Pan", "Delu", ""], ["Chen", "Jianyu", ""], ["Xiao", "Liang", ""], ["Du", "Qian", ""]]}, {"id": "2105.07371", "submitter": "Gesina Schwalbe", "authors": "Johannes Rabold, Gesina Schwalbe, Ute Schmid", "title": "Expressive Explanations of DNNs by Combining Concept Analysis with ILP", "comments": "14 pages, 4 figures; Camera-ready submission to KI2020; The final\n  authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-58285-2_11; code available at\n  https://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020", "journal-ref": null, "doi": "10.1007/978-3-030-58285-2_11", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable AI has emerged to be a key component for black-box machine\nlearning approaches in domains with a high demand for reliability or\ntransparency. Examples are medical assistant systems, and applications\nconcerned with the General Data Protection Regulation of the European Union,\nwhich features transparency as a cornerstone. Such demands require the ability\nto audit the rationale behind a classifier's decision. While visualizations are\nthe de facto standard of explanations, they come short in terms of\nexpressiveness in many ways: They cannot distinguish between different\nattribute manifestations of visual features (e.g. eye open vs. closed), and\nthey cannot accurately describe the influence of absence of, and relations\nbetween features. An alternative would be more expressive symbolic surrogate\nmodels. However, these require symbolic inputs, which are not readily available\nin most computer vision tasks. In this paper we investigate how to overcome\nthis: We use inherent features learned by the network to build a global,\nexpressive, verbal explanation of the rationale of a feed-forward convolutional\ndeep neural network (DNN). The semantics of the features are mined by a concept\nanalysis approach trained on a set of human understandable visual concepts. The\nexplanation is found by an Inductive Logic Programming (ILP) method and\npresented as first-order rules. We show that our explanation is faithful to the\noriginal black-box model.\n  The code for our experiments is available at\nhttps://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 07:00:27 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Rabold", "Johannes", ""], ["Schwalbe", "Gesina", ""], ["Schmid", "Ute", ""]]}, {"id": "2105.07372", "submitter": "Noam Janco", "authors": "Noam Janco and Tamir Bendory", "title": "An accelerated expectation-maximization for multi-reference alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-reference alignment (MRA) problem entails estimating an image from\nmultiple noisy and rotated copies of itself. If the noise level is low, one can\nreconstruct the image by estimating the missing rotations, aligning the images,\nand averaging out the noise. While accurate rotation estimation is impossible\nif the noise level is high, the rotations can still be approximated, and thus\ncan provide indispensable information. In particular, learning the\napproximation error can be harnessed for efficient image estimation. In this\npaper, we propose a new computational framework, called Synch-EM, that consists\nof angular synchronization followed by expectation-maximization (EM). The\nsynchronization step results in a concentrated distribution of rotations; this\ndistribution is learned and then incorporated into the EM as a Bayesian prior.\nThe learned distribution also dramatically reduces the search space, and thus\nthe computational load, of the EM iterations. We show by extensive numerical\nexperiments that the proposed framework can significantly accelerate EM for MRA\nin high noise levels, occasionally by a few orders of magnitude, without\ndegrading the reconstruction quality.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 07:25:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Janco", "Noam", ""], ["Bendory", "Tamir", ""]]}, {"id": "2105.07381", "submitter": "Haoyu Ma", "authors": "Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie,\n  Zhangyang Wang", "title": "Undistillable: Making A Nasty Teacher That CANNOT teach students", "comments": "ICLR 2021(Spotlight). Code is available at\n  https://github.com/VITA-Group/Nasty-Teacher", "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) is a widely used technique to transfer knowledge\nfrom pre-trained teacher models to (usually more lightweight) student models.\nHowever, in certain situations, this technique is more of a curse than a\nblessing. For instance, KD poses a potential risk of exposing intellectual\nproperties (IPs): even if a trained machine learning model is released in\n'black boxes' (e.g., as executable software or APIs without open-sourcing\ncode), it can still be replicated by KD through imitating input-output\nbehaviors. To prevent this unwanted effect of KD, this paper introduces and\ninvestigates a concept called Nasty Teacher: a specially trained teacher\nnetwork that yields nearly the same performance as a normal one, but would\nsignificantly degrade the performance of student models learned by imitating\nit. We propose a simple yet effective algorithm to build the nasty teacher,\ncalled self-undermining knowledge distillation. Specifically, we aim to\nmaximize the difference between the output of the nasty teacher and a normal\npre-trained network. Extensive experiments on several datasets demonstrate that\nour method is effective on both standard KD and data-free KD, providing the\ndesirable KD-immunity to model owners for the first time. We hope our\npreliminary study can draw more awareness and interest in this new practical\nproblem of both social and legal importance.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 08:41:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ma", "Haoyu", ""], ["Chen", "Tianlong", ""], ["Hu", "Ting-Kuei", ""], ["You", "Chenyu", ""], ["Xie", "Xiaohui", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2105.07385", "submitter": "Haruka Asanuma", "authors": "Haruka Asanuma, Shiro Takagi, Yoshihiro Nagano, Yuki Yoshida, Yasuhiko\n  Igarashi, and Masato Okada", "title": "Statistical Mechanical Analysis of Catastrophic Forgetting in Continual\n  Learning with Teacher and Student Networks", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a computational system continuously learns from an ever-changing\nenvironment, it rapidly forgets its past experiences. This phenomenon is called\ncatastrophic forgetting. While a line of studies has been proposed with respect\nto avoiding catastrophic forgetting, most of the methods are based on intuitive\ninsights into the phenomenon, and their performances have been evaluated by\nnumerical experiments using benchmark datasets. Therefore, in this study, we\nprovide the theoretical framework for analyzing catastrophic forgetting by\nusing teacher-student learning. Teacher-student learning is a framework in\nwhich we introduce two neural networks: one neural network is a target function\nin supervised learning, and the other is a learning neural network. To analyze\ncontinual learning in the teacher-student framework, we introduce the\nsimilarity of the input distribution and the input-output relationship of the\ntarget functions as the similarity of tasks. In this theoretical framework, we\nalso provide a qualitative understanding of how a single-layer linear learning\nneural network forgets tasks. Based on the analysis, we find that the network\ncan avoid catastrophic forgetting when the similarity among input distributions\nis small and that of the input-output relationship of the target functions is\nlarge. The analysis also suggests that a system often exhibits a characteristic\nphenomenon called overshoot, which means that even if the learning network has\nonce undergone catastrophic forgetting, it is possible that the network may\nperform reasonably well after further learning of the current task.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 09:02:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Asanuma", "Haruka", ""], ["Takagi", "Shiro", ""], ["Nagano", "Yoshihiro", ""], ["Yoshida", "Yuki", ""], ["Igarashi", "Yasuhiko", ""], ["Okada", "Masato", ""]]}, {"id": "2105.07407", "submitter": "Sergei Grudinin", "authors": "Elodie Laine, Stephan Eismann, Arne Elofsson, and Sergei Grudinin", "title": "Protein sequence-to-structure learning: Is this the end(-to-end\n  revolution)?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The potential of deep learning has been recognized in the protein structure\nprediction community for some time, and became indisputable after CASP13. In\nCASP14, deep learning has boosted the field to unanticipated levels reaching\nnear-experimental accuracy. This success comes from advances transferred from\nother machine learning areas, as well as methods specifically designed to deal\nwith protein sequences and structures, and their abstractions. Novel emerging\napproaches include (i) geometric learning, i.e. learning on representations\nsuch as graphs, 3D Voronoi tessellations, and point clouds; (ii) pre-trained\nprotein language models leveraging attention; (iii) equivariant architectures\npreserving the symmetry of 3D space; (iv) use of large meta-genome databases;\n(v) combinations of protein representations; (vi) and finally truly end-to-end\narchitectures, i.e. differentiable models starting from a sequence and\nreturning a 3D structure. Here, we provide an overview and our opinion of the\nnovel deep learning approaches developed in the last two years and widely used\nin CASP14.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 10:46:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Laine", "Elodie", ""], ["Eismann", "Stephan", ""], ["Elofsson", "Arne", ""], ["Grudinin", "Sergei", ""]]}, {"id": "2105.07446", "submitter": "Prem Talwai", "authors": "Prem Talwai, Ali Shameli, David Simchi-Levi", "title": "Sobolev Norm Learning Rates for Conditional Mean Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop novel learning rates for conditional mean embeddings by applying\nthe theory of interpolation for reproducing kernel Hilbert spaces (RKHS). We\nderive explicit, adaptive convergence rates for the sample estimator under the\nmisspecifed setting, where the target operator is not Hilbert-Schmidt or\nbounded with respect to the input/output RKHSs. We demonstrate that in certain\nparameter regimes, we can achieve uniform convergence rates in the output RKHS.\nWe hope our analyses will allow the much broader application of conditional\nmean embeddings to more complex ML/RL settings involving infinite dimensional\nRKHSs and continuous state spaces.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 14:43:54 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 07:33:28 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Talwai", "Prem", ""], ["Shameli", "Ali", ""], ["Simchi-Levi", "David", ""]]}, {"id": "2105.07464", "submitter": "Ning Ding", "authors": "Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie,\n  Hai-Tao Zheng, Zhiyuan Liu", "title": "Few-NERD: A Few-Shot Named Entity Recognition Dataset", "comments": "Accepted by ACL-IJCNLP 2021 (long paper), update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, considerable literature has grown up around the theme of few-shot\nnamed entity recognition (NER), but little published benchmark data\nspecifically focused on the practical and challenging task. Current approaches\ncollect existing supervised NER datasets and re-organize them to the few-shot\nsetting for empirical study. These strategies conventionally aim to recognize\ncoarse-grained entity types with few examples, while in practice, most unseen\nentity types are fine-grained. In this paper, we present Few-NERD, a\nlarge-scale human-annotated few-shot NER dataset with a hierarchy of 8\ncoarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238\nsentences from Wikipedia, 4,601,160 words are included and each is annotated as\ncontext or a part of a two-level entity type. To the best of our knowledge,\nthis is the first few-shot NER dataset and the largest human-crafted NER\ndataset. We construct benchmark tasks with different emphases to\ncomprehensively assess the generalization capability of models. Extensive\nempirical results and analysis show that Few-NERD is challenging and the\nproblem requires further research. We make Few-NERD public at\nhttps://ningding97.github.io/fewnerd/.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 15:53:17 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 08:50:14 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 06:56:03 GMT"}, {"version": "v4", "created": "Wed, 2 Jun 2021 07:23:06 GMT"}, {"version": "v5", "created": "Sun, 20 Jun 2021 14:55:18 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ding", "Ning", ""], ["Xu", "Guangwei", ""], ["Chen", "Yulin", ""], ["Wang", "Xiaobin", ""], ["Han", "Xu", ""], ["Xie", "Pengjun", ""], ["Zheng", "Hai-Tao", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "2105.07465", "submitter": "Sohil Lal Shrestha", "authors": "Sohil Lal Shrestha and Christoph Csallner", "title": "SLGPT: Using Transfer Learning to Directly Generate Simulink Model Files\n  and Find Bugs in the Simulink Toolchain", "comments": "Fixed Grammar and typo", "journal-ref": null, "doi": "10.1145/3463274.3463806", "report-no": null, "categories": "cs.SE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding bugs in a commercial cyber-physical system (CPS) development tool\nsuch as Simulink is hard as its codebase contains millions of lines of code and\ncomplete formal language specifications are not available. While deep learning\ntechniques promise to learn such language specifications from sample models,\ndeep learning needs a large number of training data to work well. SLGPT\naddresses this problem by using transfer learning to leverage the powerful\nGenerative Pre-trained Transformer 2 (GPT-2) model, which has been pre-trained\non a large set of training data. SLGPT adapts GPT-2 to Simulink with both\nrandomly generated models and models mined from open-source repositories. SLGPT\nproduced Simulink models that are both more similar to open-source models than\nits closest competitor, DeepFuzzSL, and found a super-set of the Simulink\ndevelopment toolchain bugs found by DeepFuzzSL.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 16:08:03 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 14:24:10 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Shrestha", "Sohil Lal", ""], ["Csallner", "Christoph", ""]]}, {"id": "2105.07467", "submitter": "Michael Yeung", "authors": "Michael Yeung, Evis Sala, Carola-Bibiane Sch\\\"onlieb, Leonardo Rundo", "title": "Focus U-Net: A novel dual attention-gated CNN for polyp segmentation\n  during colonoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Colonoscopy remains the gold-standard screening for colorectal\ncancer. However, significant miss rates for polyps have been reported,\nparticularly when there are multiple small adenomas. This presents an\nopportunity to leverage computer-aided systems to support clinicians and reduce\nthe number of polyps missed.\n  Method: In this work we introduce the Focus U-Net, a novel dual\nattention-gated deep neural network, which combines efficient spatial and\nchannel-based attention into a single Focus Gate module to encourage selective\nlearning of polyp features. The Focus U-Net further incorporates short-range\nskip connections and deep supervision. Furthermore, we introduce the Hybrid\nFocal loss, a new compound loss function based on the Focal loss and Focal\nTversky loss, to handle class-imbalanced image segmentation. For our\nexperiments, we selected five public datasets containing images of polyps\nobtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,\nETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we\nuse the Dice similarity coefficient (DSC) and Intersection over Union (IoU)\nmetrics.\n  Results: Our model achieves state-of-the-art results for both CVC-ClinicDB\nand Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When\nevaluated on a combination of five public polyp datasets, our model similarly\nachieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of\n0.809, a 14% and 15% improvement over the previous state-of-the-art results of\n0.768 and 0.702, respectively.\n  Conclusions: This study shows the potential for deep learning to provide fast\nand accurate polyp segmentation results for use during colonoscopy. The Focus\nU-Net may be adapted for future use in newer non-invasive screening and more\nbroadly to other biomedical image segmentation tasks involving class imbalance\nand requiring efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 16:10:32 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 11:03:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yeung", "Michael", ""], ["Sala", "Evis", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Rundo", "Leonardo", ""]]}, {"id": "2105.07485", "submitter": "Maxim Ziatdinov", "authors": "Maxim Ziatdinov, Ayana Ghosh, Tommy Wong, and Sergei V. Kalinin", "title": "AtomAI: A Deep Learning Framework for Analysis of Image and Spectroscopy\n  Data in (Scanning) Transmission Electron Microscopy and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cond-mat.dis-nn cond-mat.mtrl-sci cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AtomAI is an open-source software package bridging instrument-specific Python\nlibraries, deep learning, and simulation tools into a single ecosystem. AtomAI\nallows direct applications of the deep convolutional neural networks for atomic\nand mesoscopic image segmentation converting image and spectroscopy data into\nclass-based local descriptors for downstream tasks such as statistical and\ngraph analysis. For atomically-resolved imaging data, the output is types and\npositions of atomic species, with an option for subsequent refinement. AtomAI\nfurther allows the implementation of a broad range of image and spectrum\nanalysis functions, including invariant variational autoencoders (VAEs). The\nlatter consists of VAEs with rotational and (optionally) translational\ninvariance for unsupervised and class-conditioned disentanglement of\ncategorical and continuous data representations. In addition, AtomAI provides\nutilities for mapping structure-property relationships via im2spec and spec2im\ntype of encoder-decoder models. Finally, AtomAI allows seamless connection to\nthe first principles modeling with a Python interface, including molecular\ndynamics and density functional theory calculations on the inferred atomic\nposition. While the majority of applications to date were based on atomically\nresolved electron microscopy, the flexibility of AtomAI allows straightforward\nextension towards the analysis of mesoscopic imaging data once the labels and\nfeature identification workflows are established/available. The source code and\nexample notebooks are available at https://github.com/pycroscopy/atomai.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 17:44:59 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ziatdinov", "Maxim", ""], ["Ghosh", "Ayana", ""], ["Wong", "Tommy", ""], ["Kalinin", "Sergei V.", ""]]}, {"id": "2105.07495", "submitter": "Mahdiyar Molahasani Majdabadi", "authors": "Mahdiyar Molahasani Majdabadi and Younhee Choi and S. Deivalakshmi and\n  Seokbum Ko", "title": "Capsule GAN for Prostate MRI Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Prostate cancer is a very common disease among adult men. One in seven\nCanadian men is diagnosed with this cancer in their lifetime. Super-Resolution\n(SR) can facilitate early diagnosis and potentially save many lives. In this\npaper, a robust and accurate model is proposed for prostate MRI SR. The model\nis trained on the Prostate-Diagnosis and PROSTATEx datasets. The proposed model\noutperformed the state-of-the-art prostate SR model in all similarity metrics\nwith notable margins. A new task-specific similarity assessment is introduced\nas well. A classifier is trained for severe cancer detection and the drop in\nthe accuracy of this model when dealing with super-resolved images is used for\nevaluating the ability of medical detail reconstruction of the SR models. The\nproposed SR model is a step towards an efficient and accurate general medical\nSR platform.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 19:03:24 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 03:24:38 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Majdabadi", "Mahdiyar Molahasani", ""], ["Choi", "Younhee", ""], ["Deivalakshmi", "S.", ""], ["Ko", "Seokbum", ""]]}, {"id": "2105.07510", "submitter": "Benjamin Townsend", "authors": "Benjamin Townsend, Eamon Ito-Fisher, Lily Zhang and Madison May", "title": "Doc2Dict: Information Extraction as Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Typically, information extraction (IE) requires a pipeline approach: first, a\nsequence labeling model is trained on manually annotated documents to extract\nrelevant spans; then, when a new document arrives, a model predicts spans which\nare then post-processed and standardized to convert the information into a\ndatabase entry. We replace this labor-intensive workflow with a transformer\nlanguage model trained on existing database records to directly generate\nstructured JSON. Our solution removes the workload associated with producing\ntoken-level annotations and takes advantage of a data source which is generally\nquite plentiful (e.g. database records). As long documents are common in\ninformation extraction tasks, we use gradient checkpointing and chunked\nencoding to apply our method to sequences of up to 32,000 tokens on a single\nGPU. Our Doc2Dict approach is competitive with more complex, hand-engineered\npipelines and offers a simple but effective baseline for document-level\ninformation extraction. We release our Doc2Dict model and code to reproduce our\nexperiments and facilitate future work.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 20:46:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Townsend", "Benjamin", ""], ["Ito-Fisher", "Eamon", ""], ["Zhang", "Lily", ""], ["May", "Madison", ""]]}, {"id": "2105.07512", "submitter": "Xiao Wang", "authors": "Xiao Wang, Wei Jiang, Wei Wang, Shan Liu, Brian Kulis, Peter Chin", "title": "Substitutional Neural Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We describe Substitutional Neural Image Compression (SNIC), a general\napproach for enhancing any neural image compression model, that requires no\ndata or additional tuning of the trained model. It boosts compression\nperformance toward a flexible distortion metric and enables bit-rate control\nusing a single model instance. The key idea is to replace the image to be\ncompressed with a substitutional one that outperforms the original one in a\ndesired way. Finding such a substitute is inherently difficult for conventional\ncodecs, yet surprisingly favorable for neural compression models thanks to\ntheir fully differentiable structures. With gradients of a particular loss\nbackpropogated to the input, a desired substitute can be efficiently crafted\niteratively. We demonstrate the effectiveness of SNIC, when combined with\nvarious neural compression models and target metrics, in improving compression\nquality and performing bit-rate control measured by rate-distortion curves.\nEmpirical results of control precision and generation speed are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 20:53:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Xiao", ""], ["Jiang", "Wei", ""], ["Wang", "Wei", ""], ["Liu", "Shan", ""], ["Kulis", "Brian", ""], ["Chin", "Peter", ""]]}, {"id": "2105.07513", "submitter": "Ferdinando Fioretto", "authors": "Ferdinando Fioretto, Cuong Tran, Pascal Van Hentenryck", "title": "Decision Making with Differential Privacy under a Fairness Lens", "comments": "This paper is an extended version of the homonymous one, accepted at\n  IJCAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agencies, such as the U.S. Census Bureau, release data sets and statistics\nabout groups of individuals that are used as input to a number of critical\ndecision processes. To conform to privacy and confidentiality requirements,\nthese agencies are often required to release privacy-preserving versions of the\ndata. This paper studies the release of differentially private data sets and\nanalyzes their impact on some critical resource allocation tasks under a\nfairness perspective. {The paper shows that, when the decisions take as input\ndifferentially private data}, the noise added to achieve privacy\ndisproportionately impacts some groups over others. The paper analyzes the\nreasons for these disproportionate impacts and proposes guidelines to mitigate\nthese effects. The proposed approaches are evaluated on critical decision\nproblems that use differentially private census data.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 21:04:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Fioretto", "Ferdinando", ""], ["Tran", "Cuong", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "2105.07519", "submitter": "Xiang Deng", "authors": "Xiang Deng and Zhongfei Zhang", "title": "Graph-Free Knowledge Distillation for Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation (KD) transfers knowledge from a teacher network to a\nstudent by enforcing the student to mimic the outputs of the pretrained teacher\non training data. However, data samples are not always accessible in many cases\ndue to large data sizes, privacy, or confidentiality. Many efforts have been\nmade on addressing this problem for convolutional neural networks (CNNs) whose\ninputs lie in a grid domain within a continuous space such as images and\nvideos, but largely overlook graph neural networks (GNNs) that handle non-grid\ndata with different topology structures within a discrete space. The inherent\ndifferences between their inputs make these CNN-based approaches not applicable\nto GNNs. In this paper, we propose to our best knowledge the first dedicated\napproach to distilling knowledge from a GNN without graph data. The proposed\ngraph-free KD (GFKD) learns graph topology structures for knowledge transfer by\nmodeling them with multinomial distribution. We then introduce a gradient\nestimator to optimize this framework. Essentially, the gradients w.r.t. graph\nstructures are obtained by only using GNN forward-propagation without\nback-propagation, which means that GFKD is compatible with modern GNN libraries\nsuch as DGL and Geometric. Moreover, we provide the strategies for handling\ndifferent types of prior knowledge in the graph data or the GNNs. Extensive\nexperiments demonstrate that GFKD achieves the state-of-the-art performance for\ndistilling knowledge from GNNs without training data.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 21:38:24 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Deng", "Xiang", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "2105.07520", "submitter": "Vladimir Boza", "authors": "Vladim\\'ir Bo\\v{z}a, Peter Pere\\v{s}\\'ini, Bro\\v{n}a Brejov\\'a,\n  Tom\\'a\\v{s} Vina\\v{r}", "title": "Dynamic Pooling Improves Nanopore Base Calling Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In nanopore sequencing, electrical signal is measured as DNA molecules pass\nthrough the sequencing pores. Translating these signals into DNA bases (base\ncalling) is a highly non-trivial task, and its quality has a large impact on\nthe sequencing accuracy. The most successful nanopore base callers to date use\nconvolutional neural networks (CNN) to accomplish the task.\n  Convolutional layers in CNNs are typically composed of filters with constant\nwindow size, performing best in analysis of signals with uniform speed.\nHowever, the speed of nanopore sequencing varies greatly both within reads and\nbetween sequencing runs. Here, we present dynamic pooling, a novel neural\nnetwork component, which addresses this problem by adaptively adjusting the\npooling ratio. To demonstrate the usefulness of dynamic pooling, we developed\ntwo base callers: Heron and Osprey. Heron improves the accuracy beyond the\nexperimental high-accuracy base caller Bonito developed by Oxford Nanopore.\nOsprey is a fast base caller that can compete in accuracy with Guppy\nhigh-accuracy mode, but does not require GPU acceleration and achieves a near\nreal-time speed on common desktop CPUs.\n  Availability: https://github.com/fmfi-compbio/osprey,\nhttps://github.com/fmfi-compbio/heron\n  Keywords: nanopore sequencing, base calling, convolutional neural networks,\npooling\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 21:39:17 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bo\u017ea", "Vladim\u00edr", ""], ["Pere\u0161\u00edni", "Peter", ""], ["Brejov\u00e1", "Bro\u0148a", ""], ["Vina\u0159", "Tom\u00e1\u0161", ""]]}, {"id": "2105.07530", "submitter": "Anna Stakia", "authors": "Anna Stakia, Tommaso Dorigo, Giovanni Banelli, Daniela Bortoletto,\n  Alessandro Casa, Pablo de Castro, Christophe Delaere, Julien Donini, Livio\n  Finos, Michele Gallinaro, Andrea Giammanco, Alexander Held, Fabricio\n  Jim\\'enez Morales, Grzegorz Kotkowski, Seng Pei Liew, Fabio Maltoni, Giovanna\n  Menardi, Ioanna Papavergou, Alessia Saggio, Bruno Scarpa, Giles C. Strong,\n  Cecilia Tosciri, Jo\\~ao Varela, Pietro Vischia, Andreas Weiler", "title": "Advanced Multi-Variate Analysis Methods for New Physics Searches at the\n  Large Hadron Collider", "comments": "95 pages, 21 figures, submitted to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.LG hep-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Between the years 2015 and 2019, members of the Horizon 2020-funded\nInnovative Training Network named \"AMVA4NewPhysics\" studied the customization\nand application of advanced multivariate analysis methods and statistical\nlearning tools to high-energy physics problems, as well as developed entirely\nnew ones. Many of those methods were successfully used to improve the\nsensitivity of data analyses performed by the ATLAS and CMS experiments at the\nCERN Large Hadron Collider; several others, still in the testing phase, promise\nto further improve the precision of measurements of fundamental physics\nparameters and the reach of searches for new phenomena. In this paper, the most\nrelevant new tools, among those studied and developed, are presented along with\nthe evaluation of their performances.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 22:20:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Stakia", "Anna", ""], ["Dorigo", "Tommaso", ""], ["Banelli", "Giovanni", ""], ["Bortoletto", "Daniela", ""], ["Casa", "Alessandro", ""], ["de Castro", "Pablo", ""], ["Delaere", "Christophe", ""], ["Donini", "Julien", ""], ["Finos", "Livio", ""], ["Gallinaro", "Michele", ""], ["Giammanco", "Andrea", ""], ["Held", "Alexander", ""], ["Morales", "Fabricio Jim\u00e9nez", ""], ["Kotkowski", "Grzegorz", ""], ["Liew", "Seng Pei", ""], ["Maltoni", "Fabio", ""], ["Menardi", "Giovanna", ""], ["Papavergou", "Ioanna", ""], ["Saggio", "Alessia", ""], ["Scarpa", "Bruno", ""], ["Strong", "Giles C.", ""], ["Tosciri", "Cecilia", ""], ["Varela", "Jo\u00e3o", ""], ["Vischia", "Pietro", ""], ["Weiler", "Andreas", ""]]}, {"id": "2105.07532", "submitter": "Yang Chen", "authors": "Yang Chen, Dustin J. Kempton, Azim Ahmadzadeh and Rafal A. Angryk", "title": "Towards Synthetic Multivariate Time Series Generation for Flare\n  Forecasting", "comments": "The 20th International Conference on Artificial Intelligence and Soft\n  Computing (ICAISC02021). 13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the limiting factors in training data-driven, rare-event prediction\nalgorithms is the scarcity of the events of interest resulting in an extreme\nimbalance in the data. There have been many methods introduced in the\nliterature for overcoming this issue; simple data manipulation through\nundersampling and oversampling, utilizing cost-sensitive learning algorithms,\nor by generating synthetic data points following the distribution of the\nexisting data. While synthetic data generation has recently received a great\ndeal of attention, there are real challenges involved in doing so for\nhigh-dimensional data such as multivariate time series. In this study, we\nexplore the usefulness of the conditional generative adversarial network (CGAN)\nas a means to perform data-informed oversampling in order to balance a large\ndataset of multivariate time series. We utilize a flare forecasting benchmark\ndataset, named SWAN-SF, and design two verification methods to both\nquantitatively and qualitatively evaluate the similarity between the generated\nminority and the ground-truth samples. We further assess the quality of the\ngenerated samples by training a classical, supervised machine learning\nalgorithm on synthetic data, and testing the trained model on the unseen, real\ndata. The results show that the classifier trained on the data augmented with\nthe synthetic multivariate time series achieves a significant improvement\ncompared with the case where no augmentation is used. The popular flare\nforecasting evaluation metrics, TSS and HSS, report 20-fold and 5-fold\nimprovements, respectively, indicating the remarkable statistical similarities,\nand the usefulness of CGAN-based data generation for complicated tasks such as\nflare forecasting.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 22:23:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Yang", ""], ["Kempton", "Dustin J.", ""], ["Ahmadzadeh", "Azim", ""], ["Angryk", "Rafal A.", ""]]}, {"id": "2105.07536", "submitter": "Rong Ma", "authors": "T. Tony Cai and Rong Ma", "title": "Theoretical Foundations of t-SNE for Visualizing High-Dimensional\n  Clustered Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study investigates the theoretical foundations of t-distributed\nstochastic neighbor embedding (t-SNE), a popular nonlinear dimension reduction\nand data visualization method. A novel theoretical framework for the analysis\nof t-SNE based on the gradient descent approach is presented. For the early\nexaggeration stage of t-SNE, we show its asymptotic equivalence to a power\niteration based on the underlying graph Laplacian, characterize its limiting\nbehavior, and uncover its deep connection to Laplacian spectral clustering, and\nfundamental principles including early stopping as implicit regularization. The\nresults explain the intrinsic mechanism and the empirical benefits of such a\ncomputational strategy. For the embedding stage of t-SNE, we characterize the\nkinematics of the low-dimensional map throughout the iterations, and identify\nan amplification phase, featuring the intercluster repulsion and the expansive\nbehavior of the low-dimensional map. The general theory explains the fast\nconvergence rate and the exceptional empirical performance of t-SNE for\nvisualizing clustered data, brings forth the interpretations of the t-SNE\noutput, and provides theoretical guidance for selecting tuning parameters in\nvarious applications.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 22:43:20 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 01:04:30 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Cai", "T. Tony", ""], ["Ma", "Rong", ""]]}, {"id": "2105.07542", "submitter": "Chang Lu", "authors": "Chang Lu, Chandan K. Reddy, Prithwish Chakraborty, Samantha Kleinberg,\n  Yue Ning", "title": "Collaborative Graph Learning with Auxiliary Text for Temporal Event\n  Prediction in Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and explainable health event predictions are becoming crucial for\nhealthcare providers to develop care plans for patients. The availability of\nelectronic health records (EHR) has enabled machine learning advances in\nproviding these predictions. However, many deep learning based methods are not\nsatisfactory in solving several key challenges: 1) effectively utilizing\ndisease domain knowledge; 2) collaboratively learning representations of\npatients and diseases; and 3) incorporating unstructured text. To address these\nissues, we propose a collaborative graph learning model to explore\npatient-disease interactions and medical domain knowledge. Our solution is able\nto capture structural features of both patients and diseases. The proposed\nmodel also utilizes unstructured text data by employing an attention regulation\nstrategy and then integrates attentive text features into a sequential learning\nprocess. We conduct extensive experiments on two important healthcare problems\nto show the competitive prediction performance of the proposed method compared\nwith various state-of-the-art models. We also confirm the effectiveness of\nlearned representations and model interpretability by a set of ablation and\ncase studies.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 23:11:11 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Lu", "Chang", ""], ["Reddy", "Chandan K.", ""], ["Chakraborty", "Prithwish", ""], ["Kleinberg", "Samantha", ""], ["Ning", "Yue", ""]]}, {"id": "2105.07554", "submitter": "Laura Blattner", "authors": "Laura Blattner and Scott Nelson", "title": "How Costly is Noise? Data and Disparities in Consumer Credit", "comments": "86 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.LG q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that lenders face more uncertainty when assessing default risk of\nhistorically under-served groups in US credit markets and that this information\ndisparity is a quantitatively important driver of inefficient and unequal\ncredit market outcomes. We first document that widely used credit scores are\nstatistically noisier indicators of default risk for historically under-served\ngroups. This noise emerges primarily through the explanatory power of the\nunderlying credit report data (e.g., thin credit files), not through issues\nwith model fit (e.g., the inability to include protected class in the scoring\nmodel). Estimating a structural model of lending with heterogeneity in\ninformation, we quantify the gains from addressing these information\ndisparities for the US mortgage market. We find that equalizing the precision\nof credit scores can reduce disparities in approval rates and in credit\nmisallocation for disadvantaged groups by approximately half.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 00:42:26 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Blattner", "Laura", ""], ["Nelson", "Scott", ""]]}, {"id": "2105.07561", "submitter": "Shixiang Tang", "authors": "Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu and Wanli Ouyang", "title": "Layerwise Optimization by Gradient Decomposition for Continual Learning", "comments": "cvpr2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks achieve state-of-the-art and sometimes super-human\nperformance across various domains. However, when learning tasks sequentially,\nthe networks easily forget the knowledge of previous tasks, known as\n\"catastrophic forgetting\". To achieve the consistencies between the old tasks\nand the new task, one effective solution is to modify the gradient for update.\nPrevious methods enforce independent gradient constraints for different tasks,\nwhile we consider these gradients contain complex information, and propose to\nleverage inter-task information by gradient decomposition. In particular, the\ngradient of an old task is decomposed into a part shared by all old tasks and a\npart specific to that task. The gradient for update should be close to the\ngradient of the new task, consistent with the gradients shared by all old\ntasks, and orthogonal to the space spanned by the gradients specific to the old\ntasks. In this way, our approach encourages common knowledge consolidation\nwithout impairing the task-specific knowledge. Furthermore, the optimization is\nperformed for the gradients of each layer separately rather than the\nconcatenation of all gradients as in previous works. This effectively avoids\nthe influence of the magnitude variation of the gradients in different layers.\nExtensive experiments validate the effectiveness of both gradient-decomposed\noptimization and layer-wise updates. Our proposed method achieves\nstate-of-the-art results on various benchmarks of continual learning.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 01:15:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tang", "Shixiang", ""], ["Chen", "Dapeng", ""], ["Zhu", "Jinguo", ""], ["Yu", "Shijie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2105.07562", "submitter": "Seong-Gyu Yang", "authors": "Seong-Gyu Yang and Beom Jun Kim and Seung-Woo Son and Heetae Kim", "title": "Power-grid stability predictions using transferable machine learning", "comments": "10 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex network analyses have provided clues to improve power-grid stability\nwith the help of numerical models. The high computational cost of numerical\nsimulations, however, has inhibited the approach especially when it deals with\nthe dynamic properties of power grids such as frequency synchronization. In\nthis study, we investigate machine learning techniques to estimate the\nstability of power grid synchronization. We test three different machine\nlearning algorithms -- random forest, support vector machine, and artificial\nneural network -- training them with two different types of synthetic power\ngrids consisting of homogeneous and heterogeneous input-power distribution,\nrespectively. We find that the three machine learning models better predict the\nsynchronization stability of power-grid nodes when they are trained with the\nheterogeneous input-power distribution than the homogeneous one. With the\nreal-world power grids of Great Britain, Spain, France, and Germany, we also\ndemonstrate that the machine learning algorithms trained on synthetic power\ngrids are transferable to the stability prediction of the real-world power\ngrids, which implies the prospective applicability of machine learning\ntechniques on power-grid studies.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 01:19:01 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 07:45:11 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yang", "Seong-Gyu", ""], ["Kim", "Beom Jun", ""], ["Son", "Seung-Woo", ""], ["Kim", "Heetae", ""]]}, {"id": "2105.07566", "submitter": "Hao Xue", "authors": "Hao Xue and Flora D. Salim", "title": "Exploring Self-Supervised Representation Ensembles for COVID-19 Cough\n  Classification", "comments": "Accepted to ACM KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of smartphone-collected respiratory sound, trained with deep\nlearning models, for detecting and classifying COVID-19 becomes popular\nrecently. It removes the need for in-person testing procedures especially for\nrural regions where related medical supplies, experienced workers, and\nequipment are limited. However, existing sound-based diagnostic approaches are\ntrained in a fully supervised manner, which requires large scale well-labelled\ndata. It is critical to discover new methods to leverage unlabelled respiratory\ndata, which can be obtained more easily. In this paper, we propose a novel\nself-supervised learning enabled framework for COVID-19 cough classification. A\ncontrastive pre-training phase is introduced to train a Transformer-based\nfeature encoder with unlabelled data. Specifically, we design a random masking\nmechanism to learn robust representations of respiratory sounds. The\npre-trained feature encoder is then fine-tuned in the downstream phase to\nperform cough classification. In addition, different ensembles with varied\nrandom masking rates are also explored in the downstream phase. Through\nextensive evaluations, we demonstrate that the proposed contrastive\npre-training, the random masking mechanism, and the ensemble architecture\ncontribute to improving cough classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 01:27:20 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 06:25:29 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Xue", "Hao", ""], ["Salim", "Flora D.", ""]]}, {"id": "2105.07581", "submitter": "Pin-Yu Chen", "authors": "Sayak Paul and Pin-Yu Chen", "title": "Vision Transformers are Robust Learners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers, composed of multiple self-attention layers, hold strong\npromises toward a generic learning primitive applicable to different data\nmodalities, including the recent breakthroughs in computer vision achieving\nstate-of-the-art (SOTA) standard accuracy with better parameter efficiency.\nSince self-attention helps a model systematically align different components\npresent inside the input data, it leaves grounds to investigate its performance\nunder model robustness benchmarks. In this work, we study the robustness of the\nVision Transformer (ViT) against common corruptions and perturbations,\ndistribution shifts, and natural adversarial examples. We use six different\ndiverse ImageNet datasets concerning robust classification to conduct a\ncomprehensive performance comparison of ViT models and SOTA convolutional\nneural networks (CNNs), Big-Transfer. Through a series of six systematically\ndesigned experiments, we then present analyses that provide both quantitative\nand qualitative indications to explain why ViTs are indeed more robust\nlearners. For example, with fewer parameters and similar dataset and\npre-training combinations, ViT gives a top-1 accuracy of 28.10% on ImageNet-A\nwhich is 4.3x higher than a comparable variant of BiT. Our analyses on image\nmasking, Fourier spectrum sensitivity, and spread on discrete cosine energy\nspectrum reveal intriguing properties of ViT attributing to improved\nrobustness. Code for reproducing our experiments is available here:\nhttps://git.io/J3VO0.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 02:39:22 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 04:02:06 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Paul", "Sayak", ""], ["Chen", "Pin-Yu", ""]]}, {"id": "2105.07582", "submitter": "Alsharif Abuadbba Dr", "authors": "Keelan Evans, Alsharif Abuadbba, Mohiuddin Ahmed, Tingmin Wu, Mike\n  Johnstone, Surya Nepal", "title": "RAIDER: Reinforcement-aided Spear Phishing Detector", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spear Phishing is a harmful cyber-attack facing business and individuals\nworldwide. Considerable research has been conducted recently into the use of\nMachine Learning (ML) techniques to detect spear-phishing emails. ML-based\nsolutions may suffer from zero-day attacks; unseen attacks unaccounted for in\nthe training data. As new attacks emerge, classifiers trained on older data are\nunable to detect these new varieties of attacks resulting in increasingly\ninaccurate predictions. Spear Phishing detection also faces scalability\nchallenges due to the growth of the required features which is proportional to\nthe number of the senders within a receiver mailbox. This differs from\ntraditional phishing attacks which typically perform only a binary\nclassification between phishing and benign emails. Therefore, we devise a\npossible solution to these problems, named RAIDER: Reinforcement AIded Spear\nPhishing DEtectoR. A reinforcement-learning based feature evaluation system\nthat can automatically find the optimum features for detecting different types\nof attacks. By leveraging a reward and penalty system, RAIDER allows for\nautonomous features selection. RAIDER also keeps the number of features to a\nminimum by selecting only the significant features to represent phishing emails\nand detect spear-phishing attacks. After extensive evaluation of RAIDER over\n11,000 emails and across 3 attack scenarios, our results suggest that using\nreinforcement learning to automatically identify the significant features could\nreduce the dimensions of the required features by 55% in comparison to existing\nML-based systems. It also improves the accuracy of detecting spoofing attacks\nby 4% from 90% to 94%. In addition, RAIDER demonstrates reasonable detection\naccuracy even against a sophisticated attack named Known Sender in which\nspear-phishing emails greatly resemble those of the impersonated sender.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 02:42:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Evans", "Keelan", ""], ["Abuadbba", "Alsharif", ""], ["Ahmed", "Mohiuddin", ""], ["Wu", "Tingmin", ""], ["Johnstone", "Mike", ""], ["Nepal", "Surya", ""]]}, {"id": "2105.07592", "submitter": "Yutong Li", "authors": "Yutong Li, Ruoqing Zhu, Annie Qu and Mike Yeh", "title": "Dermoscopic Image Classification with Neural Style Transfer", "comments": "32 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skin cancer, the most commonly found human malignancy, is primarily diagnosed\nvisually via dermoscopic analysis, biopsy, and histopathological examination.\nHowever, unlike other types of cancer, automated image classification of skin\nlesions is deemed more challenging due to the irregularity and variability in\nthe lesions' appearances. In this work, we propose an adaptation of the Neural\nStyle Transfer (NST) as a novel image pre-processing step for skin lesion\nclassification problems. We represent each dermoscopic image as the style image\nand transfer the style of the lesion onto a homogeneous content image. This\ntransfers the main variability of each lesion onto the same localized region,\nwhich allows us to integrate the generated images together and extract latent,\nlow-rank style features via tensor decomposition. We train and cross-validate\nour model on a dermoscopic data set collected and preprocessed from the\nInternational Skin Imaging Collaboration (ISIC) database. We show that the\nclassification performance based on the extracted tensor features using the\nstyle-transferred images significantly outperforms that of the raw images by\nmore than 10%, and is also competitive with well-studied, pre-trained CNN\nmodels through transfer learning. Additionally, the tensor decomposition\nfurther identifies latent style clusters, which may provide clinical\ninterpretation and insights.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 03:50:51 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 04:42:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Yutong", ""], ["Zhu", "Ruoqing", ""], ["Qu", "Annie", ""], ["Yeh", "Mike", ""]]}, {"id": "2105.07593", "submitter": "Peter Karkus", "authors": "Peter Karkus, Shaojun Cai, David Hsu", "title": "Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation", "comments": "CVPR 2021, extended results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simultaneous localization and mapping (SLAM) remains challenging for a number\nof downstream applications, such as visual robot navigation, because of rapid\nturns, featureless walls, and poor camera quality. We introduce the\nDifferentiable SLAM Network (SLAM-net) along with a navigation architecture to\nenable planar robot navigation in previously unseen indoor environments.\nSLAM-net encodes a particle filter based SLAM algorithm in a differentiable\ncomputation graph, and learns task-oriented neural network components by\nbackpropagating through the SLAM algorithm. Because it can optimize all model\ncomponents jointly for the end-objective, SLAM-net learns to be robust in\nchallenging conditions. We run experiments in the Habitat platform with\ndifferent real-world RGB and RGB-D datasets. SLAM-net significantly outperforms\nthe widely adapted ORB-SLAM in noisy conditions. Our navigation architecture\nwith SLAM-net improves the state-of-the-art for the Habitat Challenge 2020\nPointNav task by a large margin (37% to 64% success). Project website:\nhttp://sites.google.com/view/slamnet\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 03:54:34 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 14:12:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Karkus", "Peter", ""], ["Cai", "Shaojun", ""], ["Hsu", "David", ""]]}, {"id": "2105.07599", "submitter": "Feng Bao", "authors": "Feng Bao", "title": "Disentangled Variational Information Bottleneck for Multiview\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiview data contain information from multiple modalities and have\npotentials to provide more comprehensive features for diverse machine learning\ntasks. A fundamental question in multiview analysis is what is the additional\ninformation brought by additional views and can quantitatively identify this\nadditional information. In this work, we try to tackle this challenge by\ndecomposing the entangled multiview features into shared latent representations\nthat are common across all views and private representations that are specific\nto each single view. We formulate this feature disentanglement in the framework\nof information bottleneck and propose disentangled variational information\nbottleneck (DVIB). DVIB explicitly defines the properties of shared and private\nrepresentations using constrains from mutual information. By deriving\nvariational upper and lower bounds of mutual information terms, representations\nare efficiently optimized. We demonstrate the shared and private\nrepresentations learned by DVIB well preserve the common labels shared between\ntwo views and unique labels corresponding to each single view, respectively.\nDVIB also shows comparable performance in classification task on images with\ncorruptions. DVIB implementation is available at\nhttps://github.com/feng-bao-ucsf/DVIB.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 04:03:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bao", "Feng", ""]]}, {"id": "2105.07603", "submitter": "Weiming Zhuang", "authors": "Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang", "title": "EasyFL: A Low-code Federated Learning Platform For Dummies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academia and industry have developed several platforms to support the popular\nprivacy-preserving distributed learning method -- Federated Learning (FL).\nHowever, these platforms are complex to use and require a deep understanding of\nFL, which imposes high barriers to entry for beginners, limits the productivity\nof researchers, and compromises deployment efficiency. In this paper, we\npropose the first low-code FL platform, EasyFL, to enable users with various\nlevels of expertise to experiment and prototype FL applications with little\ncoding. We achieve this goal while ensuring great flexibility and extensibility\nfor customization by unifying simple API design, modular design, and granular\ntraining flow abstraction. With only a few lines of code, EasyFL empowers them\nwith many out-of-the-box functionalities to accelerate experimentation and\ndeployment. These practical functionalities are heterogeneity simulation,\ncomprehensive tracking, distributed training optimization, and seamless\ndeployment. They are proposed based on challenges identified in the proposed FL\nlife cycle. Compared with other platforms, EasyFL not only requires just three\nlines of code (at least 10x lesser) to build a vanilla FL application but also\nincurs lower training overhead. Besides, our evaluations demonstrate that\nEasyFL expedites distributed training by 1.5x. It also improves the efficiency\nof deployment. We believe that EasyFL will increase the productivity of\nresearchers and democratize FL to wider audiences.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 04:15:55 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 13:18:23 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhuang", "Weiming", ""], ["Gan", "Xin", ""], ["Wen", "Yonggang", ""], ["Zhang", "Shuai", ""]]}, {"id": "2105.07606", "submitter": "Weiming Zhuang", "authors": "Weiming Zhuang, Xin Gan, Yonggang Wen, Xuesen Zhang, Shuai Zhang,\n  Shuai Yi", "title": "Towards Unsupervised Domain Adaptation for Deep Face Recognition under\n  Privacy Constraints via Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation has been widely adopted to generalize models\nfor unlabeled data in a target domain, given labeled data in a source domain,\nwhose data distributions differ from the target domain. However, existing works\nare inapplicable to face recognition under privacy constraints because they\nrequire sharing sensitive face images between two domains. To address this\nproblem, we propose a novel unsupervised federated face recognition approach\n(FedFR). FedFR improves the performance in the target domain by iteratively\naggregating knowledge from the source domain through federated learning. It\nprotects data privacy by transferring models instead of raw data between\ndomains. Besides, we propose a new domain constraint loss (DCL) to regularize\nsource domain training. DCL suppresses the data volume dominance of the source\ndomain. We also enhance a hierarchical clustering algorithm to predict pseudo\nlabels for the unlabeled target domain accurately. To this end, FedFR forms an\nend-to-end training pipeline: (1) pre-train in the source domain; (2) predict\npseudo labels by clustering in the target domain; (3) conduct\ndomain-constrained federated learning across two domains. Extensive experiments\nand analysis on two newly constructed benchmarks demonstrate the effectiveness\nof FedFR. It outperforms the baseline and classic methods in the target domain\nby over 4% on the more realistic benchmark. We believe that FedFR will shed\nlight on applying federated learning to more computer vision tasks under\nprivacy constraints.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 04:24:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhuang", "Weiming", ""], ["Gan", "Xin", ""], ["Wen", "Yonggang", ""], ["Zhang", "Xuesen", ""], ["Zhang", "Shuai", ""], ["Yi", "Shuai", ""]]}, {"id": "2105.07610", "submitter": "Maya Ramchandran", "authors": "Maya Ramchandran, Rajarshi Mukherjee, and Giovanni Parmigiani", "title": "Cross-Cluster Weighted Forests", "comments": "20 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adapting machine learning algorithms to better handle the presence of natural\nclustering or batch effects within training datasets is imperative across a\nwide variety of biological applications. This article considers the effect of\nensembling Random Forest learners trained on clusters within a single dataset\nwith heterogeneity in the distribution of the features. We find that\nconstructing ensembles of forests trained on clusters determined by algorithms\nsuch as k-means results in significant improvements in accuracy and\ngeneralizability over the traditional Random Forest algorithm. We denote our\nnovel approach as the Cross-Cluster Weighted Forest, and examine its robustness\nto various data-generating scenarios and outcome models. Furthermore, we\nexplore the influence of the data-partitioning and ensemble weighting\nstrategies on conferring the benefits of our method over the existing paradigm.\nFinally, we apply our approach to cancer molecular profiling and gene\nexpression datasets that are naturally divisible into clusters and illustrate\nthat our approach outperforms classic Random Forest. Code and supplementary\nmaterial are available at https://github.com/m-ramchandran/cross-cluster.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 04:58:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ramchandran", "Maya", ""], ["Mukherjee", "Rajarshi", ""], ["Parmigiani", "Giovanni", ""]]}, {"id": "2105.07615", "submitter": "Haoran Li", "authors": "Hao Peng, Haoran Li, Yangqiu Song, Vincent Zheng, Jianxin Li", "title": "Federated Knowledge Graphs Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel decentralized scalable learning framework,\nFederated Knowledge Graphs Embedding (FKGE), where embeddings from different\nknowledge graphs can be learnt in an asynchronous and peer-to-peer manner while\nbeing privacy-preserving. FKGE exploits adversarial generation between pairs of\nknowledge graphs to translate identical entities and relations of different\ndomains into near embedding spaces. In order to protect the privacy of the\ntraining data, FKGE further implements a privacy-preserving neural network\nstructure to guarantee no raw data leakage. We conduct extensive experiments to\nevaluate FKGE on 11 knowledge graphs, demonstrating a significant and\nconsistent improvement in model quality with at most 17.85% and 7.90% increases\nin performance on triple classification and link prediction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 05:30:41 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Peng", "Hao", ""], ["Li", "Haoran", ""], ["Song", "Yangqiu", ""], ["Zheng", "Vincent", ""], ["Li", "Jianxin", ""]]}, {"id": "2105.07627", "submitter": "Subhankar Ghosh", "authors": "Subhankar Ghosh", "title": "Shared and Private VAEs with Generative Replay for Continual Learning", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning tries to learn new tasks without forgetting previously\nlearned ones. In reality, most of the existing artificial neural network(ANN)\nmodels fail, while humans do the same by remembering previous works throughout\ntheir life. Although simply storing all past data can alleviate the problem, it\nneeds large memory and often infeasible in real-world applications where last\ndata access is limited. We hypothesize that the model that learns to solve each\ntask continually has some task-specific properties and some task-invariant\ncharacteristics. We propose a hybrid continual learning model that is more\nsuitable in real case scenarios to address the issues that has a task-invariant\nshared variational autoencoder and T task-specific variational autoencoders.\nOur model combines generative replay and architectural growth to prevent\ncatastrophic forgetting. We show our hybrid model effectively avoids forgetting\nand achieves state-of-the-art results on visual continual learning benchmarks\nsuch as MNIST, Permuted MNIST(QMNIST), CIFAR100, and miniImageNet datasets. We\ndiscuss results on a few more datasets, such as SVHN, Fashion-MNIST, EMNIST,\nand CIFAR10.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:18:36 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ghosh", "Subhankar", ""]]}, {"id": "2105.07630", "submitter": "Andr\\'e Artelt", "authors": "Andr\\'e Artelt and Barbara Hammer", "title": "Convex optimization for actionable \\& plausible counterfactual\n  explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparency is an essential requirement of machine learning based decision\nmaking systems that are deployed in real world. Often, transparency of a given\nsystem is achieved by providing explanations of the behavior and predictions of\nthe given system. Counterfactual explanations are a prominent instance of\nparticular intuitive explanations of decision making systems. While a lot of\ndifferent methods for computing counterfactual explanations exist, only very\nfew work (apart from work from the causality domain) considers feature\ndependencies as well as plausibility which might limit the set of possible\ncounterfactual explanations.\n  In this work we enhance our previous work on convex modeling for computing\ncounterfactual explanations by a mechanism for ensuring actionability and\nplausibility of the resulting counterfactual explanations.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:33:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Artelt", "Andr\u00e9", ""], ["Hammer", "Barbara", ""]]}, {"id": "2105.07634", "submitter": "Sunil Kumar Maurya", "authors": "Sunil Kumar Maurya, Xin Liu and Tsuyoshi Murata", "title": "Improving Graph Neural Networks with Simple Architecture Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks have emerged as a useful tool to learn on the data by\napplying additional constraints based on the graph structure. These graphs are\noften created with assumed intrinsic relations between the entities. In recent\nyears, there have been tremendous improvements in the architecture design,\npushing the performance up in various prediction tasks. In general, these\nneural architectures combine layer depth and node feature aggregation steps.\nThis makes it challenging to analyze the importance of features at various hops\nand the expressiveness of the neural network layers. As different graph\ndatasets show varying levels of homophily and heterophily in features and class\nlabel distribution, it becomes essential to understand which features are\nimportant for the prediction tasks without any prior information. In this work,\nwe decouple the node feature aggregation step and depth of graph neural network\nand introduce several key design strategies for graph neural networks. More\nspecifically, we propose to use softmax as a regularizer and \"Soft-Selector\" of\nfeatures aggregated from neighbors at different hop distances; and\n\"Hop-Normalization\" over GNN layers. Combining these techniques, we present a\nsimple and shallow model, Feature Selection Graph Neural Network (FSGNN), and\nshow empirically that the proposed model outperforms other state of the art GNN\nmodels and achieves up to 64% improvements in accuracy on node classification\ntasks. Moreover, analyzing the learned soft-selection parameters of the model\nprovides a simple way to study the importance of features in the prediction\ntasks. Finally, we demonstrate with experiments that the model is scalable for\nlarge graphs with millions of nodes and billions of edges.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:46:01 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Maurya", "Sunil Kumar", ""], ["Liu", "Xin", ""], ["Murata", "Tsuyoshi", ""]]}, {"id": "2105.07636", "submitter": "Sauptik Dhar", "authors": "Sauptik Dhar, Bernardo Gonzalez Torres", "title": "DOC3-Deep One Class Classification using Contradictions", "comments": "Deep Learning, Anomaly Detection, Visual Inspection, Learning from\n  Contradictions, Outlier Exposure, 18 pages, 14 tables, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces the notion of learning from contradictions (a.k.a\nUniversum learning) for deep one class classification problems. We formalize\nthis notion for the widely adopted one class large-margin loss, and propose the\nDeep One Class Classification using Contradictions (DOC3) algorithm. We show\nthat learning from contradictions incurs lower generalization error by\ncomparing the Empirical Radamacher Complexity (ERC) of DOC3 against its\ntraditional inductive learning counterpart. Our empirical results demonstrate\nthe efficacy of DOC3 algorithm achieving > 30% for CIFAR-10 and >50% for MV-Tec\nAD data sets in test AUCs compared to its inductive learning counterpart and in\nmany cases improving the state-of-the-art in anomaly detection.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:48:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dhar", "Sauptik", ""], ["Torres", "Bernardo Gonzalez", ""]]}, {"id": "2105.07659", "submitter": "Nikita Bhandari", "authors": "Nikita Bhandari, Satyajeet Khare, Rahee Walambe, Ketan Kotecha", "title": "Comparison of machine learning and deep learning techniques in promoter\n  prediction across diverse species", "comments": "17 pages, 4 figures, 4 tables", "journal-ref": "PeerJ Comput. Sci. 7:e365 (2021)", "doi": "10.7717/peerj-cs.365", "report-no": null, "categories": "q-bio.GN cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gene promoters are the key DNA regulatory elements positioned around the\ntranscription start sites and are responsible for regulating gene transcription\nprocess. Various alignment-based, signal-based and content-based approaches are\nreported for the prediction of promoters. However, since all promoter sequences\ndo not show explicit features, the prediction performance of these techniques\nis poor. Therefore, many machine learning and deep learning models have been\nproposed for promoter prediction. In this work, we studied methods for vector\nencoding and promoter classification using genome sequences of three distinct\nhigher eukaryotes viz. yeast (Saccharomyces cerevisiae), A. thaliana (plant)\nand human (Homo sapiens). We compared one-hot vector encoding method with\nfrequency-based tokenization (FBT) for data pre-processing on 1-D Convolutional\nNeural Network (CNN) model. We found that FBT gives a shorter input dimension\nreducing the training time without affecting the sensitivity and specificity of\nclassification. We employed the deep learning techniques, mainly CNN and\nrecurrent neural network with Long Short Term Memory (LSTM) and random forest\n(RF) classifier for promoter classification at k-mer sizes of 2, 4 and 8. We\nfound CNN to be superior in classification of promoters from non-promoter\nsequences (binary classification) as well as species-specific classification of\npromoter sequences (multiclass classification). In summary, the contribution of\nthis work lies in the use of synthetic shuffled negative dataset and\nfrequency-based tokenization for pre-processing. This study provides a\ncomprehensive and generic framework for classification tasks in genomic\napplications and can be extended to various classification problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:15:41 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bhandari", "Nikita", ""], ["Khare", "Satyajeet", ""], ["Walambe", "Rahee", ""], ["Kotecha", "Ketan", ""]]}, {"id": "2105.07668", "submitter": "Alexander von Rohr", "authors": "Alexander von Rohr, Matthias Neumann-Brosig, Sebastian Trimpe", "title": "Probabilistic robust linear quadratic regulators with Gaussian processes", "comments": "to be published in the proceedings of the 3rd Conference on Learning\n  for Dynamics and Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models such as Gaussian processes (GPs) are powerful tools to\nlearn unknown dynamical systems from data for subsequent use in control design.\nWhile learning-based control has the potential to yield superior performance in\ndemanding applications, robustness to uncertainty remains an important\nchallenge. Since Bayesian methods quantify uncertainty of the learning results,\nit is natural to incorporate these uncertainties into a robust design. In\ncontrast to most state-of-the-art approaches that consider worst-case\nestimates, we leverage the learning method's posterior distribution in the\ncontroller synthesis. The result is a more informed and, thus, more efficient\ntrade-off between performance and robustness. We present a novel controller\nsynthesis for linearized GP dynamics that yields robust controllers with\nrespect to a probabilistic stability margin. The formulation is based on a\nrecently proposed algorithm for linear quadratic control synthesis, which we\nextend by giving probabilistic robustness guarantees in the form of credibility\nbounds for the system's stability.Comparisons to existing methods based on\nworst-case and certainty-equivalence designs reveal superior performance and\nrobustness properties of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:36:18 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["von Rohr", "Alexander", ""], ["Neumann-Brosig", "Matthias", ""], ["Trimpe", "Sebastian", ""]]}, {"id": "2105.07671", "submitter": "Petra Posedel \\v{S}imovi\\'c", "authors": "Petra Posedel \\v{S}imovi\\'c, Davor Horvatic, Edward W. Sun", "title": "Classifying variety of customer's online engagement for churn prediction\n  with mixed-penalty logistic regression", "comments": "This version is not sufficiently exhaustive; a wrong version of\n  validation results has been released (using a wrong part of a dataset for\n  validation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using big data to analyze consumer behavior can provide effective\ndecision-making tools for preventing customer attrition (churn) in customer\nrelationship management (CRM). Focusing on a CRM dataset with several different\ncategories of factors that impact customer heterogeneity (i.e., usage of\nself-care service channels, duration of service, and responsiveness to\nmarketing actions), we provide new predictive analytics of customer churn rate\nbased on a machine learning method that enhances the classification of logistic\nregression by adding a mixed penalty term. The proposed penalized logistic\nregression can prevent overfitting when dealing with big data and minimize the\nloss function when balancing the cost from the median (absolute value) and mean\n(squared value) regularization. We show the analytical properties of the\nproposed method and its computational advantage in this research. In addition,\nwe investigate the performance of the proposed method with a CRM data set (that\nhas a large number of features) under different settings by efficiently\neliminating the disturbance of (1) least important features and (2) sensitivity\nfrom the minority (churn) class. Our empirical results confirm the expected\nperformance of the proposed method in full compliance with the common\nclassification criteria (i.e., accuracy, precision, and recall) for evaluating\nmachine learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:40:34 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 15:25:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["\u0160imovi\u0107", "Petra Posedel", ""], ["Horvatic", "Davor", ""], ["Sun", "Edward W.", ""]]}, {"id": "2105.07674", "submitter": "Andrea Cossu", "authors": "Andrea Cossu, Davide Bacciu, Antonio Carta, Claudio Gallicchio,\n  Vincenzo Lomonaco", "title": "Continual Learning with Echo State Networks", "comments": "Accepted as oral at ESANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual Learning (CL) refers to a learning setup where data is non\nstationary and the model has to learn without forgetting existing knowledge.\nThe study of CL for sequential patterns revolves around trained recurrent\nnetworks. In this work, instead, we introduce CL in the context of Echo State\nNetworks (ESNs), where the recurrent component is kept fixed. We provide the\nfirst evaluation of catastrophic forgetting in ESNs and we highlight the\nbenefits in using CL strategies which are not applicable to trained recurrent\nmodels. Our results confirm the ESN as a promising model for CL and open to its\nuse in streaming scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:49:01 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 12:26:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Cossu", "Andrea", ""], ["Bacciu", "Davide", ""], ["Carta", "Antonio", ""], ["Gallicchio", "Claudio", ""], ["Lomonaco", "Vincenzo", ""]]}, {"id": "2105.07688", "submitter": "Yuejia Xiang", "authors": "Yuejia Xiang, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Zhenxi Lin, Yefeng\n  Zheng", "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph\n  Embedding", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic embedding has been widely investigated for aligning knowledge graph\n(KG) entities. Current methods have explored and utilized the graph structure,\nthe entity names and attributes, but ignore the ontology (or ontological\nschema) which contains critical meta information such as classes and their\nmembership relationships with entities. In this paper, we propose an\nontology-guided entity alignment method named OntoEA, where both KGs and their\nontologies are jointly embedded, and the class hierarchy and the class\ndisjointness are utilized to avoid false mappings. Extensive experiments on\nseven public and industrial benchmarks have demonstrated the state-of-the-art\nperformance of OntoEA and the effectiveness of the ontologies.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 09:18:56 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 08:45:26 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Xiang", "Yuejia", ""], ["Zhang", "Ziheng", ""], ["Chen", "Jiaoyan", ""], ["Chen", "Xi", ""], ["Lin", "Zhenxi", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2105.07693", "submitter": "Joe Watson", "authors": "Joe Watson, Hany Abdulsamad, Rolf Findeisen and Jan Peters", "title": "Stochastic Control through Approximate Bayesian Input Inference", "comments": "Submitted to Transactions on Automatic Control Special Issue:\n  Learning and Control. This work has been submitted to the IEEE for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal control under uncertainty is a prevailing challenge in control, due\nto the difficulty in producing tractable solutions for the stochastic\noptimization problem. By framing the control problem as one of input\nestimation, advanced approximate inference techniques can be used to handle the\nstatistical approximations in a principled and practical manner. Analyzing the\nGaussian setting, we present a solver capable of several stochastic control\nmethods, and was found to be superior to popular baselines on nonlinear\nsimulated tasks. We draw connections that relate this inference formulation to\nprevious approaches for stochastic optimal control, and outline several\nadvantages that this inference view brings due to its statistical nature.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 09:27:12 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Watson", "Joe", ""], ["Abdulsamad", "Hany", ""], ["Findeisen", "Rolf", ""], ["Peters", "Jan", ""]]}, {"id": "2105.07698", "submitter": "Casper Hansen", "authors": "Casper Hansen and Christian Hansen and Lucas Chaves Lima", "title": "Automatic Fake News Detection: Are Models Learning to Reason?", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most fact checking models for automatic fake news detection are based on\nreasoning: given a claim with associated evidence, the models aim to estimate\nthe claim veracity based on the supporting or refuting content within the\nevidence. When these models perform well, it is generally assumed to be due to\nthe models having learned to reason over the evidence with regards to the\nclaim. In this paper, we investigate this assumption of reasoning, by exploring\nthe relationship and importance of both claim and evidence. Surprisingly, we\nfind on political fact checking datasets that most often the highest\neffectiveness is obtained by utilizing only the evidence, as the impact of\nincluding the claim is either negligible or harmful to the effectiveness. This\nhighlights an important problem in what constitutes evidence in existing\napproaches for automatic fake news detection.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 09:34:03 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["Lima", "Lucas Chaves", ""]]}, {"id": "2105.07729", "submitter": "Vinicius Luiz Santos Silva", "authors": "Vinicius L. S. Silva, Claire E. Heaney, Yaqi Li, Christopher C. Pain", "title": "Data Assimilation Predictive GAN (DA-PredGAN): applied to determine the\n  spread of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the novel use of a generative adversarial network (GAN) (i) to\nmake predictions in time (PredGAN) and (ii) to assimilate measurements\n(DA-PredGAN). In the latter case, we take advantage of the natural adjoint-like\nproperties of generative models and the ability to simulate forwards and\nbackwards in time. GANs have received much attention recently, after achieving\nexcellent results for their generation of realistic-looking images. We wish to\nexplore how this property translates to new applications in computational\nmodelling and to exploit the adjoint-like properties for efficient data\nassimilation. To predict the spread of COVID-19 in an idealised town, we apply\nthese methods to a compartmental model in epidemiology that is able to model\nspace and time variations. To do this, the GAN is set within a reduced-order\nmodel (ROM), which uses a low-dimensional space for the spatial distribution of\nthe simulation states. Then the GAN learns the evolution of the low-dimensional\nstates over time. The results show that the proposed methods can accurately\npredict the evolution of the high-fidelity numerical simulation, and can\nefficiently assimilate observed data and determine the corresponding model\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 10:56:53 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 14:55:29 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Silva", "Vinicius L. S.", ""], ["Heaney", "Claire E.", ""], ["Li", "Yaqi", ""], ["Pain", "Christopher C.", ""]]}, {"id": "2105.07730", "submitter": "Drishti Jain", "authors": "Drishti Jain (1), Tavpritesh Sethi (1) ((1) Indraprastha Institute of\n  Information Technology)", "title": "The State of Infodemic on Twitter", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following the wave of misinterpreted, manipulated and malicious information\ngrowing on the Internet, the misinformation surrounding COVID-19 has become a\nparamount issue. In the context of the current COVID-19 pandemic, social media\nposts and platforms are at risk of rumors and misinformation in the face of the\nserious uncertainty surrounding the virus itself. At the same time, the\nuncertainty and new nature of COVID-19 means that other unconfirmed information\nthat may appear \"rumored\" may be an important indicator of the behavior and\nimpact of this new virus. Twitter, in particular, has taken a center stage in\nthis storm where Covid-19 has been a much talked about subject. We have\npresented an exploratory analysis of the tweets and the users who are involved\nin spreading misinformation and then delved into machine learning models and\nnatural language processing techniques to identify if a tweet contains\nmisinformation.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 10:58:35 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Jain", "Drishti", ""], ["Sethi", "Tavpritesh", ""]]}, {"id": "2105.07733", "submitter": "Julian Rasch", "authors": "Julian Rasch and David Middelbeck", "title": "Knowledge State Networks for Effective Skill Assessment in Atomic\n  Learning", "comments": "submitted to JEDM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to introduce a new framework for fast and effective\nknowledge state assessments in the context of personalized, skill-based online\nlearning. We use knowledge state networks - specific neural networks trained on\nassessment data of previous learners - to predict the full knowledge state of\nother learners from only partial information about their skills. In combination\nwith a matching assessment strategy for asking discriminative questions we\ndemonstrate that our approach leads to a significant speed-up of the assessment\nprocess - in terms of the necessary number of assessment questions - in\ncomparison to standard assessment designs. In practice, the presented methods\nenable personalized, skill-based online learning also for skill ontologies of\nvery fine granularity without deteriorating the associated learning experience\nby a lengthy assessment process.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:05:59 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Rasch", "Julian", ""], ["Middelbeck", "David", ""]]}, {"id": "2105.07741", "submitter": "Michael Murray", "authors": "Michael Murray, Vinayak Abrol, Jared Tanner", "title": "Activation function design for deep networks: linearity and effective\n  initialisation", "comments": "33 pages, 10 figures, paper code and scripts are hosted at\n  https://github.com/Cross-Caps/AFLI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The activation function deployed in a deep neural network has great influence\non the performance of the network at initialisation, which in turn has\nimplications for training. In this paper we study how to avoid two problems at\ninitialisation identified in prior works: rapid convergence of pairwise input\ncorrelations, and vanishing and exploding gradients. We prove that both these\nproblems can be avoided by choosing an activation function possessing a\nsufficiently large linear region around the origin, relative to the bias\nvariance $\\sigma_b^2$ of the network's random initialisation. We demonstrate\nempirically that using such activation functions leads to tangible benefits in\npractice, both in terms test and training accuracy as well as training time.\nFurthermore, we observe that the shape of the nonlinear activation outside the\nlinear region appears to have a relatively limited impact on training. Finally,\nour results also allow us to train networks in a new hyperparameter regime,\nwith a much larger bias variance than has previously been possible.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:30:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Murray", "Michael", ""], ["Abrol", "Vinayak", ""], ["Tanner", "Jared", ""]]}, {"id": "2105.07743", "submitter": "Anastasis Kratsios", "authors": "Anastasis Kratsios", "title": "Universal Regular Conditional Distributions", "comments": "Keywords: Universal Regular Conditional Distributions, Geometric Deep\n  Learning, Measure-Valued Neural Networks, Conditional Expectation,\n  Uncertainty Quantification. Additional Information: 27 Pages + 22 Page\n  Appendix, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.MG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for approximating regular conditional\ndistributions (RCDs). Our approximations of these RCDs are implemented by a new\nclass of geometric deep learning models with inputs in $\\mathbb{R}^d$ and\noutputs in the Wasserstein-$1$ space $\\mathcal{P}_1(\\mathbb{R}^D)$. We find\nthat the models built using our framework can approximate any continuous\nfunctions from $\\mathbb{R}^d$ to $\\mathcal{P}_1(\\mathbb{R}^D)$ uniformly on\ncompacts, and quantitative rates are obtained. We identify two methods for\navoiding the \"curse of dimensionality\"; i.e.: the number of parameters\ndetermining the approximating neural network depends only polynomially on the\ninvolved dimension and the approximation error. The first solution describes\nfunctions in $C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ which can be\nefficiently approximated on any compact subset of $\\mathbb{R}^d$. Conversely,\nthe second approach describes sets in $\\mathbb{R}^d$, on which any function in\n$C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ can be efficiently approximated.\nOur framework is used to obtain an affirmative answer to the open conjecture of\nBishop (1994); namely: mixture density networks are universal regular\nconditional distributions. The predictive performance of the proposed models is\nevaluated against comparable learning models on various probabilistic\npredictions tasks in the context of ELMs, model uncertainty, and\nheteroscedastic regression. All the results are obtained for more general input\nand output spaces and thus apply to geometric deep learning contexts.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:34:09 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 17:51:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kratsios", "Anastasis", ""]]}, {"id": "2105.07752", "submitter": "Bencheng Yan", "authors": "Feng Li, Bencheng Yan, Qingqing Long, Pengjie Wang, Wei Lin, Jian Xu\n  and Bo Zheng", "title": "Explicit Semantic Cross Feature Learning via Pre-trained Graph Neural\n  Networks for CTR Prediction", "comments": "SIGIR 2021, 5 pages; The first two authors contributed equally to\n  this work; Pengjie Wang gave a lot of guidance in this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross features play an important role in click-through rate (CTR) prediction.\nMost of the existing methods adopt a DNN-based model to capture the cross\nfeatures in an implicit manner. These implicit methods may lead to a\nsub-optimized performance due to the limitation in explicit semantic modeling.\nAlthough traditional statistical explicit semantic cross features can address\nthe problem in these implicit methods, it still suffers from some challenges,\nincluding lack of generalization and expensive memory cost. Few works focus on\ntackling these challenges. In this paper, we take the first step in learning\nthe explicit semantic cross features and propose Pre-trained Cross Feature\nlearning Graph Neural Networks (PCF-GNN), a GNN based pre-trained model aiming\nat generating cross features in an explicit fashion. Extensive experiments are\nconducted on both public and industrial datasets, where PCF-GNN shows\ncompetence in both performance and memory-efficiency in various tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:56:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Feng", ""], ["Yan", "Bencheng", ""], ["Long", "Qingqing", ""], ["Wang", "Pengjie", ""], ["Lin", "Wei", ""], ["Xu", "Jian", ""], ["Zheng", "Bo", ""]]}, {"id": "2105.07753", "submitter": "Kimiaki Shirahama", "authors": "Kazuma Fujioka and Kimiaki Shirahama", "title": "Generic Itemset Mining Based on Reinforcement Learning", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest problems in itemset mining is the requirement of\ndeveloping a data structure or algorithm, every time a user wants to extract a\ndifferent type of itemsets. To overcome this, we propose a method, called\nGeneric Itemset Mining based on Reinforcement Learning (GIM-RL), that offers a\nunified framework to train an agent for extracting any type of itemsets. In\nGIM-RL, the environment formulates iterative steps of extracting a target type\nof itemsets from a dataset. At each step, an agent performs an action to add or\nremove an item to or from the current itemset, and then obtains from the\nenvironment a reward that represents how relevant the itemset resulting from\nthe action is to the target type. Through numerous trial-and-error steps where\nvarious rewards are obtained by diverse actions, the agent is trained to\nmaximise cumulative rewards so that it acquires the optimal action policy for\nforming as many itemsets of the target type as possible. In this framework, an\nagent for extracting any type of itemsets can be trained as long as a reward\nsuitable for the type can be defined. The extensive experiments on mining high\nutility itemsets, frequent itemsets and association rules show the general\neffectiveness and one remarkable potential (agent transfer) of GIM-RL. We hope\nthat GIM-RL opens a new research direction towards learning-based itemset\nmining.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:57:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Fujioka", "Kazuma", ""], ["Shirahama", "Kimiaki", ""]]}, {"id": "2105.07754", "submitter": "Xinjian Luo", "authors": "Xinjian Luo, Xiaokui Xiao, Yuncheng Wu, Juncheng Liu, Beng Chin Ooi", "title": "A Fusion-Denoising Attack on InstaHide with Data Augmentation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  InstaHide is a state-of-the-art mechanism for protecting private training\nimages in collaborative learning. It works by mixing multiple private images\nand modifying them in such a way that their visual features are no longer\ndistinguishable to the naked eye, without significantly degrading the accuracy\nof training. In recent work, however, Carlini et al. show that it is possible\nto reconstruct private images from the encrypted dataset generated by\nInstaHide, by exploiting the correlations among the encrypted images.\nNevertheless, Carlini et al.'s attack relies on the assumption that each\nprivate image is used without modification when mixing up with other private\nimages. As a consequence, it could be easily defeated by incorporating data\naugmentation into InstaHide. This leads to a natural question: is InstaHide\nwith data augmentation secure?\n  This paper provides a negative answer to the above question, by present an\nattack for recovering private images from the outputs of InstaHide even when\ndata augmentation is present. The basic idea of our attack is to use a\ncomparative network to identify encrypted images that are likely to correspond\nto the same private image, and then employ a fusion-denoising network for\nrestoring the private image from the encrypted ones, taking into account the\neffects of data augmentation. Extensive experiments demonstrate the\neffectiveness of the proposed attack in comparison to Carlini et al.'s attack.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:58:16 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Luo", "Xinjian", ""], ["Xiao", "Xiaokui", ""], ["Wu", "Yuncheng", ""], ["Liu", "Juncheng", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2105.07758", "submitter": "Wang-Zhou Dai", "authors": "Wang-Zhou Dai, Liam Hallett, Stephen H. Muggleton, Geoff S. Baldwin", "title": "Automated Biodesign Engineering by Abductive Meta-Interpretive Learning", "comments": "Accepted by SSS-21 (AAAI Spring Symposium Series 2021), Artificial\n  Intelligence for Synthetic Biology (AI4Synbio) track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The application of Artificial Intelligence (AI) to synthetic biology will\nprovide the foundation for the creation of a high throughput automated platform\nfor genetic design, in which a learning machine is used to iteratively optimise\nthe system through a design-build-test-learn (DBTL) cycle. However, mainstream\nmachine learning techniques represented by deep learning lacks the capability\nto represent relational knowledge and requires prodigious amounts of annotated\ntraining data. These drawbacks strongly restrict AI's role in synthetic biology\nin which experimentation is inherently resource and time intensive. In this\nwork, we propose an automated biodesign engineering framework empowered by\nAbductive Meta-Interpretive Learning ($Meta_{Abd}$), a novel machine learning\napproach that combines symbolic and sub-symbolic machine learning, to further\nenhance the DBTL cycle by enabling the learning machine to 1) exploit domain\nknowledge and learn human-interpretable models that are expressed by formal\nlanguages such as first-order logic; 2) simultaneously optimise the structure\nand parameters of the models to make accurate numerical predictions; 3) reduce\nthe cost of experiments and effort on data annotation by actively generating\nhypotheses and examples. To verify the effectiveness of $Meta_{Abd}$, we have\nmodelled a synthetic dataset for the production of proteins from a three gene\noperon in a microbial host, which represents a common synthetic biology\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:10:26 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dai", "Wang-Zhou", ""], ["Hallett", "Liam", ""], ["Muggleton", "Stephen H.", ""], ["Baldwin", "Geoff S.", ""]]}, {"id": "2105.07763", "submitter": "Bill Cassidy", "authors": "Bill Cassidy, Neil D. Reeves, Joseph M. Pappachan, Naseer Ahmad,\n  Samantha Haycocks, David Gillespie, Moi Hoon Yap", "title": "A Cloud-based Deep Learning Framework for Remote Detection of Diabetic\n  Foot Ulcers", "comments": "10 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes a mobile and cloud-based framework for the automatic\ndetection of diabetic foot ulcers and conducts an investigation of its\nperformance. The system uses a cross-platform mobile framework which enables\nthe deployment of mobile apps to multiple platforms using a single TypeScript\ncode base. A deep convolutional neural network was deployed to a cloud-based\nplatform where the mobile app could send photographs of patient's feet for\ninference to detect the presence of diabetic foot ulcers. The functionality and\nusability of the system were tested in two clinical settings: Salford Royal NHS\nFoundation Trust and Lancashire Teaching Hospitals NHS Foundation Trust. The\nbenefits of the system, such as the potential use of the app by patients to\nidentify and monitor their condition are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:15:01 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Cassidy", "Bill", ""], ["Reeves", "Neil D.", ""], ["Pappachan", "Joseph M.", ""], ["Ahmad", "Naseer", ""], ["Haycocks", "Samantha", ""], ["Gillespie", "David", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "2105.07768", "submitter": "Massih-Reza Amini", "authors": "Aleksandra Malkova, Loic Pauletto, Christophe Villien, Benoit Denis,\n  Massih-Reza Amini", "title": "Self-Learning for Received Signal Strength Map Reconstruction with\n  Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a Neural Network (NN) model based on Neural\nArchitecture Search (NAS) and self-learning for received signal strength (RSS)\nmap reconstruction out of sparse single-snapshot input measurements, in the\ncase where data-augmentation by side deterministic simulations cannot be\nperformed. The approach first finds an optimal NN architecture and\nsimultaneously train the deduced model over some ground-truth measurements of a\ngiven (RSS) map. These ground-truth measurements along with the predictions of\nthe model over a set of randomly chosen points are then used to train a second\nNN model having the same architecture. Experimental results show that signal\npredictions of this second model outperforms non-learning based interpolation\nstate-of-the-art techniques and NN models with no architecture search on five\nlarge-scale maps of RSS measurements.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:19:22 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Malkova", "Aleksandra", ""], ["Pauletto", "Loic", ""], ["Villien", "Christophe", ""], ["Denis", "Benoit", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "2105.07771", "submitter": "Maria Naumcheva", "authors": "Maria Naumcheva", "title": "Deep Learning Models in Software Requirements Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Requirements elicitation is an important phase of any software project: the\nerrors in requirements are more expensive to fix than the errors introduced at\nlater stages of software life cycle. Nevertheless, many projects do not devote\nsufficient time to requirements. Automated requirements generation can improve\nthe quality of software projects. In this article we have accomplished the\nfirst step of the research on this topic: we have applied the vanilla sentence\nautoencoder to the sentence generation task and evaluated its performance. The\ngenerated sentences are not plausible English and contain only a few meaningful\nwords. We believe that applying the model to a larger dataset may produce\nsignificantly better results. Further research is needed to improve the quality\nof generated data.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:27:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Naumcheva", "Maria", ""]]}, {"id": "2105.07775", "submitter": "Xiangmeng Wang", "authors": "Qian Li, Xiangmeng Wang, Guandong Xu", "title": "Be Causal: De-biasing Social Network Confounding in Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recommendation systems, the existence of the missing-not-at-random (MNAR)\nproblem results in the selection bias issue, degrading the recommendation\nperformance ultimately. A common practice to address MNAR is to treat missing\nentries from the so-called \"exposure\" perspective, i.e., modeling how an item\nis exposed (provided) to a user. Most of the existing approaches use heuristic\nmodels or re-weighting strategy on observed ratings to mimic the\nmissing-at-random setting. However, little research has been done to reveal how\nthe ratings are missing from a causal perspective. To bridge the gap, we\npropose an unbiased and robust method called DENC (De-bias Network Confounding\nin Recommendation) inspired by confounder analysis in causal inference. In\ngeneral, DENC provides a causal analysis on MNAR from both the inherent factors\n(e.g., latent user or item factors) and auxiliary network's perspective.\nParticularly, the proposed exposure model in DENC can control the social\nnetwork confounder meanwhile preserves the observed exposure information. We\nalso develop a deconfounding model through the balanced representation learning\nto retain the primary user and item features, which enables DENC generalize\nwell on the rating prediction. Extensive experiments on three datasets validate\nthat our proposed model outperforms the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:38:59 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 12:15:22 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Li", "Qian", ""], ["Wang", "Xiangmeng", ""], ["Xu", "Guandong", ""]]}, {"id": "2105.07789", "submitter": "Lukas Drees", "authors": "Lukas Drees, Laura Verena Junker-Frohn, Jana Kierdorf, Ribana Roscher", "title": "Temporal Prediction and Evaluation of Brassica Growth in the Field using\n  Conditional Generative Adversarial Networks", "comments": "38 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Farmers frequently assess plant growth and performance as basis for making\ndecisions when to take action in the field, such as fertilization, weed\ncontrol, or harvesting. The prediction of plant growth is a major challenge, as\nit is affected by numerous and highly variable environmental factors. This\npaper proposes a novel monitoring approach that comprises high-throughput\nimaging sensor measurements and their automatic analysis to predict future\nplant growth. Our approach's core is a novel machine learning-based growth\nmodel based on conditional generative adversarial networks, which is able to\npredict the future appearance of individual plants. In experiments with RGB\ntime-series images of laboratory-grown Arabidopsis thaliana and field-grown\ncauliflower plants, we show that our approach produces realistic, reliable, and\nreasonable images of future growth stages. The automatic interpretation of the\ngenerated images through neural network-based instance segmentation allows the\nderivation of various phenotypic traits that describe plant growth.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:00:01 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Drees", "Lukas", ""], ["Junker-Frohn", "Laura Verena", ""], ["Kierdorf", "Jana", ""], ["Roscher", "Ribana", ""]]}, {"id": "2105.07797", "submitter": "Taro Langner", "authors": "Taro Langner, Robin Strand, H{\\aa}kan Ahlstr\\\"om, Joel Kullberg", "title": "Deep regression for uncertainty-aware and interpretable analysis of\n  large-scale body MRI", "comments": "Presented at the Swedish Symposium on Deep Learning 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale medical studies such as the UK Biobank examine thousands of\nvolunteer participants with medical imaging techniques. Combined with the vast\namount of collected metadata, anatomical information from these images has the\npotential for medical analyses at unprecedented scale. However, their\nevaluation often requires manual input and long processing times, limiting the\namount of reference values for biomarkers and other measurements available for\nresearch. Recent approaches with convolutional neural networks for regression\ncan perform these evaluations automatically. On magnetic resonance imaging\n(MRI) data of more than 40,000 UK Biobank subjects, these systems can estimate\nhuman age, body composition and more. This style of analysis is almost entirely\ndata-driven and no manual intervention or guidance with manually segmented\nground truth images is required. The networks often closely emulate the\nreference method that provided their training data and can reach levels of\nagreement comparable to the expected variability between established medical\ngold standard techniques. The risk of silent failure can be individually\nquantified by predictive uncertainty obtained from a mean-variance criterion\nand ensembling. Saliency analysis furthermore enables an interpretation of the\nunderlying relevant image features and showed that the networks learned to\ncorrectly target specific organs, limbs, and regions of interest.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:12:20 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Langner", "Taro", ""], ["Strand", "Robin", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""]]}, {"id": "2105.07806", "submitter": "Jiawei Jiang", "authors": "Jiawei Jiang, Shaoduo Gan, Yue Liu, Fanlin Wang, Gustavo Alonso, Ana\n  Klimovic, Ankit Singla, Wentao Wu, Ce Zhang", "title": "Towards Demystifying Serverless Machine Learning Training", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3459240", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appeal of serverless (FaaS) has triggered a growing interest on how to\nuse it in data-intensive applications such as ETL, query processing, or machine\nlearning (ML). Several systems exist for training large-scale ML models on top\nof serverless infrastructures (e.g., AWS Lambda) but with inconclusive results\nin terms of their performance and relative advantage over \"serverful\"\ninfrastructures (IaaS). In this paper we present a systematic, comparative\nstudy of distributed ML training over FaaS and IaaS. We present a design space\ncovering design choices such as optimization algorithms and synchronization\nprotocols, and implement a platform, LambdaML, that enables a fair comparison\nbetween FaaS and IaaS. We present experimental results using LambdaML, and\nfurther develop an analytic model to capture cost/performance tradeoffs that\nmust be considered when opting for a serverless infrastructure. Our results\nindicate that ML training pays off in serverless only for models with efficient\n(i.e., reduced) communication and that quickly converge. In general, FaaS can\nbe much faster but it is never significantly cheaper than IaaS.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:19:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Jiang", "Jiawei", ""], ["Gan", "Shaoduo", ""], ["Liu", "Yue", ""], ["Wang", "Fanlin", ""], ["Alonso", "Gustavo", ""], ["Klimovic", "Ana", ""], ["Singla", "Ankit", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""]]}, {"id": "2105.07809", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Cheng-Ming Chiang, Hsien-Kai Kuo, Anastasia Sycheva,\n  Radu Timofte, Min-Hung Chen, Man-Yu Lee, Yu-Syuan Xu, Yu Tseng, Shusong Xu,\n  Jin Guo, Chao-Hung Chen, Ming-Chun Hsyu, Wen-Chia Tsai, Chao-Wei Chen,\n  Grigory Malivenko, Minsu Kwon, Myungje Lee, Jaeyoon Yoo, Changbeom Kang,\n  Shinjo Wang, Zheng Shaolong, Hao Dejun, Xie Fen, Feng Zhuang, Yipeng Ma,\n  Jingyang Peng, Tao Wang, Fenglong Song, Chih-Chung Hsu, Kwan-Lin Chen,\n  Mei-Hsuang Wu, Vishal Chudasama, Kalpesh Prajapati, Heena Patel, Anjali\n  Sarvaiya, Kishor Upla, Kiran Raja, Raghavendra Ramachandra, Christoph Busch,\n  Etienne de Stoutz", "title": "Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021\n  Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the quality of mobile cameras starts to play a crucial role in modern\nsmartphones, more and more attention is now being paid to ISP algorithms used\nto improve various perceptual aspects of mobile photos. In this Mobile AI\nchallenge, the target was to develop an end-to-end deep learning-based image\nsignal processing (ISP) pipeline that can replace classical hand-crafted ISPs\nand achieve nearly real-time performance on smartphone NPUs. For this, the\nparticipants were provided with a novel learned ISP dataset consisting of\nRAW-RGB image pairs captured with the Sony IMX586 Quad Bayer mobile sensor and\na professional 102-megapixel medium format camera. The runtime of all models\nwas evaluated on the MediaTek Dimensity 1000+ platform with a dedicated AI\nprocessing unit capable of accelerating both floating-point and quantized\nneural networks. The proposed solutions are fully compatible with the above NPU\nand are capable of processing Full HD photos under 60-100 milliseconds while\nachieving high fidelity results. A detailed description of all models developed\nin this challenge is provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:20:35 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ignatov", "Andrey", ""], ["Chiang", "Cheng-Ming", ""], ["Kuo", "Hsien-Kai", ""], ["Sycheva", "Anastasia", ""], ["Timofte", "Radu", ""], ["Chen", "Min-Hung", ""], ["Lee", "Man-Yu", ""], ["Xu", "Yu-Syuan", ""], ["Tseng", "Yu", ""], ["Xu", "Shusong", ""], ["Guo", "Jin", ""], ["Chen", "Chao-Hung", ""], ["Hsyu", "Ming-Chun", ""], ["Tsai", "Wen-Chia", ""], ["Chen", "Chao-Wei", ""], ["Malivenko", "Grigory", ""], ["Kwon", "Minsu", ""], ["Lee", "Myungje", ""], ["Yoo", "Jaeyoon", ""], ["Kang", "Changbeom", ""], ["Wang", "Shinjo", ""], ["Shaolong", "Zheng", ""], ["Dejun", "Hao", ""], ["Fen", "Xie", ""], ["Zhuang", "Feng", ""], ["Ma", "Yipeng", ""], ["Peng", "Jingyang", ""], ["Wang", "Tao", ""], ["Song", "Fenglong", ""], ["Hsu", "Chih-Chung", ""], ["Chen", "Kwan-Lin", ""], ["Wu", "Mei-Hsuang", ""], ["Chudasama", "Vishal", ""], ["Prajapati", "Kalpesh", ""], ["Patel", "Heena", ""], ["Sarvaiya", "Anjali", ""], ["Upla", "Kishor", ""], ["Raja", "Kiran", ""], ["Ramachandra", "Raghavendra", ""], ["Busch", "Christoph", ""], ["de Stoutz", "Etienne", ""]]}, {"id": "2105.07825", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Radu Timofte, Maurizio Denna, Abdel Younes, Andrew\n  Lek, Mustafa Ayazoglu, Jie Liu, Zongcai Du, Jiaming Guo, Xueyi Zhou, Hao Jia,\n  Youliang Yan, Zexin Zhang, Yixin Chen, Yunbo Peng, Yue Lin, Xindong Zhang,\n  Hui Zeng, Kun Zeng, Peirong Li, Zhihuang Liu, Shiqi Xue, Shengpeng Wang", "title": "Real-Time Quantized Image Super-Resolution on Mobile NPUs, Mobile AI\n  2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution is one of the most popular computer vision problems\nwith many important applications to mobile devices. While many solutions have\nbeen proposed for this task, they are usually not optimized even for common\nsmartphone AI hardware, not to mention more constrained smart TV platforms that\nare often supporting INT8 inference only. To address this problem, we introduce\nthe first Mobile AI challenge, where the target is to develop an end-to-end\ndeep learning-based image super-resolution solutions that can demonstrate a\nreal-time performance on mobile or edge NPUs. For this, the participants were\nprovided with the DIV2K dataset and trained quantized models to do an efficient\n3X image upscaling. The runtime of all models was evaluated on the Synaptics\nVS680 Smart Home board with a dedicated NPU capable of accelerating quantized\nneural networks. The proposed solutions are fully compatible with all major\nmobile AI accelerators and are capable of reconstructing Full HD images under\n40-60 ms while achieving high fidelity results. A detailed description of all\nmodels developed in the challenge is provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:34:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ignatov", "Andrey", ""], ["Timofte", "Radu", ""], ["Denna", "Maurizio", ""], ["Younes", "Abdel", ""], ["Lek", "Andrew", ""], ["Ayazoglu", "Mustafa", ""], ["Liu", "Jie", ""], ["Du", "Zongcai", ""], ["Guo", "Jiaming", ""], ["Zhou", "Xueyi", ""], ["Jia", "Hao", ""], ["Yan", "Youliang", ""], ["Zhang", "Zexin", ""], ["Chen", "Yixin", ""], ["Peng", "Yunbo", ""], ["Lin", "Yue", ""], ["Zhang", "Xindong", ""], ["Zeng", "Hui", ""], ["Zeng", "Kun", ""], ["Li", "Peirong", ""], ["Liu", "Zhihuang", ""], ["Xue", "Shiqi", ""], ["Wang", "Shengpeng", ""]]}, {"id": "2105.07826", "submitter": "Jamal Al Qundus", "authors": "Malik Yousef, Jamal Al Qundus, Silvio Peikert, and Adrian Paschke", "title": "TopicsRanksDC: Distance-based Topic Ranking applied on Two-Class Data", "comments": "10 pages, 5 figures", "journal-ref": "International Conference on Database and Expert Systems\n  Applications DEXA 2020: Database and Expert Systems Applications pp 11-21", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a novel approach named TopicsRanksDC for topics\nranking based on the distance between two clusters that are generated by each\ntopic. We assume that our data consists of text documents that are associated\nwith two-classes. Our approach ranks each topic contained in these text\ndocuments by its significance for separating the two-classes. Firstly, the\nalgorithm detects topics using Latent Dirichlet Allocation (LDA). The words\ndefining each topic are represented as two clusters, where each one is\nassociated with one of the classes. We compute four distance metrics, Single\nLinkage, Complete Linkage, Average Linkage and distance between the centroid.\nWe compare the results of LDA topics and random topics. The results show that\nthe rank for LDA topics is much higher than random topics. The results of\nTopicsRanksDC tool are promising for future work to enable search engines to\nsuggest related topics.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:34:45 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yousef", "Malik", ""], ["Qundus", "Jamal Al", ""], ["Peikert", "Silvio", ""], ["Paschke", "Adrian", ""]]}, {"id": "2105.07829", "submitter": "Yuchen Zhong", "authors": "Yuchen Zhong, Cong Xie, Shuai Zheng, Haibin Lin", "title": "Compressed Communication for Distributed Training: Adaptive Methods and\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication overhead severely hinders the scalability of distributed\nmachine learning systems. Recently, there has been a growing interest in using\ngradient compression to reduce the communication overhead of the distributed\ntraining. However, there is little understanding of applying gradient\ncompression to adaptive gradient methods. Moreover, its performance benefits\nare often limited by the non-negligible compression overhead. In this paper, we\nfirst introduce a novel adaptive gradient method with gradient compression. We\nshow that the proposed method has a convergence rate of\n$\\mathcal{O}(1/\\sqrt{T})$ for non-convex problems. In addition, we develop a\nscalable system called BytePS-Compress for two-way compression, where the\ngradients are compressed in both directions between workers and parameter\nservers. BytePS-Compress pipelines the compression and decompression on CPUs\nand achieves a high degree of parallelism. Empirical evaluations show that we\nimprove the training time of ResNet50, VGG16, and BERT-base by 5.0%, 58.1%,\n23.3%, respectively, without any accuracy loss with 25 Gb/s networking.\nFurthermore, for training the BERT models, we achieve a compression rate of\n333x compared to the mixed-precision training.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:41:47 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhong", "Yuchen", ""], ["Xie", "Cong", ""], ["Zheng", "Shuai", ""], ["Lin", "Haibin", ""]]}, {"id": "2105.07830", "submitter": "Suman Saha", "authors": "Suman Saha, Anton Obukhov, Danda Pani Paudel, Menelaos Kanakis, Yuhua\n  Chen, Stamatios Georgoulis, Luc Van Gool", "title": "Learning to Relate Depth and Semantics for Unsupervised Domain\n  Adaptation", "comments": "Accepted at CVPR 2021; updated results according to the released\n  source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present an approach for encoding visual task relationships to improve\nmodel performance in an Unsupervised Domain Adaptation (UDA) setting. Semantic\nsegmentation and monocular depth estimation are shown to be complementary\ntasks; in a multi-task learning setting, a proper encoding of their\nrelationships can further improve performance on both tasks. Motivated by this\nobservation, we propose a novel Cross-Task Relation Layer (CTRL), which encodes\ntask dependencies between the semantic and depth predictions. To capture the\ncross-task relationships, we propose a neural network architecture that\ncontains task-specific and cross-task refinement heads. Furthermore, we propose\nan Iterative Self-Learning (ISL) training scheme, which exploits semantic\npseudo-labels to provide extra supervision on the target domain. We\nexperimentally observe improvements in both tasks' performance because the\ncomplementary information present in these tasks is better captured.\nSpecifically, we show that: (1) our approach improves performance on all tasks\nwhen they are complementary and mutually dependent; (2) the CTRL helps to\nimprove both semantic segmentation and depth estimation tasks performance in\nthe challenging UDA setting; (3) the proposed ISL training scheme further\nimproves the semantic segmentation performance. The implementation is available\nat https://github.com/susaha/ctrl-uda.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:42:09 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 09:27:00 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Saha", "Suman", ""], ["Obukhov", "Anton", ""], ["Paudel", "Danda Pani", ""], ["Kanakis", "Menelaos", ""], ["Chen", "Yuhua", ""], ["Georgoulis", "Stamatios", ""], ["Van Gool", "Luc", ""]]}, {"id": "2105.07831", "submitter": "Hangcheng Dong", "authors": "Hangcheng Dong, Bingguo Liu, Fengdong Chen, Dong Ye and Guodong Liu", "title": "How to Explain Neural Networks: A perspective of data space division", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interpretability of intelligent algorithms represented by deep learning has\nbeen yet an open problem. We discuss the shortcomings of the existing\nexplainable method based on the two attributes of explanation, which are called\ncompleteness and explicitness. Furthermore, we point out that a model that\ncompletely relies on feed-forward mapping is extremely easy to cause\ninexplicability because it is hard to quantify the relationship between this\nmapping and the final model. Based on the perspective of the data space\ndivision, the principle of complete local interpretable model-agnostic\nexplanations (CLIMEP) is proposed in this paper. To study the classification\nproblems, we further discussed the equivalence of the CLIMEP and the decision\nboundary. As a matter of fact, it is also difficult to implementation of\nCLIMEP. To tackle the challenge, motivated by the fact that a fully-connected\nneural network (FCNN) with piece-wise linear activation functions (PWLs) can\npartition the input space into several linear regions, we extend this result to\narbitrary FCNNs by the strategy of linearizing the activation functions.\nApplying this technique to solving classification problems, it is the first\ntime that the complete decision boundary of FCNNs has been able to be obtained.\nFinally, we propose the DecisionNet (DNet), which divides the input space by\nthe hyper-planes of the decision boundary. Hence, each linear interval of the\nDNet merely contains samples of the same label. Experiments show that the\nsurprising model compression efficiency of the DNet with an arbitrary\ncontrolled precision.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:43:37 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dong", "Hangcheng", ""], ["Liu", "Bingguo", ""], ["Chen", "Fengdong", ""], ["Ye", "Dong", ""], ["Liu", "Guodong", ""]]}, {"id": "2105.07837", "submitter": "Rohith Mekala", "authors": "Karampudi Radha, Mekala Rohith", "title": "An Experimental Analysis of Work-Life Balance Among The Employees using\n  Machine Learning Classifiers", "comments": "10 pages, 16 figures, Published with International Journal of\n  Computer Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  69(4):39-48, April 2021. ISSN: 2231-2803", "doi": "10.14445/22312803/IJCTT-V69I4P108", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Researchers today have found out the importance of Artificial Intelligence,\nand Machine Learning in our daily lives, as well as they can be used to improve\nthe quality of our lives as well as the cities and nations alike. An example of\nthis is that it is currently speculated that ML can provide ways to relieve\nworkers as it can predict effective working schedules and patterns which\nincrease the efficiency of the workers. Ultimately this is leading to a\nWork-Life Balance for the workers. But how is this possible? It is practically\npossible with the Machine Learning algorithms to predict, calculate the factors\naffecting the feelings of the worker's work-life balance. In order to actually\ndo this, a sizeable amount of 12,756 people's data has been taken under\nconsideration. Upon analysing the data and calculating under various factors,\nwe have found out the correlation of various factors and WLB(Work-Life Balance\nin short). There are some factors that have to be taken into serious\nconsideration as they play a major role in WLB. We have trained 80% of our data\nwith Random Forest Classifier, SVM and Naive Bayes algorithms. Upon testing,\nthe algorithms predict the WLB with 71.5% as the best accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 21:35:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Radha", "Karampudi", ""], ["Rohith", "Mekala", ""]]}, {"id": "2105.07844", "submitter": "David Leslie", "authors": "David Leslie, Anjali Mazumder, Aidan Peppin, Maria Wolters and Alexa\n  Hagerty", "title": "Does \"AI\" stand for augmenting inequality in the era of covid-19\n  healthcare?", "comments": null, "journal-ref": "bmj, 372 (2021)", "doi": "10.1136/bmj.n304", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among the most damaging characteristics of the covid-19 pandemic has been its\ndisproportionate effect on disadvantaged communities. As the outbreak has\nspread globally, factors such as systemic racism, marginalisation, and\nstructural inequality have created path dependencies that have led to poor\nhealth outcomes. These social determinants of infectious disease and\nvulnerability to disaster have converged to affect already disadvantaged\ncommunities with higher levels of economic instability, disease exposure,\ninfection severity, and death. Artificial intelligence (AI) technologies are an\nimportant part of the health informatics toolkit used to fight contagious\ndisease. AI is well known, however, to be susceptible to algorithmic biases\nthat can entrench and augment existing inequality. Uncritically deploying AI in\nthe fight against covid-19 thus risks amplifying the pandemic's adverse effects\non vulnerable groups, exacerbating health inequity. In this paper, we claim\nthat AI systems can introduce or reflect bias and discrimination in three ways:\nin patterns of health discrimination that become entrenched in datasets, in\ndata representativeness, and in human choices made during the design,\ndevelopment, and deployment of these systems. We highlight how the use of AI\ntechnologies threaten to exacerbate the disparate effect of covid-19 on\nmarginalised, under-represented, and vulnerable groups, particularly black,\nAsian, and other minoritised ethnic people, older populations, and those of\nlower socioeconomic status. We conclude that, to mitigate the compounding\neffects of AI on inequalities associated with covid-19, decision makers,\ntechnology developers, and health officials must account for the potential\nbiases and inequities at all stages of the AI process.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:23:07 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Leslie", "David", ""], ["Mazumder", "Anjali", ""], ["Peppin", "Aidan", ""], ["Wolters", "Maria", ""], ["Hagerty", "Alexa", ""]]}, {"id": "2105.07852", "submitter": "Bryce Goodman", "authors": "Bryce Goodman", "title": "Hard Choices and Hard Limits for Artificial Intelligence", "comments": null, "journal-ref": null, "doi": "10.1145/3461702.3462539", "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence (AI) is supposed to help us make better choices. Some\nof these choices are small, like what route to take to work, or what music to\nlisten to. Others are big, like what treatment to administer for a disease or\nhow long to sentence someone for a crime. If AI can assist with these big\ndecisions, we might think it can also help with hard choices, cases where\nalternatives are neither better, worse nor equal but on a par. The aim of this\npaper, however, is to show that this view is mistaken: the fact of parity shows\nthat there are hard limits on AI in decision making and choices that AI cannot,\nand should not, resolve.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 22:56:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Goodman", "Bryce", ""]]}, {"id": "2105.07869", "submitter": "Andrey Ignatov", "authors": "Angeline Pouget, Sidharth Ramesh, Maximilian Giang, Ramithan\n  Chandrapalan, Toni Tanner, Moritz Prussing, Radu Timofte, Andrey Ignatov", "title": "Fast and Accurate Camera Scene Detection on Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI-powered automatic camera scene detection mode is nowadays available in\nnearly any modern smartphone, though the problem of accurate scene prediction\nhas not yet been addressed by the research community. This paper for the first\ntime carefully defines this problem and proposes a novel Camera Scene Detection\nDataset (CamSDD) containing more than 11K manually crawled images belonging to\n30 different scene categories. We propose an efficient and NPU-friendly CNN\nmodel for this task that demonstrates a top-3 accuracy of 99.5% on this dataset\nand achieves more than 200 FPS on the recent mobile SoCs. An additional\nin-the-wild evaluation of the obtained solution is performed to analyze its\nperformance and limitation in the real-world scenarios. The dataset and\npre-trained models used in this paper are available on the project website.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:06:21 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Pouget", "Angeline", ""], ["Ramesh", "Sidharth", ""], ["Giang", "Maximilian", ""], ["Chandrapalan", "Ramithan", ""], ["Tanner", "Toni", ""], ["Prussing", "Moritz", ""], ["Timofte", "Radu", ""], ["Ignatov", "Andrey", ""]]}, {"id": "2105.07898", "submitter": "Ruben Rodriguez Torrado", "authors": "Ruben Rodriguez-Torrado, Pablo Ruiz, Luis Cueto-Felgueroso, Michael\n  Cerny Green, Tyler Friesen, Sebastien Matringe and Julian Togelius", "title": "Physics-informed attention-based neural network for solving non-linear\n  partial differential equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Physics-Informed Neural Networks (PINNs) have enabled significant\nimprovements in modelling physical processes described by partial differential\nequations (PDEs). PINNs are based on simple architectures, and learn the\nbehavior of complex physical systems by optimizing the network parameters to\nminimize the residual of the underlying PDE. Current network architectures\nshare some of the limitations of classical numerical discretization schemes\nwhen applied to non-linear differential equations in continuum mechanics. A\nparadigmatic example is the solution of hyperbolic conservation laws that\ndevelop highly localized nonlinear shock waves. Learning solutions of PDEs with\ndominant hyperbolic character is a challenge for current PINN approaches, which\nrely, like most grid-based numerical schemes, on adding artificial dissipation.\nHere, we address the fundamental question of which network architectures are\nbest suited to learn the complex behavior of non-linear PDEs. We focus on\nnetwork architecture rather than on residual regularization. Our new\nmethodology, called Physics-Informed Attention-based Neural Networks, (PIANNs),\nis a combination of recurrent neural networks and attention mechanisms. The\nattention mechanism adapts the behavior of the deep neural network to the\nnon-linear features of the solution, and break the current limitations of\nPINNs. We find that PIANNs effectively capture the shock front in a hyperbolic\nmodel problem, and are capable of providing high-quality solutions inside and\nbeyond the training set.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:29:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Rodriguez-Torrado", "Ruben", ""], ["Ruiz", "Pablo", ""], ["Cueto-Felgueroso", "Luis", ""], ["Green", "Michael Cerny", ""], ["Friesen", "Tyler", ""], ["Matringe", "Sebastien", ""], ["Togelius", "Julian", ""]]}, {"id": "2105.07909", "submitter": "Ning Xie", "authors": "Junhao Zeng, Qingchun Zhang, Ning Xie, Bochun Yang", "title": "Application of Deep Self-Attention in Knowledge Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of intelligent tutoring system has greatly influenced the way\nstudents learn and practice, which increases their learning efficiency. The\nintelligent tutoring system must model learners' mastery of the knowledge\nbefore providing feedback and advices to learners, so one class of algorithm\ncalled \"knowledge tracing\" is surely important. This paper proposed Deep\nSelf-Attentive Knowledge Tracing (DSAKT) based on the data of PTA, an online\nassessment system used by students in many universities in China, to help these\nstudents learn more efficiently. Experimentation on the data of PTA shows that\nDSAKT outperforms the other models for knowledge tracing an improvement of AUC\nby 2.1% on average, and this model also has a good performance on the ASSIST\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:45:38 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 08:55:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zeng", "Junhao", ""], ["Zhang", "Qingchun", ""], ["Xie", "Ning", ""], ["Yang", "Bochun", ""]]}, {"id": "2105.07917", "submitter": "Giulia Cisotto", "authors": "Alberto Zancanaro, Giulia Cisotto, Jo\\~ao Ruivo Paulo, Gabriel Pires,\n  and Urbano J. Nunes", "title": "CNN-based Approaches For Cross-Subject Classification in Motor Imagery:\n  From The State-of-The-Art to DynamicNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor imagery (MI)-based brain-computer interface (BCI) systems are being\nincreasingly employed to provide alternative means of communication and control\nfor people suffering from neuro-motor impairments, with a special effort to\nbring these systems out of the controlled lab environments. Hence, accurately\nclassifying MI from brain signals, e.g., from electroencephalography (EEG), is\nessential to obtain reliable BCI systems. However, MI classification is still a\nchallenging task, because the signals are characterized by poor SNR, high\nintra-subject and cross-subject variability. Deep learning approaches have\nstarted to emerge as valid alternatives to standard machine learning\ntechniques, e.g., filter bank common spatial pattern (FBCSP), to extract\nsubject-independent features and to increase the cross-subject classification\nperformance of MI BCI systems. In this paper, we first present a review of the\nmost recent studies using deep learning for MI classification, with particular\nattention to their cross-subject performance. Second, we propose DynamicNet, a\nPython-based tool for quick and flexible implementations of deep learning\nmodels based on convolutional neural networks. We show-case the potentiality of\nDynamicNet by implementing EEGNet, a well-established architecture for\neffective EEG classification. Finally, we compare its performance with FBCSP in\na 4-class MI classification over public datasets. To explore its cross-subject\nclassification ability, we applied three different cross-validation schemes.\nFrom our results, we demonstrate that DynamicNet-implemented EEGNet outperforms\nFBCSP by about 25%, with a statistically significant difference when\ncross-subject validation schemes are applied.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:57:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zancanaro", "Alberto", ""], ["Cisotto", "Giulia", ""], ["Paulo", "Jo\u00e3o Ruivo", ""], ["Pires", "Gabriel", ""], ["Nunes", "Urbano J.", ""]]}, {"id": "2105.07944", "submitter": "Yunfei Chu", "authors": "Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang,\n  Xiaofeng He, Le Song, Jingren Zhou, Hongxia Yang", "title": "TCL: Transformer-based Dynamic Graph Modelling via Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic graph modeling has recently attracted much attention due to its\nextensive applications in many real-world scenarios, such as recommendation\nsystems, financial transactions, and social networks. Although many works have\nbeen proposed for dynamic graph modeling in recent years, effective and\nscalable models are yet to be developed. In this paper, we propose a novel\ngraph neural network approach, called TCL, which deals with the\ndynamically-evolving graph in a continuous-time fashion and enables effective\ndynamic node representation learning that captures both the temporal and\ntopology information. Technically, our model contains three novel aspects.\nFirst, we generalize the vanilla Transformer to temporal graph learning\nscenarios and design a graph-topology-aware transformer. Secondly, on top of\nthe proposed graph transformer, we introduce a two-stream encoder that\nseparately extracts representations from temporal neighborhoods associated with\nthe two interaction nodes and then utilizes a co-attentional transformer to\nmodel inter-dependencies at a semantic level. Lastly, we are inspired by the\nrecently developed contrastive learning and propose to optimize our model by\nmaximizing mutual information (MI) between the predictive representations of\ntwo future interaction nodes. Benefiting from this, our dynamic representations\ncan preserve high-level (or global) semantics about interactions and thus is\nrobust to noisy interactions. To the best of our knowledge, this is the first\nattempt to apply contrastive learning to representation learning on dynamic\ngraphs. We evaluate our model on four benchmark datasets for interaction\nprediction and experiment results demonstrate the superiority of our model.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:33:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Lu", ""], ["Chang", "Xiaofu", ""], ["Li", "Shuang", ""], ["Chu", "Yunfei", ""], ["Li", "Hui", ""], ["Zhang", "Wei", ""], ["He", "Xiaofeng", ""], ["Song", "Le", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""]]}, {"id": "2105.07957", "submitter": "Daniel Tanneberg", "authors": "Daniel Tanneberg, Elmar Rueckert, Jan Peters", "title": "Evolutionary Training and Abstraction Yields Algorithmic Generalization\n  of Neural Computers", "comments": "Nature Machine Intelligence", "journal-ref": "Nature Machine Intelligence, Vol. 2, December 2020, 753-763", "doi": "10.1038/s42256-020-00255-1", "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key feature of intelligent behaviour is the ability to learn abstract\nstrategies that scale and transfer to unfamiliar problems. An abstract strategy\nsolves every sample from a problem class, no matter its representation or\ncomplexity -- like algorithms in computer science. Neural networks are powerful\nmodels for processing sensory data, discovering hidden patterns, and learning\ncomplex functions, but they struggle to learn such iterative, sequential or\nhierarchical algorithmic strategies. Extending neural networks with external\nmemories has increased their capacities in learning such strategies, but they\nare still prone to data variations, struggle to learn scalable and transferable\nsolutions, and require massive training data. We present the Neural Harvard\nComputer (NHC), a memory-augmented network based architecture, that employs\nabstraction by decoupling algorithmic operations from data manipulations,\nrealized by splitting the information flow and separated modules. This\nabstraction mechanism and evolutionary training enable the learning of robust\nand scalable algorithmic solutions. On a diverse set of 11 algorithms with\nvarying complexities, we show that the NHC reliably learns algorithmic\nsolutions with strong generalization and abstraction: perfect generalization\nand scaling to arbitrary task configurations and complexities far beyond seen\nduring training, and being independent of the data representation and the task\ndomain.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:37:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tanneberg", "Daniel", ""], ["Rueckert", "Elmar", ""], ["Peters", "Jan", ""]]}, {"id": "2105.07959", "submitter": "Kiran Tomlinson", "authors": "Kiran Tomlinson, Johan Ugander, and Austin R. Benson", "title": "Choice Set Confounding in Discrete Choice", "comments": "12 pages, accepted to KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard methods in preference learning involve estimating the parameters of\ndiscrete choice models from data of selections (choices) made by individuals\nfrom a discrete set of alternatives (the choice set). While there are many\nmodels for individual preferences, existing learning methods overlook how\nchoice set assignment affects the data. Often, the choice set itself is\ninfluenced by an individual's preferences; for instance, a consumer choosing a\nproduct from an online retailer is often presented with options from a\nrecommender system that depend on information about the consumer's preferences.\nIgnoring these assignment mechanisms can mislead choice models into making\nbiased estimates of preferences, a phenomenon that we call choice set\nconfounding; we demonstrate the presence of such confounding in widely-used\nchoice datasets.\n  To address this issue, we adapt methods from causal inference to the discrete\nchoice setting. We use covariates of the chooser for inverse probability\nweighting and/or regression controls, accurately recovering individual\npreferences in the presence of choice set confounding under certain\nassumptions. When such covariates are unavailable or inadequate, we develop\nmethods that take advantage of structured choice set assignment to improve\nprediction. We demonstrate the effectiveness of our methods on real-world\nchoice data, showing, for example, that accounting for choice set confounding\nmakes choices observed in hotel booking and commute transportation more\nconsistent with rational utility-maximization.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:39:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tomlinson", "Kiran", ""], ["Ugander", "Johan", ""], ["Benson", "Austin R.", ""]]}, {"id": "2105.07965", "submitter": "Arpita Biswas", "authors": "Arpita Biswas, Gaurav Aggarwal, Pradeep Varakantham, Milind Tambe", "title": "Learn to Intervene: An Adaptive Learning Policy for Restless Bandits in\n  Application to Preventive Healthcare", "comments": "To appear in the 30th International Joint Conference on Artificial\n  Intelligence (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many public health settings, it is important for patients to adhere to\nhealth programs, such as taking medications and periodic health checks.\nUnfortunately, beneficiaries may gradually disengage from such programs, which\nis detrimental to their health. A concrete example of gradual disengagement has\nbeen observed by an organization that carries out a free automated call-based\nprogram for spreading preventive care information among pregnant women. Many\nwomen stop picking up calls after being enrolled for a few months. To avoid\nsuch disengagements, it is important to provide timely interventions. Such\ninterventions are often expensive and can be provided to only a small fraction\nof the beneficiaries. We model this scenario as a restless multi-armed bandit\n(RMAB) problem, where each beneficiary is assumed to transition from one state\nto another depending on the intervention. Moreover, since the transition\nprobabilities are unknown a priori, we propose a Whittle index based Q-Learning\nmechanism and show that it converges to the optimal solution. Our method\nimproves over existing learning-based methods for RMABs on multiple benchmarks\nfrom literature and also on the maternal healthcare dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:44:55 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 22:37:04 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Biswas", "Arpita", ""], ["Aggarwal", "Gaurav", ""], ["Varakantham", "Pradeep", ""], ["Tambe", "Milind", ""]]}, {"id": "2105.07985", "submitter": "Franziska Boenisch", "authors": "Franziska Boenisch, Philip Sperl, Konstantin B\\\"ottinger", "title": "Gradient Masking and the Underestimated Robustness Threats of\n  Differential Privacy in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important problem in deep learning is the privacy and security of neural\nnetworks (NNs). Both aspects have long been considered separately. To date, it\nis still poorly understood how privacy enhancing training affects the\nrobustness of NNs. This paper experimentally evaluates the impact of training\nwith Differential Privacy (DP), a standard method for privacy preservation, on\nmodel vulnerability against a broad range of adversarial attacks. The results\nsuggest that private models are less robust than their non-private\ncounterparts, and that adversarial examples transfer better among DP models\nthan between non-private and private ones. Furthermore, detailed analyses of DP\nand non-DP models suggest significant differences between their gradients.\nAdditionally, this work is the first to observe that an unfavorable choice of\nparameters in DP training can lead to gradient masking, and, thereby, results\nin a wrong sense of security.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:10:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Boenisch", "Franziska", ""], ["Sperl", "Philip", ""], ["B\u00f6ttinger", "Konstantin", ""]]}, {"id": "2105.07986", "submitter": "David Montero", "authors": "J. Javier Yebes, David Montero, Ignacio Arriola", "title": "Learning to Automatically Catch Potholes in Worldwide Road Scene Images", "comments": "in IEEE Intelligent Transportation Systems Magazine", "journal-ref": null, "doi": "10.1109/MITS.2019.2926370", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Among several road hazards that are present in any paved way in the world,\npotholes are one of the most annoying and also involving higher maintenance\ncosts. There exists an increasing interest on the automated detection of these\nhazards enabled by technological and research progress. Our research work\ntackled the challenge of pothole detection from images of real world road\nscenes. The main novelty resides on the application of the latest progress in\nAI to learn the visual appearance of potholes. We built a large dataset of\nimages with pothole annotations. They contained road scenes from different\ncities in the world, taken with different cameras, vehicles and viewpoints\nunder varied environmental conditions. Then, we fine-tuned four different\nobject detection models based on Faster R-CNN and SSD deep neural networks. We\nachieved high average precision and the pothole detector was tested on the\nNvidia DrivePX2 platform with GPGPU capability, which can be embedded on\nvehicles. Moreover, it was deployed on a real vehicle to notify the detected\npotholes to a given IoT platform as part of AUTOPILOT H2020 project.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:10:58 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 07:15:56 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yebes", "J. Javier", ""], ["Montero", "David", ""], ["Arriola", "Ignacio", ""]]}, {"id": "2105.07998", "submitter": "Swagat Kumar", "authors": "Swagat Kumar", "title": "Controlling an Inverted Pendulum with Policy Gradient Methods-A Tutorial", "comments": "8 pages, 3 figures, 2 tables etc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides the details of implementing two important policy gradient\nmethods to solve the inverted pendulum problem. These are namely the Deep\nDeterministic Policy Gradient (DDPG) and the Proximal Policy Optimization (PPO)\nalgorithm. The problem is solved by using an actor-critic model where an\nactor-network is used to learn the policy function and a critic network is to\nevaluate the actor-network by learning to estimate the Q function. Apart from\nbriefly explaining the mathematics behind these two algorithms, the details of\npython implementation are provided which helps in demystifying the underlying\ncomplexity of the algorithm. In the process, the readers will be introduced to\nOpenAI/Gym, Tensorflow 2.x and Keras utilities used for implementing the above\nconcepts.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:30:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kumar", "Swagat", ""]]}, {"id": "2105.08005", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi, Chiranjib Bhattacharyya, Ravi Kannan, David P. Woodruff\n  and Samson Zhou", "title": "Learning a Latent Simplex in Input-Sparsity Time", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of learning a latent $k$-vertex simplex\n$K\\subset\\mathbb{R}^d$, given access to $A\\in\\mathbb{R}^{d\\times n}$, which can\nbe viewed as a data matrix with $n$ points that are obtained by randomly\nperturbing latent points in the simplex $K$ (potentially beyond $K$). A large\nclass of latent variable models, such as adversarial clustering, mixed\nmembership stochastic block models, and topic models can be cast as learning a\nlatent simplex. Bhattacharyya and Kannan (SODA, 2020) give an algorithm for\nlearning such a latent simplex in time roughly $O(k\\cdot\\textrm{nnz}(A))$,\nwhere $\\textrm{nnz}(A)$ is the number of non-zeros in $A$. We show that the\ndependence on $k$ in the running time is unnecessary given a natural assumption\nabout the mass of the top $k$ singular values of $A$, which holds in many of\nthese applications. Further, we show this assumption is necessary, as otherwise\nan algorithm for learning a latent simplex would imply an algorithmic\nbreakthrough for spectral low rank approximation.\n  At a high level, Bhattacharyya and Kannan provide an adaptive algorithm that\nmakes $k$ matrix-vector product queries to $A$ and each query is a function of\nall queries preceding it. Since each matrix-vector product requires\n$\\textrm{nnz}(A)$ time, their overall running time appears unavoidable.\nInstead, we obtain a low-rank approximation to $A$ in input-sparsity time and\nshow that the column space thus obtained has small $\\sin\\Theta$ (angular)\ndistance to the right top-$k$ singular space of $A$. Our algorithm then selects\n$k$ points in the low-rank subspace with the largest inner product with $k$\ncarefully chosen random vectors. By working in the low-rank subspace, we avoid\nreading the entire matrix in each iteration and thus circumvent the\n$\\Theta(k\\cdot\\textrm{nnz}(A))$ running time.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:40:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Bhattacharyya", "Chiranjib", ""], ["Kannan", "Ravi", ""], ["Woodruff", "David P.", ""], ["Zhou", "Samson", ""]]}, {"id": "2105.08007", "submitter": "Lun Du", "authors": "Lun Du, Xu Chen, Fei Gao, Kunqing Xie, Shi Han and Dongmei Zhang", "title": "Understanding and Improvement of Adversarial Training for Network\n  Embedding from an Optimization Perspective", "comments": "9 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Embedding aims to learn a function mapping the nodes to Euclidean\nspace contribute to multiple learning analysis tasks on networks. However, the\nnoisy information behind the real-world networks and the overfitting problem\nboth negatively impact the quality of embedding vectors. To tackle these\nproblems, researchers utilize Adversarial Training for Network Embedding\n(AdvTNE) and achieve state-of-the-art performance. Unlike the mainstream\nmethods introducing perturbations on the network structure or the data feature,\nAdvTNE directly perturbs the model parameters, which provides a new chance to\nunderstand the mechanism behind. In this paper, we explain AdvTNE theoretically\nfrom an optimization perspective. Considering the Power-law property of\nnetworks and the optimization objective, we analyze the reason for its\nexcellent results. According to the above analysis, we propose a new activation\nto enhance the performance of AdvTNE. We conduct extensive experiments on four\nreal networks to validate the effectiveness of our method in node\nclassification and link prediction. The results demonstrate that our method is\nsuperior to the state-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:41:53 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Du", "Lun", ""], ["Chen", "Xu", ""], ["Gao", "Fei", ""], ["Xie", "Kunqing", ""], ["Han", "Shi", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2105.08008", "submitter": "Marco Valentino", "authors": "Julia Rozanova, Deborah Ferreira, Mokanarangan Thayaparan, Marco\n  Valentino, Andr\\'e Freitas", "title": "Supporting Context Monotonicity Abstractions in Neural NLI Models", "comments": "NALOMA'21 (NAtural LOgic Meets MAchine Learning) @IWCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language contexts display logical regularities with respect to\nsubstitutions of related concepts: these are captured in a functional\norder-theoretic property called monotonicity. For a certain class of NLI\nproblems where the resulting entailment label depends only on the context\nmonotonicity and the relation between the substituted concepts, we build on\nprevious techniques that aim to improve the performance of NLI models for these\nproblems, as consistent performance across both upward and downward monotone\ncontexts still seems difficult to attain even for state-of-the-art models. To\nthis end, we reframe the problem of context monotonicity classification to make\nit compatible with transformer-based pre-trained NLI models and add this task\nto the training pipeline. Furthermore, we introduce a sound and complete\nsimplified monotonicity logic formalism which describes our treatment of\ncontexts as abstract units. Using the notions in our formalism, we adapt\ntargeted challenge sets to investigate whether an intermediate context\nmonotonicity classification task can aid NLI models' performance on examples\nexhibiting monotonicity reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:43:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Rozanova", "Julia", ""], ["Ferreira", "Deborah", ""], ["Thayaparan", "Mokanarangan", ""], ["Valentino", "Marco", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2105.08013", "submitter": "Benjamin Seiler", "authors": "Benjamin B. Seiler, Masayoshi Mase, Art B. Owen", "title": "What makes you unique?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a uniqueness Shapley measure to compare the extent to\nwhich different variables are able to identify a subject. Revealing the value\nof a variable on subject $t$ shrinks the set of possible subjects that $t$\ncould be. The extent of the shrinkage depends on which other variables have\nalso been revealed. We use Shapley value to combine all of the reductions in\nlog cardinality due to revealing a variable after some subset of the other\nvariables has been revealed. This uniqueness Shapley measure can be aggregated\nover subjects where it becomes a weighted sum of conditional entropies.\nAggregation over subsets of subjects can address questions like how identifying\nis age for people of a given zip code. Such aggregates have a corresponding\nexpression in terms of cross entropies. We use uniqueness Shapley to\ninvestigate the differential effects of revealing variables from the North\nCarolina voter registration rolls and in identifying anomalous solar flares. An\nenormous speedup (approaching 2000 fold in one example) is obtained by using\nthe all dimension trees of Moore and Lee (1998) to store the cardinalities we\nneed.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:53:16 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Seiler", "Benjamin B.", ""], ["Mase", "Masayoshi", ""], ["Owen", "Art B.", ""]]}, {"id": "2105.08023", "submitter": "Kun Yuan", "authors": "Kun Yuan and Sulaiman A. Alghunaim", "title": "Removing Data Heterogeneity Influence Enhances Network Topology\n  Dependence of Decentralized SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider decentralized stochastic optimization problems where a network of\nagents each owns a local cost function cooperate to find a minimizer of the\nglobal-averaged cost. A widely studied decentralized algorithm for this problem\nis D-SGD in which each node applies a stochastic gradient descent step, then\naverages its estimate with its neighbors. D-SGD is attractive due to its\nefficient single-iteration communication and can achieve linear speedup in\nconvergence (in terms of the network size). However, D-SGD is very sensitive to\nthe network topology. For smooth objective functions, the transient stage\n(which measures how fast the algorithm can reach the linear speedup stage) of\nD-SGD is on the order of $O(n/(1-\\beta)^2)$ and $O(n^3/(1-\\beta)^4)$ for\nstrongly convex and generally convex cost functions, respectively, where\n$1-\\beta \\in (0,1)$ is a topology-dependent quantity that approaches $0$ for a\nlarge and sparse network. Hence, D-SGD suffers from slow convergence for large\nand sparse networks.\n  In this work, we study the non-asymptotic convergence property of the\nD$^2$/Exact-diffusion algorithm. By eliminating the influence of data\nheterogeneity between nodes, D$^2$/Exact-diffusion is shown to have an enhanced\ntransient stage that are on the order of $O(n/(1-\\beta))$ and\n$O(n^3/(1-\\beta)^2)$ for strongly convex and generally convex cost functions,\nrespectively. Moreover, we provide a lower bound of the transient stage of\nD-SGD under homogeneous data distributions, which coincides with the transient\nstage of D$^2$/Exact-diffusion in the strongly-convex setting. These results\nshow that removing the influence of data heterogeneity can ameliorate the\nnetwork topology dependence of D-SGD. Compared with existing decentralized\nalgorithms bounds, D$^2$/Exact-diffusion is least sensitive to network\ntopology.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:16:52 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yuan", "Kun", ""], ["Alghunaim", "Sulaiman A.", ""]]}, {"id": "2105.08024", "submitter": "Yuting Wei", "authors": "Gen Li, Yuxin Chen, Yuejie Chi, Yuantao Gu, Yuting Wei", "title": "Sample-Efficient Reinforcement Learning Is Feasible for Linearly\n  Realizable MDPs with Limited Revisiting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-complexity models such as linear function representation play a pivotal\nrole in enabling sample-efficient reinforcement learning (RL). The current\npaper pertains to a scenario with value-based linear representation, which\npostulates the linear realizability of the optimal Q-function (also called the\n\"linear $Q^{\\star}$ problem\"). While linear realizability alone does not allow\nfor sample-efficient solutions in general, the presence of a large\nsub-optimality gap is a potential game changer, depending on the sampling\nmechanism in use. Informally, sample efficiency is achievable with a large\nsub-optimality gap when a generative model is available but is unfortunately\ninfeasible when we turn to standard online RL settings.\n  In this paper, we make progress towards understanding this linear $Q^{\\star}$\nproblem by investigating a new sampling protocol, which draws samples in an\nonline/exploratory fashion but allows one to backtrack and revisit previous\nstates in a controlled and infrequent manner. This protocol is more flexible\nthan the standard online RL setting, while being practically relevant and far\nmore restrictive than the generative model. We develop an algorithm tailored to\nthis setting, achieving a sample complexity that scales polynomially with the\nfeature dimension, the horizon, and the inverse sub-optimality gap, but not the\nsize of the state/action space. Our findings underscore the fundamental\ninterplay between sampling protocols and low-complexity structural\nrepresentation in RL.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:22:07 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Gen", ""], ["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""], ["Gu", "Yuantao", ""], ["Wei", "Yuting", ""]]}, {"id": "2105.08037", "submitter": "Haotian Gu", "authors": "Haotian Gu, Xin Guo", "title": "An SDE Framework for Adversarial Training, with Convergence and\n  Robustness Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has gained great popularity as one of the most effective\ndefenses for deep neural networks against adversarial perturbations on data\npoints. Consequently, research interests have grown in understanding the\nconvergence and robustness of adversarial training. This paper considers the\nmin-max game of adversarial training by alternating stochastic gradient\ndescent. It approximates the training process with a continuous-time\nstochastic-differential-equation (SDE). In particular, the error bound and\nconvergence analysis is established.\n  This SDE framework allows direct comparison between adversarial training and\nstochastic gradient descent; and confirms analytically the robustness of\nadversarial training from a (new) gradient-flow viewpoint. This analysis is\nthen corroborated via numerical studies.\n  To demonstrate the versatility of this SDE framework for algorithm design and\nparameter tuning, a stochastic control problem is formulated for learning rate\nadjustment, where the advantage of adaptive learning rate over fixed learning\nrate in terms of training loss is demonstrated through numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:41:56 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gu", "Haotian", ""], ["Guo", "Xin", ""]]}, {"id": "2105.08040", "submitter": "Burhaneddin Yaman", "authors": "Mehmet Ak\\c{c}akaya, Burhaneddin Yaman, Hyungjin Chung, Jong Chul Ye", "title": "Unsupervised Deep Learning Methods for Biological Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approaches have become the main research frontier for\nbiological image reconstruction problems thanks to their high performance,\nalong with their ultra-fast reconstruction times. However, due to the\ndifficulty of obtaining matched reference data for supervised learning, there\nhas been increasing interest in unsupervised learning approaches that do not\nneed paired reference data. In particular, self-supervised learning and\ngenerative models have been successfully used for various biological imaging\napplications. In this paper, we overview these approaches from a coherent\nperspective in the context of classical inverse problems, and discuss their\napplications to biological imaging.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:43:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ak\u00e7akaya", "Mehmet", ""], ["Yaman", "Burhaneddin", ""], ["Chung", "Hyungjin", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2105.08049", "submitter": "Yang Zhang", "authors": "Yang Zhang, Vahid Noroozi, Evelina Bakhturina, Boris Ginsburg", "title": "SGD-QA: Fast Schema-Guided Dialogue State Tracking for Unseen Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dialogue state tracking is an essential part of goal-oriented dialogue\nsystems, while most of these state tracking models often fail to handle unseen\nservices. In this paper, we propose SGD-QA, a simple and extensible model for\nschema-guided dialogue state tracking based on a question answering approach.\nThe proposed multi-pass model shares a single encoder between the domain\ninformation and dialogue utterance. The domain's description represents the\nquery and the dialogue utterance serves as the context. The model improves\nperformance on unseen services by at least 1.6x compared to single-pass\nbaseline models on the SGD dataset. SGD-QA shows competitive performance\ncompared to state-of-the-art multi-pass models while being significantly more\nefficient in terms of memory consumption and training performance. We provide a\nthorough discussion on the model with ablation study and error analysis.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:54:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "Yang", ""], ["Noroozi", "Vahid", ""], ["Bakhturina", "Evelina", ""], ["Ginsburg", "Boris", ""]]}, {"id": "2105.08050", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le", "title": "Pay Attention to MLPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have become one of the most important architectural innovations\nin deep learning and have enabled many breakthroughs over the past few years.\nHere we propose a simple network architecture, gMLP, based on MLPs with gating,\nand show that it can perform as well as Transformers in key language and vision\napplications. Our comparisons show that self-attention is not critical for\nVision Transformers, as gMLP can achieve the same accuracy. For BERT, our model\nachieves parity with Transformers on pretraining perplexity and is better on\nsome downstream NLP tasks. On finetuning tasks where gMLP performs worse,\nmaking the gMLP model substantially larger can close the gap with Transformers.\nIn general, our experiments show that gMLP can scale as well as Transformers\nover increased data and compute.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:55:04 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 20:24:06 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Hanxiao", ""], ["Dai", "Zihang", ""], ["So", "David R.", ""], ["Le", "Quoc V.", ""]]}, {"id": "2105.08053", "submitter": "Charles Ellis", "authors": "Charles A. Ellis, Mohammad S.E. Sendi, Sergey M. Plis, Robyn L.\n  Miller, and Vince D. Calhoun", "title": "Algorithm-Agnostic Explainability for Unsupervised Clustering", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Supervised machine learning explainability has greatly expanded in recent\nyears. However, the field of unsupervised clustering explainability has lagged\nbehind. Here, we, to the best of our knowledge, demonstrate for the first time\nhow model-agnostic methods for supervised machine learning explainability can\nbe adapted to provide algorithm-agnostic unsupervised clustering\nexplainability. We present two novel algorithm-agnostic explainability methods,\nglobal permutation percent change (G2PC) feature importance and local\nperturbation percent change (L2PC) feature importance, that can provide insight\ninto many clustering methods on a global level by identifying the relative\nimportance of features to a clustering algorithm and on a local level by\nidentifying the relative importance of features to the clustering of individual\nsamples. We demonstrate the utility of the methods for explaining five popular\nclustering algorithms on low-dimensional, ground-truth synthetic datasets and\non high-dimensional functional network connectivity (FNC) data extracted from a\nresting state functional magnetic resonance imaging (rs-fMRI) dataset of 151\nsubjects with schizophrenia (SZ) and 160 healthy controls (HC). Our proposed\nexplainability methods robustly identify the relative importance of features\nacross multiple clustering methods and could facilitate new insights into many\napplications. We hope that this study will greatly accelerate the development\nof the field of clustering explainability.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:58:55 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ellis", "Charles A.", ""], ["Sendi", "Mohammad S. E.", ""], ["Plis", "Sergey M.", ""], ["Miller", "Robyn L.", ""], ["Calhoun", "Vince D.", ""]]}, {"id": "2105.08059", "submitter": "Salman Ul Hassan Dar", "authors": "Yilmaz Korkmaz, Salman UH Dar, Mahmut Yurt, Muzaffer \\\"Ozbey, Tolga\n  \\c{C}ukur", "title": "Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning has swiftly become a workhorse for accelerated MRI\nin recent years, offering state-of-the-art performance in image reconstruction\nfrom undersampled acquisitions. Training deep supervised models requires large\ndatasets of undersampled and fully-sampled acquisitions typically from a\nmatching set of subjects. Given scarce access to large medical datasets, this\nlimitation has sparked interest in unsupervised methods that reduce reliance on\nfully-sampled ground-truth data. A common framework is based on the deep image\nprior, where network-driven regularization is enforced directly during\ninference on undersampled acquisitions. Yet, canonical convolutional\narchitectures are suboptimal in capturing long-range relationships, and\nrandomly initialized networks may hamper convergence. To address these\nlimitations, here we introduce a novel unsupervised MRI reconstruction method\nbased on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a\ndeep adversarial network with cross-attention transformer blocks to map noise\nand latent variables onto MR images. This unconditional network learns a\nhigh-quality MRI prior in a self-supervised encoding task. A zero-shot\nreconstruction is performed on undersampled test data, where inference is\nperformed by optimizing network parameters, latent and noise variables to\nensure maximal consistency to multi-coil MRI data. Comprehensive experiments on\nbrain MRI datasets clearly demonstrate the superior performance of SLATER\nagainst several state-of-the-art unsupervised methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 02:01:21 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 12:37:20 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Korkmaz", "Yilmaz", ""], ["Dar", "Salman UH", ""], ["Yurt", "Mahmut", ""], ["\u00d6zbey", "Muzaffer", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "2105.08086", "submitter": "Elizabeth Bennewitz", "authors": "Elizabeth R. Bennewitz, Florian Hopfmueller, Bohdan Kulchytskyy, Juan\n  Carrasquilla and Pooya Ronagh", "title": "Neural Error Mitigation of Near-Term Quantum Simulations", "comments": "18 pages, 4 main figures, 6 supplementary figures, 1 supplementary\n  table", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the promising applications of early quantum computers is the\nsimulation of quantum systems. Variational methods for near-term quantum\ncomputers, such as the variational quantum eigensolver (VQE), are a promising\napproach to finding ground states of quantum systems relevant in physics,\nchemistry, and materials science. These approaches, however, are constrained by\nthe effects of noise as well as the limited quantum resources of near-term\nquantum hardware, motivating the need for quantum error mitigation techniques\nto reduce the effects of noise. Here we introduce $\\textit{neural error\nmitigation}$, a novel method that uses neural networks to improve estimates of\nground states and ground-state observables obtained using VQE on near-term\nquantum computers. To demonstrate our method's versatility, we apply neural\nerror mitigation to finding the ground states of H$_2$ and LiH molecular\nHamiltonians, as well as the lattice Schwinger model. Our results show that\nneural error mitigation improves the numerical and experimental VQE computation\nto yield low-energy errors, low infidelities, and accurate estimations of\nmore-complex observables like order parameters and entanglement entropy,\nwithout requiring additional quantum resources. Additionally, neural error\nmitigation is agnostic to both the quantum hardware and the particular noise\nchannel, making it a versatile tool for quantum simulation. Applying quantum\nmany-body machine learning techniques to error mitigation, our method is a\npromising strategy for extending the reach of near-term quantum computers to\nsolve complex quantum simulation problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:00:57 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bennewitz", "Elizabeth R.", ""], ["Hopfmueller", "Florian", ""], ["Kulchytskyy", "Bohdan", ""], ["Carrasquilla", "Juan", ""], ["Ronagh", "Pooya", ""]]}, {"id": "2105.08093", "submitter": "Gaurav Batra", "authors": "Gaurav Batra, Naresh Manwani", "title": "Multiclass Classification using dilute bandit feedback", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new online learning framework for multiclass\nclassification called learning with diluted bandit feedback. At every time\nstep, the algorithm predicts a candidate label set instead of a single label\nfor the observed example. It then receives feedback from the environment\nwhether the actual label lies in this candidate label set or not. This feedback\nis called \"diluted bandit feedback\". Learning in this setting is even more\nchallenging than the bandit feedback setting, as there is more uncertainty in\nthe supervision. We propose an algorithm for multiclass classification using\ndilute bandit feedback (MC-DBF), which uses the exploration-exploitation\nstrategy to predict the candidate set in each trial. We show that the proposed\nalgorithm achieves O(T^{1-\\frac{1}{m+2}}) mistake bound if candidate label set\nsize (in each step) is m. We demonstrate the effectiveness of the proposed\napproach with extensive simulations.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:05:34 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Batra", "Gaurav", ""], ["Manwani", "Naresh", ""]]}, {"id": "2105.08095", "submitter": "Amin Nikanjam", "authors": "Amin Nikanjam, Houssem Ben Braiek, Mohammad Mehdi Morovati, Foutse\n  Khomh", "title": "Automatic Fault Detection for Deep Learning Programs Using Graph\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, we are witnessing an increasing demand in both corporates and\nacademia for exploiting Deep Learning (DL) to solve complex real-world\nproblems. A DL program encodes the network structure of a desirable DL model\nand the process by which the model learns from the training dataset. Like any\nsoftware, a DL program can be faulty, which implies substantial challenges of\nsoftware quality assurance, especially in safety-critical domains. It is\ntherefore crucial to equip DL development teams with efficient fault detection\ntechniques and tools. In this paper, we propose NeuraLint, a model-based fault\ndetection approach for DL programs, using meta-modelling and graph\ntransformations. First, we design a meta-model for DL programs that includes\ntheir base skeleton and fundamental properties. Then, we construct a\ngraph-based verification process that covers 23 rules defined on top of the\nmeta-model and implemented as graph transformations to detect faults and design\ninefficiencies in the generated models (i.e., instances of the meta-model).\nFirst, the proposed approach is evaluated by finding faults and design\ninefficiencies in 28 synthesized examples built from common problems reported\nin the literature. Then NeuraLint successfully finds 64 faults and design\ninefficiencies in 34 real-world DL programs extracted from Stack Overflow posts\nand GitHub repositories. The results show that NeuraLint effectively detects\nfaults and design issues in both synthesized and real-world examples with a\nrecall of 70.5 % and a precision of 100 %. Although the proposed meta-model is\ndesigned for feedforward neural networks, it can be extended to support other\nneural network architectures such as recurrent neural networks. Researchers can\nalso expand our set of verification rules to cover more types of issues in DL\nprograms.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:06:11 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 02:17:12 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Nikanjam", "Amin", ""], ["Braiek", "Houssem Ben", ""], ["Morovati", "Mohammad Mehdi", ""], ["Khomh", "Foutse", ""]]}, {"id": "2105.08100", "submitter": "Nishanth Anandanadarajah", "authors": "Nishanth. Anandanadarajah, C.H. Chu, R. Loganantharaj", "title": "An Integrated Deep Learning and Dynamic Programming Method for\n  Predicting Tumor Suppressor Genes, Oncogenes, and Fusion from PDB Structures", "comments": null, "journal-ref": "Computers in Biology and Medicine, Volume 133, 2021, 104323, ISSN\n  0010-4825", "doi": "10.1016/j.compbiomed.2021.104323", "report-no": null, "categories": "cs.LG eess.IV q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mutations in proto-oncogenes (ONGO) and the loss of regulatory function of\ntumor suppression genes (TSG) are the common underlying mechanism for\nuncontrolled tumor growth. While cancer is a heterogeneous complex of distinct\ndiseases, finding the potentiality of the genes related functionality to ONGO\nor TSG through computational studies can help develop drugs that target the\ndisease. This paper proposes a classification method that starts with a\npreprocessing stage to extract the feature map sets from the input 3D protein\nstructural information. The next stage is a deep convolutional neural network\nstage (DCNN) that outputs the probability of functional classification of\ngenes. We explored and tested two approaches: in Approach 1, all filtered and\ncleaned 3D-protein-structures (PDB) are pooled together, whereas in Approach 2,\nthe primary structures and their corresponding PDBs are separated according to\nthe genes' primary structural information. Following the DCNN stage, a dynamic\nprogramming-based method is used to determine the final prediction of the\nprimary structures' functionality. We validated our proposed method using the\nCOSMIC online database. For the ONGO vs TSG classification problem, the AUROC\nof the DCNN stage for Approach 1 and Approach 2 DCNN are 0.978 and 0.765,\nrespectively. The AUROCs of the final genes' primary structure functionality\nclassification for Approach 1 and Approach 2 are 0.989, and 0.879,\nrespectively. For comparison, the current state-of-the-art reported AUROC is\n0.924.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:18:57 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Anandanadarajah", "Nishanth.", ""], ["Chu", "C. H.", ""], ["Loganantharaj", "R.", ""]]}, {"id": "2105.08111", "submitter": "Thomas Schumacher", "authors": "Thomas Schumacher", "title": "Livewired Neural Networks: Making Neurons That Fire Together Wire\n  Together", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Until recently, artificial neural networks were typically designed with a\nfixed network structure. Here, I argue that network structure is highly\nrelevant to function, and therefore neural networks should be livewired\n(Eagleman 2020): dynamically rewired to reflect relationships between higher\norder representations of the external environment identified by coincident\nactivations in individual neurons. I discuss how this approach may enable such\nnetworks to build compositional world models that operate on symbols and that\nachieve few-shot learning, capabilities thought by many to be critical to\nhuman-level cognition. Here, I also 1) discuss how such livewired neural\nnetworks maximize the information the environment provides to a model, 2)\nexplore evidence indicating that livewiring is implemented in the brain, guided\nby glial cells, 3) discuss how livewiring may give rise to the associative\nemergent behaviors of brains, and 4) suggest paths for future research using\nlivewired networks to understand and create human-like reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:45:22 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Schumacher", "Thomas", ""]]}, {"id": "2105.08120", "submitter": "Sergey Afanasiev", "authors": "Sergey Afanasiev, Anastasiya Smirnova and Diana Kotereva", "title": "Itsy Bitsy SpiderNet: Fully Connected Residual Network for Fraud\n  Detection", "comments": "11 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of high technology, the scope of fraud is increasing,\nresulting in annual losses of billions of dollars worldwide. The preventive\nprotection measures become obsolete and vulnerable over time, so effective\ndetective tools are needed. In this paper, we propose a convolutional neural\nnetwork architecture SpiderNet designed to solve fraud detection problems. We\nnoticed that the principles of pooling and convolutional layers in neural\nnetworks are very similar to the way antifraud analysts work when conducting\ninvestigations. Moreover, the skip-connections used in neural networks make the\nusage of features of various power in antifraud models possible. Our\nexperiments have shown that SpiderNet provides better quality compared to\nRandom Forest and adapted for antifraud modeling problems 1D-CNN, 1D-DenseNet,\nF-DenseNet neural networks. We also propose new approaches for fraud feature\nengineering called B-tests and W-tests, which generalize the concepts of\nBenford's Law for fraud anomalies detection. Our results showed that B-tests\nand W-tests give a significant increase to the quality of our antifraud models.\nThe SpiderNet code is available at https://github.com/aasmirnova24/SpiderNet\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 19:16:32 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Afanasiev", "Sergey", ""], ["Smirnova", "Anastasiya", ""], ["Kotereva", "Diana", ""]]}, {"id": "2105.08140", "submitter": "Yue Wu", "authors": "Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian\n  Zhang, Ruslan Salakhutdinov, Hanlin Goh", "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning", "comments": "To appear in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline Reinforcement Learning promises to learn effective policies from\npreviously-collected, static datasets without the need for exploration.\nHowever, existing Q-learning and actor-critic based off-policy RL algorithms\nfail when bootstrapping from out-of-distribution (OOD) actions or states. We\nhypothesize that a key missing ingredient from the existing methods is a proper\ntreatment of uncertainty in the offline setting. We propose Uncertainty\nWeighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs\nand down-weights their contribution in the training objectives accordingly.\nImplementation-wise, we adopt a practical and effective dropout-based\nuncertainty estimation method that introduces very little overhead over\nexisting RL algorithms. Empirically, we observe that UWAC substantially\nimproves model stability during training. In addition, UWAC out-performs\nexisting offline RL methods on a variety of competitive tasks, and achieves\nsignificant performance gains over the state-of-the-art baseline on datasets\nwith sparse demonstrations collected from human experts.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:16:46 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Wu", "Yue", ""], ["Zhai", "Shuangfei", ""], ["Srivastava", "Nitish", ""], ["Susskind", "Joshua", ""], ["Zhang", "Jian", ""], ["Salakhutdinov", "Ruslan", ""], ["Goh", "Hanlin", ""]]}, {"id": "2105.08147", "submitter": "Vignav Ramesh", "authors": "Vignav Ramesh, Blaine Rister, Daniel L. Rubin", "title": "COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN\n  on Chest X-rays Automatically Computed from Volumetric CTs", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently\nobtained to determine the extent of lung disease and are a valuable source of\ndata for creating artificial intelligence models. Most work to date assessing\ndisease severity on chest imaging has focused on segmenting computed tomography\n(CT) images; however, given that CTs are performed much less frequently than\nchest X-rays for COVID-19 patients, automated lung lesion segmentation on chest\nX-rays could be clinically valuable. There currently exists a universal\nshortage of chest X-rays with ground truth COVID-19 lung lesion annotations,\nand manually contouring lung opacities is a tedious, labor-intensive task. To\naccelerate severity detection and augment the amount of publicly available\nchest X-ray training data for supervised deep learning (DL) models, we leverage\nexisting annotated CT images to generate frontal projection \"chest X-ray\"\nimages for training COVID-19 chest X-ray models. In this paper, we propose an\nautomated pipeline for segmentation of COVID-19 lung lesions on chest X-rays\ncomprised of a Mask R-CNN trained on a mixed dataset of open-source chest\nX-rays and coronal X-ray projections computed from annotated volumetric CTs. On\na test set containing 40 chest X-rays of COVID-19 positive patients, our model\nachieved IoU scores of 0.81 $\\pm$ 0.03 and 0.79 $\\pm$ 0.03 when trained on a\ndataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50\nprojections from CTs, respectively. Our model far outperforms current baselines\nwith limited supervised training and may assist in automated COVID-19 severity\nquantification on chest X-rays.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:27:32 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 00:36:21 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ramesh", "Vignav", ""], ["Rister", "Blaine", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "2105.08149", "submitter": "Xiaochen Yang", "authors": "Xiaoxu Li, Xiaochen Yang, Zhanyu Ma, Jing-Hao Xue", "title": "Deep Metric Learning for Few-Shot Image Classification: A Selective\n  Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot image classification is a challenging problem which aims to achieve\nthe human level of recognition based only on a small number of images. Deep\nlearning algorithms such as meta-learning, transfer learning, and metric\nlearning have been employed recently and achieved the state-of-the-art\nperformance. In this survey, we review representative deep metric learning\nmethods for few-shot classification, and categorize them into three groups\naccording to the major problems and novelties they focus on. We conclude this\nreview with a discussion on current challenges and future trends in few-shot\nimage classification.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:27:59 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Xiaoxu", ""], ["Yang", "Xiaochen", ""], ["Ma", "Zhanyu", ""], ["Xue", "Jing-Hao", ""]]}, {"id": "2105.08150", "submitter": "Luke Eglington", "authors": "Philip I. Pavlik Jr, Luke G. Eglington", "title": "Modeling the EdNet Dataset with Logistic Regression", "comments": "5 pages, AAAI Workshop on AI in Education (Imagining Post-COVID\n  Education with AI)", "journal-ref": "AAAI 2021 Workshop on AI in Education", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many of these challenges are won by neural network models created by\nfull-time artificial intelligence scientists. Due to this origin, they have a\nblack-box character that makes their use and application less clear to learning\nscientists. We describe our experience with competition from the perspective of\neducational data mining, a field founded in the learning sciences and connected\nwith roots in psychology and statistics. We describe our efforts from the\nperspectives of learning scientists and the challenges to our methods, some\nreal and some imagined. We also discuss some basic results in the Kaggle system\nand our thoughts on how those results may have been improved. Finally, we\ndescribe how learner model predictions are used to make pedagogical decisions\nfor students. Their practical use entails a) model predictions and b) a\ndecision rule (based on the predictions). We point out how increased model\naccuracy can be of limited practical utility, especially when paired with\nsimple decision rules and argue instead for the need to further investigate\noptimal decision rules.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:30:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Pavlik", "Philip I.", "Jr"], ["Eglington", "Luke G.", ""]]}, {"id": "2105.08157", "submitter": "Eric Chen", "authors": "Eric Z. Chen, Xiao Chen, Jingyuan Lyu, Qi Liu, Zhongqi Zhang, Yu Ding,\n  Shuheng Zhang, Terrence Chen, Jian Xu, and Shanhui Sun", "title": "Cardiac Functional Analysis with Cine MRI via Deep Learning\n  Reconstruction", "comments": "Presented at ISMRM 2021 as the digital poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrospectively gated cine (retro-cine) MRI is the clinical standard for\ncardiac functional analysis. Deep learning (DL) based methods have been\nproposed for the reconstruction of highly undersampled MRI data and show\nsuperior image quality and magnitude faster reconstruction time than CS-based\nmethods. Nevertheless, it remains unclear whether DL reconstruction is suitable\nfor cardiac function analysis. To address this question, in this study we\nevaluate and compare the cardiac functional values (EDV, ESV and EF for LV and\nRV, respectively) obtained from highly accelerated MRI acquisition using DL\nbased reconstruction algorithm (DL-cine) with values from CS-cine and\nconventional retro-cine. To the best of our knowledge, this is the first work\nto evaluate the cine MRI with deep learning reconstruction for cardiac function\nanalysis and compare it with other conventional methods. The cardiac functional\nvalues obtained from cine MRI with deep learning reconstruction are consistent\nwith values from clinical standard retro-cine MRI.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:53:23 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Eric Z.", ""], ["Chen", "Xiao", ""], ["Lyu", "Jingyuan", ""], ["Liu", "Qi", ""], ["Zhang", "Zhongqi", ""], ["Ding", "Yu", ""], ["Zhang", "Shuheng", ""], ["Chen", "Terrence", ""], ["Xu", "Jian", ""], ["Sun", "Shanhui", ""]]}, {"id": "2105.08158", "submitter": "Tao Li", "authors": "Tao Li, Guanze Peng, Quanyan Zhu and Tamer Basar", "title": "The Confluence of Networks, Games and Learning", "comments": "The manuscript has been submitted to IEEE control system magazine\n  under review, as part of the special issue \"Distributed Nash Equilibrium\n  Seeking over Networks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed significant advances in technologies and services\nin modern network applications, including smart grid management, wireless\ncommunication, cybersecurity as well as multi-agent autonomous systems.\nConsidering the heterogeneous nature of networked entities, emerging network\napplications call for game-theoretic models and learning-based approaches in\norder to create distributed network intelligence that responds to uncertainties\nand disruptions in a dynamic or an adversarial environment. This paper\narticulates the confluence of networks, games and learning, which establishes a\ntheoretical underpinning for understanding multi-agent decision-making over\nnetworks. We provide an selective overview of game-theoretic learning\nalgorithms within the framework of stochastic approximation theory, and\nassociated applications in some representative contexts of modern network\nsystems, such as the next generation wireless communication networks, the smart\ngrid and distributed machine learning. In addition to existing research works\non game-theoretic learning over networks, we highlight several new angles and\nresearch endeavors on learning in games that are related to recent developments\nin artificial intelligence. Some of the new angles extrapolate from our own\nresearch interests. The overall objective of the paper is to provide the reader\na clear picture of the strengths and challenges of adopting game-theoretic\nlearning methods within the context of network systems, and further to identify\nfruitful future research directions on both theoretical and applied studies.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:54:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Tao", ""], ["Peng", "Guanze", ""], ["Zhu", "Quanyan", ""], ["Basar", "Tamer", ""]]}, {"id": "2105.08163", "submitter": "Eric Chen", "authors": "Eric Z. Chen, Yongquan Ye, Xiao Chen, Jingyuan Lyu, Zhongqi Zhang,\n  Yichen Hu, Terrence Chen, Jian Xu, and Shanhui Sun", "title": "Accelerating 3D MULTIPLEX MRI Reconstruction with Deep Learning", "comments": "Presented at ISMRM 2021 as the digital poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-contrast MRI images provide complementary contrast information about\nthe characteristics of anatomical structures and are commonly used in clinical\npractice. Recently, a multi-flip-angle (FA) and multi-echo GRE method\n(MULTIPLEX MRI) has been developed to simultaneously acquire multiple\nparametric images with just one single scan. However, it poses two challenges\nfor MULTIPLEX to be used in the 3D high-resolution setting: a relatively long\nscan time and the huge amount of 3D multi-contrast data for reconstruction.\nCurrently, no DL based method has been proposed for 3D MULTIPLEX data\nreconstruction. We propose a deep learning framework for undersampled 3D MRI\ndata reconstruction and apply it to MULTIPLEX MRI. The proposed deep learning\nmethod shows good performance in image quality and reconstruction time.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 21:06:14 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Eric Z.", ""], ["Ye", "Yongquan", ""], ["Chen", "Xiao", ""], ["Lyu", "Jingyuan", ""], ["Zhang", "Zhongqi", ""], ["Hu", "Yichen", ""], ["Chen", "Terrence", ""], ["Xu", "Jian", ""], ["Sun", "Shanhui", ""]]}, {"id": "2105.08164", "submitter": "John Thickstun", "authors": "Vivek Jayaram, John Thickstun", "title": "Parallel and Flexible Sampling from Autoregressive Models via Langevin\n  Dynamics", "comments": "16 pages, 8 figures, to appear in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an alternative approach to sampling from autoregressive\nmodels. Autoregressive models are typically sampled sequentially, according to\nthe transition dynamics defined by the model. Instead, we propose a sampling\nprocedure that initializes a sequence with white noise and follows a Markov\nchain defined by Langevin dynamics on the global log-likelihood of the\nsequence. This approach parallelizes the sampling process and generalizes to\nconditional sampling. Using an autoregressive model as a Bayesian prior, we can\nsteer the output of a generative model using a conditional likelihood or\nconstraints. We apply these techniques to autoregressive models in the visual\nand audio domains, with competitive results for audio source separation,\nsuper-resolution, and inpainting.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 21:07:02 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Jayaram", "Vivek", ""], ["Thickstun", "John", ""]]}, {"id": "2105.08165", "submitter": "Liming Luke Chen", "authors": "Sahraoui Dhelim, Liming Luke Chen, Huansheng Ning, Sajal K Das, Chris\n  Nugent, Devin Burns, Gerard Leavey, Dirk Pesch and Eleanor Bantry-White", "title": "Social Behavior and Mental Health: A Snapshot Survey under COVID-19\n  Pandemic", "comments": "Submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social media provides a channel for monitoring people's social\nbehaviors and their mental distress. Due to the restrictions imposed by\nCOVID-19 people are increasingly using online social networks to express their\nfeelings. Consequently, there is a significant amount of diverse user-generated\nsocial media content. However, COVID-19 pandemic has changed the way we live,\nstudy, socialize and recreate and this has affected our well-being and mental\nhealth problems. There are growing researches that leverage online social media\nanalysis to detect and assess user's mental status. In this paper, we survey\nthe literature of social media analysis for mental disorders detection, with a\nspecial focus on the studies conducted in the context of COVID-19 during\n2020-2021. Firstly, we classify the surveyed studies in terms of feature\nextraction types, varying from language usage patterns to aesthetic preferences\nand online behaviors. Secondly, we explore detection methods used for mental\ndisorders detection including machine learning and deep learning detection\nmethods. Finally, we discuss the challenges of mental disorder detection using\nsocial media data, including the privacy and ethical concerns, as well as the\ntechnical challenges of scaling and deploying such systems at large scales, and\ndiscuss the learnt lessons over the last few years.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 21:08:03 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Dhelim", "Sahraoui", ""], ["Chen", "Liming Luke", ""], ["Ning", "Huansheng", ""], ["Das", "Sajal K", ""], ["Nugent", "Chris", ""], ["Burns", "Devin", ""], ["Leavey", "Gerard", ""], ["Pesch", "Dirk", ""], ["Bantry-White", "Eleanor", ""]]}, {"id": "2105.08175", "submitter": "Guang Yang A", "authors": "Jun Lv, Guangyuan Li, Xiangrong Tong, Weibo Chen, Jiahao Huang,\n  Chengyan Wang, Guang Yang", "title": "Transfer Learning Enhanced Generative Adversarial Networks for\n  Multi-Channel MRI Reconstruction", "comments": "29 pages, 11 figures, accepted by CBM journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based generative adversarial networks (GAN) can effectively\nperform image reconstruction with under-sampled MR data. In general, a large\nnumber of training samples are required to improve the reconstruction\nperformance of a certain model. However, in real clinical applications, it is\ndifficult to obtain tens of thousands of raw patient data to train the model\nsince saving k-space data is not in the routine clinical flow. Therefore,\nenhancing the generalizability of a network based on small samples is urgently\nneeded. In this study, three novel applications were explored based on parallel\nimaging combined with the GAN model (PI-GAN) and transfer learning. The model\nwas pre-trained with public Calgary brain images and then fine-tuned for use in\n(1) patients with tumors in our center; (2) different anatomies, including knee\nand liver; (3) different k-space sampling masks with acceleration factors (AFs)\nof 2 and 6. As for the brain tumor dataset, the transfer learning results could\nremove the artifacts found in PI-GAN and yield smoother brain edges. The\ntransfer learning results for the knee and liver were superior to those of the\nPI-GAN model trained with its own dataset using a smaller number of training\ncases. However, the learning procedure converged more slowly in the knee\ndatasets compared to the learning in the brain tumor datasets. The\nreconstruction performance was improved by transfer learning both in the models\nwith AFs of 2 and 6. Of these two models, the one with AF=2 showed better\nresults. The results also showed that transfer learning with the pre-trained\nmodel could solve the problem of inconsistency between the training and test\ndatasets and facilitate generalization to unseen data.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 21:28:00 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lv", "Jun", ""], ["Li", "Guangyuan", ""], ["Tong", "Xiangrong", ""], ["Chen", "Weibo", ""], ["Huang", "Jiahao", ""], ["Wang", "Chengyan", ""], ["Yang", "Guang", ""]]}, {"id": "2105.08179", "submitter": "Yuening Li", "authors": "Yuening Li, Zhengzhang Chen, Daochen Zha, Mengnan Du, Denghui Zhang,\n  Haifeng Chen, Xia Hu", "title": "Learning Disentangled Representations for Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-series representation learning is a fundamental task for time-series\nanalysis. While significant progress has been made to achieve accurate\nrepresentations for downstream applications, the learned representations often\nlack interpretability and do not expose semantic meanings. Different from\nprevious efforts on the entangled feature space, we aim to extract the\nsemantic-rich temporal correlations in the latent interpretable factorized\nrepresentation of the data. Motivated by the success of disentangled\nrepresentation learning in computer vision, we study the possibility of\nlearning semantic-rich time-series representations, which remains unexplored\ndue to three main challenges: 1) sequential data structure introduces complex\ntemporal correlations and makes the latent representations hard to interpret,\n2) sequential models suffer from KL vanishing problem, and 3) interpretable\nsemantic concepts for time-series often rely on multiple factors instead of\nindividuals. To bridge the gap, we propose Disentangle Time Series (DTS), a\nnovel disentanglement enhancement framework for sequential data. Specifically,\nto generate hierarchical semantic concepts as the interpretable and\ndisentangled representation of time-series, DTS introduces multi-level\ndisentanglement strategies by covering both individual latent factors and group\nsemantic segments. We further theoretically show how to alleviate the KL\nvanishing problem: DTS introduces a mutual information maximization term, while\npreserving a heavier penalty on the total correlation and the dimension-wise KL\nto keep the disentanglement property. Experimental results on various\nreal-world benchmark datasets demonstrate that the representations learned by\nDTS achieve superior performance in downstream applications, with high\ninterpretability of semantic concepts.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 22:02:24 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 06:59:38 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Li", "Yuening", ""], ["Chen", "Zhengzhang", ""], ["Zha", "Daochen", ""], ["Du", "Mengnan", ""], ["Zhang", "Denghui", ""], ["Chen", "Haifeng", ""], ["Hu", "Xia", ""]]}, {"id": "2105.08180", "submitter": "Hao Yan", "authors": "Hao Yan, Nurretin Dorukhan Sergin, William A. Brenneman, Stephen\n  Joseph Lange, Shan Ba", "title": "Deep Multistage Multi-Task Learning for Quality Prediction of Multistage\n  Manufacturing Systems", "comments": "Accepted by Journal of Quality Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In multistage manufacturing systems, modeling multiple quality indices based\non the process sensing variables is important. However, the classic modeling\ntechnique predicts each quality variable one at a time, which fails to consider\nthe correlation within or between stages. We propose a deep multistage\nmulti-task learning framework to jointly predict all output sensing variables\nin a unified end-to-end learning framework according to the sequential system\narchitecture in the MMS. Our numerical studies and real case study have shown\nthat the new model has a superior performance compared to many benchmark\nmethods as well as great interpretability through developed variable selection\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 22:09:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yan", "Hao", ""], ["Sergin", "Nurretin Dorukhan", ""], ["Brenneman", "William A.", ""], ["Lange", "Stephen Joseph", ""], ["Ba", "Shan", ""]]}, {"id": "2105.08190", "submitter": "Athanasios Efthymiou", "authors": "Athanasios Efthymiou, Stevan Rudinac, Monika Kackovic, Marcel Worring,\n  Nachoem Wijnberg", "title": "Graph Neural Networks for Knowledge Enhanced Visual Representation of\n  Paintings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose ArtSAGENet, a novel multimodal architecture that integrates Graph\nNeural Networks (GNNs) and Convolutional Neural Networks (CNNs), to jointly\nlearn visual and semantic-based artistic representations. First, we illustrate\nthe significant advantages of multi-task learning for fine art analysis and\nargue that it is conceptually a much more appropriate setting in the fine art\ndomain than the single-task alternatives. We further demonstrate that several\nGNN architectures can outperform strong CNN baselines in a range of fine art\nanalysis tasks, such as style classification, artist attribution, creation\nperiod estimation, and tag prediction, while training them requires an order of\nmagnitude less computational time and only a small amount of labeled data.\nFinally, through extensive experimentation we show that our proposed ArtSAGENet\ncaptures and encodes valuable relational dependencies between the artists and\nthe artworks, surpassing the performance of traditional methods that rely\nsolely on the analysis of visual content. Our findings underline a great\npotential of integrating visual content and semantics for fine art analysis and\ncuration.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:05:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Efthymiou", "Athanasios", ""], ["Rudinac", "Stevan", ""], ["Kackovic", "Monika", ""], ["Worring", "Marcel", ""], ["Wijnberg", "Nachoem", ""]]}, {"id": "2105.08195", "submitter": "Samuel Daulton", "authors": "Samuel Daulton, Maximilian Balandat, Eytan Bakshy", "title": "Parallel Bayesian Optimization of Multiple Noisy Objectives with\n  Expected Hypervolume Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing multiple competing black-box objectives is a challenging problem\nin many fields, including science, engineering, and machine learning.\nMulti-objective Bayesian optimization is a powerful approach for identifying\nthe optimal trade-offs between the objectives with very few function\nevaluations. However, existing methods tend to perform poorly when observations\nare corrupted by noise, as they do not take into account uncertainty in the\ntrue Pareto frontier over the previously evaluated designs. We propose a novel\nacquisition function, NEHVI, that overcomes this important practical limitation\nby applying a Bayesian treatment to the popular expected hypervolume\nimprovement criterion to integrate over this uncertainty in the Pareto\nfrontier. We further argue that, even in the noiseless setting, the problem of\ngenerating multiple candidates in parallel reduces that of handling uncertainty\nin the Pareto frontier. Through this lens, we derive a natural parallel variant\nof NEHVI that can efficiently generate large batches of candidates. We provide\na theoretical convergence guarantee for optimizing a Monte Carlo estimator of\nNEHVI using exact sample-path gradients. Empirically, we show that NEHVI\nachieves state-of-the-art performance in noisy and large-batch environments.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:31:42 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Daulton", "Samuel", ""], ["Balandat", "Maximilian", ""], ["Bakshy", "Eytan", ""]]}, {"id": "2105.08204", "submitter": "Yotam Elor Dr", "authors": "Sajad Darabi and Yotam Elor", "title": "Synthesising Multi-Modal Minority Samples for Tabular Data", "comments": "Code can be found in\n  https://github.com/aws/sagemaker-scikit-learn-extension/tree/master/src/sagemaker_sklearn_extension/contrib/taei", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world binary classification tasks are in many cases imbalanced, where\nthe minority class is much smaller than the majority class. This skewness is\nchallenging for machine learning algorithms as they tend to focus on the\nmajority and greatly misclassify the minority. Adding synthetic minority\nsamples to the dataset before training the model is a popular technique to\naddress this difficulty and is commonly achieved by interpolating minority\nsamples. Tabular datasets are often multi-modal and contain discrete\n(categorical) features in addition to continuous ones which makes interpolation\nof samples non-trivial. To address this, we propose a latent space\ninterpolation framework which (1) maps the multi-modal samples to a dense\ncontinuous latent space using an autoencoder; (2) applies oversampling by\ninterpolation in the latent space; and (3) maps the synthetic samples back to\nthe original feature space. We defined metrics to directly evaluate the quality\nof the minority data generated and showed that our framework generates better\nsynthetic data than the existing methods. Furthermore, the superior synthetic\ndata yields better prediction quality in downstream binary classification\ntasks, as was demonstrated in extensive experiments with 27 publicly available\nreal-world datasets\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:54:08 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Darabi", "Sajad", ""], ["Elor", "Yotam", ""]]}, {"id": "2105.08221", "submitter": "Gia-Wei Chern", "authors": "Puhan Zhang, Gia-Wei Chern", "title": "Arrested phase separation in double-exchange models: machine-learning\n  enabled large-scale simulation", "comments": "7 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:2006.04205", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el cond-mat.dis-nn cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present large-scale dynamical simulations of electronic phase separation\nin the single-band double-exchange model based on deep-learning neural-network\npotentials trained from small-size exact diagonalization solutions. We uncover\nan intriguing correlation-induced freezing behavior as doped holes are\nsegregated from half-filled insulating background during equilibration. While\nthe aggregation of holes is stabilized by the formation of ferromagnetic\nclusters through Hund's coupling between charge carriers and local magnetic\nmoments, this stabilization also creates confining potentials for holes when\nantiferromagnetic spin-spin correlation is well developed in the background.\nThe dramatically reduced mobility of the self-trapped holes prematurely\ndisrupts further growth of the ferromagnetic clusters, leading to an arrested\nphase separation. Implications of our findings for phase separation dynamics in\nmaterials that exhibit colossal magnetoresistance effect are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 01:30:23 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhang", "Puhan", ""], ["Chern", "Gia-Wei", ""]]}, {"id": "2105.08232", "submitter": "Ziye Ma", "authors": "Ziye Ma, Yingjie Bi, Javad Lavaei, Somayeh Sojoudi", "title": "Sharp Restricted Isometry Property Bounds for Low-rank Matrix Recovery\n  Problems with Corrupted Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a general low-rank matrix recovery problem with\nlinear measurements corrupted by some noise. The objective is to understand\nunder what conditions on the restricted isometry property (RIP) of the problem\nlocal search methods can find the ground truth with a small error. By analyzing\nthe landscape of the non-convex problem, we first propose a global guarantee on\nthe maximum distance between an arbitrary local minimizer and the ground truth\nunder the assumption that the RIP constant is smaller than 1/2. We show that\nthis distance shrinks to zero as the intensity of the noise reduces. Our new\nguarantee is sharp in terms of the RIP constant and is much stronger than the\nexisting results. We then present a local guarantee for problems with an\narbitrary RIP constant, which states that any local minimizer is either\nconsiderably close to the ground truth or far away from it. The developed\nresults demonstrate how the noise intensity and the RIP constant of the problem\naffect the locations of the local minima relative to the true solution.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 02:17:59 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ma", "Ziye", ""], ["Bi", "Yingjie", ""], ["Lavaei", "Javad", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "2105.08233", "submitter": "Gang Qiao", "authors": "Gang Qiao, Weijie J. Su, Li Zhang", "title": "Oneshot Differentially Private Top-k Selection", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Being able to efficiently and accurately select the top-$k$ elements with\ndifferential privacy is an integral component of various private data analysis\ntasks. In this paper, we present the oneshot Laplace mechanism, which\ngeneralizes the well-known Report Noisy Max mechanism to reporting noisy\ntop-$k$ elements. We show that the oneshot Laplace mechanism with a noise level\nof $\\widetilde{O}(\\sqrt{k}/\\eps)$ is approximately differentially private.\nCompared to the previous peeling approach of running Report Noisy Max $k$\ntimes, the oneshot Laplace mechanism only adds noises and computes the top $k$\nelements once, hence much more efficient for large $k$. In addition, our proof\nof privacy relies on a novel coupling technique that bypasses the use of\ncomposition theorems. Finally, we present a novel application of efficient\ntop-$k$ selection in the classical problem of ranking from pairwise\ncomparisons.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 02:18:01 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 17:32:13 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Qiao", "Gang", ""], ["Su", "Weijie J.", ""], ["Zhang", "Li", ""]]}, {"id": "2105.08266", "submitter": "Abhinav Aggarwal", "authors": "Abhinav Aggarwal, Shiva Prasad Kasiviswanathan, Zekun Xu, Oluwaseyi\n  Feyisetan, Nathanael Teissier", "title": "Label Inference Attacks from Log-loss Scores", "comments": "Accepted at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Log-loss (also known as cross-entropy loss) metric is ubiquitously used\nacross machine learning applications to assess the performance of\nclassification algorithms. In this paper, we investigate the problem of\ninferring the labels of a dataset from single (or multiple) log-loss score(s),\nwithout any other access to the dataset. Surprisingly, we show that for any\nfinite number of label classes, it is possible to accurately infer the labels\nof the dataset from the reported log-loss score of a single carefully\nconstructed prediction vector if we allow arbitrary precision arithmetic.\nAdditionally, we present label inference algorithms (attacks) that succeed even\nunder addition of noise to the log-loss scores and under limited precision\narithmetic. All our algorithms rely on ideas from number theory and\ncombinatorics and require no model training. We run experimental simulations on\nsome real datasets to demonstrate the ease of running these attacks in\npractice.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:17:06 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 18:05:21 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Aggarwal", "Abhinav", ""], ["Kasiviswanathan", "Shiva Prasad", ""], ["Xu", "Zekun", ""], ["Feyisetan", "Oluwaseyi", ""], ["Teissier", "Nathanael", ""]]}, {"id": "2105.08268", "submitter": "Yan  Li", "authors": "Yan Li, Lingxiao Wang, Jiachen Yang, Ethan Wang, Zhaoran Wang, Tuo\n  Zhao, Hongyuan Zha", "title": "Permutation Invariant Policy Optimization for Mean-Field Multi-Agent\n  Reinforcement Learning: A Principled Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (MARL) becomes more challenging in the\npresence of more agents, as the capacity of the joint state and action spaces\ngrows exponentially in the number of agents. To address such a challenge of\nscale, we identify a class of cooperative MARL problems with permutation\ninvariance, and formulate it as a mean-field Markov decision processes (MDP).\nTo exploit the permutation invariance therein, we propose the mean-field\nproximal policy optimization (MF-PPO) algorithm, at the core of which is a\npermutation-invariant actor-critic neural architecture. We prove that MF-PPO\nattains the globally optimal policy at a sublinear rate of convergence.\nMoreover, its sample complexity is independent of the number of agents. We\nvalidate the theoretical advantages of MF-PPO with numerical experiments in the\nmulti-agent particle environment (MPE). In particular, we show that the\ninductive bias introduced by the permutation-invariant neural architecture\nenables MF-PPO to outperform existing competitors with a smaller number of\nmodel parameters, which is the key to its generalization performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:35:41 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Yan", ""], ["Wang", "Lingxiao", ""], ["Yang", "Jiachen", ""], ["Wang", "Ethan", ""], ["Wang", "Zhaoran", ""], ["Zhao", "Tuo", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2105.08269", "submitter": "Felix Juefei-Xu", "authors": "Qing Guo, Felix Juefei-Xu, Changqing Zhou, Yang Liu, Song Wang", "title": "Sparta: Spatially Attentive and Adversarially Robust Activation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) is one of the most effective ways for improving the\nrobustness of deep convolution neural networks (CNNs). Just like common network\ntraining, the effectiveness of AT relies on the design of basic network\ncomponents. In this paper, we conduct an in-depth study on the role of the\nbasic ReLU activation component in AT for robust CNNs. We find that the\nspatially-shared and input-independent properties of ReLU activation make CNNs\nless robust to white-box adversarial attacks with either standard or\nadversarial training. To address this problem, we extend ReLU to a novel Sparta\nactivation function (Spatially attentive and Adversarially Robust Activation),\nwhich enables CNNs to achieve both higher robustness, i.e., lower error rate on\nadversarial examples, and higher accuracy, i.e., lower error rate on clean\nexamples, than the existing state-of-the-art (SOTA) activation functions. We\nfurther study the relationship between Sparta and the SOTA activation\nfunctions, providing more insights about the advantages of our method. With\ncomprehensive experiments, we also find that the proposed method exhibits\nsuperior cross-CNN and cross-dataset transferability. For the former, the\nadversarially trained Sparta function for one CNN (e.g., ResNet-18) can be\nfixed and directly used to train another adversarially robust CNN (e.g.,\nResNet-34). For the latter, the Sparta function trained on one dataset (e.g.,\nCIFAR-10) can be employed to train adversarially robust CNNs on another dataset\n(e.g., SVHN). In both cases, Sparta leads to CNNs with higher robustness than\nthe vanilla ReLU, verifying the flexibility and versatility of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:36:46 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Zhou", "Changqing", ""], ["Liu", "Yang", ""], ["Wang", "Song", ""]]}, {"id": "2105.08275", "submitter": "Yuanming Li", "authors": "Yuanming Li, Huaizheng Zhang, Shanshan Jiang, Fan Yang, Yonggang Wen\n  and Yong Luo", "title": "ModelPS: An Interactive and Collaborative Platform for Editing\n  Pre-trained Models at Scale", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI engineering has emerged as a crucial discipline to democratize deep neural\nnetwork (DNN) models among software developers with a diverse background. In\nparticular, altering these DNN models in the deployment stage posits a\ntremendous challenge. In this research, we propose and develop a low-code\nsolution, ModelPS (an acronym for \"Model Photoshop\"), to enable and empower\ncollaborative DNN model editing and intelligent model serving. The ModelPS\nsolution embodies two transformative features: 1) a user-friendly web interface\nfor a developer team to share and edit DNN models pictorially, in a low-code\nfashion, and 2) a model genie engine in the backend to aid developers in\ncustomizing model editing configurations for given deployment requirements or\nconstraints. Our case studies with a wide range of deep learning (DL) models\nshow that the system can tremendously reduce both development and communication\noverheads with improved productivity. The code has been released as an\nopen-source package at GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:51:56 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 10:13:16 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Li", "Yuanming", ""], ["Zhang", "Huaizheng", ""], ["Jiang", "Shanshan", ""], ["Yang", "Fan", ""], ["Wen", "Yonggang", ""], ["Luo", "Yong", ""]]}, {"id": "2105.08279", "submitter": "Chang Liu", "authors": "Chang Liu, Guanjie Zheng, Zhenhui Li", "title": "Learning to Route via Theory-Guided Residual Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heavy traffic and related issues have always been concerns for modern\ncities. With the help of deep learning and reinforcement learning, people have\nproposed various policies to solve these traffic-related problems, such as\nsmart traffic signal control systems and taxi dispatching systems. People\nusually validate these policies in a city simulator, since directly applying\nthem in the real city introduces real cost. However, these policies validated\nin the city simulator may fail in the real city if the simulator is\nsignificantly different from the real world. To tackle this problem, we need to\nbuild a real-like traffic simulation system. Therefore, in this paper, we\npropose to learn the human routing model, which is one of the most essential\npart in the traffic simulator. This problem has two major challenges. First,\nhuman routing decisions are determined by multiple factors, besides the common\ntime and distance factor. Second, current historical routes data usually covers\njust a small portion of vehicles, due to privacy and device availability\nissues. To address these problems, we propose a theory-guided residual network\nmodel, where the theoretical part can emphasize the general principles for\nhuman routing decisions (e.g., fastest route), and the residual part can\ncapture drivable condition preferences (e.g., local road or highway). Since the\ntheoretical part is composed of traditional shortest path algorithms that do\nnot need data to train, our residual network can learn human routing models\nfrom limited data. We have conducted extensive experiments on multiple\nreal-world datasets to show the superior performance of our model, especially\nwith small data. Besides, we have also illustrated why our model is better at\nrecovering real routes through case studies.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 05:07:34 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 09:08:57 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Liu", "Chang", ""], ["Zheng", "Guanjie", ""], ["Li", "Zhenhui", ""]]}, {"id": "2105.08285", "submitter": "Zhaozhuo Xu", "authors": "Anshumali Shrivastava, Zhao Song, Zhaozhuo Xu", "title": "Sublinear Least-Squares Value Iteration via Locality Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present the first provable Least-Squares Value Iteration (LSVI) algorithms\nthat have runtime complexity sublinear in the number of actions. We formulate\nthe value function estimation procedure in value iteration as an approximate\nmaximum inner product search problem and propose a locality sensitive hashing\n(LSH) [Indyk and Motwani STOC'98, Andoni and Razenshteyn STOC'15, Andoni,\nLaarhoven, Razenshteyn and Waingarten SODA'17] type data structure to solve\nthis problem with sublinear time complexity. Moreover, we build the connections\nbetween the theory of approximate maximum inner product search and the regret\nanalysis of reinforcement learning. We prove that, with our choice of\napproximation factor, our Sublinear LSVI algorithms maintain the same regret as\nthe original LSVI algorithms while reducing the runtime complexity to sublinear\nin the number of actions. To the best of our knowledge, this is the first work\nthat combines LSH with reinforcement learning resulting in provable\nimprovements. We hope that our novel way of combining data-structures and\niterative algorithm will open the door for further study into cost reduction in\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 05:23:53 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 16:45:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Song", "Zhao", ""], ["Xu", "Zhaozhuo", ""]]}, {"id": "2105.08291", "submitter": "Wenjin Xie", "authors": "Wenjin Xie and Xiaomeng Wang and Tao Jia", "title": "Independent Asymmetric Embedding for Cascade Prediction on Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction for information diffusion on social networks has great\npractical significance in marketing and public opinion control. Cascade\nprediction aims to predict the individuals who will potentially repost the\nmessage on the social network. One kind of methods either exploit\ndemographical, structural, and temporal features for prediction, or explicitly\nrely on particular information diffusion models. The other kind of models are\nfully data-driven and do not require a global network structure. Thus massive\ndiffusion prediction models based on network embedding are proposed. These\nmodels embed the users into the latent space using their cascade information,\nbut are lack of consideration for the intervene among users when embedding. In\nthis paper, we propose an independent asymmetric embedding method to learn\nsocial embedding for cascade prediction. Different from existing methods, our\nmethod embeds each individual into one latent influence space and multiple\nlatent susceptibility spaces. Furthermore, our method captures the\nco-occurrence regulation of user combination in cascades to improve the\ncalculating effectiveness. The results of extensive experiments conducted on\nreal-world datasets verify both the predictive accuracy and cost-effectiveness\nof our approach.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 05:40:38 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 01:32:50 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xie", "Wenjin", ""], ["Wang", "Xiaomeng", ""], ["Jia", "Tao", ""]]}, {"id": "2105.08304", "submitter": "Jesus Cerquides", "authors": "Jesus Cerquides", "title": "Parametrization invariant interpretation of priors and posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we leverage on probability over Riemannian manifolds to rethink\nthe interpretation of priors and posteriors in Bayesian inference. The main\nmindshift is to move away from the idea that \"a prior distribution establishes\na probability distribution over the parameters of our model\" to the idea that\n\"a prior distribution establishes a probability distribution over probability\ndistributions\". To do that we assume that our probabilistic model is a\nRiemannian manifold with the Fisher metric. Under this mindset, any\ndistribution over probability distributions should be \"intrinsic\", that is,\ninvariant to the specific parametrization which is selected for the manifold.\nWe exemplify our ideas through a simple analysis of distributions over the\nmanifold of Bernoulli distributions. One of the major shortcomings of maximum a\nposteriori estimates is that they depend on the parametrization. Based on the\nunderstanding developed here, we can define the maximum a posteriori estimate\nwhich is independent of the parametrization.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 06:45:05 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 20:11:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Cerquides", "Jesus", ""]]}, {"id": "2105.08306", "submitter": "Kiran Koshy Thekumparampil", "authors": "Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong\n  Oh", "title": "Sample Efficient Linear Meta-Learning by Alternating Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning synthesizes and leverages the knowledge from a given set of\ntasks to rapidly learn new tasks using very little data. Meta-learning of\nlinear regression tasks, where the regressors lie in a low-dimensional\nsubspace, is an extensively-studied fundamental problem in this domain.\nHowever, existing results either guarantee highly suboptimal estimation errors,\nor require $\\Omega(d)$ samples per task (where $d$ is the data dimensionality)\nthus providing little gain over separately learning each task. In this work, we\nstudy a simple alternating minimization method (MLLAM), which alternately\nlearns the low-dimensional subspace and the regressors. We show that, for a\nconstant subspace dimension MLLAM obtains nearly-optimal estimation error,\ndespite requiring only $\\Omega(\\log d)$ samples per task. However, the number\nof samples required per task grows logarithmically with the number of tasks. To\nremedy this in the low-noise regime, we propose a novel task subset selection\nscheme that ensures the same strong statistical guarantee as MLLAM, even with\nbounded number of samples per task for arbitrarily large number of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 06:46:48 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Thekumparampil", "Kiran Koshy", ""], ["Jain", "Prateek", ""], ["Netrapalli", "Praneeth", ""], ["Oh", "Sewoong", ""]]}, {"id": "2105.08318", "submitter": "Hao Ding", "authors": "Hao Ding, Yifei Ma, Anoop Deoras, Yuyang Wang, Hao Wang", "title": "Zero-Shot Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of recommender systems (RS) relies heavily on the amount of\ntraining data available. This poses a chicken-and-egg problem for early-stage\nproducts, whose amount of data, in turn, relies on the performance of their RS.\nOn the other hand, zero-shot learning promises some degree of generalization\nfrom an old dataset to an entirely new dataset. In this paper, we explore the\npossibility of zero-shot learning in RS. We develop an algorithm, dubbed\nZEro-Shot Recommenders (ZESRec), that is trained on an old dataset and\ngeneralize to a new one where there are neither overlapping users nor\noverlapping items, a setting that contrasts typical cross-domain RS that has\neither overlapping users or items. Different from categorical item indices,\ni.e., item ID, in previous methods, ZESRec uses items' natural-language\ndescriptions (or description embeddings) as their continuous indices, and\ntherefore naturally generalize to any unseen items. In terms of users, ZESRec\nbuilds upon recent advances on sequential RS to represent users using their\ninteractions with items, thereby generalizing to unseen users as well. We study\ntwo pairs of real-world RS datasets and demonstrate that ZESRec can\nsuccessfully enable recommendations in such a zero-shot setting, opening up new\nopportunities for resolving the chicken-and-egg problem for data-scarce\nstartups or early-stage products.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 07:17:37 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ding", "Hao", ""], ["Ma", "Yifei", ""], ["Deoras", "Anoop", ""], ["Wang", "Yuyang", ""], ["Wang", "Hao", ""]]}, {"id": "2105.08321", "submitter": "Parth Patwa", "authors": "Parth Patwa and Viswanatha Reddy and Rohan Sukumaran and Sethuraman TV\n  and Eptehal Nashnoush and Sheshank Shankar and Rishemjit Kaur and Abhishek\n  Singh and Ramesh Raskar", "title": "Can Self Reported Symptoms Predict Daily COVID-19 Cases?", "comments": "Accepted as a full-length oral presentation at the International\n  Workshop on Artificial Intelligence for Social Good (AI4SG), IJCAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has impacted lives and economies across the globe,\nleading to many deaths. While vaccination is an important intervention, its\nroll-out is slow and unequal across the globe. Therefore, extensive testing\nstill remains one of the key methods to monitor and contain the virus. Testing\non a large scale is expensive and arduous. Hence, we need alternate methods to\nestimate the number of cases. Online surveys have been shown to be an effective\nmethod for data collection amidst the pandemic. In this work, we develop\nmachine learning models to estimate the prevalence of COVID-19 using\nself-reported symptoms. Our best model predicts the daily cases with a mean\nabsolute error (MAE) of 226.30 (normalized MAE of 27.09%) per state, which\ndemonstrates the possibility of predicting the actual number of confirmed cases\nby utilizing self-reported symptoms. The models are developed at two levels of\ndata granularity - local models, which are trained at the state level, and a\nsingle global model which is trained on the combined data aggregated across all\nstates. Our results indicate a lower error on the local models as opposed to\nthe global model. In addition, we also show that the most important symptoms\n(features) vary considerably from state to state. This work demonstrates that\nthe models developed on crowd-sourced data, curated via online platforms, can\ncomplement the existing epidemiological surveillance infrastructure in a\ncost-effective manner. The code is publicly available at\nhttps://github.com/parthpatwa/Can-Self-Reported-Symptoms-Predict-Daily-COVID-19-Cases.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 07:26:09 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:52:56 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Patwa", "Parth", ""], ["Reddy", "Viswanatha", ""], ["Sukumaran", "Rohan", ""], ["TV", "Sethuraman", ""], ["Nashnoush", "Eptehal", ""], ["Shankar", "Sheshank", ""], ["Kaur", "Rishemjit", ""], ["Singh", "Abhishek", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2105.08330", "submitter": "Huixuan Chi", "authors": "Huixuan Chi, Yuying Wang, Qinfen Hao, Hong Xia", "title": "Residual Network and Embedding Usage: New Tricks of Node Classification\n  with Graph Convolutional Networks", "comments": "This work is still working in process. (14 pages, 6 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) and subsequent variants have been\nproposed to solve tasks on graphs, especially node classification tasks. In the\nliterature, however, most tricks or techniques are either briefly mentioned as\nimplementation details or only visible in source code. In this paper, we first\nsummarize some existing effective tricks used in GCNs mini-batch training.\nBased on this, two novel tricks named GCN_res Framework and Embedding Usage are\nproposed by leveraging residual network and pre-trained embedding to improve\nbaseline's test accuracy in different datasets. Experiments on Open Graph\nBenchmark (OGB) show that, by combining these techniques, the test accuracy of\nvarious GCNs increases by 1.21%~2.84%. We open source our implementation at\nhttps://github.com/ytchx1999/PyG-OGB-Tricks.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 07:52:51 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 07:37:25 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chi", "Huixuan", ""], ["Wang", "Yuying", ""], ["Hao", "Qinfen", ""], ["Xia", "Hong", ""]]}, {"id": "2105.08339", "submitter": "Ran Ben Basat", "authors": "Shay Vargaftik, Ran Ben Basat, Amit Portnoy, Gal Mendelson, Yaniv\n  Ben-Itzhak, Michael Mitzenmacher", "title": "DRIVE: One-bit Distributed Mean Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem where $n$ clients transmit $d$-dimensional\nreal-valued vectors using $d(1+o(1))$ bits each, in a manner that allows the\nreceiver to approximately reconstruct their mean. Such compression problems\nnaturally arise in distributed and federated learning. We provide novel\nmathematical results and derive computationally efficient algorithms that are\nmore accurate than previous compression techniques. We evaluate our methods on\na collection of distributed and federated learning tasks, using a variety of\ndatasets, and show a consistent improvement over the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 08:03:39 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 16:10:02 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 17:12:21 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Vargaftik", "Shay", ""], ["Basat", "Ran Ben", ""], ["Portnoy", "Amit", ""], ["Mendelson", "Gal", ""], ["Ben-Itzhak", "Yaniv", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "2105.08348", "submitter": "Canh Hao Nguyen", "authors": "Canh Hao Nguyen, Hiroshi Mamitsuka", "title": "On Convex Clustering Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convex clustering is an attractive clustering algorithm with favorable\nproperties such as efficiency and optimality owing to its convex formulation.\nIt is thought to generalize both k-means clustering and agglomerative\nclustering. However, it is not known whether convex clustering preserves\ndesirable properties of these algorithms. A common expectation is that convex\nclustering may learn difficult cluster types such as non-convex ones. Current\nunderstanding of convex clustering is limited to only consistency results on\nwell-separated clusters. We show new understanding of its solutions. We prove\nthat convex clustering can only learn convex clusters. We then show that the\nclusters have disjoint bounding balls with significant gaps. We further\ncharacterize the solutions, regularization hyperparameters, inclusterable cases\nand consistency.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 08:19:29 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Nguyen", "Canh Hao", ""], ["Mamitsuka", "Hiroshi", ""]]}, {"id": "2105.08351", "submitter": "Rafael Reisenhofer", "authors": "Michael Scherbela, Rafael Reisenhofer, Leon Gerard, Philipp Marquetand\n  and Philipp Grohs", "title": "Solving the electronic Schr\\\"odinger equation for multiple nuclear\n  geometries with weight-sharing deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate numerical solutions for the Schr\\\"odinger equation are of utmost\nimportance in quantum chemistry. However, the computational cost of current\nhigh-accuracy methods scales poorly with the number of interacting particles.\nCombining Monte Carlo methods with unsupervised training of neural networks has\nrecently been proposed as a promising approach to overcome the curse of\ndimensionality in this setting and to obtain accurate wavefunctions for\nindividual molecules at a moderately scaling computational cost. These methods\ncurrently do not exploit the regularity exhibited by wavefunctions with respect\nto their molecular geometries. Inspired by recent successful applications of\ndeep transfer learning in machine translation and computer vision tasks, we\nattempt to leverage this regularity by introducing a weight-sharing constraint\nwhen optimizing neural network-based models for different molecular geometries.\nThat is, we restrict the optimization process such that up to 95 percent of\nweights in a neural network model are in fact equal across varying molecular\ngeometries. We find that this technique can accelerate optimization when\nconsidering sets of nuclear geometries of the same molecule by an order of\nmagnitude and that it opens a promising route towards pre-trained neural\nnetwork wavefunctions that yield high accuracy even across different molecules.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 08:23:09 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Scherbela", "Michael", ""], ["Reisenhofer", "Rafael", ""], ["Gerard", "Leon", ""], ["Marquetand", "Philipp", ""], ["Grohs", "Philipp", ""]]}, {"id": "2105.08374", "submitter": "Amirreza Farahani", "authors": "Amirreza Farahani, Laura Genga and Remco Dijkman", "title": "Online Multimodal Transportation Planning using Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a Deep Reinforcement Learning approach to solve a\nmultimodal transportation planning problem, in which containers must be\nassigned to a truck or to trains that will transport them to their destination.\nWhile traditional planning methods work \"offline\" (i.e., they take decisions\nfor a batch of containers before the transportation starts), the proposed\napproach is \"online\", in that it can take decisions for individual containers,\nwhile transportation is being executed. Planning transportation online helps to\neffectively respond to unforeseen events that may affect the original\ntransportation plan, thus supporting companies in lowering transportation\ncosts. We implemented different container selection heuristics within the\nproposed Deep Reinforcement Learning algorithm and we evaluated its performance\nfor each heuristic using data that simulate a realistic scenario, designed on\nthe basis of a real case study at a logistics company. The experimental results\nrevealed that the proposed method was able to learn effective patterns of\ncontainer assignment. It outperformed tested competitors in terms of total\ntransportation costs and utilization of train capacity by 20.48% to 55.32% for\nthe cost and by 7.51% to 20.54% for the capacity. Furthermore, it obtained\nresults within 2.7% for the cost and 0.72% for the capacity of the optimal\nsolution generated by an Integer Linear Programming solver in an offline\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:01:44 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Farahani", "Amirreza", ""], ["Genga", "Laura", ""], ["Dijkman", "Remco", ""]]}, {"id": "2105.08397", "submitter": "Wenbo Hu", "authors": "Wenkai Li, Wenbo Hu, Ning Chen, Cheng Feng", "title": "Stacking VAE with Graph Neural Networks for Effective and Interpretable\n  Time Series Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world maintenance applications, deep generative models have shown\npromising performance in detecting anomalous events of entities from\ntime-series signals collected from multiple sensors. Nevertheless, we outline\ntwo important challenges of leveraging such models for times-series anomaly\ndetection: 1) developing effective and efficient reconstruction models and 2)\nexploiting the similarity and interrelation structures among the multivariate\ntime series data channels. To address these challenges, in this paper we\npropose a stacking variational auto-encoder (VAE) model with graph neural\nnetworks for the effective and interpretable time-series anomaly detection.\nSpecifically, we propose a stacking block-wise reconstruction framework with a\nweight-sharing scheme for the multivariate time series data with similarities\namong channels. Moreover, with a graph learning module, our model learns a\nsparse adjacency matrix to explicitly capture the stable interrelation\nstructure information among multiple time series data channels for\ninterpretable reconstruction of series patterns. Experimental results show that\nour proposed model outperforms the strong baselines on three public datasets\nwith considerable improvements and meanwhile still maintains the training\nefficiency. Furthermore, we demonstrate that the intuitive stable structure\nlearned by our model significantly improves the interpretability of our\ndetection results.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:50:00 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Wenkai", ""], ["Hu", "Wenbo", ""], ["Chen", "Ning", ""], ["Feng", "Cheng", ""]]}, {"id": "2105.08399", "submitter": "Ond\\v{r}ej C\\'ifka", "authors": "Antoine Liutkus, Ond\\v{r}ej C\\'ifka, Shih-Lun Wu, Umut\n  \\c{S}im\\c{s}ekli, Yi-Hsuan Yang, Ga\\\"el Richard", "title": "Relative Positional Encoding for Transformers with Linear Complexity", "comments": "ICML 2021 (long talk) camera-ready. 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Transformer models allow for unprecedented sequence\nlengths, due to linear space and time complexity. In the meantime, relative\npositional encoding (RPE) was proposed as beneficial for classical Transformers\nand consists in exploiting lags instead of absolute positions for inference.\nStill, RPE is not available for the recent linear-variants of the Transformer,\nbecause it requires the explicit computation of the attention matrix, which is\nprecisely what is avoided by such methods. In this paper, we bridge this gap\nand present Stochastic Positional Encoding as a way to generate PE that can be\nused as a replacement to the classical additive (sinusoidal) PE and provably\nbehaves like RPE. The main theoretical contribution is to make a connection\nbetween positional encoding and cross-covariance structures of correlated\nGaussian processes. We illustrate the performance of our approach on the\nLong-Range Arena benchmark and on music generation.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:52:32 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 08:55:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liutkus", "Antoine", ""], ["C\u00edfka", "Ond\u0159ej", ""], ["Wu", "Shih-Lun", ""], ["\u015eim\u015fekli", "Umut", ""], ["Yang", "Yi-Hsuan", ""], ["Richard", "Ga\u00ebl", ""]]}, {"id": "2105.08446", "submitter": "Alejandro Puente-Castro Mr", "authors": "Alejandro Puente-Castro, Enrique Fernandez-Blanco, Alejandro Pazos,\n  Cristian R. Munteanu", "title": "Automatic Assessment of Alzheimer's Disease Diagnosis Based on Deep\n  Learning Techniques", "comments": null, "journal-ref": null, "doi": "10.1016/j.compbiomed.2020.103764", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Early detection is crucial to prevent the progression of Alzheimer's disease\n(AD). Thus, specialists can begin preventive treatment as soon as possible.\nThey demand fast and precise assessment in the diagnosis of AD in the earliest\nand hardest to detect stages. The main objective of this work is to develop a\nsystem that automatically detects the presence of the disease in sagittal\nmagnetic resonance images (MRI), which are not generally used. Sagittal MRIs\nfrom ADNI and OASIS data sets were employed. Experiments were conducted using\nTransfer Learning (TL) techniques in order to achieve more accurate results.\nThere are two main conclusions to be drawn from this work: first, the damages\nrelated to AD and its stages can be distinguished in sagittal MRI and, second,\nthe results obtained using DL models with sagittal MRIs are similar to the\nstate-of-the-art, which uses the horizontal-plane MRI. Although sagittal-plane\nMRIs are not commonly used, this work proved that they were, at least, as\neffective as MRI from other planes at identifying AD in early stages. This\ncould pave the way for further research. Finally, one should bear in mind that\nin certain fields, obtaining the examples for a data set can be very expensive.\nThis study proved that DL models could be built in these fields, whereas TL is\nan essential tool for completing the task with fewer examples.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 11:37:57 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Puente-Castro", "Alejandro", ""], ["Fernandez-Blanco", "Enrique", ""], ["Pazos", "Alejandro", ""], ["Munteanu", "Cristian R.", ""]]}, {"id": "2105.08449", "submitter": "Noura Dridi", "authors": "Noura Dridi, Lucas Drumetz, Ronan Fablet", "title": "Learning stochastic dynamical systems with neural networks mimicking the\n  Euler-Maruyama scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) are one of the most important\nrepresentations of dynamical systems. They are notable for the ability to\ninclude a deterministic component of the system and a stochastic one to\nrepresent random unknown factors. However, this makes learning SDEs much more\nchallenging than ordinary differential equations (ODEs). In this paper, we\npropose a data driven approach where parameters of the SDE are represented by a\nneural network with a built-in SDE integration scheme. The loss function is\nbased on a maximum likelihood criterion, under order one Markov Gaussian\nassumptions. The algorithm is applied to the geometric brownian motion and a\nstochastic version of the Lorenz-63 model. The latter is particularly hard to\nhandle due to the presence of a stochastic component that depends on the state.\nThe algorithm performance is attested using different simulations results.\nBesides, comparisons are performed with the reference gradient matching method\nused for non linear drift estimation, and a neural networks-based method, that\ndoes not consider the stochastic term.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 11:41:34 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Dridi", "Noura", ""], ["Drumetz", "Lucas", ""], ["Fablet", "Ronan", ""]]}, {"id": "2105.08450", "submitter": "Yuval Shahar", "authors": "Yuval Shahar and Matan Lion", "title": "Implementation and Evaluation of a Multivariate Abstraction-Based,\n  Interval-Based Dynamic Time-Warping Method as a Similarity Measure for\n  Longitudinal Medical Records", "comments": "34 pages; 5 figures; 6 tables including three in the Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We extended dynamic time warping (DTW) into interval-based dynamic time\nwarping (iDTW), including (A) interval-based representation (iRep): [1]\nabstracting raw, time-stamped data into interval-based abstractions, [2]\ncomparison-period scoping, [3] partitioning abstract intervals into a given\ntemporal granularity; (B) interval-based matching (iMatch): matching\npartitioned, abstract-concepts records, using a modified DTW. Using domain\nknowledge, we abstracted the raw data of medical records, for up to three\nconcepts out of four or five relevant concepts, into two interval types: State\nabstractions (e.g. LOW, HIGH) and Gradient abstractions (e.g. INCREASING,\nDECREASING). We created all uni-dimensional (State or Gradient) or\nmulti-dimensional (State and Gradient) abstraction combinations. Tasks:\nClassifying 161 oncology patients records as autologous or allogenic\nbone-marrow transplantation; classifying 125 hepatitis patients records as B or\nC hepatitis; predicting micro- or macro-albuminuria in the next year for 151\nType 2 diabetes patients. We used a k-Nearest-Neighbors majority, k=1 to\nSQRT(N), N = set size. 50,328 10-fold cross-validation experiments were\nperformed: 23,400 (Oncology), 19,800 (Hepatitis), 7,128 (Diabetes). Measures:\nArea Under the Curve (AUC), optimal Youden's Index. Paired t-tests compared\nresult vectors for equivalent configurations other than a tested variable, to\ndetermine a significant mean accuracy difference (P<0.05). Mean classification\nand prediction using abstractions was significantly better than using only raw\ntime-stamped data. In each domain, at least one abstraction combination led to\na significantly better performance than using raw data. Increasing feature\nnumber, and using multi-dimensional abstractions, enhanced performance. Unlike\nwhen using raw data, optimal performance was often reached with k=5, using\nabstractions.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 11:41:42 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Shahar", "Yuval", ""], ["Lion", "Matan", ""]]}, {"id": "2105.08463", "submitter": "Ankush Panwar", "authors": "Ankush Panwar, Pratyush Singh, Suman Saha, Danda Pani Paudel and Luc\n  Van Gool", "title": "Unsupervised Compound Domain Adaptation for Face Anti-Spoofing", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of face anti-spoofing which aims to make the face\nverification systems robust in the real world settings. The context of\ndetecting live vs. spoofed face images may differ significantly in the target\ndomain, when compared to that of labeled source domain where the model is\ntrained. Such difference may be caused due to new and unknown spoof types,\nillumination conditions, scene backgrounds, among many others. These varieties\nof differences make the target a compound domain, thus calling for the problem\nof the unsupervised compound domain adaptation. We demonstrate the\neffectiveness of the compound domain assumption for the task of face\nanti-spoofing, for the first time in this work. To this end, we propose a\nmemory augmentation method for adapting the source model to the target domain\nin a domain aware manner. The adaptation process is further improved by using\nthe curriculum learning and the domain agnostic source network training\napproaches. The proposed method successfully adapts to the compound target\ndomain consisting multiple new spoof types. Our experiments on multiple\nbenchmark datasets demonstrate the superiority of the proposed method over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:08:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Panwar", "Ankush", ""], ["Singh", "Pratyush", ""], ["Saha", "Suman", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""]]}, {"id": "2105.08486", "submitter": "Matthew Ross", "authors": "Blake VanBerlo, Matthew A.S. Ross, Daniel Hsia", "title": "Univariate Long-Term Municipal Water Demand Forecasting", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study describes an investigation into the modelling of citywide water\nconsumption in London, Canada. Multiple modelling techniques were evaluated for\nthe task of univariate time series forecasting with water consumption,\nincluding linear regression, Facebook's Prophet method, recurrent neural\nnetworks, and convolutional neural networks. Prophet was identified as the\nmodel of choice, having achieved a mean absolute percentage error of 2.51%,\naveraged across a 5-fold cross validation. Prophet was also found to have other\nadvantages deemed valuable to water demand management stakeholders, including\ninherent interpretability and graceful handling of missing data. The\nimplementation for the methods described in this paper has been open sourced,\nas they may be adaptable by other municipalities.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:58:24 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["VanBerlo", "Blake", ""], ["Ross", "Matthew A. S.", ""], ["Hsia", "Daniel", ""]]}, {"id": "2105.08504", "submitter": "Mathias M\\\"uller", "authors": "Mathias M\\\"uller and Rico Sennrich", "title": "Understanding the Properties of Minimum Bayes Risk Decoding in Neural\n  Machine Translation", "comments": "V1: ACL 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Machine Translation (NMT) currently exhibits biases such as producing\ntranslations that are too short and overgenerating frequent words, and shows\npoor robustness to copy noise in training data or domain shift. Recent work has\ntied these shortcomings to beam search -- the de facto standard inference\nalgorithm in NMT -- and Eikema & Aziz (2020) propose to use Minimum Bayes Risk\n(MBR) decoding on unbiased samples instead.\n  In this paper, we empirically investigate the properties of MBR decoding on a\nnumber of previously reported biases and failure cases of beam search. We find\nthat MBR still exhibits a length and token frequency bias, owing to the MT\nmetrics used as utility functions, but that MBR also increases robustness\nagainst copy noise in the training data and domain shift.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 13:31:05 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["M\u00fcller", "Mathias", ""], ["Sennrich", "Rico", ""]]}, {"id": "2105.08506", "submitter": "Sara Atito", "authors": "Sara Atito Ali Ahmed and Mehmet Can Yavuz and Mehmet Umut Sen and\n  Fatih Gulsen and Onur Tutar and Bora Korkmazer and Cesur Samanci and Sabri\n  Sirolu and Rauf Hamid and Ali Ergun Eryurekli and Toghrul Mammadov and Berrin\n  Yanikoglu", "title": "COVID-19 Detection in Computed Tomography Images with 2D and 3D\n  Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting COVID-19 in computed tomography (CT) or radiography images has been\nproposed as a supplement to the definitive RT-PCR test. We present a deep\nlearning ensemble for detecting COVID-19 infection, combining slice-based (2D)\nand volume-based (3D) approaches. The 2D system detects the infection on each\nCT slice independently, combining them to obtain the patient-level decision via\ndifferent methods (averaging and long-short term memory networks). The 3D\nsystem takes the whole CT volume to arrive to the patient-level decision in one\nstep. A new high resolution chest CT scan dataset, called the IST-C dataset, is\nalso collected in this work. The proposed ensemble, called IST-CovNet, obtains\n90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting\nCOVID-19 among normal controls and other types of lung pathologies; and 93.69%\naccuracy and 0.99 AUC score on the publicly available MosMed dataset that\nconsists of COVID-19 scans and normal controls only. The system is deployed at\nIstanbul University Cerrahpasa School of Medicine.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 20:12:02 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 08:47:45 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ahmed", "Sara Atito Ali", ""], ["Yavuz", "Mehmet Can", ""], ["Sen", "Mehmet Umut", ""], ["Gulsen", "Fatih", ""], ["Tutar", "Onur", ""], ["Korkmazer", "Bora", ""], ["Samanci", "Cesur", ""], ["Sirolu", "Sabri", ""], ["Hamid", "Rauf", ""], ["Eryurekli", "Ali Ergun", ""], ["Mammadov", "Toghrul", ""], ["Yanikoglu", "Berrin", ""]]}, {"id": "2105.08508", "submitter": "Mohammad Javad Shabanpour", "authors": "Fardin Ghorbani, Javad Shabanpour, Sina Beyraghi, Hossein Soleimani,\n  Homayoon Oraizi, Mohammad Soleimani", "title": "A deep learning approach for inverse design of the metasurface for\n  dual-polarized waves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.class-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compared to the conventional metasurface design, machine learning-based\nmethods have recently created an inspiring platform for an inverse realization\nof the metasurfaces. Here, we have used the Deep Neural Network (DNN) for the\ngeneration of desired output unit cell structures in an ultra-wide working\nfrequency band for both TE and TM polarized waves. To automatically generate\nmetasurfaces in a wide range of working frequencies from 4 to 45 GHz, we\ndeliberately design an 8 ring-shaped pattern in such a way that the unit-cells\ngenerated in the dataset can produce single or multiple notches in the desired\nworking frequency band. Compared to the general approach, whereby the final\nmetasurface structure may be formed by any randomly distributed \"0\" and \"1\", we\npropose here a restricted output structure. By restricting the output, the\nnumber of calculations will be reduced and the learning speed will be\nincreased. Moreover, we have shown that the accuracy of the network reaches\n91\\%. Obtaining the final unit cell directly without any time-consuming\noptimization algorithms for both TE and TM polarized waves, and high average\naccuracy, promises an effective strategy for the metasurface design; thus, the\ndesigner is required only to focus on the design goal.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:15:31 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 08:44:16 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ghorbani", "Fardin", ""], ["Shabanpour", "Javad", ""], ["Beyraghi", "Sina", ""], ["Soleimani", "Hossein", ""], ["Oraizi", "Homayoon", ""], ["Soleimani", "Mohammad", ""]]}, {"id": "2105.08509", "submitter": "Francisco Caldas", "authors": "Francisco Caldas, Claudia Soares, Cl\\'audia Nunes, Marta Guimar\\~aes,\n  Mariana Filipe, Rodrigo Ventura", "title": "Conjunction Data Messages behave as a Poisson Process", "comments": "submitted to AI4Spacecraft (IJCAI 2021 workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space debris is a major problem in space exploration. International bodies\ncontinuously monitor a large database of orbiting objects and emit warnings in\nthe form of conjunction data messages. An important question for satellite\noperators is to estimate when fresh information will arrive so that they can\nreact timely but sparingly with satellite maneuvers. We propose a statistical\nlearning model of the message arrival process, allowing us to answer two\nimportant questions: (1) Will there be any new message in the next specified\ntime interval? (2) When exactly and with what uncertainty will the next message\narrive? The average prediction error for question (2) of our Bayesian Poisson\nprocess model is smaller than the baseline in more than 3 hours in a test set\nof 50k close encounter events.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:47:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Caldas", "Francisco", ""], ["Soares", "Claudia", ""], ["Nunes", "Cl\u00e1udia", ""], ["Guimar\u00e3es", "Marta", ""], ["Filipe", "Mariana", ""], ["Ventura", "Rodrigo", ""]]}, {"id": "2105.08511", "submitter": "Xing Tian", "authors": "Chris Xing Tian, Haoliang Li, Yufei Wang, Shiqi Wang", "title": "Privacy-Preserving Constrained Domain Generalization for Medical Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have demonstrated unprecedented success for\nmedical imaging applications. However, due to the issue of limited dataset\navailability and the strict legal and ethical requirements for patient privacy\nprotection, the broad applications of medical imaging classification driven by\nDNN with large-scale training data have been largely hindered. For example,\nwhen training the DNN from one domain (e.g., with data only from one hospital),\nthe generalization capability to another domain (e.g., data from another\nhospital) could be largely lacking. In this paper, we aim to tackle this\nproblem by developing the privacy-preserving constrained domain generalization\nmethod, aiming to improve the generalization capability under the\nprivacy-preserving condition. In particular, We propose to improve the\ninformation aggregation process on the centralized server-side with a novel\ngradient alignment loss, expecting that the trained model can be better\ngeneralized to the \"unseen\" but related medical images. The rationale and\neffectiveness of our proposed method can be explained by connecting our\nproposed method with the Maximum Mean Discrepancy (MMD) which has been widely\nadopted as the distribution distance measurement. Experimental results on two\nchallenging medical imaging classification tasks indicate that our method can\nachieve better cross-domain generalization capability compared to the\nstate-of-the-art federated learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:21:13 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Tian", "Chris Xing", ""], ["Li", "Haoliang", ""], ["Wang", "Yufei", ""], ["Wang", "Shiqi", ""]]}, {"id": "2105.08526", "submitter": "Farid Arthaud", "authors": "Farid Arthaud, Guillaume Lecoeur, Alban Pierre", "title": "Transformers \\`a Grande Vitesse", "comments": "10 pages, including 1 page of appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Robust travel time predictions are of prime importance in managing any\ntransportation infrastructure, and particularly in rail networks where they\nhave major impacts both on traffic regulation and passenger satisfaction. We\naim at predicting the travel time of trains on rail sections at the scale of an\nentire rail network in real-time, by estimating trains' delays relative to a\ntheoretical circulation plan. Existing implementations within railway companies\ngenerally work using the approximation that a train's delay will stay constant\nfor the rest of its trip.\n  Predicting the evolution of a given train's delay is a uniquely hard problem,\ndistinct from mainstream road traffic forecasting problems, since it involves\nseveral hard-to-model phenomena: train spacing, station congestion and\nheterogeneous rolling stock among others. We first offer empirical evidence of\nthe previously unexplored phenomenon of delay propagation in the French\nNational Railway Network, leading to delays being amplified by interactions\nbetween trains. We then contribute a novel technique using the transformer\narchitecture and pre-trained embeddings to make real-time massively parallel\npredictions for train delays at the scale of the whole rail network (over 3k\ntrains at peak hours, making predictions at an average horizon of 70 minutes).\nOur approach yields very positive results on real-world data when compared to\ncurrently-used and experimental prediction techniques. Our work is in the early\nstages of implementation for industrial use at the French railway company SNCF\nfor passenger information systems, and a contender as a tool to aid traffic\nregulation decisions.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 13:43:18 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Arthaud", "Farid", ""], ["Lecoeur", "Guillaume", ""], ["Pierre", "Alban", ""]]}, {"id": "2105.08532", "submitter": "Muhammad Osama", "authors": "Muhammad Osama, Dave Zachariah, Petre Stoica", "title": "Distributionally Robust Learning in Heterogeneous Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from training data obtained in different\ncontexts, where the test data is subject to distributional shifts. We develop a\ndistributionally robust method that focuses on excess risks and achieves a more\nappropriate trade-off between performance and robustness than the conventional\nand overly conservative minimax approach. The proposed method is\ncomputationally feasible and provides statistical guarantees. We demonstrate\nits performance using both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:00:34 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Osama", "Muhammad", ""], ["Zachariah", "Dave", ""], ["Stoica", "Petre", ""]]}, {"id": "2105.08537", "submitter": "Soumyabrata Dev", "authors": "Mayank Jain, Tarek AlSkaif, and Soumyabrata Dev", "title": "A Clustering Framework for Residential Electric Demand Profiles", "comments": "Published in Proc. International Conference on Smart Energy Systems\n  and Technologies (SEST), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of residential electric demand profiles data, enabled by the\nlarge-scale deployment of smart metering infrastructure, has made it possible\nto perform more accurate analysis of electricity consumption patterns. This\npaper analyses the electric demand profiles of individual households located in\nthe city Amsterdam, the Netherlands. A comprehensive clustering framework is\ndefined to classify households based on their electricity consumption pattern.\nThis framework consists of two main steps, namely a dimensionality reduction\nstep of input electricity consumption data, followed by an unsupervised\nclustering algorithm of the reduced subspace. While any algorithm, which has\nbeen used in the literature for the aforementioned clustering task, can be used\nfor the corresponding step, the more important question is to deduce which\nparticular combination of algorithms is the best for a given dataset and a\nclustering task. This question is addressed in this paper by proposing a novel\nobjective validation strategy, whose recommendations are then cross-verified by\nperforming subjective validation.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 09:19:34 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Jain", "Mayank", ""], ["AlSkaif", "Tarek", ""], ["Dev", "Soumyabrata", ""]]}, {"id": "2105.08547", "submitter": "Cheolhei Lee", "authors": "Cheolhei Lee, Kaiwen Wang, Jianguo Wu, Wenjun Cai, and Xiaowei Yue", "title": "Partitioned Active Learning for Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cost-effective and high-precision surrogate modeling is a cornerstone of\nautomated industrial and engineering systems. Active learning coupled with\nGaussian process (GP) surrogate modeling is an indispensable tool for demanding\nand complex systems, while the existence of heterogeneity in underlying systems\nmay adversely affect the modeling process. In order to improve the learning\nefficiency under the regime, we propose the partitioned active learning\nstrategy established upon partitioned GP (PGP) modeling. Our strategy seeks the\nmost informative design point for PGP modeling systematically in twosteps. The\nglobal searching scheme accelerates the exploration aspect of active learning\nby investigating the most uncertain design space, and the local searching\nexploits the active learning criterion induced by the local GP model. We also\nprovide numerical remedies to alleviate the computational cost of active\nlearning, thereby allowing the proposed method to incorporate a large amount of\ncandidates. The proposed method is applied to numerical simulation and real\nworld cases endowed with heterogeneities in which surrogate models are\nconstructed to embed in (i) the cost-efficient automatic fuselage shape control\nsystem; and (ii) the optimal design system of tribocorrosion-resistant alloys.\nThe results show that our approach outperforms benchmark methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 02:05:31 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lee", "Cheolhei", ""], ["Wang", "Kaiwen", ""], ["Wu", "Jianguo", ""], ["Cai", "Wenjun", ""], ["Yue", "Xiaowei", ""]]}, {"id": "2105.08566", "submitter": "Yutian Chang", "authors": "Yutian Chang and Guannan Liu and Yuan Zuo and Junjie Wu", "title": "Multi-Aspect Temporal Network Embedding: A Mixture of Hawkes Process\n  View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the tremendous research interests in network\nembedding. Extant works have taken the neighborhood formation as the critical\ninformation to reveal the inherent dynamics of network structures, and\nsuggested encoding temporal edge formation sequences to capture the historical\ninfluences of neighbors. In this paper, however, we argue that the edge\nformation can be attributed to a variety of driving factors including the\ntemporal influence, which is better referred to as multiple aspects. As a\nmatter of fact, different node aspects can drive the formation of distinctive\nneighbors, giving birth to the multi-aspect embedding that relates to but goes\nbeyond a temporal scope. Along this vein, we propose a Mixture of Hawkes-based\nTemporal Network Embeddings (MHNE) model to capture the aspect-driven\nneighborhood formation of networks. In MHNE, we encode the multi-aspect\nembeddings into the mixture of Hawkes processes to gain the advantages in\nmodeling the excitation effects and the latent aspects. Specifically, a graph\nattention mechanism is used to assign different weights to account for the\nexcitation effects of history events, while a Gumbel-Softmax is plugged in to\nderive the distribution over the aspects. Extensive experiments on 8 different\ntemporal networks have demonstrated the great performance of the multi-aspect\nembeddings obtained by MHNE in comparison with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:50:26 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chang", "Yutian", ""], ["Liu", "Guannan", ""], ["Zuo", "Yuan", ""], ["Wu", "Junjie", ""]]}, {"id": "2105.08568", "submitter": "Auguste Lehuger", "authors": "Auguste Lehuger, Matthew Crosby", "title": "Fixed $\\beta$-VAE Encoding for Curious Exploration in Complex 3D\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Curiosity is a general method for augmenting an environment reward with an\nintrinsic reward, which encourages exploration and is especially useful in\nsparse reward settings. As curiosity is calculated using next state prediction\nerror, the type of state encoding used has a large impact on performance.\nRandom features and inverse-dynamics features are generally preferred over VAEs\nbased on previous results from Atari and other mostly 2D environments. However,\nunlike VAEs, they may not encode sufficient information for optimal behaviour,\nwhich becomes increasingly important as environments become more complex. In\nthis paper, we use the sparse reward 3D physics environment Animal-AI, to\ndemonstrate how a fixed $\\beta$-VAE encoding can be used effectively with\ncuriosity. We combine this with curriculum learning to solve the previously\nunsolved exploration intensive detour tasks while achieving 22\\% gain in sample\nefficiency on the training curriculum against the next best encoding. We also\ncorroborate the results on Atari Breakout, with our custom encoding\noutperforming random features and inverse-dynamics features.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:52:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lehuger", "Auguste", ""], ["Crosby", "Matthew", ""]]}, {"id": "2105.08573", "submitter": "Wenqing Chen", "authors": "Wenqing Chen, Jidong Tian, Caoyun Fan, Hao He, and Yaohui Jin", "title": "Dependent Multi-Task Learning with Causal Intervention for Image\n  Captioning", "comments": "To be published in IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work for image captioning mainly followed an extract-then-generate\nparadigm, pre-extracting a sequence of object-based features and then\nformulating image captioning as a single sequence-to-sequence task. Although\npromising, we observed two problems in generated captions: 1) content\ninconsistency where models would generate contradicting facts; 2) not\ninformative enough where models would miss parts of important information. From\na causal perspective, the reason is that models have captured spurious\nstatistical correlations between visual features and certain expressions (e.g.,\nvisual features of \"long hair\" and \"woman\"). In this paper, we propose a\ndependent multi-task learning framework with the causal intervention (DMTCI).\nFirstly, we involve an intermediate task, bag-of-categories generation, before\nthe final task, image captioning. The intermediate task would help the model\nbetter understand the visual features and thus alleviate the content\ninconsistency problem. Secondly, we apply Pearl's do-calculus on the model,\ncutting off the link between the visual features and possible confounders and\nthus letting models focus on the causal visual features. Specifically, the\nhigh-frequency concept set is considered as the proxy confounders where the\nreal confounders are inferred in the continuous space. Finally, we use a\nmulti-agent reinforcement learning (MARL) strategy to enable end-to-end\ntraining and reduce the inter-task error accumulations. The extensive\nexperiments show that our model outperforms the baseline models and achieves\ncompetitive performance with state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:57:33 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Wenqing", ""], ["Tian", "Jidong", ""], ["Fan", "Caoyun", ""], ["He", "Hao", ""], ["Jin", "Yaohui", ""]]}, {"id": "2105.08576", "submitter": "Conghao Zhou", "authors": "Wen Wu, Conghao Zhou, Mushu Li, Huaqing Wu, Haibo Zhou, Ning Zhang,\n  Xuemin (Sherman) Shen, Weihua Zhuang", "title": "AI-Native Network Slicing for 6G Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the global roll-out of the fifth generation (5G) networks, it is\nnecessary to look beyond 5G and envision the sixth generation (6G) networks.\nThe 6G networks are expected to have space-air-ground integrated networking,\nadvanced network virtualization, and ubiquitous intelligence. This article\nproposes an artificial intelligence (AI)-native network slicing architecture\nfor 6G networks to facilitate intelligent network management and support\nemerging AI services. AI is built in the proposed network slicing architecture\nto enable the synergy of AI and network slicing. AI solutions are investigated\nfor the entire lifecycle of network slicing to facilitate intelligent network\nmanagement, i.e., AI for slicing. Furthermore, network slicing approaches are\ndiscussed to support emerging AI services by constructing slice instances and\nperforming efficient resource management, i.e., slicing for AI. Finally, a case\nstudy is presented, followed by a discussion of open research issues that are\nessential for AI-native network slicing in 6G.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:01:57 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Wu", "Wen", "", "Sherman"], ["Zhou", "Conghao", "", "Sherman"], ["Li", "Mushu", "", "Sherman"], ["Wu", "Huaqing", "", "Sherman"], ["Zhou", "Haibo", "", "Sherman"], ["Zhang", "Ning", "", "Sherman"], ["Xuemin", "", "", "Sherman"], ["Shen", "", ""], ["Zhuang", "Weihua", ""]]}, {"id": "2105.08583", "submitter": "Pujan Pokhrel", "authors": "Pujan Pokhrel", "title": "Machine Learning in weakly nonlinear systems: A Case study on\n  Significant wave heights", "comments": "Significant wave heights forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a machine learning method based on the Extra Trees (ET)\nalgorithm for forecasting Significant Wave Heights in oceanic waters. To derive\nmultiple features from the CDIP buoys, which make point measurements, we first\nnowcast various parameters and then forecast them at 30-min intervals. The\nproposed algorithm has Scatter Index (SI), Bias, Correlation Coefficient, Root\nMean Squared Error (RMSE) of 0.130, -0.002, 0.97, and 0.14, respectively, for\none day ahead prediction and 0.110, -0.001, 0.98, and 0.122, respectively, for\n14-day ahead prediction on the testing dataset. While other state-of-the-art\nmethods can only forecast up to 120 hours ahead, we extend it further to 14\ndays. Our proposed setup includes spectral features, hv-block cross-validation,\nand stringent QC criteria. The proposed algorithm performs significantly better\nthan the state-of-the-art methods commonly used for significant wave height\nforecasting for one-day ahead prediction. Moreover, the improved performance of\nthe proposed machine learning method compared to the numerical methods shows\nthat this performance can be extended to even longer periods allowing for early\nprediction of significant wave heights in oceanic waters.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:12:11 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 01:21:11 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 10:41:32 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Pokhrel", "Pujan", ""]]}, {"id": "2105.08584", "submitter": "Gongfan Fang", "authors": "Gongfan Fang, Jie Song, Xinchao Wang, Chengchao Shen, Xingen Wang,\n  Mingli Song", "title": "Contrastive Model Inversion for Data-Free Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model inversion, whose goal is to recover training data from a pre-trained\nmodel, has been recently proved feasible. However, existing inversion methods\nusually suffer from the mode collapse problem, where the synthesized instances\nare highly similar to each other and thus show limited effectiveness for\ndownstream tasks, such as knowledge distillation. In this paper, we propose\nContrastive Model Inversion~(CMI), where the data diversity is explicitly\nmodeled as an optimizable objective, to alleviate the mode collapse issue. Our\nmain observation is that, under the constraint of the same amount of data,\nhigher data diversity usually indicates stronger instance discrimination. To\nthis end, we introduce in CMI a contrastive learning objective that encourages\nthe synthesizing instances to be distinguishable from the already synthesized\nones in previous batches. Experiments of pre-trained models on CIFAR-10,\nCIFAR-100, and Tiny-ImageNet demonstrate that CMI not only generates more\nvisually plausible instances than the state of the arts, but also achieves\nsignificantly superior performance when the generated data are used for\nknowledge distillation. Code is available at\n\\url{https://github.com/zju-vipa/DataFree}.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:13:00 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Fang", "Gongfan", ""], ["Song", "Jie", ""], ["Wang", "Xinchao", ""], ["Shen", "Chengchao", ""], ["Wang", "Xingen", ""], ["Song", "Mingli", ""]]}, {"id": "2105.08587", "submitter": "Leila Karimi", "authors": "Leila Karimi, Mai Abdelhakim, James Joshi", "title": "Adaptive ABAC Policy Learning: A Reinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid advances in computing systems, there is an increasing demand for\nmore effective and efficient access control (AC) approaches. Recently,\nAttribute Based Access Control (ABAC) approaches have been shown to be\npromising in fulfilling the AC needs of such emerging complex computing\nenvironments. An ABAC model grants access to a requester based on attributes of\nentities in a system and an authorization policy; however, its generality and\nflexibility come with a higher cost. Further, increasing complexities of\norganizational systems and the need for federated accesses to their resources\nmake the task of AC enforcement and management much more challenging. In this\npaper, we propose an adaptive ABAC policy learning approach to automate the\nauthorization management task. We model ABAC policy learning as a reinforcement\nlearning problem. In particular, we propose a contextual bandit system, in\nwhich an authorization engine adapts an ABAC model through a feedback control\nloop; it relies on interacting with users/administrators of the system to\nreceive their feedback that assists the model in making authorization\ndecisions. We propose four methods for initializing the learning model and a\nplanning approach based on attribute value hierarchy to accelerate the learning\nprocess. We focus on developing an adaptive ABAC policy learning model for a\nhome IoT environment as a running example. We evaluate our proposed approach\nover real and synthetic data. We consider both complete and sparse datasets in\nour evaluations. Our experimental results show that the proposed approach\nachieves performance that is comparable to ones based on supervised learning in\nmany scenarios and even outperforms them in several situations.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:18:02 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Karimi", "Leila", ""], ["Abdelhakim", "Mai", ""], ["Joshi", "James", ""]]}, {"id": "2105.08590", "submitter": "Moloud Abdar", "authors": "Moloud Abdar, Soorena Salari, Sina Qahremani, Hak-Keung Lam, Fakhri\n  Karray, Sadiq Hussain, Abbas Khosravi, U. Rajendra Acharya, Saeid Nahavandi", "title": "UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion\n  with Ensemble Monte Carlo Dropout for COVID-19 Detection", "comments": "16 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 (Coronavirus disease 2019) has infected more than 151 million\npeople and caused approximately 3.17 million deaths around the world up to the\npresent. The rapid spread of COVID-19 is continuing to threaten human's life\nand health. Therefore, the development of computer-aided detection (CAD)\nsystems based on machine and deep learning methods which are able to accurately\ndifferentiate COVID-19 from other diseases using chest computed tomography (CT)\nand X-Ray datasets is essential and of immediate priority. Different from most\nof the previous studies which used either one of CT or X-ray images, we\nemployed both data types with sufficient samples in implementation. On the\nother hand, due to the extreme sensitivity of this pervasive virus, model\nuncertainty should be considered, while most previous studies have overlooked\nit. Therefore, we propose a novel powerful fusion model named\n$UncertaintyFuseNet$ that consists of an uncertainty module: Ensemble Monte\nCarlo (EMC) dropout. The obtained results prove the effectiveness of our\nproposed fusion for COVID-19 detection using CT scan and X-Ray datasets. Also,\nour proposed $UncertaintyFuseNet$ model is significantly robust to noise and\nperforms well with the previously unseen data. The source codes and models of\nthis study are available at:\nhttps://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:20:34 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 05:25:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Abdar", "Moloud", ""], ["Salari", "Soorena", ""], ["Qahremani", "Sina", ""], ["Lam", "Hak-Keung", ""], ["Karray", "Fakhri", ""], ["Hussain", "Sadiq", ""], ["Khosravi", "Abbas", ""], ["Acharya", "U. Rajendra", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2105.08597", "submitter": "Mohammed Ibrahim", "authors": "Mohammed Ibrahim, Susan Gauch, Tyler Gerth, Brandon Cox", "title": "WOVe: Incorporating Word Order in GloVe Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word vector representations open up new opportunities to extract useful\ninformation from unstructured text. Defining a word as a vector made it easy\nfor the machine learning algorithms to understand a text and extract\ninformation from. Word vector representations have been used in many\napplications such word synonyms, word analogy, syntactic parsing, and many\nothers. GloVe, based on word contexts and matrix vectorization, is an\nef-fective vector-learning algorithm. It improves on previous vector-learning\nalgorithms. However, the GloVe model fails to explicitly consider the order in\nwhich words appear within their contexts. In this paper, multiple methods of\nincorporating word order in GloVe word embeddings are proposed. Experimental\nresults show that our Word Order Vector (WOVe) word embeddings approach\noutperforms unmodified GloVe on the natural lan-guage tasks of analogy\ncompletion and word similarity. WOVe with direct concatenation slightly\noutperformed GloVe on the word similarity task, increasing average rank by 2%.\nHowever, it greatly improved on the GloVe baseline on a word analogy task,\nachieving an average 36.34% improvement in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:28:20 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ibrahim", "Mohammed", ""], ["Gauch", "Susan", ""], ["Gerth", "Tyler", ""], ["Cox", "Brandon", ""]]}, {"id": "2105.08601", "submitter": "Lifeng Zhou", "authors": "Lifeng Zhou, Vishnu D. Sharma, Qingbiao Li, Amanda Prorok, Alejandro\n  Ribeiro, Vijay Kumar", "title": "Graph Neural Networks for Decentralized Multi-Robot Submodular Action\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a learning-based approach for decentralized\nsubmodular maximization. We focus on applications where robots are required to\njointly select actions, e.g., motion primitives, to maximize team submodular\nobjectives with local communications only. Such applications are essential for\nlarge-scale multi-robot coordination such as multi-robot motion planning for\narea coverage, environment exploration, and target tracking. But the current\ndecentralized submodular maximization algorithms either require assumptions on\nthe inter-robot communication or lose some suboptimal guarantees. In this work,\nwe propose a general-purpose learning architecture towards submodular\nmaximization at scale, with decentralized communications. Particularly, our\nlearning architecture leverages a graph neural network (GNN) to capture local\ninteractions of the robots and learns decentralized decision-making for the\nrobots. We train the learning model by imitating an expert solution and\nimplement the resulting model for decentralized action selection involving\nlocal observations and communications only. We demonstrate the performance of\nour GNN-based learning approach in a scenario of active target coverage with\nlarge networks of robots. The simulation results show our approach nearly\nmatches the coverage performance of the expert algorithm, and yet runs several\norders faster with more than 30 robots. The results also exhibit our approach's\ngeneralization capability in previously unseen scenarios, e.g., larger\nenvironments and larger networks of robots.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:32:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhou", "Lifeng", ""], ["Sharma", "Vishnu D.", ""], ["Li", "Qingbiao", ""], ["Prorok", "Amanda", ""], ["Ribeiro", "Alejandro", ""], ["Kumar", "Vijay", ""]]}, {"id": "2105.08612", "submitter": "Yuan-Ting Hu", "authors": "Yuan-Ting Hu, Jiahong Wang, Raymond A. Yeh, Alexander G. Schwing", "title": "SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and\n  3D Mesh Reconstruction from Video Data", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting detailed 3D information of objects from video data is an important\ngoal for holistic scene understanding. While recent methods have shown\nimpressive results when reconstructing meshes of objects from a single image,\nresults often remain ambiguous as part of the object is unobserved. Moreover,\nexisting image-based datasets for mesh reconstruction don't permit to study\nmodels which integrate temporal information. To alleviate both concerns we\npresent SAIL-VOS 3D: a synthetic video dataset with frame-by-frame mesh\nannotations which extends SAIL-VOS. We also develop first baselines for\nreconstruction of 3D meshes from video data via temporal models. We demonstrate\nefficacy of the proposed baseline on SAIL-VOS 3D and Pix3D, showing that\ntemporal information improves reconstruction quality. Resources and additional\ninformation are available at http://sailvos.web.illinois.edu.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:42:37 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Hu", "Yuan-Ting", ""], ["Wang", "Jiahong", ""], ["Yeh", "Raymond A.", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "2105.08619", "submitter": "Ryan Sheatsley", "authors": "Ryan Sheatsley and Blaine Hoak and Eric Pauley and Yohan Beugin and\n  Michael J. Weisman and Patrick McDaniel", "title": "On the Robustness of Domain Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is vulnerable to adversarial examples-inputs designed to\ncause models to perform poorly. However, it is unclear if adversarial examples\nrepresent realistic inputs in the modeled domains. Diverse domains such as\nnetworks and phishing have domain constraints-complex relationships between\nfeatures that an adversary must satisfy for an attack to be realized (in\naddition to any adversary-specific goals). In this paper, we explore how domain\nconstraints limit adversarial capabilities and how adversaries can adapt their\nstrategies to create realistic (constraint-compliant) examples. In this, we\ndevelop techniques to learn domain constraints from data, and show how the\nlearned constraints can be integrated into the adversarial crafting process. We\nevaluate the efficacy of our approach in network intrusion and phishing\ndatasets and find: (1) up to 82% of adversarial examples produced by\nstate-of-the-art crafting algorithms violate domain constraints, (2) domain\nconstraints are robust to adversarial examples; enforcing constraints yields an\nincrease in model accuracy by up to 34%. We observe not only that adversaries\nmust alter inputs to satisfy domain constraints, but that these constraints\nmake the generation of valid adversarial examples far more challenging.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:49:55 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Sheatsley", "Ryan", ""], ["Hoak", "Blaine", ""], ["Pauley", "Eric", ""], ["Beugin", "Yohan", ""], ["Weisman", "Michael J.", ""], ["McDaniel", "Patrick", ""]]}, {"id": "2105.08620", "submitter": "Yao Li", "authors": "Yao Li, Tongyi Tang, Cho-Jui Hsieh, Thomas C. M. Lee", "title": "Detecting Adversarial Examples with Bayesian Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new framework to detect adversarial examples\nmotivated by the observations that random components can improve the smoothness\nof predictors and make it easier to simulate output distribution of deep neural\nnetwork. With these observations, we propose a novel Bayesian adversarial\nexample detector, short for BATer, to improve the performance of adversarial\nexample detection. In specific, we study the distributional difference of\nhidden layer output between natural and adversarial examples, and propose to\nuse the randomness of Bayesian neural network (BNN) to simulate hidden layer\noutput distribution and leverage the distribution dispersion to detect\nadversarial examples. The advantage of BNN is that the output is stochastic\nwhile neural networks without random components do not have such\ncharacteristics. Empirical results on several benchmark datasets against\npopular attacks show that the proposed BATer outperforms the state-of-the-art\ndetectors in adversarial example detection.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:51:24 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:04:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Yao", ""], ["Tang", "Tongyi", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "2105.08621", "submitter": "Megha Khosla", "authors": "Thorben Funke, Megha Khosla, Avishek Anand", "title": "Zorro: Valid, Sparse, and Stable Explanations in Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ever-increasing popularity and applications of graph neural\nnetworks, several proposals have been made to interpret and understand the\ndecisions of a GNN model. Explanations for a GNN model differ in principle from\nother input settings. It is important to attribute the decision to input\nfeatures and other related instances connected by the graph structure. We find\nthat the previous explanation generation approaches that maximize the mutual\ninformation between the label distribution produced by the GNN model and the\nexplanation to be restrictive. Specifically, existing approaches do not enforce\nexplanations to be predictive, sparse, or robust to input perturbations.\n  In this paper, we lay down some of the fundamental principles that an\nexplanation method for GNNs should follow and introduce a metric fidelity as a\nmeasure of the explanation's effectiveness. We propose a novel approach Zorro\nbased on the principles from rate-distortion theory that uses a simple\ncombinatorial procedure to optimize for fidelity. Extensive experiments on real\nand synthetic datasets reveal that Zorro produces sparser, stable, and more\nfaithful explanations than existing GNN explanation approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:53:09 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Funke", "Thorben", ""], ["Khosla", "Megha", ""], ["Anand", "Avishek", ""]]}, {"id": "2105.08626", "submitter": "Andy Liaw", "authors": "Robert P. Sheridan, Andy Liaw, Matthew Tudor", "title": "Light Gradient Boosting Machine as a Regression Method for Quantitative\n  Structure-Activity Relationships", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the pharmaceutical industry, where it is common to generate many QSAR\nmodels with large numbers of molecules and descriptors, the best QSAR methods\nare those that can generate the most accurate predictions but that are also\ninsensitive to hyperparameters and are computationally efficient. Here we\ncompare Light Gradient Boosting Machine (LightGBM) to random forest,\nsingle-task deep neural nets, and Extreme Gradient Boosting (XGBoost) on 30\nin-house data sets. While any boosting algorithm has many adjustable\nhyperparameters, we can define a set of standard hyperparameters at which\nLightGBM makes predictions about as accurate as single-task deep neural nets,\nbut is a factor of 1000-fold faster than random forest and ~4-fold faster than\nXGBoost in terms of total computational time for the largest models. Another\nvery useful feature of LightGBM is that it includes a native method for\nestimating prediction intervals.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 20:19:44 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Sheridan", "Robert P.", ""], ["Liaw", "Andy", ""], ["Tudor", "Matthew", ""]]}, {"id": "2105.08629", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Kim Byeoung-su, Radu Timofte, Angeline Pouget,\n  Fenglong Song, Cheng Li, Shuai Xiao, Zhongqian Fu, Matteo Maggioni, Yibin\n  Huang, Shen Cheng, Xin Lu, Yifeng Zhou, Liangyu Chen, Donghao Liu, Xiangyu\n  Zhang, Haoqiang Fan, Jian Sun, Shuaicheng Liu, Minsu Kwon, Myungje Lee,\n  Jaeyoon Yoo, Changbeom Kang, Shinjo Wang, Bin Huang, Tianbao Zhou, Shuai Liu,\n  Lei Lei, Chaoyu Feng, Liguang Huang, Zhikun Lei, Feifei Chen", "title": "Fast Camera Image Denoising on Mobile GPUs with Deep Learning, Mobile AI\n  2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial\n  text overlap with arXiv:2105.07809, arXiv:2105.07825", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is one of the most critical problems in mobile photo\nprocessing. While many solutions have been proposed for this task, they are\nusually working with synthetic data and are too computationally expensive to\nrun on mobile devices. To address this problem, we introduce the first Mobile\nAI challenge, where the target is to develop an end-to-end deep learning-based\nimage denoising solution that can demonstrate high efficiency on smartphone\nGPUs. For this, the participants were provided with a novel large-scale dataset\nconsisting of noisy-clean image pairs captured in the wild. The runtime of all\nmodels was evaluated on the Samsung Exynos 2100 chipset with a powerful Mali\nGPU capable of accelerating floating-point and quantized neural networks. The\nproposed solutions are fully compatible with any mobile GPU and are capable of\nprocessing 480p resolution images under 40-80 ms while achieving high fidelity\nresults. A detailed description of all models developed in the challenge is\nprovided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:27:56 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ignatov", "Andrey", ""], ["Byeoung-su", "Kim", ""], ["Timofte", "Radu", ""], ["Pouget", "Angeline", ""], ["Song", "Fenglong", ""], ["Li", "Cheng", ""], ["Xiao", "Shuai", ""], ["Fu", "Zhongqian", ""], ["Maggioni", "Matteo", ""], ["Huang", "Yibin", ""], ["Cheng", "Shen", ""], ["Lu", "Xin", ""], ["Zhou", "Yifeng", ""], ["Chen", "Liangyu", ""], ["Liu", "Donghao", ""], ["Zhang", "Xiangyu", ""], ["Fan", "Haoqiang", ""], ["Sun", "Jian", ""], ["Liu", "Shuaicheng", ""], ["Kwon", "Minsu", ""], ["Lee", "Myungje", ""], ["Yoo", "Jaeyoon", ""], ["Kang", "Changbeom", ""], ["Wang", "Shinjo", ""], ["Huang", "Bin", ""], ["Zhou", "Tianbao", ""], ["Liu", "Shuai", ""], ["Lei", "Lei", ""], ["Feng", "Chaoyu", ""], ["Huang", "Liguang", ""], ["Lei", "Zhikun", ""], ["Chen", "Feifei", ""]]}, {"id": "2105.08630", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Grigory Malivenko, David Plowman, Samarth Shukla, Radu\n  Timofte, Ziyu Zhang, Yicheng Wang, Zilong Huang, Guozhong Luo, Gang Yu, Bin\n  Fu, Yiran Wang, Xingyi Li, Min Shi, Ke Xian, Zhiguo Cao, Jin-Hua Du, Pei-Lin\n  Wu, Chao Ge, Jiaoyang Yao, Fangwen Tu, Bo Li, Jung Eun Yoo, Kwanggyoon Seo,\n  Jialei Xu, Zhenyu Li, Xianming Liu, Junjun Jiang, Wei-Chi Chen, Shayan Joya,\n  Huanhuan Fan, Zhaobing Kang, Ang Li, Tianpeng Feng, Yang Liu, Chuannan Sheng,\n  Jian Yin, Fausto T. Benavide", "title": "Fast and Accurate Single-Image Depth Estimation on Mobile Devices,\n  Mobile AI 2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: text overlap\n  with arXiv:2105.07809", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation is an important computer vision problem with many practical\napplications to mobile devices. While many solutions have been proposed for\nthis task, they are usually very computationally expensive and thus are not\napplicable for on-device inference. To address this problem, we introduce the\nfirst Mobile AI challenge, where the target is to develop an end-to-end deep\nlearning-based depth estimation solutions that can demonstrate a nearly\nreal-time performance on smartphones and IoT platforms. For this, the\nparticipants were provided with a new large-scale dataset containing RGB-depth\nimage pairs obtained with a dedicated stereo ZED camera producing\nhigh-resolution depth maps for objects located at up to 50 meters. The runtime\nof all models was evaluated on the popular Raspberry Pi 4 platform with a\nmobile ARM-based Broadcom chipset. The proposed solutions can generate VGA\nresolution depth maps at up to 10 FPS on the Raspberry Pi 4 while achieving\nhigh fidelity results, and are compatible with any Android or Linux-based\nmobile devices. A detailed description of all models developed in the challenge\nis provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:49:57 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ignatov", "Andrey", ""], ["Malivenko", "Grigory", ""], ["Plowman", "David", ""], ["Shukla", "Samarth", ""], ["Timofte", "Radu", ""], ["Zhang", "Ziyu", ""], ["Wang", "Yicheng", ""], ["Huang", "Zilong", ""], ["Luo", "Guozhong", ""], ["Yu", "Gang", ""], ["Fu", "Bin", ""], ["Wang", "Yiran", ""], ["Li", "Xingyi", ""], ["Shi", "Min", ""], ["Xian", "Ke", ""], ["Cao", "Zhiguo", ""], ["Du", "Jin-Hua", ""], ["Wu", "Pei-Lin", ""], ["Ge", "Chao", ""], ["Yao", "Jiaoyang", ""], ["Tu", "Fangwen", ""], ["Li", "Bo", ""], ["Yoo", "Jung Eun", ""], ["Seo", "Kwanggyoon", ""], ["Xu", "Jialei", ""], ["Li", "Zhenyu", ""], ["Liu", "Xianming", ""], ["Jiang", "Junjun", ""], ["Chen", "Wei-Chi", ""], ["Joya", "Shayan", ""], ["Fan", "Huanhuan", ""], ["Kang", "Zhaobing", ""], ["Li", "Ang", ""], ["Feng", "Tianpeng", ""], ["Liu", "Yang", ""], ["Sheng", "Chuannan", ""], ["Yin", "Jian", ""], ["Benavide", "Fausto T.", ""]]}, {"id": "2105.08633", "submitter": "Konstantinos Spiliopoulos", "authors": "Justin Sirignano, Jonathan MacArt, Konstantinos Spiliopoulos", "title": "PDE-constrained Models with Neural Network Terms: Optimization and\n  Global Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has used deep learning to develop partial differential\nequation (PDE) models in science and engineering. The functional form of the\nPDE is determined by a neural network, and the neural network parameters are\ncalibrated to available data. Calibration of the embedded neural network can be\nperformed by optimizing over the PDE. Motivated by these applications, we\nrigorously study the optimization of a class of linear elliptic PDEs with\nneural network terms. The neural network parameters in the PDE are optimized\nusing gradient descent, where the gradient is evaluated using an adjoint PDE.\nAs the number of parameters become large, the PDE and adjoint PDE converge to a\nnon-local PDE system. Using this limit PDE system, we are able to prove\nconvergence of the neural network-PDE to a global minimum during the\noptimization. The limit PDE system contains a non-local linear operator whose\neigenvalues are positive but become arbitrarily small. The lack of a spectral\ngap for the eigenvalues poses the main challenge for the global convergence\nproof. Careful analysis of the spectral decomposition of the coupled PDE and\nadjoint PDE system is required. Finally, we use this adjoint method to train a\nneural network model for an application in fluid mechanics, in which the neural\nnetwork functions as a closure model for the Reynolds-averaged Navier-Stokes\n(RANS) equations. The RANS neural network model is trained on several datasets\nfor turbulent channel flow and is evaluated out-of-sample at different Reynolds\nnumbers.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:04:33 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:49:24 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sirignano", "Justin", ""], ["MacArt", "Jonathan", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "2105.08643", "submitter": "Zekai Chen", "authors": "Zekai Chen, Maiwang Shi, Xiao Zhang, Haochao Ying", "title": "ASM2TV: An Adaptive Semi-Supervised Multi-Task Multi-View Learning\n  Framework", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world scenarios, such as human activity recognition (HAR) in IoT,\ncan be formalized as a multi-task multi-view learning problem. Each specific\ntask consists of multiple shared feature views collected from multiple sources,\neither homogeneous or heterogeneous. Common among recent approaches is to\nemploy a typical hard/soft sharing strategy at the initial phase separately for\neach view across tasks to uncover common knowledge, underlying the assumption\nthat all views are conditionally independent. On the one hand, multiple views\nacross tasks possibly relate to each other under practical situations. On the\nother hand, supervised methods might be insufficient when labeled data is\nscarce. To tackle these challenges, we introduce a novel framework ASM2TV for\nsemi-supervised multi-task multi-view learning. We present a new perspective\nnamed gating control policy, a learnable task-view-interacted sharing policy\nthat adaptively selects the most desirable candidate shared block for any view\nacross any task, which uncovers more fine-grained task-view-interacted\nrelatedness and improves inference efficiency. Significantly, our proposed\ngathering consistency adaption procedure takes full advantage of large amounts\nof unlabeled fragmented time-series, making it a general framework that\naccommodates a wide range of applications. Experiments on two diverse\nreal-world HAR benchmark datasets collected from various subjects and sources\ndemonstrate our framework's superiority over other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:15:32 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Zekai", ""], ["Shi", "Maiwang", ""], ["Zhang", "Xiao", ""], ["Ying", "Haochao", ""]]}, {"id": "2105.08644", "submitter": "Hao Xie", "authors": "Hao Xie, Linfeng Zhang, Lei Wang", "title": "Ab-initio study of interacting fermions at finite temperature with\n  neural canonical transformation", "comments": "11 pages, 6 figures, code: https://github.com/buwantaiji/FermiFlow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el cond-mat.quant-gas cond-mat.stat-mech cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a variational density matrix approach to the thermal properties of\ninteracting fermions in the continuum. The variational density matrix is\nparametrized by a permutation equivariant many-body unitary transformation\ntogether with a discrete probabilistic model. The unitary transformation is\nimplemented as a quantum counterpart of neural canonical transformation, which\nincorporates correlation effects via a flow of fermion coordinates. As the\nfirst application, we study electrons in a two-dimensional quantum dot with an\ninteraction-induced crossover from Fermi liquid to Wigner molecule. The present\napproach provides accurate results in the low-temperature regime, where\nconventional quantum Monte Carlo methods face severe difficulties due to the\nfermion sign problem. The approach is general and flexible for further\nextensions, thus holds the promise to deliver new physical results on strongly\ncorrelated fermions in the context of ultracold quantum gases, condensed\nmatter, and warm dense matter physics.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:16:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Xie", "Hao", ""], ["Zhang", "Linfeng", ""], ["Wang", "Lei", ""]]}, {"id": "2105.08649", "submitter": "Zekai Chen", "authors": "Zekai Chen, Fangtian Zhong, Zhumin Chen, Xiao Zhang, Robert Pless,\n  Xiuzhen Cheng", "title": "DCAP: Deep Cross Attentional Product Network for User Response\n  Prediction", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User response prediction, which aims to predict the probability that a user\nwill provide a predefined positive response in a given context such as clicking\non an ad or purchasing an item, is crucial to many industrial applications such\nas online advertising, recommender systems, and search ranking. However, due to\nthe high dimensionality and super sparsity of the data collected in these\ntasks, handcrafting cross features is inevitably time expensive. Prior studies\nin predicting user response leveraged the feature interactions by enhancing\nfeature vectors with products of features to model second-order or high-order\ncross features, either explicitly or implicitly. Nevertheless, these existing\nmethods can be hindered by not learning sufficient cross features due to model\narchitecture limitations or modeling all high-order feature interactions with\nequal weights. This work aims to fill this gap by proposing a novel\narchitecture Deep Cross Attentional Product Network (DCAP), which keeps cross\nnetwork's benefits in modeling high-order feature interactions explicitly at\nthe vector-wise level. Beyond that, it can differentiate the importance of\ndifferent cross features in each network layer inspired by the multi-head\nattention mechanism and Product Neural Network (PNN), allowing practitioners to\nperform a more in-depth analysis of user behaviors. Additionally, our proposed\nmodel can be easily implemented and train in parallel. We conduct comprehensive\nexperiments on three real-world datasets. The results have robustly\ndemonstrated that our proposed model DCAP achieves superior prediction\nperformance compared with the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:27:20 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Zekai", ""], ["Zhong", "Fangtian", ""], ["Chen", "Zhumin", ""], ["Zhang", "Xiao", ""], ["Pless", "Robert", ""], ["Cheng", "Xiuzhen", ""]]}, {"id": "2105.08664", "submitter": "Farzan Soleymani", "authors": "Farzan Soleymani, Eric Paquet", "title": "Deep Graph Convolutional Reinforcement Learning for Financial Portfolio\n  Management -- DeepPocket", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2021.115127", "report-no": null, "categories": "q-fin.CP cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio management aims at maximizing the return on investment while\nminimizing risk by continuously reallocating the assets forming the portfolio.\nThese assets are not independent but correlated during a short time period. A\ngraph convolutional reinforcement learning framework called DeepPocket is\nproposed whose objective is to exploit the time-varying interrelations between\nfinancial instruments. These interrelations are represented by a graph whose\nnodes correspond to the financial instruments while the edges correspond to a\npair-wise correlation function in between assets. DeepPocket consists of a\nrestricted, stacked autoencoder for feature extraction, a convolutional network\nto collect underlying local information shared among financial instruments, and\nan actor-critic reinforcement learning agent. The actor-critic structure\ncontains two convolutional networks in which the actor learns and enforces an\ninvestment policy which is, in turn, evaluated by the critic in order to\ndetermine the best course of action by constantly reallocating the various\nportfolio assets to optimize the expected return on investment. The agent is\ninitially trained offline with online stochastic batching on historical data.\nAs new data become available, it is trained online with a passive concept drift\napproach to handle unexpected changes in their distributions. DeepPocket is\nevaluated against five real-life datasets over three distinct investment\nperiods, including during the Covid-19 crisis, and clearly outperformed market\nindexes.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:07:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Soleymani", "Farzan", ""], ["Paquet", "Eric", ""]]}, {"id": "2105.08665", "submitter": "Ambareesh Ravi", "authors": "Ambareesh Ravi, Amith Nandakumar", "title": "A multimodal deep learning framework for scalable content based visual\n  media retrieval", "comments": "Paper pertaining to a course project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel, efficient, modular and scalable framework for content\nbased visual media retrieval systems by leveraging the power of Deep Learning\nwhich is flexible to work both for images and videos conjointly and we also\nintroduce an efficient comparison and filtering metric for retrieval. We put\nforward our findings from critical performance tests comparing our method to\nthe predominant conventional approach to demonstrate the feasibility and\nefficiency of the proposed solution with best practices, possible improvements\nthat may further augment the ability of retrieval architectures.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:49:08 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ravi", "Ambareesh", ""], ["Nandakumar", "Amith", ""]]}, {"id": "2105.08666", "submitter": "Jing-Cheng Pang", "authors": "Jing-Cheng Pang, Tian Xu, Sheng-Yi Jiang, Yu-Ren Liu, Yang Yu", "title": "Sparsity Prior Regularized Q-learning for Sparse Action Tasks", "comments": "Reinforcement learning; Sparse action task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many decision-making tasks, some specific actions are limited in their\nfrequency or total amounts, such as \"fire\" in the gunfight game and \"buy/sell\"\nin the stock trading. We name such actions as \"sparse action\". Sparse action\noften plays a crucial role in achieving good performance. However, their\nQ-values, estimated by \\emph{classical Bellman update}, usually suffer from a\nlarge estimation error due to the sparsity of their samples. The \\emph{greedy}\npolicy could be greatly misled by the biased Q-function and takes sparse action\naggressively, which leads to a huge sub-optimality. This paper constructs a\nreference distribution that assigns a low probability to sparse action and\nproposes a regularized objective with an explicit constraint to the reference\ndistribution. Furthermore, we derive a regularized Bellman operator and a\nregularized optimal policy that can slow down the propagation of error and\nguide the agent to take sparse action more carefully. The experiment results\ndemonstrate that our method achieves state-of-the-art performance on typical\nsparse action tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:50:42 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 02:15:07 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Pang", "Jing-Cheng", ""], ["Xu", "Tian", ""], ["Jiang", "Sheng-Yi", ""], ["Liu", "Yu-Ren", ""], ["Yu", "Yang", ""]]}, {"id": "2105.08667", "submitter": "Uthaipon Tantipongpipat", "authors": "Kyra Yee, Uthaipon Tantipongpipat, Shubhanshu Mishra", "title": "Image Cropping on Twitter: Fairness Metrics, their Limitations, and the\n  Importance of Representation, Design, and Agency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter uses machine learning to crop images, where crops are centered around\nthe part predicted to be the most salient. In fall 2020, Twitter users raised\nconcerns that the automated image cropping system on Twitter favored\nlight-skinned over dark-skinned individuals, as well as concerns that the\nsystem favored cropping woman's bodies instead of their heads. In order to\naddress these concerns, we conduct an extensive analysis using formalized group\nfairness metrics. We find systematic disparities in cropping and identify\ncontributing factors, including the fact that the cropping based on the single\nmost salient point can amplify the disparities. However, we demonstrate that\nformalized fairness metrics and quantitative analysis on their own are\ninsufficient for capturing the risk of representational harm in automatic\ncropping. We suggest the removal of saliency-based cropping in favor of a\nsolution that better preserves user agency. For developing a new solution that\nsufficiently address concerns related to representational harm, our critique\nmotivates a combination of quantitative and qualitative methods that include\nhuman-centered design.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:50:50 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yee", "Kyra", ""], ["Tantipongpipat", "Uthaipon", ""], ["Mishra", "Shubhanshu", ""]]}, {"id": "2105.08669", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Enhancement of prediction algorithms by betting", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note proposes a procedure for enhancing the quality of probabilistic\nprediction algorithms via betting against their predictions. It is inspired by\nthe success of the conformal test martingales that have been developed\nrecently.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:53:00 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "2105.08671", "submitter": "Neel Kanwal", "authors": "Jiahui Geng, Neel Kanwal, Martin Gilje Jaatun, Chunming Rong", "title": "DID-eFed: Facilitating Federated Learning as a Service with\n  Decentralized Identities", "comments": "Paper accepted in EASE2021", "journal-ref": null, "doi": "10.1145/3463274.3463352", "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have entered the era of big data, and it is considered to be the \"fuel\"\nfor the flourishing of artificial intelligence applications. The enactment of\nthe EU General Data Protection Regulation (GDPR) raises concerns about\nindividuals' privacy in big data. Federated learning (FL) emerges as a\nfunctional solution that can help build high-performance models shared among\nmultiple parties while still complying with user privacy and data\nconfidentiality requirements. Although FL has been intensively studied and used\nin real applications, there is still limited research related to its prospects\nand applications as a FLaaS (Federated Learning as a Service) to interested 3rd\nparties. In this paper, we present a FLaaS system: DID-eFed, where FL is\nfacilitated by decentralized identities (DID) and a smart contract. DID enables\na more flexible and credible decentralized access management in our system,\nwhile the smart contract offers a frictionless and less error-prone process. We\ndescribe particularly the scenario where our DID-eFed enables the FLaaS among\nhospitals and research institutions.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:55:34 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 07:44:07 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Geng", "Jiahui", ""], ["Kanwal", "Neel", ""], ["Jaatun", "Martin Gilje", ""], ["Rong", "Chunming", ""]]}, {"id": "2105.08675", "submitter": "Christoph Hertrich", "authors": "Vincent Froese, Christoph Hertrich, Rolf Niedermeier", "title": "The Computational Complexity of ReLU Network Training Parameterized by\n  Data Dimensionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the computational complexity of training simple neural networks\nwith rectified linear units (ReLUs) has recently been a subject of intensive\nresearch. Closing gaps and complementing results from the literature, we\npresent several results on the parameterized complexity of training two-layer\nReLU networks with respect to various loss functions. After a brief discussion\nof other parameters, we focus on analyzing the influence of the dimension $d$\nof the training data on the computational complexity. We provide running time\nlower bounds in terms of W[1]-hardness for parameter $d$ and prove that known\nbrute-force strategies are essentially optimal (assuming the Exponential Time\nHypothesis). In comparison with previous work, our results hold for a broad(er)\nrange of loss functions, including $\\ell^p$-loss for all $p\\in[0,\\infty]$. In\nparticular, we extend a known polynomial-time algorithm for constant $d$ and\nconvex loss functions to a more general class of loss functions, matching our\nrunning time lower bounds also in these cases.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:05:26 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 08:32:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Froese", "Vincent", ""], ["Hertrich", "Christoph", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "2105.08678", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian", "title": "Nonparametric Modeling of Higher-Order Interactions via Hypergraphons", "comments": "To appear in Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study statistical and algorithmic aspects of using hypergraphons, that are\nlimits of large hypergraphs, for modeling higher-order interactions. Although\nhypergraphons are extremely powerful from a modeling perspective, we consider a\nrestricted class of Simple Lipschitz Hypergraphons (SLH), that are amenable to\npractically efficient estimation. We also provide rates of convergence for our\nestimator that are optimal for the class of SLH. Simulation results are\nprovided to corroborate the theory.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:08:29 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""]]}, {"id": "2105.08709", "submitter": "Ji Gao", "authors": "Ji Gao, Amin Karbasi, Mohammad Mahmoody", "title": "Learning and Certification under Instance-targeted Poisoning", "comments": "This is the full version of a paper appearing in The Conference on\n  Uncertainty in Artificial Intelligence (UAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study PAC learnability and certification under\ninstance-targeted poisoning attacks, where the adversary may change a fraction\nof the training set with the goal of fooling the learner at a specific target\ninstance. Our first contribution is to formalize the problem in various\nsettings, and explicitly discussing subtle aspects such as learner's randomness\nand whether (or not) adversary's attack can depend on it. We show that when the\nbudget of the adversary scales sublinearly with the sample complexity, PAC\nlearnability and certification are achievable. In contrast, when the\nadversary's budget grows linearly with the sample complexity, the adversary can\npotentially drive up the expected 0-1 loss to one. We further extend our\nresults to distribution-specific PAC learning in the same attack model and show\nthat proper learning with certification is possible for learning halfspaces\nunder Gaussian distribution. Finally, we empirically study the robustness of K\nnearest neighbour, logistic regression, multi-layer perceptron, and\nconvolutional neural network on real data sets, and test them against\ntargeted-poisoning attacks. Our experimental results show that many models,\nespecially state-of-the-art neural networks, are indeed vulnerable to these\nstrong attacks. Interestingly, we observe that methods with high standard\naccuracy might be more vulnerable to instance-targeted poisoning attacks.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:48:15 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Gao", "Ji", ""], ["Karbasi", "Amin", ""], ["Mahmoody", "Mohammad", ""]]}, {"id": "2105.08710", "submitter": "Anirudh Goyal", "authors": "Kanika Madan, Nan Rosemary Ke, Anirudh Goyal, Bernhard Sch\\\"olkopf,\n  Yoshua Bengio", "title": "Fast and Slow Learning of Recurrent Independent Mechanisms", "comments": "Accepted at ICLR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing knowledge into interchangeable pieces promises a generalization\nadvantage when there are changes in distribution. A learning agent interacting\nwith its environment is likely to be faced with situations requiring novel\ncombinations of existing pieces of knowledge. We hypothesize that such a\ndecomposition of knowledge is particularly relevant for being able to\ngeneralize in a systematic manner to out-of-distribution changes. To study\nthese ideas, we propose a particular training framework in which we assume that\nthe pieces of knowledge an agent needs and its reward function are stationary\nand can be re-used across tasks. An attention mechanism dynamically selects\nwhich modules can be adapted to the current task, and the parameters of the\nselected modules are allowed to change quickly as the learner is confronted\nwith variations in what it experiences, while the parameters of the attention\nmechanisms act as stable, slowly changing, meta-parameters. We focus on pieces\nof knowledge captured by an ensemble of modules sparsely communicating with\neach other via a bottleneck of attention. We find that meta-learning the\nmodular aspects of the proposed system greatly helps in achieving faster\nadaptation in a reinforcement learning setup involving navigation in a\npartially observed grid world with image-level input. We also find that\nreversing the role of parameters and meta-parameters does not work nearly as\nwell, suggesting a particular role for fast adaptation of the dynamically\nselected modules.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:50:32 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 03:10:30 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Madan", "Kanika", ""], ["Ke", "Nan Rosemary", ""], ["Goyal", "Anirudh", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2105.08714", "submitter": "Evan Shelhamer", "authors": "Dequan Wang, An Ju, Evan Shelhamer, David Wagner, Trevor Darrell", "title": "Fighting Gradients with Gradients: Dynamic Defenses against Adversarial\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks optimize against models to defeat defenses. Existing\ndefenses are static, and stay the same once trained, even while attacks change.\nWe argue that models should fight back, and optimize their defenses against\nattacks at test time. We propose dynamic defenses, to adapt the model and input\nduring testing, by defensive entropy minimization (dent). Dent alters testing,\nbut not training, for compatibility with existing models and train-time\ndefenses. Dent improves the robustness of adversarially-trained defenses and\nnominally-trained models against white-box, black-box, and adaptive attacks on\nCIFAR-10/100 and ImageNet. In particular, dent boosts state-of-the-art defenses\nby 20+ points absolute against AutoAttack on CIFAR-10 at $\\epsilon_\\infty$ =\n8/255.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:55:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Wang", "Dequan", ""], ["Ju", "An", ""], ["Shelhamer", "Evan", ""], ["Wagner", "David", ""], ["Darrell", "Trevor", ""]]}, {"id": "2105.08717", "submitter": "Michele Ceriotti", "authors": "Alexander Goscinski, F\\'elix Musil, Sergey Pozdnyakov, and Michele\n  Ceriotti", "title": "Optimal radial basis for density-based atomic representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input of almost every machine learning algorithm targeting the properties\nof matter at the atomic scale involves a transformation of the list of\nCartesian atomic coordinates into a more symmetric representation. Many of\nthese most popular representations can be seen as an expansion of the\nsymmetrized correlations of the atom density, and differ mainly by the choice\nof basis. Here we discuss how to build an adaptive, optimal numerical basis\nthat is chosen to represent most efficiently the structural diversity of the\ndataset at hand. For each training dataset, this optimal basis is unique, and\ncan be computed at no additional cost with respect to the primitive basis by\napproximating it with splines. We demonstrate that this construction yields\nrepresentations that are accurate and computationally efficient, presenting\nexamples that involve both molecular and condensed-phase machine-learning\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:57:08 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Goscinski", "Alexander", ""], ["Musil", "F\u00e9lix", ""], ["Pozdnyakov", "Sergey", ""], ["Ceriotti", "Michele", ""]]}, {"id": "2105.08721", "submitter": "Pujan Pokhrel", "authors": "Pujan Pokhrel", "title": "A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters", "comments": "arXiv admin note: text overlap with arXiv:2105.08583", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast\ndominant wave periods in oceanic waters. First, we use the data collected from\nCDIP buoys and apply various data filtering methods. The data filtering methods\nallow us to obtain a high-quality dataset for training and validation purposes.\nWe then extract various wave-based features like wave heights, periods,\nskewness, kurtosis, etc., and atmospheric features like humidity, pressure, and\nair temperature for the buoys. Afterward, we train algorithms that use LightGBM\nand Extra Trees through a hv-block cross-validation scheme to forecast dominant\nwave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,\nand 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,\nExtra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,\n15-day ahead, and 30 day ahead prediction. In case of the test dataset,\nLightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and\n30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day\nahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both\ntraining and the test dataset suggests that the machine learning models\ndeveloped in this paper are robust. Since the LightGBM algorithm outperforms ET\nfor all the windows tested, it is taken as the final algorithm. Note that the\nperformance of both methods does not decrease significantly as the forecast\nhorizon increases. Likewise, the proposed method outperforms the numerical\napproaches included in this paper in the test dataset. For 1 day ahead\nprediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,\n0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre\nfor Medium-range Weather Forecasts (ECMWF) model, which outperforms all the\nother methods in the test dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:58:05 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 19:32:22 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 01:14:50 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 10:43:25 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Pokhrel", "Pujan", ""]]}, {"id": "2105.08741", "submitter": "Dominik Dold", "authors": "Josep Soler Garrido, Dominik Dold, Johannes Frank", "title": "Machine learning on knowledge graphs for context-aware security\n  monitoring", "comments": "Accepted for publication at IEEE-CSR 2021. Data is available on\n  https://github.com/dodo47/cyberML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning techniques are gaining attention in the context of intrusion\ndetection due to the increasing amounts of data generated by monitoring tools,\nas well as the sophistication displayed by attackers in hiding their activity.\nHowever, existing methods often exhibit important limitations in terms of the\nquantity and relevance of the generated alerts. Recently, knowledge graphs are\nfinding application in the cybersecurity domain, showing the potential to\nalleviate some of these drawbacks thanks to their ability to seamlessly\nintegrate data from multiple domains using human-understandable vocabularies.\nWe discuss the application of machine learning on knowledge graphs for\nintrusion detection and experimentally evaluate a link-prediction method for\nscoring anomalous activity in industrial systems. After initial unsupervised\ntraining, the proposed method is shown to produce intuitively well-calibrated\nand interpretable alerts in a diverse range of scenarios, hinting at the\npotential benefits of relational machine learning on knowledge graphs for\nintrusion detection purposes.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:00:19 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Garrido", "Josep Soler", ""], ["Dold", "Dominik", ""], ["Frank", "Johannes", ""]]}, {"id": "2105.08748", "submitter": "Agustin Castellano", "authors": "Agustin Castellano, Hancheng Min, Juan Bazerque, Enrique Mallada", "title": "Learning to Act Safely with Limited Exposure and Almost Sure Certainty", "comments": "16 pages, 7 figures. Submitted to TAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to put forward the concept that learning to take safe actions\nin unknown environments, even with probability one guarantees, can be achieved\nwithout the need for an unbounded number of exploratory trials, provided that\none is willing to navigate trade-offs between optimality, level of exposure to\nunsafe events, and the maximum detection time of unsafe actions. We illustrate\nthis concept in two complementary settings. We first focus on the canonical\nmulti-armed bandit problem and seek to study the intrinsic trade-offs of\nlearning safety in the presence of uncertainty. Under mild assumptions on\nsufficient exploration, we provide an algorithm that provably detects all\nunsafe machines in an (expected) finite number of rounds. The analysis also\nunveils a trade-off between the number of rounds needed to secure the\nenvironment and the probability of discarding safe machines. We then consider\nthe problem of finding optimal policies for a Markov Decision Process (MDP)\nwith almost sure constraints. We show that the (action) value function\nsatisfies a barrier-based decomposition which allows for the identification of\nfeasible policies independently of the reward process. Using this\ndecomposition, we develop a Barrier-learning algorithm, that identifies such\nunsafe state-action pairs in a finite expected number of steps. Our analysis\nfurther highlights a trade-off between the time lag for the underlying MDP\nnecessary to detect unsafe actions, and the level of exposure to unsafe events.\nSimulations corroborate our theoretical findings, further illustrating the\naforementioned trade-offs, and suggesting that safety constraints can further\nspeed up the learning process.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:05:12 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 18:12:33 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Castellano", "Agustin", ""], ["Min", "Hancheng", ""], ["Bazerque", "Juan", ""], ["Mallada", "Enrique", ""]]}, {"id": "2105.08756", "submitter": "Jing Yu Koh", "authors": "Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, Peter Anderson", "title": "Pathdreamer: A World Model for Indoor Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People navigating in unfamiliar buildings take advantage of myriad visual,\nspatial and semantic cues to efficiently achieve their navigation goals.\nTowards equipping computational agents with similar capabilities, we introduce\nPathdreamer, a visual world model for agents navigating in novel indoor\nenvironments. Given one or more previous visual observations, Pathdreamer\ngenerates plausible high-resolution 360 visual observations (RGB, semantic\nsegmentation and depth) for viewpoints that have not been visited, in buildings\nnot seen during training. In regions of high uncertainty (e.g. predicting\naround corners, imagining the contents of an unseen room), Pathdreamer can\npredict diverse scenes, allowing an agent to sample multiple realistic outcomes\nfor a given trajectory. We demonstrate that Pathdreamer encodes useful and\naccessible visual, spatial and semantic knowledge about human environments by\nusing it in the downstream task of Vision-and-Language Navigation (VLN).\nSpecifically, we show that planning ahead with Pathdreamer brings about half\nthe benefit of looking ahead at actual observations from unobserved parts of\nthe environment. We hope that Pathdreamer will help unlock model-based\napproaches to challenging embodied navigation tasks such as navigating to\nspecified objects and VLN.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:13:53 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Koh", "Jing Yu", ""], ["Lee", "Honglak", ""], ["Yang", "Yinfei", ""], ["Baldridge", "Jason", ""], ["Anderson", "Peter", ""]]}, {"id": "2105.08769", "submitter": "Neil Walton", "authors": "Neil Walton, Kuang Xu", "title": "Learning and Information in Stochastic Networks and Queues", "comments": "review article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the role of information and learning in the stability and\noptimization of queueing systems. In recent years, techniques from supervised\nlearning, bandit learning and reinforcement learning have been applied to\nqueueing systems supported by increasing role of information in decision\nmaking. We present observations and new results that help rationalize the\napplication of these areas to queueing systems.\n  We prove that the MaxWeight and BackPressure policies are an application of\nBlackwell's Approachability Theorem. This connects queueing theoretic results\nwith adversarial learning. We then discuss the requirements of statistical\nlearning for service parameter estimation. As an example, we show how queue\nsize regret can be bounded when applying a perceptron algorithm to classify\nservice. Next, we discuss the role of state information in improved decision\nmaking. Here we contrast the roles of epistemic information (information on\nuncertain parameters) and aleatoric information (information on an uncertain\nstate). Finally we review recent advances in the theory of reinforcement\nlearning and queueing, as well as, provide discussion on current research\nchallenges.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:35:36 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 10:11:48 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 07:52:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Walton", "Neil", ""], ["Xu", "Kuang", ""]]}, {"id": "2105.08791", "submitter": "Xiaocheng Tang", "authors": "Xiaocheng Tang, Fan Zhang, Zhiwei Qin, Yansheng Wang, Dingyuan Shi,\n  Bingchen Song, Yongxin Tong, Hongtu Zhu, Jieping Ye", "title": "Value Function is All You Need: A Unified Learning Framework for Ride\n  Hailing Platforms", "comments": "KDD 2021; Ride-hailing marketplace open simulation platform:\n  https://outreach.didichuxing.com/Simulation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Large ride-hailing platforms, such as DiDi, Uber and Lyft, connect tens of\nthousands of vehicles in a city to millions of ride demands throughout the day,\nproviding great promises for improving transportation efficiency through the\ntasks of order dispatching and vehicle repositioning. Existing studies,\nhowever, usually consider the two tasks in simplified settings that hardly\naddress the complex interactions between the two, the real-time fluctuations\nbetween supply and demand, and the necessary coordinations due to the\nlarge-scale nature of the problem. In this paper we propose a unified\nvalue-based dynamic learning framework (V1D3) for tackling both tasks. At the\ncenter of the framework is a globally shared value function that is updated\ncontinuously using online experiences generated from real-time platform\ntransactions. To improve the sample-efficiency and the robustness, we further\npropose a novel periodic ensemble method combining the fast online learning\nwith a large-scale offline training scheme that leverages the abundant\nhistorical driver trajectory data. This allows the proposed framework to adapt\nquickly to the highly dynamic environment, to generalize robustly to recurrent\npatterns and to drive implicit coordinations among the population of managed\nvehicles. Extensive experiments based on real-world datasets show considerably\nimprovements over other recently proposed methods on both tasks. Particularly,\nV1D3 outperforms the first prize winners of both dispatching and repositioning\ntracks in the KDD Cup 2020 RL competition, achieving state-of-the-art results\non improving both total driver income and user experience related metrics.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:22:24 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 01:04:34 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 08:08:31 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Tang", "Xiaocheng", ""], ["Zhang", "Fan", ""], ["Qin", "Zhiwei", ""], ["Wang", "Yansheng", ""], ["Shi", "Dingyuan", ""], ["Song", "Bingchen", ""], ["Tong", "Yongxin", ""], ["Zhu", "Hongtu", ""], ["Ye", "Jieping", ""]]}, {"id": "2105.08793", "submitter": "Hyunsoo Cho", "authors": "Hyunsoo Cho, Jinseok Seol, Sang-goo Lee", "title": "Masked Contrastive Learning for Anomaly Detection", "comments": "Accepted to IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting anomalies is one fundamental aspect of a safety-critical software\nsystem, however, it remains a long-standing problem. Numerous branches of works\nhave been proposed to alleviate the complication and have demonstrated their\nefficiencies. In particular, self-supervised learning based methods are\nspurring interest due to their capability of learning diverse representations\nwithout additional labels. Among self-supervised learning tactics, contrastive\nlearning is one specific framework validating their superiority in various\nfields, including anomaly detection. However, the primary objective of\ncontrastive learning is to learn task-agnostic features without any labels,\nwhich is not entirely suited to discern anomalies. In this paper, we propose a\ntask-specific variant of contrastive learning named masked contrastive\nlearning, which is more befitted for anomaly detection. Moreover, we propose a\nnew inference method dubbed self-ensemble inference that further boosts\nperformance by leveraging the ability learned through auxiliary\nself-supervision tasks. By combining our models, we can outperform previous\nstate-of-the-art methods by a significant margin on various benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:27:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Cho", "Hyunsoo", ""], ["Seol", "Jinseok", ""], ["Lee", "Sang-goo", ""]]}, {"id": "2105.08810", "submitter": "Nicolas Perez-Nieves", "authors": "Nicolas Perez-Nieves and Dan F.M. Goodman", "title": "Sparse Spiking Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in emulating Spiking Neural Networks (SNNs)\non neuromorphic computing devices due to their low energy consumption. Recent\nadvances have allowed training SNNs to a point where they start to compete with\ntraditional Artificial Neural Networks (ANNs) in terms of accuracy, while at\nthe same time being energy efficient when run on neuromorphic hardware.\nHowever, the process of training SNNs is still based on dense tensor operations\noriginally developed for ANNs which do not leverage the spatiotemporally sparse\nnature of SNNs. We present here the first sparse SNN backpropagation algorithm\nwhich achieves the same or better accuracy as current state of the art methods\nwhile being significantly faster and more memory efficient. We show the\neffectiveness of our method on real datasets of varying complexity\n(Fashion-MNIST, Neuromophic-MNIST and Spiking Heidelberg Digits) achieving a\nspeedup in the backward pass of up to 70x, and 40% more memory efficient,\nwithout losing accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:00:55 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Perez-Nieves", "Nicolas", ""], ["Goodman", "Dan F. M.", ""]]}, {"id": "2105.08812", "submitter": "Mohammed Ibrahim", "authors": "Mohammed Ibrahim, Susan Gauch, Omar Salman, Mohammed Alqahatani", "title": "An Automated Method to Enrich Consumer Health Vocabularies Using GloVe\n  Word Embeddings and An Auxiliary Lexical Resource", "comments": "24 pages, 7 figures, 7 Tables, Journal", "journal-ref": null, "doi": "10.2196/preprints.26160", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Clear language makes communication easier between any two\nparties. A layman may have difficulty communicating with a professional due to\nnot understanding the specialized terms common to the domain. In healthcare, it\nis rare to find a layman knowledgeable in medical terminology which can lead to\npoor understanding of their condition and/or treatment. To bridge this gap,\nseveral professional vocabularies and ontologies have been created to map\nlaymen medical terms to professional medical terms and vice versa.\n  Objective: Many of the presented vocabularies are built manually or\nsemi-automatically requiring large investments of time and human effort and\nconsequently the slow growth of these vocabularies. In this paper, we present\nan automatic method to enrich laymen's vocabularies that has the benefit of\nbeing able to be applied to vocabularies in any domain.\n  Methods: Our entirely automatic approach uses machine learning, specifically\nGlobal Vectors for Word Embeddings (GloVe), on a corpus collected from a social\nmedia healthcare platform to extend and enhance consumer health vocabularies\n(CHV). Our approach further improves the CHV by incorporating synonyms and\nhyponyms from the WordNet ontology. The basic GloVe and our novel algorithms\nincorporating WordNet were evaluated using two laymen datasets from the\nNational Library of Medicine (NLM), Open-Access Consumer Health Vocabulary (OAC\nCHV) and MedlinePlus Healthcare Vocabulary.\n  Results: The results show that GloVe was able to find new laymen terms with\nan F-score of 48.44%. Furthermore, our enhanced GloVe approach outperformed\nbasic GloVe with an average F-score of 61%, a relative improvement of 25%.\nFurthermore, the enhanced GloVe showed a statistical significance over the two\nground truth datasets with P<.001.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:16:45 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ibrahim", "Mohammed", ""], ["Gauch", "Susan", ""], ["Salman", "Omar", ""], ["Alqahatani", "Mohammed", ""]]}, {"id": "2105.08819", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Grigory Malivenko, Radu Timofte, Sheng Chen, Xin Xia,\n  Zhaoyan Liu, Yuwei Zhang, Feng Zhu, Jiashi Li, Xuefeng Xiao, Yuan Tian,\n  Xinglong Wu, Christos Kyrkou, Yixin Chen, Zexin Zhang, Yunbo Peng, Yue Lin,\n  Saikat Dutta, Sourya Dipta Das, Nisarg A. Shah, Himanshu Kumar, Chao Ge,\n  Pei-Lin Wu, Jin-Hua Du, Andrew Batutin, Juan Pablo Federico, Konrad Lyda,\n  Levon Khojoyan, Abhishek Thanki, Sayak Paul, Shahid Siddiqui", "title": "Fast and Accurate Quantized Camera Scene Detection on Smartphones,\n  Mobile AI 2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial\n  text overlap with arXiv:2105.08630; text overlap with arXiv:2105.07825,\n  arXiv:2105.07809, arXiv:2105.08629", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera scene detection is among the most popular computer vision problem on\nsmartphones. While many custom solutions were developed for this task by phone\nvendors, none of the designed models were available publicly up until now. To\naddress this problem, we introduce the first Mobile AI challenge, where the\ntarget is to develop quantized deep learning-based camera scene classification\nsolutions that can demonstrate a real-time performance on smartphones and IoT\nplatforms. For this, the participants were provided with a large-scale CamSDD\ndataset consisting of more than 11K images belonging to the 30 most important\nscene categories. The runtime of all models was evaluated on the popular Apple\nBionic A11 platform that can be found in many iOS devices. The proposed\nsolutions are fully compatible with all major mobile AI accelerators and can\ndemonstrate more than 100-200 FPS on the majority of recent smartphone\nplatforms while achieving a top-3 accuracy of more than 98%. A detailed\ndescription of all models developed in the challenge is provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:55:38 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ignatov", "Andrey", ""], ["Malivenko", "Grigory", ""], ["Timofte", "Radu", ""], ["Chen", "Sheng", ""], ["Xia", "Xin", ""], ["Liu", "Zhaoyan", ""], ["Zhang", "Yuwei", ""], ["Zhu", "Feng", ""], ["Li", "Jiashi", ""], ["Xiao", "Xuefeng", ""], ["Tian", "Yuan", ""], ["Wu", "Xinglong", ""], ["Kyrkou", "Christos", ""], ["Chen", "Yixin", ""], ["Zhang", "Zexin", ""], ["Peng", "Yunbo", ""], ["Lin", "Yue", ""], ["Dutta", "Saikat", ""], ["Das", "Sourya Dipta", ""], ["Shah", "Nisarg A.", ""], ["Kumar", "Himanshu", ""], ["Ge", "Chao", ""], ["Wu", "Pei-Lin", ""], ["Du", "Jin-Hua", ""], ["Batutin", "Andrew", ""], ["Federico", "Juan Pablo", ""], ["Lyda", "Konrad", ""], ["Khojoyan", "Levon", ""], ["Thanki", "Abhishek", ""], ["Paul", "Sayak", ""], ["Siddiqui", "Shahid", ""]]}, {"id": "2105.08826", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Andres Romero, Heewon Kim, Radu Timofte, Chiu Man Ho,\n  Zibo Meng, Kyoung Mu Lee, Yuxiang Chen, Yutong Wang, Zeyu Long, Chenhao Wang,\n  Yifei Chen, Boshen Xu, Shuhang Gu, Lixin Duan, Wen Li, Wang Bofei, Zhang\n  Diankai, Zheng Chengjian, Liu Shaoli, Gao Si, Zhang Xiaofeng, Lu Kaidi, Xu\n  Tianyu, Zheng Hui, Xinbo Gao, Xiumei Wang, Jiaming Guo, Xueyi Zhou, Hao Jia,\n  Youliang Yan", "title": "Real-Time Video Super-Resolution on Smartphones with Deep Learning,\n  Mobile AI 2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial\n  text overlap with arXiv:2105.07825. substantial text overlap with\n  arXiv:2105.08629, arXiv:2105.07809, arXiv:2105.08630", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution has recently become one of the most important\nmobile-related problems due to the rise of video communication and streaming\nservices. While many solutions have been proposed for this task, the majority\nof them are too computationally expensive to run on portable devices with\nlimited hardware resources. To address this problem, we introduce the first\nMobile AI challenge, where the target is to develop an end-to-end deep\nlearning-based video super-resolution solutions that can achieve a real-time\nperformance on mobile GPUs. The participants were provided with the REDS\ndataset and trained their models to do an efficient 4X video upscaling. The\nruntime of all models was evaluated on the OPPO Find X2 smartphone with the\nSnapdragon 865 SoC capable of accelerating floating-point networks on its\nAdreno GPU. The proposed solutions are fully compatible with any mobile GPU and\ncan upscale videos to HD resolution at up to 80 FPS while demonstrating high\nfidelity results. A detailed description of all models developed in the\nchallenge is provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:40:50 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ignatov", "Andrey", ""], ["Romero", "Andres", ""], ["Kim", "Heewon", ""], ["Timofte", "Radu", ""], ["Ho", "Chiu Man", ""], ["Meng", "Zibo", ""], ["Lee", "Kyoung Mu", ""], ["Chen", "Yuxiang", ""], ["Wang", "Yutong", ""], ["Long", "Zeyu", ""], ["Wang", "Chenhao", ""], ["Chen", "Yifei", ""], ["Xu", "Boshen", ""], ["Gu", "Shuhang", ""], ["Duan", "Lixin", ""], ["Li", "Wen", ""], ["Bofei", "Wang", ""], ["Diankai", "Zhang", ""], ["Chengjian", "Zheng", ""], ["Shaoli", "Liu", ""], ["Si", "Gao", ""], ["Xiaofeng", "Zhang", ""], ["Kaidi", "Lu", ""], ["Tianyu", "Xu", ""], ["Hui", "Zheng", ""], ["Gao", "Xinbo", ""], ["Wang", "Xiumei", ""], ["Guo", "Jiaming", ""], ["Zhou", "Xueyi", ""], ["Jia", "Hao", ""], ["Yan", "Youliang", ""]]}, {"id": "2105.08834", "submitter": "Andrea Tirinzoni", "authors": "Riccardo Poiani, Andrea Tirinzoni, Marcello Restelli", "title": "Meta-Reinforcement Learning by Tracking Task Non-stationarity", "comments": "To appear at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world domains are subject to a structured non-stationarity which\naffects the agent's goals and the environmental dynamics. Meta-reinforcement\nlearning (RL) has been shown successful for training agents that quickly adapt\nto related tasks. However, most of the existing meta-RL algorithms for\nnon-stationary domains either make strong assumptions on the task generation\nprocess or require sampling from it at training time. In this paper, we propose\na novel algorithm (TRIO) that optimizes for the future by explicitly tracking\nthe task evolution through time. At training time, TRIO learns a variational\nmodule to quickly identify latent parameters from experience samples. This\nmodule is learned jointly with an optimal exploration policy that takes task\nuncertainty into account. At test time, TRIO tracks the evolution of the latent\nparameters online, hence reducing the uncertainty over future tasks and\nobtaining fast adaptation through the meta-learned policy. Unlike most existing\nmethods, TRIO does not assume Markovian task-evolution processes, it does not\nrequire information about the non-stationarity at training time, and it\ncaptures complex changes undergoing in the environment. We evaluate our\nalgorithm on different simulated problems and show it outperforms competitive\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:19:41 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Poiani", "Riccardo", ""], ["Tirinzoni", "Andrea", ""], ["Restelli", "Marcello", ""]]}, {"id": "2105.08840", "submitter": "Yunhao Yang", "authors": "Yunhao Yang, Zhaokun Xue", "title": "Representation Learning in Sequence to Sequence Tasks: Multi-filter\n  Gaussian Mixture Autoencoder", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heterogeneity of sentences exists in sequence to sequence tasks such as\nmachine translation. Sentences with largely varied meanings or grammatical\nstructures may increase the difficulty of convergence while training the\nnetwork. In this paper, we introduce a model to resolve the heterogeneity in\nthe sequence to sequence task. The Multi-filter Gaussian Mixture Autoencoder\n(MGMAE) utilizes an autoencoder to learn the representations of the inputs. The\nrepresentations are the outputs from the encoder, lying in the latent space\nwhose dimension is the hidden dimension of the encoder. The representations of\ntraining data in the latent space are used to train Gaussian mixtures. The\nlatent space representations are divided into several mixtures of Gaussian\ndistributions. A filter (decoder) is tuned to fit the data in one of the\nGaussian distributions specifically. Each Gaussian is corresponding to one\nfilter so that the filter is responsible for the heterogeneity within this\nGaussian. Thus the heterogeneity of the training data can be resolved.\nComparative experiments are conducted on the Geo-query dataset and\nEnglish-French translation. Our experiments show that compares to the\ntraditional encoder-decoder model, this network achieves better performance on\nsequence to sequence tasks such as machine translation and question answering.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:42:41 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Yang", "Yunhao", ""], ["Xue", "Zhaokun", ""]]}, {"id": "2105.08842", "submitter": "Ansgar Scherp", "authors": "Fabian Singhofer, Aygul Garifullina, Mathias Kern, Ansgar Scherp", "title": "rx-anon -- A Novel Approach on the De-Identification of Heterogeneous\n  Data based on a Modified Mondrian Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches for data anonymization consider relational data and\ntextual data independently. We propose rx-anon, an anonymization approach for\nheterogeneous semi-structured documents composed of relational and textual\nattributes. We map sensitive terms extracted from the text to the structured\ndata. This allows us to use concepts like k-anonymity to generate a joined,\nprivacy-preserved version of the heterogeneous data input. We introduce the\nconcept of redundant sensitive information to consistently anonymize the\nheterogeneous data. To control the influence of anonymization over unstructured\ntextual data versus structured data attributes, we introduce a modified,\nparameterized Mondrian algorithm. The parameter $\\lambda$ allows to give\ndifferent weight on the relational and textual attributes during the\nanonymization process. We evaluate our approach with two real-world datasets\nusing a Normalized Certainty Penalty score, adapted to the problem of jointly\nanonymizing relational and textual data. The results show that our approach is\ncapable of reducing information loss by using the tuning parameter to control\nthe Mondrian partitioning while guaranteeing k-anonymity for relational\nattributes as well as for sensitive terms. As rx-anon is a framework approach,\nit can be reused and extended by other anonymization algorithms, privacy\nmodels, and textual similarity metrics.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:50:12 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Singhofer", "Fabian", ""], ["Garifullina", "Aygul", ""], ["Kern", "Mathias", ""], ["Scherp", "Ansgar", ""]]}, {"id": "2105.08846", "submitter": "Robin Henry", "authors": "Robin Henry and Damien Ernst", "title": "Gym-ANM: Open-source software to leverage reinforcement learning for\n  power system management in research and education", "comments": "5 pages, 2 figures, 2 code samples", "journal-ref": null, "doi": "10.1016/j.simpa.2021.100092", "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gym-ANM is a Python package that facilitates the design of reinforcement\nlearning (RL) environments that model active network management (ANM) tasks in\nelectricity networks. Here, we describe how to implement new environments and\nhow to write code to interact with pre-existing ones. We also provide an\noverview of ANM6-Easy, an environment designed to highlight common ANM\nchallenges. Finally, we discuss the potential impact of Gym-ANM on the\nscientific community, both in terms of research and education. We hope this\npackage will facilitate collaboration between the power system and RL\ncommunities in the search for algorithms to control future energy systems.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 22:10:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Henry", "Robin", ""], ["Ernst", "Damien", ""]]}, {"id": "2105.08866", "submitter": "Suhas Vijaykumar", "authors": "Suhas Vijaykumar", "title": "Localization, Convexity, and Star Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offset Rademacher complexities have been shown to imply sharp, data-dependent\nupper bounds for the square loss in a broad class of problems including\nimproper statistical learning and online learning. We show that in the\nstatistical setting, the offset complexity upper bound can be generalized to\nany loss satisfying a certain uniform convexity condition. Amazingly, this\ncondition is shown to also capture exponential concavity and self-concordance,\nuniting several apparently disparate results. By a unified geometric argument,\nthese bounds translate directly to improper learning in a non-convex class\nusing Audibert's \"star algorithm.\" As applications, we recover the optimal\nrates for proper and improper learning with the $p$-loss, $1 < p < \\infty$ and\nshow that improper variants of empirical risk minimization can attain fast\nrates for logistic regression and other generalized linear models.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 00:47:59 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 15:36:56 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Vijaykumar", "Suhas", ""]]}, {"id": "2105.08869", "submitter": "Tianchen Zhou", "authors": "Tianchen Zhou, Jia Liu, Chaosheng Dong, Jingyuan Deng", "title": "Incentivized Bandit Learning with Self-Reinforcing User Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a new multi-armed bandit (MAB) online learning\nmodel that considers real-world phenomena in many recommender systems: (i) the\nlearning agent cannot pull the arms by itself and thus has to offer rewards to\nusers to incentivize arm-pulling indirectly; and (ii) if users with specific\narm preferences are well rewarded, they induce a \"self-reinforcing\" effect in\nthe sense that they will attract more users of similar arm preferences. Besides\naddressing the tradeoff of exploration and exploitation, another key feature of\nthis new MAB model is to balance reward and incentivizing payment. The goal of\nthe agent is to maximize the total reward over a fixed time horizon $T$ with a\nlow total payment. Our contributions in this paper are two-fold: (i) We propose\na new MAB model with random arm selection that considers the relationship of\nusers' self-reinforcing preferences and incentives; and (ii) We leverage the\nproperties of a multi-color Polya urn with nonlinear feedback model to propose\ntwo MAB policies termed \"At-Least-$n$ Explore-Then-Commit\" and \"UCB-List\". We\nprove that both policies achieve $O(log T)$ expected regret with $O(log T)$\nexpected payment over a time horizon $T$. We conduct numerical simulations to\ndemonstrate and verify the performances of these two policies and study their\nrobustness under various settings.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:06:32 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 05:44:36 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 03:15:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhou", "Tianchen", ""], ["Liu", "Jia", ""], ["Dong", "Chaosheng", ""], ["Deng", "Jingyuan", ""]]}, {"id": "2105.08875", "submitter": "Nicholas Sterge", "authors": "Nicholas Sterge, Bharath Sriperumbudur", "title": "Statistical Optimality and Computational Efficiency of Nystr\\\"om Kernel\n  PCA", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel methods provide an elegant framework for developing nonlinear learning\nalgorithms from simple linear methods. Though these methods have superior\nempirical performance in several real data applications, their usefulness is\ninhibited by the significant computational burden incurred in large sample\nsituations. Various approximation schemes have been proposed in the literature\nto alleviate these computational issues, and the approximate kernel machines\nare shown to retain the empirical performance. However, the theoretical\nproperties of these approximate kernel machines are less well understood. In\nthis work, we theoretically study the trade-off between computational\ncomplexity and statistical accuracy in Nystr\\\"om approximate kernel principal\ncomponent analysis (KPCA), wherein we show that the Nystr\\\"om approximate KPCA\nmatches the statistical performance of (non-approximate) KPCA while remaining\ncomputationally beneficial. Additionally, we show that Nystr\\\"om approximate\nKPCA outperforms the statistical behavior of another popular approximation\nscheme, the random feature approximation, when applied to KPCA.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:49:35 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Sterge", "Nicholas", ""], ["Sriperumbudur", "Bharath", ""]]}, {"id": "2105.08879", "submitter": "Brian Kenji Iwana", "authors": "Taiga Miyazono, Brian Kenji Iwana, Daichi Haraguchi, Seiichi Uchida", "title": "Font Style that Fits an Image -- Font Generation Based on Image Context", "comments": "Accepted to ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fonts are used on documents, they are intentionally selected by\ndesigners. For example, when designing a book cover, the typography of the text\nis an important factor in the overall feel of the book. In addition, it needs\nto be an appropriate font for the rest of the book cover. Thus, we propose a\nmethod of generating a book title image based on its context within a book\ncover. We propose an end-to-end neural network that inputs the book cover, a\ntarget location mask, and a desired book title and outputs stylized text\nsuitable for the cover. The proposed network uses a combination of a\nmulti-input encoder-decoder, a text skeleton prediction network, a perception\nnetwork, and an adversarial discriminator. We demonstrate that the proposed\nmethod can effectively produce desirable and appropriate book cover text\nthrough quantitative and qualitative results.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:53:04 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Miyazono", "Taiga", ""], ["Iwana", "Brian Kenji", ""], ["Haraguchi", "Daichi", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2105.08881", "submitter": "Bingqing Chen", "authors": "Bingqing Chen, Priya Donti, Kyri Baker, J. Zico Kolter, Mario Berges", "title": "Enforcing Policy Feasibility Constraints through Differentiable\n  Projection for Energy Optimization", "comments": "Accepted at Twelfth ACM International Conference on Future Energy\n  Systems (ACM e-Energy)", "journal-ref": null, "doi": "10.1145/3447555.3464874", "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While reinforcement learning (RL) is gaining popularity in energy systems\ncontrol, its real-world applications are limited due to the fact that the\nactions from learned policies may not satisfy functional requirements or be\nfeasible for the underlying physical system. In this work, we propose PROjected\nFeasibility (PROF), a method to enforce convex operational constraints within\nneural policies. Specifically, we incorporate a differentiable projection layer\nwithin a neural network-based policy to enforce that all learned actions are\nfeasible. We then update the policy end-to-end by propagating gradients through\nthis differentiable projection layer, making the policy cognizant of the\noperational constraints. We demonstrate our method on two applications:\nenergy-efficient building operation and inverter control. In the building\noperation setting, we show that PROF maintains thermal comfort requirements\nwhile improving energy efficiency by 4% over state-of-the-art methods. In the\ninverter control setting, PROF perfectly satisfies voltage constraints on the\nIEEE 37-bus feeder system, as it learns to curtail as little renewable energy\nas possible within its safety set.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:58:10 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Chen", "Bingqing", ""], ["Donti", "Priya", ""], ["Baker", "Kyri", ""], ["Kolter", "J. Zico", ""], ["Berges", "Mario", ""]]}, {"id": "2105.08882", "submitter": "Enrico Santus", "authors": "Beatrice Portelli, Daniele Passab\\`i, Edoardo Lenzi, Giuseppe Serra,\n  Enrico Santus and Emmanuele Chersoni", "title": "Improving Adverse Drug Event Extraction with SpanBERT on Different Text\n  Typologies", "comments": "11 pages, AAAI, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Internet users are reporting Adverse Drug Events (ADE) on\nsocial media, blogs and health forums. Because of the large volume of reports,\npharmacovigilance is seeking to resort to NLP to monitor these outlets. We\npropose for the first time the use of the SpanBERT architecture for the task of\nADE extraction: this new version of the popular BERT transformer showed\nimproved capabilities with multi-token text spans. We validate our hypothesis\nwith experiments on two datasets (SMM4H and CADEC) with different text\ntypologies (tweets and blog posts), finding that SpanBERT combined with a CRF\noutperforms all the competitors on both of them.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 02:01:09 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Portelli", "Beatrice", ""], ["Passab\u00ec", "Daniele", ""], ["Lenzi", "Edoardo", ""], ["Serra", "Giuseppe", ""], ["Santus", "Enrico", ""], ["Chersoni", "Emmanuele", ""]]}, {"id": "2105.08907", "submitter": "Chrisogonas Odhiambo Mr.", "authors": "Chrisogonas Odhiambo (1 and 3), Pamela Wright (2 and 3), Cindy Corbett\n  (2 and 3), Homayoun Valafar (1 and 3) ((1) Computer Science and Engineering\n  Department, (2) College of Nursing, (3) University of South Carolina)", "title": "MedSensor: Medication Adherence Monitoring Using Neural Networks on\n  Smartwatch Accelerometer Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Poor medication adherence presents serious economic and health problems\nincluding compromised treatment effectiveness, medical complications, and loss\nof billions of dollars in wasted medicine or procedures. Though various\ninterventions have been proposed to address this problem, there is an urgent\nneed to leverage light, smart, and minimally obtrusive technology such as\nsmartwatches to develop user tools to improve medication use and adherence. In\nthis study, we conducted several experiments on medication-taking activities,\ndeveloped a smartwatch android application to collect the accelerometer hand\ngesture data from the smartwatch, and conveyed the data collected to a central\ncloud database. We developed neural networks, then trained the networks on the\nsensor data to recognize medication and non-medication gestures. With the\nproposed machine learning algorithm approach, this study was able to achieve\naverage accuracy scores of 97% on the protocol-guided gesture data, and 95% on\nnatural gesture data.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 03:42:30 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Odhiambo", "Chrisogonas", "", "1 and 3"], ["Wright", "Pamela", "", "2 and 3"], ["Corbett", "Cindy", "", "2 and 3"], ["Valafar", "Homayoun", "", "1 and 3"]]}, {"id": "2105.08908", "submitter": "Sixiao Zhang", "authors": "Sixiao Zhang, Hongxu Chen, Xiao Ming, Lizhen Cui, Hongzhi Yin,\n  Guandong Xu", "title": "Where are we in embedding spaces? A Comprehensive Analysis on Network\n  Embedding Approaches for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperbolic space and hyperbolic embeddings are becoming a popular research\nfield for recommender systems. However, it is not clear under what\ncircumstances the hyperbolic space should be considered. To fill this gap, This\npaper provides theoretical analysis and empirical results on when and where to\nuse hyperbolic space and hyperbolic embeddings in recommender systems.\nSpecifically, we answer the questions that which type of models and datasets\nare more suited for hyperbolic space, as well as which latent size to choose.\nWe evaluate our answers by comparing the performance of Euclidean space and\nhyperbolic space on different latent space models in both general item\nrecommendation domain and social recommendation domain, with 6 widely used\ndatasets and different latent sizes. Additionally, we propose a new metric\nlearning based recommendation method called SCML and its hyperbolic version\nHSCML. We evaluate our conclusions regarding hyperbolic space on SCML and show\nthe state-of-the-art performance of hyperbolic space by comparing HSCML with\nother baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 03:46:41 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhang", "Sixiao", ""], ["Chen", "Hongxu", ""], ["Ming", "Xiao", ""], ["Cui", "Lizhen", ""], ["Yin", "Hongzhi", ""], ["Xu", "Guandong", ""]]}, {"id": "2105.08909", "submitter": "Wentao Ouyang", "authors": "Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Li Li, Kun Zhang, Jinmei Luo,\n  Zhaojie Liu, Yanlong Du", "title": "Learning Graph Meta Embeddings for Cold-Start Ads in Click-Through Rate\n  Prediction", "comments": "SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462879", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Click-through rate (CTR) prediction is one of the most central tasks in\nonline advertising systems. Recent deep learning-based models that exploit\nfeature embedding and high-order data nonlinearity have shown dramatic\nsuccesses in CTR prediction. However, these models work poorly on cold-start\nads with new IDs, whose embeddings are not well learned yet. In this paper, we\npropose Graph Meta Embedding (GME) models that can rapidly learn how to\ngenerate desirable initial embeddings for new ad IDs based on graph neural\nnetworks and meta learning. Previous works address this problem from the new ad\nitself, but ignore possibly useful information contained in existing old ads.\nIn contrast, GMEs simultaneously consider two information sources: the new ad\nand existing old ads. For the new ad, GMEs exploit its associated attributes.\nFor existing old ads, GMEs first build a graph to connect them with new ads,\nand then adaptively distill useful information. We propose three specific GMEs\nfrom different perspectives to explore what kind of information to use and how\nto distill information. In particular, GME-P uses Pre-trained neighbor ID\nembeddings, GME-G uses Generated neighbor ID embeddings and GME-A uses neighbor\nAttributes. Experimental results on three real-world datasets show that GMEs\ncan significantly improve the prediction performance in both cold-start (i.e.,\nno training data is available) and warm-up (i.e., a small number of training\nsamples are collected) scenarios over five major deep learning-based CTR\nprediction models. GMEs can be applied to conversion rate (CVR) prediction as\nwell.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 03:46:56 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ouyang", "Wentao", ""], ["Zhang", "Xiuwu", ""], ["Ren", "Shukui", ""], ["Li", "Li", ""], ["Zhang", "Kun", ""], ["Luo", "Jinmei", ""], ["Liu", "Zhaojie", ""], ["Du", "Yanlong", ""]]}, {"id": "2105.08911", "submitter": "Yueyao Yu", "authors": "Yin Zhang and Yueyao Yu", "title": "Variability of Artificial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes an artificial neural network easier to train and more likely to\nproduce desirable solutions than other comparable networks? In this paper, we\nprovide a new angle to study such issues under the setting of a fixed number of\nmodel parameters which in general is the most dominant cost factor. We\nintroduce a notion of variability and show that it correlates positively to the\nactivation ratio and negatively to a phenomenon called {Collapse to Constants}\n(or C2C), which is closely related but not identical to the phenomenon commonly\nknown as vanishing gradient. Experiments on a styled model problem empirically\nverify that variability is indeed a key performance indicator for fully\nconnected neural networks. The insights gained from this variability study will\nhelp the design of new and effective neural network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 03:51:52 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 03:27:56 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhang", "Yin", ""], ["Yu", "Yueyao", ""]]}, {"id": "2105.08919", "submitter": "Taehyeon Kim", "authors": "Taehyeon Kim, Jaehoon Oh, NakYil Kim, Sangwook Cho, Se-Young Yun", "title": "Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in\n  Knowledge Distillation", "comments": "Proceedings of International Joint Conference on Artificial\n  Intelligence (IJCAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD), transferring knowledge from a cumbersome teacher\nmodel to a lightweight student model, has been investigated to design efficient\nneural architectures. Generally, the objective function of KD is the\nKullback-Leibler (KL) divergence loss between the softened probability\ndistributions of the teacher model and the student model with the temperature\nscaling hyperparameter tau. Despite its widespread use, few studies have\ndiscussed the influence of such softening on generalization. Here, we\ntheoretically show that the KL divergence loss focuses on the logit matching\nwhen tau increases and the label matching when tau goes to 0 and empirically\nshow that the logit matching is positively correlated to performance\nimprovement in general. From this observation, we consider an intuitive KD loss\nfunction, the mean squared error (MSE) between the logit vectors, so that the\nstudent model can directly learn the logit of the teacher model. The MSE loss\noutperforms the KL divergence loss, explained by the difference in the\npenultimate layer representations between the two losses. Furthermore, we show\nthat sequential distillation can improve performance and that KD, particularly\nwhen using the KL divergence loss with small tau, mitigates the label noise.\nThe code to reproduce the experiments is publicly available online at\nhttps://github.com/jhoon-oh/kd_data/.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 04:40:53 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Kim", "Taehyeon", ""], ["Oh", "Jaehoon", ""], ["Kim", "NakYil", ""], ["Cho", "Sangwook", ""], ["Yun", "Se-Young", ""]]}, {"id": "2105.08923", "submitter": "Wei Xie", "authors": "Hua Zheng, Jiahao Zhu, Wei Xie, Judy Zhong", "title": "Reinforcement Learning Assisted Oxygen Therapy for COVID-19 Patients\n  Under Intensive Care", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patients with severe Coronavirus disease 19 (COVID-19) typically require\nsupplemental oxygen as an essential treatment. We developed a machine learning\nalgorithm, based on a deep Reinforcement Learning (RL), for continuous\nmanagement of oxygen flow rate for critical ill patients under intensive care,\nwhich can identify the optimal personalized oxygen flow rate with strong\npotentials to reduce mortality rate relative to the current clinical practice.\nBasically, we modeled the oxygen flow trajectory of COVID-19 patients and their\nhealth outcomes as a Markov decision process. Based on individual patient\ncharacteristics and health status, a reinforcement learning based oxygen\ncontrol policy is learned and real-time recommends the oxygen flow rate to\nreduce the mortality rate. We assessed the performance of proposed methods\nthrough cross validation by using a retrospective cohort of 1,372 critically\nill patients with COVID-19 from New York University Langone Health ambulatory\ncare with electronic health records from April 2020 to January 2021. The mean\nmortality rate under the RL algorithm is lower than standard of care by 2.57%\n(95% CI: 2.08- 3.06) reduction (P<0.001) from 7.94% under the standard of care\nto 5.37 % under our algorithm and the averaged recommended oxygen flow rate is\n1.28 L/min (95% CI: 1.14-1.42) lower than the rate actually delivered to\npatients. Thus, the RL algorithm could potentially lead to better intensive\ncare treatment that can reduce mortality rate, while saving the oxygen scarce\nresources. It can reduce the oxygen shortage issue and improve public health\nduring the COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 04:49:48 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zheng", "Hua", ""], ["Zhu", "Jiahao", ""], ["Xie", "Wei", ""], ["Zhong", "Judy", ""]]}, {"id": "2105.08944", "submitter": "Jacob Russin", "authors": "Jacob Russin, Maryam Zolfaghar, Seongmin A. Park, Erie Boorman,\n  Randall C. O'Reilly", "title": "Complementary Structure-Learning Neural Networks for Relational\n  Reasoning", "comments": "7 pages, 4 figures, Accepted to CogSci 2021 for poster presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The neural mechanisms supporting flexible relational inferences, especially\nin novel situations, are a major focus of current research. In the\ncomplementary learning systems framework, pattern separation in the hippocampus\nallows rapid learning in novel environments, while slower learning in neocortex\naccumulates small weight changes to extract systematic structure from\nwell-learned environments. In this work, we adapt this framework to a task from\na recent fMRI experiment where novel transitive inferences must be made\naccording to implicit relational structure. We show that computational models\ncapturing the basic cognitive properties of these two systems can explain\nrelational transitive inferences in both familiar and novel environments, and\nreproduce key phenomena observed in the fMRI experiment.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:25:21 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Russin", "Jacob", ""], ["Zolfaghar", "Maryam", ""], ["Park", "Seongmin A.", ""], ["Boorman", "Erie", ""], ["O'Reilly", "Randall C.", ""]]}, {"id": "2105.08952", "submitter": "Haoping Bai", "authors": "Haoping Bai, Meng Cao, Ping Huang, Jiulong Shan", "title": "BatchQuant: Quantized-for-all Architecture Search with Robust Quantizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the applications of deep learning models on edge devices increase at an\naccelerating pace, fast adaptation to various scenarios with varying resource\nconstraints has become a crucial aspect of model deployment. As a result, model\noptimization strategies with adaptive configuration are becoming increasingly\npopular. While single-shot quantized neural architecture search enjoys\nflexibility in both model architecture and quantization policy, the combined\nsearch space comes with many challenges, including instability when training\nthe weight-sharing supernet and difficulty in navigating the exponentially\ngrowing search space. Existing methods tend to either limit the architecture\nsearch space to a small set of options or limit the quantization policy search\nspace to fixed precision policies. To this end, we propose BatchQuant, a robust\nquantizer formulation that allows fast and stable training of a compact,\nsingle-shot, mixed-precision, weight-sharing supernet. We employ BatchQuant to\ntrain a compact supernet (offering over $10^{76}$ quantized subnets) within\nsubstantially fewer GPU hours than previous methods. Our approach,\nQuantized-for-all (QFA), is the first to seamlessly extend one-shot\nweight-sharing NAS supernet to support subnets with arbitrary ultra-low\nbitwidth mixed-precision quantization policies without retraining. QFA opens up\nnew possibilities in joint hardware-aware neural architecture search and\nquantization. We demonstrate the effectiveness of our method on ImageNet and\nachieve SOTA Top-1 accuracy under a low complexity constraint ($<20$ MFLOPs).\nThe code and models will be made publicly available at\nhttps://github.com/bhpfelix/QFA.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:56:43 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Bai", "Haoping", ""], ["Cao", "Meng", ""], ["Huang", "Ping", ""], ["Shan", "Jiulong", ""]]}, {"id": "2105.08960", "submitter": "Florian Rehm", "authors": "Florian Rehm, Sofia Vallecorsa, Kerstin Borras, Dirk Kr\\\"ucker", "title": "Physics Validation of Novel Convolutional 2D Architectures for Speeding\n  Up High Energy Physics Simulations", "comments": "Paper published at vCHEP2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The precise simulation of particle transport through detectors remains a key\nelement for the successful interpretation of high energy physics results.\nHowever, Monte Carlo based simulation is extremely demanding in terms of\ncomputing resources. This challenge motivates investigations of faster,\nalternative approaches for replacing the standard Monte Carlo approach.\n  We apply Generative Adversarial Networks (GANs), a deep learning technique,\nto replace the calorimeter detector simulations and speeding up the simulation\ntime by orders of magnitude. We follow a previous approach which used\nthree-dimensional convolutional neural networks and develop new two-dimensional\nconvolutional networks to solve the same 3D image generation problem faster.\nAdditionally, we increased the number of parameters and the neural networks\nrepresentational power, obtaining a higher accuracy. We compare our best\nconvolutional 2D neural network architecture and evaluate it versus the\nprevious 3D architecture and Geant4 data. Our results demonstrate a high\nphysics accuracy and further consolidate the use of GANs for fast detector\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:24:23 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Rehm", "Florian", ""], ["Vallecorsa", "Sofia", ""], ["Borras", "Kerstin", ""], ["Kr\u00fccker", "Dirk", ""]]}, {"id": "2105.08961", "submitter": "Jacob Russin", "authors": "Jacob Russin, Roland Fernandez, Hamid Palangi, Eric Rosen, Nebojsa\n  Jojic, Paul Smolensky, Jianfeng Gao", "title": "Compositional Processing Emerges in Neural Networks Solving Math\n  Problems", "comments": "7 pages, 2 figures, Accepted to CogSci 2021 for poster presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A longstanding question in cognitive science concerns the learning mechanisms\nunderlying compositionality in human cognition. Humans can infer the structured\nrelationships (e.g., grammatical rules) implicit in their sensory observations\n(e.g., auditory speech), and use this knowledge to guide the composition of\nsimpler meanings into complex wholes. Recent progress in artificial neural\nnetworks has shown that when large models are trained on enough linguistic\ndata, grammatical structure emerges in their representations. We extend this\nwork to the domain of mathematical reasoning, where it is possible to formulate\nprecise hypotheses about how meanings (e.g., the quantities corresponding to\nnumerals) should be composed according to structured rules (e.g., order of\noperations). Our work shows that neural networks are not only able to infer\nsomething about the structured relationships implicit in their training data,\nbut can also deploy this knowledge to guide the composition of individual\nmeanings into composite wholes.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:24:42 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Russin", "Jacob", ""], ["Fernandez", "Roland", ""], ["Palangi", "Hamid", ""], ["Rosen", "Eric", ""], ["Jojic", "Nebojsa", ""], ["Smolensky", "Paul", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2105.08966", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist", "title": "Latent Gaussian Model Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Gaussian models and boosting are widely used techniques in statistics\nand machine learning. Tree-boosting shows excellent predictive accuracy on many\ndata sets, but potential drawbacks are that it assumes conditional independence\nof samples, produces discontinuous predictions for, e.g., spatial data, and it\ncan have difficulty with high-cardinality categorical variables. Latent\nGaussian models, such as Gaussian process and grouped random effects models,\nare flexible prior models that allow for making probabilistic predictions.\nHowever, existing latent Gaussian models usually assume either a zero or a\nlinear prior mean function which can be an unrealistic assumption. This article\nintroduces a novel approach that combines boosting and latent Gaussian models\nin order to remedy the above-mentioned drawbacks and to leverage the advantages\nof both techniques. We obtain increased predictive accuracy compared to\nexisting approaches in both simulated and real-world data experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:36:30 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 13:42:12 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sigrist", "Fabio", ""]]}, {"id": "2105.08969", "submitter": "Wei Shao Dr", "authors": "Wei Shao, Arian Prabowo, Sichen Zhao, Piotr Koniusz, Flora D. Salim", "title": "Predicting Flight Delay with Spatio-Temporal Trajectory Convolutional\n  Network and Airport Situational Awareness Map", "comments": "single column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To model and forecast flight delays accurately, it is crucial to harness\nvarious vehicle trajectory and contextual sensor data on airport tarmac areas.\nThese heterogeneous sensor data, if modelled correctly, can be used to generate\na situational awareness map. Existing techniques apply traditional supervised\nlearning methods onto historical data, contextual features and route\ninformation among different airports to predict flight delay are inaccurate and\nonly predict arrival delay but not departure delay, which is essential to\nairlines. In this paper, we propose a vision-based solution to achieve a high\nforecasting accuracy, applicable to the airport. Our solution leverages a\nsnapshot of the airport situational awareness map, which contains various\ntrajectories of aircraft and contextual features such as weather and airline\nschedules. We propose an end-to-end deep learning architecture, TrajCNN, which\ncaptures both the spatial and temporal information from the situational\nawareness map. Additionally, we reveal that the situational awareness map of\nthe airport has a vital impact on estimating flight departure delay. Our\nproposed framework obtained a good result (around 18 minutes error) for\npredicting flight departure delay at Los Angeles International Airport.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:38:57 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Shao", "Wei", ""], ["Prabowo", "Arian", ""], ["Zhao", "Sichen", ""], ["Koniusz", "Piotr", ""], ["Salim", "Flora D.", ""]]}, {"id": "2105.08970", "submitter": "Guillaume Carbajal", "authors": "Guillaume Carbajal, Julius Richter, Timo Gerkmann", "title": "Disentanglement Learning for Variational Autoencoders Applied to\n  Audio-Visual Speech Enhancement", "comments": "arXiv admin note: text overlap with arXiv:2102.06454", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the standard variational autoencoder has been successfully used to\nlearn a probabilistic prior over speech signals, which is then used to perform\nspeech enhancement. Variational autoencoders have then been conditioned on a\nlabel describing a high-level speech attribute (e.g. speech activity) that\nallows for a more explicit control of speech generation. However, the label is\nnot guaranteed to be disentangled from the other latent variables, which\nresults in limited performance improvements compared to the standard\nvariational autoencoder. In this work, we propose to use an adversarial\ntraining scheme for variational autoencoders to disentangle the label from the\nother latent variables. At training, we use a discriminator that competes with\nthe encoder of the variational autoencoder. Simultaneously, we also use an\nadditional encoder that estimates the label for the decoder of the variational\nautoencoder, which proves to be crucial to learn disentanglement. We show the\nbenefit of the proposed disentanglement learning when a voice activity label,\nestimated from visual data, is used for speech enhancement.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:42:14 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Carbajal", "Guillaume", ""], ["Richter", "Julius", ""], ["Gerkmann", "Timo", ""]]}, {"id": "2105.08982", "submitter": "Mete Ozay", "authors": "Umberto Michieli and Mete Ozay", "title": "Prototype Guided Federated Learning of Visual Feature Representations", "comments": "11 pages manuscript, 6 pages supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated Learning (FL) is a framework which enables distributed model\ntraining using a large corpus of decentralized training data. Existing methods\naggregate models disregarding their internal representations, which are crucial\nfor training models in vision tasks. System and statistical heterogeneity\n(e.g., highly imbalanced and non-i.i.d. data) further harm model training. To\nthis end, we introduce a method, called FedProto, which computes client\ndeviations using margins of prototypical representations learned on distributed\ndata, and applies them to drive federated optimization via an attention\nmechanism. In addition, we propose three methods to analyse statistical\nproperties of feature representations learned in FL, in order to elucidate the\nrelationship between accuracy, margins and feature discrepancy of FL models. In\nexperimental analyses, FedProto demonstrates state-of-the-art accuracy and\nconvergence rate across image classification and semantic segmentation\nbenchmarks by enabling maximum margin training of FL models. Moreover, FedProto\nreduces uncertainty of predictions of FL models compared to the baseline. To\nour knowledge, this is the first work evaluating FL models in dense prediction\ntasks, such as semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 08:29:12 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Michieli", "Umberto", ""], ["Ozay", "Mete", ""]]}, {"id": "2105.08990", "submitter": "Maximilian Schenke", "authors": "Maximilian Schenke and Oliver Wallscheid", "title": "Improved Exploring Starts by Kernel Density Estimation-Based State-Space\n  Coverage Acceleration in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) is currently a popular research topic in control\nengineering and has the potential to make its way to industrial and commercial\napplications. Corresponding RL controllers are trained in direct interaction\nwith the controlled system, rendering them data-driven and performance-oriented\nsolutions. The best practice of exploring starts (ES) is used by default to\nsupport the learning process via randomly picked initial states. However, this\nmethod might deliver strongly biased results if the system's dynamic and\nconstraints lead to unfavorable sample distributions in the state space (e.g.,\ncondensed sample accumulation in certain state-space areas). To overcome this\nissue, a kernel density estimation-based state-space coverage acceleration\n(DESSCA) is proposed, which improves the ES concept by prioritizing\ninfrequently visited states for a more balanced coverage of the state space\nduring training. Considered test scenarios are mountain car, cartpole and\nelectric motor control environments. Using DQN and DDPG as exemplary RL\nalgorithms, it can be shown that DESSCA is a simple yet effective algorithmic\nextension to the established ES approach.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 08:36:26 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Schenke", "Maximilian", ""], ["Wallscheid", "Oliver", ""]]}, {"id": "2105.08997", "submitter": "Iuliia Pliushch", "authors": "Iuliia Pliushch, Martin Mundt, Nicolas Lupp, Visvanathan Ramesh", "title": "When Deep Classifiers Agree: Analyzing Correlations between Learning\n  Order and Image Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a plethora of architectural variants for deep classification has\nbeen introduced over time, recent works have found empirical evidence towards\nsimilarities in their training process. It has been hypothesized that neural\nnetworks converge not only to similar representations, but also exhibit a\nnotion of empirical agreement on which data instances are learned first.\nFollowing in the latter works$'$ footsteps, we define a metric to quantify the\nrelationship between such classification agreement over time, and posit that\nthe agreement phenomenon can be mapped to core statistics of the investigated\ndataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal,\nImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to\nbe independent of specific architectures, training hyper-parameters or labels,\nalbeit follows an ordering according to image statistics.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:03:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Pliushch", "Iuliia", ""], ["Mundt", "Martin", ""], ["Lupp", "Nicolas", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "2105.09016", "submitter": "Emiel Hoogeboom", "authors": "Victor Garcia Satorras, Emiel Hoogeboom, Fabian B. Fuchs, Ingmar\n  Posner, Max Welling", "title": "E(n) Equivariant Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a generative model equivariant to Euclidean symmetries:\nE(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the\ndiscriminative E(n) graph neural networks and integrate them as a differential\nequation to obtain an invertible equivariant function: a continuous-time\nnormalizing flow. We demonstrate that E-NFs considerably outperform baselines\nand existing methods from the literature on particle systems such as DW4 and\nLJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our\nknowledge, this is the first flow that jointly generates molecule features and\npositions in 3D.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:28:54 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 12:17:00 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Satorras", "Victor Garcia", ""], ["Hoogeboom", "Emiel", ""], ["Fuchs", "Fabian B.", ""], ["Posner", "Ingmar", ""], ["Welling", "Max", ""]]}, {"id": "2105.09046", "submitter": "Vaishali Ingale", "authors": "Vaishali Ingale, Anush Mohan, Divit Adlakha, Krishan Kumar and Mohit\n  Gupta", "title": "Music Generation using Three-layered LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper explores the idea of utilising Long Short-Term Memory neural\nnetworks (LSTMNN) for the generation of musical sequences in ABC notation. The\nproposed approach takes ABC notations from the Nottingham dataset and encodes\nit to be fed as input for the neural networks. The primary objective is to\ninput the neural networks with an arbitrary note, let the network process and\naugment a sequence based on the note until a good piece of music is produced.\nMultiple calibrations have been done to amend the parameters of the network for\noptimal generation. The output is assessed on the basis of rhythm, harmony, and\ngrammar accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:27:58 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 16:09:44 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 08:15:19 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ingale", "Vaishali", ""], ["Mohan", "Anush", ""], ["Adlakha", "Divit", ""], ["Kumar", "Krishan", ""], ["Gupta", "Mohit", ""]]}, {"id": "2105.09047", "submitter": "Pantea Haghighatkhah", "authors": "Pantea Haghighatkhah, Wouter Meulemans, Bettina Speckman, J\\'er\\^ome\n  Urhausen, Kevin Verbeek", "title": "Obstructing Classification via Projection", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning and data mining techniques are effective tools to classify\nlarge amounts of data. But they tend to preserve any inherent bias in the data,\nfor example, with regards to gender or race. Removing such bias from data or\nthe learned representations is quite challenging. In this paper we study a\ngeometric problem which models a possible approach for bias removal. Our input\nis a set of points P in Euclidean space R^d and each point is labeled with k\nbinary-valued properties. A priori we assume that it is \"easy\" to classify the\ndata according to each property. Our goal is to obstruct the classification\naccording to one property by a suitable projection to a lower-dimensional\nEuclidean space R^m (m < d), while classification according to all other\nproperties remains easy.\n  What it means for classification to be easy depends on the classification\nmodel used. We first consider classification by linear separability as employed\nby support vector machines. We use Kirchberger's Theorem to show that, under\ncertain conditions, a simple projection to R^(d-1) suffices to eliminate the\nlinear separability of one of the properties whilst maintaining the linear\nseparability of the other properties. We also study the problem of maximizing\nthe linear \"inseparability\" of the chosen property. Second, we consider more\ncomplex forms of separability and prove a connection between the number of\nprojections required to obstruct classification and the Helly-type properties\nof such separabilities.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:28:15 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Haghighatkhah", "Pantea", ""], ["Meulemans", "Wouter", ""], ["Speckman", "Bettina", ""], ["Urhausen", "J\u00e9r\u00f4me", ""], ["Verbeek", "Kevin", ""]]}, {"id": "2105.09052", "submitter": "Daryna Dementieva", "authors": "Daryna Dementieva, Daniil Moskovskiy, Varvara Logacheva, David Dale,\n  Olga Kozlova, Nikita Semenov, and Alexander Panchenko", "title": "Methods for Detoxification of Texts for the Russian Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the first study of automatic detoxification of Russian texts to\ncombat offensive language. Such a kind of textual style transfer can be used,\nfor instance, for processing toxic content in social media. While much work has\nbeen done for the English language in this field, it has never been solved for\nthe Russian language yet. We test two types of models - unsupervised approach\nbased on BERT architecture that performs local corrections and supervised\napproach based on pretrained language GPT-2 model - and compare them with\nseveral baselines. In addition, we describe evaluation setup providing training\ndatasets and metrics for automatic evaluation. The results show that the tested\napproaches can be successfully used for detoxification, although there is room\nfor improvement.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:37:44 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dementieva", "Daryna", ""], ["Moskovskiy", "Daniil", ""], ["Logacheva", "Varvara", ""], ["Dale", "David", ""], ["Kozlova", "Olga", ""], ["Semenov", "Nikita", ""], ["Panchenko", "Alexander", ""]]}, {"id": "2105.09080", "submitter": "Yiming Chen", "authors": "Yiming Chen, Kun Yuan, Yingya Zhang, Pan Pan, Yinghui Xu, Wotao Yin", "title": "Accelerating Gossip SGD with Periodic Global Averaging", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication overhead hinders the scalability of large-scale distributed\ntraining. Gossip SGD, where each node averages only with its neighbors, is more\ncommunication-efficient than the prevalent parallel SGD. However, its\nconvergence rate is reversely proportional to quantity $1-\\beta$ which measures\nthe network connectivity. On large and sparse networks where $1-\\beta \\to 0$,\nGossip SGD requires more iterations to converge, which offsets against its\ncommunication benefit. This paper introduces Gossip-PGA, which adds Periodic\nGlobal Averaging into Gossip SGD. Its transient stage, i.e., the iterations\nrequired to reach asymptotic linear speedup stage, improves from\n$\\Omega(\\beta^4 n^3/(1-\\beta)^4)$ to $\\Omega(\\beta^4 n^3 H^4)$ for non-convex\nproblems. The influence of network topology in Gossip-PGA can be controlled by\nthe averaging period $H$. Its transient-stage complexity is also superior to\nLocal SGD which has order $\\Omega(n^3 H^4)$. Empirical results of large-scale\ntraining on image classification (ResNet50) and language modeling (BERT)\nvalidate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:59:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Chen", "Yiming", ""], ["Yuan", "Kun", ""], ["Zhang", "Yingya", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""], ["Yin", "Wotao", ""]]}, {"id": "2105.09095", "submitter": "J\\\"org Martin", "authors": "J\\\"org Martin and Clemens Elster", "title": "Errors-in-Variables for deep learning: rethinking aleatoric uncertainty", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a Bayesian treatment for deep regression using an\nErrors-in-Variables model which accounts for the uncertainty associated with\nthe input to the employed neural network. It is shown how the treatment can be\ncombined with already existing approaches for uncertainty quantification that\nare based on variational inference. Our approach yields a decomposition of the\npredictive uncertainty into an aleatoric and epistemic part that is more\ncomplete and, in many cases, more consistent from a statistical perspective. We\nillustrate and discuss the approach along various toy and real world examples.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 12:37:02 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 08:51:07 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Martin", "J\u00f6rg", ""], ["Elster", "Clemens", ""]]}, {"id": "2105.09107", "submitter": "Tomas Pevny", "authors": "Simon Mandlik, Matej Racinsky, Viliam Lisy, Tomas Pevny", "title": "Mill.jl and JsonGrinder.jl: automated differentiable feature extraction\n  for learning from raw JSON data", "comments": "5 pages, 2 figures, 1 table, submitted to section on one-source\n  software of Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning from raw data input, thus limiting the need for manual feature\nengineering, is one of the key components of many successful applications of\nmachine learning methods. While machine learning problems are often formulated\non data that naturally translate into a vector representation suitable for\nclassifiers, there are data sources, for example in cybersecurity, that are\nnaturally represented in diverse files with a unifying hierarchical structure,\nsuch as XML, JSON, and Protocol Buffers. Converting this data to vector\n(tensor) representation is generally done by manual feature engineering, which\nis laborious, lossy, and prone to human bias about the importance of particular\nfeatures.\n  Mill and JsonGrinder is a tandem of libraries, which fully automates the\nconversion. Starting with an arbitrary set of JSON samples, they create a\ndifferentiable machine learning model capable of infer from further JSON\nsamples in their raw form.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:02:10 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Mandlik", "Simon", ""], ["Racinsky", "Matej", ""], ["Lisy", "Viliam", ""], ["Pevny", "Tomas", ""]]}, {"id": "2105.09108", "submitter": "Benjie Wang", "authors": "Benjie Wang, Clare Lyle, Marta Kwiatkowska", "title": "Provable Guarantees on the Robustness of Decision Rules to Causal\n  Interventions", "comments": "21 pages (8+13 Appendix). To be published in IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of decision rules to shifts in the data-generating process is\ncrucial to the successful deployment of decision-making systems. Such shifts\ncan be viewed as interventions on a causal graph, which capture (possibly\nhypothetical) changes in the data-generating process, whether due to natural\nreasons or by the action of an adversary. We consider causal Bayesian networks\nand formally define the interventional robustness problem, a novel model-based\nnotion of robustness for decision functions that measures worst-case\nperformance with respect to a set of interventions that denote changes to\nparameters and/or causal influences. By relying on a tractable representation\nof Bayesian networks as arithmetic circuits, we provide efficient algorithms\nfor computing guaranteed upper and lower bounds on the interventional\nrobustness probabilities. Experimental results demonstrate that the methods\nyield useful and interpretable bounds for a range of practical networks, paving\nthe way towards provably causally robust decision-making systems.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:09:47 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Wang", "Benjie", ""], ["Lyle", "Clare", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "2105.09111", "submitter": "Nian Liu", "authors": "Xiao Wang, Nian Liu, Hui Han, Chuan Shi", "title": "Self-supervised Heterogeneous Graph Neural Network with Co-contrastive\n  Learning", "comments": "This paper has been accepted by KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous graph neural networks (HGNNs) as an emerging technique have\nshown superior capacity of dealing with heterogeneous information network\n(HIN). However, most HGNNs follow a semi-supervised learning manner, which\nnotably limits their wide use in reality since labels are usually scarce in\nreal applications. Recently, contrastive learning, a self-supervised method,\nbecomes one of the most exciting learning paradigms and shows great potential\nwhen there are no labels. In this paper, we study the problem of\nself-supervised HGNNs and propose a novel co-contrastive learning mechanism for\nHGNNs, named HeCo. Different from traditional contrastive learning which only\nfocuses on contrasting positive and negative samples, HeCo employs\ncross-viewcontrastive mechanism. Specifically, two views of a HIN (network\nschema and meta-path views) are proposed to learn node embeddings, so as to\ncapture both of local and high-order structures simultaneously. Then the\ncross-view contrastive learning, as well as a view mask mechanism, is proposed,\nwhich is able to extract the positive and negative embeddings from two views.\nThis enables the two views to collaboratively supervise each other and finally\nlearn high-level node embeddings. Moreover, two extensions of HeCo are designed\nto generate harder negative samples with high quality, which further boosts the\nperformance of HeCo. Extensive experiments conducted on a variety of real-world\nnetworks show the superior performance of the proposed methods over the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:15:03 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Wang", "Xiao", ""], ["Liu", "Nian", ""], ["Han", "Hui", ""], ["Shi", "Chuan", ""]]}, {"id": "2105.09114", "submitter": "Bimal Bhattarai", "authors": "Bimal Bhattarai, Ole-Christoffer Granmo, Lei Jiao", "title": "Explainable Tsetlin Machine framework for fake news detection with\n  credibility score assessment", "comments": "11 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of fake news, i.e., news intentionally spread for\nmisinformation, poses a threat to individuals and society. Despite various\nfact-checking websites such as PolitiFact, robust detection techniques are\nrequired to deal with the increase in fake news. Several deep learning models\nshow promising results for fake news classification, however, their black-box\nnature makes it difficult to explain their classification decisions and\nquality-assure the models. We here address this problem by proposing a novel\ninterpretable fake news detection framework based on the recently introduced\nTsetlin Machine (TM). In brief, we utilize the conjunctive clauses of the TM to\ncapture lexical and semantic properties of both true and fake news text.\nFurther, we use the clause ensembles to calculate the credibility of fake news.\nFor evaluation, we conduct experiments on two publicly available datasets,\nPolitiFact and GossipCop, and demonstrate that the TM framework significantly\noutperforms previously published baselines by at least $5\\%$ in terms of\naccuracy, with the added benefit of an interpretable logic-based\nrepresentation. Further, our approach provides higher F1-score than BERT and\nXLNet, however, we obtain slightly lower accuracy. We finally present a case\nstudy on our model's explainability, demonstrating how it decomposes into\nmeaningful words and their negations.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:18:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Bhattarai", "Bimal", ""], ["Granmo", "Ole-Christoffer", ""], ["Jiao", "Lei", ""]]}, {"id": "2105.09121", "submitter": "Arian Bakhtiarnia", "authors": "Arian Bakhtiarnia, Qi Zhang and Alexandros Iosifidis", "title": "Single-Layer Vision Transformers for More Accurate Early Exits with Less\n  Overhead", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:30:34 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Bakhtiarnia", "Arian", ""], ["Zhang", "Qi", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2105.09124", "submitter": "Juzheng Miao", "authors": "Guang-Quan Zhou, Juzheng Miao, Xin Yang, Rui Li, En-Ze Huo, Wenlong\n  Shi, Yuhao Huang, Jikuan Qian, Chaoyu Chen, Dong Ni", "title": "Learn Fine-grained Adaptive Loss for Multiple Anatomical Landmark\n  Detection in Medical Images", "comments": "12 pages, 10 figures, accepted by IEEE Journal of Biomedical and\n  Health Informatics", "journal-ref": null, "doi": "10.1109/JBHI.2021.3080703", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic and accurate detection of anatomical landmarks is an essential\noperation in medical image analysis with a multitude of applications. Recent\ndeep learning methods have improved results by directly encoding the appearance\nof the captured anatomy with the likelihood maps (i.e., heatmaps). However,\nmost current solutions overlook another essence of heatmap regression, the\nobjective metric for regressing target heatmaps and rely on hand-crafted\nheuristics to set the target precision, thus being usually cumbersome and\ntask-specific. In this paper, we propose a novel learning-to-learn framework\nfor landmark detection to optimize the neural network and the target precision\nsimultaneously. The pivot of this work is to leverage the reinforcement\nlearning (RL) framework to search objective metrics for regressing multiple\nheatmaps dynamically during the training process, thus avoiding setting\nproblem-specific target precision. We also introduce an early-stop strategy for\nactive termination of the RL agent's interaction that adapts the optimal\nprecision for separate targets considering exploration-exploitation tradeoffs.\nThis approach shows better stability in training and improved localization\naccuracy in inference. Extensive experimental results on two different\napplications of landmark localization: 1) our in-house prenatal ultrasound (US)\ndataset and 2) the publicly available dataset of cephalometric X-Ray landmark\ndetection, demonstrate the effectiveness of our proposed method. Our proposed\nframework is general and shows the potential to improve the efficiency of\nanatomical landmark detection.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:39:18 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhou", "Guang-Quan", ""], ["Miao", "Juzheng", ""], ["Yang", "Xin", ""], ["Li", "Rui", ""], ["Huo", "En-Ze", ""], ["Shi", "Wenlong", ""], ["Huang", "Yuhao", ""], ["Qian", "Jikuan", ""], ["Chen", "Chaoyu", ""], ["Ni", "Dong", ""]]}, {"id": "2105.09128", "submitter": "Feras Almasri", "authors": "Feras Almasri, Jurgen Vandendriessche, Laurent Segers, Bruno da Silva,\n  An Braeken, Kris Steenhaut, Abdellah Touhafi and Olivier Debeir", "title": "XCycles Backprojection Acoustic Super-Resolution", "comments": null, "journal-ref": "Sensors 2021, 21, 3453", "doi": "10.3390/s21103453", "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer vision community has paid much attention to the development of\nvisible image super-resolution (SR) using deep neural networks (DNNs) and has\nachieved impressive results. The advancement of non-visible light sensors, such\nas acoustic imaging sensors, has attracted much attention, as they allow people\nto visualize the intensity of sound waves beyond the visible spectrum. However,\nbecause of the limitations imposed on acquiring acoustic data, new methods for\nimproving the resolution of the acoustic images are necessary. At this time,\nthere is no acoustic imaging dataset designed for the SR problem. This work\nproposed a novel backprojection model architecture for the acoustic image\nsuper-resolution problem, together with Acoustic Map Imaging VUB-ULB Dataset\n(AMIVU). The dataset provides large simulated and real captured images at\ndifferent resolutions. The proposed XCycles BackProjection model (XCBP), in\ncontrast to the feedforward model approach, fully uses the iterative correction\nprocedure in each cycle to reconstruct the residual error correction for the\nencoded features in both low- and high-resolution space. The proposed approach\nwas evaluated on the dataset and showed high outperformance compared to the\nclassical interpolation operators and to the recent feedforward\nstate-of-the-art models. It also contributed to a drastically reduced\nsub-sampling error produced during the data acquisition.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:43:15 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Almasri", "Feras", ""], ["Vandendriessche", "Jurgen", ""], ["Segers", "Laurent", ""], ["da Silva", "Bruno", ""], ["Braeken", "An", ""], ["Steenhaut", "Kris", ""], ["Touhafi", "Abdellah", ""], ["Debeir", "Olivier", ""]]}, {"id": "2105.09136", "submitter": "Greta Laage", "authors": "Greta Laage and Emma Frejinger and Gilles Savard", "title": "Periodic Freight Demand Forecasting for Large-scale Tactical Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crucial to freight carriers is the tactical planning of the service network.\nThe aim is to obtain a cyclic plan over a given tactical planning horizon that\nsatisfies predicted demand at a minimum cost. A central input to the planning\nprocess is the periodic demand, that is, the demand expected to repeat in every\nperiod in the planning horizon. We focus on large-scale tactical planning\nproblems that require deterministic models for computational tractability. The\nproblem of estimating periodic demand in this setting broadly present in\npractice has hitherto been overlooked in the literature. We address this gap by\nformally introducing the periodic demand estimation problem and propose a\ntwo-step methodology: Based on time series forecasts obtained in the first\nstep, we propose, in the second step, to solve a multilevel mathematical\nprogramming formulation whose solution is a periodic demand estimate that\nminimizes fixed costs, and variable costs incurred by adapting the tactical\nplan at an operational level. We report results in an extensive empirical study\nof a real large-scale application from the Canadian National Railway Company.\nWe compare our periodic demand estimates to the approach commonly used in\npractice which simply consists in using the mean of the time series forecasts.\nThe results clearly show the importance of the periodic demand estimation\nproblem. Indeed, the planning costs exhibit an important variation over\ndifferent periodic demand estimates, and using an estimate different from the\nmean forecast can lead to substantial cost reductions. For example, the costs\nassociated with the period demand estimates based on forecasts were comparable\nto, or even better than those obtained using the mean of actual demand.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:55:24 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Laage", "Greta", ""], ["Frejinger", "Emma", ""], ["Savard", "Gilles", ""]]}, {"id": "2105.09146", "submitter": "Mulugeta Haile", "authors": "Gregory Barber, Mulugeta A. Haile, Tzikang Chen", "title": "Physical Constraint Embedded Neural Networks for inference and noise\n  regulation", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.app-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks often require large amounts of data to generalize and can be\nill-suited for modeling small and noisy experimental datasets. Standard network\narchitectures trained on scarce and noisy data will return predictions that\nviolate the underlying physics. In this paper, we present methods for embedding\neven--odd symmetries and conservation laws in neural networks and propose novel\nextensions and use cases for physical constraint embedded neural networks. We\ndesign an even--odd decomposition architecture for disentangling a neural\nnetwork parameterized function into its even and odd components and demonstrate\nthat it can accurately infer symmetries without prior knowledge. We highlight\nthe noise resilient properties of physical constraint embedded neural networks\nand demonstrate their utility as physics-informed noise regulators. Here we\nemployed a conservation of energy constraint embedded network as a\nphysics-informed noise regulator for a symbolic regression task. We showed that\nour approach returns a symbolic representation of the neural network\nparameterized function that aligns well with the underlying physics while\noutperforming a baseline symbolic regression approach.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:07:20 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Barber", "Gregory", ""], ["Haile", "Mulugeta A.", ""], ["Chen", "Tzikang", ""]]}, {"id": "2105.09163", "submitter": "Hongxiang Fan", "authors": "Hongxiang Fan, Martin Ferianc, Miguel Rodrigues, Hongyu Zhou, Xinyu\n  Niu and Wayne Luk", "title": "High-Performance FPGA-based Accelerator for Bayesian Neural Networks", "comments": "Design Automation Conference (DAC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NNs) have demonstrated their potential in a wide range of\napplications such as image recognition, decision making or recommendation\nsystems. However, standard NNs are unable to capture their model uncertainty\nwhich is crucial for many safety-critical applications including healthcare and\nautonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to\nexpress uncertainty in their prediction via a mathematical grounding.\nNevertheless, BNNs have not been as widely used in industrial practice, mainly\nbecause of their expensive computational cost and limited hardware performance.\nThis work proposes a novel FPGA-based hardware architecture to accelerate BNNs\ninferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN\naccelerators, the proposed accelerator can achieve up to 4 times higher energy\nefficiency and 9 times better compute efficiency. Considering partial Bayesian\ninference, an automatic framework is proposed, which explores the trade-off\nbetween hardware and algorithmic performance. Extensive experiments are\nconducted to demonstrate that our proposed framework can effectively find the\noptimal points in the design space.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 06:20:44 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 20:01:12 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Fan", "Hongxiang", ""], ["Ferianc", "Martin", ""], ["Rodrigues", "Miguel", ""], ["Zhou", "Hongyu", ""], ["Niu", "Xinyu", ""], ["Luk", "Wayne", ""]]}, {"id": "2105.09182", "submitter": "Yu Zhu", "authors": "Yu Zhu, Ananthram Swami, Santiago Segarra", "title": "Free Energy Node Embedding via Generalized Skip-gram with Negative\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely established set of unsupervised node embedding methods can be\ninterpreted as consisting of two distinctive steps: i) the definition of a\nsimilarity matrix based on the graph of interest followed by ii) an explicit or\nimplicit factorization of such matrix. Inspired by this viewpoint, we propose\nimprovements in both steps of the framework. On the one hand, we propose to\nencode node similarities based on the free energy distance, which interpolates\nbetween the shortest path and the commute time distances, thus, providing an\nadditional degree of flexibility. On the other hand, we propose a matrix\nfactorization method based on a loss function that generalizes that of the\nskip-gram model with negative sampling to arbitrary similarity matrices.\nCompared with factorizations based on the widely used $\\ell_2$ loss, the\nproposed method can better preserve node pairs associated with higher\nsimilarity scores. Moreover, it can be easily implemented using advanced\nautomatic differentiation toolkits and computed efficiently by leveraging GPU\nresources. Node clustering, node classification, and link prediction\nexperiments on real-world datasets demonstrate the effectiveness of\nincorporating free-energy-based similarities as well as the proposed matrix\nfactorization compared with state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:58:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhu", "Yu", ""], ["Swami", "Ananthram", ""], ["Segarra", "Santiago", ""]]}, {"id": "2105.09207", "submitter": "Hiromu Yakura", "authors": "Hiromu Yakura, Yuki Koyama, Masataka Goto", "title": "Tool- and Domain-Agnostic Parameterization of Style Transfer Effects\n  Leveraging Pretrained Perceptual Metrics", "comments": "To appear in Proceedings of the 30th International Joint Conference\n  on Artificial Intelligence (IJCAI 2021); Project page available at\n  https://yumetaro.info/projects/parametric-transcription/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning techniques for style transfer would not be optimal for\ndesign support since their \"one-shot\" transfer does not fit exploratory design\nprocesses. To overcome this gap, we propose parametric transcription, which\ntranscribes an end-to-end style transfer effect into parameter values of\nspecific transformations available in an existing content editing tool. With\nthis approach, users can imitate the style of a reference sample in the tool\nthat they are familiar with and thus can easily continue further exploration by\nmanipulating the parameters. To enable this, we introduce a framework that\nutilizes an existing pretrained model for style transfer to calculate a\nperceptual style distance to the reference sample and uses black-box\noptimization to find the parameters that minimize this distance. Our\nexperiments with various third-party tools, such as Instagram and Blender, show\nthat our framework can effectively leverage deep learning techniques for\ncomputational design support.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:39:10 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Yakura", "Hiromu", ""], ["Koyama", "Yuki", ""], ["Goto", "Masataka", ""]]}, {"id": "2105.09220", "submitter": "Aniket Pramanik", "authors": "Aniket Pramanik, Xiaodong Wu, Mathews Jacob", "title": "Joint Calibrationless Reconstruction and Segmentation of Parallel MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume estimation of brain regions from MRI data is a key problem in many\nclinical applications, where the acquisition of data at high spatial resolution\nis desirable. While parallel MRI and constrained image reconstruction\nalgorithms can accelerate the scans, image reconstruction artifacts are\ninevitable, especially at high acceleration factors. We introduce a novel image\ndomain deep-learning framework for calibrationless parallel MRI reconstruction,\ncoupled with a segmentation network to improve image quality and to reduce the\nvulnerability of current segmentation algorithms to image artifacts resulting\nfrom acceleration. The combination of the proposed image domain deep\ncalibrationless approach with the segmentation algorithm offers improved image\nquality, while increasing the accuracy of the segmentations. The novel\narchitecture with an encoder shared between the reconstruction and segmentation\ntasks is seen to reduce the need for segmented training datasets. In\nparticular, the proposed few-shot training strategy requires only 10% of\nsegmented datasets to offer good performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:04:20 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Pramanik", "Aniket", ""], ["Wu", "Xiaodong", ""], ["Jacob", "Mathews", ""]]}, {"id": "2105.09232", "submitter": "Lin Fan", "authors": "Lin Fan, Peter W. Glynn", "title": "Diffusion Approximations for Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behavior of Thompson sampling from the perspective of weak\nconvergence. In the regime where the gaps between arm means scale as\n$1/\\sqrt{n}$ with the time horizon $n$, we show that the dynamics of Thompson\nsampling evolve according to discrete versions of SDEs and random ODEs. As $n\n\\to \\infty$, we show that the dynamics converge weakly to solutions of the\ncorresponding SDEs and random ODEs. (Recently, Wager and Xu (arXiv:2101.09855)\nindependently proposed this regime and developed similar SDE and random ODE\napproximations for Thompson sampling in the multi-armed bandit setting.) Our\nweak convergence theory, which covers both multi-armed and linear bandit\nsettings, is developed from first principles using the Continuous Mapping\nTheorem and can be directly adapted to analyze other sampling-based bandit\nalgorithms, for example, algorithms using the bootstrap for exploration. We\nalso establish an invariance principle for multi-armed bandits with gaps\nscaling as $1/\\sqrt{n}$ -- for Thompson sampling and related algorithms\ninvolving posterior approximation or the bootstrap, the weak diffusion limits\nare in general the same regardless of the specifics of the reward distributions\nor the choice of prior. In particular, as suggested by the classical\nBernstein-von Mises normal approximation for posterior distributions, the weak\ndiffusion limits generally coincide with the limit for normally-distributed\nrewards and priors.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:28:01 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 03:00:42 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Fan", "Lin", ""], ["Glynn", "Peter W.", ""]]}, {"id": "2105.09240", "submitter": "Gideon Dresdner", "authors": "Gideon Dresdner, Saurav Shekhar, Fabian Pedregosa, Francesco\n  Locatello, Gunnar R\\\"atsch", "title": "Boosting Variational Inference With Locally Adaptive Step-Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational Inference makes a trade-off between the capacity of the\nvariational family and the tractability of finding an approximate posterior\ndistribution. Instead, Boosting Variational Inference allows practitioners to\nobtain increasingly good posterior approximations by spending more compute. The\nmain obstacle to widespread adoption of Boosting Variational Inference is the\namount of resources necessary to improve over a strong Variational Inference\nbaseline. In our work, we trace this limitation back to the global curvature of\nthe KL-divergence. We characterize how the global curvature impacts time and\nmemory consumption, address the problem with the notion of local curvature, and\nprovide a novel approximate backtracking algorithm for estimating local\ncurvature. We give new theoretical convergence rates for our algorithms and\nprovide experimental validation on synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:41:33 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dresdner", "Gideon", ""], ["Shekhar", "Saurav", ""], ["Pedregosa", "Fabian", ""], ["Locatello", "Francesco", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "2105.09253", "submitter": "Vaishali Ingale", "authors": "Vaishali Ingale, Rishabh Singh, Pragati Patwal", "title": "Image to Image Translation : Generating maps from satellite images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generation of maps from satellite images is conventionally done by a range of\ntools. Maps became an important part of life whose conversion from satellite\nimages may be a bit expensive but Generative models can pander to this\nchallenge. These models aims at finding the patterns between the input and\noutput image. Image to image translation is employed to convert satellite image\nto corresponding map. Different techniques for image to image translations like\nGenerative adversarial network, Conditional adversarial networks and\nCo-Variational Auto encoders are used to generate the corresponding\nhuman-readable maps for that region, which takes a satellite image at a given\nzoom level as its input. We are training our model on Conditional Generative\nAdversarial Network which comprises of Generator model which which generates\nfake images while the discriminator tries to classify the image as real or fake\nand both these models are trained synchronously in adversarial manner where\nboth try to fool each other and result in enhancing model performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:58:04 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ingale", "Vaishali", ""], ["Singh", "Rishabh", ""], ["Patwal", "Pragati", ""]]}, {"id": "2105.09254", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Numair Sani, Yizhen Xu, Ilya Shpitser", "title": "Multiply Robust Causal Mediation Analysis with Continuous Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG econ.EM stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many applications, researchers are interested in the direct and indirect\ncausal effects of an intervention on an outcome of interest. Mediation analysis\noffers a rigorous framework for the identification and estimation of such\ncausal quantities. In the case of binary treatment, efficient estimators for\nthe direct and indirect effects are derived by Tchetgen Tchetgen and Shpitser\n(2012). These estimators are based on influence functions and possess desirable\nmultiple robustness properties. However, they are not readily applicable when\ntreatments are continuous, which is the case in several settings, such as drug\ndosage in medical applications. In this work, we extend the influence\nfunction-based estimator of Tchetgen Tchetgen and Shpitser (2012) to deal with\ncontinuous treatments by utilizing a kernel smoothing approach. We first\ndemonstrate that our proposed estimator preserves the multiple robustness\nproperty of the estimator in Tchetgen Tchetgen and Shpitser (2012). Then we\nshow that under certain mild regularity conditions, our estimator is\nasymptotically normal. Our estimation scheme allows for high-dimensional\nnuisance parameters that can be estimated at slower rates than the target\nparameter. Additionally, we utilize cross-fitting, which allows for weaker\nsmoothness requirements for the nuisance functions.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:58:57 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Sani", "Numair", ""], ["Xu", "Yizhen", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2105.09261", "submitter": "Rapha\\\"el d'Andrimont", "authors": "Rapha\\\"el d'Andrimont and Astrid Verhegghen and Guido Lemoine and\n  Pieter Kempeneers and Michele Meroni and Marijn van der Velde", "title": "From parcel to continental scale -- A first European crop type map based\n  on Sentinel-1 and LUCAS Copernicus in-situ observations", "comments": "19 pages, 11 Figures, 5 Tables (without appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detailed parcel-level crop type mapping for the whole European Union (EU) is\nnecessary for the evaluation of agricultural policies. The Copernicus program,\nand Sentinel-1 (S1) in particular, offers the opportunity to monitor\nagricultural land at a continental scale and in a timely manner. However, so\nfar the potential of S1 has not been explored at such a scale. Capitalizing on\nthe unique LUCAS 2018 Copernicus in-situ survey, we present the first\ncontinental crop type map at 10-m spatial resolution for the EU based on S1A\nand S1B Synthetic Aperture Radar observations for the year 2018. Random forest\nclassification algorithms are tuned to detect 19 different crop types. We\nassess the accuracy of this EU crop map with three approaches. First, the\naccuracy is assessed with independent LUCAS core in-situ observations over the\ncontinent. Second, an accuracy assessment is done specifically for main crop\ntypes from farmers declarations from 6 EU member countries or regions totaling\n>3M parcels and 8.21 Mha. Finally, the crop areas derived by classification are\ncompared to the subnational (NUTS 2) area statistics reported by Eurostat. The\noverall accuracy for the map is reported as 80.3% when grouping main crop\nclasses and 76% when considering all 19 crop type classes separately. Highest\naccuracies are obtained for rape and turnip rape with user and produced\naccuracies higher than 96%. The correlation between the remotely sensed\nestimated and Eurostat reported crop area ranges from 0.93 (potatoes) to 0.99\n(rape and turnip rape). Finally, we discuss how the framework presented here\ncan underpin the operational delivery of in-season high-resolution based crop\nmapping.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:17:45 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 13:43:03 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["d'Andrimont", "Rapha\u00ebl", ""], ["Verhegghen", "Astrid", ""], ["Lemoine", "Guido", ""], ["Kempeneers", "Pieter", ""], ["Meroni", "Michele", ""], ["van der Velde", "Marijn", ""]]}, {"id": "2105.09264", "submitter": "Haoran Wang", "authors": "Haoran Wang, Shi Yu", "title": "Robo-Advising: Enhancing Investment with Inverse Optimization and Deep\n  Reinforcement Learning", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.LG q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has been embraced as a powerful tool by the financial\nindustry, with notable applications spreading in various domains including\ninvestment management. In this work, we propose a full-cycle data-driven\ninvestment robo-advising framework, consisting of two ML agents. The first\nagent, an inverse portfolio optimization agent, infers an investor's risk\npreference and expected return directly from historical allocation data using\nonline inverse optimization. The second agent, a deep reinforcement learning\n(RL) agent, aggregates the inferred sequence of expected returns to formulate a\nnew multi-period mean-variance portfolio optimization problem that can be\nsolved using deep RL approaches. The proposed investment pipeline is applied on\nreal market data from April 1, 2016 to February 1, 2021 and has shown to\nconsistently outperform the S&P 500 benchmark portfolio that represents the\naggregate market optimal allocation. The outperformance may be attributed to\nthe the multi-period planning (versus single-period planning) and the\ndata-driven RL approach (versus classical estimation approach).\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:20:03 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Wang", "Haoran", ""], ["Yu", "Shi", ""]]}, {"id": "2105.09266", "submitter": "Giorgio Franceschelli", "authors": "Giorgio Franceschelli and Mirco Musolesi", "title": "Copyright in Generative Deep Learning", "comments": "12 pages. Second version contains updates after entry into force of\n  EU's directive on copyright in the Digital Single Market, and corrections of\n  typos. Third version contains a new section about GitHub Copilot and its\n  copyright implications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-generated artworks are now part of the contemporary art scene: they\nare attracting significant investments and they are presented in exhibitions\ntogether with those created by human artists. These artworks are mainly based\non generative deep learning techniques, which have seen a formidable\ndevelopment and remarkable refinement in the very recent years. Given the\ninherent characteristics of these techniques, a series of novel legal problems\narise.\n  In this article, we consider a set of key questions in the area of generative\ndeep learning for the arts, including the following: is it possible to use\ncopyrighted works as training set for generative models? How do we legally\nstore their copies in order to perform the training process? Who (if someone)\nwill own the copyright on the generated data? We try to answer these questions\nconsidering the law in force in both the United States of America and the\nEuropean Union, and potential future alternatives. Finally, we also formulate a\nset of practical guidelines for artists and developers working on deep learning\ngenerated art.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:22:47 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 18:00:01 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 18:00:03 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Franceschelli", "Giorgio", ""], ["Musolesi", "Mirco", ""]]}, {"id": "2105.09270", "submitter": "Zhisheng Xiao", "authors": "Zhisheng Xiao, Qing Yan, Yali Amit", "title": "Do We Really Need to Learn Representations from In-domain Data for\n  Outlier Detection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised outlier detection, which predicts if a test sample is an outlier\nor not using only the information from unlabelled inlier data, is an important\nbut challenging task. Recently, methods based on the two-stage framework\nachieve state-of-the-art performance on this task. The framework leverages\nself-supervised representation learning algorithms to train a feature extractor\non inlier data, and applies a simple outlier detector in the feature space. In\nthis paper, we explore the possibility of avoiding the high cost of training a\ndistinct representation for each outlier detection task, and instead using a\nsingle pre-trained network as the universal feature extractor regardless of the\nsource of in-domain data. In particular, we replace the task-specific feature\nextractor by one network pre-trained on ImageNet with a self-supervised loss.\nIn experiments, we demonstrate competitive or better performance on a variety\nof outlier detection benchmarks compared with previous two-stage methods,\nsuggesting that learning representations from in-domain data may be unnecessary\nfor outlier detection.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:30:28 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Xiao", "Zhisheng", ""], ["Yan", "Qing", ""], ["Amit", "Yali", ""]]}, {"id": "2105.09275", "submitter": "Adrien Bibal", "authors": "Cristina Morariu, Adrien Bibal, Rene Cutura, Beno\\^it Fr\\'enay and\n  Michael Sedlmair", "title": "DumbleDR: Predicting User Preferences of Dimensionality Reduction\n  Projection Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A plethora of dimensionality reduction techniques have emerged over the past\ndecades, leaving researchers and analysts with a wide variety of choices for\nreducing their data, all the more so given some techniques come with additional\nparametrization (e.g. t-SNE, UMAP, etc.). Recent studies are showing that\npeople often use dimensionality reduction as a black-box regardless of the\nspecific properties the method itself preserves. Hence, evaluating and\ncomparing 2D projections is usually qualitatively decided, by setting\nprojections side-by-side and letting human judgment decide which projection is\nthe best. In this work, we propose a quantitative way of evaluating\nprojections, that nonetheless places human perception at the center. We run a\ncomparative study, where we ask people to select 'good' and 'misleading' views\nbetween scatterplots of low-level projections of image datasets, simulating the\nway people usually select projections. We use the study data as labels for a\nset of quality metrics whose purpose is to discover and quantify what exactly\npeople are looking for when deciding between projections. With this proxy for\nhuman judgments, we use it to rank projections on new datasets, explain why\nthey are relevant, and quantify the degree of subjectivity in projections\nselected.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:39:20 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Morariu", "Cristina", ""], ["Bibal", "Adrien", ""], ["Cutura", "Rene", ""], ["Fr\u00e9nay", "Beno\u00eet", ""], ["Sedlmair", "Michael", ""]]}, {"id": "2105.09284", "submitter": "Preslav Nakov", "authors": "Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj Alam, Fabrizio\n  Silvestri, Hamed Firooz, Preslav Nakov, Giovanni Da San Martino", "title": "SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and\n  Images", "comments": "propaganda, disinformation, misinformation, fake news, memes,\n  multimodality", "journal-ref": "SemEval-2021", "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in\nTexts and Images: the data, the annotation guidelines, the evaluation setup,\nthe results, and the participating systems. The task focused on memes and had\nthree subtasks: (i) detecting the techniques in the text, (ii) detecting the\ntext spans where the techniques are used, and (iii) detecting techniques in the\nentire meme, i.e., both in the text and in the image. It was a popular task,\nattracting 71 registrations, and 22 teams that eventually made an official\nsubmission on the test set. The evaluation results for the third subtask\nconfirmed the importance of both modalities, the text and the image. Moreover,\nsome teams reported benefits when not just combining the two modalities, e.g.,\nby using early or late fusion, but rather modeling the interaction between them\nin a joint model.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 05:00:53 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dimitrov", "Dimitar", ""], ["Ali", "Bishr Bin", ""], ["Shaar", "Shaden", ""], ["Alam", "Firoj", ""], ["Silvestri", "Fabrizio", ""], ["Firooz", "Hamed", ""], ["Nakov", "Preslav", ""], ["Martino", "Giovanni Da San", ""]]}, {"id": "2105.09295", "submitter": "Virginie Do", "authors": "Virginie Do, Jamal Atif, J\\'er\\^ome Lang and Nicolas Usunier", "title": "Online Selection of Diverse Committees", "comments": "IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Citizens' assemblies need to represent subpopulations according to their\nproportions in the general population. These large committees are often\nconstructed in an online fashion by contacting people, asking for the\ndemographic features of the volunteers, and deciding to include them or not.\nThis raises a trade-off between the number of people contacted (and the\nincurring cost) and the representativeness of the committee. We study three\nmethods, theoretically and experimentally: a greedy algorithm that includes\nvolunteers as long as proportionality is not violated; a non-adaptive method\nthat includes a volunteer with a probability depending only on their features,\nassuming that the joint feature distribution in the volunteer pool is known;\nand a reinforcement learning based approach when this distribution is not known\na priori but learnt online.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:55:29 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Do", "Virginie", ""], ["Atif", "Jamal", ""], ["Lang", "J\u00e9r\u00f4me", ""], ["Usunier", "Nicolas", ""]]}, {"id": "2105.09352", "submitter": "Dawn Drain", "authors": "Dawn Drain, Colin B. Clement, Guillermo Serrato, and Neel Sundaresan", "title": "DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and\n  Code Skeletons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The joint task of bug localization and program repair is an integral part of\nthe software development process. In this work we present DeepDebug, an\napproach to automated debugging using large, pretrained transformers. We begin\nby training a bug-creation model on reversed commit data for the purpose of\ngenerating synthetic bugs. We apply these synthetic bugs toward two ends.\nFirst, we directly train a backtranslation model on all functions from 200K\nrepositories. Next, we focus on 10K repositories for which we can execute\ntests, and create buggy versions of all functions in those repositories that\nare covered by passing tests. This provides us with rich debugging information\nsuch as stack traces and print statements, which we use to finetune our model\nwhich was pretrained on raw source code. Finally, we strengthen all our models\nby expanding the context window beyond the buggy function itself, and adding a\nskeleton consisting of that function's parent class, imports, signatures,\ndocstrings, and method bodies, in order of priority. On the QuixBugs benchmark,\nwe increase the total number of fixes found by over 50%, while also decreasing\nthe false positive rate from 35% to 5% and decreasing the timeout from six\nhours to one minute. On our own benchmark of executable tests, our model fixes\n68% of all bugs on its first attempt without using traces, and after adding\ntraces it fixes 75% on first attempt. We will open-source our framework and\nvalidation set for evaluating on executable tests.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 18:40:16 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Drain", "Dawn", ""], ["Clement", "Colin B.", ""], ["Serrato", "Guillermo", ""], ["Sundaresan", "Neel", ""]]}, {"id": "2105.09356", "submitter": "Seyed Saeed Changiz Rezaei", "authors": "Seyed Saeed Changiz Rezaei, Fred X. Han, Di Niu, Mohammad Salameh,\n  Keith Mills, Shuo Lian, Wei Lu, and Shangling Jui", "title": "Generative Adversarial Neural Architecture Search", "comments": "17 pages, 9 figures, 13 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 18:54:44 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 23:06:50 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 04:49:43 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Rezaei", "Seyed Saeed Changiz", ""], ["Han", "Fred X.", ""], ["Niu", "Di", ""], ["Salameh", "Mohammad", ""], ["Mills", "Keith", ""], ["Lian", "Shuo", ""], ["Lu", "Wei", ""], ["Jui", "Shangling", ""]]}, {"id": "2105.09365", "submitter": "Onur Boyar", "authors": "Enes Sadi Uysal, M.\\c{S}afak Bilici, B. Selin Zaza, M. Yi\\u{g}it\n  \\\"Ozgen\\c{c}, Onur Boyar", "title": "Exploring The Limits Of Data Augmentation For Retinal Vessel\n  Segmentation", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retinal Vessel Segmentation is important for the diagnosis of various\ndiseases. The research on retinal vessel segmentation focuses mainly on the\nimprovement of the segmentation model which is usually based on U-Net\narchitecture. In our study, we use the U-Net architecture and we rely on heavy\ndata augmentation in order to achieve better performance. The success of the\ndata augmentation relies on successfully addressing the problem of input\nimages. By analyzing input images and performing the augmentation accordingly\nwe show that the performance of the U-Net model can be increased dramatically.\nResults are reported using the most widely used retina dataset, DRIVE.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 19:15:31 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 13:04:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Uysal", "Enes Sadi", ""], ["Bilici", "M. \u015eafak", ""], ["Zaza", "B. Selin", ""], ["\u00d6zgen\u00e7", "M. Yi\u011fit", ""], ["Boyar", "Onur", ""]]}, {"id": "2105.09369", "submitter": "Aidmar Wainakh", "authors": "Aidmar Wainakh and Fabrizio Ventola and Till M\\\"u{\\ss}ig and Jens Keim\n  and Carlos Garcia Cordero and Ephraim Zimmer and Tim Grube and Kristian\n  Kersting and Max M\\\"uhlh\\\"auser", "title": "User Label Leakage from Gradients in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning enables multiple users to build a joint model by sharing\ntheir model updates (gradients), while their raw data remains local on their\ndevices. In contrast to the common belief that this provides privacy benefits,\nwe here add to the very recent results on privacy risks when sharing gradients.\nSpecifically, we propose Label Leakage from Gradients (LLG), a novel attack to\nextract the labels of the users' training data from their shared gradients. The\nattack exploits the direction and magnitude of gradients to determine the\npresence or absence of any label. LLG is simple yet effective, capable of\nleaking potential sensitive information represented by labels, and scales well\nto arbitrary batch sizes and multiple classes. We empirically and\nmathematically demonstrate the validity of our attack under different settings.\nMoreover, empirical results show that LLG successfully extracts labels with\nhigh accuracy at the early stages of model training. We also discuss different\ndefense mechanisms against such leakage. Our findings suggest that gradient\ncompression is a practical technique to prevent our attack.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 19:21:05 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 06:44:42 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 18:45:21 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wainakh", "Aidmar", ""], ["Ventola", "Fabrizio", ""], ["M\u00fc\u00dfig", "Till", ""], ["Keim", "Jens", ""], ["Cordero", "Carlos Garcia", ""], ["Zimmer", "Ephraim", ""], ["Grube", "Tim", ""], ["Kersting", "Kristian", ""], ["M\u00fchlh\u00e4user", "Max", ""]]}, {"id": "2105.09371", "submitter": "Haresh Karnan", "authors": "Haresh Karnan, Garrett Warnell, Xuesu Xiao, Peter Stone", "title": "VOILA: Visual-Observation-Only Imitation Learning for Autonomous\n  Navigation", "comments": "Under Submission to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While imitation learning for vision based autonomous mobile robot navigation\nhas recently received a great deal of attention in the research community,\nexisting approaches typically require state action demonstrations that were\ngathered using the deployment platform. However, what if one cannot easily\noutfit their platform to record these demonstration signals or worse yet the\ndemonstrator does not have access to the platform at all? Is imitation learning\nfor vision based autonomous navigation even possible in such scenarios? In this\nwork, we hypothesize that the answer is yes and that recent ideas from the\nImitation from Observation (IfO) literature can be brought to bear such that a\nrobot can learn to navigate using only ego centric video collected by a\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\nintroduce a new algorithm, Visual Observation only Imitation Learning for\nAutonomous navigation (VOILA), that can successfully learn navigation policies\nfrom a single video demonstration collected from a physically different agent.\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\nnot only successfully imitates the expert, but that it also learns navigation\npolicies that can generalize to novel environments. Further, we demonstrate the\neffectiveness of VOILA in a real world setting by showing that it allows a\nwheeled Jackal robot to successfully imitate a human walking in an environment\nusing a video recorded using a mobile phone camera.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 19:25:23 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Karnan", "Haresh", ""], ["Warnell", "Garrett", ""], ["Xiao", "Xuesu", ""], ["Stone", "Peter", ""]]}, {"id": "2105.09379", "submitter": "Avraham Adler", "authors": "Avraham Adler", "title": "Using Machine Learning Techniques to Identify Key Risk Factors for\n  Diabetes and Undiagnosed Diabetes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews a wide selection of machine learning models built to\npredict both the presence of diabetes and the presence of undiagnosed diabetes\nusing eight years of National Health and Nutrition Examination Survey (NHANES)\ndata. Models are tuned and compared via their Brier Scores. The most relevant\nvariables of the best performing models are then compared. A Support Vector\nMachine with a linear kernel performed best for predicting diabetes, returning\na Brier score of 0.0654 and an AUROC of 0.9235 on the test set. An elastic net\nregression performed best for predicting undiagnosed diabetes with a Brier\nscore of 0.0294 and an AUROC of 0.9439 on the test set. Similar features appear\nprominently in the models for both sets of models. Blood osmolality, family\nhistory, the prevalance of various compounds, and hypertension are key\nindicators for all diabetes risk. For undiagnosed diabetes in particular, there\nare ethnicity or genetic components which arise as strong correlates as well.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 20:02:35 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Adler", "Avraham", ""]]}, {"id": "2105.09384", "submitter": "Zhe Xu", "authors": "Zhe Xu and Hanghang Tong", "title": "Graph Sanitation with Application to Node Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decades have witnessed the prosperity of graph mining, with a\nmultitude of sophisticated models and algorithms designed for various mining\ntasks, such as ranking, classification, clustering and anomaly detection.\nGenerally speaking, the vast majority of the existing works aim to answer the\nfollowing question, that is, given a graph, what is the best way to mine it? In\nthis paper, we introduce the graph sanitation problem, to answer an orthogonal\nquestion. That is, given a mining task and an initial graph, what is the best\nway to improve the initially provided graph? By learning a better graph as part\nof the input of the mining model, it is expected to benefit graph mining in a\nvariety of settings, ranging from denoising, imputation to defense. We\nformulate the graph sanitation problem as a bilevel optimization problem, and\nfurther instantiate it by semi-supervised node classification, together with an\neffective solver named GaSoliNe. Extensive experimental results demonstrate\nthat the proposed method is (1) broadly applicable with respect to different\ngraph neural network models and flexible graph modification strategies, (2)\neffective in improving the node classification accuracy on both the original\nand contaminated graphs in various perturbation scenarios. In particular, it\nbrings up to 25% performance improvement over the existing robust graph neural\nnetwork methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 20:22:15 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Xu", "Zhe", ""], ["Tong", "Hanghang", ""]]}, {"id": "2105.09394", "submitter": "Seungyeon Kim", "authors": "Seungyeon Kim, Daniel Glasner, Srikumar Ramalingam, Cho-Jui Hsieh,\n  Kishore Papineni, Sanjiv Kumar", "title": "Balancing Robustness and Sensitivity using Feature Contrastive Learning", "comments": "31 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is generally believed that robust training of extremely large networks is\ncritical to their success in real-world applications. However, when taken to\nthe extreme, methods that promote robustness can hurt the model's sensitivity\nto rare or underrepresented patterns. In this paper, we discuss this trade-off\nbetween sensitivity and robustness to natural (non-adversarial) perturbations\nby introducing two notions: contextual feature utility and contextual feature\nsensitivity. We propose Feature Contrastive Learning (FCL) that encourages a\nmodel to be more sensitive to the features that have higher contextual utility.\nEmpirical results demonstrate that models trained with FCL achieve a better\nbalance of robustness and sensitivity, leading to improved generalization in\nthe presence of noise on both vision and NLP datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 20:53:02 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Kim", "Seungyeon", ""], ["Glasner", "Daniel", ""], ["Ramalingam", "Srikumar", ""], ["Hsieh", "Cho-Jui", ""], ["Papineni", "Kishore", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "2105.09400", "submitter": "Zhongshu Gu", "authors": "Pau-Chen Cheng, Kevin Eykholt, Zhongshu Gu, Hani Jamjoom, K. R.\n  Jayaram, Enriquillo Valdez, Ashish Verma", "title": "Separation of Powers in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) enables collaborative training among mutually\ndistrusting parties. Model updates, rather than training data, are concentrated\nand fused in a central aggregation server. A key security challenge in FL is\nthat an untrustworthy or compromised aggregation process might lead to\nunforeseeable information leakage. This challenge is especially acute due to\nrecently demonstrated attacks that have reconstructed large fractions of\ntraining data from ostensibly \"sanitized\" model updates.\n  In this paper, we introduce TRUDA, a new cross-silo FL system, employing a\ntrustworthy and decentralized aggregation architecture to break down\ninformation concentration with regard to a single aggregator. Based on the\nunique computational properties of model-fusion algorithms, all exchanged model\nupdates in TRUDA are disassembled at the parameter-granularity and re-stitched\nto random partitions designated for multiple TEE-protected aggregators. Thus,\neach aggregator only has a fragmentary and shuffled view of model updates and\nis oblivious to the model architecture. Our new security mechanisms can\nfundamentally mitigate training reconstruction attacks, while still preserving\nthe final accuracy of trained models and keeping performance overheads low.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 21:00:44 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cheng", "Pau-Chen", ""], ["Eykholt", "Kevin", ""], ["Gu", "Zhongshu", ""], ["Jamjoom", "Hani", ""], ["Jayaram", "K. R.", ""], ["Valdez", "Enriquillo", ""], ["Verma", "Ashish", ""]]}, {"id": "2105.09401", "submitter": "Lecheng Zheng", "authors": "Lecheng Zheng, Yada Zhu, Jingrui He, and Jinjun Xiong", "title": "Heterogeneous Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and characterized with multiple labels,\nthus exhibiting the co-existence of multiple types of heterogeneity. Although\nstate-of-the-art techniques are good at modeling the complex heterogeneity with\nsufficient label information, such label information can be quite expensive to\nobtain in real applications, leading to sub-optimal performance using these\ntechniques. Inspired by the capability of contrastive learning to utilize rich\nunlabeled data for improving performance, in this paper, we propose a unified\nheterogeneous learning framework, which combines both weighted unsupervised\ncontrastive loss and weighted supervised contrastive loss to model multiple\ntypes of heterogeneity. We also provide theoretical analyses showing that the\nproposed weighted supervised contrastive loss is the lower bound of the mutual\ninformation of two samples from the same class and the weighted unsupervised\ncontrastive loss is the lower bound of the mutual information between the\nhidden representation of two views of the same sample. Experimental results on\nreal-world data sets demonstrate the effectiveness and the efficiency of the\nproposed method modeling multiple types of heterogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 21:01:41 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zheng", "Lecheng", ""], ["Zhu", "Yada", ""], ["He", "Jingrui", ""], ["Xiong", "Jinjun", ""]]}, {"id": "2105.09406", "submitter": "Behzad Javaheri PhD", "authors": "Behzad Javaheri", "title": "Speech & Song Emotion Recognition Using Multilayer Perceptron and\n  Standard Vector Machine", "comments": null, "journal-ref": null, "doi": "10.20944/preprints202105.0441.v1", "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Herein, we have compared the performance of SVM and MLP in emotion\nrecognition using speech and song channels of the RAVDESS dataset. We have\nundertaken a journey to extract various audio features, identify optimal\nscaling strategy and hyperparameter for our models. To increase sample size, we\nhave performed audio data augmentation and addressed data imbalance using\nSMOTE. Our data indicate that optimised SVM outperforms MLP with an accuracy of\n82 compared to 75%. Following data augmentation, the performance of both\nalgorithms was identical at ~79%, however, overfitting was evident for the SVM.\nOur final exploration indicated that the performance of both SVM and MLP were\nsimilar in which both resulted in lower accuracy for the speech channel\ncompared to the song channel. Our findings suggest that both SVM and MLP are\npowerful classifiers for emotion recognition in a vocal-dependent manner.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 21:28:05 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Javaheri", "Behzad", ""]]}, {"id": "2105.09421", "submitter": "Ming-Chang Lee", "authors": "Ming-Chang Lee, Jia-Chun Lin, and Ernst Gunnar Gran", "title": "DistTune: Distributed Fine-Grained Adaptive Traffic Speed Prediction for\n  Growing Transportation Networks", "comments": "16 pages,21 figures, 4 tables", "journal-ref": "Transportation Research Record, May 2021", "doi": "10.1177/03611981211011170", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the past decade, many approaches have been introduced for traffic speed\nprediction. However, providing fine-grained, accurate, time-efficient, and\nadaptive traffic speed prediction for a growing transportation network where\nthe size of the network keeps increasing and new traffic detectors are\nconstantly deployed has not been well studied. To address this issue, this\npaper presents DistTune based on Long Short-Term Memory (LSTM) and the\nNelder-Mead method. Whenever encountering an unprocessed detector, DistTune\ndecides if it should customize an LSTM model for this detector by comparing the\ndetector with other processed detectors in terms of the normalized traffic\nspeed patterns they have observed. If similarity is found, DistTune directly\nshares an existing LSTM model with this detector to achieve time-efficient\nprocessing. Otherwise, DistTune customizes an LSTM model for the detector to\nachieve fine-grained prediction. To make DistTune even more time-efficient,\nDistTune performs on a cluster of computing nodes in parallel. To achieve\nadaptive traffic speed prediction, DistTune also provides LSTM re-customization\nfor detectors that suffer from unsatisfactory prediction accuracy due to for\ninstance traffic speed pattern change. Extensive experiments based on traffic\ndata collected from freeway I5-N in California are conducted to evaluate the\nperformance of DistTune. The results demonstrate that DistTune provides\nfine-grained, accurate, time-efficient, and adaptive traffic speed prediction\nfor a growing transportation network.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 22:20:58 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lee", "Ming-Chang", ""], ["Lin", "Jia-Chun", ""], ["Gran", "Ernst Gunnar", ""]]}, {"id": "2105.09428", "submitter": "Chuhong Lahlou", "authors": "Chuhong Lahlou, Ancil Crayton, Caroline Trier, Evan Willett", "title": "Explainable Health Risk Predictor with Transformer-based Medicare Claim\n  Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an\nArtificial Intelligence (AI) Health Outcomes Challenge seeking solutions to\npredict risk in value-based care for incorporation into CMS Innovation Center\npayment and service delivery models. Recently, modern language models have\nplayed key roles in a number of health related tasks. This paper presents, to\nthe best of our knowledge, the first application of these models to patient\nreadmission prediction. To facilitate this, we create a dataset of 1.2 million\nmedical history samples derived from the Limited Dataset (LDS) issued by CMS.\nMoreover, we propose a comprehensive modeling solution centered on a deep\nlearning framework for this data. To demonstrate the framework, we train an\nattention-based Transformer to learn Medicare semantics in support of\nperforming downstream prediction tasks thereby achieving 0.91 AUC and 0.91\nrecall on readmission classification. We also introduce a novel data\npre-processing pipeline and discuss pertinent deployment considerations\nsurrounding model explainability and bias.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 22:39:15 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lahlou", "Chuhong", ""], ["Crayton", "Ancil", ""], ["Trier", "Caroline", ""], ["Willett", "Evan", ""]]}, {"id": "2105.09433", "submitter": "Aditya Parulekar", "authors": "Aditya Parulekar, Advait Parulekar, Eric Price", "title": "L1 Regression with Lewis Weights Subsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of finding an approximate solution to $\\ell_1$\nregression while only observing a small number of labels. Given an $n \\times d$\nunlabeled data matrix $X$, we must choose a small set of $m \\ll n$ rows to\nobserve the labels of, then output an estimate $\\widehat{\\beta}$ whose error on\nthe original problem is within a $1 + \\varepsilon$ factor of optimal. We show\nthat sampling from $X$ according to its Lewis weights and outputting the\nempirical minimizer succeeds with probability $1-\\delta$ for $m >\nO(\\frac{1}{\\varepsilon^2} d \\log \\frac{d}{\\varepsilon \\delta})$. This is\nanalogous to the performance of sampling according to leverage scores for\n$\\ell_2$ regression, but with exponentially better dependence on $\\delta$. We\nalso give a corresponding lower bound of $\\Omega(\\frac{d}{\\varepsilon^2} + (d +\n\\frac{1}{\\varepsilon^2}) \\log\\frac{1}{\\delta})$.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 23:15:00 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Parulekar", "Aditya", ""], ["Parulekar", "Advait", ""], ["Price", "Eric", ""]]}, {"id": "2105.09446", "submitter": "Evan Lowe", "authors": "Evan Lowe, Levent Guven\\c{c}", "title": "A Review of Autonomous Road Vehicle Integrated Approaches to an\n  Emergency Obstacle Avoidance Maneuver", "comments": "(Bookmarks for section headings included) 59 pages of text, 14 pages\n  of references, 60 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As passenger vehicle technologies have advanced, so have their capabilities\nto avoid obstacles, especially with developments in tires, suspensions,\nsteering, as well as safety technologies like ABS, ESC, and more recently, ADAS\nsystems. However, environments around passenger vehicles have also become more\ncomplex, and dangerous. There have previously been studies that outline driver\ntendencies and performance capabilities when attempting to avoid obstacles\nwhile driving passenger vehicles. Now that autonomous vehicles are being\ndeveloped with obstacle avoidance capabilities, it is important to target\nperformance that meets or exceeds that of human drivers. This manuscript\nhighlights systems that are crucial for an emergency obstacle avoidance\nmaneuver (EOAM) and identifies the state-of-the-art for each of the related\nsystems, while considering the nuances of traveling at highway speeds. Some of\nthe primary EOAM-related systems/areas that are discussed in this review are:\ngeneral path planning methods, system hierarchies, decision-making, trajectory\ngeneration, and trajectory-tracking control methods. After concluding remarks,\nsuggestions for future work which could lead to an ideal EOAM development, are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 01:11:26 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 13:06:11 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 22:09:38 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lowe", "Evan", ""], ["Guven\u00e7", "Levent", ""]]}, {"id": "2105.09448", "submitter": "Gunjan Chhablani", "authors": "Gunjan Chhablani, Abheesht Sharma, Harshit Pandey, Tirtharaj Dash", "title": "Superpixel-based Domain-Knowledge Infusion in Computer Vision", "comments": "6 pages, 1 figure, Under review at ESANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixels are higher-order perceptual groups of pixels in an image, often\ncarrying much more information than raw pixels. There is an inherent relational\nstructure to the relationship among different superpixels of an image. This\nrelational information can convey some form of domain information about the\nimage, e.g. relationship between superpixels representing two eyes in a cat\nimage. Our interest in this paper is to construct computer vision models,\nspecifically those based on Deep Neural Networks (DNNs) to incorporate these\nsuperpixels information. We propose a methodology to construct a hybrid model\nthat leverages (a) Convolutional Neural Network (CNN) to deal with spatial\ninformation in an image, and (b) Graph Neural Network (GNN) to deal with\nrelational superpixel information in the image. The proposed deep model is\nlearned using a generic hybrid loss function that we call a `hybrid' loss. We\nevaluate the predictive performance of our proposed hybrid vision model on four\npopular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.\nMoreover, we evaluate our method on three real-world classification tasks:\nCOVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint\nIdentification. The results demonstrate that the relational superpixel\ninformation provided via a GNN could improve the performance of standard\nCNN-based vision systems.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 01:25:42 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chhablani", "Gunjan", ""], ["Sharma", "Abheesht", ""], ["Pandey", "Harshit", ""], ["Dash", "Tirtharaj", ""]]}, {"id": "2105.09452", "submitter": "Lucas N. Alegre", "authors": "Lucas N. Alegre, Ana L. C. Bazzan, Bruno C. da Silva", "title": "Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via\n  Online High-Confidence Change-Point Detection", "comments": "Published at Proc. of the 20th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2021)", "journal-ref": "Proceedings of the 20th International Conference on Autonomous\n  Agents and Multiagent Systems. 2021. 97-105", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-stationary environments are challenging for reinforcement learning\nalgorithms. If the state transition and/or reward functions change based on\nlatent factors, the agent is effectively tasked with optimizing a behavior that\nmaximizes performance over a possibly infinite random sequence of Markov\nDecision Processes (MDPs), each of which drawn from some unknown distribution.\nWe call each such MDP a context. Most related works make strong assumptions\nsuch as knowledge about the distribution over contexts, the existence of\npre-training phases, or a priori knowledge about the number, sequence, or\nboundaries between contexts. We introduce an algorithm that efficiently learns\npolicies in non-stationary environments. It analyzes a possibly infinite stream\nof data and computes, in real-time, high-confidence change-point detection\nstatistics that reflect whether novel, specialized policies need to be created\nand deployed to tackle novel contexts, or whether previously-optimized ones\nmight be reused. We show that (i) this algorithm minimizes the delay until\nunforeseen changes to a context are detected, thereby allowing for rapid\nresponses; and (ii) it bounds the rate of false alarm, which is important in\norder to minimize regret. Our method constructs a mixture model composed of a\n(possibly infinite) ensemble of probabilistic dynamics predictors that model\nthe different modes of the distribution over underlying latent MDPs. We\nevaluate our algorithm on high-dimensional continuous reinforcement learning\nproblems and show that it outperforms state-of-the-art (model-free and\nmodel-based) RL algorithms, as well as state-of-the-art meta-learning methods\nspecially designed to deal with non-stationarity.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 01:57:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Alegre", "Lucas N.", ""], ["Bazzan", "Ana L. C.", ""], ["da Silva", "Bruno C.", ""]]}, {"id": "2105.09461", "submitter": "Ayman Al-Kababji", "authors": "Ayman Al-Kababji, Abbes Amira, Faycal Bensaali, Abdulah Jarouf, Lisan\n  Shidqi, Hamza Djelouat", "title": "An IoT-Based Framework for Remote Fall Monitoring", "comments": "30 Pages, 9 figures, 9 tables. This is a the Accepted Manuscript\n  version of the article published in Biomedical Signal Processing and Control\n  (URL: https://doi.org/10.1016/j.bspc.2021.102532)", "journal-ref": null, "doi": "10.1016/j.bspc.2021.102532", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fall detection is a serious healthcare issue that needs to be solved. Falling\nwithout quick medical intervention would lower the chances of survival for the\nelderly, especially if living alone. Hence, the need is there for developing\nfall detection algorithms with high accuracy. This paper presents a novel\nIoT-based system for fall detection that includes a sensing device transmitting\ndata to a mobile application through a cloud-connected gateway device. Then,\nthe focus is shifted to the algorithmic aspect where multiple features are\nextracted from 3-axis accelerometer data taken from existing datasets. The\nresults emphasize on the significance of Continuous Wavelet Transform (CWT) as\nan influential feature for determining falls. CWT, Signal Energy (SE), Signal\nMagnitude Area (SMA), and Signal Vector Magnitude (SVM) features have shown\npromising classification results using K-Nearest Neighbors (KNN) and E-Nearest\nNeighbors (ENN). For all performance metrics (accuracy, recall, precision,\nspecificity, and F1 Score), the achieved results are higher than 95% for a\ndataset of small size, while more than 98.47% score is achieved in the\naforementioned criteria over the UniMiB-SHAR dataset by the same algorithms,\nwhere the classification time for a single test record is extremely efficient\nand is real-time\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:37:19 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Al-Kababji", "Ayman", ""], ["Amira", "Abbes", ""], ["Bensaali", "Faycal", ""], ["Jarouf", "Abdulah", ""], ["Shidqi", "Lisan", ""], ["Djelouat", "Hamza", ""]]}, {"id": "2105.09467", "submitter": "Bicheng Yan", "authors": "Bicheng Yan, Dylan Robert Harp, Bailian Chen, Rajesh Pawar", "title": "A Physics-Constrained Deep Learning Model for Simulating Multiphase Flow\n  in 3D Heterogeneous Porous Media", "comments": "26 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG physics.comp-ph physics.flu-dyn", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, an efficient physics-constrained deep learning model is\ndeveloped for solving multiphase flow in 3D heterogeneous porous media. The\nmodel fully leverages the spatial topology predictive capability of\nconvolutional neural networks, and is coupled with an efficient\ncontinuity-based smoother to predict flow responses that need spatial\ncontinuity. Furthermore, the transient regions are penalized to steer the\ntraining process such that the model can accurately capture flow in these\nregions. The model takes inputs including properties of porous media, fluid\nproperties and well controls, and predicts the temporal-spatial evolution of\nthe state variables (pressure and saturation). While maintaining the continuity\nof fluid flow, the 3D spatial domain is decomposed into 2D images for reducing\ntraining cost, and the decomposition results in an increased number of training\ndata samples and better training efficiency. Additionally, a surrogate model is\nseparately constructed as a postprocessor to calculate well flow rate based on\nthe predictions of state variables from the deep learning model. We use the\nexample of CO2 injection into saline aquifers, and apply the\nphysics-constrained deep learning model that is trained from physics-based\nsimulation data and emulates the physics process. The model performs prediction\nwith a speedup of ~1400 times compared to physics-based simulations, and the\naverage temporal errors of predicted pressure and saturation plumes are 0.27%\nand 0.099% respectively. Furthermore, water production rate is efficiently\npredicted by a surrogate model for well flow rate, with a mean error less than\n5%. Therefore, with its unique scheme to cope with the fidelity in fluid flow\nin porous media, the physics-constrained deep learning model can become an\nefficient predictive model for computationally demanding inverse problems or\nother coupled processes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:15:01 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Yan", "Bicheng", ""], ["Harp", "Dylan Robert", ""], ["Chen", "Bailian", ""], ["Pawar", "Rajesh", ""]]}, {"id": "2105.09468", "submitter": "Hewei Tang Dr.", "authors": "Hewei Tang, Pengcheng Fu, Christopher S. Sherman, Jize Zhang, Xin Ju,\n  Fran\\c{c}ois Hamon, Nicholas A. Azzolina, Matthew Burton-Kelly, and Joseph P.\n  Morris", "title": "A Deep Learning-Accelerated Data Assimilation and Forecasting Workflow\n  for Commercial-Scale Geologic Carbon Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast assimilation of monitoring data to update forecasts of pressure buildup\nand carbon dioxide (CO2) plume migration under geologic uncertainties is a\nchallenging problem in geologic carbon storage. The high computational cost of\ndata assimilation with a high-dimensional parameter space impedes fast\ndecision-making for commercial-scale reservoir management. We propose to\nleverage physical understandings of porous medium flow behavior with deep\nlearning techniques to develop a fast history matching-reservoir response\nforecasting workflow. Applying an Ensemble Smoother Multiple Data Assimilation\nframework, the workflow updates geologic properties and predicts reservoir\nperformance with quantified uncertainty from pressure history and CO2 plumes\ninterpreted through seismic inversion. As the most computationally expensive\ncomponent in such a workflow is reservoir simulation, we developed surrogate\nmodels to predict dynamic pressure and CO2 plume extents under multi-well\ninjection. The surrogate models employ deep convolutional neural networks,\nspecifically, a wide residual network and a residual U-Net. The workflow is\nvalidated against a flat three-dimensional reservoir model representative of a\nclastic shelf depositional environment. Intelligent treatments are applied to\nbridge between quantities in a true-3D reservoir model and those in a\nsingle-layer reservoir model. The workflow can complete history matching and\nreservoir forecasting with uncertainty quantification in less than one hour on\na mainstream personal workstation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 16:38:29 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Tang", "Hewei", ""], ["Fu", "Pengcheng", ""], ["Sherman", "Christopher S.", ""], ["Zhang", "Jize", ""], ["Ju", "Xin", ""], ["Hamon", "Fran\u00e7ois", ""], ["Azzolina", "Nicholas A.", ""], ["Burton-Kelly", "Matthew", ""], ["Morris", "Joseph P.", ""]]}, {"id": "2105.09469", "submitter": "Roy Frostig", "authors": "Roy Frostig, Matthew J. Johnson, Dougal Maclaurin, Adam Paszke, Alexey\n  Radul", "title": "Decomposing reverse-mode automatic differentiation", "comments": "Presented at the LAFI 2021 workshop at POPL, 17 January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We decompose reverse-mode automatic differentiation into (forward-mode)\nlinearization followed by transposition. Doing so isolates the essential\ndifference between forward- and reverse-mode AD, and simplifies their joint\nimplementation. In particular, once forward-mode AD rules are defined for every\nprimitive operation in a source language, only linear primitives require an\nadditional transposition rule in order to arrive at a complete reverse-mode AD\nimplementation. This is how reverse-mode AD is written in JAX and Dex.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 02:33:56 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Frostig", "Roy", ""], ["Johnson", "Matthew J.", ""], ["Maclaurin", "Dougal", ""], ["Paszke", "Adam", ""], ["Radul", "Alexey", ""]]}, {"id": "2105.09471", "submitter": "As{\\i}m Leblebici", "authors": "Asim Leblebici, Omer Gesoglu, Yasemin Basbinar", "title": "AI-Decision Support System Interface Using Cancer Related Data for Lung\n  Cancer Prognosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Until the beginning of 2021, lung cancer is known to be the most common\ncancer in the world. The disease is common due to factors such as occupational\nexposure, smoking and environmental pollution. The early diagnosis and\ntreatment of the disease is of great importance as well as the prevention of\nthe causes that cause the disease. The study was planned to create a web\ninterface that works with machine learning algorithms to predict prognosis\nusing lung cancer clinical and gene expression in the GDC data portal.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:22:37 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Leblebici", "Asim", ""], ["Gesoglu", "Omer", ""], ["Basbinar", "Yasemin", ""]]}, {"id": "2105.09474", "submitter": "Stanley Lazic", "authors": "Stanley E. Lazic, Dominic P. Williams", "title": "Quantifying sources of uncertainty in drug discovery predictions with\n  probabilistic models", "comments": "34 pages, 9 figures", "journal-ref": "Artificial Intelligence in the Life Sciences (2021)", "doi": "10.1016/j.ailsci.2021.100004", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Knowing the uncertainty in a prediction is critical when making expensive\ninvestment decisions and when patient safety is paramount, but machine learning\n(ML) models in drug discovery typically provide only a single best estimate and\nignore all sources of uncertainty. Predictions from these models may therefore\nbe over-confident, which can put patients at risk and waste resources when\ncompounds that are destined to fail are further developed. Probabilistic\npredictive models (PPMs) can incorporate uncertainty in both the data and\nmodel, and return a distribution of predicted values that represents the\nuncertainty in the prediction. PPMs not only let users know when predictions\nare uncertain, but the intuitive output from these models makes communicating\nrisk easier and decision making better. Many popular machine learning methods\nhave a PPM or Bayesian analogue, making PPMs easy to fit into current\nworkflows. We use toxicity prediction as a running example, but the same\nprinciples apply for all prediction models used in drug discovery. The\nconsequences of ignoring uncertainty and how PPMs account for uncertainty are\nalso described. We aim to make the discussion accessible to a broad\nnon-mathematical audience. Equations are provided to make ideas concrete for\nmathematical readers (but can be skipped without loss of understanding) and\ncode is available for computational researchers\n(https://github.com/stanlazic/ML_uncertainty_quantification).\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:54:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Lazic", "Stanley E.", ""], ["Williams", "Dominic P.", ""]]}, {"id": "2105.09477", "submitter": "Ehsan Haghighat", "authors": "Ehsan Haghighat, Ali Can Bekar, Erdogan Madenci, Ruben Juanes", "title": "Deep learning for solution and inversion of structural mechanics and\n  vibrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has been the most popular machine learning method in the last\nfew years. In this chapter, we present the application of deep learning and\nphysics-informed neural networks concerning structural mechanics and vibration\nproblems. Demonstration problems involve de-noising data, solution to\ntime-dependent ordinary and partial differential equations, and characterizing\nthe system's response for a given data.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:26:06 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Haghighat", "Ehsan", ""], ["Bekar", "Ali Can", ""], ["Madenci", "Erdogan", ""], ["Juanes", "Ruben", ""]]}, {"id": "2105.09481", "submitter": "Yotam Barnoy", "authors": "Will Pryor, Yotam Barnoy, Suraj Raval, Xiaolong Liu, Lamar Mair,\n  Daniel Lerner, Onder Erin, Gregory D. Hager, Yancy Diaz-Mercado, Axel Krieger", "title": "Localization and Control of Magnetic Suture Needles in Cluttered\n  Surgical Site with Blood and Tissue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Real-time visual localization of needles is necessary for various surgical\napplications, including surgical automation and visual feedback. In this study\nwe investigate localization and autonomous robotic control of needles in the\ncontext of our magneto-suturing system. Our system holds the potential for\nsurgical manipulation with the benefit of minimal invasiveness and reduced\npatient side effects. However, the non-linear magnetic fields produce\nunintuitive forces and demand delicate position-based control that exceeds the\ncapabilities of direct human manipulation. This makes automatic needle\nlocalization a necessity. Our localization method combines neural network-based\nsegmentation and classical techniques, and we are able to consistently locate\nour needle with 0.73 mm RMS error in clean environments and 2.72 mm RMS error\nin challenging environments with blood and occlusion. The average localization\nRMS error is 2.16 mm for all environments we used in the experiments. We\ncombine this localization method with our closed-loop feedback control system\nto demonstrate the further applicability of localization to autonomous control.\nOur needle is able to follow a running suture path in (1) no blood, no tissue;\n(2) heavy blood, no tissue; (3) no blood, with tissue; and (4) heavy blood,\nwith tissue environments. The tip position tracking error ranges from 2.6 mm to\n3.7 mm RMS, opening the door towards autonomous suturing tasks.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 02:50:09 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Pryor", "Will", ""], ["Barnoy", "Yotam", ""], ["Raval", "Suraj", ""], ["Liu", "Xiaolong", ""], ["Mair", "Lamar", ""], ["Lerner", "Daniel", ""], ["Erin", "Onder", ""], ["Hager", "Gregory D.", ""], ["Diaz-Mercado", "Yancy", ""], ["Krieger", "Axel", ""]]}, {"id": "2105.09492", "submitter": "Rundi Wu", "authors": "Rundi Wu, Chang Xiao, Changxi Zheng", "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models", "comments": "14 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation -- describing a shape as\na sequence of computer-aided design (CAD) operations. Unlike meshes and point\nclouds, CAD models encode the user creation process of 3D shapes, widely used\nin numerous industrial and engineering design tasks. However, the sequential\nand irregular structure of CAD operations poses significant challenges for\nexisting 3D generative models. Drawing an analogy between CAD operations and\nnatural language, we propose a CAD generative network based on the Transformer.\nWe demonstrate the performance of our model for both shape autoencoding and\nrandom shape generation. To train our network, we create a new CAD dataset\nconsisting of 179,133 models and their CAD construction sequences. We have made\nthis dataset publicly available to promote future research on this topic.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:29:18 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wu", "Rundi", ""], ["Xiao", "Chang", ""], ["Zheng", "Changxi", ""]]}, {"id": "2105.09494", "submitter": "Kamal Berahmand", "authors": "Kamal Berahmand, Elahe Nasiri, Saman Forouzandeh, Yuefeng Li", "title": "A Preference Random Walk Algorithm for Link Prediction through Mutual\n  Influence Nodes in Complex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting links in complex networks has been one of the essential topics\nwithin the realm of data mining and science discovery over the past few years.\nThis problem remains an attempt to identify future, deleted, and redundant\nlinks using the existing links in a graph. Local random walk is considered to\nbe one of the most well-known algorithms in the category of quasi-local\nmethods. It traverses the network using the traditional random walk with a\nlimited number of steps, randomly selecting one adjacent node in each step\namong the nodes which have equal importance. Then this method uses the\ntransition probability between node pairs to calculate the similarity between\nthem. However, in most datasets, this method is not able to perform accurately\nin scoring remarkably similar nodes. In the present article, an efficient\nmethod is proposed for improving local random walk by encouraging random walk\nto move, in every step, towards the node which has a stronger influence.\nTherefore, the next node is selected according to the influence of the source\nnode. To do so, using mutual information, the concept of the asymmetric mutual\ninfluence of nodes is presented. A comparison between the proposed method and\nother similarity-based methods (local, quasi-local, and global) has been\nperformed, and results have been reported for 11 real-world networks. It had a\nhigher prediction accuracy compared with other link prediction approaches.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:35:38 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Berahmand", "Kamal", ""], ["Nasiri", "Elahe", ""], ["Forouzandeh", "Saman", ""], ["Li", "Yuefeng", ""]]}, {"id": "2105.09501", "submitter": "Xiao Pan", "authors": "Xiao Pan, Mingxuan Wang, Liwei Wu, Lei Li", "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine\n  Translation", "comments": "accepted as long paper in ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose mRASP2, a\ntraining method to obtain a single unified multilingual translation model.\nmRASP2 is empowered by two techniques: a) a contrastive learning scheme to\nclose the gap among representations of different languages, and b) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mRASP2 outperforms\nexisting best unified model and achieves competitive or even better performance\nthan the pre-trained and fine-tuned model mBART on tens of WMT's translation\ndirections. For non-English directions, mRASP2 achieves an improvement of\naverage 10+ BLEU compared with the multilingual Transformer baseline. Code,\ndata and trained models are available at https://github.com/PANXiao1994/mRASP2.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:59:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 03:47:28 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 12:01:44 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Pan", "Xiao", ""], ["Wang", "Mingxuan", ""], ["Wu", "Liwei", ""], ["Li", "Lei", ""]]}, {"id": "2105.09506", "submitter": "Minglang Yin", "authors": "Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, George Em\n  Karniadakis", "title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant progress over the last 50 years in simulating flow\nproblems using numerical discretization of the Navier-Stokes equations (NSE),\nwe still cannot incorporate seamlessly noisy data into existing algorithms,\nmesh-generation is complex, and we cannot tackle high-dimensional problems\ngoverned by parametrized NSE. Moreover, solving inverse flow problems is often\nprohibitively expensive and requires complex and expensive formulations and new\ncomputer codes. Here, we review flow physics-informed learning, integrating\nseamlessly data and mathematical models, and implementing them using\nphysics-informed neural networks (PINNs). We demonstrate the effectiveness of\nPINNs for inverse problems related to three-dimensional wake flows, supersonic\nflows, and biomedical flows.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 04:14:55 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cai", "Shengze", ""], ["Mao", "Zhiping", ""], ["Wang", "Zhicheng", ""], ["Yin", "Minglang", ""], ["Karniadakis", "George Em", ""]]}, {"id": "2105.09513", "submitter": "Ameya Jagtap Dr", "authors": "Ameya D. Jagtap, Yeonjong Shin, Kenji Kawaguchi, George Em Karniadakis", "title": "Deep Kronecker neural networks: A general framework for neural networks\n  with adaptive activation functions", "comments": "26 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new type of neural networks, Kronecker neural networks (KNNs),\nthat form a general framework for neural networks with adaptive activation\nfunctions. KNNs employ the Kronecker product, which provides an efficient way\nof constructing a very wide network while keeping the number of parameters low.\nOur theoretical analysis reveals that under suitable conditions, KNNs induce a\nfaster decay of the loss than that by the feed-forward networks. This is also\nempirically verified through a set of computational examples. Furthermore,\nunder certain technical assumptions, we establish global convergence of\ngradient descent for KNNs. As a specific case, we propose the Rowdy activation\nfunction that is designed to get rid of any saturation region by injecting\nsinusoidal fluctuations, which include trainable parameters. The proposed Rowdy\nactivation function can be employed in any neural network architecture like\nfeed-forward neural networks, Recurrent neural networks, Convolutional neural\nnetworks etc. The effectiveness of KNNs with Rowdy activation is demonstrated\nthrough various computational experiments including function approximation\nusing feed-forward neural networks, solution inference of partial differential\nequations using the physics-informed neural networks, and standard deep\nlearning benchmark problems using convolutional and fully-connected neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 04:54:57 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Jagtap", "Ameya D.", ""], ["Shin", "Yeonjong", ""], ["Kawaguchi", "Kenji", ""], ["Karniadakis", "George Em", ""]]}, {"id": "2105.09536", "submitter": "Sela Fried", "authors": "Sela Fried, Geoffrey Wolfer", "title": "On the $\\alpha$-lazy version of Markov chains in estimation and testing\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate extendibility of the minimax one-trajectory length of several\nstatistical Markov chains inference problems and give sufficient conditions for\nboth the possibility and impossibility of such extensions. We follow up and\napply this framework to recently published results on learning and identity\ntesting of ergodic Markov chains. In particular, we show that for some of the\naforementioned results, we can omit the aperiodicity requirement by simulating\nan $\\alpha$-lazy version of the original process, and quantify the incurred\ncost of removing this assumption.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 06:26:13 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Fried", "Sela", ""], ["Wolfer", "Geoffrey", ""]]}, {"id": "2105.09540", "submitter": "Xiaolin Chen", "authors": "Xiaolin Chen, Shuai Zhou, Kai Yang, Hao Fan, Zejin Feng, Zhong Chen,\n  Hu Wang, Yongji Wang", "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for\n  Decision Tree Ensembles in Federated Learning", "comments": "10 pages, 5 figures, accepted by International Workshop on Federated\n  Learning for User Privacy and Data Confidentiality in Conjunction with ICML\n  2021(FL-ICML'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing concerns about data privacy and security drive an emerging\nfield of studying privacy-preserving machine learning from isolated data\nsources, i.e., federated learning. A class of federated learning, vertical\nfederated learning, where different parties hold different features for common\nusers, has a great potential of driving a more variety of business cooperation\namong enterprises in many fields. In machine learning, decision tree ensembles\nsuch as gradient boosting decision tree (GBDT) and random forest are widely\napplied powerful models with high interpretability and modeling efficiency.\nHowever, the interpretability is compromised in state-of-the-art vertical\nfederated learning frameworks such as SecureBoost with anonymous features to\navoid possible data breaches. To address this issue in the inference process,\nin this paper, we propose Fed-EINI to protect data privacy and allow the\ndisclosure of feature meaning by concealing decision paths with a\ncommunication-efficient secure computation method for inference outputs. The\nadvantages of Fed-EINI will be demonstrated through both theoretical analysis\nand extensive numerical results.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 06:40:05 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 08:09:39 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 08:07:13 GMT"}, {"version": "v4", "created": "Mon, 19 Jul 2021 13:17:56 GMT"}, {"version": "v5", "created": "Tue, 20 Jul 2021 14:25:09 GMT"}, {"version": "v6", "created": "Mon, 26 Jul 2021 13:10:19 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Xiaolin", ""], ["Zhou", "Shuai", ""], ["Yang", "Kai", ""], ["Fan", "Hao", ""], ["Feng", "Zejin", ""], ["Chen", "Zhong", ""], ["Wang", "Hu", ""], ["Wang", "Yongji", ""]]}, {"id": "2105.09543", "submitter": "Tianyu Gao", "authors": "Tianyu Gao, Xu Han, Keyue Qiu, Yuzhuo Bai, Zhiyu Xie, Yankai Lin,\n  Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou", "title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly\n  Supervised Relation Extraction", "comments": "ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distantly supervised (DS) relation extraction (RE) has attracted much\nattention in the past few years as it can utilize large-scale auto-labeled\ndata. However, its evaluation has long been a problem: previous works either\ntook costly and inconsistent methods to manually examine a small sample of\nmodel predictions, or directly test models on auto-labeled data -- which, by\nour check, produce as much as 53% wrong labels at the entity pair level in the\npopular NYT10 dataset. This problem has not only led to inaccurate evaluation,\nbut also made it hard to understand where we are and what's left to improve in\nthe research of DS-RE. To evaluate DS-RE models in a more credible way, we\nbuild manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,\nand thoroughly evaluate several competitive models, especially the latest\npre-trained ones. The experimental results show that the manual evaluation can\nindicate very different conclusions from automatic ones, especially some\nunexpected observations, e.g., pre-trained models can achieve dominating\nperformance while being more susceptible to false-positives compared to\nprevious methods. We hope that both our manual test sets and novel observations\ncan help advance future DS-RE research.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 06:55:40 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gao", "Tianyu", ""], ["Han", "Xu", ""], ["Qiu", "Keyue", ""], ["Bai", "Yuzhuo", ""], ["Xie", "Zhiyu", ""], ["Lin", "Yankai", ""], ["Liu", "Zhiyuan", ""], ["Li", "Peng", ""], ["Sun", "Maosong", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.09557", "submitter": "Takashi Mori", "authors": "Takashi Mori, Liu Ziyin, Kangqiao Liu, Masahito Ueda", "title": "Logarithmic landscape and power-law escape rate of SGD", "comments": "15+6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) undergoes complicated multiplicative noise\nfor the mean-square loss. We use this property of the SGD noise to derive a\nstochastic differential equation (SDE) with simpler additive noise by\nperforming a non-uniform transformation of the time variable. In the SDE, the\ngradient of the loss is replaced by that of the logarithmized loss.\nConsequently, we show that, near a local or global minimum, the stationary\ndistribution $P_\\mathrm{ss}(\\theta)$ of the network parameters $\\theta$ follows\na power-law with respect to the loss function $L(\\theta)$, i.e.\n$P_\\mathrm{ss}(\\theta)\\propto L(\\theta)^{-\\phi}$ with the exponent $\\phi$\nspecified by the mini-batch size, the learning rate, and the Hessian at the\nminimum. We obtain the escape rate formula from a local minimum, which is\ndetermined not by the loss barrier height $\\Delta L=L(\\theta^s)-L(\\theta^*)$\nbetween a minimum $\\theta^*$ and a saddle $\\theta^s$ but by the logarithmized\nloss barrier height $\\Delta\\log L=\\log[L(\\theta^s)/L(\\theta^*)]$. Our\nescape-rate formula explains an empirical fact that SGD prefers flat minima\nwith low effective dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 07:25:07 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Mori", "Takashi", ""], ["Ziyin", "Liu", ""], ["Liu", "Kangqiao", ""], ["Ueda", "Masahito", ""]]}, {"id": "2105.09564", "submitter": "Yang Wang", "authors": "Yang Wang, Chen Zhang, Zhiqiang Xie, Cong Guo, Yunxin Liu, Jingwen\n  Leng", "title": "Dual-side Sparse Tensor Core", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Leveraging sparsity in deep neural network (DNN) models is promising for\naccelerating model inference. Yet existing GPUs can only leverage the sparsity\nfrom weights but not activations, which are dynamic, unpredictable, and hence\nchallenging to exploit. In this work, we propose a novel architecture to\nefficiently harness the dual-side sparsity (i.e., weight and activation\nsparsity). We take a systematic approach to understand the (dis)advantages of\nprevious sparsity-related architectures and propose a novel, unexplored\nparadigm that combines outer-product computation primitive and bitmap-based\nencoding format. We demonstrate the feasibility of our design with minimal\nchanges to the existing production-scale inner-product-based Tensor Core. We\npropose a set of novel ISA extensions and co-design the matrix-matrix\nmultiplication and convolution algorithms, which are the two dominant\ncomputation patterns in today's DNN models, to exploit our new dual-side sparse\nTensor Core. Our evaluation shows that our design can fully unleash the\ndual-side DNN sparsity and improve the performance by up to one order of\nmagnitude with \\hl{small} hardware overhead.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 07:36:16 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wang", "Yang", ""], ["Zhang", "Chen", ""], ["Xie", "Zhiqiang", ""], ["Guo", "Cong", ""], ["Liu", "Yunxin", ""], ["Leng", "Jingwen", ""]]}, {"id": "2105.09579", "submitter": "Daisuke Moriwaki", "authors": "Takamichi Toda, Daisuke Moriwaki, Kazuhiro Ota", "title": "Aggregate Learning for Mixed Frequency Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large and acute economic shocks such as the 2007-2009 financial crisis and\nthe current COVID-19 infections rapidly change the economic environment. In\nsuch a situation, the importance of real-time economic analysis using\nalternative datais emerging. Alternative data such as search query and location\ndata are closer to real-time and richer than official statistics that are\ntypically released once a month in an aggregated form. We take advantage of\nspatio-temporal granularity of alternative data and propose a\nmixed-FrequencyAggregate Learning (MF-AGL)model that predicts economic\nindicators for the smaller areas in real-time. We apply the model for the\nreal-world problem; prediction of the number of job applicants which is closely\nrelated to the unemployment rates. We find that the proposed model predicts (i)\nthe regional heterogeneity of the labor market condition and (ii) the rapidly\nchanging economic status. The model can be applied to various tasks, especially\neconomic analysis\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:12:43 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Toda", "Takamichi", ""], ["Moriwaki", "Daisuke", ""], ["Ota", "Kazuhiro", ""]]}, {"id": "2105.09580", "submitter": "Nanqing Dong", "authors": "Nanqing Dong, Michael Kampffmeyer, Irina Voiculescu, Eric Xing", "title": "Negational Symmetry of Quantum Neural Networks for Binary Pattern\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entanglement is a physical phenomenon, which has fueled recent successes of\nquantum algorithms. Although quantum neural networks (QNNs) have shown\npromising results in solving simple machine learning tasks recently, for the\ntime being, the effect of entanglement in QNNs and the behavior of QNNs in\nbinary pattern classification are still underexplored. In this work, we provide\nsome theoretical insight into the properties of QNNs by presenting and\nanalyzing a new form of invariance embedded in QNNs for both quantum binary\nclassification and quantum representation learning, which we term negational\nsymmetry. Given a quantum binary signal and its negational counterpart where a\nbitwise NOT operation is applied to each quantum bit of the binary signal, a\nQNN outputs the same logits. That is to say, QNNs cannot differentiate a\nquantum binary signal and its negational counterpart in a binary classification\ntask. We further empirically evaluate the negational symmetry of QNNs in binary\npattern classification tasks using Google's quantum computing framework. The\ntheoretical and experimental results suggest that negational symmetry is a\nfundamental property of QNNs, which is not shared by classical models. Our\nfindings also imply that negational symmetry is a double-edged sword in\npractical quantum applications.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:13:38 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Dong", "Nanqing", ""], ["Kampffmeyer", "Michael", ""], ["Voiculescu", "Irina", ""], ["Xing", "Eric", ""]]}, {"id": "2105.09592", "submitter": "Konstantinos Bountrogiannis", "authors": "Konstantinos Bountrogiannis, George Tzagkarakis, Panagiotis Tsakalides", "title": "Distribution Agnostic Symbolic Representations for Time Series\n  Dimensionality Reduction and Online Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the importance of the lower bounding distances and the attractiveness\nof symbolic representations, the family of symbolic aggregate approximations\n(SAX) has been used extensively for encoding time series data. However, typical\nSAX-based methods rely on two restrictive assumptions; the Gaussian\ndistribution and equiprobable symbols. This paper proposes two novel\ndata-driven SAX-based symbolic representations, distinguished by their\ndiscretization steps. The first representation, oriented for general data\ncompaction and indexing scenarios, is based on the combination of kernel\ndensity estimation and Lloyd-Max quantization to minimize the information loss\nand mean squared error in the discretization step. The second method, oriented\nfor high-level mining tasks, employs the Mean-Shift clustering method and is\nshown to enhance anomaly detection in the lower-dimensional space. Besides, we\nverify on a theoretical basis a previously observed phenomenon of the intrinsic\nprocess that results in a lower than the expected variance of the intermediate\npiecewise aggregate approximation. This phenomenon causes an additional\ninformation loss but can be avoided with a simple modification. The proposed\nrepresentations possess all the attractive properties of the conventional SAX\nmethod. Furthermore, experimental evaluation on real-world datasets\ndemonstrates their superiority compared to the traditional SAX and an\nalternative data-driven SAX variant.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:35:50 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Bountrogiannis", "Konstantinos", ""], ["Tzagkarakis", "George", ""], ["Tsakalides", "Panagiotis", ""]]}, {"id": "2105.09601", "submitter": "Yash Kumar Atri", "authors": "Yash Kumar Atri, Shraman Pramanick, Vikram Goyal, Tanmoy Chakraborty", "title": "See, Hear, Read: Leveraging Multimodality with Guided Attention for\n  Abstractive Text Summarization", "comments": "Journal paper accepted in Knowledge Based Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose {\\name}, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. {\\name} utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that {\\name} achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:56:33 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Atri", "Yash Kumar", ""], ["Pramanick", "Shraman", ""], ["Goyal", "Vikram", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2105.09618", "submitter": "Noa Malem-Shinitski", "authors": "Noa Malem-Shinitski, Cesar Ojeda and Manfred Opper", "title": "Nonlinear Hawkes Process with Gaussian Process Self Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditionally, Hawkes processes are used to model time--continuous point\nprocesses with history dependence. Here we propose an extended model where the\nself--effects are of both excitatory and inhibitory type and follow a Gaussian\nProcess. Whereas previous work either relies on a less flexible\nparameterization of the model, or requires a large amount of data, our\nformulation allows for both a flexible model and learning when data are scarce.\nWe continue the line of work of Bayesian inference for Hawkes processes, and\nour approach dispenses with the necessity of estimating a branching structure\nfor the posterior, as we perform inference on an aggregated sum of Gaussian\nProcesses. Efficient approximate Bayesian inference is achieved via data\naugmentation, and we describe a mean--field variational inference approach to\nlearn the model parameters. To demonstrate the flexibility of the model we\napply our methodology on data from three different domains and compare it to\npreviously reported results.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 09:20:35 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Malem-Shinitski", "Noa", ""], ["Ojeda", "Cesar", ""], ["Opper", "Manfred", ""]]}, {"id": "2105.09632", "submitter": "Vivek Kumar Mr.", "authors": "Danilo Dessi, Rim Helaoui, Vivek Kumar, Diego Reforgiato Recupero, and\n  Daniele Riboni", "title": "TF-IDF vs Word Embeddings for Morbidity Identification in Clinical\n  Notes: An Initial Study", "comments": "12 pages, 2 figures, 2 tables, SmartPhil 2020-First Workshop on Smart\n  Personal Health Interfaces, Associated to ACM IUI 2020", "journal-ref": null, "doi": "10.5281/zenodo.4777594", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, we are seeing an ever-increasing number of clinical notes that contain\nclinical results, images, and textual descriptions of patient's health state.\nAll these data can be analyzed and employed to cater novel services that can\nhelp people and domain experts with their common healthcare tasks. However,\nmany technologies such as Deep Learning and tools like Word Embeddings have\nstarted to be investigated only recently, and many challenges remain open when\nit comes to healthcare domain applications. To address these challenges, we\npropose the use of Deep Learning and Word Embeddings for identifying sixteen\nmorbidity types within textual descriptions of clinical records. For this\npurpose, we have used a Deep Learning model based on Bidirectional Long-Short\nTerm Memory (LSTM) layers which can exploit state-of-the-art vector\nrepresentations of data such as Word Embeddings. We have employed pre-trained\nWord Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained\non the target domain. Furthermore, we have compared the performances of the\ndeep learning approaches against the traditional tf-idf using Support Vector\nMachine and Multilayer perceptron (our baselines). From the obtained results it\nseems that the latter outperforms the combination of Deep Learning approaches\nusing any word embeddings. Our preliminary results indicate that there are\nspecific features that make the dataset biased in favour of traditional machine\nlearning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 09:57:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 10:51:57 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dessi", "Danilo", ""], ["Helaoui", "Rim", ""], ["Kumar", "Vivek", ""], ["Recupero", "Diego Reforgiato", ""], ["Riboni", "Daniele", ""]]}, {"id": "2105.09637", "submitter": "Sam Devlin", "authors": "Sam Devlin, Raluca Georgescu, Ida Momennejad, Jaroslaw Rzepecki,\n  Evelyn Zuniga, Gavin Costello, Guy Leroy, Ali Shaw and Katja Hofmann", "title": "Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation", "comments": "All data collected throughout this study, plus the code to reproduce\n  our analysis and ANTT are available at https://github.com/microsoft/NTT", "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning (ICML), 139:2644-2653, 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key challenge on the path to developing agents that learn complex\nhuman-like behavior is the need to quickly and accurately quantify\nhuman-likeness. While human assessments of such behavior can be highly\naccurate, speed and scalability are limited. We address these limitations\nthrough a novel automated Navigation Turing Test (ANTT) that learns to predict\nhuman judgments of human-likeness. We demonstrate the effectiveness of our\nautomated NTT on a navigation task in a complex 3D environment. We investigate\nsix classification models to shed light on the types of architectures best\nsuited to this task, and validate them against data collected through a human\nNTT. Our best models achieve high accuracy when distinguishing true human and\nagent behavior. At the same time, we show that predicting finer-grained human\nassessment of agents' progress towards human-like behavior remains unsolved.\nOur work takes an important step towards agents that more effectively learn\ncomplex human-like behavior.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:14:23 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 12:49:43 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Devlin", "Sam", ""], ["Georgescu", "Raluca", ""], ["Momennejad", "Ida", ""], ["Rzepecki", "Jaroslaw", ""], ["Zuniga", "Evelyn", ""], ["Costello", "Gavin", ""], ["Leroy", "Guy", ""], ["Shaw", "Ali", ""], ["Hofmann", "Katja", ""]]}, {"id": "2105.09670", "submitter": "Jingyi Zhang", "authors": "Jingyi Zhang, Huolan Zhu, Yongkai Chen, Chenguang Yang, Huimin Cheng,\n  Yi Li, Wenxuan Zhong, Fang Wang", "title": "Ensemble machine learning approach for screening of coronary heart\n  disease based on echocardiography and risk factors", "comments": "30 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Extensive clinical evidence suggests that a preventive screening\nof coronary heart disease (CHD) at an earlier stage can greatly reduce the\nmortality rate. We use 64 two-dimensional speckle tracking echocardiography\n(2D-STE) features and seven clinical features to predict whether one has CHD.\nMethods: We develop a machine learning approach that integrates a number of\npopular classification methods together by model stacking, and generalize the\ntraditional stacking method to a two-step stacking method to improve the\ndiagnostic performance. Results: By borrowing strengths from multiple\nclassification models through the proposed method, we improve the CHD\nclassification accuracy from around 70% to 87.7% on the testing set. The\nsensitivity of the proposed method is 0.903 and the specificity is 0.843, with\nan AUC of 0.904, which is significantly higher than those of the individual\nclassification models. Conclusions: Our work lays a foundation for the\ndeployment of speckle tracking echocardiography-based screening tools for\ncoronary heart disease.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:04:58 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhang", "Jingyi", ""], ["Zhu", "Huolan", ""], ["Chen", "Yongkai", ""], ["Yang", "Chenguang", ""], ["Cheng", "Huimin", ""], ["Li", "Yi", ""], ["Zhong", "Wenxuan", ""], ["Wang", "Fang", ""]]}, {"id": "2105.09673", "submitter": "Elad Granot", "authors": "Amit Daniely and Elad Granot", "title": "An Exact Poly-Time Membership-Queries Algorithm for Extraction a\n  three-Layer ReLU Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As machine learning increasingly becomes more prevalent in our everyday life,\nmany organizations offer neural-networks based services as a black-box. The\nreasons for hiding a learning model may vary: e.g., preventing copying of its\nbehavior or keeping back an adversarial from reverse-engineering its mechanism\nand revealing sensitive information about its training data.\n  However, even as a black-box, some information can still be discovered by\nspecific queries. In this work, we show a polynomial-time algorithm that uses a\npolynomial number of queries to mimic precisely the behavior of a three-layer\nneural network that uses ReLU activation.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:24:08 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Daniely", "Amit", ""], ["Granot", "Elad", ""]]}, {"id": "2105.09675", "submitter": "Niels Gr\\\"uttemeier", "authors": "Niels Gr\\\"uttemeier, Christian Komusiewicz, Nils Morawietz", "title": "On the Parameterized Complexity of Polytree Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Bayesian network is a directed acyclic graph that represents statistical\ndependencies between variables of a joint probability distribution. A\nfundamental task in data science is to learn a Bayesian network from observed\ndata. \\textsc{Polytree Learning} is the problem of learning an optimal Bayesian\nnetwork that fulfills the additional property that its underlying undirected\ngraph is a forest. In this work, we revisit the complexity of \\textsc{Polytree\nLearning}. We show that \\textsc{Polytree Learning} can be solved in $3^n \\cdot\n|I|^{\\mathcal{O}(1)}$ time where $n$ is the number of variables and $|I|$ is\nthe total instance size. Moreover, we consider the influence of the number of\nvariables $d$ that might receive a nonempty parent set in the final DAG on the\ncomplexity of \\textsc{Polytree Learning}. We show that \\textsc{Polytree\nLearning} has no $f(d)\\cdot |I|^{\\mathcal{O}(1)}$-time algorithm, unlike\nBayesian network learning which can be solved in $2^d \\cdot\n|I|^{\\mathcal{O}(1)}$ time. We show that, in contrast, if $d$ and the maximum\nparent set size are bounded, then we can obtain efficient algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:29:12 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gr\u00fcttemeier", "Niels", ""], ["Komusiewicz", "Christian", ""], ["Morawietz", "Nils", ""]]}, {"id": "2105.09679", "submitter": "Koujin Takeda", "authors": "Shun Kimura, Keisuke Ota, Koujin Takeda", "title": "Improved Neuronal Ensemble Inference with Generative Model and MCMC", "comments": "23 pages, 8 figures, partially overlapped with arXiv:1911.06509", "journal-ref": "J. Stat. Mech. (2021) 063501", "doi": "10.1088/1742-5468/abffd5", "report-no": null, "categories": "cond-mat.dis-nn cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal ensemble inference is a significant problem in the study of\nbiological neural networks. Various methods have been proposed for ensemble\ninference from experimental data of neuronal activity. Among them, Bayesian\ninference approach with generative model was proposed recently. However, this\nmethod requires large computational cost for appropriate inference. In this\nwork, we give an improved Bayesian inference algorithm by modifying update rule\nin Markov chain Monte Carlo method and introducing the idea of simulated\nannealing for hyperparameter control. We compare the performance of ensemble\ninference between our algorithm and the original one, and discuss the advantage\nof our method.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:37:38 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kimura", "Shun", ""], ["Ota", "Keisuke", ""], ["Takeda", "Koujin", ""]]}, {"id": "2105.09681", "submitter": "Zhuangwei Shi", "authors": "Chen Jin, Zhuangwei Shi, Weihua Li, Yanbu Guo", "title": "Bidirectional LSTM-CRF Attention-based Model for Chinese Word\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chinese word segmentation (CWS) is the basic of Chinese natural language\nprocessing (NLP). The quality of word segmentation will directly affect the\nrest of NLP tasks. Recently, with the artificial intelligence tide rising\nagain, Long Short-Term Memory (LSTM) neural network, as one of easily modeling\nin sequence, has been widely utilized in various kinds of NLP tasks, and\nfunctions well. Attention mechanism is an ingenious method to solve the memory\ncompression problem on LSTM. Furthermore, inspired by the powerful abilities of\nbidirectional LSTM models for modeling sequence and CRF model for decoding, we\npropose a Bidirectional LSTM-CRF Attention-based Model in this paper.\nExperiments on PKU and MSRA benchmark datasets show that our model performs\nbetter than the baseline methods modeling by other neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:46:53 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Jin", "Chen", ""], ["Shi", "Zhuangwei", ""], ["Li", "Weihua", ""], ["Guo", "Yanbu", ""]]}, {"id": "2105.09685", "submitter": "Jaydeep Borkar", "authors": "Jaydeep Borkar, Pin-Yu Chen", "title": "Simple Transparent Adversarial Examples", "comments": "14 pages, 9 figures, Published at ICLR 2021 Workshop on Security and\n  Safety in Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a rise in the use of Machine Learning as a Service (MLaaS)\nVision APIs as they offer multiple services including pre-built models and\nalgorithms, which otherwise take a huge amount of resources if built from\nscratch. As these APIs get deployed for high-stakes applications, it's very\nimportant that they are robust to different manipulations. Recent works have\nonly focused on typical adversarial attacks when evaluating the robustness of\nvision APIs. We propose two new aspects of adversarial image generation methods\nand evaluate them on the robustness of Google Cloud Vision API's optical\ncharacter recognition service and object detection APIs deployed in real-world\nsettings such as sightengine.com, picpurify.com, Google Cloud Vision API, and\nMicrosoft Azure's Computer Vision API. Specifically, we go beyond the\nconventional small-noise adversarial attacks and introduce secret embedding and\ntransparent adversarial examples as a simpler way to evaluate robustness. These\nmethods are so straightforward that even non-specialists can craft such\nattacks. As a result, they pose a serious threat where APIs are used for\nhigh-stakes applications. Our transparent adversarial examples successfully\nevade state-of-the art object detections APIs such as Azure Cloud Vision\n(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).\n90% of the images have a secret embedded text that successfully fools the\nvision of time-limited humans but is detected by Google Cloud Vision API's\noptical character recognition. Complementing to current research, our results\nprovide simple but unconventional methods on robustness evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:54:26 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Borkar", "Jaydeep", ""], ["Chen", "Pin-Yu", ""]]}, {"id": "2105.09716", "submitter": "Yongfeng Li", "authors": "Yongfeng Li, Mingming Zhao, Weijie Chen, and Zaiwen Wen", "title": "A Stochastic Composite Augmented Lagrangian Method For Reinforcement\n  Learning", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the linear programming (LP) formulation for deep\nreinforcement learning. The number of the constraints depends on the size of\nstate and action spaces, which makes the problem intractable in large or\ncontinuous environments. The general augmented Lagrangian method suffers the\ndouble-sampling obstacle in solving the LP. Namely, the conditional\nexpectations originated from the constraint functions and the quadratic\npenalties in the augmented Lagrangian function impose difficulties in sampling\nand evaluation. Motivated from the updates of the multipliers, we overcome the\nobstacles in minimizing the augmented Lagrangian function by replacing the\nintractable conditional expectations with the multipliers. Therefore, a deep\nparameterized augment Lagrangian method is proposed. Furthermore, the\nreplacement provides a promising breakthrough to integrate the two steps in the\naugmented Lagrangian method into a single constrained problem. A general\ntheoretical analysis shows that the solutions generated from a sequence of the\nconstrained optimizations converge to the optimal solution of the LP if the\nerror is controlled properly. A theoretical analysis on the quadratic penalty\nalgorithm under neural tangent kernel setting shows the residual can be\narbitrarily small if the parameter in network and optimization algorithm is\nchosen suitably. Preliminary experiments illustrate that our method is\ncompetitive to other state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:08:06 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Li", "Yongfeng", ""], ["Zhao", "Mingming", ""], ["Chen", "Weijie", ""], ["Wen", "Zaiwen", ""]]}, {"id": "2105.09720", "submitter": "Thosini Bamunu Mudiyanselage", "authors": "Thosini Bamunu Mudiyanselage, Nipuna Senanayake, Chunyan Ji, Yi Pan\n  and Yanqing Zhang", "title": "Covid-19 Detection from Chest X-ray and Patient Metadata using Graph\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel corona virus (Covid-19) has introduced significant challenges due\nto its rapid spreading nature through respiratory transmission. As a result,\nthere is a huge demand for Artificial Intelligence (AI) based quick disease\ndiagnosis methods as an alternative to high demand tests such as Polymerase\nChain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective\nradiography technique due to resource availability and quick screening. But, a\nsufficient and systematic data collection that is required by complex deep\nleaning (DL) models is more difficult and hence there are recent efforts that\nutilize transfer learning to address this issue. Still these transfer learnt\nmodels suffer from lack of generalization and increased bias to the training\ndataset resulting poor performance for unseen data. Limited correlation of the\ntransferred features from the pre-trained model to a specific medical imaging\ndomain like X-ray and overfitting on fewer data can be reasons for this\ncircumstance. In this work, we propose a novel Graph Convolution Neural Network\n(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR\nimages and meta information about patients. The proposed method exploits\nimportant relational knowledge between data instances and their features using\ngraph representation and applies convolution to learn the graph data which is\nnot possible with conventional convolution on Euclidean domain. The results of\nextensive experiments of proposed model on binary (Covid vs normal) and three\nclass (Covid, normal, other pneumonia) classification problems outperform\ndifferent benchmark transfer learnt models, hence overcoming the aforementioned\ndrawbacks.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:13:29 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 12:38:45 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Mudiyanselage", "Thosini Bamunu", ""], ["Senanayake", "Nipuna", ""], ["Ji", "Chunyan", ""], ["Pan", "Yi", ""], ["Zhang", "Yanqing", ""]]}, {"id": "2105.09737", "submitter": "Kasra Arnavaz", "authors": "Kasra Arnavaz, Oswin Krause, Jelena M. Krivokapic, Silja Heilmann,\n  Jakob Andreas B{\\ae}rentzen, Pia Nyeng, Aasa Feragen", "title": "Semi-supervised, Topology-Aware Segmentation of Tubular Structures from\n  Live Imaging 3D Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by a challenging tubular network segmentation task, this paper\ntackles two commonly encountered problems in biomedical imaging: Topological\nconsistency of the segmentation, and limited annotations. We propose a\ntopological score which measures both topological and geometric consistency\nbetween the predicted and ground truth segmentations, applied for model\nselection and validation. We apply our topological score in three scenarios: i.\na U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised\nU-net architecture, which offers a straightforward approach to jointly training\nthe network both as an autoencoder and a segmentation algorithm. This allows us\nto utilize un-annotated data for training a representation that generalizes\nacross test data variability, in spite of our annotated training data having\nvery limited variation. Our contributions are validated on a challenging\nsegmentation task, locating tubular structures in the fetal pancreas from noisy\nlive imaging confocal microscopy.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:35:44 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Arnavaz", "Kasra", ""], ["Krause", "Oswin", ""], ["Krivokapic", "Jelena M.", ""], ["Heilmann", "Silja", ""], ["B\u00e6rentzen", "Jakob Andreas", ""], ["Nyeng", "Pia", ""], ["Feragen", "Aasa", ""]]}, {"id": "2105.09783", "submitter": "Binh Nguyen-Thai", "authors": "Binh Nguyen-Thai, Vuong Le, Catherine Morgan, Nadia Badawi, Truyen\n  Tran, and Svetha Venkatesh", "title": "A Spatio-temporal Attention-based Model for Infant Movement Assessment\n  from Videos", "comments": "Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)", "journal-ref": null, "doi": "10.1109/JBHI.2021.3077957", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The absence or abnormality of fidgety movements of joints or limbs is\nstrongly indicative of cerebral palsy in infants. Developing computer-based\nmethods for assessing infant movements in videos is pivotal for improved\ncerebral palsy screening. Most existing methods use appearance-based features\nand are thus sensitive to strong but irrelevant signals caused by background\nclutter or a moving camera. Moreover, these features are computed over the\nwhole frame, thus they measure gross whole body movements rather than specific\njoint/limb motion.\n  Addressing these challenges, we develop and validate a new method for fidgety\nmovement assessment from consumer-grade videos using human poses extracted from\nshort clips. Human poses capture only relevant motion profiles of joints and\nlimbs and are thus free from irrelevant appearance artifacts. The dynamics and\ncoordination between joints are modeled using spatio-temporal graph\nconvolutional networks. Frames and body parts that contain discriminative\ninformation about fidgety movements are selected through a spatio-temporal\nattention mechanism. We validate the proposed model on the cerebral palsy\nscreening task using a real-life consumer-grade video dataset collected at an\nAustralian hospital through the Cerebral Palsy Alliance, Australia. Our\nexperiments show that the proposed method achieves the ROC-AUC score of 81.87%,\nsignificantly outperforming existing competing methods with better\ninterpretability.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:31:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Nguyen-Thai", "Binh", ""], ["Le", "Vuong", ""], ["Morgan", "Catherine", ""], ["Badawi", "Nadia", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2105.09787", "submitter": "Devleena Das", "authors": "Devleena Das, Yasutaka Nishimura, Rajan P. Vivek, Naoto Takeda, Sean\n  T. Fish, Thomas Ploetz, Sonia Chernova", "title": "Explainable Activity Recognition for Smart Home Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart home environments are designed to provide services that help improve\nthe quality of life for the occupant via a variety of sensors and actuators\ninstalled throughout the space. Many automated actions taken by a smart home\nare governed by the output of an underlying activity recognition system.\nHowever, activity recognition systems may not be perfectly accurate and\ntherefore inconsistencies in smart home operations can lead a user to wonder\n\"why did the smart home do that?\" In this work, we build on insights from\nExplainable Artificial Intelligence (XAI) techniques to contribute\ncomputational methods for explainable activity recognition. Specifically, we\ngenerate explanations for smart home activity recognition systems that explain\nwhat about an activity led to the given classification. To do so, we introduce\nfour computational techniques for generating natural language explanations of\nsmart home data and compare their effectiveness at generating meaningful\nexplanations. Through a study with everyday users, we evaluate user preferences\ntowards the four explanation types. Our results show that the leading approach,\nSHAP, has a 92% success rate in generating accurate explanations. Moreover, 84%\nof sampled scenarios users preferred natural language explanations over a\nsimple activity label, underscoring the need for explainable activity\nrecognition systems. Finally, we show that explanations generated by some XAI\nmethods can lead users to lose confidence in the accuracy of the underlying\nactivity recognition model, while others lead users to gain confidence. Taking\nall studied factors into consideration, we make a recommendation regarding\nwhich existing XAI method leads to the best performance in the domain of smart\nhome automation, and discuss a range of topics for future work in this area.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:35:51 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Das", "Devleena", ""], ["Nishimura", "Yasutaka", ""], ["Vivek", "Rajan P.", ""], ["Takeda", "Naoto", ""], ["Fish", "Sean T.", ""], ["Ploetz", "Thomas", ""], ["Chernova", "Sonia", ""]]}, {"id": "2105.09788", "submitter": "Ruiqi Liu", "authors": "Ruiqi Liu, Ganggang Xu, Zuofeng Shang", "title": "Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data is of an extraordinarily large size or physically stored in\ndifferent locations, the distributed nearest neighbor (NN) classifier is an\nattractive tool for classification. We propose a novel distributed adaptive NN\nclassifier for which the number of nearest neighbors is a tuning parameter\nstochastically chosen by a data-driven criterion. An early stopping rule is\nproposed when searching for the optimal tuning parameter, which not only speeds\nup the computation but also improves the finite sample performance of the\nproposed Algorithm. Convergence rate of excess risk of the distributed adaptive\nNN classifier is investigated under various sub-sample size compositions. In\nparticular, we show that when the sub-sample sizes are sufficiently large, the\nproposed classifier achieves the nearly optimal convergence rate. Effectiveness\nof the proposed approach is demonstrated through simulation studies as well as\nan empirical application to a real-world dataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:38:41 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Liu", "Ruiqi", ""], ["Xu", "Ganggang", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2105.09801", "submitter": "Shuangshuang Chen", "authors": "Shuangshuang Chen, Sihao Ding, Yiannis Karayiannidis, M{\\aa}rten\n  Bj\\\"orkman", "title": "Monte Carlo Filtering Objectives: A New Family of Variational Objectives\n  to Learn Generative Model and Neural Adaptive Proposal for Time Series", "comments": "A complete version of manuscript accepted by IJCAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning generative models and inferring latent trajectories have shown to be\nchallenging for time series due to the intractable marginal likelihoods of\nflexible generative models. It can be addressed by surrogate objectives for\noptimization. We propose Monte Carlo filtering objectives (MCFOs), a family of\nvariational objectives for jointly learning parametric generative models and\namortized adaptive importance proposals of time series. MCFOs extend the\nchoices of likelihood estimators beyond Sequential Monte Carlo in\nstate-of-the-art objectives, possess important properties revealing the factors\nfor the tightness of objectives, and allow for less biased and variant gradient\nestimates. We demonstrate that the proposed MCFOs and gradient estimations lead\nto efficient and stable model learning, and learned generative models well\nexplain data and importance proposals are more sample efficient on various\nkinds of time series data.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:56:30 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chen", "Shuangshuang", ""], ["Ding", "Sihao", ""], ["Karayiannidis", "Yiannis", ""], ["Bj\u00f6rkman", "M\u00e5rten", ""]]}, {"id": "2105.09821", "submitter": "Noor Awad", "authors": "Noor Awad, Neeratyoy Mallik, Frank Hutter", "title": "DEHB: Evolutionary Hyberband for Scalable, Robust and Efficient\n  Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern machine learning algorithms crucially rely on several design decisions\nto achieve strong performance, making the problem of Hyperparameter\nOptimization (HPO) more important than ever. Here, we combine the advantages of\nthe popular bandit-based HPO method Hyperband (HB) and the evolutionary search\napproach of Differential Evolution (DE) to yield a new HPO method which we call\nDEHB. Comprehensive results on a very broad range of HPO problems, as well as a\nwide range of tabular benchmarks from neural architecture search, demonstrate\nthat DEHB achieves strong performance far more robustly than all previous HPO\nmethods we are aware of, especially for high-dimensional problems with discrete\ninput dimensions. For example, DEHB is up to 1000x faster than random search.\nIt is also efficient in computational time, conceptually simple and easy to\nimplement, positioning it well to become a new default HPO method.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:13:30 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Awad", "Noor", ""], ["Mallik", "Neeratyoy", ""], ["Hutter", "Frank", ""]]}, {"id": "2105.09829", "submitter": "Yongfeng Zhang", "authors": "Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Yongfeng Zhang", "title": "Personalized Counterfactual Fairness in Recommendation", "comments": "10 pages. Accepted to ACM SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462966", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are gaining increasing and critical impacts on human and\nsociety since a growing number of users use them for information seeking and\ndecision making. Therefore, it is crucial to address the potential unfairness\nproblems in recommendations. Just like users have personalized preferences on\nitems, users' demands for fairness are also personalized in many scenarios.\nTherefore, it is important to provide personalized fair recommendations for\nusers to satisfy their personalized fairness demands. Besides, previous works\non fair recommendation mainly focus on association-based fairness. However, it\nis important to advance from associative fairness notions to causal fairness\nnotions for assessing fairness more properly in recommender systems. Based on\nthe above considerations, this paper focuses on achieving personalized\ncounterfactual fairness for users in recommender systems. To this end, we\nintroduce a framework for achieving counterfactually fair recommendations\nthrough adversary learning by generating feature-independent user embeddings\nfor recommendation. The framework allows recommender systems to achieve\npersonalized fairness for users while also covering non-personalized\nsituations. Experiments on two real-world datasets with shallow and deep\nrecommendation algorithms show that our method can generate fairer\nrecommendations for users with a desirable recommendation performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:24:34 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 15:52:26 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Li", "Yunqi", ""], ["Chen", "Hanxiong", ""], ["Xu", "Shuyuan", ""], ["Ge", "Yingqiang", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2105.09837", "submitter": "Junxiang Wang", "authors": "Junxiang Wang, Hongyi Li, Zheng Chai, Yongchao Wang, Yue Cheng and\n  Liang Zhao", "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on\n  Gradient-Free ADMM framework", "comments": "Junxiang Wang and Hongyi Li contribute equally to this work, and\n  Yongchao Wang and Liang Zhao are corresponding authors. This work is under\n  progress. arXiv admin note: substantial text overlap with arXiv:2009.02868", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graph Augmented Multi-layer Perceptron (GA-MLP) model is an attractive\nalternative to Graph Neural Networks (GNNs). This is because it is resistant to\nthe over-smoothing problem, and deeper GA-MLP models yield better performance.\nGA-MLP models are traditionally optimized by the Stochastic Gradient Descent\n(SGD). However, SGD suffers from the layer dependency problem, which prevents\nthe gradients of different layers of GA-MLP models from being calculated in\nparallel. In this paper, we propose a parallel deep learning Alternating\nDirection Method of Multipliers (pdADMM) framework to achieve model\nparallelism: parameters in each layer of GA-MLP models can be updated in\nparallel. The extended pdADMM-Q algorithm reduces communication cost by\nutilizing the quantization technique. Theoretical convergence to a critical\npoint of the pdADMM algorithm and the pdADMM-Q algorithm is provided with a\nsublinear convergence rate $o(1/k)$. Extensive experiments in six benchmark\ndatasets demonstrate that the pdADMM can lead to high speedup, and outperforms\nall the existing state-of-the-art comparison methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:37:42 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wang", "Junxiang", ""], ["Li", "Hongyi", ""], ["Chai", "Zheng", ""], ["Wang", "Yongchao", ""], ["Cheng", "Yue", ""], ["Zhao", "Liang", ""]]}, {"id": "2105.09848", "submitter": "Yanli Zhou", "authors": "Yanli Zhou, Brenden M. Lake", "title": "Flexible Compositional Learning of Structured Visual Concepts", "comments": "Please cite as: Zhou, Y. and Lake, B. M. (2021). Flexible\n  compositional learning of structured visual concepts. In Proceedings of the\n  43rd Annual Conference of the Cognitive Science Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are highly efficient learners, with the ability to grasp the meaning\nof a new concept from just a few examples. Unlike popular computer vision\nsystems, humans can flexibly leverage the compositional structure of the visual\nworld, understanding new concepts as combinations of existing concepts. In the\ncurrent paper, we study how people learn different types of visual\ncompositions, using abstract visual forms with rich relational structure. We\nfind that people can make meaningful compositional generalizations from just a\nfew examples in a variety of scenarios, and we develop a Bayesian program\ninduction model that provides a close fit to the behavioral data. Unlike past\nwork examining special cases of compositionality, our work shows how a single\ncomputational approach can account for many distinct types of compositional\ngeneralization.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:48:05 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhou", "Yanli", ""], ["Lake", "Brenden M.", ""]]}, {"id": "2105.09855", "submitter": "Lekshmi Ramesh", "authors": "Lekshmi Ramesh, Chandra R. Murthy, Himanshu Tyagi", "title": "Multiple Support Recovery Using Very Few Measurements Per Sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of multiple support recovery, we are given access to linear\nmeasurements of multiple sparse samples in $\\mathbb{R}^{d}$. These samples can\nbe partitioned into $\\ell$ groups, with samples having the same support\nbelonging to the same group. For a given budget of $m$ measurements per sample,\nthe goal is to recover the $\\ell$ underlying supports, in the absence of the\nknowledge of group labels. We study this problem with a focus on the\nmeasurement-constrained regime where $m$ is smaller than the support size $k$\nof each sample. We design a two-step procedure that estimates the union of the\nunderlying supports first, and then uses a spectral algorithm to estimate the\nindividual supports. Our proposed estimator can recover the supports with $m<k$\nmeasurements per sample, from $\\tilde{O}(k^{4}\\ell^{4}/m^{4})$ samples. Our\nguarantees hold for a general, generative model assumption on the samples and\nmeasurement matrices. We also provide results from experiments conducted on\nsynthetic data and on the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:02:27 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ramesh", "Lekshmi", ""], ["Murthy", "Chandra R.", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2105.09856", "submitter": "Patrick Lumban Tobing", "authors": "Patrick Lumban Tobing, Tomoki Toda", "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on\n  Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform\n  Modeling", "comments": "Accepted for INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7 GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:02:45 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 00:43:39 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tobing", "Patrick Lumban", ""], ["Toda", "Tomoki", ""]]}, {"id": "2105.09858", "submitter": "Patrick Lumban Tobing", "authors": "Patrick Lumban Tobing, Tomoki Toda", "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic\n  Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear\n  Prediction", "comments": "Accepted for SSW11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$ GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:06:11 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 00:59:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tobing", "Patrick Lumban", ""], ["Toda", "Tomoki", ""]]}, {"id": "2105.09866", "submitter": "Zhe Wang Dr", "authors": "Zhe Wang and Claude Guet", "title": "Deep learning in physics: a study of dielectric quasi-cubic particles in\n  a uniform electric field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.class-ph cond-mat.mtrl-sci cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Solving physics problems for which we know the equations, boundary conditions\nand symmetries can be done by deep learning. The constraints can be either\nimposed as terms in a loss function or used to formulate a neural ansatz. In\nthe present case study, we calculate the induced field inside and outside a\ndielectric cube placed in a uniform electric field, wherein the dielectric\nmismatch at edges and corners of the cube makes accurate calculations\nnumerically challenging. The electric potential is expressed as an ansatz\nincorporating neural networks with known leading order behaviors and symmetries\nand the Laplace's equation is then solved with boundary conditions at the\ndielectric interface by minimizing a loss function. The loss function ensures\nthat both Laplace's equation and boundary conditions are satisfied everywhere\ninside a large solution domain. We study how the electric potential inside and\noutside a quasi-cubic particle evolves through a sequence of shapes from a\nsphere to a cube. The neural network being differentiable, it is\nstraightforward to calculate the electric field over the whole domain, the\ninduced surface charge distribution and the polarizability. The neural network\nbeing retentive, one can efficiently follow how the field changes upon\nparticle's shape or dielectric constant by iterating from any previously\nconverged solution. The present work's objective is two-fold, first to show how\nan a priori knowledge can be incorporated into neural networks to achieve\nefficient learning and second to apply the method and study how the induced\nfield and polarizability change when a dielectric particle progressively\nchanges its shape from a sphere to a cube.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:40:03 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wang", "Zhe", ""], ["Guet", "Claude", ""]]}, {"id": "2105.09872", "submitter": "Seyoung Kim", "authors": "Jun Ho Yoon and Seyoung Kim", "title": "EiGLasso for Scalable Sparse Kronecker-Sum Inverse Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems, complex dependencies are present both among\nsamples and among features. The Kronecker sum or the Cartesian product of two\ngraphs, each modeling dependencies across features and across samples, has been\nused as an inverse covariance matrix for a matrix-variate Gaussian\ndistribution, as an alternative to a Kronecker-product inverse covariance\nmatrix, due to its more intuitive sparse structure. However, the existing\nmethods for sparse Kronecker-sum inverse covariance estimation are limited in\nthat they do not scale to more than a few hundred features and samples and that\nthe unidentifiable parameters pose challenges in estimation. In this paper, we\nintroduce EiGLasso, a highly scalable method for sparse Kronecker-sum inverse\ncovariance estimation, based on Newton's method combined with\neigendecomposition of the two graphs for exploiting the structure of Kronecker\nsum. EiGLasso further reduces computation time by approximating the Hessian\nbased on the eigendecomposition of the sample and feature graphs. EiGLasso\nachieves quadratic convergence with the exact Hessian and linear convergence\nwith the approximate Hessian. We describe a simple new approach to estimating\nthe unidentifiable parameters that generalizes the existing methods. On\nsimulated and real-world data, we demonstrate that EiGLasso achieves two to\nthree orders-of-magnitude speed-up compared to the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:22:50 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Yoon", "Jun Ho", ""], ["Kim", "Seyoung", ""]]}, {"id": "2105.09899", "submitter": "Ran Zhu", "authors": "Ran Zhu, Mingkun Yang, Wang Liu, Rujun Song, Bo Yan, Zhuoling Xiao", "title": "DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual\n  Odometry", "comments": "17 pages,14 figures, Neurocomputing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technology for Visual Odometry (VO) that estimates the position and\norientation of the moving object through analyzing the image sequences captured\nby on-board cameras, has been well investigated with the rising interest in\nautonomous driving. This paper studies monocular VO from the perspective of\nDeep Learning (DL). Unlike most current learning-based methods, our approach,\ncalled DeepAVO, is established on the intuition that features contribute\ndiscriminately to different motion patterns. Specifically, we present a novel\nfour-branch network to learn the rotation and translation by leveraging\nConvolutional Neural Networks (CNNs) to focus on different quadrants of optical\nflow input. To enhance the ability of feature selection, we further introduce\nan effective channel-spatial attention mechanism to force each branch to\nexplicitly distill related information for specific Frame to Frame (F2F) motion\nestimation. Experiments on various datasets involving outdoor driving and\nindoor walking scenarios show that the proposed DeepAVO outperforms the\nstate-of-the-art monocular methods by a large margin, demonstrating competitive\nperformance to the stereo VO algorithm and verifying promising potential for\ngeneralization.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:05:31 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhu", "Ran", ""], ["Yang", "Mingkun", ""], ["Liu", "Wang", ""], ["Song", "Rujun", ""], ["Yan", "Bo", ""], ["Xiao", "Zhuoling", ""]]}, {"id": "2105.09900", "submitter": "Luiz Giovanini", "authors": "Luiz Giovanini, Fabr\\'icio Ceschin, Mirela Silva, Aokun Chen,\n  Ramchandra Kulkarni, Sanjay Banda, Madison Lysaght, Heng Qiao, Nikolaos\n  Sapountzis, Ruimin Sun, Brandon Matthews, Dapeng Oliver Wu, Andr\\'e Gr\\'egio,\n  Daniela Oliveira", "title": "Computer Users Have Unique Yet Temporally Inconsistent Computer Usage\n  Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates whether computer usage profiles comprised of\nprocess-, network-, mouse- and keystroke-related events are unique and\ntemporally consistent in a naturalistic setting, discussing challenges and\nopportunities of using such profiles in applications of continuous\nauthentication. We collected ecologically-valid computer usage profiles from 28\nMS Windows 10 computer users over 8 weeks and submitted this data to\ncomprehensive machine learning analysis involving a diverse set of online and\noffline classifiers. We found that (i) computer usage profiles have the\npotential to uniquely characterize computer users (with a maximum F-score of\n99.94%); (ii) network-related events were the most useful features to properly\nrecognize profiles (95.14% of the top features distinguishing users being\nnetwork-related); (iii) user profiles were mostly inconsistent over the 8-week\ndata collection period, with 92.86% of users exhibiting drifts in terms of time\nand usage habits; and (iv) online models are better suited to handle computer\nusage profiles compared to offline models (maximum F-score for each approach\nwas 95.99% and 99.94%, respectively).\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:06:05 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Giovanini", "Luiz", ""], ["Ceschin", "Fabr\u00edcio", ""], ["Silva", "Mirela", ""], ["Chen", "Aokun", ""], ["Kulkarni", "Ramchandra", ""], ["Banda", "Sanjay", ""], ["Lysaght", "Madison", ""], ["Qiao", "Heng", ""], ["Sapountzis", "Nikolaos", ""], ["Sun", "Ruimin", ""], ["Matthews", "Brandon", ""], ["Wu", "Dapeng Oliver", ""], ["Gr\u00e9gio", "Andr\u00e9", ""], ["Oliveira", "Daniela", ""]]}, {"id": "2105.09903", "submitter": "Abhinav Valada", "authors": "Manav Madan, Peter Jakob, Tobias Schmid-Schirling, Abhinav Valada", "title": "Multi-Perspective Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view classification is inspired by the behavior of humans, especially\nwhen fine-grained features or in our case rarely occurring anomalies are to be\ndetected. Current contributions point to the problem of how high-dimensional\ndata can be fused. In this work, we build upon the deep support vector data\ndescription algorithm and address multi-perspective anomaly detection using\nthree different fusion techniques i.e. early fusion, late fusion, and late\nfusion with multiple decoders. We employ different augmentation techniques with\na denoising process to deal with scarce one-class data, which further improves\nthe performance (ROC AUC = 80\\%). Furthermore, we introduce the dices dataset\nthat consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g. drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed approach\nexceeds the state-of-the-art on both the MNIST and dices datasets. To the best\nof our knowledge, this is the first work that focuses on addressing\nmulti-perspective anomaly detection in images by jointly using different\nperspectives together with one single objective function for anomaly detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:07:36 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Madan", "Manav", ""], ["Jakob", "Peter", ""], ["Schmid-Schirling", "Tobias", ""], ["Valada", "Abhinav", ""]]}, {"id": "2105.09908", "submitter": "Filip Biljecki", "authors": "Wangyang Chen, Abraham Noah Wu, Filip Biljecki", "title": "Classification of Urban Morphology with Deep Learning: Application on\n  Urban Vitality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from the human's visual and intuitive perspective. We\ntake the first step to bridge the gap by proposing a deep learning-based\ntechnique to automatically classify road networks into four classes on a visual\nbasis. The method is implemented by generating an image of the street network\n(Colored Road Hierarchy Diagram), which we introduce in this paper, and\nclassifying it using a deep convolutional neural network (ResNet-34). The model\nachieves an overall classification accuracy of 0.875. Nine cities around the\nworld are selected as the study areas with their road networks acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: we apply\nour method in a case study of urban vitality prediction. An advanced tree-based\nregression model (LightGBM) is for the first time designated to establish the\nrelationship between morphological indices and vitality indicators. The effect\nof road network classification is found to be small but positively associated\nwith urban vitality. This work expands the toolkit of quantitative urban\nmorphology study with new techniques, supporting further studies in the future.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:53:31 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 13:48:38 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Chen", "Wangyang", ""], ["Wu", "Abraham Noah", ""], ["Biljecki", "Filip", ""]]}, {"id": "2105.09917", "submitter": "Aleksandr Beknazaryan", "authors": "Aleksandr Beknazaryan", "title": "Neural networks with superexpressive activations and integer weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An example of an activation function $\\sigma$ is given such that networks\nwith activations $\\{\\sigma, \\lfloor\\cdot\\rfloor\\}$, integer weights and a fixed\narchitecture depending on $d$ approximate continuous functions on $[0,1]^d$.\nThe range of integer weights required for $\\varepsilon$-approximation of\nH\\\"older continuous functions is derived, which leads to a convergence rate of\norder $n^{\\frac{-2\\beta}{2\\beta+d}}\\log_2n$ for neural network regression\nestimation of unknown $\\beta$-H\\\"older continuous function with given $n$\nsamples.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:29:08 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 06:40:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Beknazaryan", "Aleksandr", ""]]}, {"id": "2105.09930", "submitter": "Ellie Chio", "authors": "Sukhdeep S. Sodhi, Ellie Ka-In Chio, Ambarish Jash, Santiago\n  Onta\\~n\\'on, Ajit Apte, Ankit Kumar, Ayooluwakunmi Jeje, Dima Kuzmin, Harry\n  Fung, Heng-Tze Cheng, Jon Effrat, Tarush Bali, Nitin Jindal, Pei Cao,\n  Sarvjeet Singh, Senqiang Zhou, Tameen Khan, Amol Wankhede, Moustafa Alzantot,\n  Allen Wu, Tushar Chandra", "title": "Mondegreen: A Post-Processing Solution to Speech Recognition Error\n  Correction for Voice Search Queries", "comments": "Accepted in KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more and more online search queries come from voice, automatic speech\nrecognition becomes a key component to deliver relevant search results. Errors\nintroduced by automatic speech recognition (ASR) lead to irrelevant search\nresults returned to the user, thus causing user dissatisfaction. In this paper,\nwe introduce an approach, Mondegreen, to correct voice queries in text space\nwithout depending on audio signals, which may not always be available due to\nsystem constraints or privacy or bandwidth (for example, some ASR systems run\non-device) considerations. We focus on voice queries transcribed via several\nproprietary commercial ASR systems. These queries come from users making\ninternet, or online service search queries. We first present an analysis\nshowing how different the language distribution coming from user voice queries\nis from that in traditional text corpora used to train off-the-shelf ASR\nsystems. We then demonstrate that Mondegreen can achieve significant\nimprovements in increased user interaction by correcting user voice queries in\none of the largest search systems in Google. Finally, we see Mondegreen as\ncomplementing existing highly-optimized production ASR systems, which may not\nbe frequently retrained and thus lag behind due to vocabulary drifts.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:45:46 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Sodhi", "Sukhdeep S.", ""], ["Chio", "Ellie Ka-In", ""], ["Jash", "Ambarish", ""], ["Onta\u00f1\u00f3n", "Santiago", ""], ["Apte", "Ajit", ""], ["Kumar", "Ankit", ""], ["Jeje", "Ayooluwakunmi", ""], ["Kuzmin", "Dima", ""], ["Fung", "Harry", ""], ["Cheng", "Heng-Tze", ""], ["Effrat", "Jon", ""], ["Bali", "Tarush", ""], ["Jindal", "Nitin", ""], ["Cao", "Pei", ""], ["Singh", "Sarvjeet", ""], ["Zhou", "Senqiang", ""], ["Khan", "Tameen", ""], ["Wankhede", "Amol", ""], ["Alzantot", "Moustafa", ""], ["Wu", "Allen", ""], ["Chandra", "Tushar", ""]]}, {"id": "2105.09938", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika\n  and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He\n  and Dawn Song and Jacob Steinhardt", "title": "Measuring Coding Challenge Competence With APPS", "comments": "Code and the APPS dataset is available at\n  https://github.com/hendrycks/apps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While programming is one of the most broadly applicable skills in modern\nsociety, modern machine learning models still cannot code solutions to basic\nproblems. Despite its importance, there has been surprisingly little work on\nevaluating code generation, and it can be difficult to accurately assess code\ngeneration performance rigorously. To meet this challenge, we introduce APPS, a\nbenchmark for code generation. Unlike prior work in more restricted settings,\nour benchmark measures the ability of models to take an arbitrary natural\nlanguage specification and generate satisfactory Python code. Similar to how\ncompanies assess candidate software developers, we then evaluate models by\nchecking their generated code on test cases. Our benchmark includes 10,000\nproblems, which range from having simple one-line solutions to being\nsubstantial algorithmic challenges. We fine-tune large language models on both\nGitHub and our training set, and we find that the prevalence of syntax errors\nis decreasing exponentially as models improve. Recent models such as GPT-Neo\ncan pass approximately 20% of the test cases of introductory problems, so we\nfind that machine learning models are now beginning to learn how to code. As\nthe social significance of automatic code generation increases over the coming\nyears, our benchmark can provide an important measure for tracking\nadvancements.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:58:42 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 19:41:58 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hendrycks", "Dan", ""], ["Basart", "Steven", ""], ["Kadavath", "Saurav", ""], ["Mazeika", "Mantas", ""], ["Arora", "Akul", ""], ["Guo", "Ethan", ""], ["Burns", "Collin", ""], ["Puranik", "Samir", ""], ["He", "Horace", ""], ["Song", "Dawn", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2105.09945", "submitter": "Yimin Peng", "authors": "Yunlong Li, Yiming Peng, Dengzheng Zhang, Yingan Mai, Zhengrong Ruan", "title": "XGBoost energy consumption prediction based on multi-system data HVAC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The energy consumption of the HVAC system accounts for a significant portion\nof the energy consumption of the public building system, and using an efficient\nenergy consumption prediction model can assist it in carrying out effective\nenergy-saving transformation. Unlike the traditional energy consumption\nprediction model, this paper extracts features from large data sets using\nXGBoost, trains them separately to obtain multiple models, then fuses them with\nLightGBM's independent prediction results using MAE, infers energy consumption\nrelated variables, and successfully applies this model to the self-developed\nInternet of Things platform.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:41:17 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Li", "Yunlong", ""], ["Peng", "Yiming", ""], ["Zhang", "Dengzheng", ""], ["Mai", "Yingan", ""], ["Ruan", "Zhengrong", ""]]}, {"id": "2105.09966", "submitter": "Emily Sandford", "authors": "Emily Sandford, David Kipping, Michael Collins", "title": "On planetary systems as ordered sequences", "comments": "25 pages, 19 figures, accepted to MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stab1480", "report-no": null, "categories": "astro-ph.EP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A planetary system consists of a host star and one or more planets, arranged\ninto a particular configuration. Here, we consider what information belongs to\nthe configuration, or ordering, of 4286 Kepler planets in their 3277 planetary\nsystems. First, we train a neural network model to predict the radius and\nperiod of a planet based on the properties of its host star and the radii and\nperiod of its neighbors. The mean absolute error of the predictions of the\ntrained model is a factor of 2.1 better than the MAE of the predictions of a\nnaive model which draws randomly from dynamically allowable periods and radii.\nSecond, we adapt a model used for unsupervised part-of-speech tagging in\ncomputational linguistics to investigate whether planets or planetary systems\nfall into natural categories with physically interpretable \"grammatical rules.\"\nThe model identifies two robust groups of planetary systems: (1) compact\nmulti-planet systems and (2) systems around giant stars ($\\log{g} \\lesssim\n4.0$), although the latter group is strongly sculpted by the selection bias of\nthe transit method. These results reinforce the idea that planetary systems are\nnot random sequences -- instead, as a population, they contain predictable\npatterns that can provide insight into the formation and evolution of planetary\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:00:29 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Sandford", "Emily", ""], ["Kipping", "David", ""], ["Collins", "Michael", ""]]}, {"id": "2105.09967", "submitter": "Boaz Shmueli", "authors": "Boaz Shmueli, Soumya Ray, Lun-Wei Ku", "title": "Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on\n  Twitter", "comments": "To be published in ACL 2021. 7 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets with induced emotion labels are scarce but of utmost importance for\nmany NLP tasks. We present a new, automated method for collecting texts along\nwith their induced reaction labels. The method exploits the online use of\nreaction GIFs, which capture complex affective states. We show how to augment\nthe data with induced emotion and induced sentiment labels. We use our method\nto create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K\ntweets. We provide baselines for three new tasks, including induced sentiment\nprediction and multilabel classification of induced emotions. Our method and\ndataset open new research opportunities in emotion detection and affective\ncomputing.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:01:05 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Shmueli", "Boaz", ""], ["Ray", "Soumya", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "2105.09974", "submitter": "Juan Pedro Dominguez-Morales", "authors": "Lourdes Duran-Lopez, Juan P. Dominguez-Morales, Daniel\n  Gutierrez-Galan, Antonio Rios-Navarro, Angel Jimenez-Fernandez, Saturnino\n  Vicente-Diaz, Alejandro Linares-Barranco", "title": "Wide & Deep neural network model for patch aggregation in CNN-based\n  prostate cancer detection systems", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Prostate cancer (PCa) is one of the most commonly diagnosed cancer and one of\nthe leading causes of death among men, with almost 1.41 million new cases and\naround 375,000 deaths in 2020. Artificial Intelligence algorithms have had a\nhuge impact in medical image analysis, including digital histopathology, where\nConvolutional Neural Networks (CNNs) are used to provide a fast and accurate\ndiagnosis, supporting experts in this task. To perform an automatic diagnosis,\nprostate tissue samples are first digitized into gigapixel-resolution\nwhole-slide images. Due to the size of these images, neural networks cannot use\nthem as input and, therefore, small subimages called patches are extracted and\npredicted, obtaining a patch-level classification. In this work, a novel patch\naggregation method based on a custom Wide & Deep neural network model is\npresented, which performs a slide-level classification using the patch-level\nclasses obtained from a CNN. The malignant tissue ratio, a 10-bin malignant\nprobability histogram, the least squares regression line of the histogram, and\nthe number of malignant connected components are used by the proposed model to\nperform the classification. An accuracy of 94.24% and a sensitivity of 98.87%\nwere achieved, proving that the proposed system could aid pathologists by\nspeeding up the screening process and, thus, contribute to the fight against\nPCa.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:13:58 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Duran-Lopez", "Lourdes", ""], ["Dominguez-Morales", "Juan P.", ""], ["Gutierrez-Galan", "Daniel", ""], ["Rios-Navarro", "Antonio", ""], ["Jimenez-Fernandez", "Angel", ""], ["Vicente-Diaz", "Saturnino", ""], ["Linares-Barranco", "Alejandro", ""]]}, {"id": "2105.09980", "submitter": "Nikolaos Napoleon Vlassis", "authors": "Xiao Sun, Bahador Bahmani, Nikolaos N. Vlassis, WaiChing Sun, Yanxun\n  Xu", "title": "Data-driven discovery of interpretable causal relations for deep\n  learning material laws with uncertainty propagation", "comments": "43 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a computational framework that generates ensemble\npredictive mechanics models with uncertainty quantification (UQ). We first\ndevelop a causal discovery algorithm to infer causal relations among\ntime-history data measured during each representative volume element (RVE)\nsimulation through a directed acyclic graph (DAG). With multiple plausible sets\nof causal relationships estimated from multiple RVE simulations, the\npredictions are propagated in the derived causal graph while using a deep\nneural network equipped with dropout layers as a Bayesian approximation for\nuncertainty quantification. We select two representative numerical examples\n(traction-separation laws for frictional interfaces, elastoplasticity models\nfor granular assembles) to examine the accuracy and robustness of the proposed\ncausal discovery method for the common material law predictions in civil\nengineering applications.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:25:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sun", "Xiao", ""], ["Bahmani", "Bahador", ""], ["Vlassis", "Nikolaos N.", ""], ["Sun", "WaiChing", ""], ["Xu", "Yanxun", ""]]}, {"id": "2105.09983", "submitter": "Malek Mouhoub", "authors": "Wael Korani, Malek Mouhoub and Samira Sadaoui", "title": "Optimizing Neural Network Weights using Nature-Inspired Algorithms", "comments": "15 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study aims to optimize Deep Feedforward Neural Networks (DFNNs) training\nusing nature-inspired optimization algorithms, such as PSO, MTO, and its\nvariant called MTOCL. We show how these algorithms efficiently update the\nweights of DFNNs when learning from data. We evaluate the performance of DFNN\nfused with optimization algorithms using three Wisconsin breast cancer\ndatasets, Original, Diagnostic, and Prognosis, under different experimental\nscenarios. The empirical analysis demonstrates that MTOCL is the most\nperforming in most scenarios across the three datasets. Also, MTOCL is\ncomparable to past weight optimization algorithms for the original dataset, and\nsuperior for the other datasets, especially for the challenging Prognostic\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:32:29 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Korani", "Wael", ""], ["Mouhoub", "Malek", ""], ["Sadaoui", "Samira", ""]]}, {"id": "2105.09985", "submitter": "Pranjal Awasthi", "authors": "Flavien Prost, Pranjal Awasthi, Nick Blumm, Aditee Kumthekar, Trevor\n  Potter, Li Wei, Xuezhi Wang, Ed H. Chi, Jilin Chen, Alex Beutel", "title": "Measuring Model Fairness under Noisy Covariates: A Theoretical\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we study the problem of measuring the fairness of a machine\nlearning model under noisy information. Focusing on group fairness metrics, we\ninvestigate the particular but common situation when the evaluation requires\ncontrolling for the confounding effect of covariate variables. In a practical\nsetting, we might not be able to jointly observe the covariate and group\ninformation, and a standard workaround is to then use proxies for one or more\nof these variables. Prior works have demonstrated the challenges with using a\nproxy for sensitive attributes, and strong independence assumptions are needed\nto provide guarantees on the accuracy of the noisy estimates. In contrast, in\nthis work we study using a proxy for the covariate variable and present a\ntheoretical analysis that aims to characterize weaker conditions under which\naccurate fairness evaluation is possible.\n  Furthermore, our theory identifies potential sources of errors and decouples\nthem into two interpretable parts $\\gamma$ and $\\epsilon$. The first part\n$\\gamma$ depends solely on the performance of the proxy such as precision and\nrecall, whereas the second part $\\epsilon$ captures correlations between all\nthe variables of interest. We show that in many scenarios the error in the\nestimates is dominated by $\\gamma$ via a linear dependence, whereas the\ndependence on the correlations $\\epsilon$ only constitutes a lower order term.\nAs a result we expand the understanding of scenarios where measuring model\nfairness via proxies can be an effective approach. Finally, we compare, via\nsimulations, the theoretical upper-bounds to the distribution of simulated\nestimation errors and show that assuming some structure on the data, even weak,\nis key to significantly improve both theoretical guarantees and empirical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:36:28 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Prost", "Flavien", ""], ["Awasthi", "Pranjal", ""], ["Blumm", "Nick", ""], ["Kumthekar", "Aditee", ""], ["Potter", "Trevor", ""], ["Wei", "Li", ""], ["Wang", "Xuezhi", ""], ["Chi", "Ed H.", ""], ["Chen", "Jilin", ""], ["Beutel", "Alex", ""]]}, {"id": "2105.09987", "submitter": "Robert Amelard", "authors": "Robert Amelard, Eric T Hedge, Richard L Hughson", "title": "Temporal prediction of oxygen uptake dynamics from wearable sensors\n  during low-, moderate-, and heavy-intensity exercise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Oxygen consumption (VO$_2$) provides established clinical and physiological\nindicators of cardiorespiratory function and exercise capacity. However, VO$_2$\nmonitoring is largely limited to specialized laboratory settings, making its\nwidespread monitoring elusive. Here, we investigate temporal prediction of\nVO$_2$ from wearable sensors during cycle ergometer exercise using a temporal\nconvolutional network (TCN). Cardiorespiratory signals were acquired from a\nsmart shirt with integrated textile sensors alongside ground-truth VO$_2$ from\na metabolic system on twenty-two young healthy adults. Participants performed\none ramp-incremental and three pseudorandom binary sequence exercise protocols\nto assess a range of VO$_2$ dynamics. A TCN model was developed using causal\nconvolutions across an effective history length to model the time-dependent\nnature of VO$_2$. Optimal history length was determined through minimum\nvalidation loss across hyperparameter values. The best performing model encoded\n218 s history length (TCN-VO$_2$ A), with 187 s, 97 s, and 76 s yielding less\nthan 3% deviation from the optimal validation loss. TCN-VO$_2$ A showed strong\nprediction accuracy (mean, 95% CI) across all exercise intensities (-22\nml.min$^{-1}$, [-262, 218]), spanning transitions from low-moderate (-23\nml.min$^{-1}$, [-250, 204]), low-heavy (14 ml.min$^{-1}$, [-252, 280]),\nventilatory threshold-heavy (-49 ml.min$^{-1}$, [-274, 176]), and maximal (-32\nml.min$^{-1}$, [-261, 197]) exercise. Second-by-second classification of\nphysical activity across 16090 s of predicted VO$_2$ was able to discern\nbetween vigorous, moderate, and light activity with high accuracy (94.1%). This\nsystem enables quantitative aerobic activity monitoring in non-laboratory\nsettings across a range of exercise intensities using wearable sensors for\nmonitoring exercise prescription adherence and personal fitness.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:40:17 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Amelard", "Robert", ""], ["Hedge", "Eric T", ""], ["Hughson", "Richard L", ""]]}, {"id": "2105.09989", "submitter": "Gal Yona", "authors": "Guy N Rothblum, Gal Yona", "title": "Multi-group Agnostic PAC Learnability", "comments": "To appear in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An agnostic PAC learning algorithm finds a predictor that is competitive with\nthe best predictor in a benchmark hypothesis class, where competitiveness is\nmeasured with respect to a given loss function. However, its predictions might\nbe quite sub-optimal for structured subgroups of individuals, such as protected\ndemographic groups. Motivated by such fairness concerns, we study \"multi-group\nagnostic PAC learnability\": fixing a measure of loss, a benchmark class $\\H$\nand a (potentially) rich collection of subgroups $\\G$, the objective is to\nlearn a single predictor such that the loss experienced by every group $g \\in\n\\G$ is not much larger than the best possible loss for this group within $\\H$.\nUnder natural conditions, we provide a characterization of the loss functions\nfor which such a predictor is guaranteed to exist. For any such loss function\nwe construct a learning algorithm whose sample complexity is logarithmic in the\nsize of the collection $\\G$. Our results unify and extend previous positive and\nnegative results from the multi-group fairness literature, which applied for\nspecific loss functions.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:43:36 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Rothblum", "Guy N", ""], ["Yona", "Gal", ""]]}, {"id": "2105.09992", "submitter": "Mathieu Seurin", "authors": "Mathieu Seurin, Florian Strub, Philippe Preux, Olivier Pietquin", "title": "Don't Do What Doesn't Matter: Intrinsic Motivation with Action\n  Usefulness", "comments": "Accepted at Internationnal Joint Conference on Artificial\n  Intelligence (IJCAI'21) and Self-Supervision for Reinforcement Learning\n  Workshop (SSL-RL @ICLR'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse rewards are double-edged training signals in reinforcement learning:\neasy to design but hard to optimize. Intrinsic motivation guidances have thus\nbeen developed toward alleviating the resulting exploration problem. They\nusually incentivize agents to look for new states through novelty signals. Yet,\nsuch methods encourage exhaustive exploration of the state space rather than\nfocusing on the environment's salient interaction opportunities. We propose a\nnew exploration method, called Don't Do What Doesn't Matter (DoWhaM), shifting\nthe emphasis from state novelty to state with relevant actions. While most\nactions consistently change the state when used, \\textit{e.g.} moving the\nagent, some actions are only effective in specific states, \\textit{e.g.},\n\\emph{opening} a door, \\emph{grabbing} an object. DoWhaM detects and rewards\nactions that seldom affect the environment. We evaluate DoWhaM on the\nprocedurally-generated environment MiniGrid, against state-of-the-art methods\nand show that DoWhaM greatly reduces sample complexity.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:55:11 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 09:03:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Seurin", "Mathieu", ""], ["Strub", "Florian", ""], ["Preux", "Philippe", ""], ["Pietquin", "Olivier", ""]]}, {"id": "2105.09994", "submitter": "Anna Korba", "authors": "Anna Korba, Pierre-Cyril Aubin-Frankowski, Szymon Majewski, Pierre\n  Ablin", "title": "Kernel Stein Discrepancy Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among dissimilarities between probability distributions, the Kernel Stein\nDiscrepancy (KSD) has received much interest recently. We investigate the\nproperties of its Wasserstein gradient flow to approximate a target probability\ndistribution $\\pi$ on $\\mathbb{R}^d$, known up to a normalization constant.\nThis leads to a straightforwardly implementable, deterministic score-based\nmethod to sample from $\\pi$, named KSD Descent, which uses a set of particles\nto approximate $\\pi$. Remarkably, owing to a tractable loss function, KSD\nDescent can leverage robust parameter-free optimization schemes such as L-BFGS;\nthis contrasts with other popular particle-based schemes such as the Stein\nVariational Gradient Descent algorithm. We study the convergence properties of\nKSD Descent and demonstrate its practical relevance. However, we also highlight\nfailure cases by showing that the algorithm can get stuck in spurious local\nminima.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:05:23 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Korba", "Anna", ""], ["Aubin-Frankowski", "Pierre-Cyril", ""], ["Majewski", "Szymon", ""], ["Ablin", "Pierre", ""]]}, {"id": "2105.10005", "submitter": "C.-H. Huck Yang", "authors": "C.-H. Huck Yang, Mohit Chhabra, Y.-C. Liu, Quan Kong, Tomoaki\n  Yoshinaga, Tomokazu Murakami", "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments", "comments": "Accepted to IEEE ICIP 2021", "journal-ref": "2021 IEEE International Conference on Image Processing (ICIP)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physical processes, camera movement, and unpredictable environmental\nconditions like the presence of dust can induce noise and artifacts in video\nfeeds. We observe that popular unsupervised MOT methods are dependent on\nnoise-free inputs. We show that the addition of a small amount of artificial\nrandom noise causes a sharp degradation in model performance on benchmark\nmetrics. We resolve this problem by introducing a robust unsupervised\nmulti-object tracking (MOT) model: AttU-Net. The proposed single-head attention\nmodel helps limit the negative impact of noise by learning visual\nrepresentations at different segment scales. AttU-Net shows better unsupervised\nMOT tracking performance over variational inference-based state-of-the-art\nbaselines. We evaluate our method in the MNIST-MOT and the Atari game video\nbenchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''\nwhich consists of moving Japanese characters and ``Fashion-MNIST MOT'' to\nvalidate the effectiveness of the MOT models.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:38:03 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 14:29:32 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 01:36:22 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 06:52:21 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yang", "C. -H. Huck", ""], ["Chhabra", "Mohit", ""], ["Liu", "Y. -C.", ""], ["Kong", "Quan", ""], ["Yoshinaga", "Tomoaki", ""], ["Murakami", "Tomokazu", ""]]}, {"id": "2105.10011", "submitter": "Leonard Berrada", "authors": "Leonard Berrada, Andrew Zisserman, M. Pawan Kumar", "title": "Comment on Stochastic Polyak Step-Size: Performance of ALI-G", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a short note on the performance of the ALI-G algorithm (Berrada et\nal., 2020) as reported in (Loizou et al., 2021). ALI-G (Berrada et al., 2020)\nand SPS (Loizou et al., 2021) are both adaptations of the Polyak step-size to\noptimize machine learning models that can interpolate the training data. The\nmain algorithmic differences are that (1) SPS employs a multiplicative constant\nin the denominator of the learning-rate while ALI-G uses an additive constant,\nand (2) SPS uses an iteration-dependent maximal learning-rate while ALI-G uses\na constant one. There are also differences in the analysis provided by the two\nworks, with less restrictive assumptions proposed in (Loizou et al., 2021). In\ntheir experiments, (Loizou et al., 2021) did not use momentum for ALI-G (which\nis a standard part of the algorithm) or standard hyper-parameter tuning (for\ne.g. learning-rate and regularization). Hence this note as a reference for the\nimproved performance that ALI-G can obtain with well-chosen hyper-parameters.\nIn particular, we show that when training a ResNet-34 on CIFAR-10 and\nCIFAR-100, the performance of ALI-G can reach respectively 93.5% (+6%) and 76%\n(+8%) with a very small amount of tuning. Thus ALI-G remains a very competitive\nmethod for training interpolating neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:57:34 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Berrada", "Leonard", ""], ["Zisserman", "Andrew", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "2105.10013", "submitter": "Hugo Oliveira", "authors": "Marcos Vendramini and Hugo Oliveira and Alexei Machado and Jefersson\n  A. dos Santos", "title": "Opening Deep Neural Networks with Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identify\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:02:29 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 20:34:15 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 03:00:04 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Vendramini", "Marcos", ""], ["Oliveira", "Hugo", ""], ["Machado", "Alexei", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "2105.10014", "submitter": "Florence Carton", "authors": "Florence Carton, David Filliat, Jaonary Rabarisoa and Quoc Cuong Pham", "title": "Evaluating Robustness over High Level Driving Instruction for Autonomous\n  Driving", "comments": "Accepted to IV21, 32nd IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have witnessed increasingly high performance in the field\nof autonomous end-to-end driving. In particular, more and more research is\nbeing done on driving in urban environments, where the car has to follow high\nlevel commands to navigate. However, few evaluations are made on the ability of\nthese agents to react in an unexpected situation. Specifically, no evaluations\nare conducted on the robustness of driving agents in the event of a bad\nhigh-level command. We propose here an evaluation method, namely a benchmark\nthat allows to assess the robustness of an agent, and to appreciate its\nunderstanding of the environment through its ability to keep a safe behavior,\nregardless of the instruction.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:10:14 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Carton", "Florence", ""], ["Filliat", "David", ""], ["Rabarisoa", "Jaonary", ""], ["Pham", "Quoc Cuong", ""]]}, {"id": "2105.10018", "submitter": "Sandeep Manjanna", "authors": "Sandeep Manjanna and Ani Hsieh and Gregory Dudek", "title": "Scalable Multi-Robot System for Non-myopic Spatial Sampling", "comments": "2021 ICRA workshop on Robot Swarms in the Real World From Design to\n  Deployment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a distributed scalable multi-robot planning algorithm for\nnon-uniform sampling of quasi-static spatial fields. We address the problem of\nefficient data collection using multiple autonomous vehicles. In this paper, we\nare interested in analyzing the effect of communication between multiple\nrobots, acting independently, on the overall sampling performance of the team.\nOur focus is on distributed sampling problem where the robots are operating\nindependent of their teammates, but have the ability to communicate their\nstates to other neighbors with a constraint on the communication range. We\ndesign and apply an informed non-myopic path planning technique on multiple\nrobotic platforms to efficiently collect measurements from a spatial field. Our\nproposed approach is highly adaptive to challenging environments, growing team\nsize, and runs in real-time, which are the key features for any real-world\nscenario. The results show that our distributed sampling approach is able to\nachieve efficient sampling with minimal communication between the robots. We\nevaluate our approach in simulation over multiple distributions commonly\noccurring in nature and on the real-world data collected during a field trial.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:30:10 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Manjanna", "Sandeep", ""], ["Hsieh", "Ani", ""], ["Dudek", "Gregory", ""]]}, {"id": "2105.10019", "submitter": "Daniel Poh", "authors": "Daniel Poh, Bryan Lim, Stefan Zohren and Stephen Roberts", "title": "Enhancing Cross-Sectional Currency Strategies by Ranking Refinement with\n  Transformer-based Architectures", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.IR cs.LG q-fin.TR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of a cross-sectional currency strategy depends crucially on\naccurately ranking instruments prior to portfolio construction. While this\nranking step is traditionally performed using heuristics, or by sorting outputs\nproduced by pointwise regression or classification models, Learning to Rank\nalgorithms have recently presented themselves as competitive and viable\nalternatives. Despite improving ranking accuracy on average however, these\ntechniques do not account for the possibility that assets positioned at the\nextreme ends of the ranked list -- which are ultimately used to construct the\nlong/short portfolios -- can assume different distributions in the input space,\nand thus lead to sub-optimal strategy performance. Drawing from research in\nInformation Retrieval that demonstrates the utility of contextual information\nembedded within top-ranked documents to learn the query's characteristics to\nimprove ranking, we propose an analogous approach: exploiting the features of\nboth out- and under-performing instruments to learn a model for refining the\noriginal ranked list. Under a re-ranking framework, we adapt the Transformer\narchitecture to encode the features of extreme assets for refining our\nselection of long/short instruments obtained with an initial retrieval.\nBacktesting on a set of 31 currencies, our proposed methodology significantly\nboosts Sharpe ratios -- by approximately 20% over the original LTR algorithms\nand double that of traditional baselines.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:30:41 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Poh", "Daniel", ""], ["Lim", "Bryan", ""], ["Zohren", "Stefan", ""], ["Roberts", "Stephen", ""]]}, {"id": "2105.10037", "submitter": "Dripta S. Raychaudhuri", "authors": "Dripta S. Raychaudhuri, Sujoy Paul, Jeroen van Baar, Amit K.\n  Roy-Chowdhury", "title": "Cross-domain Imitation from Observations", "comments": "Accepted at ICML 2021 as a long presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imitation learning seeks to circumvent the difficulty in designing proper\nreward functions for training agents by utilizing expert behavior. With\nenvironments modeled as Markov Decision Processes (MDP), most of the existing\nimitation algorithms are contingent on the availability of expert\ndemonstrations in the same MDP as the one in which a new imitation policy is to\nbe learned. In this paper, we study the problem of how to imitate tasks when\nthere exist discrepancies between the expert and agent MDP. These discrepancies\nacross domains could include differing dynamics, viewpoint, or morphology; we\npresent a novel framework to learn correspondences across such domains.\nImportantly, in contrast to prior works, we use unpaired and unaligned\ntrajectories containing only states in the expert domain, to learn this\ncorrespondence. We utilize a cycle-consistency constraint on both the state\nspace and a domain agnostic latent space to do this. In addition, we enforce\nconsistency on the temporal position of states via a normalized position\nestimator function, to align the trajectories across the two domains. Once this\ncorrespondence is found, we can directly transfer the demonstrations on one\ndomain to the other and use it for imitation. Experiments across a wide variety\nof challenging domains demonstrate the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 21:08:25 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Raychaudhuri", "Dripta S.", ""], ["Paul", "Sujoy", ""], ["van Baar", "Jeroen", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2105.10049", "submitter": "Julian Whitman", "authors": "Julian Whitman, Matthew Travers, and Howie Choset", "title": "Learning Modular Robot Control Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To make a modular robotic system both capable and scalable, the controller\nmust be equally as modular as the mechanism. Given the large number of designs\nthat can be generated from even a small set of modules, it becomes impractical\nto create a new system-wide controller for each design. Instead, we construct a\nmodular control policy that handles a broad class of designs. We take the view\nthat a module is both form and function, i.e. both mechanism and controller. As\nthe modules are physically re-configured, the policy automatically\nre-configures to match the kinematic structure. This novel policy is trained\nwith a new model-based reinforcement learning algorithm, which interleaves\nmodel learning and trajectory optimization to guide policy learning for\nmultiple designs simultaneously. Training the policy on a varied set of designs\nteaches it how to adapt its behavior to the design. We show that the policy can\nthen generalize to a larger set of designs not seen during training. We\ndemonstrate one policy controlling many designs with different combinations of\nlegs and wheels to locomote both in simulation and on real robots.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 21:54:37 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Whitman", "Julian", ""], ["Travers", "Matthew", ""], ["Choset", "Howie", ""]]}, {"id": "2105.10056", "submitter": "Zhuangdi Zhu", "authors": "Zhuangdi Zhu, Junyuan Hong, Jiayu Zhou", "title": "Data-Free Knowledge Distillation for Heterogeneous Federated Learning", "comments": null, "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning, PMLR 139, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Federated Learning (FL) is a decentralized machine-learning paradigm, in\nwhich a global server iteratively averages the model parameters of local users\nwithout accessing their data. User heterogeneity has imposed significant\nchallenges to FL, which can incur drifted global models that are slow to\nconverge. Knowledge Distillation has recently emerged to tackle this issue, by\nrefining the server model using aggregated knowledge from heterogeneous users,\nother than directly averaging their model parameters. This approach, however,\ndepends on a proxy dataset, making it impractical unless such a prerequisite is\nsatisfied. Moreover, the ensemble knowledge is not fully utilized to guide\nlocal model learning, which may in turn affect the quality of the aggregated\nmodel. Inspired by the prior art, we propose a data-free knowledge\ndistillation} approach to address heterogeneous FL, where the server learns a\nlightweight generator to ensemble user information in a data-free manner, which\nis then broadcasted to users, regulating local training using the learned\nknowledge as an inductive bias. Empirical studies powered by theoretical\nimplications show that, our approach facilitates FL with better generalization\nperformance using fewer communication rounds, compared with the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 22:30:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 19:31:35 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhu", "Zhuangdi", ""], ["Hong", "Junyuan", ""], ["Zhou", "Jiayu", ""]]}, {"id": "2105.10059", "submitter": "Arhum Ishtiaq", "authors": "Arhum Ishtiaq, Sara Mahmood, Maheen Anees, Neha Mumtaz", "title": "Model Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With time, machine learning models have increased in their scope,\nfunctionality and size. Consequently, the increased functionality and size of\nsuch models requires high-end hardware to both train and provide inference\nafter the fact. This paper aims to explore the possibilities within the domain\nof model compression and discuss the efficiency of each of the possible\napproaches while comparing model size and performance with respect to pre- and\npost-compression.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 22:48:04 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ishtiaq", "Arhum", ""], ["Mahmood", "Sara", ""], ["Anees", "Maheen", ""], ["Mumtaz", "Neha", ""]]}, {"id": "2105.10065", "submitter": "Xin Qian", "authors": "Xin Qian, Diego Klabjan", "title": "A Probabilistic Approach to Neural Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network pruning techniques reduce the number of parameters without\ncompromising predicting ability of a network. Many algorithms have been\ndeveloped for pruning both over-parameterized fully-connected networks (FCNs)\nand convolutional neural networks (CNNs), but analytical studies of\ncapabilities and compression ratios of such pruned sub-networks are lacking. We\ntheoretically study the performance of two pruning techniques (random and\nmagnitude-based) on FCNs and CNNs. Given a target network {whose weights are\nindependently sampled from appropriate distributions}, we provide a universal\napproach to bound the gap between a pruned and the target network in a\nprobabilistic sense. The results establish that there exist pruned networks\nwith expressive power within any specified bound from the target network.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 23:19:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Qian", "Xin", ""], ["Klabjan", "Diego", ""]]}, {"id": "2105.10066", "submitter": "Pei Xu", "authors": "Pei Xu and Ioannis Karamouzas", "title": "A GAN-Like Approach for Physics-Based Imitation Learning and Interactive\n  Character Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple and intuitive approach for interactive control of\nphysically simulated characters. Our work builds upon generative adversarial\nnetworks (GAN) and reinforcement learning, and introduces an imitation learning\nframework where an ensemble of classifiers and an imitation policy are trained\nin tandem given pre-processed reference clips. The classifiers are trained to\ndiscriminate the reference motion from the motion generated by the imitation\npolicy, while the policy is rewarded for fooling the discriminators. Using our\nGAN-based approach, multiple motor control policies can be trained separately\nto imitate different behaviors. In runtime, our system can respond to external\ncontrol signal provided by the user and interactively switch between different\npolicies. Compared to existing methods, our proposed approach has the following\nattractive properties: 1) achieves state-of-the-art imitation performance\nwithout manually designing and fine tuning a reward function; 2) directly\ncontrols the character without having to track any target reference pose\nexplicitly or implicitly through a phase state; and 3) supports interactive\npolicy switching without requiring any motion generation or motion matching\nmechanism. We highlight the applicability of our approach in a range of\nimitation and interactive control tasks, while also demonstrating its ability\nto withstand external perturbations as well as to recover balance. Overall, our\napproach generates high-fidelity motion, has low runtime cost, and can be\neasily integrated into interactive applications and games.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 00:03:29 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xu", "Pei", ""], ["Karamouzas", "Ioannis", ""]]}, {"id": "2105.10067", "submitter": "Jacob Searcy", "authors": "Jacob A. Searcy and Susan L. Sokolowski", "title": "Towards Automatic Sizing for PPE with a Point Cloud Based Variational\n  Autoencoder", "comments": "4 pages, Short PEARC Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sizing and fitting of Personal Protective Equipment (PPE) is a critical part\nof the product creation process; however, traditional methods to do this type\nof work can be labor intensive and based on limited or non-representative\nanthropomorphic data. In the case of PPE, a poor fit can jeopardize an\nindividual's health and safety. In this paper we present an unsupervised\nmachine learning algorithm that can identify a representative set of exemplars,\nindividuals that can be utilized by designers as idealized sizing models. The\nalgorithm is based around a Variational Autoencoder (VAE) with a Point-Net\ninspired encoder and decoder architecture trained on Human point-cloud data\nobtained from the CEASAR dataset. The learned latent space is then clustered to\nidentify a specified number of sizing groups. We demonstrate this technique on\nscans of human faces to provide designers of masks and facial coverings a\nreference set of individuals to test existing mask styles.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 00:07:59 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Searcy", "Jacob A.", ""], ["Sokolowski", "Susan L.", ""]]}, {"id": "2105.10077", "submitter": "Leman Akoglu", "authors": "Leman Akoglu", "title": "Anomaly Mining -- Past, Present and Future", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Anomaly mining is an important problem that finds numerous applications in\nvarious real world domains such as environmental monitoring, cybersecurity,\nfinance, healthcare and medicine, to name a few. In this article, I focus on\ntwo areas, (1) point-cloud and (2) graph-based anomaly mining. I aim to present\na broad view of each area, and discuss classes of main research problems,\nrecent trends and future directions. I conclude with key take-aways and\noverarching open problems.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 00:56:25 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 13:29:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Akoglu", "Leman", ""]]}, {"id": "2105.10090", "submitter": "Dmitrii Avdiukhin", "authors": "Dmitrii Avdiukhin, Grigory Yaroslavtsev", "title": "Escaping Saddle Points with Compressed SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic gradient descent (SGD) is a prevalent optimization technique for\nlarge-scale distributed machine learning. While SGD computation can be\nefficiently divided between multiple machines, communication typically becomes\na bottleneck in the distributed setting. Gradient compression methods can be\nused to alleviate this problem, and a recent line of work shows that SGD\naugmented with gradient compression converges to an $\\varepsilon$-first-order\nstationary point. In this paper we extend these results to convergence to an\n$\\varepsilon$-second-order stationary point ($\\varepsilon$-SOSP), which is to\nthe best of our knowledge the first result of this type. In addition, we show\nthat, when the stochastic gradient is not Lipschitz, compressed SGD with\nRandomK compressor converges to an $\\varepsilon$-SOSP with the same number of\niterations as uncompressed SGD [Jin et al.,2021] (JACM), while improving the\ntotal communication by a factor of $\\tilde \\Theta(\\sqrt{d}\n\\varepsilon^{-3/4})$, where $d$ is the dimension of the optimization problem.\nWe present additional results for the cases when the compressor is arbitrary\nand when the stochastic gradient is Lipschitz.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 01:56:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Avdiukhin", "Dmitrii", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "2105.10100", "submitter": "Jiajia Guo", "authors": "Muhan Chen, Jiajia Guo, Chao-Kai Wen, Shi Jin, Geoffrey Ye Li, Ang\n  Yang", "title": "Deep Learning-based Implicit CSI Feedback in Massive MIMO", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive multiple-input multiple-output can obtain more performance gain by\nexploiting the downlink channel state information (CSI) at the base station\n(BS). Therefore, studying CSI feedback with limited communication resources in\nfrequency-division duplexing systems is of great importance. Recently, deep\nlearning (DL)-based CSI feedback has shown considerable potential. However, the\nexisting DL-based explicit feedback schemes are difficult to deploy because\ncurrent fifth-generation mobile communication protocols and systems are\ndesigned based on an implicit feedback mechanism. In this paper, we propose a\nDL-based implicit feedback architecture to inherit the low-overhead\ncharacteristic, which uses neural networks (NNs) to replace the precoding\nmatrix indicator (PMI) encoding and decoding modules. By using environment\ninformation, the NNs can achieve a more refined mapping between the precoding\nmatrix and the PMI compared with codebooks. The correlation between subbands is\nalso used to further improve the feedback performance. Simulation results show\nthat, for a single resource block (RB), the proposed architecture can save\n25.0% and 40.0% of overhead compared with Type I codebook under two antenna\nconfigurations, respectively. For a wideband system with 52 RBs, overhead can\nbe saved by 30.7% and 48.0% compared with Type II codebook when ignoring and\nconsidering extracting subband correlation, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 02:43:02 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Muhan", ""], ["Guo", "Jiajia", ""], ["Wen", "Chao-Kai", ""], ["Jin", "Shi", ""], ["Li", "Geoffrey Ye", ""], ["Yang", "Ang", ""]]}, {"id": "2105.10101", "submitter": "George Kesidis", "authors": "Hang Wang, David J. Miller, George Kesidis", "title": "Anomaly Detection of Test-Time Evasion Attacks using Class-conditional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been shown vulnerable to adversarial\n(Test-Time Evasion (TTE)) attacks which, by making small changes to the input,\nalter the DNN's decision. We propose an attack detector based on\nclass-conditional Generative Adversarial Networks (GANs). We model the\ndistribution of clean data conditioned on the predicted class label by an\nAuxiliary Classifier GAN (ACGAN). Given a test sample and its predicted class,\nthree detection statistics are calculated using the ACGAN Generator and\nDiscriminator. Experiments on image classification datasets under different TTE\nattack methods show that our method outperforms state-of-the-art detection\nmethods. We also investigate the effectiveness of anomaly detection using\ndifferent DNN layers (input features or internal-layer features) and\ndemonstrate that anomalies are harder to detect using features closer to the\nDNN's output layer.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 02:51:58 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Wang", "Hang", ""], ["Miller", "David J.", ""], ["Kesidis", "George", ""]]}, {"id": "2105.10102", "submitter": "He Zhang", "authors": "He Zhang, John Harlim, Xiantao Li", "title": "Error Bounds of the Invariant Statistics in Machine Learning of Ergodic\n  It\\^o Diffusions", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the theoretical underpinnings of machine learning of\nergodic It\\^o diffusions. The objective is to understand the convergence\nproperties of the invariant statistics when the underlying system of stochastic\ndifferential equations (SDEs) is empirically estimated with a supervised\nregression framework. Using the perturbation theory of ergodic Markov chains\nand the linear response theory, we deduce a linear dependence of the errors of\none-point and two-point invariant statistics on the error in the learning of\nthe drift and diffusion coefficients. More importantly, our study shows that\nthe usual $L^2$-norm characterization of the learning generalization error is\ninsufficient for achieving this linear dependence result. We find that\nsufficient conditions for such a linear dependence result are through learning\nalgorithms that produce a uniformly Lipschitz and consistent estimator in the\nhypothesis space that retains certain characteristics of the drift\ncoefficients, such as the usual linear growth condition that guarantees the\nexistence of solutions of the underlying SDEs. We examine these conditions on\ntwo well-understood learning algorithms: the kernel-based spectral regression\nmethod and the shallow random neural networks with the ReLU activation\nfunction.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 02:55:59 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 04:38:56 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "He", ""], ["Harlim", "John", ""], ["Li", "Xiantao", ""]]}, {"id": "2105.10113", "submitter": "Yu Li", "authors": "Yu Li, Min Li, Qiuxia Lai, Yannan Liu, and Qiang Xu", "title": "TestRank: Bringing Order into Unlabeled Test Instances for Deep Learning\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has achieved unprecedented success in a variety of tasks.\nHowever, DL systems are notoriously difficult to test and debug due to the lack\nof explainability of DL models and the huge test input space to cover.\nGenerally speaking, it is relatively easy to collect a massive amount of test\ndata, but the labeling cost can be quite high. Consequently, it is essential to\nconduct test selection and label only those selected \"high quality\"\nbug-revealing test inputs for test cost reduction.\n  In this paper, we propose a novel test prioritization technique that brings\norder into the unlabeled test instances according to their bug-revealing\ncapabilities, namely TestRank. Different from existing solutions, TestRank\nleverages both intrinsic attributes and contextual attributes of test instances\nwhen prioritizing them. To be specific, we first build a similarity graph on\ntest instances and training samples, and we conduct graph-based semi-supervised\nlearning to extract contextual features. Then, for a particular test instance,\nthe contextual features extracted from the graph neural network (GNN) and the\nintrinsic features obtained with the DL model itself are combined to predict\nits bug-revealing probability. Finally, TestRank prioritizes unlabeled test\ninstances in descending order of the above probability value. We evaluate the\nperformance of TestRank on a variety of image classification datasets.\nExperimental results show that the debugging efficiency of our method\nsignificantly outperforms existing test prioritization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 03:41:10 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Li", "Yu", ""], ["Li", "Min", ""], ["Lai", "Qiuxia", ""], ["Liu", "Yannan", ""], ["Xu", "Qiang", ""]]}, {"id": "2105.10118", "submitter": "Eric Wang", "authors": "Eric Wang, Pasha Khosravi, Guy Van den Broeck", "title": "Probabilistic Sufficient Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behavior of learned classifiers is an important task, and\nvarious black-box explanations, logical reasoning approaches, and\nmodel-specific methods have been proposed. In this paper, we introduce\nprobabilistic sufficient explanations, which formulate explaining an instance\nof classification as choosing the \"simplest\" subset of features such that only\nobserving those features is \"sufficient\" to explain the classification. That\nis, sufficient to give us strong probabilistic guarantees that the model will\nbehave similarly when all features are observed under the data distribution. In\naddition, we leverage tractable probabilistic reasoning tools such as\nprobabilistic circuits and expected predictions to design a scalable algorithm\nfor finding the desired explanations while keeping the guarantees intact. Our\nexperiments demonstrate the effectiveness of our algorithm in finding\nsufficient explanations, and showcase its advantages compared to Anchors and\nlogical explanations.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 04:03:10 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Wang", "Eric", ""], ["Khosravi", "Pasha", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "2105.10126", "submitter": "Prasanth Shyamsundar", "authors": "Doojin Kim, Kyoungchul Kong, Konstantin T. Matchev, Myeonghun Park,\n  Prasanth Shyamsundar", "title": "Deep-Learned Event Variables for Collider Phenomenology", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": "FERMILAB-PUB-21-244-QIS, MI-TH-2111", "categories": "hep-ph cs.LG hep-ex physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The choice of optimal event variables is crucial for achieving the maximal\nsensitivity of experimental analyses. Over time, physicists have derived\nsuitable kinematic variables for many typical event topologies in collider\nphysics. Here we introduce a deep learning technique to design good event\nvariables, which are sensitive over a wide range of values for the unknown\nmodel parameters. We demonstrate that the neural networks trained with our\ntechnique on some simple event topologies are able to reproduce standard event\nvariables like invariant mass, transverse mass, and stransverse mass. The\nmethod is automatable, completely general, and can be used to derive sensitive,\npreviously unknown, event variables for other, more complex event topologies.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 04:39:28 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kim", "Doojin", ""], ["Kong", "Kyoungchul", ""], ["Matchev", "Konstantin T.", ""], ["Park", "Myeonghun", ""], ["Shyamsundar", "Prasanth", ""]]}, {"id": "2105.10134", "submitter": "Matthew Wicker", "authors": "Matthew Wicker, Luca Laurenti, Andrea Patane, Nicola Paoletti,\n  Alessandro Abate, Marta Kwiatkowska", "title": "Certification of Iterative Predictions in Bayesian Neural Networks", "comments": "Accepted, UAI 2021. 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of computing reach-avoid probabilities for iterative\npredictions made with Bayesian neural network (BNN) models. Specifically, we\nleverage bound propagation techniques and backward recursion to compute lower\nbounds for the probability that trajectories of the BNN model reach a given set\nof states while avoiding a set of unsafe states. We use the lower bounds in the\ncontext of control and reinforcement learning to provide safety certification\nfor given control policies, as well as to synthesize control policies that\nimprove the certification bounds. On a set of benchmarks, we demonstrate that\nour framework can be employed to certify policies over BNNs predictions for\nproblems of more than $10$ dimensions, and to effectively synthesize policies\nthat significantly increase the lower bound on the satisfaction probability.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 05:23:57 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 19:20:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wicker", "Matthew", ""], ["Laurenti", "Luca", ""], ["Patane", "Andrea", ""], ["Paoletti", "Nicola", ""], ["Abate", "Alessandro", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "2105.10142", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng, Alois Knoll, Hsuan-Cheng Liao", "title": "Safety Metrics for Semantic Segmentation in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the context of autonomous driving, safety-related metrics for deep\nneural networks have been widely studied for image classification and object\ndetection. In this paper, we further consider safety-aware correctness and\nrobustness metrics specialized for semantic segmentation. The novelty of our\nproposal is to move beyond pixel-level metrics: Given two images with each\nhaving N pixels being class-flipped, the designed metrics should, depending on\nthe clustering of pixels being class-flipped or the location of occurrence,\nreflect a different level of safety criticality. The result evaluated on an\nautonomous driving dataset demonstrates the validity and practicality of our\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 05:59:49 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Cheng", "Chih-Hong", ""], ["Knoll", "Alois", ""], ["Liao", "Hsuan-Cheng", ""]]}, {"id": "2105.10148", "submitter": "Yutian Chen", "authors": "Yutian Chen, Liyuan Xu, Caglar Gulcehre, Tom Le Paine, Arthur Gretton,\n  Nando de Freitas, Arnaud Doucet", "title": "On Instrumental Variable Regression for Deep Offline Policy Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the popular reinforcement learning (RL) strategy of estimating\nthe state-action value (Q-function) by minimizing the mean squared Bellman\nerror leads to a regression problem with confounding, the inputs and output\nnoise being correlated. Hence, direct minimization of the Bellman error can\nresult in significantly biased Q-function estimates. We explain why fixing the\ntarget Q-network in Deep Q-Networks and Fitted Q Evaluation provides a way of\novercoming this confounding, thus shedding new light on this popular but not\nwell understood trick in the deep RL literature. An alternative approach to\naddress confounding is to leverage techniques developed in the causality\nliterature, notably instrumental variables (IV). We bring together here the\nliterature on IV and RL by investigating whether IV approaches can lead to\nimproved Q-function estimates. This paper analyzes and compares a wide range of\nrecent IV methods in the context of offline policy evaluation (OPE), where the\ngoal is to estimate the value of a policy using logged data only. By applying\ndifferent IV techniques to OPE, we are not only able to recover previously\nproposed OPE methods such as model-based techniques but also to obtain\ncompetitive new techniques. We find empirically that state-of-the-art OPE\nmethods are closely matched in performance by some IV methods such as AGMM,\nwhich were not developed for OPE. We open-source all our code and datasets at\nhttps://github.com/liyuan9988/IVOPEwithACME.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:22:34 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Yutian", ""], ["Xu", "Liyuan", ""], ["Gulcehre", "Caglar", ""], ["Paine", "Tom Le", ""], ["Gretton", "Arthur", ""], ["de Freitas", "Nando", ""], ["Doucet", "Arnaud", ""]]}, {"id": "2105.10162", "submitter": "Amandeep Bhatia", "authors": "Pinaki Sen and Amandeep Singh Bhatia", "title": "Variational Quantum Classifiers Through the Lens of the Hessian", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG math.QA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In quantum computing, the variational quantum algorithms (VQAs) are well\nsuited for finding optimal combinations of things in specific applications\nranging from chemistry all the way to finance. The training of VQAs with\ngradient descent optimization algorithm has shown a good convergence. At an\nearly stage, the simulation of variational quantum circuits on noisy\nintermediate-scale quantum (NISQ) devices suffers from noisy outputs. Just like\nclassical deep learning, it also suffers from vanishing gradient problems. It\nis a realistic goal to study the topology of loss landscape, to visualize the\ncurvature information and trainability of these circuits in the existence of\nvanishing gradients. In this paper, we calculated the Hessian and visualized\nthe loss landscape of variational quantum classifiers at different points in\nparameter space. The curvature information of variational quantum classifiers\n(VQC) is interpreted and the loss function's convergence is shown. It helps us\nbetter understand the behavior of variational quantum circuits to tackle\noptimization problems efficiently. We investigated the variational quantum\nclassifiers via Hessian on quantum computers, started with a simple 4-bit\nparity problem to gain insight into the practical behavior of Hessian, then\nthoroughly analyzed the behavior of Hessian's eigenvalues on training the\nvariational quantum classifier for the Diabetes dataset. Finally, we show that\nhow the adaptive Hessian learning rate can influence the convergence while\ntraining the variational circuits.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:57:34 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 15:06:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Sen", "Pinaki", ""], ["Bhatia", "Amandeep Singh", ""]]}, {"id": "2105.10165", "submitter": "Dipendra Misra", "authors": "Andrew Bennett, Dipendra Misra, and Nga Than", "title": "Have you tried Neural Topic Models? Comparative Analysis of Neural and\n  Non-Neural Topic Models with Application to COVID-19 Twitter Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are widely used in studying social phenomena. We conduct a\ncomparative study examining state-of-the-art neural versus non-neural topic\nmodels, performing a rigorous quantitative and qualitative assessment on a\ndataset of tweets about the COVID-19 pandemic. Our results show that not only\ndo neural topic models outperform their classical counterparts on standard\nevaluation metrics, but they also produce more coherent topics, which are of\ngreat benefit when studying complex social problems. We also propose a novel\nregularization term for neural topic models, which is designed to address the\nwell-documented problem of mode collapse, and demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:24:09 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Bennett", "Andrew", ""], ["Misra", "Dipendra", ""], ["Than", "Nga", ""]]}, {"id": "2105.10172", "submitter": "Katharina Beckh", "authors": "Katharina Beckh, Sebastian M\\\"uller, Matthias Jakobs, Vanessa Toborek,\n  Hanxiao Tan, Raphael Fischer, Pascal Welke, Sebastian Houben, Laura von\n  Rueden", "title": "Explainable Machine Learning with Prior Knowledge: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey presents an overview of integrating prior knowledge into machine\nlearning systems in order to improve explainability. The complexity of machine\nlearning models has elicited research to make them more explainable. However,\nmost explainability methods cannot provide insight beyond the given data,\nrequiring additional information about the context. We propose to harness prior\nknowledge to improve upon the explanation capabilities of machine learning\nmodels. In this paper, we present a categorization of current research into\nthree main categories which either integrate knowledge into the machine\nlearning pipeline, into the explainability method or derive knowledge from\nexplanations. To classify the papers, we build upon the existing taxonomy of\ninformed machine learning and extend it from the perspective of explainability.\nWe conclude with open challenges and research directions.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:33:22 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Beckh", "Katharina", ""], ["M\u00fcller", "Sebastian", ""], ["Jakobs", "Matthias", ""], ["Toborek", "Vanessa", ""], ["Tan", "Hanxiao", ""], ["Fischer", "Raphael", ""], ["Welke", "Pascal", ""], ["Houben", "Sebastian", ""], ["von Rueden", "Laura", ""]]}, {"id": "2105.10185", "submitter": "Tiago Pimentel", "authors": "Jennifer C. White, Tiago Pimentel, Naomi Saphra, Ryan Cotterell", "title": "A Non-Linear Structural Probe", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probes are models devised to investigate the encoding of knowledge -- e.g.\nsyntactic structure -- in contextual representations. Probes are often designed\nfor simplicity, which has led to restrictions on probe design that may not\nallow for the full exploitation of the structure of encoded information; one\nsuch restriction is linearity. We examine the case of a structural probe\n(Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic\nstructure in contextual representations through learning only linear\ntransformations. By observing that the structural probe learns a metric, we are\nable to kernelize it and develop a novel non-linear variant with an identical\nnumber of parameters. We test on 6 languages and find that the radial-basis\nfunction (RBF) kernel, in conjunction with regularization, achieves a\nstatistically significant improvement over the baseline in all languages --\nimplying that at least part of the syntactic knowledge is encoded non-linearly.\nWe conclude by discussing how the RBF kernel resembles BERT's self-attention\nlayers and speculate that this resemblance leads to the RBF-based probe's\nstronger performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:53:10 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["White", "Jennifer C.", ""], ["Pimentel", "Tiago", ""], ["Saphra", "Naomi", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2105.10190", "submitter": "Juan M Haut", "authors": "S.K. Roy, M.E. Paoletti, J.M. Haut, S.R. Dubey, P. Kar, A. Plaza, B.B.\n  Chaudhuri", "title": "AngularGrad: A New Optimization Technique for Angular Convergence of\n  Convolutional Neural Networks", "comments": "Submitted in IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are trained using stochastic gradient\ndescent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam)\noptimizer has become very popular due to its adaptive momentum, which tackles\nthe dying gradient problem of SGD. Nevertheless, existing optimizers are still\nunable to exploit the optimization curvature information efficiently. This\npaper proposes a new AngularGrad optimizer that considers the behavior of the\ndirection/angle of consecutive gradients. This is the first attempt in the\nliterature to exploit the gradient angular information apart from its\nmagnitude. The proposed AngularGrad generates a score to control the step size\nbased on the gradient angular information of previous iterations. Thus, the\noptimization steps become smoother as a more accurate step size of immediate\npast gradients is captured through the angular information. Two variants of\nAngularGrad are developed based on the use of Tangent or Cosine functions for\ncomputing the gradient angular information. Theoretically, AngularGrad exhibits\nthe same regret bound as Adam for convergence purposes. Nevertheless, extensive\nexperiments conducted on benchmark data sets against state-of-the-art methods\nreveal a superior performance of AngularGrad. The source code will be made\npublicly available at: https://github.com/mhaut/AngularGrad.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:00:53 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Roy", "S. K.", ""], ["Paoletti", "M. E.", ""], ["Haut", "J. M.", ""], ["Dubey", "S. R.", ""], ["Kar", "P.", ""], ["Plaza", "A.", ""], ["Chaudhuri", "B. B.", ""]]}, {"id": "2105.10193", "submitter": "Ayush Maheshwari", "authors": "Atul Sahay, Anshul Nasery, Ayush Maheshwari, Ganesh Ramakrishnan and\n  Rishabh Iyer", "title": "Rule Augmented Unsupervised Constituency Parsing", "comments": "Accepted at Findings of ACL 2021. 10 Pages, 5 Tables, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, unsupervised parsing of syntactic trees has gained considerable\nattention. A prototypical approach to such unsupervised parsing employs\nreinforcement learning and auto-encoders. However, no mechanism ensures that\nthe learnt model leverages the well-understood language grammar. We propose an\napproach that utilizes very generic linguistic knowledge of the language\npresent in the form of syntactic rules, thus inducing better syntactic\nstructures. We introduce a novel formulation that takes advantage of the\nsyntactic grammar rules and is independent of the base system. We achieve new\nstate-of-the-art results on two benchmarks datasets, MNLI and WSJ. The source\ncode of the paper is available at https://github.com/anshuln/Diora_with_rules.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:06:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sahay", "Atul", ""], ["Nasery", "Anshul", ""], ["Maheshwari", "Ayush", ""], ["Ramakrishnan", "Ganesh", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2105.10197", "submitter": "Lukas Heppe", "authors": "Katharina Morik and Helena Kotthaus and Lukas Heppe and Danny Heinrich\n  and Raphael Fischer and Sascha M\\\"ucke and Andreas Pauly and Matthias Jakobs\n  and Nico Piatkowski", "title": "Yes We Care! -- Certification for Machine Learning Methods through the\n  Care Label Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applications have become ubiquitous. Their applications from\nmachine embedded control in production over process optimization in diverse\nareas (e.g., traffic, finance, sciences) to direct user interactions like\nadvertising and recommendations. This has led to an increased effort of making\nmachine learning trustworthy. Explainable and fair AI have already matured.\nThey address knowledgeable users and application engineers. However, there are\nusers that want to deploy a learned model in a similar way as their washing\nmachine. These stakeholders do not want to spend time understanding the model.\nInstead, they want to rely on guaranteed properties. What are the relevant\nproperties? How can they be expressed to stakeholders without presupposing\nmachine learning knowledge? How can they be guaranteed for a certain\nimplementation of a model? These questions move far beyond the current\nstate-of-the-art and we want to address them here. We propose a unified\nframework that certifies learning methods via care labels. They are easy to\nunderstand and draw inspiration from well-known certificates like textile\nlabels or property cards of electronic devices. Our framework considers both,\nthe machine learning theory and a given implementation. We test the\nimplementation's compliance with theoretical properties and bounds. In this\npaper, we illustrate care labels by a prototype implementation of a\ncertification suite for a selection of probabilistic graphical models.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:15:21 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Morik", "Katharina", ""], ["Kotthaus", "Helena", ""], ["Heppe", "Lukas", ""], ["Heinrich", "Danny", ""], ["Fischer", "Raphael", ""], ["M\u00fccke", "Sascha", ""], ["Pauly", "Andreas", ""], ["Jakobs", "Matthias", ""], ["Piatkowski", "Nico", ""]]}, {"id": "2105.10213", "submitter": "Tobias Rohrer", "authors": "Tobias Rohrer, Jascha Kolberg", "title": "GAN pretraining for deep convolutional autoencoders applied to\n  Software-based Fingerprint Presentation Attack Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The need for reliable systems to determine fingerprint presentation attacks\ngrows with the rising use of the fingerprint for authentication. This work\npresents a new approach to single-class classification for software-based\nfingerprint presentation attach detection. The described method utilizes a\nWasserstein GAN to apply transfer learning to a deep convolutional autoencoder.\nBy doing so, the autoencoder could be pretrained and finetuned on the\nLivDet2021 Dermalog sensor dataset with only 1122 bona fide training samples.\nWithout making use of any presentation attack samples, the model could archive\nan average classification error rate of 16.79%. The Wasserstein GAN implemented\nto pretrain the autoencoders weights can further be used to generate\nrealistic-looking artificial fingerprint patches. Extensive testing of\ndifferent autoencoder architectures and hyperparameters led to coarse\narchitectural guidelines as well as multiple implementations which can be\nutilized for future work.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:08:34 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Rohrer", "Tobias", ""], ["Kolberg", "Jascha", ""]]}, {"id": "2105.10238", "submitter": "Anna Zapaishchykova", "authors": "Anna Zapaishchykova, David Dreizin, Zhaoshuo Li, Jie Ying Wu, Shahrooz\n  Faghih Roohi, Mathias Unberath", "title": "An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pelvic ring disruptions result from blunt injury mechanisms and are often\nfound in patients with multi-system trauma. To grade pelvic fracture severity\nin trauma victims based on whole-body CT, the Tile AO/OTA classification is\nfrequently used. Due to the high volume of whole-body trauma CTs generated in\nbusy trauma centers, an automated approach to Tile classification would provide\nsubstantial value, e.,g., to prioritize the reading queue of the attending\ntrauma radiologist. In such scenario, an automated method should perform\ngrading based on a transparent process and based on interpretable features to\nenable interaction with human readers and lower their workload by offering\ninsights from a first automated read of the scan. This paper introduces an\nautomated yet interpretable pelvic trauma decision support system to assist\nradiologists in fracture detection and Tile grade classification. The method\noperates similarly to human interpretation of CT scans and first detects\ndistinct pelvic fractures on CT with high specificity using a Faster-RCNN model\nthat are then interpreted using a structural causal model based on clinical\nbest practices to infer an initial Tile grade. The Bayesian causal model and\nfinally, the object detector are then queried for likely co-occurring fractures\nthat may have been rejected initially due to the highly specific operating\npoint of the detector, resulting in an updated list of detected fractures and\ncorresponding final Tile grade. Our method is transparent in that it provides\nfinding location and type using the object detector, as well as information on\nimportant counterfactuals that would invalidate the system's recommendation and\nachieves an AUC of 83.3%/85.1% for translational/rotational instability.\nDespite being designed for human-machine teaming, our approach does not\ncompromise on performance compared to previous black-box approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:52:33 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Zapaishchykova", "Anna", ""], ["Dreizin", "David", ""], ["Li", "Zhaoshuo", ""], ["Wu", "Jie Ying", ""], ["Roohi", "Shahrooz Faghih", ""], ["Unberath", "Mathias", ""]]}, {"id": "2105.10239", "submitter": "Shiv Ram Dubey", "authors": "Anirudh Ambati, Shiv Ram Dubey", "title": "AC-CovidNet: Attention Guided Contrastive CNN for Recognition of\n  Covid-19 in Chest X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covid-19 global pandemic continues to devastate health care systems across\nthe world. In many countries, the 2nd wave is very severe. Economical and rapid\ntesting, as well as diagnosis, is urgently needed to control the pandemic. At\npresent, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR)\ntesting can be the fastest, scalable, and non-invasive method. The existing\nmethods suffer due to the limited CXR samples available from Covid-19. Thus,\ninspired by the limitations of the open-source work in this field, we propose\nattention guided contrastive CNN architecture (AC-CovidNet) for Covid-19\ndetection in CXR images. The proposed method learns the robust and\ndiscriminative features with the help of contrastive loss. Moreover, the\nproposed method gives more importance to the infected regions as guided by the\nattention mechanism. We compute the sensitivity of the proposed method over the\npublicly available Covid-19 dataset. It is observed that the proposed\nAC-CovidNet exhibits very promising performance as compared to the existing\nmethods even with limited training data. It can tackle the bottleneck of CXR\nCovid-19 datasets being faced by the researchers. The code used in this paper\nis released publicly at \\url{https://github.com/shivram1987/AC-CovidNet/}.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:53:07 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ambati", "Anirudh", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "2105.10266", "submitter": "Carl-Johan Hoel", "authors": "Carl-Johan Hoel, Krister Wolff, Leo Laine", "title": "Ensemble Quantile Networks: Uncertainty-Aware Reinforcement Learning\n  with Applications in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) can be used to create a decision-making agent for\nautonomous driving. However, previous approaches provide only black-box\nsolutions, which do not offer information on how confident the agent is about\nits decisions. An estimate of both the aleatoric and epistemic uncertainty of\nthe agent's decisions is fundamental for real-world applications of autonomous\ndriving. Therefore, this paper introduces the Ensemble Quantile Networks (EQN)\nmethod, which combines distributional RL with an ensemble approach, to obtain a\ncomplete uncertainty estimate. The distribution over returns is estimated by\nlearning its quantile function implicitly, which gives the aleatoric\nuncertainty, whereas an ensemble of agents is trained on bootstrapped data to\nprovide a Bayesian estimation of the epistemic uncertainty. A criterion for\nclassifying which decisions that have an unacceptable uncertainty is also\nintroduced. The results show that the EQN method can balance risk and time\nefficiency in different occluded intersection scenarios, by considering the\nestimated aleatoric uncertainty. Furthermore, it is shown that the trained\nagent can use the epistemic uncertainty information to identify situations that\nthe agent has not been trained for and thereby avoid making unfounded,\npotentially dangerous, decisions outside of the training distribution.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 10:36:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Hoel", "Carl-Johan", ""], ["Wolff", "Krister", ""], ["Laine", "Leo", ""]]}, {"id": "2105.10267", "submitter": "Philipp Ennen", "authors": "Philipp Ennen, Yen-Ting Lin, Ali Girayhan Ozbay, Ferdinando Insalata,\n  Maolin Li, Ye Tian, Sepehr Jalali, Da-shan Shiu", "title": "Towards a Universal NLG for Dialogue Systems and Simulators with Future\n  Bridging", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a dialogue system pipeline, a natural language generation (NLG) unit\nconverts the dialogue direction and content to a corresponding natural language\nrealization. A recent trend for dialogue systems is to first pre-train on large\ndatasets and then fine-tune in a supervised manner using datasets annotated\nwith application-specific features. Though novel behaviours can be learned from\ncustom annotation, the required effort severely bounds the quantity of the\ntraining set, and the application-specific nature limits the reuse. In light of\nthe recent success of data-driven approaches, we propose the novel future\nbridging NLG (FBNLG) concept for dialogue systems and simulators. The critical\nstep is for an FBNLG to accept a future user or system utterance to bridge the\npresent context towards. Future bridging enables self supervised training over\nannotation-free datasets, decoupled the training of NLG from the rest of the\nsystem. An FBNLG, pre-trained with massive datasets, is expected to apply in\nclassical or new dialogue scenarios with minimal adaptation effort. We evaluate\na prototype FBNLG to show that future bridging can be a viable approach to a\nuniversal few-shot NLG for task-oriented and chit-chat dialogues.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 10:37:10 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 10:33:55 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ennen", "Philipp", ""], ["Lin", "Yen-Ting", ""], ["Ozbay", "Ali Girayhan", ""], ["Insalata", "Ferdinando", ""], ["Li", "Maolin", ""], ["Tian", "Ye", ""], ["Jalali", "Sepehr", ""], ["Shiu", "Da-shan", ""]]}, {"id": "2105.10272", "submitter": "Hema Karande", "authors": "Hema Karande, Rahee Walambe, Victor Benjamin, Ketan Kotecha and T. S.\n  Raghu", "title": "Stance Detection with BERT Embeddings for Credibility Analysis of\n  Information on Social Media", "comments": null, "journal-ref": null, "doi": "10.7717/peerj-cs.467", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The evolution of electronic media is a mixed blessing. Due to the easy\naccess, low cost, and faster reach of the information, people search out and\ndevour news from online social networks. In contrast, the increasing acceptance\nof social media reporting leads to the spread of fake news. This is a minacious\nproblem that causes disputes and endangers societal stability and harmony. Fake\nnews spread has gained attention from researchers due to its vicious nature.\nproliferation of misinformation in all media, from the internet to cable news,\npaid advertising and local news outlets, has made it essential for people to\nidentify the misinformation and sort through the facts. Researchers are trying\nto analyze the credibility of information and curtail false information on such\nplatforms. Credibility is the believability of the piece of information at\nhand. Analyzing the credibility of fake news is challenging due to the intent\nof its creation and the polychromatic nature of the news. In this work, we\npropose a model for detecting fake news. Our method investigates the content of\nthe news at the early stage i.e. when the news is published but is yet to be\ndisseminated through social media. Our work interprets the content with\nautomatic feature extraction and the relevance of the text pieces. In summary,\nwe introduce stance as one of the features along with the content of the\narticle and employ the pre-trained contextualized word embeddings BERT to\nobtain the state-of-art results for fake news detection. The experiment\nconducted on the real-world dataset indicates that our model outperforms the\nprevious work and enables fake news detection with an accuracy of 95.32%.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 10:46:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Karande", "Hema", ""], ["Walambe", "Rahee", ""], ["Benjamin", "Victor", ""], ["Kotecha", "Ketan", ""], ["Raghu", "T. S.", ""]]}, {"id": "2105.10277", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl", "title": "Maximum and Leaky Maximum Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an alternative to conventional residual connections,\nwhich is inspired by maxout nets. This means that instead of the addition in\nresidual connections, our approach only propagates the maximum value or, in the\nleaky formulation, propagates a percentage of both. In our evaluation, we show\non different public data sets that the presented approaches are comparable to\nthe residual connections and have other interesting properties, such as better\ngeneralization with a constant batch normalization, faster learning, and also\nthe possibility to generalize without additional activation functions. In\naddition, the proposed approaches work very well if ensembles together with\nresidual networks are formed.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 11:04:49 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Fuhl", "Wolfgang", ""]]}, {"id": "2105.10278", "submitter": "Yacine Izza", "authors": "Yacine Izza and Joao Marques-Silva", "title": "On Explaining Random Forests with SAT", "comments": "8 pages, 1 figure, 1 table, IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Random Forest (RFs) are among the most widely used Machine Learning (ML)\nclassifiers. Even though RFs are not interpretable, there are no dedicated\nnon-heuristic approaches for computing explanations of RFs. Moreover, there is\nrecent work on polynomial algorithms for explaining ML models, including naive\nBayes classifiers. Hence, one question is whether finding explanations of RFs\ncan be solved in polynomial time. This paper answers this question negatively,\nby proving that computing one PI-explanation of an RF is D^P-complete.\nFurthermore, the paper proposes a propositional encoding for computing\nexplanations of RFs, thus enabling finding PI-explanations with a SAT solver.\nThis contrasts with earlier work on explaining boosted trees (BTs) and neural\nnetworks (NNs), which requires encodings based on SMT/MILP. Experimental\nresults, obtained on a wide range of publicly available datasets, demontrate\nthat the proposed SAT-based approach scales to RFs of sizes common in practical\napplications. Perhaps more importantly, the experimental results demonstrate\nthat, for the vast majority of examples considered, the SAT-based approach\nproposed in this paper significantly outperforms existing heuristic approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 11:05:14 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Izza", "Yacine", ""], ["Marques-Silva", "Joao", ""]]}, {"id": "2105.10288", "submitter": "Mustafa Ayazoglu", "authors": "Mustafa Ayazoglu", "title": "Extremely Lightweight Quantization Robust Real-Time Single-Image Super\n  Resolution for Mobile Devices", "comments": null, "journal-ref": "IEEE Computer Vision Pattern Recognition Workshops (Mobile AI 2021\n  Workshop)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single-Image Super Resolution (SISR) is a classical computer vision problem\nand it has been studied for over decades. With the recent success of deep\nlearning methods, recent work on SISR focuses solutions with deep learning\nmethodologies and achieves state-of-the-art results. However most of the\nstate-of-the-art SISR methods contain millions of parameters and layers, which\nlimits their practical applications. In this paper, we propose a hardware\n(Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization\nrobust real-time super resolution network (XLSR). The proposed model's building\nblock is inspired from root modules for Image classification. We successfully\napplied root modules to SISR problem, further more to make the model uint8\nquantization robust we used Clipped ReLU at the last layer of the network and\nachieved great balance between reconstruction quality and runtime. Furthermore,\nalthough the proposed network contains 30x fewer parameters than VDSR its\nperformance surpasses it on Div2K validation set. The network proved itself by\nwinning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 11:29:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ayazoglu", "Mustafa", ""]]}, {"id": "2105.10302", "submitter": "Enrico Tabanelli", "authors": "Enrico Tabanelli, Davide Brunelli, Andrea Acquaviva, Luca Benini", "title": "Trimming Feature Extraction and Inference for MCU-based Edge NILM: a\n  Systematic Approach", "comments": null, "journal-ref": "Transactions on Industrial Informatics (2021)", "doi": "10.1109/TII.2021.3078186", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Intrusive Load Monitoring (NILM) enables the disaggregation of the global\npower consumption of multiple loads, taken from a single smart electrical\nmeter, into appliance-level details. State-of-the-Art approaches are based on\nMachine Learning methods and exploit the fusion of time- and frequency-domain\nfeatures from current and voltage sensors. Unfortunately, these methods are\ncompute-demanding and memory-intensive. Therefore, running low-latency NILM on\nlow-cost, resource-constrained MCU-based meters is currently an open challenge.\nThis paper addresses the optimization of the feature spaces as well as the\ncomputational and storage cost reduction needed for executing State-of-the-Art\n(SoA) NILM algorithms on memory- and compute-limited MCUs. We compare four\nsupervised learning techniques on different classification scenarios and\ncharacterize the overall NILM pipeline's implementation on a MCU-based Smart\nMeasurement Node. Experimental results demonstrate that optimizing the feature\nspace enables edge MCU-based NILM with 95.15% accuracy, resulting in a small\ndrop compared to the most-accurate feature vector deployment (96.19%) while\nachieving up to 5.45x speed-up and 80.56% storage reduction. Furthermore, we\nshow that low-latency NILM relying only on current measurements reaches almost\n80% accuracy, allowing a major cost reduction by removing voltage sensors from\nthe hardware design.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:08:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Tabanelli", "Enrico", ""], ["Brunelli", "Davide", ""], ["Acquaviva", "Andrea", ""], ["Benini", "Luca", ""]]}, {"id": "2105.10304", "submitter": "Leo Schwinn", "authors": "Leo Schwinn, Ren\\'e Raab, An Nguyen, Dario Zanca, Bjoern Eskofier", "title": "Exploring Misclassifications of Robust Neural Networks to Enhance\n  Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in making neural networks more robust against adversarial attacks is\nmostly marginal, despite the great efforts of the research community. Moreover,\nthe robustness evaluation is often imprecise, making it difficult to identify\npromising approaches. We analyze the classification decisions of 19 different\nstate-of-the-art neural networks trained to be robust against adversarial\nattacks. Our findings suggest that current untargeted adversarial attacks\ninduce misclassification towards only a limited amount of different classes.\nAdditionally, we observe that both over- and under-confidence in model\npredictions result in an inaccurate assessment of model robustness. Based on\nthese observations, we propose a novel loss function for adversarial attacks\nthat consistently improves attack success rate compared to prior loss functions\nfor 19 out of 19 analyzed models.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:10:38 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 07:43:47 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Schwinn", "Leo", ""], ["Raab", "Ren\u00e9", ""], ["Nguyen", "An", ""], ["Zanca", "Dario", ""], ["Eskofier", "Bjoern", ""]]}, {"id": "2105.10305", "submitter": "Mark Collier", "authors": "Mark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton and\n  Jesse Berent", "title": "Correlated Input-Dependent Label Noise in Large-Scale Image\n  Classification", "comments": "Accepted as Oral at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale image classification datasets often contain noisy labels. We take\na principled probabilistic approach to modelling input-dependent, also known as\nheteroscedastic, label noise in these datasets. We place a multivariate Normal\ndistributed latent variable on the final hidden layer of a neural network\nclassifier. The covariance matrix of this latent variable, models the aleatoric\nuncertainty due to label noise. We demonstrate that the learned covariance\nstructure captures known sources of label noise between semantically similar\nand co-occurring classes. Compared to standard neural network training and\nother baselines, we show significantly improved accuracy on Imagenet ILSVRC\n2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a\nnew state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These\ndatasets range from over 1M to over 300M training examples and from 1k classes\nto more than 21k classes. Our method is simple to use, and we provide an\nimplementation that is a drop-in replacement for the final fully-connected\nlayer in a deep classifier.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:30:59 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Collier", "Mark", ""], ["Mustafa", "Basil", ""], ["Kokiopoulou", "Efi", ""], ["Jenatton", "Rodolphe", ""], ["Berent", "Jesse", ""]]}, {"id": "2105.10315", "submitter": "Ruiqi Liu", "authors": "Ruiqi Liu, Mingao Yuan, Zuofeng Shang", "title": "Online Statistical Inference for Parameters Estimation with\n  Linear-Equality Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) and projected stochastic gradient descent\n(PSGD) are scalable algorithms to compute model parameters in unconstrained and\nconstrained optimization problems. In comparison with stochastic gradient\ndescent (SGD), PSGD forces its iterative values into the constrained parameter\nspace via projection. The convergence rate of PSGD-type estimates has been\nexhaustedly studied, while statistical properties such as asymptotic\ndistribution remain less explored. From a purely statistical point of view,\nthis paper studies the limiting distribution of PSGD-based estimate when the\ntrue parameters satisfying some linear-equality constraints. Our theoretical\nfindings reveal the role of projection played in the uncertainty of the PSGD\nestimate. As a byproduct, we propose an online hypothesis testing procedure to\ntest the linear-equality constraints. Simulation studies on synthetic data and\nan application to a real-world dataset confirm our theory.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:39:53 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liu", "Ruiqi", ""], ["Yuan", "Mingao", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2105.10325", "submitter": "Jana Kierdorf", "authors": "Jana Kierdorf, Immanuel Weber, Anna Kicherer, Laura Zabawa, Lukas\n  Drees, Ribana Roscher", "title": "Behind the leaves -- Estimation of occluded grapevine berries with\n  conditional generative adversarial networks", "comments": "45 pages, 18 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for accurate yield estimates for viticulture is becoming more\nimportant due to increasing competition in the wine market worldwide. One of\nthe most promising methods to estimate the harvest is berry counting, as it can\nbe approached non-destructively, and its process can be automated. In this\narticle, we present a method that addresses the challenge of occluded berries\nwith leaves to obtain a more accurate estimate of the number of berries that\nwill enable a better estimate of the harvest. We use generative adversarial\nnetworks, a deep learning-based approach that generates a likely scenario\nbehind the leaves exploiting learned patterns from images with non-occluded\nberries. Our experiments show that the estimate of the number of berries after\napplying our method is closer to the manually counted reference. In contrast to\napplying a factor to the berry count, our approach better adapts to local\nconditions by directly involving the appearance of the visible berries.\nFurthermore, we show that our approach can identify which areas in the image\nshould be changed by adding new berries without explicitly requiring\ninformation about hidden areas.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:57:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kierdorf", "Jana", ""], ["Weber", "Immanuel", ""], ["Kicherer", "Anna", ""], ["Zabawa", "Laura", ""], ["Drees", "Lukas", ""], ["Roscher", "Ribana", ""]]}, {"id": "2105.10335", "submitter": "Debasmit Das", "authors": "Debasmit Das, Yash Bhalgat and Fatih Porikli", "title": "Data-driven Weight Initialization with Sylvester Solvers", "comments": "Practical Machine Learning for Developing Countries Workshop,\n  International Conference on Learning Representations, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a data-driven scheme to initialize the parameters of\na deep neural network. This is in contrast to traditional approaches which\nrandomly initialize parameters by sampling from transformed standard\ndistributions. Such methods do not use the training data to produce a more\ninformed initialization. Our method uses a sequential layer-wise approach where\neach layer is initialized using its input activations. The initialization is\ncast as an optimization problem where we minimize a combination of encoding and\ndecoding losses of the input activations, which is further constrained by a\nuser-defined latent code. The optimization problem is then restructured into\nthe well-known Sylvester equation, which has fast and efficient gradient-free\nsolutions. Our data-driven method achieves a boost in performance compared to\nrandom initialization methods, both before start of training and after training\nis over. We show that our proposed method is especially effective in few-shot\nand fine-tuning settings. We conclude this paper with analyses on time\ncomplexity and the effect of different latent codes on the recognition\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 07:33:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Das", "Debasmit", ""], ["Bhalgat", "Yash", ""], ["Porikli", "Fatih", ""]]}, {"id": "2105.10341", "submitter": "Ivan Bajic", "authors": "Lior Bragilevsky and Ivan V. Baji\\'c", "title": "Error Resilient Collaborative Intelligence via Low-Rank Tensor\n  Completion", "comments": "2 pages, 1 figure, extended abstract for a poster at IEEE\n  Communication Theory Workshop (CTW) 2020 (moved to 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the race to bring Artificial Intelligence (AI) to the edge, collaborative\nintelligence has emerged as a promising way to lighten the computation load on\nedge devices that run applications based on Deep Neural Networks (DNNs).\nTypically, a deep model is split at a certain layer into edge and cloud\nsub-models. The deep feature tensor produced by the edge sub-model is\ntransmitted to the cloud, where the remaining computationally intensive\nworkload is performed by the cloud sub-model. The communication channel between\nthe edge and cloud is imperfect, which will result in missing data in the deep\nfeature tensor received at the cloud side. In this study, we examine the\neffectiveness of four low-rank tensor completion methods in recovering missing\ndata in the deep feature tensor. We consider both sparse tensors, such as those\nproduced by the VGG16 model, as well as non-sparse tensors, such as those\nproduced by ResNet34 model. We study tensor completion effectiveness in both\nconplexity-constrained and unconstrained scenario.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:47:25 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Bragilevsky", "Lior", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2105.10347", "submitter": "Inass Sekkat", "authors": "Inass Sekkat, Gabriel Stoltz", "title": "Removing the mini-batching error in Bayesian inference using Adaptive\n  Langevin dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational cost of usual Monte Carlo methods for sampling a posteriori\nlaws in Bayesian inference scales linearly with the number of data points. One\noption to reduce it to a fraction of this cost is to resort to mini-batching in\nconjunction with unadjusted discretizations of Langevin dynamics, in which case\nonly a random fraction of the data is used to estimate the gradient. However,\nthis leads to an additional noise in the dynamics and hence a bias on the\ninvariant measure which is sampled by the Markov chain. We advocate using the\nso-called Adaptive Langevin dynamics, which is a modification of standard\ninertial Langevin dynamics with a dynamical friction which automatically\ncorrects for the increased noise arising from mini-batching. We investigate the\npractical relevance of the assumptions underpinning Adaptive Langevin (constant\ncovariance for the estimation of the gradient), which are not satisfied in\ntypical models of Bayesian inference; and show how to extend the approach to\nmore general situations.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 13:39:39 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sekkat", "Inass", ""], ["Stoltz", "Gabriel", ""]]}, {"id": "2105.10350", "submitter": "Wenyu Chen", "authors": "Wenyu Chen, Mathias Drton and Ali Shojaie", "title": "Definite Non-Ancestral Relations and Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal graphical models based on directed acyclic graphs (DAGs), directed\npaths represent causal pathways between the corresponding variables. The\nvariable at the beginning of such a path is referred to as an ancestor of the\nvariable at the end of the path. Ancestral relations between variables play an\nimportant role in causal modeling. In existing literature on structure\nlearning, these relations are usually deduced from learned structures and used\nfor orienting edges or formulating constraints of the space of possible DAGs.\nHowever, they are usually not posed as immediate target of inference. In this\nwork we investigate the graphical characterization of ancestral relations via\nCPDAGs and d-separation relations. We propose a framework that can learn\ndefinite non-ancestral relations without first learning the skeleton. This\nframe-work yields structural information that can be used in both score- and\nconstraint-based algorithms to learn causal DAGs more efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 06:53:52 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Wenyu", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "2105.10358", "submitter": "Toshihisa Tanaka", "authors": "Taku Shoji, Noboru Yoshida, Toshihisa Tanaka", "title": "Automated Detection of Abnormalities from an EEG Recording of Epilepsy\n  Patients With a Compact Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) is essential for the diagnosis of epilepsy, but\nit requires expertise and experience to identify abnormalities. It is thus\ncrucial to develop automated models for the detection of abnormalities in EEGs\nrelated to epilepsy. This paper describes the development of a novel class of\ncompact convolutional neural networks (CNNs) for detecting abnormal patterns\nand electrodes in EEGs for epilepsy. The designed model is inspired by a CNN\ndeveloped for brain-computer interfacing called multichannel EEGNet (mEEGNet).\nUnlike the EEGNet, the proposed model, mEEGNet, has the same number of\nelectrode inputs and outputs to detect abnormal patterns. The mEEGNet was\nevaluated with a clinical dataset consisting of 29 cases of juvenile and\nchildhood absence epilepsy labeled by a clinical expert. The labels were given\nto paroxysmal discharges visually observed in both ictal (seizure) and\ninterictal (nonseizure) durations. Results showed that the mEEGNet detected\nabnormalities with the area under the curve, F1-values, and sensitivity\nequivalent to or higher than those of existing CNNs. Moreover, the number of\nparameters is much smaller than other CNN models. To our knowledge, the dataset\nof absence epilepsy validated with machine learning through this research is\nthe largest in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:52:56 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 12:59:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Shoji", "Taku", ""], ["Yoshida", "Noboru", ""], ["Tanaka", "Toshihisa", ""]]}, {"id": "2105.10360", "submitter": "Doudou Zhou", "authors": "Doudou Zhou, and Tianxi Cai, and Junwei Lu", "title": "BELT: Block-wise Missing Embedding Learning Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion has attracted attention in many fields, including\nstatistics, applied mathematics, and electrical engineering. Most of the works\nfocus on the independent sampling models under which the observed entries are\nsampled independently. Motivated by applications in the integration of multiple\nElectronic Health Record (EHR) datasets, we propose the method {\\bf B}lock-wise\nmissing {\\bf E}mbedding {\\bf L}earning {\\bf T}ransformer (BELT) to treat\nrow-wise/column-wise missingness. Specifically, BELT can recover block-wise\nmissing matrices efficiently when every pair of matrices has an overlap. Our\nidea is to exploit the orthogonal Procrustes problem to align the eigenspace of\nthe two sub-matrices using their overlap, then complete the missing blocks by\nthe inner product of the two low-rank components. Besides, we prove the\nstatistical rate for the eigenspace of the underlying matrix, which is\ncomparable to the rate under the independently missing assumption. Simulation\nstudies show that the method performs well under a variety of configurations.\nIn the real data analysis, the method is applied to two tasks: (i) the\nintegrating of several point-wise mutual information matrices built by English\nEHR and Chinese medical text data, and (ii) the machine translation between\nEnglish and Chinese medical concepts. Our method shows an advantage over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 13:55:30 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 12:20:03 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhou", "Doudou", ""], ["Cai", "Tianxi", ""], ["Lu", "Junwei", ""]]}, {"id": "2105.10368", "submitter": "Pedro A. Moreno-Sanchez PhD", "authors": "Pedro A. Moreno-Sanchez", "title": "An Explainable Classification Model for Chronic Kidney Disease Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, Chronic Kidney Disease (CKD) is experiencing a globally increasing\nincidence and high cost to health systems. A delayed recognition leads to\npremature mortality due to progressive loss of kidney function. The employment\nof data mining to discover subtle patterns in CKD indicators would contribute\nto an early diagnosis. This work develops a classifier model that would support\nhealthcare professionals in the early diagnosis of CKD patients. Through a data\npipeline, an exhaustive search is performed to find the best data mining\nclassifier with different parameters of the data preparation's sub-stages like\ndata missing or feature selection. Therefore, Extra Trees is selected as the\nbest classifier with a 100% and 99% of accuracy with, respectively,\ncross-validation technique and with new unseen data. Moreover, the 8 features\nselected are employed to assess the explainability of the model's results\ndenoting which features are more relevant in the model's output.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:09:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Moreno-Sanchez", "Pedro A.", ""]]}, {"id": "2105.10371", "submitter": "Antonio Ramires", "authors": "Pritish Chandna, Ant\\'onio Ramires, Xavier Serra, Emilia G\\'omez", "title": "LoopNet: Musical Loop Synthesis Conditioned On Intuitive Musical\n  Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Loops, seamlessly repeatable musical segments, are a cornerstone of modern\nmusic production. Contemporary artists often mix and match various sampled or\npre-recorded loops based on musical criteria such as rhythm, harmony and\ntimbral texture to create compositions. Taking such criteria into account, we\npresent LoopNet, a feed-forward generative model for creating loops conditioned\non intuitive parameters. We leverage Music Information Retrieval (MIR) models\nas well as a large collection of public loop samples in our study and use the\nWave-U-Net architecture to map control parameters to audio. We also evaluate\nthe quality of the generated audio and propose intuitive controls for composers\nto map the ideas in their minds to an audio loop.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:24:34 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chandna", "Pritish", ""], ["Ramires", "Ant\u00f3nio", ""], ["Serra", "Xavier", ""], ["G\u00f3mez", "Emilia", ""]]}, {"id": "2105.10373", "submitter": "Houssem Sifaou", "authors": "Houssem Sifaou, Abla kammoun, Mohamed-Slim Alouini", "title": "A Precise Performance Analysis of Support Vector Regression", "comments": "Accepted for publication at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the hard and soft support vector regression\ntechniques applied to a set of $n$ linear measurements of the form\n$y_i=\\boldsymbol{\\beta}_\\star^{T}{\\bf x}_i +n_i$ where\n$\\boldsymbol{\\beta}_\\star$ is an unknown vector, $\\left\\{{\\bf\nx}_i\\right\\}_{i=1}^n$ are the feature vectors and\n$\\left\\{{n}_i\\right\\}_{i=1}^n$ model the noise. Particularly, under some\nplausible assumptions on the statistical distribution of the data, we\ncharacterize the feasibility condition for the hard support vector regression\nin the regime of high dimensions and, when feasible, derive an asymptotic\napproximation for its risk. Similarly, we study the test risk for the soft\nsupport vector regression as a function of its parameters. Our results are then\nused to optimally tune the parameters intervening in the design of hard and\nsoft support vector regression algorithms. Based on our analysis, we illustrate\nthat adding more samples may be harmful to the test performance of support\nvector regression, while it is always beneficial when the parameters are\noptimally selected. Such a result reminds a similar phenomenon observed in\nmodern learning architectures according to which optimally tuned architectures\npresent a decreasing test performance curve with respect to the number of\nsamples.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:26:28 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sifaou", "Houssem", ""], ["kammoun", "Abla", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2105.10377", "submitter": "Andrea Apicella", "authors": "Andrea Apicella, Francesco Isgr\\`o, Andrea Pollastro, Roberto Prevete", "title": "Dynamic Filters in Graph Convolutional Neural Networks", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, we have seen increasing data generated from\nnon-Euclidean domains, which are usually represented as graphs with complex\nrelationships, and Graph Neural Networks (GNN) have gained a high interest\nbecause of their potential in processing graph-structured data. In particular,\nthere is a strong interest in exploring the possibilities in performing\nconvolution on graphs using an extension of the GNN architecture, generally\nreferred to as Graph Convolutional Neural Networks (GCNN). Convolution on\ngraphs has been achieved mainly in two forms: spectral and spatial\nconvolutions. Due to the higher flexibility in exploring and exploiting the\ngraph structure of data, recently, there is an increasing interest in\ninvestigating the possibilities that the spatial approach can offer. The idea\nof finding a way to adapt the network behaviour to the inputs they process to\nmaximize the total performances has aroused much interest in the neural\nnetworks literature over the years. This paper presents a novel method to adapt\nthe behaviour of a GCNN to the input proposing two ways to perform spatial\nconvolution on graphs using input-based filters which are dynamically\ngenerated. Our model also investigates the problem of discovering and refining\nrelations among nodes. The experimental assessment confirms the capabilities of\nthe proposed approach, which achieves satisfying results using simple\narchitectures with a low number of filters.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:36:39 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 07:39:31 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Apicella", "Andrea", ""], ["Isgr\u00f2", "Francesco", ""], ["Pollastro", "Andrea", ""], ["Prevete", "Roberto", ""]]}, {"id": "2105.10381", "submitter": "Eric Gaussier", "authors": "Karim Assaad, Emilie Devijver, Eric Gaussier, Ali Ait-Bachir", "title": "Entropy-based Discovery of Summary Causal Graphs in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address in this study the problem of learning a summary causal graph on\ntime series with potentially different sampling rates. To do so, we first\npropose a new temporal mutual information measure defined on a window-based\nrepresentation of time series. We then show how this measure relates to an\nentropy reduction principle that can be seen as a special case of the\nProbabilistic Raising Principle. We finally combine these two ingredients in a\nPC-like algorithm to construct the summary causal graph. This algorithm is\nevaluated on several datasets that shows both its efficacy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:47:18 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Assaad", "Karim", ""], ["Devijver", "Emilie", ""], ["Gaussier", "Eric", ""], ["Ait-Bachir", "Ali", ""]]}, {"id": "2105.10389", "submitter": "Xingyu Lin", "authors": "Xingyu Lin, Yufei Wang, David Held", "title": "Learning Visible Connectivity Dynamics for Cloth Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic manipulation of cloth remains challenging for robotics due to the\ncomplex dynamics of the cloth, lack of a low-dimensional state representation,\nand self-occlusions. In contrast to previous model-based approaches that learn\na pixel-based dynamics model or a compressed latent vector dynamics, we propose\nto learn a particle-based dynamics model from a partial point cloud\nobservation. To overcome the challenges of partial observability, we infer\nwhich visible points are connected on the underlying cloth mesh. We then learn\na dynamics model over this visible connectivity graph. Compared to previous\nlearning-based approaches, our model poses strong inductive bias with its\nparticle based representation for learning the underlying cloth physics; it is\ninvariant to visual features; and the predictions can be more easily\nvisualized. We show that our method greatly outperforms previous\nstate-of-the-art model-based and model-free reinforcement learning methods in\nsimulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we\ndeploy the model trained in simulation on a Franka arm and show that the model\ncan successfully smooth different types of cloth from crumpled configurations.\nVideos can be found on our project website.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:03:29 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Lin", "Xingyu", ""], ["Wang", "Yufei", ""], ["Held", "David", ""]]}, {"id": "2105.10400", "submitter": "Ozan Ozyegen", "authors": "Ozan Ozyegen, Devika Kabe and Mucahit Cevik", "title": "Word-level Text Highlighting of Medical Texts forTelehealth Services", "comments": "33 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medical domain is often subject to information overload. The digitization\nof healthcare, constant updates to online medical repositories, and increasing\navailability of biomedical datasets make it challenging to effectively analyze\nthe data. This creates additional work for medical professionals who are\nheavily dependent on medical data to complete their research and consult their\npatients. This paper aims to show how different text highlighting techniques\ncan capture relevant medical context. This would reduce the doctors' cognitive\nload and response time to patients by facilitating them in making faster\ndecisions, thus improving the overall quality of online medical services. Three\ndifferent word-level text highlighting methodologies are implemented and\nevaluated. The first method uses TF-IDF scores directly to highlight important\nparts of the text. The second method is a combination of TF-IDF scores and the\napplication of Local Interpretable Model-Agnostic Explanations to\nclassification models. The third method uses neural networks directly to make\npredictions on whether or not a word should be highlighted. The results of our\nexperiments show that the neural network approach is successful in highlighting\nmedically-relevant terms and its performance is improved as the size of the\ninput segment increases.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:13:54 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ozyegen", "Ozan", ""], ["Kabe", "Devika", ""], ["Cevik", "Mucahit", ""]]}, {"id": "2105.10414", "submitter": "Henry Kvinge", "authors": "Henry Kvinge, Brett Jefferson, Cliff Joslyn, Emilie Purvine", "title": "Sheaves as a Framework for Understanding and Interpreting Model Fit", "comments": "12 page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.AT math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data grows in size and complexity, finding frameworks which aid in\ninterpretation and analysis has become critical. This is particularly true when\ndata comes from complex systems where extensive structure is available, but\nmust be drawn from peripheral sources. In this paper we argue that in such\nsituations, sheaves can provide a natural framework to analyze how well a\nstatistical model fits at the local level (that is, on subsets of related\ndatapoints) vs the global level (on all the data). The sheaf-based approach\nthat we propose is suitably general enough to be useful in a range of\napplications, from analyzing sensor networks to understanding the feature space\nof a deep learning model.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:34:09 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kvinge", "Henry", ""], ["Jefferson", "Brett", ""], ["Joslyn", "Cliff", ""], ["Purvine", "Emilie", ""]]}, {"id": "2105.10424", "submitter": "Hufei Zhu", "authors": "Hufei Zhu", "title": "Low-Memory Implementations of Ridge Solutions for Broad Learning System\n  with Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing low-memory BLS implementation proposed recently avoids the need\nfor storing and inverting large matrices, to achieve efficient usage of\nmemories. However, the existing low-memory BLS implementation sacrifices the\ntesting accuracy as a price for efficient usage of memories, since it can no\nlonger obtain the generalized inverse or ridge solution for the output weights\nduring incremental learning, and it cannot work under the very small ridge\nparameter that is utilized in the original BLS. Accordingly, it is required to\ndevelop the low-memory BLS implementations, which can work under very small\nridge parameters and compute the generalized inverse or ridge solution for the\noutput weights in the process of incremental learning. In this paper, firstly\nwe propose the low-memory implementations for the recently proposed recursive\nand square-root BLS algorithms on added inputs and the recently proposed\nsquareroot BLS algorithm on added nodes, by simply processing a batch of inputs\nor nodes in each recursion. Since the recursive BLS implementation includes the\nrecursive updates of the inverse matrix that may introduce numerical\ninstabilities after a large number of iterations, and needs the extra\ncomputational load to decompose the inverse matrix into the Cholesky factor\nwhen cooperating with the proposed low-memory implementation of the square-root\nBLS algorithm on added nodes, we only improve the low-memory implementations of\nthe square-root BLS algorithms on added inputs and nodes, to propose the full\nlowmemory implementation of the square-root BLS algorithm. All the proposed\nlow-memory BLS implementations compute the ridge solution for the output\nweights in the process of incremental learning, and most of them can work under\nvery small ridge parameters.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:48:47 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 13:04:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhu", "Hufei", ""]]}, {"id": "2105.10430", "submitter": "Zihao Zhang", "authors": "Zihao Zhang, Stefan Zohren", "title": "Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning\n  Approaches and Hardware Acceleration using Intelligent Processing Units", "comments": "12 pages, 6 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design multi-horizon forecasting models for limit order book (LOB) data by\nusing deep learning techniques. Unlike standard structures where a single\nprediction is made, we adopt encoder-decoder models with sequence-to-sequence\nand Attention mechanisms, to generate a forecasting path. Our methods achieve\ncomparable performance to state-of-art algorithms at short prediction horizons.\nImportantly, they outperform when generating predictions over long horizons by\nleveraging the multi-horizon setup. Given that encoder-decoder models rely on\nrecurrent neural layers, they generally suffer from a slow training process. To\nremedy this, we experiment with utilising novel hardware, so-called Intelligent\nProcessing Units (IPUs) produced by Graphcore. IPUs are specifically designed\nfor machine intelligence workload with the aim to speed up the computation\nprocess. We show that in our setup this leads to significantly faster training\ntimes when compared to training models with GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:06:41 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Zhang", "Zihao", ""], ["Zohren", "Stefan", ""]]}, {"id": "2105.10439", "submitter": "Alexander Lin", "authors": "Alexander Lin, Andrew H. Song, Berkin Bilgic, and Demba Ba", "title": "Covariance-Free Sparse Bayesian Learning", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse Bayesian learning (SBL) is a powerful framework for tackling the\nsparse coding problem while also providing uncertainty quantification. However,\nthe most popular inference algorithms for SBL become too expensive for\nhigh-dimensional problems due to the need to maintain a large covariance\nmatrix. To resolve this issue, we introduce a new SBL inference algorithm that\navoids explicit computation of the covariance matrix, thereby saving\nsignificant time and space. Instead of performing costly matrix inversions, our\ncovariance-free method solves multiple linear systems to obtain provably\nunbiased estimates of the posterior statistics needed by SBL. These systems can\nbe solved in parallel, enabling further acceleration of the algorithm via\ngraphics processing units. In practice, our method can be up to thousands of\ntimes faster than existing baselines, reducing hours of computation time to\nseconds. We showcase how our new algorithm enables SBL to tractably tackle\nhigh-dimensional signal recovery problems, such as deconvolution of calcium\nimaging data and multi-contrast reconstruction of magnetic resonance images.\nFinally, we open-source a toolbox containing all of our implementations to\ndrive future research in SBL.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:20:07 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Lin", "Alexander", ""], ["Song", "Andrew H.", ""], ["Bilgic", "Berkin", ""], ["Ba", "Demba", ""]]}, {"id": "2105.10446", "submitter": "Yaodong Yu", "authors": "Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, Yi\n  Ma", "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate\n  Reduction", "comments": "This paper integrates previous two manuscripts: arXiv:2006.08558 and\n  arXiv:2010.14765, with significantly improved organization, presentation, and\n  new results; V2 polishes writing and adds citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work attempts to provide a plausible theoretical framework that aims to\ninterpret modern deep (convolutional) networks from the principles of data\ncompression and discriminative representation. We argue that for\nhigh-dimensional multi-class data, the optimal linear discriminative\nrepresentation maximizes the coding rate difference between the whole dataset\nand the average of all the subsets. We show that the basic iterative gradient\nascent scheme for optimizing the rate reduction objective naturally leads to a\nmulti-layer deep network, named ReduNet, which shares common characteristics of\nmodern deep networks. The deep layered architectures, linear and nonlinear\noperators, and even parameters of the network are all explicitly constructed\nlayer-by-layer via forward propagation, although they are amenable to\nfine-tuning via back propagation. All components of so-obtained ``white-box''\nnetwork have precise optimization, statistical, and geometric interpretation.\nMoreover, all linear operators of the so-derived network naturally become\nmulti-channel convolutions when we enforce classification to be rigorously\nshift-invariant. The derivation in the invariant setting suggests a trade-off\nbetween sparsity and invariance, and also indicates that such a deep\nconvolution network is significantly more efficient to construct and learn in\nthe spectral domain. Our preliminary simulations and experiments clearly verify\nthe effectiveness of both the rate reduction objective and the associated\nReduNet. All code and data are available at https://github.com/Ma-Lab-Berkeley.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:29:57 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 22:09:24 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chan", "Kwan Ho Ryan", ""], ["Yu", "Yaodong", ""], ["You", "Chong", ""], ["Qi", "Haozhi", ""], ["Wright", "John", ""], ["Ma", "Yi", ""]]}, {"id": "2105.10448", "submitter": "Ricardo Real", "authors": "Ric Real, James Gopsill, David Jones, Chris Snider, Ben Hicks", "title": "Distinguishing artefacts: evaluating the saturation point of\n  convolutional neural networks", "comments": "6 Pages, 5 Figures, 2 Tables, Conference, Design Engineering, CNN,\n  Digital Twin", "journal-ref": "January 2021 Procedia CIRP 100:385-390", "doi": "10.1016/j.procir.2021.05.089", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Prior work has shown Convolutional Neural Networks (CNNs) trained on\nsurrogate Computer Aided Design (CAD) models are able to detect and classify\nreal-world artefacts from photographs. The applications of which support\ntwinning of digital and physical assets in design, including rapid extraction\nof part geometry from model repositories, information search \\& retrieval and\nidentifying components in the field for maintenance, repair, and recording. The\nperformance of CNNs in classification tasks have been shown dependent on\ntraining data set size and number of classes. Where prior works have used\nrelatively small surrogate model data sets ($<100$ models), the question\nremains as to the ability of a CNN to differentiate between models in\nincreasingly large model repositories. This paper presents a method for\ngenerating synthetic image data sets from online CAD model repositories, and\nfurther investigates the capacity of an off-the-shelf CNN architecture trained\non synthetic data to classify models as class size increases. 1,000 CAD models\nwere curated and processed to generate large scale surrogate data sets,\nfeaturing model coverage at steps of 10$^{\\circ}$, 30$^{\\circ}$, 60$^{\\circ}$,\nand 120$^{\\circ}$ degrees. The findings demonstrate the capability of computer\nvision algorithms to classify artefacts in model repositories of up to 200,\nbeyond this point the CNN's performance is observed to deteriorate\nsignificantly, limiting its present ability for automated twinning of physical\nto digital artefacts. Although, a match is more often found in the top-5\nresults showing potential for information search and retrieval on large\nrepositories of surrogate models.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:33:20 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Real", "Ric", ""], ["Gopsill", "James", ""], ["Jones", "David", ""], ["Snider", "Chris", ""], ["Hicks", "Ben", ""]]}, {"id": "2105.10457", "submitter": "Aissatou Diallo", "authors": "A\\\"issatou Diallo and Johannes F\\\"urnkranz", "title": "Elliptical Ordinal Embedding", "comments": "14 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ordinal embedding aims at finding a low dimensional representation of objects\nfrom a set of constraints of the form \"item $j$ is closer to item $i$ than item\n$k$\". Typically, each object is mapped onto a point vector in a low dimensional\nmetric space. We argue that mapping to a density instead of a point vector\nprovides some interesting advantages, including an inherent reflection of the\nuncertainty about the representation itself and its relative location in the\nspace. Indeed, in this paper, we propose to embed each object as a Gaussian\ndistribution. We investigate the ability of these embeddings to capture the\nunderlying structure of the data while satisfying the constraints, and explore\nproperties of the representation. Experiments on synthetic and real-world\ndatasets showcase the advantages of our approach. In addition, we illustrate\nthe merit of modelling uncertainty, which enriches the visual perception of the\nmapped objects in the space.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:54:53 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 16:45:02 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Diallo", "A\u00efssatou", ""], ["F\u00fcrnkranz", "Johannes", ""]]}, {"id": "2105.10475", "submitter": "Jing Wang", "authors": "Atsushi Suzuki, Atsushi Nitanda, Jing Wang, Linchuan Xu, Marc Cavazza,\n  Kenji Yamanishi", "title": "Generalization Error Bound for Hyperbolic Ordinal Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperbolic ordinal embedding (HOE) represents entities as points in\nhyperbolic space so that they agree as well as possible with given constraints\nin the form of entity i is more similar to entity j than to entity k. It has\nbeen experimentally shown that HOE can obtain representations of hierarchical\ndata such as a knowledge base and a citation network effectively, owing to\nhyperbolic space's exponential growth property. However, its theoretical\nanalysis has been limited to ideal noiseless settings, and its generalization\nerror in compensation for hyperbolic space's exponential representation ability\nhas not been guaranteed. The difficulty is that existing generalization error\nbound derivations for ordinal embedding based on the Gramian matrix do not work\nin HOE, since hyperbolic space is not inner-product space. In this paper,\nthrough our novel characterization of HOE with decomposed Lorentz Gramian\nmatrices, we provide a generalization error bound of HOE for the first time,\nwhich is at most exponential with respect to the embedding space's radius. Our\ncomparison between the bounds of HOE and Euclidean ordinal embedding shows that\nHOE's generalization error is reasonable as a cost for its exponential\nrepresentation ability.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:31:08 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Suzuki", "Atsushi", ""], ["Nitanda", "Atsushi", ""], ["Wang", "Jing", ""], ["Xu", "Linchuan", ""], ["Cavazza", "Marc", ""], ["Yamanishi", "Kenji", ""]]}, {"id": "2105.10478", "submitter": "Zichuan Liu", "authors": "Zichuan Liu, Rui Zhang, Chen Wang, Hongbo Jiang", "title": "Spatial-Temporal Conv-sequence Learning with Accident Encoding for\n  Traffic Flow Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In intelligent transportation system, the key problem of traffic forecasting\nis how to extract the periodic temporal dependencies and complex spatial\ncorrelation. Current state-of-the-art methods for traffic flow prediction are\nbased on graph architectures and sequence learning models, but they do not\nfully exploit spatial-temporal dynamic information in traffic system.\nSpecifically, the temporal dependence of short-range is diluted by recurrent\nneural networks, and existing sequence model ignores local spatial information\nbecause the convolution operation uses global average pooling. Besides, there\nwill be some traffic accidents during the transitions of objects causing\ncongestion in the real world that trigger increased prediction deviation. To\novercome these challenges, we propose the Spatial-Temporal Conv-sequence\nLearning (STCL), in which a focused temporal block uses unidirectional\nconvolution to effectively capture short-term periodic temporal dependence, and\na spatial-temporal fusion module is able to extract the dependencies of both\ninteractions and decrease the feature dimensions. Moreover, the accidents\nfeatures impact on local traffic congestion and position encoding is employed\nto detect anomalies in complex traffic situations. We conduct extensive\nexperiments on large-scale real-world tasks and verify the effectiveness of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:43:07 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liu", "Zichuan", ""], ["Zhang", "Rui", ""], ["Wang", "Chen", ""], ["Jiang", "Hongbo", ""]]}, {"id": "2105.10488", "submitter": "Stephen Bonner", "authors": "Stephen Bonner and Ian P Barrett and Cheng Ye and Rowan Swiers and Ola\n  Engkvist and Charles Tapley Hoyt and William L Hamilton", "title": "Understanding the Performance of Knowledge Graph Embeddings in Drug\n  Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge Graphs (KG) and associated Knowledge Graph Embedding (KGE) models\nhave recently begun to be explored in the context of drug discovery and have\nthe potential to assist in key challenges such as target identification. In the\ndrug discovery domain, KGs can be employed as part of a process which can\nresult in lab-based experiments being performed, or impact on other decisions,\nincurring significant time and financial costs and most importantly, ultimately\ninfluencing patient healthcare. For KGE models to have impact in this domain, a\nbetter understanding of not only of performance, but also the various factors\nwhich determine it, is required.\n  In this study we investigate, over the course of many thousands of\nexperiments, the predictive performance of five KGE models on two public drug\ndiscovery-oriented KGs. Our goal is not to focus on the best overall model or\nconfiguration, instead we take a deeper look at how performance can be affected\nby changes in the training setup, choice of hyperparameters, model parameter\ninitialisation seed and different splits of the datasets. Our results highlight\nthat these factors have significant impact on performance and can even affect\nthe ranking of models. Indeed these factors should be reported along with model\narchitectures to ensure complete reproducibility and fair comparisons of future\nwork, and we argue this is critical for the acceptance of use, and impact of\nKGEs in a biomedical setting. To aid reproducibility of our own work, we\nrelease all experimentation code.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:39:54 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 09:50:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bonner", "Stephen", ""], ["Barrett", "Ian P", ""], ["Ye", "Cheng", ""], ["Swiers", "Rowan", ""], ["Engkvist", "Ola", ""], ["Hoyt", "Charles Tapley", ""], ["Hamilton", "William L", ""]]}, {"id": "2105.10489", "submitter": "Sutanay Choudhury", "authors": "Jenna Bilbrey, Logan Ward, Sutanay Choudhury, Neeraj Kumar, Ganesh\n  Sivaraman", "title": "Evening the Score: Targeting SARS-CoV-2 Protease Inhibition in Graph\n  Generative Models for Therapeutic Candidates", "comments": "arXiv admin note: substantial text overlap with arXiv:2102.04977", "journal-ref": "Published at ICLR 2021 Workshop on Machine Learning for Preventing\n  and Combating Pandemics", "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine a pair of graph generative models for the therapeutic design of\nnovel drug candidates targeting SARS-CoV-2 viral proteins. Due to a sense of\nurgency, we chose well-validated models with unique strengths: an autoencoder\nthat generates molecules with similar structures to a dataset of drugs with\nanti-SARS activity and a reinforcement learning algorithm that generates highly\nnovel molecules. During generation, we explore optimization toward several\ndesign targets to balance druglikeness, synthetic accessability, and anti-SARS\nactivity based on \\icfifty. This generative\nframework\\footnote{https://github.com/exalearn/covid-drug-design} will\naccelerate drug discovery in future pandemics through the high-throughput\ngeneration of targeted therapeutic candidates.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 18:39:25 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Bilbrey", "Jenna", ""], ["Ward", "Logan", ""], ["Choudhury", "Sutanay", ""], ["Kumar", "Neeraj", ""], ["Sivaraman", "Ganesh", ""]]}, {"id": "2105.10497", "submitter": "Salman Khan Dr.", "authors": "Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat,\n  Fahad Shahbaz Khan, Ming-Hsuan Yang", "title": "Intriguing Properties of Vision Transformers", "comments": "Code: https://git.io/Js15X", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformers (ViT) have demonstrated impressive performance across\nvarious machine vision problems. These models are based on multi-head\nself-attention mechanisms that can flexibly attend to a sequence of image\npatches to encode contextual cues. An important question is how such\nflexibility in attending image-wide context conditioned on a given patch can\nfacilitate handling nuisances in natural images e.g., severe occlusions, domain\nshifts, spatial permutations, adversarial and natural perturbations. We\nsystematically study this question via an extensive set of experiments\nencompassing three ViT families and comparisons with a high-performing\nconvolutional neural network (CNN). We show and analyze the following\nintriguing properties of ViT: (a) Transformers are highly robust to severe\nocclusions, perturbations and domain shifts, e.g., retain as high as 60% top-1\naccuracy on ImageNet even after randomly occluding 80% of the image content.\n(b) The robust performance to occlusions is not due to a bias towards local\ntextures, and ViTs are significantly less biased towards textures compared to\nCNNs. When properly trained to encode shape-based features, ViTs demonstrate\nshape recognition capability comparable to that of human visual system,\npreviously unmatched in the literature. (c) Using ViTs to encode shape\nrepresentation leads to an interesting consequence of accurate semantic\nsegmentation without pixel-level supervision. (d) Off-the-shelf features from a\nsingle ViT model can be combined to create a feature ensemble, leading to high\naccuracy rates across a range of classification datasets in both traditional\nand few-shot learning paradigms. We show effective features of ViTs are due to\nflexible and dynamic receptive fields possible via the self-attention\nmechanism.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:59:18 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 13:21:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Naseer", "Muzammal", ""], ["Ranasinghe", "Kanchana", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2105.10500", "submitter": "Yingjie Zhou", "authors": "Yingjie Zhou, Xucheng Song, Yanru Zhang, Fanxing Liu, Ce Zhu and\n  Lingqiao Liu", "title": "Feature Encoding with AutoEncoders for Weakly-supervised Anomaly\n  Detection", "comments": "12pages,4 figures, published by IEEE Transactions on Neural Networks\n  and Learning Systems,2021,DOI: 10.1109/TNNLS.2021.3086137", "journal-ref": "IEEE Transactions on Neural Networks and Learning\n  Systems,2021,DOI: 10.1109/TNNLS.2021.3086137", "doi": "10.1109/TNNLS.2021.3086137", "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised anomaly detection aims at learning an anomaly detector from\na limited amount of labeled data and abundant unlabeled data. Recent works\nbuild deep neural networks for anomaly detection by discriminatively mapping\nthe normal samples and abnormal samples to different regions in the feature\nspace or fitting different distributions. However, due to the limited number of\nannotated anomaly samples, directly training networks with the discriminative\nloss may not be sufficient. To overcome this issue, this paper proposes a novel\nstrategy to transform the input data into a more meaningful representation that\ncould be used for anomaly detection. Specifically, we leverage an autoencoder\nto encode the input data and utilize three factors, hidden representation,\nreconstruction residual vector, and reconstruction error, as the new\nrepresentation for the input data. This representation amounts to encode a test\nsample with its projection on the training data manifold, its direction to its\nprojection and its distance to its projection. In addition to this encoding, we\nalso propose a novel network architecture to seamlessly incorporate those three\nfactors. From our extensive experiments, the benefits of the proposed strategy\nare clearly demonstrated by its superior performance over the competitive\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 16:23:05 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 11:42:19 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 05:06:18 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Zhou", "Yingjie", ""], ["Song", "Xucheng", ""], ["Zhang", "Yanru", ""], ["Liu", "Fanxing", ""], ["Zhu", "Ce", ""], ["Liu", "Lingqiao", ""]]}, {"id": "2105.10536", "submitter": "Tony Zhang", "authors": "Tony Zhang, Szymon Zmyslony, Sergei Nozdrenkov, Matthew Smith, Brandon\n  Hopkins", "title": "Semi-Supervised Audio Representation Learning for Modeling Beehive\n  Strengths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Honey bees are critical to our ecosystem and food security as a pollinator,\ncontributing 35% of our global agriculture yield. In spite of their importance,\nbeekeeping is exclusively dependent on human labor and experience-derived\nheuristics, while requiring frequent human checkups to ensure the colony is\nhealthy, which can disrupt the colony. Increasingly, pollinator populations are\ndeclining due to threats from climate change, pests, environmental toxicity,\nmaking their management even more critical than ever before in order to ensure\nsustained global food security. To start addressing this pressing challenge, we\ndeveloped an integrated hardware sensing system for beehive monitoring through\naudio and environment measurements, and a hierarchical semi-supervised deep\nlearning model, composed of an audio modeling module and a predictor, to model\nthe strength of beehives. The model is trained jointly on audio reconstruction\nand prediction losses based on human inspections, in order to model both\nlow-level audio features and circadian temporal dynamics. We show that this\nmodel performs well despite limited labels, and can learn an audio embedding\nthat is useful for characterizing different sound profiles of beehives. This is\nthe first instance to our knowledge of applying audio-based deep learning to\nmodel beehives and population size in an observational setting across a large\nnumber of hives.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 18:59:29 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Tony", ""], ["Zmyslony", "Szymon", ""], ["Nozdrenkov", "Sergei", ""], ["Smith", "Matthew", ""], ["Hopkins", "Brandon", ""]]}, {"id": "2105.10545", "submitter": "Reza NasiriGerdeh", "authors": "Reza Nasirigerdeh, Reihaneh Torkzadehmahani, Julian Matschinske, Jan\n  Baumbach, Daniel Rueckert, Georgios Kaissis", "title": "HyFed: A Hybrid Federated Framework for Privacy-preserving Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated learning (FL) enables multiple clients to jointly train a global\nmodel under the coordination of a central server. Although FL is a\nprivacy-aware paradigm, where raw data sharing is not required, recent studies\nhave shown that FL might leak the private data of a client through the model\nparameters shared with the server or the other clients. In this paper, we\npresent the HyFed framework, which enhances the privacy of FL while preserving\nthe utility of the global model. HyFed provides developers with a generic API\nto develop federated, privacy-preserving algorithms. HyFed supports both\nsimulation and federated operation modes and its source code is publicly\navailable at https://github.com/tum-aimed/hyfed.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 19:30:43 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Nasirigerdeh", "Reza", ""], ["Torkzadehmahani", "Reihaneh", ""], ["Matschinske", "Julian", ""], ["Baumbach", "Jan", ""], ["Rueckert", "Daniel", ""], ["Kaissis", "Georgios", ""]]}, {"id": "2105.10554", "submitter": "Sachin Sapatnekar", "authors": "Sudipta Mondal, Susmita Dey Manasi, Kishor Kunal, and Sachin S.\n  Sapatnekar", "title": "GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific\n  Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis engines based on Graph Neural Networks (GNNs) are vital for many\nreal-world problems that model relationships using large graphs. Challenges for\na GNN hardware platform include the ability to (a) host a variety of GNNs, (b)\nhandle high sparsity in input node feature vectors and the graph adjacency\nmatrix and the accompanying random memory access patterns, and (c) maintain\nload-balanced computation in the face of uneven workloads induced by high\nsparsity and power-law vertex degree distributions in real datasets. The\nproposes GNNIE, an accelerator designed to run a broad range of GNNs. It\ntackles workload imbalance by (i) splitting node feature operands into blocks,\n(ii) reordering and redistributing computations, and (iii) using a flexible MAC\narchitecture with low communication overheads among the processing elements. In\naddition, it adopts a graph partitioning scheme and a graph-specific caching\npolicy that efficiently uses off-chip memory bandwidth that is well suited to\nthe characteristics of real-world graphs. Random memory access effects are\nmitigated by partitioning and degree-aware caching to enable the reuse of\nhigh-degree vertices. GNNIE achieves average speedups of over 8890x over a CPU\nand 295x over a GPU over multiple datasets on graph attention networks (GATs),\ngraph convolutional networks (GCNs), GraphSAGE, GINConv, and DiffPool, Compared\nto prior approaches, GNNIE achieves an average speedup of 9.74x over HyGCN for\nGCN, GraphSAGE, and GINConv; HyGCN cannot implement GATs. GNNIE achieves an\naverage speedup of 2.28x over AWB-GCN (which runs only GCNs), despite using\n3.4x fewer processing units.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:07:14 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Mondal", "Sudipta", ""], ["Manasi", "Susmita Dey", ""], ["Kunal", "Kishor", ""], ["Sapatnekar", "Sachin S.", ""]]}, {"id": "2105.10578", "submitter": "Rowan Swiers", "authors": "Cheng Ye, Rowan Swiers, Stephen Bonner, Ian Barrett", "title": "Predicting Potential Drug Targets Using Tensor Factorisation and\n  Knowledge Graph Embeddings", "comments": "Accepted by biokdd https://biokdd.org/biokdd21/accepted.html 9 pages,\n  5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The drug discovery and development process is a long and expensive one,\ncosting over 1 billion USD on average per drug and taking 10-15 years. To\nreduce the high levels of attrition throughout the process, there has been a\ngrowing interest in applying machine learning methodologies to various stages\nof drug discovery process in the recent decade, including at the earliest stage\n- identification of druggable disease genes. In this paper, we have developed a\nnew tensor factorisation model to predict potential drug targets (i.e.,genes or\nproteins) for diseases. We created a three dimensional tensor which consists of\n1,048 targets, 860 diseases and 230,011 evidence attributes and clinical\noutcomes connecting them, using data extracted from the Open Targets and\nPharmaProjects databases. We enriched the data with gene representations\nlearned from a drug discovery-oriented knowledge graph and applied our proposed\nmethod to predict the clinical outcomes for unseen target and dis-ease pairs.\nWe designed three evaluation strategies to measure the prediction performance\nand benchmarked several commonly used machine learning classifiers together\nwith matrix and tensor factorisation methods. The result shows that\nincorporating knowledge graph embeddings significantly improves the prediction\naccuracy and that training tensor factorisation alongside a dense neural\nnetwork outperforms other methods. In summary, our framework combines two\nactively studied machine learning approaches to disease target identification,\ntensor factorisation and knowledge graph representation learning, which could\nbe a promising avenue for further exploration in data-driven drug discovery.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:19:00 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 14:51:10 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ye", "Cheng", ""], ["Swiers", "Rowan", ""], ["Bonner", "Stephen", ""], ["Barrett", "Ian", ""]]}, {"id": "2105.10585", "submitter": "Phil Long", "authors": "Philip M. Long", "title": "Properties of the After Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Neural Tangent Kernel (NTK) is the wide-network limit of a kernel defined\nusing neural networks at initialization, whose embedding is the gradient of the\noutput of the network with respect to its parameters. We study the \"after\nkernel\", which is defined using the same embedding, except after training, for\nneural networks with standard architectures, on binary classification problems\nextracted from MNIST and CIFAR-10, trained using SGD in a standard way. For\nsome dataset-architecture pairs, after a few epochs of neural network training,\na hard-margin SVM using the network's after kernel is much more accurate than\nwhen the network's initial kernel is used. For networks with an architecture\nsimilar to VGG, the after kernel is more \"global\", in the sense that it is less\ninvariant to transformations of input images that disrupt the global structure\nof the image while leaving the local statistics largely intact. For fully\nconnected networks, the after kernel is less global in this sense. The after\nkernel tends to be more invariant to small shifts, rotations and zooms; data\naugmentation does not improve these invariances. The (finite approximation to\nthe) conjugate kernel, obtained using the last layer of hidden nodes,\nsometimes, but not always, provides a good approximation to the NTK and the\nafter kernel.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 21:50:18 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 21:39:21 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Long", "Philip M.", ""]]}, {"id": "2105.10587", "submitter": "Michael Tashman", "authors": "Michael Tashman, John Hoffman, Jiayi Xie, Fengdan Ye, Atefeh Morsali,\n  Lee Winikor, Rouzbeh Gerami", "title": "Techniques Toward Optimizing Viewability in RTB Ad Campaigns Using\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) is an effective technique for training\ndecision-making agents through interactions with their environment. The advent\nof deep learning has been associated with highly notable successes with\nsequential decision making problems - such as defeating some of the\nhighest-ranked human players at Go. In digital advertising, real-time bidding\n(RTB) is a common method of allocating advertising inventory through real-time\nauctions. Bidding strategies need to incorporate logic for dynamically\nadjusting parameters in order to deliver pre-assigned campaign goals. Here we\ndiscuss techniques toward using RL to train bidding agents. As a campaign\nmetric we particularly focused on viewability: the percentage of inventory\nwhich goes on to be viewed by an end user.\n  This paper is presented as a survey of techniques and experiments which we\ndeveloped through the course of this research. We discuss expanding our\ntraining data to include edge cases by training on simulated interactions. We\ndiscuss the experimental results comparing the performance of several promising\nRL algorithms, and an approach to hyperparameter optimization of an\nactor/critic training pipeline through Bayesian optimization. Finally, we\npresent live-traffic tests of some of our RL agents against a rule-based\nfeedback-control approach, demonstrating the potential for this method as well\nas areas for further improvement. This paper therefore presents an arrangement\nof our findings in this quickly developing field, and ways that it can be\napplied to an RTB use case.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 21:56:12 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Tashman", "Michael", ""], ["Hoffman", "John", ""], ["Xie", "Jiayi", ""], ["Ye", "Fengdan", ""], ["Morsali", "Atefeh", ""], ["Winikor", "Lee", ""], ["Gerami", "Rouzbeh", ""]]}, {"id": "2105.10590", "submitter": "Nilesh Tripuraneni", "authors": "Jeffrey Chan, Aldo Pacchiano, Nilesh Tripuraneni, Yun S. Song, Peter\n  Bartlett, Michael I. Jordan", "title": "Parallelizing Contextual Linear Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.BM q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard approaches to decision-making under uncertainty focus on sequential\nexploration of the space of decisions. However, \\textit{simultaneously}\nproposing a batch of decisions, which leverages available resources for\nparallel experimentation, has the potential to rapidly accelerate exploration.\nWe present a family of (parallel) contextual linear bandit algorithms, whose\nregret is nearly identical to their perfectly sequential counterparts -- given\naccess to the same total number of oracle queries -- up to a lower-order\n\"burn-in\" term that is dependent on the context-set geometry. We provide\nmatching information-theoretic lower bounds on parallel regret performance to\nestablish our algorithms are asymptotically optimal in the time horizon.\nFinally, we also present an empirical evaluation of these parallel algorithms\nin several domains, including materials discovery and biological sequence\ndesign problems, to demonstrate the utility of parallelized bandits in\npractical settings.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 22:22:02 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chan", "Jeffrey", ""], ["Pacchiano", "Aldo", ""], ["Tripuraneni", "Nilesh", ""], ["Song", "Yun S.", ""], ["Bartlett", "Peter", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2105.10594", "submitter": "Jacob Imola", "authors": "Jacob Imola, Kamalika Chaudhuri", "title": "Privacy Amplification Via Bernoulli Sampling", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Balancing privacy and accuracy is a major challenge in designing\ndifferentially private machine learning algorithms. To improve this tradeoff,\nprior work has looked at privacy amplification methods which analyze how common\ntraining operations such as iteration and subsampling the data can lead to\nhigher privacy. In this paper, we analyze privacy amplification properties of a\nnew operation, sampling from the posterior, that is used in Bayesian inference.\nIn particular, we look at Bernoulli sampling from a posterior that is described\nby a differentially private parameter. We provide an algorithm to compute the\namplification factor in this setting, and establish upper and lower bounds on\nthis factor. Finally, we look at what happens when we draw k posterior samples\ninstead of one.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 22:34:32 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Imola", "Jacob", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "2105.10598", "submitter": "Coen Needell", "authors": "Coen D. Needell and Wilma A. Bainbridge", "title": "Embracing New Techniques in Deep Learning for Estimating Image\n  Memorability", "comments": "27 pages, 15 figures, Presented at the Proceedings of the Vision\n  Sciences Society 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Various work has suggested that the memorability of an image is consistent\nacross people, and thus can be treated as an intrinsic property of an image.\nUsing computer vision models, we can make specific predictions about what\npeople will remember or forget. While older work has used now-outdated deep\nlearning architectures to predict image memorability, innovations in the field\nhave given us new techniques to apply to this problem. Here, we propose and\nevaluate five alternative deep learning models which exploit developments in\nthe field from the last five years, largely the introduction of residual neural\nnetworks, which are intended to allow the model to use semantic information in\nthe memorability estimation process. These new models were tested against the\nprior state of the art with a combined dataset built to optimize both\nwithin-category and across-category predictions. Our findings suggest that the\nkey prior memorability network had overstated its generalizability and was\noverfit on its training set. Our new models outperform this prior model,\nleading us to conclude that Residual Networks outperform simpler convolutional\nneural networks in memorability regression. We make our new state-of-the-art\nmodel readily available to the research community, allowing memory researchers\nto make predictions about memorability on a wider range of images.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 23:05:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Needell", "Coen D.", ""], ["Bainbridge", "Wilma A.", ""]]}, {"id": "2105.10625", "submitter": "Orestis Papadigenopoulos", "authors": "Alexia Atsidakou, Orestis Papadigenopoulos, Soumya Basu, Constantine\n  Caramanis, Sanjay Shakkottai", "title": "Combinatorial Blocking Bandits with Stochastic Delays", "comments": "International Conference on Machine Learning, ICML'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has considered natural variations of the multi-armed bandit\nproblem, where the reward distribution of each arm is a special function of the\ntime passed since its last pulling. In this direction, a simple (yet widely\napplicable) model is that of blocking bandits, where an arm becomes unavailable\nfor a deterministic number of rounds after each play. In this work, we extend\nthe above model in two directions: (i) We consider the general combinatorial\nsetting where more than one arms can be played at each round, subject to\nfeasibility constraints. (ii) We allow the blocking time of each arm to be\nstochastic. We first study the computational/unconditional hardness of the\nabove setting and identify the necessary conditions for the problem to become\ntractable (even in an approximate sense). Based on these conditions, we provide\na tight analysis of the approximation guarantee of a natural greedy heuristic\nthat always plays the maximum expected reward feasible subset among the\navailable (non-blocked) arms. When the arms' expected rewards are unknown, we\nadapt the above heuristic into a bandit algorithm, based on UCB, for which we\nprovide sublinear (approximate) regret guarantees, matching the theoretical\nlower bounds in the limiting case of absence of delays.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 02:46:04 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Atsidakou", "Alexia", ""], ["Papadigenopoulos", "Orestis", ""], ["Basu", "Soumya", ""], ["Caramanis", "Constantine", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "2105.10635", "submitter": "Bo Wang", "authors": "Jiabin Liu, Bo Wang, Xin Shen, Zhiquan Qi, Yingjie Tian", "title": "Two-stage Training for Learning from Label Proportions", "comments": "10 pages, 4 figures, 5 tables, accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from label proportions (LLP) aims at learning an instance-level\nclassifier with label proportions in grouped training data. Existing deep\nlearning based LLP methods utilize end-to-end pipelines to obtain the\nproportional loss with Kullback-Leibler divergence between the bag-level prior\nand posterior class distributions. However, the unconstrained optimization on\nthis objective can hardly reach a solution in accordance with the given\nproportions. Besides, concerning the probabilistic classifier, this strategy\nunavoidably results in high-entropy conditional class distributions at the\ninstance level. These issues further degrade the performance of the\ninstance-level classification. In this paper, we regard these problems as noisy\npseudo labeling, and instead impose the strict proportion consistency on the\nclassifier with a constrained optimization as a continuous training stage for\nexisting LLP classifiers. In addition, we introduce the mixup strategy and\nsymmetric crossentropy to further reduce the label noise. Our framework is\nmodel-agnostic, and demonstrates compelling performance improvement in\nextensive experiments, when incorporated into other deep LLP models as a\npost-hoc phase.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 03:55:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liu", "Jiabin", ""], ["Wang", "Bo", ""], ["Shen", "Xin", ""], ["Qi", "Zhiquan", ""], ["Tian", "Yingjie", ""]]}, {"id": "2105.10644", "submitter": "Yusuke Ohtsubo", "authors": "Yusuke Ohtsubo, Tetsu Matsukawa, Einoshin Suzuki", "title": "Semi-Supervised Few-Shot Classification with Deep Invertible Hybrid\n  Models", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a deep invertible hybrid model which integrates\ndiscriminative and generative learning at a latent space level for\nsemi-supervised few-shot classification. Various tasks for classifying new\nspecies from image data can be modeled as a semi-supervised few-shot\nclassification, which assumes a labeled and unlabeled training examples and a\nsmall support set of the target classes. Predicting target classes with a few\nsupport examples per class makes the learning task difficult for existing\nsemi-supervised classification methods, including selftraining, which\niteratively estimates class labels of unlabeled training examples to learn a\nclassifier for the training classes. To exploit unlabeled training examples\neffectively, we adopt as the objective function the composite likelihood, which\nintegrates discriminative and generative learning and suits better with deep\nneural networks than the parameter coupling prior, the other popular integrated\nlearning approach. In our proposed model, the discriminative and generative\nmodels are respectively Prototypical Networks, which have shown excellent\nperformance in various kinds of few-shot learning, and Normalizing Flow a deep\ninvertible model which returns the exact marginal likelihood unlike the other\nthree major methods, i.e., VAE, GAN, and autoregressive model. Our main\noriginality lies in our integration of these components at a latent space\nlevel, which is effective in preventing overfitting. Experiments using\nmini-ImageNet and VGG-Face datasets show that our method outperforms\nselftraining based Prototypical Networks.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 05:55:16 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ohtsubo", "Yusuke", ""], ["Matsukawa", "Tetsu", ""], ["Suzuki", "Einoshin", ""]]}, {"id": "2105.10651", "submitter": "Xingcheng Fu", "authors": "Jianxin Li, Xingcheng Fu, Hao Peng, Senzhang Wang, Shijie Zhu, Qingyun\n  Sun, Philip S. Yu, Lifang He", "title": "A Robust and Generalized Framework for Adversarial Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding is essential for graph mining tasks. With the prevalence of\ngraph data in real-world applications, many methods have been proposed in\nrecent years to learn high-quality graph embedding vectors various types of\ngraphs. However, most existing methods usually randomly select the negative\nsamples from the original graph to enhance the training data without\nconsidering the noise. In addition, most of these methods only focus on the\nexplicit graph structures and cannot fully capture complex semantics of edges\nsuch as various relationships or asymmetry. In order to address these issues,\nwe propose a robust and generalized framework for adversarial graph embedding\nbased on generative adversarial networks. Inspired by generative adversarial\nnetwork, we propose a robust and generalized framework for adversarial graph\nembedding, named AGE. AGE generates the fake neighbor nodes as the enhanced\nnegative samples from the implicit distribution, and enables the discriminator\nand generator to jointly learn each node's robust and generalized\nrepresentation. Based on this framework, we propose three models to handle\nthree types of graph data and derive the corresponding optimization algorithms,\ni.e., UG-AGE and DG-AGE for undirected and directed homogeneous graphs,\nrespectively, and HIN-AGE for heterogeneous information networks. Extensive\nexperiments show that our methods consistently and significantly outperform\nexisting state-of-the-art methods across multiple graph mining tasks, including\nlink prediction, node classification, and graph reconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 07:05:48 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Li", "Jianxin", ""], ["Fu", "Xingcheng", ""], ["Peng", "Hao", ""], ["Wang", "Senzhang", ""], ["Zhu", "Shijie", ""], ["Sun", "Qingyun", ""], ["Yu", "Philip S.", ""], ["He", "Lifang", ""]]}, {"id": "2105.10671", "submitter": "Tanveer Khan", "authors": "Tanveer Khan, Antonis Michalas, Adnan Akhunzada", "title": "SOK: Fake News Outbreak 2021: Can We Stop the Viral Spread?", "comments": "34 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social Networks' omnipresence and ease of use has revolutionized the\ngeneration and distribution of information in today's world. However, easy\naccess to information does not equal an increased level of public knowledge.\nUnlike traditional media channels, social networks also facilitate faster and\nwider spread of disinformation and misinformation. Viral spread of false\ninformation has serious implications on the behaviors, attitudes and beliefs of\nthe public, and ultimately can seriously endanger the democratic processes.\nLimiting false information's negative impact through early detection and\ncontrol of extensive spread presents the main challenge facing researchers\ntoday. In this survey paper, we extensively analyze a wide range of different\nsolutions for the early detection of fake news in the existing literature. More\nprecisely, we examine Machine Learning (ML) models for the identification and\nclassification of fake news, online fake news detection competitions,\nstatistical outputs as well as the advantages and disadvantages of some of the\navailable data sets. Finally, we evaluate the online web browsing tools\navailable for detecting and mitigating fake news and present some open research\nchallenges.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 09:26:13 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Khan", "Tanveer", ""], ["Michalas", "Antonis", ""], ["Akhunzada", "Adnan", ""]]}, {"id": "2105.10682", "submitter": "Haitong Ma", "authors": "Haitong Ma, Yang Guan, Shegnbo Eben Li, Xiangteng Zhang, Sifa Zheng,\n  Jianyu Chen", "title": "Feasible Actor-Critic: Constrained Reinforcement Learning for Ensuring\n  Statewise Safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The safety constraints commonly used by existing safe reinforcement learning\n(RL) methods are defined only on expectation of initial states, but allow each\ncertain state to be unsafe, which is unsatisfying for real-world\nsafety-critical tasks. In this paper, we introduce the feasible actor-critic\n(FAC) algorithm, which is the first model-free constrained RL method that\nconsiders statewise safety, e.g, safety for each initial state. We claim that\nsome states are inherently unsafe no matter what policy we choose, while for\nother states there exist policies ensuring safety, where we say such states and\npolicies are feasible. By constructing a statewise Lagrange function available\non RL sampling and adopting an additional neural network to approximate the\nstatewise Lagrange multiplier, we manage to obtain the optimal feasible policy\nwhich ensures safety for each feasible state and the safest possible policy for\ninfeasible states. Furthermore, the trained multiplier net can indicate whether\na given state is feasible or not through the statewise complementary slackness\ncondition. We provide theoretical guarantees that FAC outperforms previous\nexpectation-based constrained RL methods in terms of both constraint\nsatisfaction and reward optimization. Experimental results on both robot\nlocomotive tasks and safe exploration tasks verify the safety enhancement and\nfeasibility interpretation of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 10:40:58 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 06:23:44 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 04:07:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ma", "Haitong", ""], ["Guan", "Yang", ""], ["Li", "Shegnbo Eben", ""], ["Zhang", "Xiangteng", ""], ["Zheng", "Sifa", ""], ["Chen", "Jianyu", ""]]}, {"id": "2105.10688", "submitter": "Yue Zhang", "authors": "Yue Zhang, Yajie Zou, Lingtao Wu", "title": "V2V Spatiotemporal Interactive Pattern Recognition and Risk Analysis in\n  Lane Changes", "comments": "23 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex lane change (LC) scenarios, semantic interpretation and safety\nanalysis of dynamic interactive pattern are necessary for autonomous vehicles\nto make appropriate decisions. This study proposes an unsupervised learning\nframework that combines primitive-based interactive pattern recognition methods\nand risk analysis methods. The Hidden Markov Model with the Gaussian mixture\nmodel (GMM-HMM) approach is developed to decompose the LC scenarios into\nprimitives. Then the Dynamic Time Warping (DTW) distance based K-means\nclustering is applied to gather the primitives to 13 types of interactive\npatterns. Finally, this study considers two types of time-to-collision (TTC)\ninvolved in the LC process as indicators to analyze the risk of the interactive\npatterns and extract high-risk LC interactive patterns. The results obtained\nfrom The Highway Drone Dataset (highD) demonstrate that the identified LC\ninteractive patterns contain interpretable semantic information. This study\nexplores the spatiotemporal evolution law and risk formation mechanism of the\nLC interactive patterns and the findings are useful for comprehensively\nunderstanding the latent interactive patterns, improving the rationality and\nsafety of autonomous vehicle's decision-making.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 11:00:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Yue", ""], ["Zou", "Yajie", ""], ["Wu", "Lingtao", ""]]}, {"id": "2105.10699", "submitter": "Yulin Shao", "authors": "Yulin Shao and Soung Chang Liew and Deniz Gunduz", "title": "Denoising Noisy Neural Networks: A Bayesian Approach with Compensation", "comments": "Keywords: Noisy neural network, Bayesian estimation, analog device,\n  federated edge learning, over-the-air computation, analog storage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy neural networks (NoisyNNs) refer to the inference and training of NNs\nin the presence of noise. Noise is inherent in most communication and storage\nsystems; hence, NoisyNNs emerge in many new applications, including federated\nedge learning, where wireless devices collaboratively train a NN over a noisy\nwireless channel, or when NNs are implemented/stored in an analog storage\nmedium. This paper studies a fundamental problem of NoisyNNs: how to estimate\nthe uncontaminated NN weights from their noisy observations or manifestations.\nWhereas all prior works relied on the maximum likelihood (ML) estimation to\nmaximize the likelihood function of the estimated NN weights, this paper\ndemonstrates that the ML estimator is in general suboptimal. To overcome the\nsuboptimality of the conventional ML estimator, we put forth an\n$\\text{MMSE}_{pb}$ estimator to minimize a compensated mean squared error (MSE)\nwith a population compensator and a bias compensator. Our approach works well\nfor NoisyNNs arising in both 1) noisy inference, where noise is introduced only\nin the inference phase on the already-trained NN weights; and 2) noisy\ntraining, where noise is introduced over the course of training. Extensive\nexperiments on the CIFAR-10 and SST-2 datasets with different NN architectures\nverify the significant performance gains of the $\\text{MMSE}_{pb}$ estimator\nover the ML estimator when used to denoise the NoisyNN. For noisy inference,\nthe average gains are up to $156\\%$ for a noisy ResNet34 model and $14.7\\%$ for\na noisy BERT model; for noisy training, the average gains are up to $18.1$ dB\nfor a noisy ResNet18 model.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 11:51:20 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shao", "Yulin", ""], ["Liew", "Soung Chang", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2105.10702", "submitter": "Aydan Gasimova", "authors": "Aydan Gasimova, Giovanni Montana, Daniel Rueckert", "title": "Automated Knee X-ray Report Generation", "comments": null, "journal-ref": "NeurIPS Machine Learning for Health Workshop 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gathering manually annotated images for the purpose of training a predictive\nmodel is far more challenging in the medical domain than for natural images as\nit requires the expertise of qualified radiologists. We therefore propose to\ntake advantage of past radiological exams (specifically, knee X-ray\nexaminations) and formulate a framework capable of learning the correspondence\nbetween the images and reports, and hence be capable of generating diagnostic\nreports for a given X-ray examination consisting of an arbitrary number of\nimage views. We demonstrate how aggregating the image features of individual\nexams and using them as conditional inputs when training a language generation\nmodel results in auto-generated exam reports that correlate well with\nradiologist-generated reports.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 11:59:42 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gasimova", "Aydan", ""], ["Montana", "Giovanni", ""], ["Rueckert", "Daniel", ""]]}, {"id": "2105.10707", "submitter": "Yifan Jia", "authors": "Yifan Jia, Jingyi Wang, Christopher M. Poskitt, Sudipta Chattopadhyay,\n  Jun Sun, Yuqi Chen", "title": "Adversarial Attacks and Mitigation for Anomaly Detectors of\n  Cyber-Physical Systems", "comments": "Accepted by the International Journal of Critical Infrastructure\n  Protection (IJCIP)", "journal-ref": "Int. J. Crit. Infrastructure Prot. 34:100452, 2021", "doi": "10.1016/j.ijcip.2021.100452", "report-no": null, "categories": "cs.CR cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The threats faced by cyber-physical systems (CPSs) in critical infrastructure\nhave motivated research into a multitude of attack detection mechanisms,\nincluding anomaly detectors based on neural network models. The effectiveness\nof anomaly detectors can be assessed by subjecting them to test suites of\nattacks, but less consideration has been given to adversarial attackers that\ncraft noise specifically designed to deceive them. While successfully applied\nin domains such as images and audio, adversarial attacks are much harder to\nimplement in CPSs due to the presence of other built-in defence mechanisms such\nas rule checkers(or invariant checkers). In this work, we present an\nadversarial attack that simultaneously evades the anomaly detectors and rule\ncheckers of a CPS. Inspired by existing gradient-based approaches, our\nadversarial attack crafts noise over the sensor and actuator values, then uses\na genetic algorithm to optimise the latter, ensuring that the neural network\nand the rule checking system are both deceived.We implemented our approach for\ntwo real-world critical infrastructure testbeds, successfully reducing the\nclassification accuracy of their detectors by over 50% on average, while\nsimultaneously avoiding detection by rule checkers. Finally, we explore whether\nthese attacks can be mitigated by training the detectors on adversarial\nsamples.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 12:19:03 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Jia", "Yifan", ""], ["Wang", "Jingyi", ""], ["Poskitt", "Christopher M.", ""], ["Chattopadhyay", "Sudipta", ""], ["Sun", "Jun", ""], ["Chen", "Yuqi", ""]]}, {"id": "2105.10709", "submitter": "Tirtharaj Dash", "authors": "Tirtharaj Dash, Ashwin Srinivasan, A Baskar", "title": "Inclusion of Domain-Knowledge into GNNs using Mode-Directed Inverse\n  Entailment", "comments": "submitted to Machine Learning Journal (MLJ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general technique for constructing Graph Neural Networks (GNNs)\ncapable of using multi-relational domain knowledge. The technique is based on\nmode-directed inverse entailment (MDIE) developed in Inductive Logic\nProgramming (ILP). Given a data instance $e$ and background knowledge $B$, MDIE\nidentifies a most-specific logical formula $\\bot_B(e)$ that contains all the\nrelational information in $B$ that is related to $e$. We transform $\\bot_B(e)$\ninto a corresponding \"bottom-graph\" that can be processed for use by standard\nGNN implementations. This transformation allows a principled way of\nincorporating generic background knowledge into GNNs: we use the term `BotGNN'\nfor this form of graph neural networks. For several GNN variants, using\nreal-world datasets with substantial background knowledge, we show that BotGNNs\nperform significantly better than both GNNs without background knowledge and a\nrecently proposed simplified technique for including domain knowledge into\nGNNs. We also provide experimental evidence comparing BotGNNs favourably to\nmulti-layer perceptrons (MLPs) that use features representing a\n\"propositionalised\" form of the background knowledge; and BotGNNs to a standard\nILP based on the use of most-specific clauses. Taken together, these results\npoint to BotGNNs as capable of combining the computational efficacy of GNNs\nwith the representational versatility of ILP.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 12:25:13 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Dash", "Tirtharaj", ""], ["Srinivasan", "Ashwin", ""], ["Baskar", "A", ""]]}, {"id": "2105.10716", "submitter": "Won Joon Yun", "authors": "Won Joon Yun, Byungju Lim, Soyi Jung, Young-Chai Ko, Jihong Park,\n  Joongheon Kim, Mehdi Bennis", "title": "Attention-based Reinforcement Learning for Real-Time UAV Semantic\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we study the problem of air-to-ground ultra-reliable and\nlow-latency communication (URLLC) for a moving ground user. This is done by\ncontrolling multiple unmanned aerial vehicles (UAVs) in real time while\navoiding inter-UAV collisions. To this end, we propose a novel multi-agent deep\nreinforcement learning (MADRL) framework, coined a graph attention exchange\nnetwork (GAXNet). In GAXNet, each UAV constructs an attention graph locally\nmeasuring the level of attention to its neighboring UAVs, while exchanging the\nattention weights with other UAVs so as to reduce the attention mismatch\nbetween them. Simulation results corroborates that GAXNet achieves up to 4.5x\nhigher rewards during training. At execution, without incurring inter-UAV\ncollisions, GAXNet achieves 6.5x lower latency with the target 0.0000001 error\nrate, compared to a state-of-the-art baseline framework.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 12:43:25 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yun", "Won Joon", ""], ["Lim", "Byungju", ""], ["Jung", "Soyi", ""], ["Ko", "Young-Chai", ""], ["Park", "Jihong", ""], ["Kim", "Joongheon", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2105.10719", "submitter": "Jie Ren", "authors": "Jie Ren, Zhanpeng Zhou, Qirui Chen, Quanshi Zhang", "title": "Learning Baseline Values for Shapley Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to formulate the problem of estimating the optimal baseline\nvalues for the Shapley value in game theory. The Shapley value measures the\nattribution of each input variable of a complex model, which is computed as the\nmarginal benefit from the presence of this variable w.r.t.its absence under\ndifferent contexts. To this end, people usually set the input variable to its\nbaseline value to represent the absence of this variable (i.e.the no-signal\nstate of this variable). Previous studies usually determine the baseline values\nin an empirical manner, which hurts the trustworthiness of the Shapley value.\nIn this paper, we revisit the feature representation of a deep model from the\nperspective of game theory, and define the multi-variate interaction patterns\nof input variables to define the no-signal state of an input variable. Based on\nthe multi-variate interaction, we learn the optimal baseline value of each\ninput variable. Experimental results have demonstrated the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 13:03:18 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 08:49:00 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ren", "Jie", ""], ["Zhou", "Zhanpeng", ""], ["Chen", "Qirui", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2105.10721", "submitter": "Anand Kalvit", "authors": "Anand Kalvit and Assaf Zeevi", "title": "From Finite to Countable-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic bandit problem with countably many arms that belong\nto a finite set of types, each characterized by a unique mean reward. In\naddition, there is a fixed distribution over types which sets the proportion of\neach type in the population of arms. The decision maker is oblivious to the\ntype of any arm and to the aforementioned distribution over types, but\nperfectly knows the total number of types occurring in the population of arms.\nWe propose a fully adaptive online learning algorithm that achieves O(log n)\ndistribution-dependent expected cumulative regret after any number of plays n,\nand show that this order of regret is best possible. The analysis of our\nalgorithm relies on newly discovered concentration and convergence properties\nof optimism-based policies like UCB in finite-armed bandit problems with \"zero\ngap,\" which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 13:09:50 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kalvit", "Anand", ""], ["Zeevi", "Assaf", ""]]}, {"id": "2105.10723", "submitter": "Changqing Xu", "authors": "ChangQing Xu, Yi Liu, XinFang Liao, JiaLiang Cheng and YinTang Yang", "title": "Machine Learning Regression based Single Event Transient Modeling Method\n  for Circuit-Level Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a novel machine learning regression based single event\ntransient (SET) modeling method is proposed. The proposed method can obtain a\nreasonable and accurate model without considering the complex physical\nmechanism. We got plenty of SET current data of SMIC 130nm bulk CMOS by TCAD\nsimulation under different conditions (e.g. different LET and different drain\nbias voltage). A multilayer feedfordward neural network is used to build the\nSET pulse current model by learning the data from TCAD simulation. The proposed\nmodel is validated with the simulation results from TCAD simulation. The\ntrained SET pulse current model is implemented as a Verilog-A current source in\nthe Cadence Spectre circuit simulator and an inverter with five fan-outs is\nused to show the practicability and reasonableness of the proposed SET pulse\ncurrent model for circuit-level single-event effect (SEE) simulation.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 13:24:13 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Xu", "ChangQing", ""], ["Liu", "Yi", ""], ["Liao", "XinFang", ""], ["Cheng", "JiaLiang", ""], ["Yang", "YinTang", ""]]}, {"id": "2105.10759", "submitter": "G Manjunath", "authors": "G Manjunath and A de Clercq", "title": "Universal set of Observables for the Koopman Operator through Causal\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining repeated measurements from physical and natural systems for\nbuilding a more informative dynamical model of such systems is engraved in\nmodern science. Results in reconstructing equivalent chaotic dynamical systems\nthrough delay coordinate mappings, Koopman operator based data-driven approach\nand reservoir computing methods have shown the possibility of finding model\nequations on a new phase space that is relatable to the dynamical system\ngenerating the data. Recently, rigorous results that point to reducing the\nfunctional complexity of the map that describes the dynamics in the new phase\nhave made the Koopman operator based approach very attractive for data-driven\nmodeling. However, choosing a set of nonlinear observable functions that can\nwork for different data sets is an open challenge. We use driven dynamical\nsystems comparable to that in reservoir computing with the \\emph{causal\nembedding property} to obtain the right set of observables through which the\ndynamics in the new space is made equivalent or topologically conjugate to the\noriginal system. Deep learning methods are used to learn a map that emerges as\na consequence of the topological conjugacy. Besides stability, amenability for\nhardware implementations, causal embedding based models provide long-term\nconsistency even for maps that have failed under previously reported\ndata-driven or machine learning methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 16:28:57 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Manjunath", "G", ""], ["de Clercq", "A", ""]]}, {"id": "2105.10762", "submitter": "Yuchen Jin", "authors": "Yuchen Jin, Tianyi Zhou, Liangyu Zhao, Yibo Zhu, Chuanxiong Guo, Marco\n  Canini, Arvind Krishnamurthy", "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on\n  the Fly", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The learning rate (LR) schedule is one of the most important hyper-parameters\nneeding careful tuning in training DNNs. However, it is also one of the least\nautomated parts of machine learning systems and usually costs significant\nmanual effort and computing. Though there are pre-defined LR schedules and\noptimizers with adaptive LR, they introduce new hyperparameters that need to be\ntuned separately for different tasks/datasets. In this paper, we consider the\nquestion: Can we automatically tune the LR over the course of training without\nhuman involvement? We propose an efficient method, AutoLRS, which automatically\noptimizes the LR for each training stage by modeling training dynamics. AutoLRS\naims to find an LR applied to every $\\tau$ steps that minimizes the resulted\nvalidation loss. We solve this black-box optimization on the fly by Bayesian\noptimization (BO). However, collecting training instances for BO requires a\nsystem to evaluate each LR queried by BO's acquisition function for $\\tau$\nsteps, which is prohibitively expensive in practice. Instead, we apply each\ncandidate LR for only $\\tau'\\ll\\tau$ steps and train an exponential model to\npredict the validation loss after $\\tau$ steps. This mutual-training process\nbetween BO and the loss-prediction model allows us to limit the training steps\ninvested in the BO search. We demonstrate the advantages and the generality of\nAutoLRS through extensive experiments of training DNNs for tasks from diverse\ndomains using different optimizers. The LR schedules auto-generated by AutoLRS\nlead to a speedup of $1.22\\times$, $1.43\\times$, and $1.5\\times$ when training\nResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in\ntheir original papers, and an average speedup of $1.31\\times$ over\nstate-of-the-art heavily-tuned LR schedules.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 16:41:10 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Jin", "Yuchen", ""], ["Zhou", "Tianyi", ""], ["Zhao", "Liangyu", ""], ["Zhu", "Yibo", ""], ["Guo", "Chuanxiong", ""], ["Canini", "Marco", ""], ["Krishnamurthy", "Arvind", ""]]}, {"id": "2105.10766", "submitter": "G Manjunath", "authors": "G Manjunath", "title": "Embedding Information onto a Dynamical System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated Takens' embedding theorem concerns embedding an attractor of a\ndynamical system in a Euclidean space of appropriate dimension through a\ngeneric delay-observation map. The embedding also establishes a topological\nconjugacy. In this paper, we show how an arbitrary sequence can be mapped into\nanother space as an attractive solution of a nonautonomous dynamical system.\nSuch mapping also entails a topological conjugacy and an embedding between the\nsequence and the attractive solution spaces. This result is not a\ngeneralization of Takens embedding theorem but helps us understand what exactly\nis required by discrete-time state space models widely used in applications to\nembed an external stimulus onto its solution space. Our results settle another\nbasic problem concerning the perturbation of an autonomous dynamical system. We\ndescribe what exactly happens to the dynamics when exogenous noise perturbs\ncontinuously a local irreducible attracting set (such as a stable fixed point)\nof a discrete-time autonomous dynamical system.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 16:54:16 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Manjunath", "G", ""]]}, {"id": "2105.10796", "submitter": "Sree Ram Kamabattula", "authors": "Sree Ram Kamabattula, Kumudha Musini, Babak Namazi, Ganesh\n  Sankaranarayanan, Venkat Devarajan", "title": "Generation and Analysis of Feature-Dependent Pseudo Noise for Training\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Deep neural networks (DNNs) on noisy labeled datasets is a\nchallenging problem, because learning on mislabeled examples deteriorates the\nperformance of the network. As the ground truth availability is limited with\nreal-world noisy datasets, previous papers created synthetic noisy datasets by\nrandomly modifying the labels of training examples of clean datasets. However,\nno final conclusions can be derived by just using this random noise, since it\nexcludes feature-dependent noise. Thus, it is imperative to generate\nfeature-dependent noisy datasets that additionally provide ground truth.\nTherefore, we propose an intuitive approach to creating feature-dependent noisy\ndatasets by utilizing the training predictions of DNNs on clean datasets that\nalso retain true label information. We refer to these datasets as \"Pseudo Noisy\ndatasets\". We conduct several experiments to establish that Pseudo noisy\ndatasets resemble feature-dependent noisy datasets across different conditions.\nWe further randomly generate synthetic noisy datasets with the same noise\ndistribution as that of Pseudo noise (referred as \"Randomized Noise\") to\nempirically show that i) learning is easier with feature-dependent label noise\ncompared to random noise, ii) irrespective of noise distribution, Pseudo noisy\ndatasets mimic feature-dependent label noise and iii) current training methods\nare not generalizable to feature-dependent label noise. Therefore, we believe\nthat Pseudo noisy datasets will be quite helpful to study and develop robust\ntraining methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 19:15:26 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kamabattula", "Sree Ram", ""], ["Musini", "Kumudha", ""], ["Namazi", "Babak", ""], ["Sankaranarayanan", "Ganesh", ""], ["Devarajan", "Venkat", ""]]}, {"id": "2105.10816", "submitter": "Shadab Hussain", "authors": "Shadab Hussain, Susmith Barigidad, Shadab Akhtar, Md Suaib", "title": "Novel Deep Learning Architecture for Heart Disease Prediction using\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Healthcare is one of the most important aspects of human life. Heart disease\nis known to be one of the deadliest diseases which is hampering the lives of\nmany people around the world. Heart disease must be detected early so the loss\nof lives can be prevented. The availability of large-scale data for medical\ndiagnosis has helped developed complex machine learning and deep learning-based\nmodels for automated early diagnosis of heart diseases. The classical\napproaches have been limited in terms of not generalizing well to new data\nwhich have not been seen in the training set. This is indicated by a large gap\nin training and test accuracies. This paper proposes a novel deep learning\narchitecture using a 1D convolutional neural network for classification between\nhealthy and non-healthy persons to overcome the limitations of classical\napproaches. Various clinical parameters are used for assessing the risk profile\nin the patients which helps in early diagnosis. Various techniques are used to\navoid overfitting in the proposed network. The proposed network achieves over\n97% training accuracy and 96% test accuracy on the dataset. The accuracy of the\nmodel is compared in detail with other classification algorithms using various\nperformance parameters which proves the effectiveness of the proposed\narchitecture.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 22:00:57 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 15:22:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hussain", "Shadab", ""], ["Barigidad", "Susmith", ""], ["Akhtar", "Shadab", ""], ["Suaib", "Md", ""]]}, {"id": "2105.10832", "submitter": "Takashi Furuya", "authors": "Takashi Furuya, Kazuma Suetake, Koichi Taniguchi, Hiroyuki Kusumoto,\n  Ryuji Saiin, Tomohiro Daimon", "title": "Spectral Pruning for Recurrent Neural Networks", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning techniques for neural networks with a recurrent architecture, such as\nthe recurrent neural network (RNN), are strongly desired for their application\nto edge-computing devices. However, the recurrent architecture is generally not\nrobust to pruning because even small pruning causes accumulation error and the\ntotal error increases significantly over time. In this paper, we propose an\nappropriate pruning algorithm for RNNs inspired by \"spectral pruning\", and\nprovide the generalization error bounds for compressed RNNs. We also provide\nnumerical experiments to demonstrate our theoretical results and show the\neffectiveness of our pruning method compared with existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 00:30:59 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Furuya", "Takashi", ""], ["Suetake", "Kazuma", ""], ["Taniguchi", "Koichi", ""], ["Kusumoto", "Hiroyuki", ""], ["Saiin", "Ryuji", ""], ["Daimon", "Tomohiro", ""]]}, {"id": "2105.10862", "submitter": "Boxin Du", "authors": "Boxin Du, Changhe Yuan, Robert Barton, Tal Neiman, Hanghang Tong", "title": "Hypergraph Pre-training with Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the prevalence of hypergraphs in a variety of high-impact\napplications, there are relatively few works on hypergraph representation\nlearning, most of which primarily focus on hyperlink prediction, often\nrestricted to the transductive learning setting. Among others, a major hurdle\nfor effective hypergraph representation learning lies in the label scarcity of\nnodes and/or hyperedges. To address this issue, this paper presents an\nend-to-end, bi-level pre-training strategy with Graph Neural Networks for\nhypergraphs. The proposed framework named HyperGene bears three distinctive\nadvantages. First, it is capable of ingesting the labeling information when\navailable, but more importantly, it is mainly designed in the self-supervised\nfashion which significantly broadens its applicability. Second, at the heart of\nthe proposed HyperGene are two carefully designed pretexts, one on the node\nlevel and the other on the hyperedge level, which enable us to encode both the\nlocal and the global context in a mutually complementary way. Third, the\nproposed framework can work in both transductive and inductive settings. When\napplying the two proposed pretexts in tandem, it can accelerate the adaptation\nof the knowledge from the pre-trained model to downstream applications in the\ntransductive setting, thanks to the bi-level nature of the proposed method. The\nextensive experimental results demonstrate that: (1) HyperGene achieves up to\n5.69% improvements in hyperedge classification, and (2) improves pre-training\nefficiency by up to 42.80% on average.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 06:33:57 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Du", "Boxin", ""], ["Yuan", "Changhe", ""], ["Barton", "Robert", ""], ["Neiman", "Tal", ""], ["Tong", "Hanghang", ""]]}, {"id": "2105.10866", "submitter": "Zeinab Rouhollahi", "authors": "Zeinab Rouhollahi", "title": "Towards Artificial Intelligence Enabled Financial Crime Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, financial institutes have been dealing with an increase in\nfinancial crimes. In this context, financial services firms started to improve\ntheir vigilance and use new technologies and approaches to identify and predict\nfinancial fraud and crime possibilities. This task is challenging as\ninstitutions need to upgrade their data and analytics capabilities to enable\nnew technologies such as Artificial Intelligence (AI) to predict and detect\nfinancial crimes. In this paper, we put a step towards AI-enabled financial\ncrime detection in general and money laundering detection in particular to\naddress this challenge. We study and analyse the recent works done in financial\ncrime detection and present a novel model to detect money laundering cases with\nminimum human intervention needs.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 06:57:25 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Rouhollahi", "Zeinab", ""]]}, {"id": "2105.10867", "submitter": "SeungHwan An", "authors": "SeungHwan An, Hosik Choi, Jong-June Jeon", "title": "EXoN: EXplainable encoder Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new semi-supervised learning method of Variational AutoEncoder\n(VAE) which yields an explainable latent space by EXplainable encoder Network\n(EXoN). The EXoN provides two useful tools for implementing VAE. First, we can\nfreely assign a conceptual center of the latent distribution for a specific\nlabel. The latent space of VAE is separated with the multi-modal property of\nthe Gaussian mixture distribution according to the labels of observations.\nNext, we can easily investigate the latent subspace by a simple statistics\nobtained from the EXoN. We found that both the negative cross-entropy and the\nKullback-Leibler divergence play a crucial role in constructing explainable\nlatent space and the variability of generated samples from our proposed model\ndepends on a specific subspace, called `activated latent subspace'. With MNIST\nand CIFAR-10 dataset, we show that the EXoN can produce an explainable latent\nspace that effectively represents the labels and characteristics of the images.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 07:04:30 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 13:45:31 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["An", "SeungHwan", ""], ["Choi", "Hosik", ""], ["Jeon", "Jong-June", ""]]}, {"id": "2105.10878", "submitter": "Guandong Xu", "authors": "Hamad Zogan, Imran Razzak, Shoaib Jameel, Guandong Xu", "title": "DepressionNet: A Novel Summarization Boosted Deep Framework for\n  Depression Detection on Social Media", "comments": null, "journal-ref": null, "doi": "10.1145/3404835.3462938", "report-no": null, "categories": "cs.LG cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is currently a popular online social media platform which allows\nusers to share their user-generated content. This publicly-generated user data\nis also crucial to healthcare technologies because the discovered patterns\nwould hugely benefit them in several ways. One of the applications is in\nautomatically discovering mental health problems, e.g., depression. Previous\nstudies to automatically detect a depressed user on online social media have\nlargely relied upon the user behaviour and their linguistic patterns including\nuser's social interactions. The downside is that these models are trained on\nseveral irrelevant content which might not be crucial towards detecting a\ndepressed user. Besides, these content have a negative impact on the overall\nefficiency and effectiveness of the model. To overcome the shortcomings in the\nexisting automatic depression detection methods, we propose a novel\ncomputational framework for automatic depression detection that initially\nselects relevant content through a hybrid extractive and abstractive\nsummarization strategy on the sequence of all user tweets leading to a more\nfine-grained and relevant content. The content then goes to our novel deep\nlearning framework comprising of a unified learning machinery comprising of\nConvolutional Neural Network (CNN) coupled with attention-enhanced Gated\nRecurrent Units (GRU) models leading to better empirical performance than\nexisting strong baselines.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 08:05:53 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zogan", "Hamad", ""], ["Razzak", "Imran", ""], ["Jameel", "Shoaib", ""], ["Xu", "Guandong", ""]]}, {"id": "2105.10880", "submitter": "Yang Li", "authors": "Yang Li, Hermawan Mulyono, Ying Chen, Zhiyin Lu, Desmond Chan", "title": "RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the\n  US", "comments": "Source code: https://github.com/yangland/rtfps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Climate change has largely impacted our daily lives. As one of its\nconsequences, we are experiencing more wildfires. In the year 2020, wildfires\nburned a record number of 8,888,297 acres in the US. To awaken people's\nattention to climate change, and to visualize the current risk of wildfires, We\ndeveloped RtFPS, \"Real-Time Fire Prediction System\". It provides a real-time\nprediction visualization of wildfire risk at specific locations base on a\nMachine Learning model. It also provides interactive map features that show the\nhistorical wildfire events with environmental info.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 08:07:01 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 01:58:27 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Li", "Yang", ""], ["Mulyono", "Hermawan", ""], ["Chen", "Ying", ""], ["Lu", "Zhiyin", ""], ["Chan", "Desmond", ""]]}, {"id": "2105.10884", "submitter": "Jie Qiao", "authors": "Ruichu Cai, Siyu Wu, Jie Qiao, Zhifeng Hao, Keli Zhang, Xi Zhang", "title": "THP: Topological Hawkes Processes for Learning Granger Causality on\n  Event Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning Granger causality among event types on multi-type event sequences is\nan important but challenging task. Existing methods, such as the Multivariate\nHawkes processes, mostly assumed that each sequence is independent and\nidentically distributed. However, in many real-world applications, it is\ncommonplace to encounter a topological network behind the event sequences such\nthat an event is excited or inhibited not only by its history but also by its\ntopological neighbors. Consequently, the failure in describing the topological\ndependency among the event sequences leads to the error detection of the causal\nstructure. By considering the Hawkes processes from the view of temporal\nconvolution, we propose a Topological Hawkes processes (THP) to draw a\nconnection between the graph convolution in topology domain and the temporal\nconvolution in time domains. We further propose a Granger causality learning\nmethod on THP in a likelihood framework. The proposed method is featured with\nthe graph convolution-based likelihood function of THP and a sparse\noptimization scheme with an Expectation-Maximization of the likelihood\nfunction. Theoretical analysis and experiments on both synthetic and real-world\ndata demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 08:33:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cai", "Ruichu", ""], ["Wu", "Siyu", ""], ["Qiao", "Jie", ""], ["Hao", "Zhifeng", ""], ["Zhang", "Keli", ""], ["Zhang", "Xi", ""]]}, {"id": "2105.10909", "submitter": "Lingjuan Lyu", "authors": "Lingjuan Lyu, Xuanli He, Fangzhao Wu, Lichao Sun", "title": "Killing Two Birds with One Stone: Stealing Model and Inferring Attribute\n  from BERT-based APIs", "comments": "paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances in pre-trained models (e.g., BERT, XLNET and etc) have largely\nrevolutionized the predictive performance of various modern natural language\nprocessing tasks. This allows corporations to provide machine learning as a\nservice (MLaaS) by encapsulating fine-tuned BERT-based models as commercial\nAPIs. However, previous works have discovered a series of vulnerabilities in\nBERT- based APIs. For example, BERT-based APIs are vulnerable to both model\nextraction attack and adversarial example transferrability attack. However, due\nto the high capacity of BERT-based APIs, the fine-tuned model is easy to be\noverlearned, what kind of information can be leaked from the extracted model\nremains unknown and is lacking. To bridge this gap, in this work, we first\npresent an effective model extraction attack, where the adversary can\npractically steal a BERT-based API (the target/victim model) by only querying a\nlimited number of queries. We further develop an effective attribute inference\nattack to expose the sensitive attribute of the training data used by the\nBERT-based APIs. Our extensive experiments on benchmark datasets under various\nrealistic settings demonstrate the potential vulnerabilities of BERT-based\nAPIs.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 10:38:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Lyu", "Lingjuan", ""], ["He", "Xuanli", ""], ["Wu", "Fangzhao", ""], ["Sun", "Lichao", ""]]}, {"id": "2105.10911", "submitter": "Amin Beheshti", "authors": "Amin Beheshti, Boualem Benatallah, Hamid Reza Motahari-Nezhad, Samira\n  Ghodratnama, Farhad Amouzgar", "title": "A Query Language for Summarizing and Analyzing Business Process Data", "comments": "arXiv admin note: text overlap with arXiv:1908.09232 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern enterprises, Business Processes (BPs) are realized over a mix of\nworkflows, IT systems, Web services and direct collaborations of people.\nAccordingly, process data (i.e., BP execution data such as logs containing\nevents, interaction messages and other process artifacts) is scattered across\nseveral systems and data sources, and increasingly show all typical properties\nof the Big Data. Understanding the execution of process data is challenging as\nkey business insights remain hidden in the interactions among process entities:\nmost objects are interconnected, forming complex, heterogeneous but often\nsemi-structured networks. In the context of business processes, we consider the\nBig Data problem as a massive number of interconnected data islands from\npersonal, shared and business data. We present a framework to model process\ndata as graphs, i.e., Process Graph, and present abstractions to summarize the\nprocess graph and to discover concept hierarchies for entities based on both\ndata objects and their interactions in process graphs. We present a language,\nnamely BP-SPARQL, for the explorative querying and understanding of process\ngraphs from various user perspectives. We have implemented a scalable\narchitecture for querying, exploration and analysis of process graphs. We\nreport on experiments performed on both synthetic and real-world datasets that\nshow the viability and efficiency of the approach.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 11:07:53 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Beheshti", "Amin", ""], ["Benatallah", "Boualem", ""], ["Motahari-Nezhad", "Hamid Reza", ""], ["Ghodratnama", "Samira", ""], ["Amouzgar", "Farhad", ""]]}, {"id": "2105.10912", "submitter": "Dustin Wright", "authors": "Dustin Wright and Isabelle Augenstein", "title": "CiteWorth: Cite-Worthiness Detection for Improved Scientific Document\n  Understanding", "comments": "12 pages, 9 tables, 1 figure", "journal-ref": "Findings of ACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific document understanding is challenging as the data is highly domain\nspecific and diverse. However, datasets for tasks with scientific text require\nexpensive manual annotation and tend to be small and limited to only one or a\nfew fields. At the same time, scientific documents contain many potential\ntraining signals, such as citations, which can be used to build large labelled\ndatasets. Given this, we present an in-depth study of cite-worthiness detection\nin English, where a sentence is labelled for whether or not it cites an\nexternal source. To accomplish this, we introduce CiteWorth, a large,\ncontextualized, rigorously cleaned labelled dataset for cite-worthiness\ndetection built from a massive corpus of extracted plain-text scientific\ndocuments. We show that CiteWorth is high-quality, challenging, and suitable\nfor studying problems such as domain adaptation. Our best performing\ncite-worthiness detection model is a paragraph-level contextualized sentence\nlabelling model based on Longformer, exhibiting a 5 F1 point improvement over\nSciBERT which considers only individual sentences. Finally, we demonstrate that\nlanguage model fine-tuning with cite-worthiness as a secondary task leads to\nimproved performance on downstream scientific document understanding tasks.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 11:08:45 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 09:20:30 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wright", "Dustin", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2105.10915", "submitter": "Daniel Wilke", "authors": "Younghwan Chae, Daniel N. Wilke, Dominic Kafka", "title": "GOALS: Gradient-Only Approximations for Line Searches Towards Robust and\n  Consistent Training of Deep Neural Networks", "comments": "26 pages, 8 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mini-batch sub-sampling (MBSS) is favored in deep neural network training to\nreduce the computational cost. Still, it introduces an inherent sampling error,\nmaking the selection of appropriate learning rates challenging. The sampling\nerrors can manifest either as a bias or variances in a line search. Dynamic\nMBSS re-samples a mini-batch at every function evaluation. Hence, dynamic MBSS\nresults in point-wise discontinuous loss functions with smaller bias but larger\nvariance than static sampled loss functions. However, dynamic MBSS has the\nadvantage of having larger data throughput during training but requires the\ncomplexity regarding discontinuities to be resolved. This study extends the\ngradient-only surrogate (GOS), a line search method using quadratic\napproximation models built with only directional derivative information, for\ndynamic MBSS loss functions. We propose a gradient-only approximation line\nsearch (GOALS) with strong convergence characteristics with defined optimality\ncriterion. We investigate GOALS's performance by applying it on various\noptimizers that include SGD, RMSprop and Adam on ResNet-18 and EfficientNetB0.\nWe also compare GOALS's against the other existing learning rate methods. We\nquantify both the best performing and most robust algorithms. For the latter,\nwe introduce a relative robust criterion that allows us to quantify the\ndifference between an algorithm and the best performing algorithm for a given\nproblem. The results show that training a model with the recommended learning\nrate for a class of search directions helps to reduce the model errors in\nmultimodal cases.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 11:21:01 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chae", "Younghwan", ""], ["Wilke", "Daniel N.", ""], ["Kafka", "Dominic", ""]]}, {"id": "2105.10919", "submitter": "Piotr Mi{\\l}o\\'s", "authors": "Maciej Wo{\\l}czyk, Micha{\\l} Zaj\\k{a}c, Razvan Pascanu, {\\L}ukasz\n  Kuci\\'nski, Piotr Mi{\\l}o\\'s", "title": "Continual World: A Robotic Benchmark For Continual Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning (CL) -- the ability to continuously learn, building on\npreviously acquired knowledge -- is a natural requirement for long-lived\nautonomous reinforcement learning (RL) agents. While building such agents, one\nneeds to balance opposing desiderata, such as constraints on capacity and\ncompute, the ability to not catastrophically forget, and to exhibit positive\ntransfer on new tasks. Understanding the right trade-off is conceptually and\ncomputationally challenging, which we argue has led the community to overly\nfocus on catastrophic forgetting. In response to these issues, we advocate for\nthe need to prioritize forward transfer and propose Continual World, a\nbenchmark consisting of realistic and meaningfully diverse robotic tasks built\non top of Meta-World as a testbed. Following an in-depth empirical evaluation\nof existing CL methods, we pinpoint their limitations and highlight unique\nalgorithmic challenges in the RL setting. Our benchmark aims to provide a\nmeaningful and computationally inexpensive challenge for the community and thus\nhelp better understand the performance of existing and future solutions.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 11:33:04 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 13:03:28 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wo\u0142czyk", "Maciej", ""], ["Zaj\u0105c", "Micha\u0142", ""], ["Pascanu", "Razvan", ""], ["Kuci\u0144ski", "\u0141ukasz", ""], ["Mi\u0142o\u015b", "Piotr", ""]]}, {"id": "2105.10937", "submitter": "Marco Visca", "authors": "Marco Visca, Sampo Kuutti, Roger Powell, Yang Gao and Saber Fallah", "title": "Deep Learning Traversability Estimator for Mobile Robots in Unstructured\n  Environments", "comments": "Accepted for inclusion in Towards Autonomous Robotic Systems\n  Conference (TAROS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Terrain traversability analysis plays a major role in ensuring safe robotic\nnavigation in unstructured environments. However, real-time constraints\nfrequently limit the accuracy of online tests especially in scenarios where\nrealistic robot-terrain interactions are complex to model. In this context, we\npropose a deep learning framework trained in an end-to-end fashion from\nelevation maps and trajectories to estimate the occurrence of failure events.\nThe network is first trained and tested in simulation over synthetic maps\ngenerated by the OpenSimplex algorithm. The prediction performance of the Deep\nLearning framework is illustrated by being able to retain over 94% recall of\nthe original simulator at 30% of the computational time. Finally, the network\nis transferred and tested on real elevation maps collected by the SEEKER\nconsortium during the Martian rover test trial in the Atacama desert in Chile.\nWe show that transferring and fine-tuning of an application-independent\npre-trained model retains better performance than training uniquely on scarcely\navailable real data.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 13:49:05 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 15:03:56 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Visca", "Marco", ""], ["Kuutti", "Sampo", ""], ["Powell", "Roger", ""], ["Gao", "Yang", ""], ["Fallah", "Saber", ""]]}, {"id": "2105.10948", "submitter": "Javier Carnerero-Cano", "authors": "Javier Carnerero-Cano, Luis Mu\\~noz-Gonz\\'alez, Phillippa Spencer,\n  Emil C. Lupu", "title": "Regularization Can Help Mitigate Poisoning Attacks... with the Right\n  Hyperparameters", "comments": "Published at ICLR 2021 Workshop on Security and Safety in Machine\n  Learning Systems. arXiv admin note: text overlap with arXiv:2003.00040", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning algorithms are vulnerable to poisoning attacks, where a\nfraction of the training data is manipulated to degrade the algorithms'\nperformance. We show that current approaches, which typically assume that\nregularization hyperparameters remain constant, lead to an overly pessimistic\nview of the algorithms' robustness and of the impact of regularization. We\npropose a novel optimal attack formulation that considers the effect of the\nattack on the hyperparameters, modelling the attack as a \\emph{minimax bilevel\noptimization problem}. This allows to formulate optimal attacks, select\nhyperparameters and evaluate robustness under worst case conditions. We apply\nthis formulation to logistic regression using $L_2$ regularization, empirically\nshow the limitations of previous strategies and evidence the benefits of using\n$L_2$ regularization to dampen the effect of poisoning attacks.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 14:34:47 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Carnerero-Cano", "Javier", ""], ["Mu\u00f1oz-Gonz\u00e1lez", "Luis", ""], ["Spencer", "Phillippa", ""], ["Lupu", "Emil C.", ""]]}, {"id": "2105.10959", "submitter": "Mohamed Hamama", "authors": "Mohamed Hamama", "title": "A Study imbalance handling by various data sampling methods in binary\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this research report is to present the our learning curve and\nthe exposure to the Machine Learning life cycle, with the use of a Kaggle\nbinary classification data set and taking to explore various techniques from\npre-processing to the final optimization and model evaluation, also we\nhighlight on the data imbalance issue and we discuss the different methods of\nhandling that imbalance on the data level by over-sampling and under sampling\nnot only to reach a balanced class representation but to improve the overall\nperformance. This work also opens some gaps for future work.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 15:27:47 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Hamama", "Mohamed", ""]]}, {"id": "2105.11004", "submitter": "Aleksandros Sobczyk", "authors": "Aleksandros Sobczyk (1) and Efstratios Gallopoulos (2) ((1) IBM\n  Research Europe, Zurich, Switzerland (2) Computer Engineering and Informatics\n  Department, University of Patras, Greece)", "title": "Estimating leverage scores via rank revealing methods and randomization", "comments": "To appear in SIAM Journal on Matrix Analysis and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study algorithms for estimating the statistical leverage scores of\nrectangular dense or sparse matrices of arbitrary rank. Our approach is based\non combining rank revealing methods with compositions of dense and sparse\nrandomized dimensionality reduction transforms. We first develop a set of fast\nnovel algorithms for rank estimation, column subset selection and least squares\npreconditioning. We then describe the design and implementation of leverage\nscore estimators based on these primitives. These estimators are also effective\nfor rank deficient input, which is frequently the case in data analytics\napplications. We provide detailed complexity analyses for all algorithms as\nwell as meaningful approximation bounds and comparisons with the\nstate-of-the-art. We conduct extensive numerical experiments to evaluate our\nalgorithms and to illustrate their properties and performance using synthetic\nand real world data sets.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 19:21:55 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sobczyk", "Aleksandros", ""], ["Gallopoulos", "Efstratios", ""]]}, {"id": "2105.11010", "submitter": "Gil Shomron", "authors": "Gil Shomron, Freddy Gabbay, Samer Kurzum, Uri Weiser", "title": "Post-Training Sparsity-Aware Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization is a technique used in deep neural networks (DNNs) to increase\nexecution performance and hardware efficiency. Uniform post-training\nquantization (PTQ) methods are common, since they can be implemented\nefficiently in hardware and do not require extensive hardware resources or a\ntraining set. Mapping FP32 models to INT8 using uniform PTQ yields models with\nnegligible accuracy degradation; however, reducing precision below 8 bits with\nPTQ is challenging, as accuracy degradation becomes noticeable, due to the\nincrease in quantization noise. In this paper, we propose a sparsity-aware\nquantization (SPARQ) method, in which the unstructured and dynamic activation\nsparsity is leveraged in different representation granularities. 4-bit\nquantization, for example, is employed by dynamically examining the bits of\n8-bit values and choosing a window of 4 bits, while first skipping zero-value\nbits. Moreover, instead of quantizing activation-by-activation to 4 bits, we\nfocus on pairs of 8-bit activations and examine whether one of the two is equal\nto zero. If one is equal to zero, the second can opportunistically use the\nother's 4-bit budget; if both do not equal zero, then each is dynamically\nquantized to 4 bits, as described. SPARQ achieves minor accuracy degradation,\n2x speedup over widely used hardware architectures, and a practical hardware\nimplementation. The code is available at https://github.com/gilshm/sparq.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 20:12:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shomron", "Gil", ""], ["Gabbay", "Freddy", ""], ["Kurzum", "Samer", ""], ["Weiser", "Uri", ""]]}, {"id": "2105.11013", "submitter": "Emna Baccour", "authors": "Mohammed Jouhari, Abdulla Al-Ali, Emna Baccour, Amr Mohamed, Aiman\n  Erbad, Mohsen Guizani, Mounir Hamdi", "title": "Distributed CNN Inference on Resource-Constrained UAVs for Surveillance\n  Systems: Design and Optimization", "comments": "Accepted in IEEE Internet of Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2021.3079164", "report-no": null, "categories": "cs.DC cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs) have attracted great interest in the last few\nyears owing to their ability to cover large areas and access difficult and\nhazardous target zones, which is not the case of traditional systems relying on\ndirect observations obtained from fixed cameras and sensors. Furthermore,\nthanks to the advancements in computer vision and machine learning, UAVs are\nbeing adopted for a broad range of solutions and applications. However, Deep\nNeural Networks (DNNs) are progressing toward deeper and complex models that\nprevent them from being executed on-board. In this paper, we propose a DNN\ndistribution methodology within UAVs to enable data classification in\nresource-constrained devices and avoid extra delays introduced by the\nserver-based solutions due to data communication over air-to-ground links. The\nproposed method is formulated as an optimization problem that aims to minimize\nthe latency between data collection and decision-making while considering the\nmobility model and the resource constraints of the UAVs as part of the\nair-to-air communication. We also introduce the mobility prediction to adapt\nour system to the dynamics of UAVs and the network variation. The simulation\nconducted to evaluate the performance and benchmark the proposed methods,\nnamely Optimal UAV-based Layer Distribution (OULD) and OULD with Mobility\nPrediction (OULD-MP), were run in an HPC cluster. The obtained results show\nthat our optimization solution outperforms the existing and heuristic-based\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 20:19:43 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Jouhari", "Mohammed", ""], ["Al-Ali", "Abdulla", ""], ["Baccour", "Emna", ""], ["Mohamed", "Amr", ""], ["Erbad", "Aiman", ""], ["Guizani", "Mohsen", ""], ["Hamdi", "Mounir", ""]]}, {"id": "2105.11018", "submitter": "Lei Sha", "authors": "Lei Sha, Patrick Hohenecker, Thomas Lukasiewicz", "title": "Controlling Text Edition by Changing Answers of Specific Questions", "comments": null, "journal-ref": "ACL 2021 findings", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce the new task of controllable text edition, in\nwhich we take as input a long text, a question, and a target answer, and the\noutput is a minimally modified text, so that it fits the target answer. This\ntask is very important in many situations, such as changing some conditions,\nconsequences, or properties in a legal document, or changing some key\ninformation of an event in a news text. This is very challenging, as it is hard\nto obtain a parallel corpus for training, and we need to first find all text\npositions that should be changed and then decide how to change them. We\nconstructed the new dataset WikiBioCTE for this task based on the existing\ndataset WikiBio (originally created for table-to-text generation). We use\nWikiBioCTE for training, and manually labeled a test set for testing. We also\npropose novel evaluation metrics and a novel method for solving the new task.\nExperimental results on the test set show that our proposed method is a good\nfit for this novel NLP task.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 20:44:15 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sha", "Lei", ""], ["Hohenecker", "Patrick", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2105.11021", "submitter": "Pasccal Fischer", "authors": "Pascal Fischer, Alen Smajic, Alexander Mehler, Giuseppe Abrami", "title": "Multi-Type-TD-TSR -- Extracting Tables from Document Images using a\n  Multi-stage Pipeline for Table Detection and Table Structure Recognition:\n  from OCR to Structured Table Representations", "comments": "8 pages, 8 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As global trends are shifting towards data-driven industries, the demand for\nautomated algorithms that can convert digital images of scanned documents into\nmachine readable information is rapidly growing. Besides the opportunity of\ndata digitization for the application of data analytic tools, there is also a\nmassive improvement towards automation of processes, which previously would\nrequire manual inspection of the documents. Although the introduction of\noptical character recognition technologies mostly solved the task of converting\nhuman-readable characters from images into machine-readable characters, the\ntask of extracting table semantics has been less focused on over the years. The\nrecognition of tables consists of two main tasks, namely table detection and\ntable structure recognition. Most prior work on this problem focuses on either\ntask without offering an end-to-end solution or paying attention to real\napplication conditions like rotated images or noise artefacts inside the\ndocument image. Recent work shows a clear trend towards deep learning\napproaches coupled with the use of transfer learning for the task of table\nstructure recognition due to the lack of sufficiently large datasets. In this\npaper we present a multistage pipeline named Multi-Type-TD-TSR, which offers an\nend-to-end solution for the problem of table recognition. It utilizes\nstate-of-the-art deep learning models for table detection and differentiates\nbetween 3 different types of tables based on the tables' borders. For the table\nstructure recognition we use a deterministic non-data driven algorithm, which\nworks on all table types. We additionally present two algorithms. One for\nunbordered tables and one for bordered tables, which are the base of the used\ntable structure recognition algorithm. We evaluate Multi-Type-TD-TSR on the\nICDAR 2019 table structure recognition dataset and achieve a new\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 21:17:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Fischer", "Pascal", ""], ["Smajic", "Alen", ""], ["Mehler", "Alexander", ""], ["Abrami", "Giuseppe", ""]]}, {"id": "2105.11025", "submitter": "John Shin", "authors": "John Y. Shin", "title": "Compressing Heavy-Tailed Weight Matrices for Non-Vacuous Generalization\n  Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed distributions have been studied in statistics, random matrix\ntheory, physics, and econometrics as models of correlated systems, among other\ndomains. Further, heavy-tail distributed eigenvalues of the covariance matrix\nof the weight matrices in neural networks have been shown to empirically\ncorrelate with test set accuracy in several works (e.g. arXiv:1901.08276), but\na formal relationship between heavy-tail distributed parameters and\ngeneralization bounds was yet to be demonstrated. In this work, the compression\nframework of arXiv:1802.05296 is utilized to show that matrices with heavy-tail\ndistributed matrix elements can be compressed, resulting in networks with\nsparse weight matrices. Since the parameter count has been reduced to a sum of\nthe non-zero elements of sparse matrices, the compression framework allows us\nto bound the generalization gap of the resulting compressed network with a\nnon-vacuous generalization bound. Further, the action of these matrices on a\nvector is discussed, and how they may relate to compression and resilient\nclassification is analyzed.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 21:36:33 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shin", "John Y.", ""]]}, {"id": "2105.11028", "submitter": "Milad Khademinori", "authors": "Milad Khademi Nori, Sangseok Yun, and Il-Min Kim", "title": "Fast Federated Learning by Balancing Communication Trade-Offs", "comments": "14 pages, 24 figures, accepted for publication in IEEE Transactions\n  on Communications", "journal-ref": null, "doi": "10.1109/TCOMM.2021.3083316", "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) has recently received a lot of attention for\nlarge-scale privacy-preserving machine learning. However, high communication\noverheads due to frequent gradient transmissions decelerate FL. To mitigate the\ncommunication overheads, two main techniques have been studied: (i) local\nupdate of weights characterizing the trade-off between communication and\ncomputation and (ii) gradient compression characterizing the trade-off between\ncommunication and precision. To the best of our knowledge, studying and\nbalancing those two trade-offs jointly and dynamically while considering their\nimpacts on convergence has remained unresolved even though it promises\nsignificantly faster FL. In this paper, we first formulate our problem to\nminimize learning error with respect to two variables: local update\ncoefficients and sparsity budgets of gradient compression who characterize\ntrade-offs between communication and computation/precision, respectively. We\nthen derive an upper bound of the learning error in a given wall-clock time\nconsidering the interdependency between the two variables. Based on this\ntheoretical analysis, we propose an enhanced FL scheme, namely Fast FL (FFL),\nthat jointly and dynamically adjusts the two variables to minimize the learning\nerror. We demonstrate that FFL consistently achieves higher accuracies faster\nthan similar schemes existing in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 21:55:14 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Nori", "Milad Khademi", ""], ["Yun", "Sangseok", ""], ["Kim", "Il-Min", ""]]}, {"id": "2105.11043", "submitter": "Huy Phan", "authors": "Huy Phan, Kaare Mikkelsen, Oliver Y. Ch\\'en, Philipp Koch, Alfred\n  Mertins, Maarten De Vos", "title": "SleepTransformer: Automatic Sleep Staging with Interpretability and\n  Uncertainty Quantification", "comments": "Corrected equations, 17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box skepticism is one of the main hindrances impeding\ndeep-learning-based automatic sleep scoring from being used in clinical\nenvironments. Towards interpretability, this work proposes a\nsequence-to-sequence sleep-staging model, namely SleepTransformer. It is based\non the transformer backbone whose self-attention scores offer interpretability\nof the model's decisions at both the epoch and sequence level. At the epoch\nlevel, the attention scores can be encoded as a heat map to highlight\nsleep-relevant features captured from the input EEG signal. At the sequence\nlevel, the attention scores are visualized as the influence of different\nneighboring epochs in an input sequence (i.e. the context) to recognition of a\ntarget epoch, mimicking the way manual scoring is done by human experts. We\nfurther propose a simple yet efficient method to quantify uncertainty in the\nmodel's decisions. The method, which is based on entropy, can serve as a metric\nfor deferring low-confidence epochs to a human expert for further inspection.\nAdditionally, we demonstrate that the proposed SleepTransformer outperforms\nexisting methods at a lower computational cost and achieves state-of-the-art\nperformance on two experimental databases of different sizes.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 23:30:58 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 11:35:39 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Phan", "Huy", ""], ["Mikkelsen", "Kaare", ""], ["Ch\u00e9n", "Oliver Y.", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""], ["De Vos", "Maarten", ""]]}, {"id": "2105.11045", "submitter": "Yuankai Teng", "authors": "Yuankai Teng, Xiaoping Zhang, Zhu Wang, Lili Ju", "title": "Learning Green's Functions of Linear Reaction-Diffusion Equations with\n  Application to Fast Numerical Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial differential equations are often used to model various physical\nphenomena, such as heat diffusion, wave propagation, fluid dynamics,\nelasticity, electrodynamics and image processing, and many analytic approaches\nor traditional numerical methods have been developed and widely used for their\nsolutions. Inspired by rapidly growing impact of deep learning on scientific\nand engineering research, in this paper we propose a novel neural network,\nGF-Net, for learning the Green's functions of linear reaction-diffusion\nequations in an unsupervised fashion. The proposed method overcomes the\nchallenges for finding the Green's functions of the equations on arbitrary\ndomains by utilizing physics-informed approach and the symmetry of the Green's\nfunction. As a consequence, it particularly leads to an efficient way for\nsolving the target equations under different boundary conditions and sources.\nWe also demonstrate the effectiveness of the proposed approach by experiments\nin square, annular and L-shape domains.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 23:36:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Teng", "Yuankai", ""], ["Zhang", "Xiaoping", ""], ["Wang", "Zhu", ""], ["Ju", "Lili", ""]]}, {"id": "2105.11060", "submitter": "Miguel Angel Saavedra Ruiz", "authors": "Gustavo A. Salazar-Gomez, Miguel A. Saavedra-Ruiz, Victor A.\n  Romero-Cano", "title": "High-level camera-LiDAR fusion for 3D object detection with machine\n  learning", "comments": "LatinX Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the 3D object detection problem, which is of vital\nimportance for applications such as autonomous driving. Our framework uses a\nMachine Learning (ML) pipeline on a combination of monocular camera and LiDAR\ndata to detect vehicles in the surrounding 3D space of a moving platform. It\nuses frustum region proposals generated by State-Of-The-Art (SOTA) 2D object\ndetectors to segment LiDAR point clouds into point clusters which represent\npotentially individual objects. We evaluate the performance of classical ML\nalgorithms as part of an holistic pipeline for estimating the parameters of 3D\nbounding boxes which surround the vehicles around the moving platform. Our\nresults demonstrate an efficient and accurate inference on a validation set,\nachieving an overall accuracy of 87.1%.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 01:57:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Salazar-Gomez", "Gustavo A.", ""], ["Saavedra-Ruiz", "Miguel A.", ""], ["Romero-Cano", "Victor A.", ""]]}, {"id": "2105.11062", "submitter": "Ting Pan", "authors": "Ting Pan and Zhuqing Jiang and Jianan Han and Shiping Wen and Aidong\n  Men and Haiying Wang", "title": "Taylor saves for later: disentanglement for video prediction using\n  Taylor representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction is a challenging task with wide application prospects in\nmeteorology and robot systems. Existing works fail to trade off short-term and\nlong-term prediction performances and extract robust latent dynamics laws in\nvideo frames. We propose a two-branch seq-to-seq deep model to disentangle the\nTaylor feature and the residual feature in video frames by a novel recurrent\nprediction module (TaylorCell) and residual module. TaylorCell can expand the\nvideo frames' high-dimensional features into the finite Taylor series to\ndescribe the latent laws. In TaylorCell, we propose the Taylor prediction unit\n(TPU) and the memory correction unit (MCU). TPU employs the first input frame's\nderivative information to predict the future frames, avoiding error\naccumulation. MCU distills all past frames' information to correct the\npredicted Taylor feature from TPU. Correspondingly, the residual module\nextracts the residual feature complementary to the Taylor feature. On three\ngeneralist datasets (Moving MNIST, TaxiBJ, Human 3.6), our model outperforms or\nreaches state-of-the-art models, and ablation experiments demonstrate the\neffectiveness of our model in long-term prediction.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 01:59:21 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Pan", "Ting", ""], ["Jiang", "Zhuqing", ""], ["Han", "Jianan", ""], ["Wen", "Shiping", ""], ["Men", "Aidong", ""], ["Wang", "Haiying", ""]]}, {"id": "2105.11066", "submitter": "Shicong Cen", "authors": "Wenhao Zhan, Shicong Cen, Baihe Huang, Yuxin Chen, Jason D. Lee,\n  Yuejie Chi", "title": "Policy Mirror Descent for Regularized Reinforcement Learning: A\n  Generalized Framework with Linear Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy optimization, which learns the policy of interest by maximizing the\nvalue function via large-scale optimization techniques, lies at the heart of\nmodern reinforcement learning (RL). In addition to value maximization, other\npractical considerations arise commonly as well, including the need of\nencouraging exploration, and that of ensuring certain structural properties of\nthe learned policy due to safety, resource and operational constraints. These\nconsiderations can often be accounted for by resorting to regularized RL, which\naugments the target value function with a structure-promoting regularization\nterm.\n  Focusing on an infinite-horizon discounted Markov decision process, this\npaper proposes a generalized policy mirror descent (GPMD) algorithm for solving\nregularized RL. As a generalization of policy mirror descent Lan (2021), the\nproposed algorithm accommodates a general class of convex regularizers as well\nas a broad family of Bregman divergence in cognizant of the regularizer in use.\nWe demonstrate that our algorithm converges linearly over an entire range of\nlearning rates, in a dimension-free fashion, to the global solution, even when\nthe regularizer lacks strong convexity and smoothness. In addition, this linear\nconvergence feature is provably stable in the face of inexact policy evaluation\nand imperfect policy updates. Numerical experiments are provided to corroborate\nthe applicability and appealing performance of GPMD.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 02:21:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhan", "Wenhao", ""], ["Cen", "Shicong", ""], ["Huang", "Baihe", ""], ["Chen", "Yuxin", ""], ["Lee", "Jason D.", ""], ["Chi", "Yuejie", ""]]}, {"id": "2105.11069", "submitter": "Jian Kang", "authors": "Jian Kang, Tiankai Xie, Xintao Wu, Ross Maciejewski, Hanghang Tong", "title": "MultiFair: Multi-Group Fairness in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithmic fairness is becoming increasingly important in data mining and\nmachine learning, and one of the most fundamental notions is group fairness.\nThe vast majority of the existing works on group fairness, with a few\nexceptions, primarily focus on debiasing with respect to a single sensitive\nattribute, despite the fact that the co-existence of multiple sensitive\nattributes (e.g., gender, race, marital status, etc.) in the real-world is\ncommonplace. As such, methods that can ensure a fair learning outcome with\nrespect to all sensitive attributes of concern simultaneously need to be\ndeveloped. In this paper, we study multi-group fairness in machine learning\n(MultiFair), where statistical parity, a representative group fairness measure,\nis guaranteed among demographic groups formed by multiple sensitive attributes\nof interest. We formulate it as a mutual information minimization problem and\npropose a generic end-to-end algorithmic framework to solve it. The key idea is\nto leverage a variational representation of mutual information, which considers\nthe variational distribution between learning outcomes and sensitive\nattributes, as well as the density ratio between the variational and the\noriginal distributions. Our proposed framework is generalizable to many\ndifferent settings, including other statistical notions of fairness, and could\nhandle any type of learning task equipped with a gradient-based optimizer.\nEmpirical evaluations in the fair classification task on three real-world\ndatasets demonstrate that our proposed framework can effectively debias the\nclassification results with minimal impact to the classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 02:30:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kang", "Jian", ""], ["Xie", "Tiankai", ""], ["Wu", "Xintao", ""], ["Maciejewski", "Ross", ""], ["Tong", "Hanghang", ""]]}, {"id": "2105.11082", "submitter": "Shrikanth N.C.", "authors": "N.C. Shrikanth and Tim Menzies", "title": "The Early Bird Catches the Worm: Better Early Life Cycle Defect\n  Predictors", "comments": "15 pages (In Review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before researchers rush to reason across all available data, they should\nfirst check if the information is densest within some small region. We say this\nsince, in 240 GitHub projects, we find that the information in that data\n``clumps'' towards the earliest parts of the project. In fact, a defect\nprediction model learned from just the first 150 commits works as well, or\nbetter than state-of-the-art alternatives. Using just this early life cycle\ndata, we can build models very quickly (using weeks, not months, of CPU time).\nAlso, we can find simple models (with just two features) that generalize to\nhundreds of software projects. Based on this experience, we warn that prior\nwork on generalizing software engineering defect prediction models may have\nneedlessly complicated an inherently simple process. Further, prior work that\nfocused on later-life cycle data now needs to be revisited since their\nconclusions were drawn from relatively uninformative regions. Replication note:\nall our data and scripts are online at\nhttps://github.com/snaraya7/early-defect-prediction-tse.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 03:49:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shrikanth", "N. C.", ""], ["Menzies", "Tim", ""]]}, {"id": "2105.11085", "submitter": "Haijin Wang", "authors": "Haijin Wang, Caomingzhe Si, Junhua Zhao, Guolong Liu, Fushuan Wen", "title": "Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring\n  Method for Privacy-Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive load monitoring (NILM) is essential for understanding\ncustomer's power consumption patterns and may find wide applications like\ncarbon emission reduction and energy conservation. The training of NILM models\nrequires massive load data containing different types of appliances. However,\ninadequate load data and the risk of power consumer privacy breaches may be\nencountered by local data owners during the NILM model training. To prevent\nsuch potential risks, a novel NILM method named Fed-NILM which is based on\nFederated Learning (FL) is proposed in this paper. In Fed-NILM, local model\nparameters instead of local load data are shared among multiple data owners.\nThe global model is obtained by weighted averaging the parameters. Experiments\nbased on two measured load datasets are conducted to explore the generalization\nability of Fed-NILM. Besides, a comparison of Fed-NILM with locally-trained\nNILMs and the centrally-trained NILM is conducted. The experimental results\nshow that Fed-NILM has superior performance in scalability and convergence.\nFed-NILM outperforms locally-trained NILMs operated by local data owners and\napproximates the centrally-trained NILM which is trained on the entire load\ndataset without privacy protection. The proposed Fed-NILM significantly\nimproves the co-modeling capabilities of local data owners while protecting\npower consumers' privacy.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 04:12:10 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 13:04:03 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Haijin", ""], ["Si", "Caomingzhe", ""], ["Zhao", "Junhua", ""], ["Liu", "Guolong", ""], ["Wen", "Fushuan", ""]]}, {"id": "2105.11095", "submitter": "Shadrokh Samavi", "authors": "Maedeh Jamali, Nader Karim, Pejman Khadivi, Shahram Shirani, Shadrokh\n  Samavi", "title": "Robust Watermarking using Diffusion of Logo into Autoencoder Feature\n  Maps", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital contents have grown dramatically in recent years, leading to\nincreased attention to copyright. Image watermarking has been considered one of\nthe most popular methods for copyright protection. With the recent advancements\nin applying deep neural networks in image processing, these networks have also\nbeen used in image watermarking. Robustness and imperceptibility are two\nchallenging features of watermarking methods that the trade-off between them\nshould be satisfied. In this paper, we propose to use an end-to-end network for\nwatermarking. We use a convolutional neural network (CNN) to control the\nembedding strength based on the image content. Dynamic embedding helps the\nnetwork to have the lowest effect on the visual quality of the watermarked\nimage. Different image processing attacks are simulated as a network layer to\nimprove the robustness of the model. Our method is a blind watermarking\napproach that replicates the watermark string to create a matrix of the same\nsize as the input image. Instead of diffusing the watermark data into the input\nimage, we inject the data into the feature space and force the network to do\nthis in regions that increase the robustness against various attacks.\nExperimental results show the superiority of the proposed method in terms of\nimperceptibility and robustness compared to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 05:18:33 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Jamali", "Maedeh", ""], ["Karim", "Nader", ""], ["Khadivi", "Pejman", ""], ["Shirani", "Shahram", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2105.11099", "submitter": "Huanding Zhang", "authors": "Huanding Zhang, Tao Shen, Fei Wu, Mingyang Yin, Hongxia Yang, Chao Wu", "title": "Federated Graph Learning -- A Position Paper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNN) have been successful in many fields, and derived\nvarious researches and applications in real industries. However, in some\nprivacy sensitive scenarios (like finance, healthcare), training a GNN model\ncentrally faces challenges due to the distributed data silos. Federated\nlearning (FL) is a an emerging technique that can collaboratively train a\nshared model while keeping the data decentralized, which is a rational solution\nfor distributed GNN training. We term it as federated graph learning (FGL).\nAlthough FGL has received increasing attention recently, the definition and\nchallenges of FGL is still up in the air. In this position paper, we present a\ncategorization to clarify it. Considering how graph data are distributed among\nclients, we propose four types of FGL: inter-graph FL, intra-graph FL and\ngraph-structured FL, where intra-graph is further divided into horizontal and\nvertical FGL. For each type of FGL, we make a detailed discussion about the\nformulation and applications, and propose some potential challenges.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 05:39:24 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Huanding", ""], ["Shen", "Tao", ""], ["Wu", "Fei", ""], ["Yin", "Mingyang", ""], ["Yang", "Hongxia", ""], ["Wu", "Chao", ""]]}, {"id": "2105.11118", "submitter": "John Thorpe", "authors": "John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou Hu,\n  Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, Guoqing\n  Harry Xu", "title": "Dorylus: Affordable, Scalable, and Accurate GNN Training with\n  Distributed CPU Servers and Serverless Threads", "comments": "Paper accepted in OSDI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph neural network (GNN) enables deep learning on structured graph data.\nThere are two major GNN training obstacles: 1) it relies on high-end servers\nwith many GPUs which are expensive to purchase and maintain, and 2) limited\nmemory on GPUs cannot scale to today's billion-edge graphs. This paper presents\nDorylus: a distributed system for training GNNs. Uniquely, Dorylus can take\nadvantage of serverless computing to increase scalability at a low cost.\n  The key insight guiding our design is computation separation. Computation\nseparation makes it possible to construct a deep, bounded-asynchronous pipeline\nwhere graph and tensor parallel tasks can fully overlap, effectively hiding the\nnetwork latency incurred by Lambdas. With the help of thousands of Lambda\nthreads, Dorylus scales GNN training to billion-edge graphs. Currently, for\nlarge graphs, CPU servers offer the best performance-per-dollar over GPU\nservers. Just using Lambdas on top of CPU servers offers up to 2.75x more\nperformance-per-dollar than training only with CPU servers. Concretely, Dorylus\nis 1.22x faster and 4.83x cheaper than GPU servers for massive sparse graphs.\nDorylus is up to 3.8x faster and 10.7x cheaper compared to existing\nsampling-based systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 06:49:08 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 01:14:05 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Thorpe", "John", ""], ["Qiao", "Yifan", ""], ["Eyolfson", "Jonathan", ""], ["Teng", "Shen", ""], ["Hu", "Guanzhou", ""], ["Jia", "Zhihao", ""], ["Wei", "Jinliang", ""], ["Vora", "Keval", ""], ["Netravali", "Ravi", ""], ["Kim", "Miryung", ""], ["Xu", "Guoqing Harry", ""]]}, {"id": "2105.11122", "submitter": "Le Yu", "authors": "Le Yu, Leilei Sun, Bowen Du, Chuanren Liu, Weifeng Lv, Hui Xiong", "title": "Heterogeneous Graph Representation Learning with Relation Awareness", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning on heterogeneous graphs aims to obtain meaningful\nnode representations to facilitate various downstream tasks, such as node\nclassification and link prediction. Existing heterogeneous graph learning\nmethods are primarily developed by following the propagation mechanism of node\nrepresentations. There are few efforts on studying the role of relations for\nimproving the learning of more fine-grained node representations. Indeed, it is\nimportant to collaboratively learn the semantic representations of relations\nand discern node representations with respect to different relation types. To\nthis end, in this paper, we propose a novel Relation-aware Heterogeneous Graph\nNeural Network, namely R-HGNN, to learn node representations on heterogeneous\ngraphs at a fine-grained level by considering relation-aware characteristics.\nSpecifically, a dedicated graph convolution component is first designed to\nlearn unique node representations from each relation-specific graph separately.\nThen, a cross-relation message passing module is developed to improve the\ninteractions of node representations across different relations. Also, the\nrelation representations are learned in a layer-wise manner to capture relation\nsemantics, which are used to guide the node representation learning process.\nMoreover, a semantic fusing module is presented to aggregate relation-aware\nnode representations into a compact representation with the learned relation\nrepresentations. Finally, we conduct extensive experiments on a variety of\ngraph learning tasks, and experimental results demonstrate that our approach\nconsistently outperforms existing methods among all the tasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 07:01:41 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yu", "Le", ""], ["Sun", "Leilei", ""], ["Du", "Bowen", ""], ["Liu", "Chuanren", ""], ["Lv", "Weifeng", ""], ["Xiong", "Hui", ""]]}, {"id": "2105.11126", "submitter": "Kun Wang", "authors": "Kun Wang, Jing Dong, Baoxiang Wang, Shuai Li, Shuo Shao", "title": "Cascading Bandit under Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies \\emph{differential privacy (DP)} and \\emph{local\ndifferential privacy (LDP)} in cascading bandits. Under DP, we propose an\nalgorithm which guarantees $\\epsilon$-indistinguishability and a regret of\n$\\mathcal{O}((\\frac{\\log T}{\\epsilon})^{1+\\xi})$ for an arbitrarily small\n$\\xi$. This is a significant improvement from the previous work of\n$\\mathcal{O}(\\frac{\\log^3 T}{\\epsilon})$ regret. Under\n($\\epsilon$,$\\delta$)-LDP, we relax the $K^2$ dependence through the tradeoff\nbetween privacy budget $\\epsilon$ and error probability $\\delta$, and obtain a\nregret of $\\mathcal{O}(\\frac{K\\log (1/\\delta) \\log T}{\\epsilon^2})$, where $K$\nis the size of the arm subset. This result holds for both Gaussian mechanism\nand Laplace mechanism by analyses on the composition. Our results extend to\ncombinatorial semi-bandit. We show respective lower bounds for DP and LDP\ncascading bandits. Extensive experiments corroborate our theoretic findings.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 07:19:01 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 06:53:11 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Kun", ""], ["Dong", "Jing", ""], ["Wang", "Baoxiang", ""], ["Li", "Shuai", ""], ["Shao", "Shuo", ""]]}, {"id": "2105.11135", "submitter": "Matthew J. Holland", "authors": "Matthew J. Holland", "title": "Robust learning with anytime-guaranteed feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under data distributions which may be heavy-tailed, many stochastic\ngradient-based learning algorithms are driven by feedback queried at points\nwith almost no performance guarantees on their own. Here we explore a modified\n\"anytime online-to-batch\" mechanism which for smooth objectives admits\nhigh-probability error bounds while requiring only lower-order moment bounds on\nthe stochastic gradients. Using this conversion, we can derive a wide variety\nof \"anytime robust\" procedures, for which the task of performance analysis can\nbe effectively reduced to regret control, meaning that existing regret bounds\n(for the bounded gradient case) can be robustified and leveraged in a\nstraightforward manner. As a direct takeaway, we obtain an easily implemented\nstochastic gradient-based algorithm for which all queried points formally enjoy\nsub-Gaussian error bounds, and in practice show noteworthy gains on real-world\ndata applications.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 07:31:52 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Holland", "Matthew J.", ""]]}, {"id": "2105.11144", "submitter": "Mingyang Yi", "authors": "Mingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu,\n  Zhi-Ming Ma", "title": "Improved OOD Generalization via Adversarial Training and Pre-training", "comments": "published in ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning a model that generalizes well on out-of-distribution (OOD)\ndata has attracted great attention in the machine learning community. In this\npaper, after defining OOD generalization via Wasserstein distance, we\ntheoretically show that a model robust to input perturbation generalizes well\non OOD data. Inspired by previous findings that adversarial training helps\nimprove input-robustness, we theoretically show that adversarially trained\nmodels have converged excess risk on OOD data, and empirically verify it on\nboth image classification and natural language understanding tasks. Besides, in\nthe paradigm of first pre-training and then fine-tuning, we theoretically show\nthat a pre-trained model that is more robust to input perturbation provides a\nbetter initialization for generalization on downstream OOD data. Empirically,\nafter fine-tuning, this better-initialized model from adversarial pre-training\nalso has better OOD generalization.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 08:06:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yi", "Mingyang", ""], ["Hou", "Lu", ""], ["Sun", "Jiacheng", ""], ["Shang", "Lifeng", ""], ["Jiang", "Xin", ""], ["Liu", "Qun", ""], ["Ma", "Zhi-Ming", ""]]}, {"id": "2105.11152", "submitter": "Maya Okawa", "authors": "Maya Okawa, Tomoharu Iwata, Yusuke Tanaka, Hiroyuki Toda, Takeshi\n  Kurashima, Hisashi Kashima", "title": "Dynamic Hawkes Processes for Discovering Time-evolving Communities'\n  States behind Diffusion Processes", "comments": "11 pages, Proceedings of the 27th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD '21)", "journal-ref": null, "doi": "10.1145/3447548.3467248", "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequences of events including infectious disease outbreaks, social network\nactivities, and crimes are ubiquitous and the data on such events carry\nessential information about the underlying diffusion processes between\ncommunities (e.g., regions, online user groups). Modeling diffusion processes\nand predicting future events are crucial in many applications including\nepidemic control, viral marketing, and predictive policing. Hawkes processes\noffer a central tool for modeling the diffusion processes, in which the\ninfluence from the past events is described by the triggering kernel. However,\nthe triggering kernel parameters, which govern how each community is influenced\nby the past events, are assumed to be static over time. In the real world, the\ndiffusion processes depend not only on the influences from the past, but also\nthe current (time-evolving) states of the communities, e.g., people's awareness\nof the disease and people's current interests. In this paper, we propose a\nnovel Hawkes process model that is able to capture the underlying dynamics of\ncommunity states behind the diffusion processes and predict the occurrences of\nevents based on the dynamics. Specifically, we model the latent dynamic\nfunction that encodes these hidden dynamics by a mixture of neural networks.\nThen we design the triggering kernel using the latent dynamic function and its\nintegral. The proposed method, termed DHP (Dynamic Hawkes Processes), offers a\nflexible way to learn complex representations of the time-evolving communities'\nstates, while at the same time it allows to computing the exact likelihood,\nwhich makes parameter learning tractable. Extensive experiments on four\nreal-world event datasets show that DHP outperforms five widely adopted methods\nfor event prediction.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 08:35:48 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 05:14:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Okawa", "Maya", ""], ["Iwata", "Tomoharu", ""], ["Tanaka", "Yusuke", ""], ["Toda", "Hiroyuki", ""], ["Kurashima", "Takeshi", ""], ["Kashima", "Hisashi", ""]]}, {"id": "2105.11160", "submitter": "Celia Cintas", "authors": "Hannah Kim, Girmaw Abebe Tadesse, Celia Cintas, Skyler Speakman, Kush\n  Varshney", "title": "Out-of-Distribution Detection in Dermatology using Input Perturbation\n  and Subset Scanning", "comments": "Under review for 6th Outlier Detection & Description Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep learning have led to breakthroughs in the development\nof automated skin disease classification. As we observe an increasing interest\nin these models in the dermatology space, it is crucial to address aspects such\nas the robustness towards input data distribution shifts. Current skin disease\nmodels could make incorrect inferences for test samples from different hardware\ndevices and clinical settings or unknown disease samples, which are\nout-of-distribution (OOD) from the training samples. To this end, we propose a\nsimple yet effective approach that detect these OOD samples prior to making any\ndecision. The detection is performed via scanning in the latent space\nrepresentation (e.g., activations of the inner layers of any pre-trained skin\ndisease classifier). The input samples could also perturbed to maximise\ndivergence of OOD samples. We validate our ODD detection approach in two use\ncases: 1) identify samples collected from different protocols, and 2) detect\nsamples from unknown disease classes. Additionally, we evaluate the performance\nof the proposed approach and compare it with other state-of-the-art methods.\nFurthermore, data-driven dermatology applications may deepen the disparity in\nclinical care across racial and ethnic groups since most datasets are reported\nto suffer from bias in skin tone distribution. Therefore, we also evaluate the\nfairness of these OOD detection methods across different skin tones. Our\nexperiments resulted in competitive performance across multiple datasets in\ndetecting OOD samples, which could be used (in the future) to design more\neffective transfer learning techniques prior to inferring on these samples.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 09:04:47 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 11:53:15 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 07:40:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kim", "Hannah", ""], ["Tadesse", "Girmaw Abebe", ""], ["Cintas", "Celia", ""], ["Speakman", "Skyler", ""], ["Varshney", "Kush", ""]]}, {"id": "2105.11166", "submitter": "Mikolaj Jankowski", "authors": "Mikolaj Jankowski, Deniz Gunduz, Krystian Mikolajczyk", "title": "AirNet: Neural Network Transmission over the Air", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art performance for many emerging edge applications is achieved\nby deep neural networks (DNNs). Often, these DNNs are location and time\nsensitive, and the parameters of a specific DNN must be delivered from an edge\nserver to the edge device rapidly and efficiently to carry out time-sensitive\ninference tasks. We introduce AirNet, a novel training and analog transmission\nmethod that allows efficient wireless delivery of DNNs. We first train the DNN\nwith noise injection to counter the wireless channel noise. We also employ\npruning to reduce the channel bandwidth necessary for transmission, and perform\nknowledge distillation from a larger model to achieve satisfactory performance,\ndespite the channel perturbations. We show that AirNet achieves significantly\nhigher test accuracy compared to digital alternatives under the same bandwidth\nand power constraints. It also exhibits graceful degradation with channel\nquality, which reduces the requirement for accurate channel estimation.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 09:16:04 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:44:49 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Jankowski", "Mikolaj", ""], ["Gunduz", "Deniz", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2105.11181", "submitter": "Yuyan Wu", "authors": "Yuyan Wu, Haimin Guo, Hongwei Song, Rui Deng", "title": "Fuzzy inference system application for oil-water flow patterns\n  identification", "comments": "20pages,12figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the continuous development of the petroleum industry, long-distance\ntransportation of oil and gas has been the norm. Due to gravity differentiation\nin horizontal wells and highly deviated wells (non-vertical wells), the water\nphase at the bottom of the pipeline will cause scaling and corrosion in the\npipeline. Scaling and corrosion will make the transportation process difficult,\nand transportation costs will be considerably increased. Therefore, the study\nof the oil-water two-phase flow pattern is of great importance to oil\nproduction. In this paper, a fuzzy inference system is used to predict the flow\npattern of the fluid, get the prediction result, and compares it with the\nprediction result of the BP neural network. From the comparison of the results,\nwe found that the prediction results of the fuzzy inference system are more\naccurate and reliable than the prediction results of the BP neural network. At\nthe same time, it can realize real-time monitoring and has less error control.\nExperimental results demonstrate that in the entire production logging process\nof non-vertical wells, the use of a fuzzy inference system to predict fluid\nflow patterns can greatly save production costs while ensuring the safe\noperation of production equipment.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 10:08:02 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wu", "Yuyan", ""], ["Guo", "Haimin", ""], ["Song", "Hongwei", ""], ["Deng", "Rui", ""]]}, {"id": "2105.11187", "submitter": "Chairi Kiourt", "authors": "Chairi Kiourt, Georgios Feretzakis, Konstantinos Dalamarinis, Dimitris\n  Kalles, Georgios Pantos, Ioannis Papadopoulos, Spyros Kouris, George\n  Ioannakis, Evangelos Loupelis, Petros Antonopoulos, Aikaterini Sakagianni", "title": "Pulmonary embolism identification in computerized tomography pulmonary\n  angiography scans with deep learning technologies in COVID-19 patients", "comments": "16 pages, 6 figures, 1 table, Submitted to the European Radiology\n  journal of Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main objective of this work is to utilize state-of-the-art deep learning\napproaches for the identification of pulmonary embolism in CTPA-Scans for\nCOVID-19 patients, provide an initial assessment of their performance and,\nultimately, provide a fast-track prototype solution (system). We adopted and\nassessed some of the most popular convolutional neural network architectures\nthrough transfer learning approaches, to strive to combine good model accuracy\nwith fast training. Additionally, we exploited one of the most popular\none-stage object detection models for the localization (through object\ndetection) of the pulmonary embolism regions-of-interests. The models of both\napproaches are trained on an original CTPA-Scan dataset, where we annotated of\n673 CTPA-Scan images with 1,465 bounding boxes in total, highlighting pulmonary\nembolism regions-of-interests. We provide a brief assessment of some\nstate-of-the-art image classification models by achieving validation accuracies\nof 91% in pulmonary embolism classification. Additionally, we achieved a\nprecision of about 68% on average in the object detection model for the\npulmonary embolism localization under 50% IoU threshold. For both approaches,\nwe provide the entire training pipelines for future studies (step by step\nprocesses through source code). In this study, we present some of the most\naccurate and fast deep learning models for pulmonary embolism identification in\nCTPA-Scans images, through classification and localization (object detection)\napproaches for patients infected by COVID-19. We provide a fast-track solution\n(system) for the research community of the area, which combines both\nclassification and object detection models for improving the precision of\nidentifying pulmonary embolisms.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 10:23:21 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 06:50:04 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 06:05:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Kiourt", "Chairi", ""], ["Feretzakis", "Georgios", ""], ["Dalamarinis", "Konstantinos", ""], ["Kalles", "Dimitris", ""], ["Pantos", "Georgios", ""], ["Papadopoulos", "Ioannis", ""], ["Kouris", "Spyros", ""], ["Ioannakis", "George", ""], ["Loupelis", "Evangelos", ""], ["Antonopoulos", "Petros", ""], ["Sakagianni", "Aikaterini", ""]]}, {"id": "2105.11207", "submitter": "Andres C Rodriguez", "authors": "Andr\\'es C. Rodr\\'iguez, Stefano D'Aronco, Konrad Schindler, Jan\n  D.Wegner", "title": "Mapping oil palm density at country scale: An active learning approach", "comments": null, "journal-ref": "Remote Sensing of Environment Volume 261, August 2021, 112479", "doi": "10.1016/j.rse.2021.112479", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate mapping of oil palm is important for understanding its past and\nfuture impact on the environment. We propose to map and count oil palms by\nestimating tree densities per pixel for large-scale analysis. This allows for\nfine-grained analysis, for example regarding different planting patterns. To\nthat end, we propose a new, active deep learning method to estimate oil palm\ndensity at large scale from Sentinel-2 satellite images, and apply it to\ngenerate complete maps for Malaysia and Indonesia. What makes the regression of\noil palm density challenging is the need for representative reference data that\ncovers all relevant geographical conditions across a large territory.\nSpecifically for density estimation, generating reference data involves\ncounting individual trees. To keep the associated labelling effort low we\npropose an active learning (AL) approach that automatically chooses the most\nrelevant samples to be labelled. Our method relies on estimates of the\nepistemic model uncertainty and of the diversity among samples, making it\npossible to retrieve an entire batch of relevant samples in a single iteration.\nMoreover, our algorithm has linear computational complexity and is easily\nparallelisable to cover large areas. We use our method to compute the first oil\npalm density map with $10\\,$m Ground Sampling Distance (GSD) , for all of\nIndonesia and Malaysia and for two different years, 2017 and 2019. The maps\nhave a mean absolute error of $\\pm$7.3 trees/$ha$, estimated from an\nindependent validation set. We also analyse density variations between\ndifferent states within a country and compare them to official estimates.\nAccording to our estimates there are, in total, $>1.2$ billion oil palms in\nIndonesia covering $>$15 million $ha$, and $>0.5$ billion oil palms in Malaysia\ncovering $>6$ million $ha$.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:23:55 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Rodr\u00edguez", "Andr\u00e9s C.", ""], ["D'Aronco", "Stefano", ""], ["Schindler", "Konrad", ""], ["Wegner", "Jan D.", ""]]}, {"id": "2105.11213", "submitter": "Avi Mohan", "authors": "Avinash Mohan, Arpan Chattopadhyay, Shivam Vinayak Vatsa, and Anurag\n  Kumar", "title": "Decentralized, Hybrid MAC Design with Reduced State Information Exchange\n  for Low-Delay IoT Applications", "comments": "56 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a system of several collocated nodes sharing a time slotted\nwireless channel, and seek a MAC that (i) provides low mean delay, (ii) has\ndistributed control (i.e., there is no central scheduler), and (iii) does not\nrequire explicit exchange of state information or control signals. The design\nof such MAC protocols must keep in mind the need for contention access at light\ntraffic, and scheduled access in heavy traffic, leading to the long-standing\ninterest in hybrid, adaptive MACs.\n  We first propose EZMAC, a simple extension of an existing decentralized,\nhybrid MAC called ZMAC. Next, motivated by our results on delay and throughput\noptimality in partially observed, constrained queuing networks, we develop\nanother decentralized MAC protocol that we term QZMAC. A method to improve the\nshort-term fairness of QZMAC is proposed and analysed, and the resulting\nmodified algorithm is shown to possess better fairness properties than QZMAC.\nThe theory developed to reduce delay is also shown to work %with different\ntraffic types (batch arrivals, for example) and even in the presence of\ntransmission errors and fast fading.\n  Extensions to handle time critical traffic (alarms, for example) and hidden\nnodes are also discussed. Practical implementation issues, such as handling\nClear Channel Assessment (CCA) errors, are outlined. We implement and\ndemonstrate the performance of QZMAC on a test bed consisting of CC2420 based\nCrossbow telosB motes, running the 6TiSCH communication stack on the Contiki\noperating system over the 2.4GHz ISM band.\n  Finally, using simulations, we show that both protocols achieve mean delays\nmuch lower than those achieved by ZMAC, and QZMAC provides mean delays very\nclose to the minimum achievable in this setting, i.e., that of the centralized\ncomplete knowledge scheduler.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:44:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Mohan", "Avinash", ""], ["Chattopadhyay", "Arpan", ""], ["Vatsa", "Shivam Vinayak", ""], ["Kumar", "Anurag", ""]]}, {"id": "2105.11219", "submitter": "Parth Patwa", "authors": "Parth Patwa, Srinivas PYKL, Amitava Das, Prerana Mukherjee, Viswanath\n  Pulabaigari", "title": "Hater-O-Genius Aggression Classification using Capsule Networks", "comments": "Accepted at the 17th International Conference on Natural Language\n  Processing (ICON 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contending hate speech in social media is one of the most challenging social\nproblems of our time. There are various types of anti-social behavior in social\nmedia. Foremost of them is aggressive behavior, which is causing many social\nissues such as affecting the social lives and mental health of social media\nusers. In this paper, we propose an end-to-end ensemble-based architecture to\nautomatically identify and classify aggressive tweets. Tweets are classified\ninto three categories - Covertly Aggressive, Overtly Aggressive, and\nNon-Aggressive. The proposed architecture is an ensemble of smaller subnetworks\nthat are able to characterize the feature embeddings effectively. We\ndemonstrate qualitatively that each of the smaller subnetworks is able to learn\nunique features. Our best model is an ensemble of Capsule Networks and results\nin a 65.2% F1 score on the Facebook test set, which results in a performance\ngain of 0.95% over the TRAC-2018 winners. The code and the model weights are\npublicly available at\nhttps://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:53:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Patwa", "Parth", ""], ["PYKL", "Srinivas", ""], ["Das", "Amitava", ""], ["Mukherjee", "Prerana", ""], ["Pulabaigari", "Viswanath", ""]]}, {"id": "2105.11233", "submitter": "Hans-Christian Ruiz-Euler Dr.", "authors": "Marcus N. Boon, Hans-Christian Ruiz Euler, Tao Chen, Bram van de Ven,\n  Unai Alegre Ibarra, Peter A. Bobbert, Wilfred G. van der Wiel", "title": "Gradient Descent in Materio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning, a multi-layered neural network approach inspired by the brain,\nhas revolutionized machine learning. One of its key enablers has been\nbackpropagation, an algorithm that computes the gradient of a loss function\nwith respect to the weights in the neural network model, in combination with\nits use in gradient descent. However, the implementation of deep learning in\ndigital computers is intrinsically wasteful, with energy consumption becoming\nprohibitively high for many applications. This has stimulated the development\nof specialized hardware, ranging from neuromorphic CMOS integrated circuits and\nintegrated photonic tensor cores to unconventional, material-based computing\nsystems. The learning process in these material systems, taking place, e.g., by\nartificial evolution or surrogate neural network modelling, is still a\ncomplicated and time-consuming process. Here, we demonstrate an efficient and\naccurate homodyne gradient extraction method for performing gradient descent on\nthe loss function directly in the material system. We demonstrate the method in\nour recently developed dopant network processing units, where we readily\nrealize all Boolean gates. This shows that gradient descent can in principle be\nfully implemented in materio using simple electronics, opening up the way to\nautonomously learning material systems.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 12:18:31 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Boon", "Marcus N.", ""], ["Euler", "Hans-Christian Ruiz", ""], ["Chen", "Tao", ""], ["van de Ven", "Bram", ""], ["Ibarra", "Unai Alegre", ""], ["Bobbert", "Peter A.", ""], ["van der Wiel", "Wilfred G.", ""]]}, {"id": "2105.11240", "submitter": "Nastaran Bajalan", "authors": "Saeed Bajalan and Nastaran Bajalan", "title": "Novel ANN method for solving ordinary and fractional Black-Scholes\n  equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main aim of this study is to introduce a 2-layered Artificial Neural\nNetwork (ANN) for solving the Black-Scholes partial differential equation (PDE)\nof either fractional or ordinary orders. Firstly, a discretization method is\nemployed to change the model into a sequence of Ordinary Differential Equations\n(ODE). Then each of these ODEs is solved with the aid of an ANN. Adam\noptimization is employed as the learning paradigm since it can add the\nforeknowledge of slowing down the process of optimization when getting close to\nthe actual optimum solution. The model also takes advantage of fine tuning for\nspeeding up the process and domain mapping to confront infinite domain issue.\nFinally, the accuracy, speed, and convergence of the method for solving several\ntypes of Black-Scholes model are reported.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:10:15 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Bajalan", "Saeed", ""], ["Bajalan", "Nastaran", ""]]}, {"id": "2105.11241", "submitter": "Saurabh Mittal", "authors": "Prerak Mann, Sahaj Jain, Saurabh Mittal, Aruna Bhat", "title": "Generation of COVID-19 Chest CT Scan Images using Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SARS-CoV-2, also known as COVID-19 or Coronavirus, is a viral contagious\ndisease that is infected by a novel coronavirus, and has been rapidly spreading\nacross the globe. It is very important to test and isolate people to reduce\nspread, and from here comes the need to do this quickly and efficiently.\nAccording to some studies, Chest-CT outperforms RT-PCR lab testing, which is\nthe current standard, when diagnosing COVID-19 patients. Due to this, computer\nvision researchers have developed various deep learning systems that can\npredict COVID-19 using a Chest-CT scan correctly to a certain degree. The\naccuracy of these systems is limited since deep learning neural networks such\nas CNNs (Convolutional Neural Networks) need a significantly large quantity of\ndata for training in order to produce good quality results. Since the disease\nis relatively recent and more focus has been on CXR (Chest XRay) images, the\navailable chest CT Scan image dataset is much less. We propose a method, by\nutilizing GANs, to generate synthetic chest CT images of both positive and\nnegative COVID-19 patients. Using a pre-built predictive model, we concluded\nthat around 40% of the generated images are correctly predicted as COVID-19\npositive. The dataset thus generated can be used to train a CNN-based\nclassifier which can help determine COVID-19 in a patient with greater\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:04:21 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Mann", "Prerak", ""], ["Jain", "Sahaj", ""], ["Mittal", "Saurabh", ""], ["Bhat", "Aruna", ""]]}, {"id": "2105.11255", "submitter": "Anthony Bellotti", "authors": "Anthony Bellotti", "title": "Optimized conformal classification using gradient descent approximation", "comments": "26 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conformal predictors are an important class of algorithms that allow\npredictions to be made with a user-defined confidence level. They are able to\ndo this by outputting prediction sets, rather than simple point predictions.\nThe conformal predictor is valid in the sense that the accuracy of its\npredictions is guaranteed to meet the confidence level, only assuming\nexchangeability in the data. Since accuracy is guaranteed, the performance of a\nconformal predictor is measured through the efficiency of the prediction sets.\nTypically, a conformal predictor is built on an underlying machine learning\nalgorithm and hence its predictive power is inherited from this algorithm.\nHowever, since the underlying machine learning algorithm is not trained with\nthe objective of minimizing predictive efficiency it means that the resulting\nconformal predictor may be sub-optimal and not aligned sufficiently to this\nobjective. Hence, in this study we consider an approach to train the conformal\npredictor directly with maximum predictive efficiency as the optimization\nobjective, and we focus specifically on the inductive conformal predictor for\nclassification. To do this, the conformal predictor is approximated by a\ndifferentiable objective function and gradient descent used to optimize it. The\nresulting parameter estimates are then passed to a proper inductive conformal\npredictor to give valid prediction sets. We test the method on several real\nworld data sets and find that the method is promising and in most cases gives\nimproved predictive efficiency against a baseline conformal predictor.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:14:41 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Bellotti", "Anthony", ""]]}, {"id": "2105.11260", "submitter": "Andrew Halterman", "authors": "Andrew Halterman, Benjamin J. Radford", "title": "Few-Shot Upsampling for Protest Size Detection", "comments": "Accepted into Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new task and dataset for a common problem in social science\nresearch: \"upsampling\" coarse document labels to fine-grained labels or spans.\nWe pose the problem in a question answering format, with the answers providing\nthe fine-grained labels. We provide a benchmark dataset and baselines on a\nsocially impactful task: identifying the exact crowd size at protests and\ndemonstrations in the United States given only order-of-magnitude information\nabout protest attendance, a very small sample of fine-grained examples, and\nEnglish-language news text. We evaluate several baseline models, including\nzero-shot results from rule-based and question-answering models, few-shot\nmodels fine-tuned on a small set of documents, and weakly supervised models\nusing a larger set of coarsely-labeled documents. We find that our rule-based\nmodel initially outperforms a zero-shot pre-trained transformer language model\nbut that further fine-tuning on a very small subset of 25 examples\nsubstantially improves out-of-sample performance. We also demonstrate a method\nfor fine-tuning the transformer span on only the coarse labels that performs\nsimilarly to our rule-based approach. This work will contribute to social\nscientists' ability to generate data to understand the causes and successes of\ncollective action.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:27:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Halterman", "Andrew", ""], ["Radford", "Benjamin J.", ""]]}, {"id": "2105.11283", "submitter": "Eugene Valassakis", "authors": "Eugene Valassakis, Norman Di Palo and Edward Johns", "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide\n  Task Spaces", "comments": "To be published at IROS 2021. 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of zero-shot sim-to-real when the task\nrequires both highly precise control with sub-millimetre error tolerance, and\nwide task space generalisation. Our framework involves a coarse-to-fine\ncontroller, where trajectories begin with classical motion planning using\nICP-based pose estimation, and transition to a learned end-to-end controller\nwhich maps images to actions and is trained in simulation with domain\nrandomisation. In this way, we achieve precise control whilst also generalising\nthe controller across wide task spaces, and keeping the robustness of\nvision-based, end-to-end control. Real-world experiments on a range of\ndifferent tasks show that, by exploiting the best of both worlds, our framework\nsignificantly outperforms purely motion planning methods, and purely\nlearning-based methods. Furthermore, we answer a range of questions on best\npractices for precise sim-to-real transfer, such as how different image sensor\nmodalities and image feature representations perform.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:12:38 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 13:42:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Valassakis", "Eugene", ""], ["Di Palo", "Norman", ""], ["Johns", "Edward", ""]]}, {"id": "2105.11309", "submitter": "Chichun Zhou", "authors": "Chen-Xin Qin, Ru-Hao Liu, Mao-Cai Li, Chi-Chun Zhou, and Yi-Liua", "title": "An Effective and Efficient Method to Solve the High-Order and the\n  Non-Linear Ordinary Differential Equations: the Ratio Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An effective and efficient method that solves the high-order and the\nnon-linear ordinary differential equations is provided. The method is based on\nthe ratio net. By comparing the method with existing methods such as the\npolynomial based method and the multilayer perceptron network based method, we\nshow that the ratio net gives good results and has higher efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:59:52 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Qin", "Chen-Xin", ""], ["Liu", "Ru-Hao", ""], ["Li", "Mao-Cai", ""], ["Zhou", "Chi-Chun", ""], ["Yi-Liua", "", ""]]}, {"id": "2105.11321", "submitter": "Kasra Hosseini", "authors": "Kasra Hosseini, Kaspar Beelen, Giovanni Colavizza, Mariona Coll\n  Ardanuy", "title": "Neural Language Models for Nineteenth-Century English", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present four types of neural language models trained on a large historical\ndataset of books in English, published between 1760-1900 and comprised of ~5.1\nbillion tokens. The language model architectures include static (word2vec and\nfastText) and contextualized models (BERT and Flair). For each architecture, we\ntrained a model instance using the whole dataset. Additionally, we trained\nseparate instances on text published before 1850 for the two static models, and\nfour instances considering different time slices for BERT. Our models have\nalready been used in various downstream tasks where they consistently improved\nperformance. In this paper, we describe how the models have been created and\noutline their reuse potential.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:57:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Hosseini", "Kasra", ""], ["Beelen", "Kaspar", ""], ["Colavizza", "Giovanni", ""], ["Ardanuy", "Mariona Coll", ""]]}, {"id": "2105.11328", "submitter": "Rich Riley Mr", "authors": "Henry Charlesworth, Adrian Millea, Eddie Pottrill, Rich Riley", "title": "Room Clearance with Feudal Hierarchical Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reinforcement learning (RL) is a general framework that allows systems to\nlearn autonomously through trial-and-error interaction with their environment.\nIn recent years combining RL with expressive, high-capacity neural network\nmodels has led to impressive performance in a diverse range of domains.\nHowever, dealing with the large state and action spaces often required for\nproblems in the real world still remains a significant challenge. In this paper\nwe introduce a new simulation environment, \"Gambit\", designed as a tool to\nbuild scenarios that can drive RL research in a direction useful for military\nanalysis. Using this environment we focus on an abstracted and simplified room\nclearance scenario, where a team of blue agents have to make their way through\na building and ensure that all rooms are cleared of (and remain clear) of enemy\nred agents. We implement a multi-agent version of feudal hierarchical RL that\nintroduces a command hierarchy where a commander at the higher level sends\norders to multiple agents at the lower level who simply have to learn to follow\nthese orders. We find that breaking the task down in this way allows us to\nsolve a number of non-trivial floorplans that require the coordination of\nmultiple agents much more efficiently than the standard baseline RL algorithms\nwe compare with. We then go on to explore how qualitatively different behaviour\ncan emerge depending on what we prioritise in the agent's reward function (e.g.\nclearing the building quickly vs. prioritising rescuing civilians).\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:05:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Charlesworth", "Henry", ""], ["Millea", "Adrian", ""], ["Pottrill", "Eddie", ""], ["Riley", "Rich", ""]]}, {"id": "2105.11335", "submitter": "Lijun Sun Mr", "authors": "Xudong Wang, Yuankai Wu, Dingyi Zhuang, Lijun Sun", "title": "Low-Rank Hankel Tensor Completion for Traffic Speed Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the traffic state estimation (TSE) problem using sparse\nobservations from mobile sensors. TSE can be considered a spatiotemporal\ninterpolation problem in which the evolution of traffic variables (e.g.,\nspeed/density) is governed by traffic flow dynamics (e.g., partial differential\nequations). Most existing TSE methods either rely on well-defined physical\ntraffic flow models or require large amounts of simulation data as input to\ntrain machine learning models. Different from previous studies, in this paper\nwe propose a purely data-driven and model-free solution. We consider TSE as a\nspatiotemporal matrix completion/interpolation problem, and apply\nspatiotemporal Hankel delay embedding to transforms the original incomplete\nmatrix to a fourth-order tensor. By imposing a low-rank assumption on this\ntensor structure, we can approximate and characterize both global patterns and\nthe unknown and complex local spatiotemporal dynamics in a data-driven manner.\nWe use the truncated nuclear norm of the spatiotemporal unfolding (i.e., square\nnorm) to approximate the tensor rank and develop an efficient solution\nalgorithm based on the Alternating Direction Method of Multipliers (ADMM). The\nproposed framework only involves two hyperparameters -- spatial and temporal\nwindow lengths, which are easy to set given the degree of data sparsity. We\nconduct numerical experiments on both synthetic simulation data and real-world\nhigh-resolution trajectory data, and our results demonstrate the effectiveness\nand superiority of the proposed model in some challenging scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 00:08:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Xudong", ""], ["Wu", "Yuankai", ""], ["Zhuang", "Dingyi", ""], ["Sun", "Lijun", ""]]}, {"id": "2105.11346", "submitter": "Zhenyue Qin", "authors": "Zhenyue Qin and Saeed Anwar and Dongwoo Kim and Yang Liu and Pan Ji\n  and Tom Gedeon", "title": "Position-Sensing Graph Neural Networks: Proactively Learning Nodes\n  Relative Positions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing graph neural networks (GNNs) learn node embeddings using the\nframework of message passing and aggregation. Such GNNs are incapable of\nlearning relative positions between graph nodes within a graph. To empower GNNs\nwith the awareness of node positions, some nodes are set as anchors. Then,\nusing the distances from a node to the anchors, GNNs can infer relative\npositions between nodes. However, P-GNNs arbitrarily select anchors, leading to\ncompromising position-awareness and feature extraction. To eliminate this\ncompromise, we demonstrate that selecting evenly distributed and asymmetric\nanchors is essential. On the other hand, we show that choosing anchors that can\naggregate embeddings of all the nodes within a graph is NP-hard. Therefore,\ndevising efficient optimal algorithms in a deterministic approach is\npractically not feasible. To ensure position-awareness and bypass\nNP-completeness, we propose Position-Sensing Graph Neural Networks (PSGNNs),\nlearning how to choose anchors in a back-propagatable fashion. Experiments\nverify the effectiveness of PSGNNs against state-of-the-art GNNs, substantially\nimproving performance on various synthetic and real-world graph datasets while\nenjoying stable scalability. Specifically, PSGNNs on average boost AUC more\nthan 14% for pairwise node classification and 18% for link prediction over the\nexisting state-of-the-art position-aware methods. Our source code is publicly\navailable at: https://github.com/ZhenyueQin/PSGNN\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:30:30 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Qin", "Zhenyue", ""], ["Anwar", "Saeed", ""], ["Kim", "Dongwoo", ""], ["Liu", "Yang", ""], ["Ji", "Pan", ""], ["Gedeon", "Tom", ""]]}, {"id": "2105.11354", "submitter": "Payam Karisani", "authors": "Payam Karisani, Jinho D. Choi, Li Xiong", "title": "View Distillation with Unlabeled Data for Extracting Adverse Drug\n  Effects from User-Generated Data", "comments": "NAACL 2021 (workshops)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm based on multi-layer transformers for identifying\nAdverse Drug Reactions (ADR) in social media data. Our model relies on the\nproperties of the problem and the characteristics of contextual word embeddings\nto extract two views from documents. Then a classifier is trained on each view\nto label a set of unlabeled documents to be used as an initializer for a new\nclassifier in the other view. Finally, the initialized classifier in each view\nis further trained using the initial training examples. We evaluated our model\nin the largest publicly available ADR dataset. The experiments testify that our\nmodel significantly outperforms the transformer-based models pretrained on\ndomain-specific data.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:38:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Karisani", "Payam", ""], ["Choi", "Jinho D.", ""], ["Xiong", "Li", ""]]}, {"id": "2105.11363", "submitter": "Yizheng Chen", "authors": "Yizheng Chen, Shiqi Wang, Yue Qin, Xiaojing Liao, Suman Jana, David\n  Wagner", "title": "Learning Security Classifiers with Verified Global Robustness Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have proposed methods to train classifiers with local robustness\nproperties, which can provably eliminate classes of evasion attacks for most\ninputs, but not all inputs. Since data distribution shift is very common in\nsecurity applications, e.g., often observed for malware detection, local\nrobustness cannot guarantee that the property holds for unseen inputs at the\ntime of deploying the classifier. Therefore, it is more desirable to enforce\nglobal robustness properties that hold for all inputs, which is strictly\nstronger than local robustness.\n  In this paper, we present a framework and tools for training classifiers that\nsatisfy global robustness properties. We define new notions of global\nrobustness that are more suitable for security classifiers. We design a novel\nbooster-fixer training framework to enforce global robustness properties. We\nstructure our classifier as an ensemble of logic rules and design a new\nverifier to verify the properties. In our training algorithm, the booster\nincreases the classifier's capacity, and the fixer enforces verified global\nrobustness properties following counterexample guided inductive synthesis.\n  To the best of our knowledge, the only global robustness property that has\nbeen previously achieved is monotonicity. Several previous works have defined\nglobal robustness properties, but their training techniques failed to achieve\nverified global robustness. In comparison, we show that we can train\nclassifiers to satisfy different global robustness properties for three\nsecurity datasets, and even multiple properties at the same time, with modest\nimpact on the classifier's performance. For example, we train a Twitter spam\naccount classifier to satisfy five global robustness properties, with 5.4%\ndecrease in true positive rate, and 0.1% increase in false positive rate,\ncompared to a baseline XGBoost model that doesn't satisfy any property.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:46:20 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chen", "Yizheng", ""], ["Wang", "Shiqi", ""], ["Qin", "Yue", ""], ["Liao", "Xiaojing", ""], ["Jana", "Suman", ""], ["Wagner", "David", ""]]}, {"id": "2105.11364", "submitter": "Nikhil Kasukurthi", "authors": "Shivam Shah, Nikhil Kasukurthi, Harshit Pande", "title": "Dynamic region proposal networks for semantic segmentation in automated\n  glaucoma screening", "comments": null, "journal-ref": null, "doi": "10.1109/ISBI.2019.8759171", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Screening for the diagnosis of glaucoma through a fundus image can be\ndetermined by the optic cup to disc diameter ratio (CDR), which requires the\nsegmentation of the cup and disc regions. In this paper, we propose two novel\napproaches, namely Parameter-Shared Branched Network (PSBN) andWeak Region of\nInterest Model-based segmentation (WRoIM) to identify disc and cup boundaries.\nUnlike the previous approaches, the proposed methods are trained end-to-end\nthrough a single neural network architecture and use dynamic cropping instead\nof manual or traditional computer vision-based cropping. We are able to achieve\nsimilar performance as that of state-of-the-art approaches with less number of\nnetwork parameters. Our experiments include comparison with different best\nknown methods on publicly available Drishti-GS1 and RIM-ONE v3 datasets. With\n$7.8 \\times 10^6$ parameters our approach achieves a Dice score of 0.96/0.89\nfor disc/cup segmentation on Drishti-GS1 data whereas the existing\nstate-of-the-art approach uses $19.8\\times 10^6$ parameters to achieve a dice\nscore of 0.97/0.89.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:19:14 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shah", "Shivam", ""], ["Kasukurthi", "Nikhil", ""], ["Pande", "Harshit", ""]]}, {"id": "2105.11366", "submitter": "Daniel Nam", "authors": "Daniel Wontae Nam, Younghoon Kim, Chan Y. Park", "title": "GMAC: A Distributional Perspective on Actor-Critic Framework", "comments": null, "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning, PMLR 139:7927-7936, 2021", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we devise a distributional framework on actor-critic as a\nsolution to distributional instability, action type restriction, and conflation\nbetween samples and statistics. We propose a new method that minimizes the\nCram\\'er distance with the multi-step Bellman target distribution generated\nfrom a novel Sample-Replacement algorithm denoted SR($\\lambda$), which learns\nthe correct value distribution under multiple Bellman operations.\nParameterizing a value distribution with Gaussian Mixture Model further\nimproves the efficiency and the performance of the method, which we name GMAC.\nWe empirically show that GMAC captures the correct representation of value\ndistributions and improves the performance of a conventional actor-critic\nmethod with low computational cost, in both discrete and continuous action\nspaces using Arcade Learning Environment (ALE) and PyBullet environment.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:50:26 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 10:53:51 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Nam", "Daniel Wontae", ""], ["Kim", "Younghoon", ""], ["Park", "Chan Y.", ""]]}, {"id": "2105.11367", "submitter": "Fan Lai", "authors": "Fan Lai, Yinwei Dai, Xiangfeng Zhu, Mosharaf Chowdhury", "title": "FedScale: Benchmarking Model and System Performance of Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FedScale, a diverse set of challenging and realistic benchmark\ndatasets to facilitate scalable, comprehensive, and reproducible federated\nlearning (FL) research. FedScale datasets are large-scale, encompassing a\ndiverse range of important FL tasks, such as image classification, object\ndetection, language modeling, speech recognition, and reinforcement learning.\nFor each dataset, we provide a unified evaluation protocol using realistic data\nsplits and evaluation metrics. To meet the pressing need for reproducing\nrealistic FL at scale, we have also built an efficient evaluation platform to\nsimplify and standardize the process of FL experimental setup and model\nevaluation. Our evaluation platform provides flexible APIs to implement new FL\nalgorithms and includes new execution backends with minimal developer efforts.\nFinally, we perform indepth benchmark experiments on these datasets. Our\nexperiments suggest fruitful opportunities in heterogeneity-aware\nco-optimizations of the system and statistical efficiency under realistic FL\ncharacteristics. FedScale is open-source with permissive licenses and actively\nmaintained,1 and we welcome feedback and contributions from the community.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:55:27 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 03:58:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lai", "Fan", ""], ["Dai", "Yinwei", ""], ["Zhu", "Xiangfeng", ""], ["Chowdhury", "Mosharaf", ""]]}, {"id": "2105.11376", "submitter": "Xin Jin", "authors": "Xin Jin", "title": "Can we imitate stock price behavior to reinforcement learn option price?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a framework of imitating the price behavior of the\nunderlying stock for reinforcement learning option price. We use accessible\nfeatures of the equities pricing data to construct a non-deterministic Markov\ndecision process for modeling stock price behavior driven by principal\ninvestor's decision making. However, low signal-to-noise ratio and instability\nthat appear immanent in equity markets pose challenges to determine the state\ntransition (price change) after executing an action (principal investor's\ndecision) as well as decide an action based on current state (spot price). In\norder to conquer these challenges, we resort to a Bayesian deep neural network\nfor computing the predictive distribution of the state transition led by an\naction. Additionally, instead of exploring a state-action relationship to\nformulate a policy, we seek for an episode based visible-hidden state-action\nrelationship to probabilistically imitate principal investor's successive\ndecision making. Our algorithm then maps imitative principal investor's\ndecisions to simulated stock price paths by a Bayesian deep neural network.\nEventually the optimal option price is reinforcement learned through maximizing\nthe cumulative risk-adjusted return of a dynamically hedged portfolio over\nsimulated price paths of the underlying.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:08:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Jin", "Xin", ""]]}, {"id": "2105.11397", "submitter": "Weiqi Ji", "authors": "Weiqi Ji, Franz Richter, Michael J. Gollner, Sili Deng", "title": "Autonomous Kinetic Modeling of Biomass Pyrolysis using Chemical Reaction\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modeling the burning processes of biomass such as wood, grass, and crops is\ncrucial for the modeling and prediction of wildland and urban fire behavior.\nDespite its importance, the burning of solid fuels remains poorly understood,\nwhich can be partly attributed to the unknown chemical kinetics of most solid\nfuels. Most available kinetic models were built upon expert knowledge, which\nrequires chemical insights and years of experience. This work presents a\nframework for autonomously discovering biomass pyrolysis kinetic models from\nthermogravimetric analyzer (TGA) experimental data using the recently developed\nchemical reaction neural networks (CRNN). The approach incorporated the CRNN\nmodel into the framework of neural ordinary differential equations to predict\nthe residual mass in TGA data. In addition to the flexibility of\nneural-network-based models, the learned CRNN model is fully interpretable, by\nincorporating the fundamental physics laws, such as the law of mass action and\nArrhenius law, into the neural network structure. The learned CRNN model can\nthen be translated into the classical forms of biomass chemical kinetic models,\nwhich facilitates the extraction of chemical insights and the integration of\nthe kinetic model into large-scale fire simulations. We demonstrated the\neffectiveness of the framework in predicting the pyrolysis and oxidation of\ncellulose. This successful demonstration opens the possibility of rapid and\nautonomous chemical kinetic modeling of solid fuels, such as wildfire fuels and\nindustrial polymers.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:38:40 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ji", "Weiqi", ""], ["Richter", "Franz", ""], ["Gollner", "Michael J.", ""], ["Deng", "Sili", ""]]}, {"id": "2105.11417", "submitter": "Sahil Singla", "authors": "Sahil Singla and Soheil Feizi", "title": "Skew Orthogonal Convolutions", "comments": "Accepted at ICML, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training convolutional neural networks with a Lipschitz constraint under the\n$l_{2}$ norm is useful for provable adversarial robustness, interpretable\ngradients, stable training, etc. While 1-Lipschitz networks can be designed by\nimposing a 1-Lipschitz constraint on each layer, training such networks\nrequires each layer to be gradient norm preserving (GNP) to prevent gradients\nfrom vanishing. However, existing GNP convolutions suffer from slow training,\nlead to significant reduction in accuracy and provide no guarantees on their\napproximations. In this work, we propose a GNP convolution layer called Skew\nOrthogonal Convolution (SOC) that uses the following mathematical property:\nwhen a matrix is {\\it Skew-Symmetric}, its exponential function is an {\\it\northogonal} matrix. To use this property, we first construct a convolution\nfilter whose Jacobian is Skew-Symmetric. Then, we use the Taylor series\nexpansion of the Jacobian exponential to construct the SOC layer that is\northogonal. To efficiently implement SOC, we keep a finite number of terms from\nthe Taylor series and provide a provable guarantee on the approximation error.\nOur experiments on CIFAR-10 and CIFAR-100 show that SOC allows us to train\nprovably Lipschitz, large convolutional neural networks significantly faster\nthan prior works while achieving significant improvements for both standard and\ncertified robust accuracies.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:11:44 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 07:04:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Singla", "Sahil", ""], ["Feizi", "Soheil", ""]]}, {"id": "2105.11418", "submitter": "Ruijiang Gao", "authors": "Ruijiang Gao, Maytal Saar-tsechansky", "title": "Cost-Accuracy Aware Adaptive Labeling for Active Learning", "comments": "Accepted at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional active learning algorithms assume a single labeler that produces\nnoiseless label at a given, fixed cost, and aim to achieve the best\ngeneralization performance for given classifier under a budget constraint.\nHowever, in many real settings, different labelers have different labeling\ncosts and can yield different labeling accuracies. Moreover, a given labeler\nmay exhibit different labeling accuracies for different instances. This setting\ncan be referred to as active learning with diverse labelers with varying costs\nand accuracies, and it arises in many important real settings. It is therefore\nbeneficial to understand how to effectively trade-off between labeling accuracy\nfor different instances, labeling costs, as well as the informativeness of\ntraining instances, so as to achieve the best generalization performance at the\nlowest labeling cost. In this paper, we propose a new algorithm for selecting\ninstances, labelers (and their corresponding costs and labeling accuracies),\nthat employs generalization bound of learning with label noise to select\ninformative instances and labelers so as to achieve higher generalization\naccuracy at a lower cost. Our proposed algorithm demonstrates state-of-the-art\nperformance on five UCI and a real crowdsourcing dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:21:00 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gao", "Ruijiang", ""], ["Saar-tsechansky", "Maytal", ""]]}, {"id": "2105.11425", "submitter": "Valeriy Avanesov", "authors": "Valeriy Avanesov", "title": "Uncertainty quantification for distributed regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-growing size of the datasets renders well-studied learning\ntechniques, such as Kernel Ridge Regression, inapplicable, posing a serious\ncomputational challenge. Divide-and-conquer is a common remedy, suggesting to\nsplit the dataset into disjoint partitions, obtain the local estimates and\naverage them, it allows to scale-up an otherwise ineffective base approach. In\nthe current study we suggest a fully data-driven approach to quantify\nuncertainty of the averaged estimator. Namely, we construct simultaneous\nelement-wise confidence bands for the predictions yielded by the averaged\nestimator on a given deterministic prediction set. The novel approach features\nrigorous theoretical guaranties for a wide class of base learners with Kernel\nRidge regression being a special case. As a by-product of our analysis we also\nobtain a sup-norm consistency result for the divide-and-conquer Kernel Ridge\nRegression. The simulation study supports the theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:33:19 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Avanesov", "Valeriy", ""]]}, {"id": "2105.11439", "submitter": "Michael Zimmer", "authors": "Michael F. Zimmer", "title": "2nd-order Updates with 1st-order Complexity", "comments": "12 pages, 3 figures, conference preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has long been a goal to efficiently compute and use second order\ninformation on a function ($f$) to assist in numerical approximations. Here it\nis shown how, using only basic physics and a numerical approximation, such\ninformation can be accurately obtained at a cost of ${\\cal O}(N)$ complexity,\nwhere $N$ is the dimensionality of the parameter space of $f$. In this paper,\nan algorithm ({\\em VA-Flow}) is developed to exploit this second order\ninformation, and pseudocode is presented. It is applied to two classes of\nproblems, that of inverse kinematics (IK) and gradient descent (GD). In the IK\napplication, the algorithm is fast and robust, and is shown to lead to smooth\nbehavior even near singularities. For GD the algorithm also works very well,\nprovided the cost function is locally well-described by a polynomial.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:47:51 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 20:04:58 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zimmer", "Michael F.", ""]]}, {"id": "2105.11447", "submitter": "Ethan Perez", "authors": "Ethan Perez, Douwe Kiela, Kyunghyun Cho", "title": "True Few-Shot Learning with Language Models", "comments": "Code at https://github.com/ethanjperez/true_few_shot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained language models (LMs) perform well on many tasks even when\nlearning from a few examples, but prior work uses many held-out examples to\ntune various aspects of learning, such as hyperparameters, training objectives,\nand natural language templates (\"prompts\"). Here, we evaluate the few-shot\nability of LMs when such held-out examples are unavailable, a setting we call\ntrue few-shot learning. We test two model selection criteria, cross-validation\nand minimum description length, for choosing LM prompts and hyperparameters in\nthe true few-shot setting. On average, both marginally outperform random\nselection and greatly underperform selection based on held-out examples.\nMoreover, selection criteria often prefer models that perform significantly\nworse than randomly-selected ones. We find similar results even when taking\ninto account our uncertainty in a model's true performance during selection, as\nwell as when varying the amount of computation and number of examples used for\nselection. Overall, our findings suggest that prior work significantly\noverestimated the true few-shot ability of LMs given the difficulty of few-shot\nmodel selection.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:55:51 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Perez", "Ethan", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2105.11452", "submitter": "Berkay Kopru", "authors": "Berkay K\\\"opr\\\"u, Murat Aslan, Alisher Kholmatov", "title": "Neural Network Based Sleep Phases Classification for Resource Constraint\n  Environments", "comments": "in Turkish language", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sleep is restoration process of the body. The efficiency of this restoration\nprocess is directly correlated to the amount of time spent at each sleep phase.\nHence, automatic tracking of sleep via wearable devices has attracted both the\nresearchers and industry. Current state-of-the-art sleep tracking solutions are\nmemory and processing greedy and they require cloud or mobile phone\nconnectivity. We propose a memory efficient sleep tracking architecture which\ncan work in the embedded environment without needing any cloud or mobile phone\nconnection. In this study, a novel architecture is proposed that consists of a\nfeature extraction and Artificial Neural Networks based stacking classifier.\nBesides, we discussed how to tackle with sequential nature of the sleep staging\nfor the memory constraint environments through the proposed framework. To\nverify the system, a dataset is collected from 24 different subjects for 31\nnights with a wrist worn device having 3-axis accelerometer (ACC) and\nphotoplethysmogram (PPG) sensors. Over the collected dataset, the proposed\nclassification architecture achieves 20\\% and 14\\% better F1 scores than its\ncompetitors. Apart from the superior performance, proposed architecture is a\npromising solution for resource constraint embedded systems by allocating only\n4.2 kilobytes of memory (RAM).\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:06:07 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["K\u00f6pr\u00fc", "Berkay", ""], ["Aslan", "Murat", ""], ["Kholmatov", "Alisher", ""]]}, {"id": "2105.11453", "submitter": "Zeheng Wang", "authors": "Zeheng Wang, Liang Li, Ross C. C. Leon and Arne Laucht", "title": "Improving Machine Learning-Based Modeling of Semiconductor Devices by\n  Data Self-Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the electronics industry, introducing Machine Learning (ML)-based\ntechniques can enhance Technology Computer-Aided Design (TCAD) methods.\nHowever, the performance of ML models is highly dependent on their training\ndatasets. Particularly in the semiconductor industry, given the fact that the\nfabrication process of semiconductor devices is complicated and expensive, it\nis of great difficulty to obtain datasets with sufficient size and good\nquality. In this paper, we propose a strategy for improving ML-based device\nmodeling by data self-augmentation using variational autoencoder-based\ntechniques, where initially only a few experimental data points are required\nand TCAD tools are not essential. Taking a deep neural network-based prediction\ntask of the Ohmic resistance value in Gallium Nitride devices as an example, we\napply our proposed strategy to augment data points and achieve a reduction in\nthe mean absolute error of predicting the experimental results by up to 70%.\nThe proposed method could be easily modified for different tasks, rendering it\nof high interest to the semiconductor industry in general.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:52:44 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Zeheng", ""], ["Li", "Liang", ""], ["Leon", "Ross C. C.", ""], ["Laucht", "Arne", ""]]}, {"id": "2105.11475", "submitter": "Maxim Ziatdinov", "authors": "Maxim Ziatdinov, Muammer Yusuf Yaman, Yongtao Liu, David Ginger, and\n  Sergei V. Kalinin", "title": "Semi-supervised learning of images with strong rotational disorder:\n  assembling nanoparticle libraries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.mtrl-sci physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The proliferation of optical, electron, and scanning probe microscopies gives\nrise to large volumes of imaging data of objects as diversified as cells,\nbacteria, pollen, to nanoparticles and atoms and molecules. In most cases, the\nexperimental data streams contain images having arbitrary rotations and\ntranslations within the image. At the same time, for many cases, small amounts\nof labeled data are available in the form of prior published results, image\ncollections, and catalogs, or even theoretical models. Here we develop an\napproach that allows generalizing from a small subset of labeled data with a\nweak orientational disorder to a large unlabeled dataset with a much stronger\norientational (and positional) disorder, i.e., it performs a classification of\nimage data given a small number of examples even in the presence of a\ndistribution shift between the labeled and unlabeled parts. This approach is\nbased on the semi-supervised rotationally invariant variational autoencoder\n(ss-rVAE) model consisting of the encoder-decoder \"block\" that learns a\nrotationally (and translationally) invariant continuous latent representation\nof data and a classifier that encodes data into a finite number of discrete\nclasses. The classifier part of the trained ss-rVAE inherits the rotational\n(and translational) invariances and can be deployed independently of the other\nparts of the model. The performance of the ss-rVAE is illustrated using the\nsynthetic data sets with known factors of variation. We further demonstrate its\napplication for experimental data sets of nanoparticles, creating nanoparticle\nlibraries and disentangling the representations defining the physical factors\nof variation in the data. The code reproducing the results is available at\nhttps://github.com/ziatdinovmax/Semi-Supervised-VAE-nanoparticles.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 18:01:57 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ziatdinov", "Maxim", ""], ["Yaman", "Muammer Yusuf", ""], ["Liu", "Yongtao", ""], ["Ginger", "David", ""], ["Kalinin", "Sergei V.", ""]]}, {"id": "2105.11486", "submitter": "Ashwin Prakash Nalwade", "authors": "Ashwin Nalwade, Jackie Kisa", "title": "Experimenting with Knowledge Distillation techniques for performing\n  Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-modal magnetic resonance imaging (MRI) is a crucial method for\nanalyzing the human brain. It is usually used for diagnosing diseases and for\nmaking valuable decisions regarding the treatments - for instance, checking for\ngliomas in the human brain. With varying degrees of severity and detection,\nproperly diagnosing gliomas is one of the most daunting and significant\nanalysis tasks in modern-day medicine. Our primary focus is on working with\ndifferent approaches to perform the segmentation of brain tumors in multimodal\nMRI scans. Now, the quantity, variability of the data used for training has\nalways been considered to be crucial for developing excellent models. Hence, we\nalso want to experiment with Knowledge Distillation techniques.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 18:17:01 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Nalwade", "Ashwin", ""], ["Kisa", "Jackie", ""]]}, {"id": "2105.11492", "submitter": "Mohamadreza Sheibani", "authors": "Mohamadreza Sheibani, Ge Ou", "title": "Adaptive Local Kernels Formulation of Mutual Information with\n  Application to Active Post-Seismic Building Damage Inference", "comments": "The article is under review in journal of Reliability Engineering &\n  System Safety", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of training data is not guaranteed in various supervised\nlearning applications. One of these situations is the post-earthquake regional\ndamage assessment of buildings. Querying the damage label of each building\nrequires a thorough inspection by experts, and thus, is an expensive task. A\npractical approach is to sample the most informative buildings in a sequential\nlearning scheme. Active learning methods recommend the most informative cases\nthat are able to maximally reduce the generalization error. The information\ntheoretic measure of mutual information (MI) is one of the most effective\ncriteria to evaluate the effectiveness of the samples in a pool-based sample\nselection scenario. However, the computational complexity of the standard MI\nalgorithm prevents the utilization of this method on large datasets. A local\nkernels strategy was proposed to reduce the computational costs, but the\nadaptability of the kernels to the observed labels was not considered in the\noriginal formulation of this strategy. In this article, an adaptive local\nkernels methodology is developed that allows for the conformability of the\nkernels to the observed output data while enhancing the computational\ncomplexity of the standard MI algorithm. The proposed algorithm is developed to\nwork on a Gaussian process regression (GPR) framework, where the kernel\nhyperparameters are updated after each label query using the maximum likelihood\nestimation. In the sequential learning procedure, the updated hyperparameters\ncan be used in the MI kernel matrices to improve the sample suggestion\nperformance. The advantages are demonstrated on a simulation of the 2018\nAnchorage, AK, earthquake. It is shown that while the proposed algorithm\nenables GPR to reach acceptable performance with fewer training data, the\ncomputational demands remain lower than the standard local kernels strategy.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 18:34:46 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sheibani", "Mohamadreza", ""], ["Ou", "Ge", ""]]}, {"id": "2105.11508", "submitter": "Erfan Aasi", "authors": "Erfan Aasi, Cristian Ioan Vasile, Mahroo Bahreinian, Calin Belta", "title": "Inferring Temporal Logic Properties from Data using Boosted Decision\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.FL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many autonomous systems, such as robots and self-driving cars, involve\nreal-time decision making in complex environments, and require prediction of\nfuture outcomes from limited data. Moreover, their decisions are increasingly\nrequired to be interpretable to humans for safe and trustworthy co-existence.\nThis paper is a first step towards interpretable learning-based robot control.\nWe introduce a novel learning problem, called incremental formula and predictor\nlearning, to generate binary classifiers with temporal logic structure from\ntime-series data. The classifiers are represented as pairs of Signal Temporal\nLogic (STL) formulae and predictors for their satisfaction. The incremental\nproperty provides prediction of labels for prefix signals that are revealed\nover time. We propose a boosted decision-tree algorithm that leverages weak,\nbut computationally inexpensive, learners to increase prediction and runtime\nperformance. The effectiveness and classification accuracy of our algorithms\nare evaluated on autonomous-driving and naval surveillance case studies.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 19:29:02 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Aasi", "Erfan", ""], ["Vasile", "Cristian Ioan", ""], ["Bahreinian", "Mahroo", ""], ["Belta", "Calin", ""]]}, {"id": "2105.11516", "submitter": "Hyekang Joo", "authors": "Hyekang Joo, Calvin Bao, Ishan Sen, Furong Huang, and Leilani Battle", "title": "Guided Hyperparameter Tuning Through Visualization and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For deep learning practitioners, hyperparameter tuning for optimizing model\nperformance can be a computationally expensive task. Though visualization can\nhelp practitioners relate hyperparameter settings to overall model performance,\nsignificant manual inspection is still required to guide the hyperparameter\nsettings in the next batch of experiments. In response, we present a\nstreamlined visualization system enabling deep learning practitioners to more\nefficiently explore, tune, and optimize hyperparameters in a batch of\nexperiments. A key idea is to directly suggest more optimal hyperparameter\nvalues using a predictive mechanism. We then integrate this mechanism with\ncurrent visualization practices for deep learning. Moreover, an analysis on the\nvariance in a selected performance metric in the context of the model\nhyperparameters shows the impact that certain hyperparameters have on the\nperformance metric. We evaluate the tool with a user study on deep learning\nmodel builders, finding that our participants have little issue adopting the\ntool and working with it as part of their workflow.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 19:55:24 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Joo", "Hyekang", ""], ["Bao", "Calvin", ""], ["Sen", "Ishan", ""], ["Huang", "Furong", ""], ["Battle", "Leilani", ""]]}, {"id": "2105.11521", "submitter": "Adil Rasheed Professor", "authors": "Sindre Stenen Blakseth and Adil Rasheed and Trond Kvamsdal and Omer\n  San", "title": "Deep neural network enabled corrective source term approach to hybrid\n  analysis and modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid Analysis and Modeling (HAM) is an emerging modeling paradigm which\naims to combine physics-based modeling (PBM) and data-driven modeling (DDM) to\ncreate generalizable, trustworthy, accurate, computationally efficient and\nself-evolving models. Here, we introduce, justify and demonstrate a novel\napproach to HAM -- the Corrective Source Term Approach (CoSTA) -- which\naugments the governing equation of a PBM model with a corrective source term\ngenerated by a deep neural network (DNN). In a series of numerical experiments\non one-dimensional heat diffusion, CoSTA is generally found to outperform\ncomparable DDM and PBM models in terms of accuracy -- often reducing predictive\nerrors by several orders of magnitude -- while also generalizing better than\npure DDM. Due to its flexible but solid theoretical foundation, CoSTA provides\na modular framework for leveraging novel developments within both PBM and DDM,\nand due to the interpretability of the DNN-generated source term within the PBM\nparadigm, CoSTA can be a potential door-opener for data-driven techniques to\nenter high-stakes applications previously reserved for pure PBM.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 20:17:13 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Blakseth", "Sindre Stenen", ""], ["Rasheed", "Adil", ""], ["Kvamsdal", "Trond", ""], ["San", "Omer", ""]]}, {"id": "2105.11522", "submitter": "Marco Ballesio", "authors": "Marco Ballesio and Ajay Jasra", "title": "Unbiased Estimation of the Gradient of the Log-Likelihood for a Class of\n  Continuous-Time State-Space Models", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider static parameter estimation for a class of\ncontinuous-time state-space models. Our goal is to obtain an unbiased estimate\nof the gradient of the log-likelihood (score function), which is an estimate\nthat is unbiased even if the stochastic processes involved in the model must be\ndiscretized in time. To achieve this goal, we apply a doubly randomized scheme,\nthat involves a novel coupled conditional particle filter (CCPF) on the second\nlevel of randomization. Our novel estimate helps facilitate the application of\ngradient-based estimation algorithms, such as stochastic-gradient Langevin\ndescent. We illustrate our methodology in the context of stochastic gradient\ndescent (SGD) in several numerical examples and compare with the Rhee & Glynn\nestimator.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 20:31:48 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:56:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ballesio", "Marco", ""], ["Jasra", "Ajay", ""]]}, {"id": "2105.11535", "submitter": "Martin Jankowiak", "authors": "Martin Jankowiak, Geoff Pleiss", "title": "Scalable Cross Validation Losses for Gaussian Process Models", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple and scalable method for training Gaussian process (GP)\nmodels that exploits cross-validation and nearest neighbor truncation. To\naccommodate binary and multi-class classification we leverage P\\`olya-Gamma\nauxiliary variables and variational inference. In an extensive empirical\ncomparison with a number of alternative methods for scalable GP regression and\nclassification, we find that our method offers fast training and excellent\npredictive performance. We argue that the good predictive performance can be\ntraced to the non-parametric nature of the resulting predictive distributions\nas well as to the cross-validation loss, which provides robustness against\nmodel mis-specification.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 21:01:47 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Jankowiak", "Martin", ""], ["Pleiss", "Geoff", ""]]}, {"id": "2105.11537", "submitter": "Shiwei Lyu", "authors": "Shiwei Lyu, Shuai Ling, Kaihao Guo, Haipeng Zhang, Kunpeng Zhang,\n  Suting Hong, Qing Ke, Jinjie Gu", "title": "Graph Neural Network Based VC Investment Success Prediction", "comments": "11pages, 5figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the start-ups that will eventually succeed is essentially\nimportant for the venture capital business and worldwide policy makers,\nespecially at an early stage such that rewards can possibly be exponential.\n  Though various empirical studies and data-driven modeling work have been\ndone, the predictive power of the complex networks of stakeholders including\nventure capital investors, start-ups, and start-ups' managing members has not\nbeen thoroughly explored. We design an incremental representation learning\nmechanism and a sequential learning model, utilizing the network structure\ntogether with the rich attributes of the nodes. In general, our method achieves\nthe state-of-the-art prediction performance on a comprehensive dataset of\nglobal venture capital investments and surpasses human investors by large\nmargins. Specifically, it excels at predicting the outcomes for start-ups in\nindustries such as healthcare and IT. Meanwhile, we shed light on impacts on\nstart-up success from observable factors including gender, education, and\nnetworking, which can be of value for practitioners as well as policy makers\nwhen they screen ventures of high growth potentials.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:29:45 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lyu", "Shiwei", ""], ["Ling", "Shuai", ""], ["Guo", "Kaihao", ""], ["Zhang", "Haipeng", ""], ["Zhang", "Kunpeng", ""], ["Hong", "Suting", ""], ["Ke", "Qing", ""], ["Gu", "Jinjie", ""]]}, {"id": "2105.11544", "submitter": "Oliver Stephenson", "authors": "Oliver L. Stephenson, Tobias K\\\"ohne, Eric Zhan, Brent E. Cahill,\n  Sang-Ho Yun, Zachary E. Ross, Mark Simons", "title": "Deep Learning-based Damage Mapping with InSAR Coherence Time Series", "comments": "18 pages, 7 figures, plus supplementary materials", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing (2021)", "doi": "10.1109/TGRS.2021.3084209", "report-no": null, "categories": "physics.geo-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite remote sensing is playing an increasing role in the rapid mapping\nof damage after natural disasters. In particular, synthetic aperture radar\n(SAR) can image the Earth's surface and map damage in all weather conditions,\nday and night. However, current SAR damage mapping methods struggle to separate\ndamage from other changes in the Earth's surface. In this study, we propose a\nnovel approach to damage mapping, combining deep learning with the full time\nhistory of SAR observations of an impacted region in order to detect anomalous\nvariations in the Earth's surface properties due to a natural disaster. We\nquantify Earth surface change using time series of Interferometric SAR\ncoherence, then use a recurrent neural network (RNN) as a probabilistic anomaly\ndetector on these coherence time series. The RNN is first trained on pre-event\ncoherence time series, and then forecasts a probability distribution of the\ncoherence between pre- and post-event SAR images. The difference between the\nforecast and observed co-event coherence provides a measure of the confidence\nin the identification of damage. The method allows the user to choose a damage\ndetection threshold that is customized for each location, based on the local\nbehavior of coherence through time before the event. We apply this method to\ncalculate estimates of damage for three earthquakes using multi-year time\nseries of Sentinel-1 SAR acquisitions. Our approach shows good agreement with\nobserved damage and quantitative improvement compared to using pre- to co-event\ncoherence loss as a damage proxy.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 21:21:03 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Stephenson", "Oliver L.", ""], ["K\u00f6hne", "Tobias", ""], ["Zhan", "Eric", ""], ["Cahill", "Brent E.", ""], ["Yun", "Sang-Ho", ""], ["Ross", "Zachary E.", ""], ["Simons", "Mark", ""]]}, {"id": "2105.11549", "submitter": "Hongjing Zhang", "authors": "Hongjing Zhang, Ian Davidson", "title": "Deep Descriptive Clustering", "comments": "Paper accepted at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on explainable clustering allows describing clusters when the\nfeatures are interpretable. However, much modern machine learning focuses on\ncomplex data such as images, text, and graphs where deep learning is used but\nthe raw features of data are not interpretable. This paper explores a novel\nsetting for performing clustering on complex data while simultaneously\ngenerating explanations using interpretable tags. We propose deep descriptive\nclustering that performs sub-symbolic representation learning on complex data\nwhile generating explanations based on symbolic data. We form good clusters by\nmaximizing the mutual information between empirical distribution on the inputs\nand the induced clustering labels for clustering objectives. We generate\nexplanations by solving an integer linear programming that generates concise\nand orthogonal descriptions for each cluster. Finally, we allow the explanation\nto inform better clustering by proposing a novel pairwise loss with\nself-generated constraints to maximize the clustering and explanation module's\nconsistency. Experimental results on public data demonstrate that our model\noutperforms competitive baselines in clustering performance while offering\nhigh-quality cluster-level explanations.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 21:40:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zhang", "Hongjing", ""], ["Davidson", "Ian", ""]]}, {"id": "2105.11558", "submitter": "Suhas S Kowshik", "authors": "Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, Praneeth Netrapalli", "title": "Near-optimal Offline and Streaming Algorithms for Learning Non-Linear\n  Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of vector valued non-linear dynamical systems\n$X_{t+1} = \\phi(A^* X_t) + \\eta_t$, where $\\eta_t$ is unbiased noise and $\\phi\n: \\mathbb{R} \\to \\mathbb{R}$ is a known link function that satisfies certain\n{\\em expansivity property}. The goal is to learn $A^*$ from a single trajectory\n$X_1,\\cdots,X_T$ of {\\em dependent or correlated} samples. While the problem is\nwell-studied in the linear case, where $\\phi$ is identity, with optimal error\nrates even for non-mixing systems, existing results in the non-linear case hold\nonly for mixing systems. In this work, we improve existing results for learning\nnonlinear systems in a number of ways: a) we provide the first offline\nalgorithm that can learn non-linear dynamical systems without the mixing\nassumption, b) we significantly improve upon the sample complexity of existing\nresults for mixing systems, c) in the much harder one-pass, streaming setting\nwe study a SGD with Reverse Experience Replay ($\\mathsf{SGD-RER}$) method, and\ndemonstrate that for mixing systems, it achieves the same sample complexity as\nour offline algorithm, d) we justify the expansivity assumption by showing that\nfor the popular ReLU link function -- a non-expansive but easy to learn link\nfunction with i.i.d. samples -- any method would require exponentially many\nsamples (with respect to dimension of $X_t$) from the dynamical system. We\nvalidate our results via. simulations and demonstrate that a naive application\nof SGD can be highly sub-optimal. Indeed, our work demonstrates that for\ncorrelated data, specialized methods designed for the dependency structure in\ndata can significantly outperform standard SGD based methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 22:14:26 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 16:48:00 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Jain", "Prateek", ""], ["Kowshik", "Suhas S", ""], ["Nagaraj", "Dheeraj", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "2105.11559", "submitter": "Taylor Archibald", "authors": "Taylor Archibald, Mason Poggemann, Aaron Chan, Tony Martinez", "title": "TRACE: A Differentiable Approach to Line-level Stroke Recovery for\n  Offline Handwritten Text", "comments": "Accepted as a conference paper at the 16th International Conference\n  on Document Analysis and Recognition (ICDAR), Lausanne, Switzerland, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stroke order and velocity are helpful features in the fields of signature\nverification, handwriting recognition, and handwriting synthesis. Recovering\nthese features from offline handwritten text is a challenging and well-studied\nproblem. We propose a new model called TRACE (Trajectory Recovery by an\nAdaptively-trained Convolutional Encoder). TRACE is a differentiable approach\nthat uses a convolutional recurrent neural network (CRNN) to infer temporal\nstroke information from long lines of offline handwritten text with many\ncharacters and dynamic time warping (DTW) to align predictions and ground truth\npoints. TRACE is perhaps the first system to be trained end-to-end on entire\nlines of text of arbitrary width and does not require the use of dynamic\nexemplars. Moreover, the system does not require images to undergo any\npre-processing, nor do the predictions require any post-processing.\nConsequently, the recovered trajectory is differentiable and can be used as a\nloss function for other tasks, including synthesizing offline handwritten text.\n  We demonstrate that temporal stroke information recovered by TRACE from\noffline data can be used for handwriting synthesis and establish the first\nbenchmarks for a stroke trajectory recovery system trained on the IAM online\nhandwriting dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 22:15:50 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Archibald", "Taylor", ""], ["Poggemann", "Mason", ""], ["Chan", "Aaron", ""], ["Martinez", "Tony", ""]]}, {"id": "2105.11570", "submitter": "Xintao Wu", "authors": "Wei Du and Xintao Wu", "title": "Robust Fairness-aware Learning Under Sample Selection Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The underlying assumption of many machine learning algorithms is that the\ntraining data and test data are drawn from the same distributions. However, the\nassumption is often violated in real world due to the sample selection bias\nbetween the training and test data. Previous research works focus on reweighing\nbiased training data to match the test data and then building classification\nmodels on the reweighed training data. However, how to achieve fairness in the\nbuilt classification models is under-explored. In this paper, we propose a\nframework for robust and fair learning under sample selection bias. Our\nframework adopts the reweighing estimation approach for bias correction and the\nminimax robust estimation approach for achieving robustness on prediction\naccuracy. Moreover, during the minimax optimization, the fairness is achieved\nunder the worst case, which guarantees the model's fairness on test data. We\nfurther develop two algorithms to handle sample selection bias when test data\nis both available and unavailable. We conduct experiments on two real-world\ndatasets and the experimental results demonstrate its effectiveness in terms of\nboth utility and fairness metrics.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 23:23:36 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Du", "Wei", ""], ["Wu", "Xintao", ""]]}, {"id": "2105.11576", "submitter": "Xiao Huang", "authors": "Jiaming Wang, Zhenfeng Shao, Xiao Huang, Tao Lu, Ruiqian Zhang, Jiayi\n  Ma", "title": "Pan-sharpening via High-pass Modification Convolutional Neural Network", "comments": "5 pages, 5 figures, accepted by the 28th IEEE International\n  Conference on Image Processing (ICIP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing deep learning-based pan-sharpening methods have several widely\nrecognized issues, such as spectral distortion and insufficient spatial texture\nenhancement, we propose a novel pan-sharpening convolutional neural network\nbased on a high-pass modification block. Different from existing methods, the\nproposed block is designed to learn the high-pass information, leading to\nenhance spatial information in each band of the multi-spectral-resolution\nimages. To facilitate the generation of visually appealing pan-sharpened\nimages, we propose a perceptual loss function and further optimize the model\nbased on high-level features in the near-infrared space. Experiments\ndemonstrate the superior performance of the proposed method compared to the\nstate-of-the-art pan-sharpening methods, both quantitatively and qualitatively.\nThe proposed model is open-sourced at https://github.com/jiaming-wang/HMB.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 23:39:04 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Jiaming", ""], ["Shao", "Zhenfeng", ""], ["Huang", "Xiao", ""], ["Lu", "Tao", ""], ["Zhang", "Ruiqian", ""], ["Ma", "Jiayi", ""]]}, {"id": "2105.11589", "submitter": "Karthik Gopalakrishnan", "authors": "Ayush Shrivastava, Karthik Gopalakrishnan, Yang Liu, Robinson\n  Piramuthu, Gokhan T\\\"ur, Devi Parikh, Dilek Hakkani-T\\\"ur", "title": "VISITRON: Visual Semantics-Aligned Interactively Trained\n  Object-Navigator", "comments": "Accepted at NAACL 2021, Visually Grounded Interaction and Language\n  (ViGIL) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive robots navigating photo-realistic environments face challenges\nunderlying vision-and-language navigation (VLN), but in addition, they need to\nbe trained to handle the dynamic nature of dialogue. However, research in\nCooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts\nwith a guide in natural language in order to reach a goal, treats the dialogue\nhistory as a VLN-style static instruction. In this paper, we present VISITRON,\na navigator better suited to the interactive regime inherent to CVDN by being\ntrained to: i) identify and associate object-level concepts and semantics\nbetween the environment and dialogue history, ii) identify when to interact vs.\nnavigate via imitation learning of a binary classification head. We perform\nextensive ablations with VISITRON to gain empirical insights and improve\nperformance on CVDN. VISITRON is competitive with models on the static CVDN\nleaderboard. We also propose a generalized interactive regime to fine-tune and\nevaluate VISITRON and future such models with pre-trained guides for\nadaptability.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:21:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shrivastava", "Ayush", ""], ["Gopalakrishnan", "Karthik", ""], ["Liu", "Yang", ""], ["Piramuthu", "Robinson", ""], ["T\u00fcr", "Gokhan", ""], ["Parikh", "Devi", ""], ["Hakkani-T\u00fcr", "Dilek", ""]]}, {"id": "2105.11590", "submitter": "Nathan Miller", "authors": "Nathan Eli Miller and Saibal Mukhopadhyay", "title": "A Quantum Hopfield Associative Memory Implemented on an Actual Quantum\n  Processor", "comments": "17 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a Quantum Hopfield Associative Memory (QHAM) and\ndemonstrate its capabilities in simulation and hardware using IBM Quantum\nExperience. The QHAM is based on a quantum neuron design which can be utilized\nfor many different machine learning applications and can be implemented on real\nquantum hardware without requiring mid-circuit measurement or reset operations.\nWe analyze the accuracy of the neuron and the full QHAM considering hardware\nerrors via simulation with hardware noise models as well as with implementation\non the 15-qubit ibmq_16_melbourne device. The quantum neuron and the QHAM are\nshown to be resilient to noise and require low qubit and time overhead. We\nbenchmark the QHAM by testing its effective memory capacity against qubit- and\ncircuit-level errors and demonstrate its capabilities in the NISQ-era of\nquantum hardware. This demonstration of the first functional QHAM to be\nimplemented in NISQ-era quantum hardware is a significant step in machine\nlearning at the leading edge of quantum computing.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:45:57 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Miller", "Nathan Eli", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "2105.11598", "submitter": "Jackson Shields", "authors": "Jackson Shields, Oscar Pizarro, Stefan B. Williams", "title": "Feature Space Exploration For Planning Initial Benthic AUV Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Special-purpose Autonomous Underwater Vehicles (AUVs) are utilised for\nbenthic (seafloor) surveys, where the vehicle collects optical imagery of near\nthe seafloor. Due to the small-sensor footprint of the cameras and the vast\nareas to be surveyed, these AUVs can not feasibly full coverage of areas larger\nthan a few tens of thousands of square meters. Therefore AUV paths which sample\nsparsely, yet effectively, the survey areas are necessary. Broad scale acoustic\nbathymetric data is ready available over large areas, and often is a useful\nprior of seafloor cover. As such, prior bathymetry can be used to guide AUV\ndata collection. This research proposes methods for planning initial AUV\nsurveys that efficiently explore a feature space representation of the\nbathymetry, in order to sample from a diverse set of bathymetric terrain. This\nwill enable the AUV to visit areas that likely contain unique habitats and are\nrepresentative of the entire survey site. The suitability of these methods to\nplan AUV surveys is evaluated based on the coverage of the feature space and\nalso the ability to visit all classes of benthic habitat on the initial dive.\nThis is a valuable tool for AUV surveys as it increases the utility of initial\ndives. It also delivers a comprehensive training set to learn a relationship\nbetween acoustic bathymetry and visually-derived seafloor classifications.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:20:18 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shields", "Jackson", ""], ["Pizarro", "Oscar", ""], ["Williams", "Stefan B.", ""]]}, {"id": "2105.11601", "submitter": "Lei Li", "authors": "Lei Li, Yongfeng Zhang, Li Chen", "title": "Personalized Transformer for Explainable Recommendation", "comments": "Published as a conference paper at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalization of natural language generation plays a vital role in a large\nspectrum of tasks, such as explainable recommendation, review summarization and\ndialog systems. In these tasks, user and item IDs are important identifiers for\npersonalization. Transformer, which is demonstrated with strong language\nmodeling capability, however, is not personalized and fails to make use of the\nuser and item IDs since the ID tokens are not even in the same semantic space\nas the words. To address this problem, we present a PErsonalized Transformer\nfor Explainable Recommendation (PETER), on which we design a simple and\neffective learning objective that utilizes the IDs to predict the words in the\ntarget explanation, so as to endow the IDs with linguistic meanings and to\nachieve personalized Transformer. Besides generating explanations, PETER can\nalso make recommendations, which makes it a unified model for the whole\nrecommendation-explanation pipeline. Extensive experiments show that our small\nunpretrained model outperforms fine-tuned BERT on the generation task, in terms\nof both effectiveness and efficiency, which highlights the importance and the\nnice utility of our design.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:42:47 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 01:19:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Lei", ""], ["Zhang", "Yongfeng", ""], ["Chen", "Li", ""]]}, {"id": "2105.11602", "submitter": "Saeedreza Shehnepoor", "authors": "Saeedreza Shehnepoor, Roberto Togneri, Wei Liu, Mohammed Bennamoun", "title": "HIN-RNN: A Graph Representation Learning Neural Network for Fraudster\n  Group Detection With No Handcrafted Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Social reviews are indispensable resources for modern consumers' decision\nmaking. For financial gain, companies pay fraudsters preferably in groups to\ndemote or promote products and services since consumers are more likely to be\nmisled by a large number of similar reviews from groups. Recent approaches on\nfraudster group detection employed handcrafted features of group behaviors\nwithout considering the semantic relation between reviews from the reviewers in\na group. In this paper, we propose the first neural approach, HIN-RNN, a\nHeterogeneous Information Network (HIN) Compatible RNN for fraudster group\ndetection that requires no handcrafted features. HIN-RNN provides a unifying\narchitecture for representation learning of each reviewer, with the initial\nvector as the sum of word embeddings of all review text written by the same\nreviewer, concatenated by the ratio of negative reviews. Given a co-review\nnetwork representing reviewers who have reviewed the same items with the same\nratings and the reviewers' vector representation, a collaboration matrix is\nacquired through HIN-RNN training. The proposed approach is confirmed to be\neffective with marked improvement over state-of-the-art approaches on both the\nYelp (22% and 12% in terms of recall and F1-value, respectively) and Amazon (4%\nand 2% in terms of recall and F1-value, respectively) datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:48:28 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shehnepoor", "Saeedreza", ""], ["Togneri", "Roberto", ""], ["Liu", "Wei", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "2105.11603", "submitter": "Areeq Hasan", "authors": "Areeq I. Hasan", "title": "IGO-QNN: Quantum Neural Network Architecture for Inductive Grover\n  Oracularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel paradigm of integration of Grover's algorithm in a machine\nlearning framework: the inductive Grover oracular quantum neural network\n(IGO-QNN). The model defines a variational quantum circuit with hidden layers\nof parameterized quantum neurons densely connected via entangle synapses to\nencode a dynamic Grover's search oracle that can be trained from a set of\ndatabase-hit training examples. This widens the range of problem applications\nof Grover's unstructured search algorithm to include the vast majority of\nproblems lacking analytic descriptions of solution verifiers, allowing for\nquadratic speed-up in unstructured search for the set of search problems with\nrelationships between input and output spaces that are tractably underivable\ndeductively. This generalization of Grover's oracularization may prove\nparticularly effective in deep reinforcement learning, computer vision, and,\nmore generally, as a feature vector classifier at the top of an existing model.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:52:44 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:25:50 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hasan", "Areeq I.", ""]]}, {"id": "2105.11611", "submitter": "Zijian Gao", "authors": "Zijian Gao, Kele Xu, Bo Ding, Huaimin Wang, Yiying Li, Hongda Jia", "title": "KnowSR: Knowledge Sharing among Homogeneous Agents in Multi-agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep reinforcement learning (RL) algorithms have made great\nprogress in multi-agent domain. However, due to characteristics of RL, training\nfor complex tasks would be resource-intensive and time-consuming. To meet this\nchallenge, mutual learning strategy between homogeneous agents is essential,\nwhich is under-explored in previous studies, because most existing methods do\nnot consider to use the knowledge of agent models. In this paper, we present an\nadaptation method of the majority of multi-agent reinforcement learning (MARL)\nalgorithms called KnowSR which takes advantage of the differences in learning\nbetween agents. We employ the idea of knowledge distillation (KD) to share\nknowledge among agents to shorten the training phase. To empirically\ndemonstrate the robustness and effectiveness of KnowSR, we performed extensive\nexperiments on state-of-the-art MARL algorithms in collaborative and\ncompetitive scenarios. The results demonstrate that KnowSR outperforms recently\nreported methodologies, emphasizing the importance of the proposed knowledge\nsharing for MARL.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:19:41 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Gao", "Zijian", ""], ["Xu", "Kele", ""], ["Ding", "Bo", ""], ["Wang", "Huaimin", ""], ["Li", "Yiying", ""], ["Jia", "Hongda", ""]]}, {"id": "2105.11617", "submitter": "Amartya Mukherjee", "authors": "Amartya Mukherjee", "title": "A Comparison of Reward Functions in Q-Learning Applied to a Cart\n  Position Problem", "comments": "19 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Growing advancements in reinforcement learning has led to advancements in\ncontrol theory. Reinforcement learning has effectively solved the inverted\npendulum problem and more recently the double inverted pendulum problem. In\nreinforcement learning, our agents learn by interacting with the control system\nwith the goal of maximizing rewards. In this paper, we explore three such\nreward functions in the cart position problem. This paper concludes that a\ndiscontinuous reward function that gives non-zero rewards to agents only if\nthey are within a given distance from the desired position gives the best\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:26:01 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Mukherjee", "Amartya", ""]]}, {"id": "2105.11622", "submitter": "Shihang Feng", "authors": "Shihang Feng, Xitong Zhang, Brendt Wohlberg, Neill Symons and Youzuo\n  Lin", "title": "Connect the Dots: In Situ 4D Seismic Monitoring of CO$_2$ Storage with\n  Spatio-temporal CNNs", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  4D seismic imaging has been widely used in CO$_2$ sequestration projects to\nmonitor the fluid flow in the volumetric subsurface region that is not sampled\nby wells. Ideally, real-time monitoring and near-future forecasting would\nprovide site operators with great insights to understand the dynamics of the\nsubsurface reservoir and assess any potential risks. However, due to obstacles\nsuch as high deployment cost, availability of acquisition equipment, exclusion\nzones around surface structures, only very sparse seismic imaging data can be\nobtained during monitoring. That leads to an unavoidable and growing knowledge\ngap over time. The operator needs to understand the fluid flow throughout the\nproject lifetime and the seismic data are only available at a limited number of\ntimes, this is insufficient for understanding the reservoir behavior. To\novercome those challenges, we have developed spatio-temporal\nneural-network-based models that can produce high-fidelity interpolated or\nextrapolated images effectively and efficiently. Specifically, our models are\nbuilt on an autoencoder, and incorporate the long short-term memory (LSTM)\nstructure with a new loss function regularized by optical flow. We validate the\nperformance of our models using real 4D post-stack seismic imaging data\nacquired at the Sleipner CO$_2$ sequestration field. We employ two different\nstrategies in evaluating our models. Numerically, we compare our models with\ndifferent baseline approaches using classic pixel-based metrics. We also\nconduct a blind survey and collect a total of 20 responses from domain experts\nto evaluate the quality of data generated by our models. Via both numerical and\nexpert evaluation, we conclude that our models can produce high-quality 2D/3D\nseismic imaging data at a reasonable cost, offering the possibility of\nreal-time monitoring or even near-future forecasting of the CO$_2$ storage\nreservoir.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:38:22 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Feng", "Shihang", ""], ["Zhang", "Xitong", ""], ["Wohlberg", "Brendt", ""], ["Symons", "Neill", ""], ["Lin", "Youzuo", ""]]}, {"id": "2105.11625", "submitter": "Shuhao Shi", "authors": "S. Shi, Kai Qiao, Shuai Yang, L. Wang, J. Chen and Bin Yan", "title": "AdaGCN:Adaptive Boosting Algorithm for Graph Convolutional Networks on\n  Imbalanced Node Classification", "comments": "17 pages, 5 figures, Submitted to MACHINE LEARNING", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graph Neural Network (GNN) has achieved remarkable success in graph data\nrepresentation. However, the previous work only considered the ideal balanced\ndataset, and the practical imbalanced dataset was rarely considered, which, on\nthe contrary, is of more significance for the application of GNN. Traditional\nmethods such as resampling, reweighting and synthetic samples that deal with\nimbalanced datasets are no longer applicable in GNN. Ensemble models can handle\nimbalanced datasets better compared with single estimator. Besides, ensemble\nlearning can achieve higher estimation accuracy and has better reliability\ncompared with the single estimator. In this paper, we propose an ensemble model\ncalled AdaGCN, which uses a Graph Convolutional Network (GCN) as the base\nestimator during adaptive boosting. In AdaGCN, a higher weight will be set for\nthe training samples that are not properly classified by the previous\nclassifier, and transfer learning is used to reduce computational cost and\nincrease fitting capability. Experiments show that the AdaGCN model we proposed\nachieves better performance than GCN, GraphSAGE, GAT, N-GCN and the most of\nadvanced reweighting and resampling methods on synthetic imbalanced datasets,\nwith an average improvement of 4.3%. Our model also improves state-of-the-art\nbaselines on all of the challenging node classification tasks we consider:\nCora, Citeseer, Pubmed, and NELL.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:43:31 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shi", "S.", ""], ["Qiao", "Kai", ""], ["Yang", "Shuai", ""], ["Wang", "L.", ""], ["Chen", "J.", ""], ["Yan", "Bin", ""]]}, {"id": "2105.11627", "submitter": "Jingshuang Chen", "authors": "Zhiqiang Cai, Jingshuang Chen, Min Liu", "title": "Least-Squares ReLU Neural Network (LSNN) Method For Scalar Nonlinear\n  Hyperbolic Conservation Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduced the least-squares ReLU neural network (LSNN) method for solving\nthe linear advection-reaction problem with discontinuous solution and showed\nthat the method outperforms mesh-based numerical methods in terms of the number\nof degrees of freedom. This paper studies the LSNN method for scalar nonlinear\nhyperbolic conservation law. The method is a discretization of an equivalent\nleast-squares (LS) formulation in the set of neural network functions with the\nReLU activation function. Evaluation of the LS functional is done by using\nnumerical integration and conservative finite volume scheme. Numerical results\nof some test problems show that the method is capable of approximating the\ndiscontinuous interface of the underlying problem automatically through the\nfree breaking lines of the ReLU neural network. Moreover, the method does not\nexhibit the common Gibbs phenomena along the discontinuous interface.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:59:48 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Cai", "Zhiqiang", ""], ["Chen", "Jingshuang", ""], ["Liu", "Min", ""]]}, {"id": "2105.11632", "submitter": "Jingshuang Chen", "authors": "Zhiqiang Cai, Jingshuang Chen, Min Liu", "title": "Least-Squares ReLU Neural Network (LSNN) Method For Linear\n  Advection-Reaction Equation", "comments": "submitted to Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2021.110514", "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies least-squares ReLU neural network method for solving the\nlinear advection-reaction problem with discontinuous solution. The method is a\ndiscretization of an equivalent least-squares formulation in the set of neural\nnetwork functions with the ReLU activation function. The method is capable of\napproximating the discontinuous interface of the underlying problem\nautomatically through the free hyper-planes of the ReLU neural network and,\nhence, outperforms mesh-based numerical methods in terms of the number of\ndegrees of freedom. Numerical results of some benchmark test problems show that\nthe method can not only approximate the solution with the least number of\nparameters, but also avoid the common Gibbs phenomena along the discontinuous\ninterface. Moreover, a three-layer ReLU neural network is necessary and\nsufficient in order to well approximate a discontinuous solution with an\ninterface in $\\mathbb{R}^2$ that is not a straight line.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:13:15 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Cai", "Zhiqiang", ""], ["Chen", "Jingshuang", ""], ["Liu", "Min", ""]]}, {"id": "2105.11634", "submitter": "Diaa Badawi", "authors": "Hongyi Pan, Diaa Badawi, Erdem Koyuncu, A. Enis Cetin", "title": "Robust Principal Component Analysis Using a Novel Kernel Related with\n  the L1-Norm", "comments": "6 pages, 3 tables and one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a family of vector dot products that can be implemented using\nsign changes and addition operations only. The dot products are\nenergy-efficient as they avoid the multiplication operation entirely. Moreover,\nthe dot products induce the $\\ell_1$-norm, thus providing robustness to\nimpulsive noise. First, we analytically prove that the dot products yield\nsymmetric, positive semi-definite generalized covariance matrices, thus\nenabling principal component analysis (PCA). Moreover, the generalized\ncovariance matrices can be constructed in an Energy Efficient (EEF) manner due\nto the multiplication-free property of the underlying vector products. We\npresent image reconstruction examples in which our EEF PCA method result in the\nhighest peak signal-to-noise ratios compared to the ordinary $\\ell_2$-PCA and\nthe recursive $\\ell_1$-PCA.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:17:51 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Pan", "Hongyi", ""], ["Badawi", "Diaa", ""], ["Koyuncu", "Erdem", ""], ["Cetin", "A. Enis", ""]]}, {"id": "2105.11636", "submitter": "Bo Li", "authors": "Bo Li, Qili Wang, Gim Hee Lee", "title": "FILTRA: Rethinking Steerable CNN by Filter Transform", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steerable CNN imposes the prior knowledge of transformation invariance or\nequivariance in the network architecture to enhance the the network robustness\non geometry transformation of data and reduce overfitting. It has been an\nintuitive and widely used technique to construct a steerable filter by\naugmenting a filter with its transformed copies in the past decades, which is\nnamed as filter transform in this paper. Recently, the problem of steerable CNN\nhas been studied from aspect of group representation theory, which reveals the\nfunction space structure of a steerable kernel function. However, it is not yet\nclear on how this theory is related to the filter transform technique. In this\npaper, we show that kernel constructed by filter transform can also be\ninterpreted in the group representation theory. This interpretation help\ncomplete the puzzle of steerable CNN theory and provides a novel and simple\napproach to implement steerable convolution operators. Experiments are executed\non multiple datasets to verify the feasibility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:32:34 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Bo", ""], ["Wang", "Qili", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2105.11640", "submitter": "Zhaoxuan Zhu", "authors": "Zhaoxuan Zhu, Nicola Pivaro, Shobhit Gupta, Abhishek Gupta and\n  Marcello Canova", "title": "Safe Model-based Off-policy Reinforcement Learning for Eco-Driving in\n  Connected and Automated Hybrid Electric Vehicles", "comments": "This work has been submitted to the IEEE for possible publication and\n  is under review. Paper summary: 14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Connected and Automated Hybrid Electric Vehicles have the potential to reduce\nfuel consumption and travel time in real-world driving conditions. The\neco-driving problem seeks to design optimal speed and power usage profiles\nbased upon look-ahead information from connectivity and advanced mapping\nfeatures. Recently, Deep Reinforcement Learning (DRL) has been applied to the\neco-driving problem. While the previous studies synthesize simulators and\nmodel-free DRL to reduce online computation, this work proposes a Safe\nOff-policy Model-Based Reinforcement Learning algorithm for the eco-driving\nproblem. The advantages over the existing literature are three-fold. First, the\ncombination of off-policy learning and the use of a physics-based model\nimproves the sample efficiency. Second, the training does not require any\nextrinsic rewarding mechanism for constraint satisfaction. Third, the\nfeasibility of trajectory is guaranteed by using a safe set approximated by\ndeep generative models.\n  The performance of the proposed method is benchmarked against a baseline\ncontroller representing human drivers, a previously designed model-free DRL\nstrategy, and the wait-and-see optimal solution. In simulation, the proposed\nalgorithm leads to a policy with a higher average speed and a better fuel\neconomy compared to the model-free agent. Compared to the baseline controller,\nthe learned strategy reduces the fuel consumption by more than 21\\% while\nkeeping the average speed comparable.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:41:29 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zhu", "Zhaoxuan", ""], ["Pivaro", "Nicola", ""], ["Gupta", "Shobhit", ""], ["Gupta", "Abhishek", ""], ["Canova", "Marcello", ""]]}, {"id": "2105.11646", "submitter": "Yassine Yaakoubi", "authors": "Yassine Yaakoubi, Fran\\c{c}ois Soumis, Simon Lacoste-Julien", "title": "Structured Convolutional Kernel Networks for Airline Crew Scheduling", "comments": "ICML 2021 (Proceedings of the 38th International Conference on\n  Machine Learning, PMLR 139:11626-11636)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the needs from an airline crew scheduling application, we\nintroduce structured convolutional kernel networks (Struct-CKN), which combine\nCKNs from Mairal et al. (2014) in a structured prediction framework that\nsupports constraints on the outputs. CKNs are a particular kind of\nconvolutional neural networks that approximate a kernel feature map on training\ndata, thus combining properties of deep learning with the non-parametric\nflexibility of kernel methods. Extending CKNs to structured outputs allows us\nto obtain useful initial solutions on a flight-connection dataset that can be\nfurther refined by an airline crew scheduling solver. More specifically, we use\na flight-based network modeled as a general conditional random field capable of\nincorporating local constraints in the learning process. Our experiments\ndemonstrate that this approach yields significant improvements for the\nlarge-scale crew pairing problem (50,000 flights per month) over standard\napproaches, reducing the solution cost by 17% (a gain of millions of dollars)\nand the cost of global constraints by 97%.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:47:06 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 23:01:43 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yaakoubi", "Yassine", ""], ["Soumis", "Fran\u00e7ois", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "2105.11653", "submitter": "Anand Rajagopalan", "authors": "Baris Sumengen (1), Anand Rajagopalan (1), Gui Citovsky (1), David\n  Simcha (1), Olivier Bachem (1), Pradipta Mitra (1), Sam Blasiak (1), Mason\n  Liang (2), Sanjiv Kumar (1) ((1) Google Research, (2) 0x Labs)", "title": "Scaling Hierarchical Agglomerative Clustering to Billion-sized Datasets", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hierarchical Agglomerative Clustering (HAC) is one of the oldest but still\nmost widely used clustering methods. However, HAC is notoriously hard to scale\nto large data sets as the underlying complexity is at least quadratic in the\nnumber of data points and many algorithms to solve HAC are inherently\nsequential. In this paper, we propose {Reciprocal Agglomerative Clustering\n(RAC)}, a distributed algorithm for HAC, that uses a novel strategy to\nefficiently merge clusters in parallel. We prove theoretically that RAC\nrecovers the exact solution of HAC. Furthermore, under clusterability and\nbalancedness assumption we show provable speedups in total runtime due to the\nparallelism. We also show that these speedups are achievable for certain\nprobabilistic data models. In extensive experiments, we show that this\nparallelism is achieved on real world data sets and that the proposed RAC\nalgorithm can recover the HAC hierarchy on billions of data points connected by\ntrillions of edges in less than an hour.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 04:14:21 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sumengen", "Baris", "", "Google Research"], ["Rajagopalan", "Anand", "", "Google Research"], ["Citovsky", "Gui", "", "Google Research"], ["Simcha", "David", "", "Google Research"], ["Bachem", "Olivier", "", "Google Research"], ["Mitra", "Pradipta", "", "Google Research"], ["Blasiak", "Sam", "", "Google Research"], ["Liang", "Mason", "", "0x Labs"], ["Kumar", "Sanjiv", "", "Google Research"]]}, {"id": "2105.11654", "submitter": "Jianhao Ding", "authors": "Jianhao Ding, Zhaofei Yu, Yonghong Tian and Tiejun Huang", "title": "Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep\n  Spiking Neural Networks", "comments": "9 pages, 7 figures, 2 tables. To appear in the 30th International\n  Joint Conference on Artificial Intelligence (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural\nnetworks, have attracted great attentions from researchers and industry. The\nmost efficient way to train deep SNNs is through ANN-SNN conversion. However,\nthe conversion usually suffers from accuracy loss and long inference time,\nwhich impede the practical application of SNN. In this paper, we theoretically\nanalyze ANN-SNN conversion and derive sufficient conditions of the optimal\nconversion. To better correlate ANN-SNN and get greater accuracy, we propose\nRate Norm Layer to replace the ReLU activation function in source ANN training,\nenabling direct conversion from a trained ANN to an SNN. Moreover, we propose\nan optimal fit curve to quantify the fit between the activation value of source\nANN and the actual firing rate of target SNN. We show that the inference time\ncan be reduced by optimizing the upper bound of the fit curve in the revised\nANN to achieve fast inference. Our theory can explain the existing work on fast\nreasoning and get better results. The experimental results show that the\nproposed method achieves near loss less conversion with VGG-16,\nPreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster\nreasoning performance under 0.265x energy consumption of the typical method.\nThe code is available at\nhttps://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 04:15:06 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ding", "Jianhao", ""], ["Yu", "Zhaofei", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""]]}, {"id": "2105.11672", "submitter": "Weihong Lin", "authors": "Weihong Lin, Qifang Gao, Lei Sun, Zhuoyao Zhong, Kai Hu, Qin Ren,\n  Qiang Huo", "title": "ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for\n  Key Information Extraction from Documents", "comments": "To be published at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent grid-based document representations like BERTgrid allow the\nsimultaneous encoding of the textual and layout information of a document in a\n2D feature map so that state-of-the-art image segmentation and/or object\ndetection models can be straightforwardly leveraged to extract key information\nfrom documents. However, such methods have not achieved comparable performance\nto state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK\nyet. In this paper, we propose a new multi-modal backbone network by\nconcatenating a BERTgrid to an intermediate layer of a CNN model, where the\ninput of CNN is a document image and the BERTgrid is a grid of word embeddings,\nto generate a more powerful grid-based document representation, named\nViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal\nbackbone network are trained jointly. Our experimental results demonstrate that\nthis joint training strategy improves significantly the representation ability\nof ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction\napproach has achieved state-of-the-art performance on real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:12:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lin", "Weihong", ""], ["Gao", "Qifang", ""], ["Sun", "Lei", ""], ["Zhong", "Zhuoyao", ""], ["Hu", "Kai", ""], ["Ren", "Qin", ""], ["Huo", "Qiang", ""]]}, {"id": "2105.11674", "submitter": "Andrea Baisero", "authors": "Andrea Baisero and Christopher Amato", "title": "Unbiased Asymmetric Actor-Critic for Partially Observable Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In partially observable reinforcement learning, offline training gives access\nto latent information which is not available during online training and/or\nexecution, such as the system state. Asymmetric actor-critic methods exploit\nsuch information by training a history-based policy via a state-based critic.\nHowever, many asymmetric methods lack theoretical foundation, and are only\nevaluated on limited domains. We examine the theory of asymmetric actor-critic\nmethods which use state-based critics, and expose fundamental issues which\nundermine the validity of a common variant, and its ability to address high\npartial observability. We propose an unbiased asymmetric actor-critic variant\nwhich is able to exploit state information while remaining theoretically sound,\nmaintaining the validity of the policy gradient theorem, and introducing no\nbias and relatively low variance into the training process. An empirical\nevaluation performed on domains which exhibit significant partial observability\nconfirms our analysis, and shows the unbiased asymmetric actor-critic converges\nto better policies and/or faster than symmetric actor-critic and standard\nasymmetric actor-critic baselines.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:18:44 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Baisero", "Andrea", ""], ["Amato", "Christopher", ""]]}, {"id": "2105.11675", "submitter": "Zhiwei Wang", "authors": "Tao Luo, Zheng Ma, Zhiwei Wang, Zhi-Qin John Xu, Yaoyu Zhang", "title": "An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural\n  Network", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.03238", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep neural network (DNN) usually learns the target function from low to high\nfrequency, which is called frequency principle or spectral bias. This frequency\nprinciple sheds light on a high-frequency curse of DNNs -- difficult to learn\nhigh-frequency information. Inspired by the frequency principle, a series of\nworks are devoted to develop algorithms for overcoming the high-frequency\ncurse. A natural question arises: what is the upper limit of the decaying rate\nw.r.t. frequency when one trains a DNN? In this work, our theory, confirmed by\nnumerical experiments, suggests that there is a critical decaying rate w.r.t.\nfrequency in DNN training. Below the upper limit of the decaying rate, the DNN\ninterpolates the training data by a function with a certain regularity.\nHowever, above the upper limit, the DNN interpolates the training data by a\ntrivial function, i.e., a function is only non-zero at training data points.\nOur results indicate a better way to overcome the high-frequency curse is to\ndesign a proper pre-condition approach to shift high-frequency information to\nlow-frequency one, which coincides with several previous developed algorithms\nfor fast learning high-frequency information. More importantly, this work\nrigorously proves that the high-frequency curse is an intrinsic difficulty of\nDNNs.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:27:11 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 07:40:24 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Luo", "Tao", ""], ["Ma", "Zheng", ""], ["Wang", "Zhiwei", ""], ["Xu", "Zhi-Qin John", ""], ["Zhang", "Yaoyu", ""]]}, {"id": "2105.11681", "submitter": "Daniela Noemi Rim", "authors": "Daniela N. Rim, Inseon Jang, Heeyoul Choi", "title": "Deep Neural Networks and End-to-End Learning for Audio Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent achievements in end-to-end deep learning have encouraged the\nexploration of tasks dealing with highly structured data with unified deep\nnetwork models. Having such models for compressing audio signals has been\nchallenging since it requires discrete representations that are not easy to\ntrain with end-to-end backpropagation. In this paper, we present an end-to-end\ndeep learning approach that combines recurrent neural networks (RNNs) within\nthe training strategy of variational autoencoders (VAEs) with a binary\nrepresentation of the latent space. We apply a reparametrization trick for the\nBernoulli distribution for the discrete representations, which allows smooth\nbackpropagation. In addition, our approach allows the separation of the encoder\nand decoder, which is necessary for compression tasks. To our best knowledge,\nthis is the first end-to-end learning for a single audio compression model with\nRNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:36:30 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 10:30:42 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Rim", "Daniela N.", ""], ["Jang", "Inseon", ""], ["Choi", "Heeyoul", ""]]}, {"id": "2105.11686", "submitter": "Hanxu Zhou", "authors": "Zhi-Qin John Xu, Hanxu Zhou, Tao Luo, Yaoyu Zhang", "title": "Towards Understanding the Condensation of Two-layer Neural Networks at\n  Initial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is important to study what implicit regularization is imposed on the loss\nfunction during the training that leads over-parameterized neural networks\n(NNs) to good performance on real dataset. Empirically, existing works have\nshown that weights of NNs condense on isolated orientations with small\ninitialization. The condensation implies that the NN learns features from the\ntraining data and is effectively a much smaller network. In this work, we show\nthat the singularity of the activation function at original point is a key\nfactor to understanding the condensation at initial training stage. Our\nexperiments suggest that the maximal number of condensed orientations is twice\nof the singularity order. Our theoretical analysis confirms experiments for two\ncases, one is for the first-order singularity activation function and the other\nis for the one-dimensional input. This work takes a step towards understanding\nhow small initialization implicitly leads NNs to condensation at initial\ntraining, which is crucial to understand the training and the learning of deep\nNNs.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:47:55 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 04:23:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Zhi-Qin John", ""], ["Zhou", "Hanxu", ""], ["Luo", "Tao", ""], ["Zhang", "Yaoyu", ""]]}, {"id": "2105.11689", "submitter": "Xiang Gao", "authors": "Xiang Gao, Wei Hu, Guo-Jun Qi", "title": "Self-Supervised Graph Representation Learning via Topology\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Topology Transformation Equivariant Representation learning, a\ngeneral paradigm of self-supervised learning for node representations of graph\ndata to enable the wide applicability of Graph Convolutional Neural Networks\n(GCNNs). We formalize the proposed model from an information-theoretic\nperspective, by maximizing the mutual information between topology\ntransformations and node representations before and after the transformations.\nWe derive that maximizing such mutual information can be relaxed to minimizing\nthe cross entropy between the applied topology transformation and its\nestimation from node representations. In particular, we seek to sample a subset\nof node pairs from the original graph and flip the edge connectivity between\neach pair to transform the graph topology. Then, we self-train a representation\nencoder to learn node representations by reconstructing the topology\ntransformations from the feature representations of the original and\ntransformed graphs. In experiments, we apply the proposed model to the\ndownstream node and graph classification tasks, and results show that the\nproposed method outperforms the state-of-the-art unsupervised approaches.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 06:11:03 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Gao", "Xiang", ""], ["Hu", "Wei", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "2105.11692", "submitter": "Liyue Shen", "authors": "Liyue Shen, Wei Zhao, Dante Capaldi, John Pauly, Lei Xing", "title": "A Geometry-Informed Deep Learning Framework for Ultra-Sparse 3D\n  Tomographic Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning affords enormous opportunities to augment the armamentarium of\nbiomedical imaging, albeit its design and implementation have potential flaws.\nFundamentally, most deep learning models are driven entirely by data without\nconsideration of any prior knowledge, which dramatically increases the\ncomplexity of neural networks and limits the application scope and model\ngeneralizability. Here we establish a geometry-informed deep learning framework\nfor ultra-sparse 3D tomographic image reconstruction. We introduce a novel\nmechanism for integrating geometric priors of the imaging system. We\ndemonstrate that the seamless inclusion of known priors is essential to enhance\nthe performance of 3D volumetric computed tomography imaging with ultra-sparse\nsampling. The study opens new avenues for data-driven biomedical imaging and\npromises to provide substantially improved imaging tools for various clinical\nimaging and image-guided interventions.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 06:20:03 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shen", "Liyue", ""], ["Zhao", "Wei", ""], ["Capaldi", "Dante", ""], ["Pauly", "John", ""], ["Xing", "Lei", ""]]}, {"id": "2105.11694", "submitter": "Jihao Liu", "authors": "Jihao Liu and Ming Zhang and Yangting Sun and Boxiao Liu and Guanglu\n  Song and Yu Liu and Hongsheng Li", "title": "FNAS: Uncertainty-Aware Fast Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL)-based neural architecture search (NAS) generally\nguarantees better convergence yet suffers from the requirement of huge\ncomputational resources compared with gradient-based approaches, due to the\nrollout bottleneck -- exhaustive training for each sampled generation on proxy\ntasks. In this paper, we propose a general pipeline to accelerate the\nconvergence of the rollout process as well as the RL process in NAS. It is\nmotivated by the interesting observation that both the architecture and the\nparameter knowledge can be transferred between different experiments and even\ndifferent tasks. We first introduce an uncertainty-aware critic (value\nfunction) in Proximal Policy Optimization (PPO) to utilize the architecture\nknowledge in previous experiments, which stabilizes the training process and\nreduces the searching time by 4 times. Further, an architecture knowledge pool\ntogether with a block similarity function is proposed to utilize parameter\nknowledge and reduces the searching time by 2 times. It is the first to\nintroduce block-level weight sharing in RLbased NAS. The block similarity\nfunction guarantees a 100% hitting ratio with strict fairness. Besides, we show\nthat a simply designed off-policy correction factor used in \"replay buffer\" in\nRL optimization can further reduce half of the searching time. Experiments on\nthe Mobile Neural Architecture Search (MNAS) search space show the proposed\nFast Neural Architecture Search (FNAS) accelerates standard RL-based NAS\nprocess by ~10x (e.g. ~256 2x2 TPUv2 x days / 20,000 GPU x hour -> 2,000 GPU x\nhour for MNAS), and guarantees better performance on various vision tasks.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 06:32:52 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 10:36:28 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 07:53:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Liu", "Jihao", ""], ["Zhang", "Ming", ""], ["Sun", "Yangting", ""], ["Liu", "Boxiao", ""], ["Song", "Guanglu", ""], ["Liu", "Yu", ""], ["Li", "Hongsheng", ""]]}, {"id": "2105.11697", "submitter": "Pietro Barbiero", "authors": "Pietro Barbiero, Gabriele Ciravegna, Dobrik Georgiev, Franscesco\n  Giannini", "title": "PyTorch, Explain! A Python library for Logic Explained Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \"PyTorch, Explain!\" is a Python module integrating a variety of\nstate-of-the-art approaches to provide logic explanations from neural networks.\nThis package focuses on bringing these methods to non-specialists. It has\nminimal dependencies and it is distributed under the Apache 2.0 licence\nallowing both academic and commercial use. Source code and documentation can be\ndownloaded from the github repository:\nhttps://github.com/pietrobarbiero/pytorch_explain.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 06:41:54 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 11:22:28 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Barbiero", "Pietro", ""], ["Ciravegna", "Gabriele", ""], ["Georgiev", "Dobrik", ""], ["Giannini", "Franscesco", ""]]}, {"id": "2105.11702", "submitter": "Zhao Yang", "authors": "Zhao Yang, Mike Preuss, Aske Plaat", "title": "Transfer Learning and Curriculum Learning in Sokoban", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning can speed up training in machine learning and is regularly\nused in classification tasks. It reuses prior knowledge from other tasks to\npre-train networks for new tasks. In reinforcement learning, learning actions\nfor a behavior policy that can be applied to new environments is still a\nchallenge, especially for tasks that involve much planning. Sokoban is a\nchallenging puzzle game. It has been used widely as a benchmark in\nplanning-based reinforcement learning. In this paper, we show how prior\nknowledge improves learning in Sokoban tasks. We find that reusing feature\nrepresentations learned previously can accelerate learning new, more complex,\ninstances. In effect, we show how curriculum learning, from simple to complex\ntasks, works in Sokoban. Furthermore, feature representations learned in\nsimpler instances are more general, and thus lead to positive transfers towards\nmore complex tasks, but not vice versa. We have also studied which part of the\nknowledge is most important for transfer to succeed, and identify which layers\nshould be used for pre-training.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:01:32 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yang", "Zhao", ""], ["Preuss", "Mike", ""], ["Plaat", "Aske", ""]]}, {"id": "2105.11706", "submitter": "Elham Abbasi", "authors": "Laleh Armi, Elham Abbasi, Jamal Zarepour-Ahmadabadi", "title": "Mixture of ELM based experts with trainable gating network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixture of experts method is a neural network based ensemble learning that\nhas great ability to improve the overall classification accuracy. This method\nis based on the divide and conquer principle, in which the problem space is\ndivided between several experts by supervisition of gating network. In this\npaper, we propose an ensemble learning method based on mixture of experts which\nis named mixture of ELM based experts with trainable gating network (MEETG) to\nimprove the computing cost and to speed up the learning process of ME. The\nstructure of ME consists of multi layer perceptrons (MLPs) as base experts and\ngating network, in which gradient-based learning algorithm is applied for\ntraining the MLPs which is an iterative and time consuming process. In order to\novercome on these problems, we use the advantages of extreme learning machine\n(ELM) for designing the structure of ME. ELM as a learning algorithm for single\nhidden-layer feed forward neural networks provides much faster learning process\nand better generalization ability in comparision with some other traditional\nlearning algorithms. Also, in the proposed method a trainable gating network is\napplied to aggregate the outputs of the experts dynamically according to the\ninput sample. Our experimental results and statistical analysis on 11 benchmark\ndatasets confirm that MEETG has an acceptable performance in classification\nproblems. Furthermore, our experimental results show that the proposed approach\noutperforms the original ELM on prediction stability and classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:13:35 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Armi", "Laleh", ""], ["Abbasi", "Elham", ""], ["Zarepour-Ahmadabadi", "Jamal", ""]]}, {"id": "2105.11724", "submitter": "Clement Benard", "authors": "Cl\\'ement B\\'enard (LPSM), G\\'erard Biau (LPSM), S\\'ebastien da Veiga,\n  Erwan Scornet (CMAP)", "title": "SHAFF: Fast and consistent SHApley eFfect estimates via random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability of learning algorithms is crucial for applications involving\ncritical decisions, and variable importance is one of the main interpretation\ntools. Shapley effects are now widely used to interpret both tree ensembles and\nneural networks, as they can efficiently handle dependence and interactions in\nthe data, as opposed to most other variable importance measures. However,\nestimating Shapley effects is a challenging task, because of the computational\ncomplexity and the conditional expectation estimates. Accordingly, existing\nShapley algorithms have flaws: a costly running time, or a bias when input\nvariables are dependent. Therefore, we introduce SHAFF, SHApley eFfects via\nrandom Forests, a fast and accurate Shapley effect estimate, even when input\nvariables are dependent. We show SHAFF efficiency through both a theoretical\nanalysis of its consistency, and the practical performance improvements over\ncompetitors with extensive experiments. An implementation of SHAFF in C++ and R\nis available online.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:48:07 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["B\u00e9nard", "Cl\u00e9ment", "", "LPSM"], ["Biau", "G\u00e9rard", "", "LPSM"], ["da Veiga", "S\u00e9bastien", "", "CMAP"], ["Scornet", "Erwan", "", "CMAP"]]}, {"id": "2105.11726", "submitter": "Michael Schmitt", "authors": "Michael Schmitt, Seyed Ali Ahmadi, Ronny H\\\"ansch", "title": "There is no data like more data -- current status of machine learning\n  datasets in remote sensing", "comments": "accepted for the Proceedings of IEEE International Geoscience and\n  Remote Sensing Symposium (IGARSS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotated datasets have become one of the most crucial preconditions for the\ndevelopment and evaluation of machine learning-based methods designed for the\nautomated interpretation of remote sensing data. In this paper, we review the\nhistoric development of such datasets, discuss their features based on a few\nselected examples, and address open issues for future developments.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:49:21 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 11:49:59 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Schmitt", "Michael", ""], ["Ahmadi", "Seyed Ali", ""], ["H\u00e4nsch", "Ronny", ""]]}, {"id": "2105.11728", "submitter": "Md Sahidullah", "authors": "Nirmalya Sen, Md Sahidullah (MULTISPEECH), Hemant Patil (DA-IICT),\n  Shyamal Kumar das Mandal (IIT Kharagpur), Sreenivasa Krothapalli Rao (IIT\n  Kharagpur), Tapan Kumar Basu (IIT Kharagpur)", "title": "Utterance partitioning for speaker recognition: an experimental review\n  and analysis with new findings under GMM-SVM framework", "comments": "International Journal of Speech Technology, Springer Verlag, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of speaker recognition system is highly dependent on the\namount of speech used in enrollment and test. This work presents a detailed\nexperimental review and analysis of the GMM-SVM based speaker recognition\nsystem in presence of duration variability. This article also reports a\ncomparison of the performance of GMM-SVM classifier with its precursor\ntechnique Gaussian mixture model-universal background model (GMM-UBM)\nclassifier in presence of duration variability. The goal of this research work\nis not to propose a new algorithm for improving speaker recognition performance\nin presence of duration variability. However, the main focus of this work is on\nutterance partitioning (UP), a commonly used strategy to compensate the\nduration variability issue. We have analysed in detailed the impact of training\nutterance partitioning in speaker recognition performance under GMM-SVM\nframework. We further investigate the reason why the utterance partitioning is\nimportant for boosting speaker recognition performance. We have also shown in\nwhich case the utterance partitioning could be useful and where not. Our study\nhas revealed that utterance partitioning does not reduce the data imbalance\nproblem of the GMM-SVM classifier as claimed in earlier study. Apart from\nthese, we also discuss issues related to the impact of parameters such as\nnumber of Gaussians, supervector length, amount of splitting required for\nobtaining better performance in short and long duration test conditions from\nspeech duration perspective. We have performed the experiments with telephone\nspeech from POLYCOST corpus consisting of 130 speakers.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:50:09 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sen", "Nirmalya", "", "MULTISPEECH"], ["Sahidullah", "Md", "", "MULTISPEECH"], ["Patil", "Hemant", "", "DA-IICT"], ["Mandal", "Shyamal Kumar das", "", "IIT Kharagpur"], ["Rao", "Sreenivasa Krothapalli", "", "IIT\n  Kharagpur"], ["Basu", "Tapan Kumar", "", "IIT Kharagpur"]]}, {"id": "2105.11730", "submitter": "Jinyang Liu", "authors": "Jinyang Liu, Sheng Di, Kai Zhao, Sian Jin, Dingwen Tao, Xin Liang,\n  Zizhong Chen, Franck Cappello", "title": "Exploring Autoencoder-based Error-bounded Compression for Scientific\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error-bounded lossy compression is becoming an indispensable technique for\nthe success of today's scientific projects with vast volumes of data produced\nduring the simulations or instrument data acquisitions. Not only can it\nsignificantly reduce data size, but it also can control the compression errors\nbased on user-specified error bounds. Autoencoder (AE) models have been widely\nused in image compression, but few AE-based compression approaches support\nerror-bounding features, which are highly required by scientific applications.\nTo address this issue, we explore using convolutional autoencoders to improve\nerror-bounded lossy compression for scientific data, with the following three\nkey contributions. (1) We provide an in-depth investigation of the\ncharacteristics of various autoencoder models and develop an error-bounded\nautoencoder-based framework in terms of the SZ model. (2) We optimize the\ncompression quality for main stages in our designed AE-based error-bounded\ncompression framework, fine-tuning the block sizes and latent sizes and also\noptimizing the compression efficiency of latent vectors. (3) We evaluate our\nproposed solution using five real-world scientific datasets and comparing them\nwith six other related works. Experiments show that our solution exhibits a\nvery competitive compression quality from among all the compressors in our\ntests. In absolute terms, it can obtain a much better compression quality (100%\n~ 800% improvement in compression ratio with the same data distortion) compared\nwith SZ2.1 and ZFP in cases with a high compression ratio.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:53:32 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 22:15:29 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 03:07:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Liu", "Jinyang", ""], ["Di", "Sheng", ""], ["Zhao", "Kai", ""], ["Jin", "Sian", ""], ["Tao", "Dingwen", ""], ["Liang", "Xin", ""], ["Chen", "Zizhong", ""], ["Cappello", "Franck", ""]]}, {"id": "2105.11732", "submitter": "Gersende Fort", "authors": "Gersende Fort (IMT), Eric Moulines (X-DEP-MATHAPP, XPOP)", "title": "The Perturbed Prox-Preconditioned SPIDER algorithm for EM-based large\n  scale learning", "comments": null, "journal-ref": "IEEE Statistical Signal Processing Workshop, Jul 2021, Rio de\n  Janeiro, Brazil", "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental Expectation Maximization (EM) algorithms were introduced to\ndesign EM for the large scale learning framework by avoiding the full data set\nto be processed at each iteration. Nevertheless, these algorithms all assume\nthat the conditional expectations of the sufficient statistics are explicit. In\nthis paper, we propose a novel algorithm named Perturbed Prox-Preconditioned\nSPIDER (3P-SPIDER), which builds on the Stochastic Path Integral Differential\nEstimatoR EM (SPIDER-EM) algorithm. The 3P-SPIDER algorithm addresses many\nintractabilities of the E-step of EM; it also deals with non-smooth\nregularization and convex constraint set. Numerical experiments show that\n3P-SPIDER outperforms other incremental EM methods and discuss the role of some\ndesign parameters.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:54:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Fort", "Gersende", "", "IMT"], ["Moulines", "Eric", "", "X-DEP-MATHAPP, XPOP"]]}, {"id": "2105.11739", "submitter": "Anton Mallasto", "authors": "Anton Mallasto, Karol Arndt, Markus Heinonen, Samuel Kaski, Ville\n  Kyrki", "title": "Affine Transport for Sim-to-Real Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample-efficient domain adaptation is an open problem in robotics. In this\npaper, we present affine transport -- a variant of optimal transport, which\nmodels the mapping between state transition distributions between the source\nand target domains with an affine transformation. First, we derive the affine\ntransport framework; then, we extend the basic framework with Procrustes\nalignment to model arbitrary affine transformations. We evaluate the method in\na number of OpenAI Gym sim-to-sim experiments with simulation environments, as\nwell as on a sim-to-real domain adaptation task of a robot hitting a hockeypuck\nsuch that it slides and stops at a target position. In each experiment, we\nevaluate the results when transferring between each pair of dynamics domains.\nThe results show that affine transport can significantly reduce the model\nadaptation error in comparison to using the original, non-adapted dynamics\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 08:05:06 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Mallasto", "Anton", ""], ["Arndt", "Karol", ""], ["Heinonen", "Markus", ""], ["Kaski", "Samuel", ""], ["Kyrki", "Ville", ""]]}, {"id": "2105.11748", "submitter": "Weiyi Xie", "authors": "Weiyi Xie, Colin Jacobs, Bram van Ginneken", "title": "Dense Regression Activation Maps For Lesion Segmentation in CT scans of\n  COVID-19 patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic lesion segmentation on thoracic CT enables rapid quantitative\nanalysis of lung involvement in COVID- 19 infections. Obtaining voxel-level\nannotations for training segmentation networks is prohibitively expensive.\nTherefore we propose a weakly-supervised segmentation method based on dense\nregression activation maps (dRAM). Most advanced weakly supervised segmentation\napproaches exploit class activation maps (CAMs) to localize objects generated\nfrom high-level semantic features at a coarse resolution. As a result, CAMs\nprovide coarse outlines that do not align precisely with the object\nsegmentations. Instead, we exploit dense features from a segmentation network\nto compute dense regression activation maps (dRAMs) for preserving local\ndetails. During training, dRAMs are pooled lobe-wise to regress the per-lobe\nlesion percentage. In such a way, the network achieves additional information\nregarding the lesion quantification in comparison with the classification\napproach. Furthermore, we refine dRAMs based on an attention module and dense\nconditional random field trained together with the main regression task. The\nrefined dRAMs are served as the pseudo labels for training a final segmentation\nnetwork. When evaluated on 69 CT scans, our method substantially improves the\nintersection over union from 0.335 in the CAM-based weakly supervised\nsegmentation method to 0.495.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 08:29:35 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Xie", "Weiyi", ""], ["Jacobs", "Colin", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2105.11765", "submitter": "Ann-Katrin Thebille", "authors": "Ann-Katrin Thebille and Esther Dietrich and Martin Klaus and Lukas\n  Gernhold and Maximilian Lennartz and Christoph Kuppe and Rafael Kramann and\n  Tobias B. Huber and Guido Sauter and Victor G. Puelles and Marina Zimmermann\n  and Stefan Bonn", "title": "Deep learning-based bias transfer for overcoming laboratory differences\n  of microscopic images", "comments": "Accepted as a regular conference paper at MIUA 2021", "journal-ref": null, "doi": "10.1007/978-3-030-80432-9_25", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The automated analysis of medical images is currently limited by technical\nand biological noise and bias. The same source tissue can be represented by\nvastly different images if the image acquisition or processing protocols vary.\nFor an image analysis pipeline, it is crucial to compensate such biases to\navoid misinterpretations. Here, we evaluate, compare, and improve existing\ngenerative model architectures to overcome domain shifts for immunofluorescence\n(IF) and Hematoxylin and Eosin (H&E) stained microscopy images. To determine\nthe performance of the generative models, the original and transformed images\nwere segmented or classified by deep neural networks that were trained only on\nimages of the target bias. In the scope of our analysis, U-Net cycleGANs\ntrained with an additional identity and an MS-SSIM-based loss and Fixed-Point\nGANs trained with an additional structure loss led to the best results for the\nIF and H&E stained samples, respectively. Adapting the bias of the samples\nsignificantly improved the pixel-level segmentation for human kidney glomeruli\nand podocytes and improved the classification accuracy for human prostate\nbiopsies by up to 14%.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:02:30 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Thebille", "Ann-Katrin", ""], ["Dietrich", "Esther", ""], ["Klaus", "Martin", ""], ["Gernhold", "Lukas", ""], ["Lennartz", "Maximilian", ""], ["Kuppe", "Christoph", ""], ["Kramann", "Rafael", ""], ["Huber", "Tobias B.", ""], ["Sauter", "Guido", ""], ["Puelles", "Victor G.", ""], ["Zimmermann", "Marina", ""], ["Bonn", "Stefan", ""]]}, {"id": "2105.11781", "submitter": "Xiangzhu Meng", "authors": "Xiangzhu Meng, Lin Feng, Chonghui Guo", "title": "A unified framework based on graph consensus term for multi-view\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recent years, multi-view learning technologies for various applications\nhave attracted a surge of interest. Due to more compatible and complementary\ninformation from multiple views, existing multi-view methods could achieve more\npromising performance than conventional single-view methods in most situations.\nHowever, there are still no sufficient researches on the unified framework in\nexisting multi-view works. Meanwhile, how to efficiently integrate multi-view\ninformation is still full of challenges. In this paper, we propose a novel\nmulti-view learning framework, which aims to leverage most existing graph\nembedding works into a unified formula via introducing the graph consensus\nterm. In particular, our method explores the graph structure in each view\nindependently to preserve the diversity property of graph embedding methods.\nMeanwhile, we choose heterogeneous graphs to construct the graph consensus term\nto explore the correlations among multiple views jointly. To this end, the\ndiversity and complementary information among different views could be\nsimultaneously considered. Furthermore, the proposed framework is utilized to\nimplement the multi-view extension of Locality Linear Embedding, named\nMulti-view Locality Linear Embedding (MvLLE), which could be efficiently solved\nby applying the alternating optimization strategy. Empirical validations\nconducted on six benchmark datasets can show the effectiveness of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:22:21 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 14:30:26 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Meng", "Xiangzhu", ""], ["Feng", "Lin", ""], ["Guo", "Chonghui", ""]]}, {"id": "2105.11798", "submitter": "Carlos Basto", "authors": "Carlos Basto", "title": "Extending the Abstraction of Personality Types based on MBTI with\n  Machine Learning and Natural Language Processing", "comments": "23 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A data-centric approach with Natural Language Processing (NLP) to predict\npersonality types based on the MBTI (an introspective self-assessment\nquestionnaire that indicates different psychological preferences about how\npeople perceive the world and make decisions) through systematic enrichment of\ntext representation, based on the domain of the area, under the generation of\nfeatures based on three types of analysis: sentimental, grammatical and\naspects. The experimentation had a robust baseline of stacked models, with\npremature optimization of hyperparameters through grid search, with gradual\nfeedback, for each of the four classifiers (dichotomies) of MBTI. The results\nshowed that attention to the data iteration loop focused on quality,\nexplanatory power and representativeness for the abstraction of more\nrelevant/important resources for the studied phenomenon made it possible to\nimprove the evaluation metrics results more quickly and less costly than\ncomplex models such as the LSTM or state of the art ones as BERT, as well as\nthe importance of these results by comparisons made from various perspectives.\nIn addition, the study demonstrated a broad spectrum for the evolution and\ndeepening of the task and possible approaches for a greater extension of the\nabstraction of personality types.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:00:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Basto", "Carlos", ""]]}, {"id": "2105.11802", "submitter": "Johannes Kirschner", "authors": "Johannes Kirschner and Andreas Krause", "title": "Bias-Robust Bayesian Optimization via Dueling Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian optimization in settings where observations can be\nadversarially biased, for example by an uncontrolled hidden confounder. Our\nfirst contribution is a reduction of the confounded setting to the dueling\nbandit model. Then we propose a novel approach for dueling bandits based on\ninformation-directed sampling (IDS). Thereby, we obtain the first efficient\nkernelized algorithm for dueling bandits that comes with cumulative regret\nguarantees. Our analysis further generalizes a previously proposed\nsemi-parametric linear bandit model to non-linear reward functions, and\nuncovers interesting links to doubly-robust estimation.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:08:41 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 07:04:02 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Kirschner", "Johannes", ""], ["Krause", "Andreas", ""]]}, {"id": "2105.11804", "submitter": "Etienne Bennequin", "authors": "Etienne Bennequin, Victor Bouvier, Myriam Tami, Antoine Toubhans,\n  C\\'eline Hudelot", "title": "Bridging Few-Shot Learning and Adaptation: New Challenges of\n  Support-Query Shift", "comments": "Preprint. Under review at ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Few-Shot Learning (FSL) algorithms have made substantial progress in learning\nnovel concepts with just a handful of labelled data. To classify query\ninstances from novel classes encountered at test-time, they only require a\nsupport set composed of a few labelled samples. FSL benchmarks commonly assume\nthat those queries come from the same distribution as instances in the support\nset. However, in a realistic set-ting, data distribution is plausibly subject\nto change, a situation referred to as Distribution Shift (DS). The present work\naddresses the new and challenging problem of Few-Shot Learning under\nSupport/Query Shift (FSQS) i.e., when support and query instances are sampled\nfrom related but different distributions. Our contributions are the following.\nFirst, we release a testbed for FSQS, including datasets, relevant baselines\nand a protocol for a rigorous and reproducible evaluation. Second, we observe\nthat well-established FSL algorithms unsurprisingly suffer from a considerable\ndrop in accuracy when facing FSQS, stressing the significance of our study.\nFinally, we show that transductive algorithms can limit the inopportune effect\nof DS. In particular, we study both the role of Batch-Normalization and Optimal\nTransport (OT) in aligning distributions, bridging Unsupervised Domain\nAdaptation with FSL. This results in a new method that efficiently combines OT\nwith the celebrated Prototypical Networks. We bring compelling experiments\ndemonstrating the advantage of our method. Our work opens an exciting line of\nresearch by providing a testbed and strong baselines. Our code is available at\nhttps://github.com/ebennequin/meta-domain-shift.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:10:09 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Bennequin", "Etienne", ""], ["Bouvier", "Victor", ""], ["Tami", "Myriam", ""], ["Toubhans", "Antoine", ""], ["Hudelot", "C\u00e9line", ""]]}, {"id": "2105.11812", "submitter": "Firas Jarboui", "authors": "Firas Jarboui, Vianney Perchet", "title": "A Generalised Inverse Reinforcement Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gloabal objective of inverse Reinforcement Learning (IRL) is to estimate\nthe unknown cost function of some MDP base on observed trajectories generated\nby (approximate) optimal policies. The classical approach consists in tuning\nthis cost function so that associated optimal trajectories (that minimise the\ncumulative discounted cost, i.e. the classical RL loss) are 'similar' to the\nobserved ones. Prior contributions focused on penalising degenerate solutions\nand improving algorithmic scalability. Quite orthogonally to them, we question\nthe pertinence of characterising optimality with respect to the cumulative\ndiscounted cost as it induces an implicit bias against policies with longer\nmixing times. State of the art value based RL algorithms circumvent this issue\nby solving for the fixed point of the Bellman optimality operator, a stronger\ncriterion that is not well defined for the inverse problem. To alleviate this\nbias in IRL, we introduce an alternative training loss that puts more weights\non future states which yields a reformulation of the (maximum entropy) IRL\nproblem. The algorithms we devised exhibit enhanced performances (and similar\ntractability) than off-the-shelf ones in multiple OpenAI gym environments.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:30:45 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Jarboui", "Firas", ""], ["Perchet", "Vianney", ""]]}, {"id": "2105.11813", "submitter": "Constantine Doumanidis", "authors": "Constantine C. Doumanidis (1), Christina Anagnostou (1),\n  Evangelia-Sofia Arvaniti (1), Anthi Papadopoulou (1) ((1) Aristotle\n  University of Thessaloniki)", "title": "RNNoise-Ex: Hybrid Speech Enhancement System based on RNN and Spectral\n  Features", "comments": "6 pages, 5 figures, presented at ECESCON 12, for code see\n  https://github.com/CedArctic/rnnoise-ex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent interest in exploiting Deep Learning techniques for Noise Suppression,\nhas led to the creation of Hybrid Denoising Systems that combine classic Signal\nProcessing with Deep Learning. In this paper, we concentrated our efforts on\nextending the RNNoise denoising system (arXiv:1709.08243) with the inclusion of\ncomplementary features during the training phase. We present a comprehensive\nexplanation of the set-up process of a modified system and present the\ncomparative results derived from a performance evaluation analysis, using a\nreference version of RNNoise as control.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:32:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Doumanidis", "Constantine C.", ""], ["Anagnostou", "Christina", ""], ["Arvaniti", "Evangelia-Sofia", ""], ["Papadopoulou", "Anthi", ""]]}, {"id": "2105.11816", "submitter": "Ozioma Paul Miss", "authors": "Ozioma Paul and Patrick McSharry", "title": "Public Transportation Demand Analysis: A Case Study of Metropolitan\n  Lagos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modelling, simulation, and forecasting offer a means of facilitating better\nplanning and decision-making. These quantitative approaches can add value\nbeyond traditional methods that do not rely on data and are particularly\nrelevant for public transportation. Lagos is experiencing rapid urbanization\nand currently has a population of just under 15 million. Both long waiting\ntimes and uncertain travel times has driven many people to acquire their own\nvehicle or use alternative modes of transport. This has significantly increased\nthe number of vehicles on the roads leading to even more traffic and greater\ntraffic congestion. This paper investigates urban travel demand in Lagos and\nexplores passenger dynamics in time and space. Using individual commuter trip\ndata from tickets purchased from the Lagos State Bus Rapid Transit (BRT), the\ndemand patterns through the hours of the day, days of the week and bus stations\nare analysed. This study aims to quantify demand from actual passenger trips\nand estimate the impact that dynamic scheduling could have on passenger waiting\ntimes. Station segmentation is provided to cluster stations by their demand\ncharacteristics in order to tailor specific bus schedules. Intra-day public\ntransportation demand in Lagos BRT is analysed and predictions are compared.\nSimulations using fixed and dynamic bus scheduling demonstrate that the average\nwaiting time could be reduced by as much as 80%. The load curves, insights and\nthe approach developed will be useful for informing policymaking in Lagos and\nsimilar African cities facing the challenges of rapid urbanization.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:35:30 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Paul", "Ozioma", ""], ["McSharry", "Patrick", ""]]}, {"id": "2105.11818", "submitter": "Remi Leluc", "authors": "R\\'emi Leluc and Fran\\c{c}ois Portier", "title": "SGD with Coordinate Sampling: Theory and Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While classical forms of stochastic gradient descent algorithm treat the\ndifferent coordinates in the same way, a framework allowing for adaptive (non\nuniform) coordinate sampling is developed to leverage structure in data. In a\nnon-convex setting and including zeroth order gradient estimate, almost sure\nconvergence as well as non-asymptotic bounds are established. Within the\nproposed framework, we develop an algorithm, MUSKETEER, based on a\nreinforcement strategy: after collecting information on the noisy gradients, it\nsamples the most promising coordinate (all for one); then it moves along the\none direction yielding an important decrease of the objective (one for all).\nNumerical experiments on both synthetic and real data examples confirm the\neffectiveness of MUSKETEER in large scale problems.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:37:50 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Leluc", "R\u00e9mi", ""], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "2105.11826", "submitter": "Yunshan Ma", "authors": "Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong, Tat-Seng\n  Chua, Jinyoung Moon, Hong-Han Shuai", "title": "Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend\n  Forecasting", "comments": null, "journal-ref": "ICMR 2021", "doi": "10.1145/3460426.3463598", "report-no": null, "categories": "cs.LG cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This companion paper supports the replication of the fashion trend\nforecasting experiments with the KERN (Knowledge Enhanced Recurrent Network)\nmethod that we presented in the ICMR 2020. We provide an artifact that allows\nthe replication of the experiments using a Python implementation. The artifact\nis easy to deploy with simple installation, training and evaluation. We\nreproduce the experiments conducted in the original paper and obtain similar\nperformance as previously reported. The replication results of the experiments\nsupport the main claims in the original paper.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:53:11 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ma", "Yunshan", ""], ["Ding", "Yujuan", ""], ["Yang", "Xun", ""], ["Liao", "Lizi", ""], ["Wong", "Wai Keung", ""], ["Chua", "Tat-Seng", ""], ["Moon", "Jinyoung", ""], ["Shuai", "Hong-Han", ""]]}, {"id": "2105.11836", "submitter": "Cyrus Vahidi", "authors": "Cyrus Vahidi, Charalampos Saitis, Gy\\\"orgy Fazekas", "title": "A Modulation Front-End for Music Audio Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have been extensively explored in the task of\nautomatic music tagging. The problem can be approached by using either\nengineered time-frequency features or raw audio as input. Modulation filter\nbank representations that have been actively researched as a basis for timbre\nperception have the potential to facilitate the extraction of perceptually\nsalient features. We explore end-to-end learned front-ends for audio\nrepresentation learning, ModNet and SincModNet, that incorporate a temporal\nmodulation processing block. The structure is effectively analogous to a\nmodulation filter bank, where the FIR filter center frequencies are learned in\na data-driven manner. The expectation is that a perceptually motivated filter\nbank can provide a useful representation for identifying music features. Our\nexperimental results provide a fully visualisable and interpretable front-end\ntemporal modulation decomposition of raw audio. We evaluate the performance of\nour model against the state-of-the-art of music tagging on the MagnaTagATune\ndataset. We analyse the impact on performance for particular tags when\ntime-frequency bands are subsampled by the modulation filters at a\nprogressively reduced rate. We demonstrate that modulation filtering provides\npromising results for music tagging and feature representation, without using\nextensive musical domain knowledge in the design of this front-end.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:05:24 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Vahidi", "Cyrus", ""], ["Saitis", "Charalampos", ""], ["Fazekas", "Gy\u00f6rgy", ""]]}, {"id": "2105.11839", "submitter": "Lars Lorch", "authors": "Lars Lorch, Jonas Rothfuss, Bernhard Sch\\\"olkopf, Andreas Krause", "title": "DiBS: Differentiable Bayesian Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian structure learning allows inferring Bayesian network structure from\ndata while reasoning about the epistemic uncertainty -- a key element towards\nenabling active causal discovery and designing interventions in real world\nsystems. In this work, we propose a general, fully differentiable framework for\nBayesian structure learning (DiBS) that operates in the continuous space of a\nlatent probabilistic graph representation. Building on recent advances in\nvariational inference, we use DiBS to devise an efficient method for\napproximating posteriors over structural models. Contrary to existing work,\nDiBS is agnostic to the form of the local conditional distributions and allows\nfor joint posterior inference of both the graph structure and the conditional\ndistribution parameters. This makes our method directly applicable to posterior\ninference of nonstandard Bayesian network models, e.g., with nonlinear\ndependencies encoded by neural networks. In evaluations on simulated and\nreal-world data, DiBS significantly outperforms related approaches to joint\nposterior inference.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:23:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lorch", "Lars", ""], ["Rothfuss", "Jonas", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Krause", "Andreas", ""]]}, {"id": "2105.11844", "submitter": "Francisco P\\'erez-Hern\\'andez", "authors": "P\\'erez-Hern\\'andez Francisco, Rodr\\'iguez-Ortega Jos\\'e, Benhammou\n  Yassir, Herrera Francisco, Tabik Siham", "title": "Small and large scale critical infrastructures detection based on deep\n  learning using high resolution orthogonal images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of critical infrastructures is of high importance in several\nfields such as security, anomaly detection, land use planning and land use\nchange detection. However, critical infrastructures detection in aerial and\nsatellite images is still a challenge as each one has completely different size\nand requires different spacial resolution to be identified correctly.\nHeretofore, there are no special datasets for training critical infrastructures\ndetectors. This paper presents a smart dataset as well as a\nresolution-independent critical infrastructure detection system. In particular,\nguided by the performance of the detection model, we built a dataset organized\ninto two scales, small and large scale, and designed a two-stage deep learning\ndetection of different scale critical infrastructures (DetDSCI) methodology in\northo-images. DetDSCI methodology first determines the input image zoom level\nusing a classification model, then analyses the input image with the\nappropriate scale detection model. Our experiments show that DetDSCI\nmethodology achieves up to 37,53% F1 improvement with respect to the baseline\ndetector.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:38:15 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Francisco", "P\u00e9rez-Hern\u00e1ndez", ""], ["Jos\u00e9", "Rodr\u00edguez-Ortega", ""], ["Yassir", "Benhammou", ""], ["Francisco", "Herrera", ""], ["Siham", "Tabik", ""]]}, {"id": "2105.11852", "submitter": "Cheikh Brahim El Vaigh", "authors": "Cheikh Brahim El Vaigh, Noa Garcia, Benjamin Renoust, Chenhui Chu,\n  Yuta Nakashima and Hajime Nagahara", "title": "GCNBoost: Artwork Classification by Label Propagation through a\n  Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise of digitization of cultural documents offers large-scale contents,\nopening the road for development of AI systems in order to preserve, search,\nand deliver cultural heritage. To organize such cultural content also means to\nclassify them, a task that is very familiar to modern computer science.\nContextual information is often the key to structure such real world data, and\nwe propose to use it in form of a knowledge graph. Such a knowledge graph,\ncombined with content analysis, enhances the notion of proximity between\nartworks so it improves the performances in classification tasks. In this\npaper, we propose a novel use of a knowledge graph, that is constructed on\nannotated data and pseudo-labeled data. With label propagation, we boost\nartwork classification by training a model using a graph convolutional network,\nrelying on the relationships between entities of the knowledge graph. Following\na transductive learning framework, our experiments show that relying on a\nknowledge graph modeling the relations between labeled data and unlabeled data\nallows to achieve state-of-the-art results on multiple classification tasks on\na dataset of paintings, and on a dataset of Buddha statues. Additionally, we\nshow state-of-the-art results for the difficult case of dealing with unbalanced\ndata, with the limitation of disregarding classes with extremely low degrees in\nthe knowledge graph.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:50:05 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Vaigh", "Cheikh Brahim El", ""], ["Garcia", "Noa", ""], ["Renoust", "Benjamin", ""], ["Chu", "Chenhui", ""], ["Nakashima", "Yuta", ""], ["Nagahara", "Hajime", ""]]}, {"id": "2105.11853", "submitter": "Nam Nguyen", "authors": "Nam Nguyen and Kwang-Chen Chen", "title": "Quantum Embedding Search for Quantum Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a novel quantum embedding search algorithm (QES,\npronounced as \"quest\"), enabling search for optimal quantum embedding design\nfor a specific dataset of interest. First, we establish the connection between\nthe structures of quantum embedding and the representations of directed\nmulti-graphs, enabling a well-defined search space. Second, we instigate the\nentanglement level to reduce the cardinality of the search space to a feasible\nsize for practical implementations. Finally, we mitigate the cost of evaluating\nthe true loss function by using surrogate models via sequential model-based\noptimization. We demonstrate the feasibility of our proposed approach on\nsynthesis and Iris datasets, which empirically shows that found quantum\nembedding architecture by QES outperforms manual designs whereas achieving\ncomparable performance to classical machine learning models.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:50:57 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Nguyen", "Nam", ""], ["Chen", "Kwang-Chen", ""]]}, {"id": "2105.11856", "submitter": "Micha{\\l} Ko\\'smider", "authors": "Micha{\\l} Ko\\'smider", "title": "Spectrum Correction: Acoustic Scene Classification with Mismatched\n  Recording Devices", "comments": "5 pages, 1 figure, published at Interspeech 2020, see\n  https://isca-speech.org/archive/Interspeech_2020/abstracts/3088.html", "journal-ref": "Interspeech (2020) 4641-4645", "doi": "10.21437/Interspeech.2020-3088", "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms, when trained on audio recordings from a limited\nset of devices, may not generalize well to samples recorded using other devices\nwith different frequency responses. In this work, a relatively straightforward\nmethod is introduced to address this problem. Two variants of the approach are\npresented. First requires aligned examples from multiple devices, the second\napproach alleviates this requirement. This method works for both time and\nfrequency domain representations of audio recordings. Further, a relation to\nstandardization and Cepstral Mean Subtraction is analysed. The proposed\napproach becomes effective even when very few examples are provided. This\nmethod was developed during the Detection and Classification of Acoustic Scenes\nand Events (DCASE) 2019 challenge and won the 1st place in the scenario with\nmis-matched recording devices with the accuracy of 75%. Source code for the\nexperiments can be found online.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:53:17 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ko\u015bmider", "Micha\u0142", ""]]}, {"id": "2105.11863", "submitter": "Manvel Avetisian", "authors": "Manvel Avetisian, Ilya Burenko, Konstantin Egorov, Vladimir Kokh,\n  Aleksandr Nesterov, Aleksandr Nikolaev, Alexander Ponomarchuk, Elena\n  Sokolova, Alex Tuzhilin, Dmitry Umerenkov", "title": "CoRSAI: A System for Robust Interpretation of CT Scans of COVID-19\n  Patients Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of chest CT scans can be used in detecting parts of lungs that are\naffected by infectious diseases such as COVID-19.Determining the volume of\nlungs affected by lesions is essential for formulating treatment\nrecommendations and prioritizingpatients by severity of the disease. In this\npaper we adopted an approach based on using an ensemble of deep\nconvolutionalneural networks for segmentation of slices of lung CT scans. Using\nour models we are able to segment the lesions, evaluatepatients dynamics,\nestimate relative volume of lungs affected by lesions and evaluate the lung\ndamage stage. Our modelswere trained on data from different medical centers. We\ncompared predictions of our models with those of six experiencedradiologists\nand our segmentation model outperformed most of them. On the task of\nclassification of disease severity, ourmodel outperformed all the radiologists.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:06:55 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Avetisian", "Manvel", ""], ["Burenko", "Ilya", ""], ["Egorov", "Konstantin", ""], ["Kokh", "Vladimir", ""], ["Nesterov", "Aleksandr", ""], ["Nikolaev", "Aleksandr", ""], ["Ponomarchuk", "Alexander", ""], ["Sokolova", "Elena", ""], ["Tuzhilin", "Alex", ""], ["Umerenkov", "Dmitry", ""]]}, {"id": "2105.11866", "submitter": "Zekun Li", "authors": "Zekun Li, Shu Wu, Zeyu Cui, Xiaoyu Zhang", "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling", "comments": "submitted to tkde", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:10:54 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 09:33:33 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Li", "Zekun", ""], ["Wu", "Shu", ""], ["Cui", "Zeyu", ""], ["Zhang", "Xiaoyu", ""]]}, {"id": "2105.11871", "submitter": "Yawen Duan", "authors": "Yawen Duan, Xin Chen, Hang Xu, Zewei Chen, Xiaodan Liang, Tong Zhang,\n  Zhenguo Li", "title": "TransNAS-Bench-101: Improving Transferability and Generalizability of\n  Cross-Task Neural Architecture Search", "comments": "Published at CVPR 2021. 8 pages main paper, 13 pages in total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent breakthroughs of Neural Architecture Search (NAS) extend the field's\nresearch scope towards a broader range of vision tasks and more diversified\nsearch spaces. While existing NAS methods mostly design architectures on a\nsingle task, algorithms that look beyond single-task search are surging to\npursue a more efficient and universal solution across various tasks. Many of\nthem leverage transfer learning and seek to preserve, reuse, and refine network\ndesign knowledge to achieve higher efficiency in future tasks. However, the\nenormous computational cost and experiment complexity of cross-task NAS are\nimposing barriers for valuable research in this direction. Existing NAS\nbenchmarks all focus on one type of vision task, i.e., classification. In this\nwork, we propose TransNAS-Bench-101, a benchmark dataset containing network\nperformance across seven tasks, covering classification, regression,\npixel-level prediction, and self-supervised tasks. This diversity provides\nopportunities to transfer NAS methods among tasks and allows for more complex\ntransfer schemes to evolve. We explore two fundamentally different types of\nsearch space: cell-level search space and macro-level search space. With 7,352\nbackbones evaluated on seven tasks, 51,464 trained models with detailed\ntraining information are provided. With TransNAS-Bench-101, we hope to\nencourage the advent of exceptional NAS algorithms that raise cross-task search\nefficiency and generalizability to the next level. Our dataset file will be\navailable at Mindspore, VEGA.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:15:21 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Duan", "Yawen", ""], ["Chen", "Xin", ""], ["Xu", "Hang", ""], ["Chen", "Zewei", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Tong", ""], ["Li", "Zhenguo", ""]]}, {"id": "2105.11904", "submitter": "Yongbin Liu", "authors": "Qing Lin, Yongbin Liu, Wen Wen, Zhihua Tao", "title": "Ensemble Making Few-Shot Learning Stronger", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning has been proposed and rapidly emerging as a viable means\nfor completing various tasks. Many few-shot models have been widely used for\nrelation learning tasks. However, each of these models has a shortage of\ncapturing a certain aspect of semantic features, for example, CNN on long-range\ndependencies part, Transformer on local features. It is difficult for a single\nmodel to adapt to various relation learning, which results in the high variance\nproblem. Ensemble strategy could be competitive on improving the accuracy of\nfew-shot relation extraction and mitigating high variance risks. This paper\nexplores an ensemble approach to reduce the variance and introduces fine-tuning\nand feature attention strategies to calibrate relation-level features. Results\non several few-shot relation learning tasks show that our model significantly\noutperforms the previous state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:11:10 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lin", "Qing", ""], ["Liu", "Yongbin", ""], ["Wen", "Wen", ""], ["Tao", "Zhihua", ""]]}, {"id": "2105.11908", "submitter": "Ansgar Scherp", "authors": "M. Lautaro Hickmann and Fabian Wurzberger and Megi Hoxhalli and Arne\n  Lochner and Jessica T\\\"ollich and Ansgar Scherp", "title": "Analysis of GraphSum's Attention Weights to Improve the Explainability\n  of Multi-Document Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern multi-document summarization (MDS) methods are based on transformer\narchitectures. They generate state of the art summaries, but lack\nexplainability. We focus on graph-based transformer models for MDS as they\ngained recent popularity. We aim to improve the explainability of the\ngraph-based MDS by analyzing their attention weights. In a graph-based MDS such\nas GraphSum, vertices represent the textual units, while the edges form some\nsimilarity graph over the units. We compare GraphSum's performance utilizing\ndifferent textual units, i. e., sentences versus paragraphs, on two news\nbenchmark datasets, namely WikiSum and MultiNews. Our experiments show that\nparagraph-level representations provide the best summarization performance.\nThus, we subsequently focus oAnalysisn analyzing the paragraph-level attention\nweights of GraphSum's multi-heads and decoding layers in order to improve the\nexplainability of a transformer-based MDS model. As a reference metric, we\ncalculate the ROUGE scores between the input paragraphs and each sentence in\nthe generated summary, which indicate source origin information via text\nsimilarity. We observe a high correlation between the attention weights and\nthis reference metric, especially on the the later decoding layers of the\ntransformer architecture. Finally, we investigate if the generated summaries\nfollow a pattern of positional bias by extracting which paragraph provided the\nmost information for each generated summary. Our results show that there is a\nhigh correlation between the position in the summary and the source origin.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 08:18:59 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hickmann", "M. Lautaro", ""], ["Wurzberger", "Fabian", ""], ["Hoxhalli", "Megi", ""], ["Lochner", "Arne", ""], ["T\u00f6llich", "Jessica", ""], ["Scherp", "Ansgar", ""]]}, {"id": "2105.11931", "submitter": "Guy Amir", "authors": "Guy Amir, Michael Schapira and Guy Katz", "title": "Towards Scalable Verification of RL-Driven Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have gained significant popularity in recent\nyears, becoming the state of the art in a variety of domains. In particular,\ndeep reinforcement learning (DRL) has recently been employed to train DNNs that\nact as control policies for various types of real-world systems. In this work,\nwe present the whiRL 2.0 tool, which implements a new approach for verifying\ncomplex properties of interest for such DRL systems. To demonstrate the\nbenefits of whiRL 2.0, we apply it to case studies from the communication\nnetworks domain that have recently been used to motivate formal verification of\nDRL systems, and which exhibit characteristics that are conducive for scalable\nverification. We propose techniques for performing k-induction and automated\ninvariant inference on such systems, and use these techniques for proving\nsafety and liveness properties of interest that were previously impossible to\nverify due to the scalability barriers of prior approaches. Furthermore, we\nshow how our proposed techniques provide insights into the inner workings and\nthe generalizability of DRL systems. whiRL 2.0 is publicly available online.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:34:40 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Amir", "Guy", ""], ["Schapira", "Michael", ""], ["Katz", "Guy", ""]]}, {"id": "2105.11953", "submitter": "Peter Gloor", "authors": "Luis A. Corujo, Peter A. Gloor, Emily Kieson", "title": "Emotion Recognition in Horses with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating intelligent systems capable of recognizing emotions is a difficult\ntask, especially when looking at emotions in animals. This paper describes the\nprocess of designing a \"proof of concept\" system to recognize emotions in\nhorses. This system is formed by two elements, a detector and a model. The\ndetector is a faster region-based convolutional neural network that detects\nhorses in an image. The second one, the model, is a convolutional neural\nnetwork that predicts the emotion of those horses. These two models were\ntrained with multiple images of horses until they achieved high accuracy in\ntheir tasks, creating therefore the desired system. 400 images of horses were\nused to train both the detector and the model while 80 were used to validate\nthe system. Once the two components were validated they were combined into a\ntestable system that would detect equine emotions based on established\nbehavioral ethograms indicating emotional affect through head, neck, ear,\nmuzzle, and eye position. The system showed an accuracy of between 69% and 74%\non the validation set, demonstrating that it is possible to predict emotions in\nanimals using autonomous intelligent systems. It is a first \"proof of concept\"\napproach that can be enhanced in many ways. Such a system has multiple\napplications including further studies in the growing field of animal emotions\nas well as in the veterinary field to determine the physical welfare of horses\nor other livestock.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:04:43 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Corujo", "Luis A.", ""], ["Gloor", "Peter A.", ""], ["Kieson", "Emily", ""]]}, {"id": "2105.11956", "submitter": "Vinayak Killedar", "authors": "Vinayak Killedar, Praveen Kumar Pokala, Chandra Sekhar Seelamantula", "title": "Learning Generative Prior with Latent Space Sparsity Constraints", "comments": "Submitted to UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of compressed sensing using a deep generative prior\nmodel and consider both linear and learned nonlinear sensing mechanisms, where\nthe nonlinear one involves either a fully connected neural network or a\nconvolutional neural network. Recently, it has been argued that the\ndistribution of natural images do not lie in a single manifold but rather lie\nin a union of several submanifolds. We propose a sparsity-driven latent space\nsampling (SDLSS) framework and develop a proximal meta-learning (PML) algorithm\nto enforce sparsity in the latent space. SDLSS allows the range-space of the\ngenerator to be considered as a union-of-submanifolds. We also derive the\nsample complexity bounds within the SDLSS framework for the linear measurement\nmodel. The results demonstrate that for a higher degree of compression, the\nSDLSS method is more efficient than the state-of-the-art method. We first\nconsider a comparison between linear and nonlinear sensing mechanisms on\nFashion-MNIST dataset and show that the learned nonlinear version is superior\nto the linear one. Subsequent comparisons with the deep compressive sensing\n(DCS) framework proposed in the literature are reported. We also consider the\neffect of the dimension of the latent space and the sparsity factor in\nvalidating the SDLSS framework. Performance quantification is carried out by\nemploying three objective metrics: peak signal-to-noise ratio (PSNR),\nstructural similarity index metric (SSIM), and reconstruction error (RE).\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:12:04 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Killedar", "Vinayak", ""], ["Pokala", "Praveen Kumar", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "2105.11964", "submitter": "Martin Hellkvist", "authors": "Martin Hellkvist, Ay\\c{c}a \\\"Oz\\c{c}elikkale", "title": "Model Mismatch Trade-offs in LMMSE Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a linear minimum mean squared error (LMMSE) estimation framework\nwith model mismatch where the assumed model order is smaller than that of the\nunderlying linear system which generates the data used in the estimation\nprocess. By modelling the regressors of the underlying system as random\nvariables, we analyze the average behaviour of the mean squared error (MSE).\nOur results quantify how the MSE depends on the interplay between the number of\nsamples and the number of parameters in the underlying system and in the\nassumed model. In particular, if the number of samples is not sufficiently\nlarge, neither increasing the number of samples nor the assumed model\ncomplexity is sufficient to guarantee a performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:16:45 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hellkvist", "Martin", ""], ["\u00d6z\u00e7elikkale", "Ay\u00e7a", ""]]}, {"id": "2105.11977", "submitter": "Olivier Sigaud", "authors": "Olivier Sigaud and Hugo Caselles-Dupr\\'e and C\\'edric Colas and Ahmed\n  Akakzia and Pierre-Yves Oudeyer and Mohamed Chetouani", "title": "Towards Teachable Autonomous Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous discovery and direct instruction are two extreme sources of\nlearning in children, but educational sciences have shown that intermediate\napproaches such as assisted discovery or guided play resulted in better\nacquisition of skills. When turning to Artificial Intelligence, the above\ndichotomy is translated into the distinction between autonomous agents which\nlearn in isolation and interactive learning agents which can be taught by\nsocial partners but generally lack autonomy. In between should stand teachable\nautonomous agents: agents learning from both internal and teaching signals to\nbenefit from the higher efficiency of assisted discovery. Such agents could\nlearn on their own in the real world, but non-expert users could drive their\nlearning behavior towards their expectations. More fundamentally, combining\nboth capabilities might also be a key step towards general intelligence. In\nthis paper we elucidate obstacles along this research line. First, we build on\na seminal work of Bruner to extract relevant features of the assisted discovery\nprocesses. Second, we describe current research on autotelic agents, i.e.\nagents equipped with forms of intrinsic motivations that enable them to\nrepresent, self-generate and pursue their own goals. We argue that autotelic\ncapabilities are paving the way towards teachable and autonomous agents.\nFinally, we adopt a social learning perspective on tutoring interactions and we\nhighlight some components that are currently missing to autotelic agents before\nthey can be taught by ordinary people using natural pedagogy, and we provide a\nlist of specific research questions that emerge from this perspective.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:28:58 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Sigaud", "Olivier", ""], ["Caselles-Dupr\u00e9", "Hugo", ""], ["Colas", "C\u00e9dric", ""], ["Akakzia", "Ahmed", ""], ["Oudeyer", "Pierre-Yves", ""], ["Chetouani", "Mohamed", ""]]}, {"id": "2105.11982", "submitter": "Xinyue Xiong", "authors": "Dongxia Wu, Liyao Gao, Xinyue Xiong, Matteo Chinazzi, Alessandro\n  Vespignani, Yi-An Ma, Rose Yu", "title": "Quantifying Uncertainty in Deep Spatiotemporal Forecasting", "comments": "arXiv admin note: text overlap with arXiv:2102.06684", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is gaining increasing popularity for spatiotemporal\nforecasting. However, prior works have mostly focused on point estimates\nwithout quantifying the uncertainty of the predictions. In high stakes domains,\nbeing able to generate probabilistic forecasts with confidence intervals is\ncritical to risk assessment and decision making. Hence, a systematic study of\nuncertainty quantification (UQ) methods for spatiotemporal forecasting is\nmissing in the community. In this paper, we describe two types of\nspatiotemporal forecasting problems: regular grid-based and graph-based. Then\nwe analyze UQ methods from both the Bayesian and the frequentist point of view,\ncasting in a unified framework via statistical decision theory. Through\nextensive experiments on real-world road network traffic, epidemics, and air\nquality forecasting tasks, we reveal the statistical and computational\ntrade-offs for different UQ methods: Bayesian methods are typically more robust\nin mean prediction, while confidence levels obtained from frequentist methods\nprovide more extensive coverage over data variations. Computationally, quantile\nregression type methods are cheaper for a single confidence interval but\nrequire re-training for different intervals. Sampling based methods generate\nsamples that can form multiple confidence intervals, albeit at a higher\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:35:46 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 12:59:06 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wu", "Dongxia", ""], ["Gao", "Liyao", ""], ["Xiong", "Xinyue", ""], ["Chinazzi", "Matteo", ""], ["Vespignani", "Alessandro", ""], ["Ma", "Yi-An", ""], ["Yu", "Rose", ""]]}, {"id": "2105.11989", "submitter": "Rushabh Patel", "authors": "Rushabh Patel, Yanhui Guo", "title": "Graph Based Link Prediction between Human Phenotypes and Genes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The learning of genotype-phenotype associations and history of\nhuman disease by doing detailed and precise analysis of phenotypic\nabnormalities can be defined as deep phenotyping. To understand and detect this\ninteraction between phenotype and genotype is a fundamental step when\ntranslating precision medicine to clinical practice. The recent advances in the\nfield of machine learning is efficient to predict these interactions between\nabnormal human phenotypes and genes.\n  Methods: In this study, we developed a framework to predict links between\nhuman phenotype ontology (HPO) and genes. The annotation data from the\nheterogeneous knowledge resources i.e., orphanet, is used to parse human\nphenotype-gene associations. To generate the embeddings for the nodes (HPO &\ngenes), an algorithm called node2vec was used. It performs node sampling on\nthis graph based on random walks, then learns features over these sampled nodes\nto generate embeddings. These embeddings were used to perform the downstream\ntask to predict the presence of the link between these nodes using 5 different\nsupervised machine learning algorithms.\n  Results: The downstream link prediction task shows that the Gradient Boosting\nDecision Tree based model (LightGBM) achieved an optimal AUROC 0.904 and AUCPR\n0.784. In addition, LightGBM achieved an optimal weighted F1 score of 0.87.\nCompared to the other 4 methods LightGBM is able to find more accurate\ninteraction/link between human phenotype & gene pairs.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:47:07 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 18:28:30 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Patel", "Rushabh", ""], ["Guo", "Yanhui", ""]]}, {"id": "2105.11990", "submitter": "Danny Panknin", "authors": "Danny Panknin and Klaus Robert M\\\"uller and Shinichi Nakajima", "title": "Optimal Sampling Density for Nonparametric Regression", "comments": "50 pages, plus 33 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel active learning strategy for regression, which is\nmodel-agnostic, robust against model mismatch, and interpretable. Assuming that\na small number of initial samples are available, we derive the optimal training\ndensity that minimizes the generalization error of local polynomial smoothing\n(LPS) with its kernel bandwidth tuned locally: We adopt the mean integrated\nsquared error (MISE) as a generalization criterion, and use the asymptotic\nbehavior of the MISE as well as the locally optimal bandwidths (LOB) - the\nbandwidth function that minimizes MISE in the asymptotic limit. The asymptotic\nexpression of our objective then reveals the dependence of the MISE on the\ntraining density, enabling analytic minimization. As a result,we obtain the\noptimal training density in a closed-form. The almost model-free nature of our\napproach thus helps to encode the essential properties of the target problem,\nproviding a robust and model-agnostic active learning strategy. Furthermore,\nthe obtained training density factorizes the influence of local function\ncomplexity, noise level and test density in a transparent and interpretable\nway. We validate our theory in numerical simulations, and show that the\nproposed active learning method outperforms the existing state-of-the-art\nmodel-agnostic approaches.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:52:17 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 08:15:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Panknin", "Danny", ""], ["M\u00fcller", "Klaus Robert", ""], ["Nakajima", "Shinichi", ""]]}, {"id": "2105.12002", "submitter": "Chen Liang", "authors": "Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu,\n  Pengcheng He, Tuo Zhao and Weizhu Chen", "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to\n  Improving Generalization", "comments": "The 59th annual meeting of the Association for Computational\n  Linguistics (ACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lottery Ticket Hypothesis suggests that an over-parametrized network\nconsists of ``lottery tickets'', and training a certain collection of them\n(i.e., a subnetwork) can match the performance of the full model. In this\npaper, we study such a collection of tickets, which is referred to as ``winning\ntickets'', in extremely over-parametrized models, e.g., pre-trained language\nmodels. We observe that at certain compression ratios, the generalization\nperformance of the winning tickets can not only match but also exceed that of\nthe full model. In particular, we observe a phase transition phenomenon: As the\ncompression ratio increases, generalization performance of the winning tickets\nfirst improves then deteriorates after a certain threshold. We refer to the\ntickets on the threshold as ``super tickets''. We further show that the phase\ntransition is task and model dependent -- as the model size becomes larger and\nthe training data set becomes smaller, the transition becomes more pronounced.\nOur experiments on the GLUE benchmark show that the super tickets improve\nsingle task fine-tuning by $0.9$ points on BERT-base and $1.0$ points on\nBERT-large, in terms of task-average score. We also demonstrate that adaptively\nsharing the super tickets across tasks benefits multi-task learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:10:05 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:58:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liang", "Chen", ""], ["Zuo", "Simiao", ""], ["Chen", "Minshuo", ""], ["Jiang", "Haoming", ""], ["Liu", "Xiaodong", ""], ["He", "Pengcheng", ""], ["Zhao", "Tuo", ""], ["Chen", "Weizhu", ""]]}, {"id": "2105.12005", "submitter": "Parisa Abdolrahim Poorheravi", "authors": "Parisa Abdolrahim Poorheravi and Vincent Gaudet", "title": "Hierarchical Subspace Learning for Dimensionality Reduction to Improve\n  Classification Accuracy in Large Data Sets", "comments": "6 pages with 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning is used for dimensionality reduction, with the goal of\nfinding a projection subspace to increase and decrease the inter- and\nintraclass variances, respectively. However, a bottleneck for subspace learning\nmethods often arises from the high dimensionality of datasets. In this paper, a\nhierarchical approach is proposed to scale subspace learning methods, with the\ngoal of improving classification in large datasets by a range of 3% to 10%.\nDifferent combinations of methods are studied. We assess the proposed method on\nfive publicly available large datasets, for different eigen-value based\nsubspace learning methods such as linear discriminant analysis, principal\ncomponent analysis, generalized discriminant analysis, and reconstruction\nindependent component analysis. To further examine the effect of the proposed\nmethod on various classification methods, we fed the generated result to linear\ndiscriminant analysis, quadratic linear analysis, k-nearest neighbor, and\nrandom forest classifiers. The resulting classification accuracies are compared\nto show the effectiveness of the hierarchical approach, reporting results of an\naverage of 5% increase in classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:15:12 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Poorheravi", "Parisa Abdolrahim", ""], ["Gaudet", "Vincent", ""]]}, {"id": "2105.12018", "submitter": "Andres Daniel Perez", "authors": "Ernesto Arganda, Anibal D. Medina, Andres D. Perez, Alejandro Szynkman", "title": "Towards a method to anticipate dark matter signals with deep learning at\n  the LHC", "comments": "51 pages, 28 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.LG hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study several simplified dark matter (DM) models and their signatures at\nthe LHC using neural networks. We focus on the usual monojet plus missing\ntransverse energy channel, but to train the algorithms we organize the data in\n2D histograms instead of event-by-event arrays. This results in a large\nperformance boost to distinguish between standard model (SM) only and SM plus\nnew physics signals. We use the kinematic monojet features as input data which\nallow us to describe families of models with a single data sample. We found\nthat the neural network performance does not depend on the simulated number of\nbackground events if they are presented as a function of $S/\\sqrt{B}$, where\n$S$ and $B$ are the number of signal and background events per histogram,\nrespectively. This provides flexibility to the method, since testing a\nparticular model in that case only requires knowing the new physics monojet\ncross section. Furthermore, we also discuss the network performance under\nincorrect assumptions about the true DM nature. Finally, we propose multimodel\nclassifiers to search and identify new signals in a more general way, for the\nnext LHC run.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:38:13 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Arganda", "Ernesto", ""], ["Medina", "Anibal D.", ""], ["Perez", "Andres D.", ""], ["Szynkman", "Alejandro", ""]]}, {"id": "2105.12019", "submitter": "Septimia Sarbu", "authors": "Septimia Sarbu and Abdellatif Zaidi", "title": "On learning parametric distributions from quantized samples", "comments": "Accepted for publication at the IEEE Information Theory Symposium\n  (ISIT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of learning parametric distributions from their\nquantized samples in a network. Specifically, $n$ agents or sensors observe\nindependent samples of an unknown parametric distribution; and each of them\nuses $k$ bits to describe its observed sample to a central processor whose goal\nis to estimate the unknown distribution. First, we establish a generalization\nof the well-known van Trees inequality to general $L_p$-norms, with $p > 1$, in\nterms of Generalized Fisher information. Then, we develop minimax lower bounds\non the estimation error for two losses: general $L_p$-norms and the related\nWasserstein loss from optimal transport.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:38:28 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sarbu", "Septimia", ""], ["Zaidi", "Abdellatif", ""]]}, {"id": "2105.12022", "submitter": "Robbie Vreugdenhil", "authors": "Robbie Vreugdenhil, Viet Anh Nguyen, Armin Eftekhari, Peyman Mohajerin\n  Esfahani", "title": "Principal Component Hierarchy for Sparse Quadratic Programs", "comments": null, "journal-ref": "ICML 2021", "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approximation hierarchy for cardinality-constrained,\nconvex quadratic programs that exploits the rank-dominating eigenvectors of the\nquadratic matrix. Each level of approximation admits a min-max characterization\nwhose objective function can be optimized over the binary variables\nanalytically, while preserving convexity in the continuous variables.\nExploiting this property, we propose two scalable optimization algorithms,\ncoined as the \"best response\" and the \"dual program\", that can efficiently\nscreen the potential indices of the nonzero elements of the original program.\nWe show that the proposed methods are competitive with the existing screening\nmethods in the current sparse regression literature, and it is particularly\nfast on instances with high number of measurements in experiments with both\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:45:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Vreugdenhil", "Robbie", ""], ["Nguyen", "Viet Anh", ""], ["Eftekhari", "Armin", ""], ["Esfahani", "Peyman Mohajerin", ""]]}, {"id": "2105.12033", "submitter": "Van Hai Nguyen", "authors": "Hai V. Nguyen, Tan Bui-Thanh", "title": "Model-Constrained Deep Learning Approaches for Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL), in particular deep neural networks (DNN), by design is\npurely data-driven and in general does not require physics. This is the\nstrength of DL but also one of its key limitations when applied to science and\nengineering problems in which underlying physical properties (such as\nstability, conservation, and positivity) and desired accuracy need to be\nachieved. DL methods in their original forms are not capable of respecting the\nunderlying mathematical models or achieving desired accuracy even in big-data\nregimes. On the other hand, many data-driven science and engineering problems,\nsuch as inverse problems, typically have limited experimental or observational\ndata, and DL would overfit the data in this case. Leveraging information\nencoded in the underlying mathematical models, we argue, not only compensates\nmissing information in low data regimes but also provides opportunities to\nequip DL methods with the underlying physics and hence obtaining higher\naccuracy. This short communication introduces several model-constrained DL\napproaches (including both feed-forward DNN and autoencoders) that are capable\nof learning not only information hidden in the training data but also in the\nunderlying mathematical models to solve inverse problems. We present and\nprovide intuitions for our formulations for general nonlinear problems. For\nlinear inverse problems and linear networks, the first order optimality\nconditions show that our model-constrained DL approaches can learn information\nencoded in the underlying mathematical models, and thus can produce consistent\nor equivalent inverse solutions, while naive purely data-based counterparts\ncannot.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:12:39 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 22:18:15 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Nguyen", "Hai V.", ""], ["Bui-Thanh", "Tan", ""]]}, {"id": "2105.12034", "submitter": "L\\'eonard Hussenot", "authors": "Leonard Hussenot, Marcin Andrychowicz, Damien Vincent, Robert Dadashi,\n  Anton Raichuk, Lukasz Stafiniak, Sertan Girgin, Raphael Marinier, Nikola\n  Momchev, Sabela Ramos, Manu Orsini, Olivier Bachem, Matthieu Geist, Olivier\n  Pietquin", "title": "Hyperparameter Selection for Imitation Learning", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address the issue of tuning hyperparameters (HPs) for imitation learning\nalgorithms in the context of continuous-control, when the underlying reward\nfunction of the demonstrating expert cannot be observed at any time. The vast\nliterature in imitation learning mostly considers this reward function to be\navailable for HP selection, but this is not a realistic setting. Indeed, would\nthis reward function be available, it could then directly be used for policy\ntraining and imitation would not be necessary. To tackle this mostly ignored\nproblem, we propose a number of possible proxies to the external reward. We\nevaluate them in an extensive empirical study (more than 10'000 agents across 9\nenvironments) and make practical recommendations for selecting HPs. Our results\nshow that while imitation learning algorithms are sensitive to HP choices, it\nis often possible to select good enough HPs through a proxy to the reward\nfunction.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:14:04 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hussenot", "Leonard", ""], ["Andrychowicz", "Marcin", ""], ["Vincent", "Damien", ""], ["Dadashi", "Robert", ""], ["Raichuk", "Anton", ""], ["Stafiniak", "Lukasz", ""], ["Girgin", "Sertan", ""], ["Marinier", "Raphael", ""], ["Momchev", "Nikola", ""], ["Ramos", "Sabela", ""], ["Orsini", "Manu", ""], ["Bachem", "Olivier", ""], ["Geist", "Matthieu", ""], ["Pietquin", "Olivier", ""]]}, {"id": "2105.12049", "submitter": "Mohammad Malekzadeh", "authors": "Mohammad Malekzadeh and Anastasia Borovykh and Deniz G\\\"und\\\"uz", "title": "Honest-but-Curious Nets: Sensitive Attributes of Private Inputs can be\n  Secretly Coded into the Entropy of Classifiers' Outputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that deep neural networks, trained for the classification of a\nnon-sensitive target attribute, can reveal sensitive attributes of their input\ndata; through features of different granularity extracted by the classifier.\nWe, taking a step forward, show that deep classifiers can be trained to\nsecretly encode a sensitive attribute of users' input data, at inference time,\ninto the classifier's outputs for the target attribute. An attack that works\neven if users have a white-box view of the classifier, and can keep all\ninternal representations hidden except for the classifier's estimation of the\ntarget attribute. We introduce an information-theoretical formulation of such\nadversaries and present efficient empirical implementations for training\nhonest-but-curious (HBC) classifiers based on this formulation: deep models\nthat can be accurate in predicting the target attribute, but also can utilize\ntheir outputs to secretly encode a sensitive attribute. Our evaluations on\nseveral tasks in real-world datasets show that a semi-trusted server can build\na classifier that is not only perfectly honest but also accurately curious. Our\nwork highlights a vulnerability that can be exploited by malicious machine\nlearning service providers to attack their user's privacy in several seemingly\nsafe scenarios; such as encrypted inferences, computations at the edge, or\nprivate knowledge distillation. We conclude by showing the difficulties in\ndistinguishing between standard and HBC classifiers and discussing potential\nproactive defenses against this vulnerability of deep classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:27:57 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Malekzadeh", "Mohammad", ""], ["Borovykh", "Anastasia", ""], ["G\u00fcnd\u00fcz", "Deniz", ""]]}, {"id": "2105.12056", "submitter": "Cassandra Burgess", "authors": "Cassandra Burgess, Cordelia Neisinger, Rafael Dinner", "title": "Matching Targets Across Domains with RADON, the Re-Identification Across\n  Domain Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel convolutional neural network that learns to match images\nof an object taken from different viewpoints or by different optical sensors.\nOur Re-Identification Across Domain Network (RADON) scores pairs of input\nimages from different domains on similarity. Our approach extends previous work\non Siamese networks and modifies them to more challenging use cases, including\nlow- and no-shot learning, in which few images of a specific target are\navailable for training. RADON shows strong performance on cross-view vehicle\nmatching and cross-domain person identification in a no-shot learning\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:36:38 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Burgess", "Cassandra", ""], ["Neisinger", "Cordelia", ""], ["Dinner", "Rafael", ""]]}, {"id": "2105.12062", "submitter": "Kaiwen Zhou", "authors": "Kaiwen Zhou, Lai Tian, Anthony Man-Cho So, James Cheng", "title": "Practical Schemes for Finding Near-Stationary Points of Convex\n  Finite-Sums", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding near-stationary points in convex optimization has not\nbeen adequately studied yet, unlike other optimality measures such as\nminimizing function value. Even in the deterministic case, the optimal method\n(OGM-G, due to Kim and Fessler (2021)) has just been discovered recently. In\nthis work, we conduct a systematic study of the algorithmic techniques in\nfinding near-stationary points of convex finite-sums. Our main contributions\nare several algorithmic discoveries: (1) we discover a memory-saving variant of\nOGM-G based on the performance estimation problem approach (Drori and Teboulle,\n2014); (2) we design a new accelerated SVRG variant that can simultaneously\nachieve fast rates for both minimizing gradient norm and function value; (3) we\npropose an adaptively regularized accelerated SVRG variant, which does not\nrequire the knowledge of some unknown initial constants and achieves\nnear-optimal complexities. We put an emphasis on the simplicity and\npracticality of the new schemes, which could facilitate future developments.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:46:35 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zhou", "Kaiwen", ""], ["Tian", "Lai", ""], ["So", "Anthony Man-Cho", ""], ["Cheng", "James", ""]]}, {"id": "2105.12068", "submitter": "Tedo Vrbanec", "authors": "Tedo Vrbanec and Ana Mestrovic", "title": "Taxonomy of academic plagiarism methods", "comments": "18 pages, 3 figures, 1 table", "journal-ref": "Journal of the Polytechnic of Rijeka, 2021, Volume 9, Issue 1, pp.\n  283-300", "doi": "10.31784/zvr.9.1.17", "report-no": "This work has been supported in part by the University of Rijeka\n  under the project \"uniri-drustv-18-38\"", "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The article gives an overview of the plagiarism domain, with focus on\nacademic plagiarism. The article defines plagiarism, explains the origin of the\nterm, as well as plagiarism related terms. It identifies the extent of the\nplagiarism domain and then focuses on the plagiarism subdomain of text\ndocuments, for which it gives an overview of current classifications and\ntaxonomies and then proposes a more comprehensive classification according to\nseveral criteria: their origin and purpose, technical implementation,\nconsequence, complexity of detection and according to the number of linguistic\nsources. The article suggests the new classification of academic plagiarism,\ndescribes sorts and methods of plagiarism, types and categories, approaches and\nphases of plagiarism detection, the classification of methods and algorithms\nfor plagiarism detection. The title of the article explicitly targets the\nacademic community, but it is sufficiently general and interdisciplinary, so it\ncan be useful for many other professionals like software developers, linguists\nand librarians.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:49:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Vrbanec", "Tedo", ""], ["Mestrovic", "Ana", ""]]}, {"id": "2105.12089", "submitter": "Piyush Sharma", "authors": "Piyush K. Sharma, Gary Holness, and Poopalasingam Sivakumar, Yuri\n  Markushin, Noureddine Melikechi", "title": "Investigating Manifold Neighborhood size for Nonlinear Analysis of LIBS\n  Amino Acid Spectra", "comments": "In ISCA 24th International Conference on Software Engineering and\n  Data Engineering (SEDE 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and identification of amino acids in aqueous solutions is\nimportant in the study of biomacromolecules. Laser Induced Breakdown\nSpectroscopy (LIBS) uses high energy laser-pulses for ablation of chemical\ncompounds whose radiated spectra are captured and recorded to reveal molecular\nstructure. Spectral peaks and noise from LIBS are impacted by experimental\nprotocols. Current methods for LIBS spectral analysis achieves promising\nresults using PCA, a linear method. It is well-known that the underlying\nphysical processes behind LIBS are highly nonlinear. Our work set out to\nunderstand the impact of LIBS spectra on suitable neighborhood size over which\nto consider pattern phenomena, if nonlinear methods capture pattern phenomena\nwith increased efficacy, and how they improve classification and identification\nof compounds. We analyzed four amino acids, polysaccharide, and a control\ngroup, water. We developed an information theoretic method for measurement of\nLIBS energy spectra, implemented manifold methods for nonlinear dimensionality\nreduction, and found while clustering results were not statistically\nsignificantly different, nonlinear methods lead to increased classification\naccuracy. Moreover, our approach uncovered the contribution of micro-wells\n(experimental protocol) in LIBS spectra. To the best of our knowledge, ours is\nthe first application of Manifold methods to LIBS amino-acid analysis in the\nresearch literature.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:17:00 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sharma", "Piyush K.", ""], ["Holness", "Gary", ""], ["Sivakumar", "Poopalasingam", ""], ["Markushin", "Yuri", ""], ["Melikechi", "Noureddine", ""]]}, {"id": "2105.12092", "submitter": "Anselmo Pitombeira-Neto", "authors": "Anselmo R. Pitombeira-Neto, Helano P. Santos, Ticiana L. Coelho da\n  Silva, Jos\\'e Antonio F. de Macedo", "title": "Trajectory Modeling via Random Utility Inverse Reinforcement Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modeling trajectories of drivers in a road network\nfrom the perspective of inverse reinforcement learning. As rational agents,\ndrivers are trying to maximize some reward function unknown to an external\nobserver as they make up their trajectories. We apply the concept of random\nutility from microeconomic theory to model the unknown reward function as a\nfunction of observable features plus an error term which represents features\nknown only to the driver. We develop a parameterized generative model for the\ntrajectories based on a random utility Markov decision process formulation of\ndrivers decisions. We show that maximum entropy inverse reinforcement learning\nis a particular case of our proposed formulation when we assume a Gumbel\ndensity function for the unobserved reward error terms. We illustrate Bayesian\ninference on model parameters through a case study with real trajectory data\nfrom a large city obtained from sensors placed on sparsely distributed points\non the street network.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:19:09 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Pitombeira-Neto", "Anselmo R.", ""], ["Santos", "Helano P.", ""], ["da Silva", "Ticiana L. Coelho", ""], ["de Macedo", "Jos\u00e9 Antonio F.", ""]]}, {"id": "2105.12096", "submitter": "Nelly Elsayed", "authors": "Nelly Elsayed, Zaghloul Saad Zaghloul, Sylvia Worlali Azumah,\n  Chengcheng Li", "title": "Intrusion Detection System in Smart Home Network Using Bidirectional\n  LSTM and Convolutional Neural Networks Hybrid Model", "comments": "4 pages, 6 figures, Accepted in the MWSCAS 2021. This material is\n  based upon work supported by the National Science Foundation under Grant No.\n  (CNS-1801593). Any opinions, findings, and conclusions or recommendations\n  expressed in this material are those of the author(s) and do not necessarily\n  reflect the views of the National Science Foundation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) allowed smart homes to improve the quality and the\ncomfort of our daily lives. However, these conveniences introduced several\nsecurity concerns that increase rapidly. IoT devices, smart home hubs, and\ngateway raise various security risks. The smart home gateways act as a\ncentralized point of communication between the IoT devices, which can create a\nbackdoor into network data for hackers. One of the common and effective ways to\ndetect such attacks is intrusion detection in the network traffic. In this\npaper, we proposed an intrusion detection system (IDS) to detect anomalies in a\nsmart home network using a bidirectional long short-term memory (BiLSTM) and\nconvolutional neural network (CNN) hybrid model. The BiLSTM recurrent behavior\nprovides the intrusion detection model to preserve the learned information\nthrough time, and the CNN extracts perfectly the data features. The proposed\nmodel can be applied to any smart home network gateway.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:32:03 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 01:44:52 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Elsayed", "Nelly", ""], ["Zaghloul", "Zaghloul Saad", ""], ["Azumah", "Sylvia Worlali", ""], ["Li", "Chengcheng", ""]]}, {"id": "2105.12115", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Ehsan Zabihi Naeini", "title": "Calibration and Uncertainty Quantification of Bayesian Convolutional\n  Neural Networks for Geophysical Applications", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks offer numerous potential applications across geoscience,\nfor example, one could argue that they are the state-of-the-art method for\npredicting faults in seismic datasets. In quantitative reservoir\ncharacterization workflows, it is common to incorporate the uncertainty of\npredictions thus such subsurface models should provide calibrated probabilities\nand the associated uncertainties in their predictions. It has been shown that\npopular Deep Learning-based models are often miscalibrated, and due to their\ndeterministic nature, provide no means to interpret the uncertainty of their\npredictions. We compare three different approaches to obtaining probabilistic\nmodels based on convolutional neural networks in a Bayesian formalism, namely\nDeep Ensembles, Concrete Dropout, and Stochastic Weight Averaging-Gaussian\n(SWAG). These methods are consistently applied to fault detection case studies\nwhere Deep Ensembles use independently trained models to provide fault\nprobabilities, Concrete Dropout represents an extension to the popular Dropout\ntechnique to approximate Bayesian neural networks, and finally, we apply SWAG,\na recent method that is based on the Bayesian inference equivalence of\nmini-batch Stochastic Gradient Descent. We provide quantitative results in\nterms of model calibration and uncertainty representation, as well as\nqualitative results on synthetic and real seismic datasets. Our results show\nthat the approximate Bayesian methods, Concrete Dropout and SWAG, both provide\nwell-calibrated predictions and uncertainty attributes at a lower computational\ncost when compared to the baseline Deep Ensemble approach. The resulting\nuncertainties also offer a possibility to further improve the model performance\nas well as enhancing the interpretability of the models.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:54:23 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Mosser", "Lukas", ""], ["Naeini", "Ehsan Zabihi", ""]]}, {"id": "2105.12152", "submitter": "Christian Horvat", "authors": "Christian Horvat, Jean-Pascal Pfister", "title": "Density estimation on low-dimensional manifolds: an inflation-deflation\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Normalizing Flows (NFs) are universal density estimators based on Neuronal\nNetworks. However, this universality is limited: the density's support needs to\nbe diffeomorphic to a Euclidean space. In this paper, we propose a novel method\nto overcome this limitation without sacrificing universality. The proposed\nmethod inflates the data manifold by adding noise in the normal space, trains\nan NF on this inflated manifold, and, finally, deflates the learned density.\nOur main result provides sufficient conditions on the manifold and the specific\nchoice of noise under which the corresponding estimator is exact. Our method\nhas the same computational complexity as NFs and does not require computing an\ninverse flow. We also show that, if the embedding dimension is much larger than\nthe manifold dimension, noise in the normal space can be well approximated by\nGaussian noise. This allows to use our method for approximating arbitrary\ndensities on non-flat manifolds provided that the manifold dimension is known.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 18:08:09 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 12:42:45 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Horvat", "Christian", ""], ["Pfister", "Jean-Pascal", ""]]}, {"id": "2105.12161", "submitter": "Srishti Yadav", "authors": "Srishti Yadav", "title": "Occlusion Aware Kernel Correlation Filter Tracker using RGB-D", "comments": "Thesis, Simon Fraser University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unlike deep learning which requires large training datasets, correlation\nfilter-based trackers like Kernelized Correlation Filter (KCF) uses implicit\nproperties of tracked images (circulant matrices) for training in real-time.\nDespite their practical application in tracking, a need for a better\nunderstanding of the fundamentals associated with KCF in terms of\ntheoretically, mathematically, and experimentally exists. This thesis first\ndetails the workings prototype of the tracker and investigates its\neffectiveness in real-time applications and supporting visualizations. We\nfurther address some of the drawbacks of the tracker in cases of occlusions,\nscale changes, object rotation, out-of-view and model drift with our novel\nRGB-D Kernel Correlation tracker. We also study the use of particle filters to\nimprove trackers' accuracy. Our results are experimentally evaluated using a)\nstandard dataset and b) real-time using the Microsoft Kinect V2 sensor. We\nbelieve this work will set the basis for a better understanding of the\neffectiveness of kernel-based correlation filter trackers and to further define\nsome of its possible advantages in tracking.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 18:37:39 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Yadav", "Srishti", ""]]}, {"id": "2105.12189", "submitter": "Michael Lutter", "authors": "Michael Lutter and Shie Mannor and Jan Peters and Dieter Fox and\n  Animesh Garg", "title": "Robust Value Iteration for Continuous Control Tasks", "comments": "Accepted Paper at Robotics: Science and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When transferring a control policy from simulation to a physical system, the\npolicy needs to be robust to variations in the dynamics to perform well.\nCommonly, the optimal policy overfits to the approximate model and the\ncorresponding state-distribution, often resulting in failure to trasnfer\nunderlying distributional shifts. In this paper, we present Robust Fitted Value\nIteration, which uses dynamic programming to compute the optimal value function\non the compact state domain and incorporates adversarial perturbations of the\nsystem dynamics. The adversarial perturbations encourage a optimal policy that\nis robust to changes in the dynamics. Utilizing the continuous-time perspective\nof reinforcement learning, we derive the optimal perturbations for the states,\nactions, observations and model parameters in closed-form. Notably, the\nresulting algorithm does not require discretization of states or actions.\nTherefore, the optimal adversarial perturbations can be efficiently\nincorporated in the min-max value function update. We apply the resulting\nalgorithm to the physical Furuta pendulum and cartpole. By changing the masses\nof the systems we evaluate the quantitative and qualitative performance across\ndifferent model parameters. We show that robust value iteration is more robust\ncompared to deep reinforcement learning algorithm and the non-robust version of\nthe algorithm. Videos of the experiments are shown at\nhttps://sites.google.com/view/rfvi\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 19:48:35 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Lutter", "Michael", ""], ["Mannor", "Shie", ""], ["Peters", "Jan", ""], ["Fox", "Dieter", ""], ["Garg", "Animesh", ""]]}, {"id": "2105.12195", "submitter": "Joymallya Chakraborty Mr.", "authors": "Joymallya Chakraborty, Suvodeep Majumder, Tim Menzies", "title": "Bias in Machine Learning Software: Why? How? What to do?", "comments": null, "journal-ref": "ESEC/FSE'2021: The 29th ACM Joint European Software Engineering\n  Conference and Symposium on the Foundations of Software Engineering\n  (ESEC/FSE), Athens, Greece, August 23-28, 2021", "doi": "10.1145/3468264.3468537", "report-no": null, "categories": "cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly, software is making autonomous decisions in case of criminal\nsentencing, approving credit cards, hiring employees, and so on. Some of these\ndecisions show bias and adversely affect certain social groups (e.g. those\ndefined by sex, race, age, marital status). Many prior works on bias mitigation\ntake the following form: change the data or learners in multiple ways, then see\nif any of that improves fairness. Perhaps a better approach is to postulate\nroot causes of bias and then applying some resolution strategy. This paper\npostulates that the root causes of bias are the prior decisions that affect-\n(a) what data was selected and (b) the labels assigned to those examples. Our\nFair-SMOTE algorithm removes biased labels; and rebalances internal\ndistributions such that based on sensitive attribute, examples are equal in\nboth positive and negative classes. On testing, it was seen that this method\nwas just as effective at reducing bias as prior approaches. Further, models\ngenerated via Fair-SMOTE achieve higher performance (measured in terms of\nrecall and F1) than other state-of-the-art fairness improvement algorithms. To\nthe best of our knowledge, measured in terms of number of analyzed learners and\ndatasets, this study is one of the largest studies on bias mitigation yet\npresented in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:15:50 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 21:37:02 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 06:35:38 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Chakraborty", "Joymallya", ""], ["Majumder", "Suvodeep", ""], ["Menzies", "Tim", ""]]}, {"id": "2105.12202", "submitter": "Andrew Dunn", "authors": "Andrew Dunn, Diana Inkpen, R\\u{a}zvan Andonie", "title": "Context-Sensitive Visualization of Deep Learning Natural Language\n  Processing Models", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The introduction of Transformer neural networks has changed the landscape of\nNatural Language Processing (NLP) during the last years. So far, none of the\nvisualization systems has yet managed to examine all the facets of the\nTransformers. This gave us the motivation of the current work. We propose a new\nNLP Transformer context-sensitive visualization method that leverages existing\nNLP tools to find the most significant groups of tokens (words) that have the\ngreatest effect on the output, thus preserving some context from the original\ntext. First, we use a sentence-level dependency parser to highlight promising\nword groups. The dependency parser creates a tree of relationships between the\nwords in the sentence. Next, we systematically remove adjacent and non-adjacent\ntuples of \\emph{n} tokens from the input text, producing several new texts with\nthose tokens missing. The resulting texts are then passed to a pre-trained BERT\nmodel. The classification output is compared with that of the full text, and\nthe difference in the activation strength is recorded. The modified texts that\nproduce the largest difference in the target classification output neuron are\nselected, and the combination of removed words are then considered to be the\nmost influential on the model's output. Finally, the most influential word\ncombinations are visualized in a heatmap.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:26:38 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Dunn", "Andrew", ""], ["Inkpen", "Diana", ""], ["Andonie", "R\u0103zvan", ""]]}, {"id": "2105.12204", "submitter": "Pierre-Fran\\c{c}ois Massiani", "authors": "Pierre-Fran\\c{c}ois Massiani, Steve Heim, Friedrich Solowjow,\n  Sebastian Trimpe", "title": "Safe Value Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.RO cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The relationship between safety and optimality in control is not well\nunderstood, and they are often seen as important yet conflicting objectives.\nThere is a pressing need to formalize this relationship, especially given the\ngrowing prominence of learning-based methods. Indeed, it is common practice in\nreinforcement learning to simply modify reward functions by penalizing\nfailures, with the penalty treated as a mere heuristic. We rigorously examine\nthis relationship, and formalize the requirements for safe value functions:\nvalue functions that are both optimal for a given task, and enforce safety. We\nreveal the structure of this relationship through a proof of strong duality,\nshowing that there always exists a finite penalty that induces a safe value\nfunction. This penalty is not unique, but upper-unbounded: larger penalties do\nnot harm optimality. Although it is often not possible to compute the minimum\nrequired penalty, we reveal clear structure of how the penalty, rewards,\ndiscount factor, and dynamics interact. This insight suggests practical,\ntheory-guided heuristics to design reward functions for control problems where\nsafety is important.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:35:30 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Massiani", "Pierre-Fran\u00e7ois", ""], ["Heim", "Steve", ""], ["Solowjow", "Friedrich", ""], ["Trimpe", "Sebastian", ""]]}, {"id": "2105.12208", "submitter": "Yu Qin", "authors": "Yu Qin, Brittany Terese Fasy, Carola Wenk, and Brian Summa", "title": "A Domain-Oblivious Approach for Learning Concise Representations of\n  Filtered Topological Spaces", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams have been widely used to quantify the underlying\nfeatures of filtered topological spaces in data visualization. In many\napplications, computing distances between diagrams is essential; however,\ncomputing these distances has been challenging due to the computational cost.\nIn this paper, we propose a persistence diagram hashing framework that learns a\nbinary code representation of persistence diagrams, which allows for fast\ncomputation of distances. This framework is built upon a generative adversarial\nnetwork (GAN) with a diagram distance loss function to steer the learning\nprocess. Instead of attempting to transform diagrams into vectorized\nrepresentations, we hash diagrams into binary codes, which have natural\nadvantages in large-scale tasks. The training of this model is domain-oblivious\nin that it can be computed purely from synthetic, randomly created diagrams. As\na consequence, our proposed method is directly applicable to various datasets\nwithout the need of retraining the model. These binary codes, when compared\nusing fast Hamming distance, better maintain topological similarity properties\nbetween datasets than other vectorized representations. To evaluate this\nmethod, we apply our framework to the problem of diagram clustering and we\ncompare the quality and performance of our approach to the state-of-the-art. In\naddition, we show the scalability of our approach on a dataset with 10k\npersistence diagrams, which is not possible with current techniques. Moreover,\nour experimental results demonstrate that our method is significantly faster\nwith less memory usage, while retaining comparable or better quality\ncomparisons.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:44:28 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Qin", "Yu", ""], ["Fasy", "Brittany Terese", ""], ["Wenk", "Carola", ""], ["Summa", "Brian", ""]]}, {"id": "2105.12210", "submitter": "George Philipp", "authors": "George Philipp", "title": "The Nonlinearity Coefficient - A Practical Guide to Neural Architecture\n  Design", "comments": "This work is based on the PhD thesis with the same name, author, year\n  and institution. Both works may be cited interchangeably", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In essence, a neural network is an arbitrary differentiable, parametrized\nfunction. Choosing a neural network architecture for any task is as complex as\nsearching the space of those functions. For the last few years, 'neural\narchitecture design' has been largely synonymous with 'neural architecture\nsearch' (NAS), i.e. brute-force, large-scale search. NAS has yielded\nsignificant gains on practical tasks. However, NAS methods end up searching for\na local optimum in architecture space in a small neighborhood around\narchitectures that often go back decades, based on CNN or LSTM.\n  In this work, we present a different and complementary approach to\narchitecture design, which we term 'zero-shot architecture design' (ZSAD). We\ndevelop methods that can predict, without any training, whether an architecture\nwill achieve a relatively high test or training error on a task after training.\nWe then go on to explain the error in terms of the architecture definition\nitself and develop tools for modifying the architecture based on this\nexplanation. This confers an unprecedented level of control on the deep\nlearning practitioner. They can make informed design decisions before the first\nline of code is written, even for tasks for which no prior art exists.\n  Our first major contribution is to show that the 'degree of nonlinearity' of\na neural architecture is a key causal driver behind its performance, and a\nprimary aspect of the architecture's model complexity. We introduce the\n'nonlinearity coefficient' (NLC), a scalar metric for measuring nonlinearity.\nVia extensive empirical study, we show that the value of the NLC in the\narchitecture's randomly initialized state before training is a powerful\npredictor of test error after training and that attaining a right-sized NLC is\nessential for attaining an optimal test error. The NLC is also conceptually\nsimple, well-defined for any feedforward network, easy and cheap to compute,\nhas extensive theoretical, empirical and conceptual grounding, follows\ninstructively from the architecture definition, and can be easily controlled\nvia our 'nonlinearity normalization' algorithm. We argue that the NLC is the\nmost powerful scalar statistic for architecture design specifically and neural\nnetwork analysis in general. Our analysis is fueled by mean field theory, which\nwe use to uncover the 'meta-distribution' of layers.\n  Beyond the NLC, we uncover and flesh out a range of metrics and properties\nthat have a significant explanatory influence on test and training error. We go\non to explain the majority of the error variation across a wide range of\nrandomly generated architectures with these metrics and properties. We compile\nour insights into a practical guide for architecture designers, which we argue\ncan significantly shorten the trial-and-error phase of deep learning\ndeployment.\n  Our results are grounded in an experimental protocol that exceeds that of the\nvast majority of other deep learning studies in terms of carefulness and rigor.\nWe study the impact of e.g. dataset, learning rate, floating-point precision,\nloss function, statistical estimation error and batch inter-dependency on\nperformance and other key properties. We promote research practices that we\nbelieve can significantly accelerate progress in architecture design research.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:47:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Philipp", "George", ""]]}, {"id": "2105.12221", "submitter": "Berfin Simsek Mrs.", "authors": "Berfin \\c{S}im\\c{s}ek, Fran\\c{c}ois Ged, Arthur Jacot, Francesco\n  Spadaro, Cl\\'ement Hongler, Wulfram Gerstner, Johanni Brea", "title": "Geometry of the Loss Landscape in Overparameterized Neural Networks:\n  Symmetries and Invariances", "comments": "To appear at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study how permutation symmetries in overparameterized multi-layer neural\nnetworks generate `symmetry-induced' critical points. Assuming a network with $\nL $ layers of minimal widths $ r_1^*, \\ldots, r_{L-1}^* $ reaches a zero-loss\nminimum at $ r_1^*! \\cdots r_{L-1}^*! $ isolated points that are permutations\nof one another, we show that adding one extra neuron to each layer is\nsufficient to connect all these previously discrete minima into a single\nmanifold. For a two-layer overparameterized network of width $ r^*+ h =: m $ we\nexplicitly describe the manifold of global minima: it consists of $ T(r^*, m) $\naffine subspaces of dimension at least $ h $ that are connected to one another.\nFor a network of width $m$, we identify the number $G(r,m)$ of affine subspaces\ncontaining only symmetry-induced critical points that are related to the\ncritical points of a smaller network of width $r<r^*$. Via a combinatorial\nanalysis, we derive closed-form formulas for $ T $ and $ G $ and show that the\nnumber of symmetry-induced critical subspaces dominates the number of affine\nsubspaces forming the global minima manifold in the mildly overparameterized\nregime (small $ h $) and vice versa in the vastly overparameterized regime ($h\n\\gg r^*$). Our results provide new insights into the minimization of the\nnon-convex loss function of overparameterized neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 21:19:07 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["\u015eim\u015fek", "Berfin", ""], ["Ged", "Fran\u00e7ois", ""], ["Jacot", "Arthur", ""], ["Spadaro", "Francesco", ""], ["Hongler", "Cl\u00e9ment", ""], ["Gerstner", "Wulfram", ""], ["Brea", "Johanni", ""]]}, {"id": "2105.12237", "submitter": "Yatong Bai", "authors": "Yatong Bai, Tanmay Gautam, Yu Gai, Somayeh Sojoudi", "title": "Practical Convex Formulation of Robust One-hidden-layer Neural Network\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that the training of a one-hidden-layer, scalar-output\nfully-connected ReLU neural network can be reformulated as a finite-dimensional\nconvex program. Unfortunately, the scale of such a convex program grows\nexponentially in data size. In this work, we prove that a stochastic procedure\nwith a linear complexity well approximates the exact formulation. Moreover, we\nderive a convex optimization approach to efficiently solve the \"adversarial\ntraining\" problem, which trains neural networks that are robust to adversarial\ninput perturbations. Our method can be applied to binary classification and\nregression, and provides an alternative to the current adversarial training\nmethods, such as Fast Gradient Sign Method (FGSM) and Projected Gradient\nDescent (PGD). We demonstrate in experiments that the proposed method achieves\na noticeably better adversarial robustness and performance than the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:06:27 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bai", "Yatong", ""], ["Gautam", "Tanmay", ""], ["Gai", "Yu", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "2105.12238", "submitter": "Benjamin Jones", "authors": "Benjamin Jones, Dalton Hildreth, Duowen Chen, Ilya Baran, Vova Kim,\n  Adriana Schulz", "title": "SB-GCN: Structured BREP Graph Convolutional Network for Automatic Mating\n  of CAD Assemblies", "comments": "16 pages, 17 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assembly modeling is a core task of computer aided design (CAD), comprising\naround one third of the work in a CAD workflow. Optimizing this process\ntherefore represents a huge opportunity in the design of a CAD system, but\ncurrent research of assembly based modeling is not directly applicable to\nmodern CAD systems because it eschews the dominant data structure of modern\nCAD: parametric boundary representations (BREPs). CAD assembly modeling defines\nassemblies as a system of pairwise constraints, called mates, between parts,\nwhich are defined relative to BREP topology rather than in world coordinates\ncommon to existing work. We propose SB-GCN, a representation learning scheme on\nBREPs that retains the topological structure of parts, and use these learned\nrepresentations to predict CAD type mates. To train our system, we compiled the\nfirst large scale dataset of BREP CAD assemblies, which we are releasing along\nwith benchmark mate prediction tasks. Finally, we demonstrate the compatibility\nof our model with an existing commercial CAD system by building a tool that\nassists users in mate creation by suggesting mate completions, with 72.2%\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:07:55 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Jones", "Benjamin", ""], ["Hildreth", "Dalton", ""], ["Chen", "Duowen", ""], ["Baran", "Ilya", ""], ["Kim", "Vova", ""], ["Schulz", "Adriana", ""]]}, {"id": "2105.12245", "submitter": "Alain Rossier", "authors": "Alain-Sam Cohen, Rama Cont, Alain Rossier, Renyuan Xu", "title": "Scaling Properties of Deep Residual Networks", "comments": "Published at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) have displayed impressive results in pattern\nrecognition and, recently, have garnered considerable theoretical interest due\nto a perceived link with neural ordinary differential equations (neural ODEs).\nThis link relies on the convergence of network weights to a smooth function as\nthe number of layers increases. We investigate the properties of weights\ntrained by stochastic gradient descent and their scaling with network depth\nthrough detailed numerical experiments. We observe the existence of scaling\nregimes markedly different from those assumed in neural ODE literature.\nDepending on certain features of the network architecture, such as the\nsmoothness of the activation function, one may obtain an alternative ODE limit,\na stochastic differential equation or neither of these. These findings cast\ndoubts on the validity of the neural ODE model as an adequate asymptotic\ndescription of deep ResNets and point to an alternative class of differential\nequations as a better description of the deep network limit.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:31:30 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 13:46:14 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Cohen", "Alain-Sam", ""], ["Cont", "Rama", ""], ["Rossier", "Alain", ""], ["Xu", "Renyuan", ""]]}, {"id": "2105.12247", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Graph Self Supervised Learning: the BT, the HSIC, and the VICReg", "comments": "Paper Accepted in the Weakly Supervised Representation Learning\n  Workshop, IJCAI 2021 (IJCAI2021-WSRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning and pre-training strategies have developed over the\nlast few years especially for Convolutional Neural Networks (CNNs). Recently\napplication of such methods can also be noticed for Graph Neural Networks\n(GNNs) . In this paper, we have used a graph based self-supervised learning\nstrategy with different loss functions (Barlow Twins[Zbontar et al., 2021],\nHSIC[Tsai et al., 2021], VICReg[Bardes et al., 2021]) which have shown\npromising results when applied with CNNs previously. We have also proposed a\nhybrid loss function combining the advantages of VICReg and HSIC and called it\nas VICRegHSIC. The performance of these aforementioned methods have been\ncompared when applied to different datasets such as MUTAG, PROTEINS and\nIMDB-Binary. Moreover, the impact of different batch sizes, projector\ndimensions and data augmentation strategies have also been explored\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:34:19 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 04:51:26 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 03:18:43 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "2105.12249", "submitter": "Po-Kan Shih", "authors": "Po-Kan Shih", "title": "Bayesian Nonparametric Reinforcement Learning in LTE and Wi-Fi\n  Coexistence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the formation of next generation wireless communication, a growing\nnumber of new applications like internet of things, autonomous car, and drone\nis crowding the unlicensed spectrum. Licensed network such as the long-term\nevolution (LTE) also comes to the unlicensed spectrum for better providing\nhigh-capacity contents with low cost. However, LTE was not designed for sharing\nspectrum with others. A cooperation center for these networks is costly because\nthey possess heterogeneous properties and everyone can enter and leave the\nspectrum unrestrictedly, so the design will be challenging. Since it is\ninfeasible to incorporate potentially infinite scenarios with one unified\ndesign, an alternative solution is to let each network learn its own\ncoexistence policy. Previous solutions only work on fixed scenarios. In this\nwork a reinforcement learning algorithm is presented to cope with the\ncoexistence between Wi-Fi and LTE agents in 5 GHz unlicensed spectrum. The\ncoexistence problem was modeled as a decentralized partially observable Markov\ndecision process (Dec-POMDP) and Bayesian approach was adopted for policy\nlearning with nonparametric prior to accommodate the uncertainty of policy for\ndifferent agents. A fairness measure was introduced in the reward function to\nencourage fair sharing between agents. The reinforcement learning was turned\ninto an optimization problem by transforming the value function as likelihood\nand variational inference for posterior approximation. Simulation results\ndemonstrate that this algorithm can reach high value with compact policy\nrepresentations, and stay computationally efficient when applying to agent set.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:40:44 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 02:14:17 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shih", "Po-Kan", ""]]}, {"id": "2105.12254", "submitter": "Deepak-George Thomas", "authors": "Deepak-George Thomas, Daniil Olshanskyi, Karter Krueger, Tichakorn\n  Wongpiromsarn, Ali Jannesari", "title": "Interpretable UAV Collision Avoidance using Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The significant components of any successful autonomous flight system are\ntask completion and collision avoidance. Most deep learning algorithms\nsuccessfully execute these aspects under the environment and conditions they\nare trained. However, they fail when subjected to novel environments. This\npaper presents an autonomous multi-rotor flight algorithm, using Deep\nReinforcement Learning augmented with Self-Attention Models, that can\neffectively reason when subjected to varying inputs. In addition to their\nreasoning ability, they are also interpretable, enabling it to be used under\nreal-world conditions. We have tested our algorithm under different weather\nconditions and environments and found it robust compared to conventional Deep\nReinforcement Learning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 23:21:54 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 23:10:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Thomas", "Deepak-George", ""], ["Olshanskyi", "Daniil", ""], ["Krueger", "Karter", ""], ["Wongpiromsarn", "Tichakorn", ""], ["Jannesari", "Ali", ""]]}, {"id": "2105.12256", "submitter": "Mathew Schwartz", "authors": "Mathew Schwartz, Tomer Weiss, Esra Ataer-Cansizoglu, Jae-Woo Choi", "title": "Style Similarity as Feedback for Product Design", "comments": "15 pages, 9 figures, interdisciplinary book chapter on using computer\n  vision and style similarity for industrial design", "journal-ref": "In: Lee JH. (eds) A New Perspective of Cultural DNA. KAIST\n  Research Series. Springer, Singapore (2021)", "doi": "10.1007/978-981-15-7707-9_3", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matching and recommending products is beneficial for both customers and\ncompanies. With the rapid increase in home goods e-commerce, there is an\nincreasing demand for quantitative methods for providing such recommendations\nfor millions of products. This approach is facilitated largely by online stores\nsuch as Amazon and Wayfair, in which the goal is to maximize overall sales.\nInstead of focusing on overall sales, we take a product design perspective, by\nemploying big-data analysis for determining the design qualities of a highly\nrecommended product. Specifically, we focus on the visual style compatibility\nof such products. We build off previous work which implemented a style-based\nsimilarity metric for thousands of furniture products. Using analysis and\nvisualization, we extract attributes of furniture products that are highly\ncompatible style-wise. We propose a designer in-the-loop workflow that mirrors\nmethods of displaying similar products to consumers browsing e-commerce\nwebsites. Our findings are useful when designing new products, since they\nprovide insight regarding what furniture will be strongly compatible across\nmultiple styles, and hence, more likely to be recommended.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 23:30:29 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Schwartz", "Mathew", ""], ["Weiss", "Tomer", ""], ["Ataer-Cansizoglu", "Esra", ""], ["Choi", "Jae-Woo", ""]]}, {"id": "2105.12257", "submitter": "Antoine Bodin", "authors": "Antoine Bodin, Nicolas Macris", "title": "Rank-one matrix estimation: analytic time evolution of gradient descent\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a rank-one symmetric matrix corrupted by additive noise. The\nrank-one matrix is formed by an $n$-component unknown vector on the sphere of\nradius $\\sqrt{n}$, and we consider the problem of estimating this vector from\nthe corrupted matrix in the high dimensional limit of $n$ large, by gradient\ndescent for a quadratic cost function on the sphere. Explicit formulas for the\nwhole time evolution of the overlap between the estimator and unknown vector,\nas well as the cost, are rigorously derived. In the long time limit we recover\nthe well known spectral phase transition, as a function of the signal-to-noise\nratio. The explicit formulas also allow to point out interesting transient\nfeatures of the time evolution. Our analysis technique is based on recent\nprogress in random matrix theory and uses local versions of the semi-circle\nlaw.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 23:31:08 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bodin", "Antoine", ""], ["Macris", "Nicolas", ""]]}, {"id": "2105.12271", "submitter": "Yu Wang", "authors": "Yu Wang and Alfred Hero", "title": "SG-PALM: a Fast Physically Interpretable Tensor Graphical Model", "comments": "Accepted in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new graphical model inference procedure, called SG-PALM, for\nlearning conditional dependency structure of high-dimensional tensor-variate\ndata. Unlike most other tensor graphical models the proposed model is\ninterpretable and computationally scalable to high dimension. Physical\ninterpretability follows from the Sylvester generative (SG) model on which\nSG-PALM is based: the model is exact for any observation process that is a\nsolution of a partial differential equation of Poisson type. Scalability\nfollows from the fast proximal alternating linearized minimization (PALM)\nprocedure that SG-PALM uses during training. We establish that SG-PALM\nconverges linearly (i.e., geometric convergence rate) to a global optimum of\nits objective function. We demonstrate the scalability and accuracy of SG-PALM\nfor an important but challenging climate prediction problem: spatio-temporal\nforecasting of solar flares from multimodal imaging data.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 00:24:25 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Wang", "Yu", ""], ["Hero", "Alfred", ""]]}, {"id": "2105.12272", "submitter": "Ofir Nachum", "authors": "Ofir Nachum, Mengjiao Yang", "title": "Provable Representation Learning for Imitation with Contrastive Fourier\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In imitation learning, it is common to learn a behavior policy to match an\nunknown target policy via max-likelihood training on a collected set of target\ndemonstrations. In this work, we consider using offline experience datasets -\npotentially far from the target distribution - to learn low-dimensional state\nrepresentations that provably accelerate the sample-efficiency of downstream\nimitation learning. A central challenge in this setting is that the unknown\ntarget policy itself may not exhibit low-dimensional behavior, and so there is\na potential for the representation learning objective to alias states in which\nthe target policy acts differently. Circumventing this challenge, we derive a\nrepresentation learning objective which provides an upper bound on the\nperformance difference between the target policy and a lowdimensional policy\ntrained with max-likelihood, and this bound is tight regardless of whether the\ntarget policy itself exhibits low-dimensional structure. Moving to the\npracticality of our method, we show that our objective can be implemented as\ncontrastive learning, in which the transition dynamics are approximated by\neither an implicit energy-based model or, in some special cases, an implicit\nlinear model with representations given by random Fourier features. Experiments\non both tabular environments and high-dimensional Atari games provide\nquantitative evidence for the practical benefits of our proposed objective.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 00:31:30 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Nachum", "Ofir", ""], ["Yang", "Mengjiao", ""]]}, {"id": "2105.12277", "submitter": "Michael Taylor", "authors": "Michael Taylor, Sergey Bashkirov, Javier Fernandez Rico, Ike Toriyama,\n  Naoyuki Miyada, Hideki Yanagisawa, Kensaku Ishizuka", "title": "Learning Bipedal Robot Locomotion from Human Movement", "comments": "8 pages, 12 figures, 1 table. Accepted to ICRA 2021. For\n  supplementary video, see https://www.youtube.com/watch?v=k4oXPD4kVM0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching an anthropomorphic robot from human example offers the opportunity\nto impart humanlike qualities on its movement. In this work we present a\nreinforcement learning based method for teaching a real world bipedal robot to\nperform movements directly from human motion capture data. Our method\nseamlessly transitions from training in a simulation environment to executing\non a physical robot without requiring any real world training iterations or\noffline steps. To overcome the disparity in joint configurations between the\nrobot and the motion capture actor, our method incorporates motion re-targeting\ninto the training process. Domain randomization techniques are used to\ncompensate for the differences between the simulated and physical systems. We\ndemonstrate our method on an internally developed humanoid robot with movements\nranging from a dynamic walk cycle to complex balancing and waving. Our\ncontroller preserves the style imparted by the motion capture data and exhibits\ngraceful failure modes resulting in safe operation for the robot. This work was\nperformed for research purposes only.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 00:49:37 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Taylor", "Michael", ""], ["Bashkirov", "Sergey", ""], ["Rico", "Javier Fernandez", ""], ["Toriyama", "Ike", ""], ["Miyada", "Naoyuki", ""], ["Yanagisawa", "Hideki", ""], ["Ishizuka", "Kensaku", ""]]}, {"id": "2105.12290", "submitter": "Benjamin Leinwand", "authors": "Benjamin Leinwand, Vladas Pipiras", "title": "Block Dense Weighted Networks with Augmented Degree Correction", "comments": "43 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dense networks with weighted connections often exhibit a community like\nstructure, where although most nodes are connected to each other, different\npatterns of edge weights may emerge depending on each node's community\nmembership. We propose a new framework for generating and estimating dense\nweighted networks with potentially different connectivity patterns across\ndifferent communities. The proposed model relies on a particular class of\nfunctions which map individual node characteristics to the edges connecting\nthose nodes, allowing for flexibility while requiring a small number of\nparameters relative to the number of edges. By leveraging the estimation\ntechniques, we also develop a bootstrap methodology for generating new networks\non the same set of vertices, which may be useful in circumstances where\nmultiple data sets cannot be collected. Performance of these methods are\nanalyzed in theory, simulations, and real data.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 01:25:07 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Leinwand", "Benjamin", ""], ["Pipiras", "Vladas", ""]]}, {"id": "2105.12295", "submitter": "Stephen Casey", "authors": "Willis Hoke, Daniel Shea, and Stephen Casey", "title": "Operator Autoencoders: Learning Physical Operations on Encoded Molecular\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Molecular dynamics simulations produce data with complex nonlinear dynamics.\nIf the timestep behavior of such a dynamic system can be represented by a\nlinear operator, future states can be inferred directly without expensive\nsimulations. The use of an autoencoder in combination with a physical timestep\noperator allows both the relevant structural characteristics of the molecular\ngraphs and the underlying physics of the system to be isolated during the\ntraining process. In this work, we develop a pipeline for establishing\ngraph-structured representations of time-series volumetric data from molecular\ndynamics simulations. We then train an autoencoder to find nonlinear mappings\nto a latent space where future timesteps can be predicted through application\nof a linear operator trained in tandem with the autoencoder. Increasing the\ndimensionality of the autoencoder output is shown to improve the accuracy of\nthe physical timestep operator.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 01:55:01 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hoke", "Willis", ""], ["Shea", "Daniel", ""], ["Casey", "Stephen", ""]]}, {"id": "2105.12307", "submitter": "Venkata Vaishnav Tadiparthi", "authors": "Vaishnav Tadiparthi and Raktim Bhattacharya", "title": "Optimal Transport Based Refinement of Physics-Informed Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG cs.NE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a refinement strategy to the well-known\nPhysics-Informed Neural Networks (PINNs) for solving partial differential\nequations (PDEs) based on the concept of Optimal Transport (OT).\n  Conventional black-box PINNs solvers have been found to suffer from a host of\nissues: spectral bias in fully-connected architectures, unstable gradient\npathologies, as well as difficulties with convergence and accuracy.\n  Current network training strategies are agnostic to dimension sizes and rely\non the availability of powerful computing resources to optimize through a large\nnumber of collocation points.\n  This is particularly challenging when studying stochastic dynamical systems\nwith the Fokker-Planck-Kolmogorov Equation (FPKE), a second-order PDE which is\ntypically solved in high-dimensional state space.\n  While we focus exclusively on the stationary form of the FPKE, positivity and\nnormalization constraints on its solution make it all the more unfavorable to\nsolve directly using standard PINNs approaches.\n  To mitigate the above challenges, we present a novel training strategy for\nsolving the FPKE using OT-based sampling to supplement the existing PINNs\nframework.\n  It is an iterative approach that induces a network trained on a small dataset\nto add samples to its training dataset from regions where it nominally makes\nthe most error.\n  The new samples are found by solving a linear programming problem at every\niteration.\n  The paper is complemented by an experimental evaluation of the proposed\nmethod showing its applicability on a variety of stochastic systems with\nnonlinear dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 02:51:20 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 16:26:01 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tadiparthi", "Vaishnav", ""], ["Bhattacharya", "Raktim", ""]]}, {"id": "2105.12315", "submitter": "Koichi Saito", "authors": "Koichi Saito, Stefan Uhlich, Giorgio Fabbro, Yuki Mitsufuji", "title": "Training Speech Enhancement Systems with Noisy Speech Datasets", "comments": "5 pages, 3 figures, submitted to WASPAA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently, deep neural network (DNN)-based speech enhancement (SE) systems\nhave been used with great success. During training, such systems require clean\nspeech data - ideally, in large quantity with a variety of acoustic conditions,\nmany different speaker characteristics and for a given sampling rate (e.g.,\n48kHz for fullband SE). However, obtaining such clean speech data is not\nstraightforward - especially, if only considering publicly available datasets.\nAt the same time, a lot of material for automatic speech recognition (ASR) with\nthe desired acoustic/speaker/sampling rate characteristics is publicly\navailable except being clean, i.e., it also contains background noise as this\nis even often desired in order to have ASR systems that are noise-robust.\nHence, using such data to train SE systems is not straightforward. In this\npaper, we propose two improvements to train SE systems on noisy speech data.\nFirst, we propose several modifications of the loss functions, which make them\nrobust against noisy speech targets. In particular, computing the median over\nthe sample axis before averaging over time-frequency bins allows to use such\ndata. Furthermore, we propose a noise augmentation scheme for mixture-invariant\ntraining (MixIT), which allows using it also in such scenarios. For our\nexperiments, we use the Mozilla Common Voice dataset and we show that using our\nrobust loss function improves PESQ by up to 0.19 compared to a system trained\nin the traditional way. Similarly, for MixIT we can see an improvement of up to\n0.27 in PESQ when using our proposed noise augmentation.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 03:32:39 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Saito", "Koichi", ""], ["Uhlich", "Stefan", ""], ["Fabbro", "Giorgio", ""], ["Mitsufuji", "Yuki", ""]]}, {"id": "2105.12332", "submitter": "Peter Ondruska", "authors": "Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del\n  Pero, Blazej Osinski, Hugo Grimmett, Peter Ondruska", "title": "SimNet: Learning Reactive Self-driving Simulations from Real-world\n  Observations", "comments": "Published at 2021 International Conference on Robotics and Automation\n  (ICRA2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a simple end-to-end trainable machine learning\nsystem capable of realistically simulating driving experiences. This can be\nused for the verification of self-driving system performance without relying on\nexpensive and time-consuming road testing. In particular, we frame the\nsimulation problem as a Markov Process, leveraging deep neural networks to\nmodel both state distribution and transition function. These are trainable\ndirectly from the existing raw observations without the need for any\nhandcrafting in the form of plant or kinematic models. All that is needed is a\ndataset of historical traffic episodes. Our formulation allows the system to\nconstruct never seen scenes that unfold realistically reacting to the\nself-driving car's behaviour. We train our system directly from 1,000 hours of\ndriving logs and measure both realism, reactivity of the simulation as the two\nkey properties of the simulation. At the same time, we apply the method to\nevaluate the performance of a recently proposed state-of-the-art ML planning\nsystem trained from human driving logs. We discover this planning system is\nprone to previously unreported causal confusion issues that are difficult to\ntest by non-reactive simulation. To the best of our knowledge, this is the\nfirst work that directly merges highly realistic data-driven simulations with a\nclosed-loop evaluation for self-driving vehicles. We make the data, code, and\npre-trained models publicly available to further stimulate simulation\ndevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 05:14:23 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bergamini", "Luca", ""], ["Ye", "Yawei", ""], ["Scheel", "Oliver", ""], ["Chen", "Long", ""], ["Hu", "Chih", ""], ["Del Pero", "Luca", ""], ["Osinski", "Blazej", ""], ["Grimmett", "Hugo", ""], ["Ondruska", "Peter", ""]]}, {"id": "2105.12337", "submitter": "Peter Ondruska", "authors": "Long Chen, Lukas Platinsky, Stefanie Speichert, Blazej Osinski, Oliver\n  Scheel, Yawei Ye, Hugo Grimmett, Luca del Pero, Peter Ondruska", "title": "What data do we need for training an AV motion planner?", "comments": "Published at 2021 International Conference on Robotics and Automation\n  (ICRA2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate what grade of sensor data is required for training an\nimitation-learning-based AV planner on human expert demonstration.\nMachine-learned planners are very hungry for training data, which is usually\ncollected using vehicles equipped with the same sensors used for autonomous\noperation. This is costly and non-scalable. If cheaper sensors could be used\nfor collection instead, data availability would go up, which is crucial in a\nfield where data volume requirements are large and availability is small. We\npresent experiments using up to 1000 hours worth of expert demonstration and\nfind that training with 10x lower-quality data outperforms 1x AV-grade data in\nterms of planner performance. The important implication of this is that cheaper\nsensors can indeed be used. This serves to improve data access and democratize\nthe field of imitation-based motion planning. Alongside this, we perform a\nsensitivity analysis of planner performance as a function of perception range,\nfield-of-view, accuracy, and data volume, and the reason why lower-quality data\nstill provide good planning results.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 05:37:12 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chen", "Long", ""], ["Platinsky", "Lukas", ""], ["Speichert", "Stefanie", ""], ["Osinski", "Blazej", ""], ["Scheel", "Oliver", ""], ["Ye", "Yawei", ""], ["Grimmett", "Hugo", ""], ["del Pero", "Luca", ""], ["Ondruska", "Peter", ""]]}, {"id": "2105.12342", "submitter": "Andrew Lim", "authors": "Jun-ya Gotoh, Michael Jong Kim, Andrew E.B. Lim", "title": "A data-driven approach to beating SAA out-of-sample", "comments": "20 pages, 2 Figures, 2 page Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY econ.EM eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While solutions of Distributionally Robust Optimization (DRO) problems can\nsometimes have a higher out-of-sample expected reward than the Sample Average\nApproximation (SAA), there is no guarantee. In this paper, we introduce the\nclass of Distributionally Optimistic Optimization (DOO) models, and show that\nit is always possible to \"beat\" SAA out-of-sample if we consider not just\nworst-case (DRO) models but also best-case (DOO) ones. We also show, however,\nthat this comes at a cost: Optimistic solutions are more sensitive to model\nerror than either worst-case or SAA optimizers, and hence are less robust.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 06:10:12 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 11:49:24 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Gotoh", "Jun-ya", ""], ["Kim", "Michael Jong", ""], ["Lim", "Andrew E. B.", ""]]}, {"id": "2105.12356", "submitter": "Michelangelo Conserva", "authors": "Michelangelo Conserva, Marc Peter Deisenroth, K S Sesh Kumar", "title": "Submodular Kernels for Efficient Rankings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many algorithms for ranked data become computationally intractable as the\nnumber of objects grows due to complex geometric structure induced by rankings.\nAn additional challenge is posed by partial rankings, i.e. rankings in which\nthe preference is only known for a subset of all objects. For these reasons,\nstate-of-the-art methods cannot scale to real-world applications, such as\nrecommender systems. We address this challenge by exploiting geometric\nstructure of ranked data and additional available information about the objects\nto derive a submodular kernel for ranking. The submodular kernel combines the\nefficiency of submodular optimization with the theoretical properties of\nkernel-based methods. We demonstrate that the submodular kernel drastically\nreduces the computational cost compared to state-of-the-art kernels and scales\nwell to large datasets while attaining good empirical performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 06:42:06 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Conserva", "Michelangelo", ""], ["Deisenroth", "Marc Peter", ""], ["Kumar", "K S Sesh", ""]]}, {"id": "2105.12357", "submitter": "Alfred Laugros", "authors": "Alfred Laugros and Alice Caplier and Matthieu Ospici", "title": "Using the Overlapping Score to Improve Corruption Benchmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks are sensitive to various corruptions that usually occur in\nreal-world applications such as blurs, noises, low-lighting conditions, etc. To\nestimate the robustness of neural networks to these common corruptions, we\ngenerally use a group of modeled corruptions gathered into a benchmark.\nUnfortunately, no objective criterion exists to determine whether a benchmark\nis representative of a large diversity of independent corruptions. In this\npaper, we propose a metric called corruption overlapping score, which can be\nused to reveal flaws in corruption benchmarks. Two corruptions overlap when the\nrobustnesses of neural networks to these corruptions are correlated. We argue\nthat taking into account overlappings between corruptions can help to improve\nexisting benchmarks or build better ones.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 06:42:54 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Laugros", "Alfred", ""], ["Caplier", "Alice", ""], ["Ospici", "Matthieu", ""]]}, {"id": "2105.12358", "submitter": "Yahya Sattar", "authors": "Zhe Du, Yahya Sattar, Davoud Ataee Tarzanagh, Laura Balzano, Samet\n  Oymak and Necmiye Ozay", "title": "Certainty Equivalent Quadratic Control for Markov Jump Systems", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world control applications often involve complex dynamics subject to\nabrupt changes or variations. Markov jump linear systems (MJS) provide a rich\nframework for modeling such dynamics. Despite an extensive history, theoretical\nunderstanding of parameter sensitivities of MJS control is somewhat lacking.\nMotivated by this, we investigate robustness aspects of certainty equivalent\nmodel-based optimal control for MJS with quadratic cost function. Given the\nuncertainty in the system matrices and in the Markov transition matrix is\nbounded by $\\epsilon$ and $\\eta$ respectively, robustness results are\nestablished for (i) the solution to coupled Riccati equations and (ii) the\noptimal cost, by providing explicit perturbation bounds which decay as\n$\\mathcal{O}(\\epsilon + \\eta)$ and $\\mathcal{O}((\\epsilon + \\eta)^2)$\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 06:45:47 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Du", "Zhe", ""], ["Sattar", "Yahya", ""], ["Tarzanagh", "Davoud Ataee", ""], ["Balzano", "Laura", ""], ["Oymak", "Samet", ""], ["Ozay", "Necmiye", ""]]}, {"id": "2105.12364", "submitter": "Nawshad Farruque", "authors": "Nawshad Farruque, Chenyang Huang, Osmar Zaiane, Randy Goebel", "title": "Basic and Depression Specific Emotion Identification in Tweets:\n  Multi-label Classification Experiments", "comments": "Accepted at CICLing, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present empirical analysis on basic and depression specific\nmulti-emotion mining in Tweets with the help of state of the art multi-label\nclassifiers. We choose our basic emotions from a hybrid emotion model\nconsisting of the common emotions from four highly regarded psychological\nmodels of emotions. Moreover, we augment that emotion model with new emotion\ncategories because of their importance in the analysis of depression. Most of\nthose additional emotions have not been used in previous emotion mining\nresearch. Our experimental analyses show that a cost sensitive RankSVM\nalgorithm and a Deep Learning model are both robust, measured by both Macro\nF-measures and Micro F-measures. This suggests that these algorithms are\nsuperior in addressing the widely known data imbalance problem in multi-label\nlearning. Moreover, our application of Deep Learning performs the best, giving\nit an edge in modeling deep semantic features of our extended emotional\ncategories.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 07:13:50 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 09:08:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Farruque", "Nawshad", ""], ["Huang", "Chenyang", ""], ["Zaiane", "Osmar", ""], ["Goebel", "Randy", ""]]}, {"id": "2105.12371", "submitter": "Yijiang Lian", "authors": "Yijiang Lian, Shuang Li, Chaobing Feng, YanFeng Zhu", "title": "Quotient Space-Based Keyword Retrieval in Sponsored Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synonymous keyword retrieval has become an important problem for sponsored\nsearch ever since major search engines relax the exact match product's matching\nrequirement to a synonymous level. Since the synonymous relations between\nqueries and keywords are quite scarce, the traditional information retrieval\nframework is inefficient in this scenario. In this paper, we propose a novel\nquotient space-based retrieval framework to address this problem. Considering\nthe synonymy among keywords as a mathematical equivalence relation, we can\ncompress the synonymous keywords into one representative, and the corresponding\nquotient space would greatly reduce the size of the keyword repository. Then an\nembedding-based retrieval is directly conducted between queries and the keyword\nrepresentatives. To mitigate the semantic gap of the quotient space-based\nretrieval, a single semantic siamese model is utilized to detect both the\nkeyword--keyword and query-keyword synonymous relations. The experiments show\nthat with our quotient space-based retrieval method, the synonymous keyword\nretrieving performance can be greatly improved in terms of memory cost and\nrecall efficiency. This method has been successfully implemented in Baidu's\nonline sponsored search system and has yielded a significant improvement in\nrevenue.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 07:27:54 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Lian", "Yijiang", ""], ["Li", "Shuang", ""], ["Feng", "Chaobing", ""], ["Zhu", "YanFeng", ""]]}, {"id": "2105.12374", "submitter": "Khadija Shaheen", "authors": "Khadija Shaheen, Muhammad Abdullah Hanif, Osman Hasan, Muhammad\n  Shafique", "title": "Continual Learning for Real-World Autonomous Systems: Algorithms,\n  Challenges and Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Continual learning is essential for all real-world applications, as frozen\npre-trained models cannot effectively deal with non-stationary data\ndistributions. The purpose of this study is to review the state-of-the-art\nmethods that allow continuous learning of computational models over time. We\nprimarily focus on the learning algorithms that perform continuous learning in\nan online fashion from considerably large (or infinite) sequential data and\nrequire substantially low computational and memory resources. We critically\nanalyze the key challenges associated with continual learning for autonomous\nreal-world systems and compare current methods in terms of computations,\nmemory, and network/model complexity. We also briefly describe the\nimplementations of continuous learning algorithms under three main autonomous\nsystems, i.e., self-driving vehicles, unmanned aerial vehicles, and robotics.\nThe learning methods of these autonomous systems and their strengths and\nlimitations are extensively explored in this article.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 07:38:20 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Shaheen", "Khadija", ""], ["Hanif", "Muhammad Abdullah", ""], ["Hasan", "Osman", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2105.12385", "submitter": "Allan Gr{\\o}nlund", "authors": "Allan Gr{\\o}nlund and Jonas Tranberg", "title": "Learning to Detect Fortified Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution data models like grid terrain models made from LiDAR data are\na prerequisite for modern day Geographic Information Systems applications.\nBesides providing the foundation for the very accurate digital terrain models,\nLiDAR data is also extensively used to classify which parts of the considered\nsurface comprise relevant elements like water, buildings and vegetation. In\nthis paper we consider the problem of classifying which areas of a given\nsurface are fortified by for instance, roads, sidewalks, parking spaces, paved\ndriveways and terraces. We consider using LiDAR data and orthophotos, combined\nand alone, to show how well the modern machine learning algorithms Gradient\nBoosted Trees and Convolutional Neural Networks are able to detect fortified\nareas on large real world data. The LiDAR data features, in particular the\nintensity feature that measures the signal strength of the return, that we\nconsider in this project are heavily dependent on the actual LiDAR sensor that\nmade the measurement. This is highly problematic, in particular for the\ngeneralisation capability of pattern matching algorithms, as this means that\ndata features for test data may be very different from the data the model is\ntrained on. We propose an algorithmic solution to this problem by designing a\nneural net embedding architecture that transforms data from all the different\nsensor systems into a new common representation that works as well as if the\ntraining data and test data originated from the same sensor. The final\nalgorithm result has an accuracy above 96 percent, and an AUC score above 0.99.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:03:42 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Gr\u00f8nlund", "Allan", ""], ["Tranberg", "Jonas", ""]]}, {"id": "2105.12395", "submitter": "Khaled Koutini", "authors": "Khaled Koutini, Hamid Eghbal-zadeh, Gerhard Widmer", "title": "Receptive Field Regularization Techniques for Audio Classification and\n  Tagging with Deep Convolutional Neural Networks", "comments": "Accepted in IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. Code available: https://github.com/kkoutini/cpjku_dcase20", "journal-ref": null, "doi": "10.1109/TASLP.2021.3082307", "report-no": null, "categories": "cs.SD cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the performance of variants of well-known\nConvolutional Neural Network (CNN) architectures on different audio tasks. We\nshow that tuning the Receptive Field (RF) of CNNs is crucial to their\ngeneralization. An insufficient RF limits the CNN's ability to fit the training\ndata. In contrast, CNNs with an excessive RF tend to over-fit the training data\nand fail to generalize to unseen testing data. As state-of-the-art CNN\narchitectures-in computer vision and other domains-tend to go deeper in terms\nof number of layers, their RF size increases and therefore they degrade in\nperformance in several audio classification and tagging tasks. We study\nwell-known CNN architectures and how their building blocks affect their\nreceptive field. We propose several systematic approaches to control the RF of\nCNNs and systematically test the resulting architectures on different audio\nclassification and tagging tasks and datasets. The experiments show that\nregularizing the RF of CNNs using our proposed approaches can drastically\nimprove the generalization of models, out-performing complex architectures and\npre-trained models on larger datasets. The proposed CNNs achieve\nstate-of-the-art results in multiple tasks, from acoustic scene classification\nto emotion and theme detection in music to instrument recognition, as\ndemonstrated by top ranks in several pertinent challenges (DCASE, MediaEval).\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:36:29 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Koutini", "Khaled", ""], ["Eghbal-zadeh", "Hamid", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2105.12419", "submitter": "Heng Chang", "authors": "Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng\n  Cui, Xin Wang, Wenwu Zhu, Junzhou Huang", "title": "Adversarial Attack Framework on Graph Embedding Models with Limited\n  Knowledge", "comments": "Journal extension of GF-Attack. arXiv admin note: text overlap with\n  arXiv:1908.01297", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of the graph embedding model in both academic and industry\nareas, the robustness of graph embedding against adversarial attack inevitably\nbecomes a crucial problem in graph learning. Existing works usually perform the\nattack in a white-box fashion: they need to access the predictions/labels to\nconstruct their adversarial loss. However, the inaccessibility of\npredictions/labels makes the white-box attack impractical to a real graph\nlearning system. This paper promotes current frameworks in a more general and\nflexible sense -- we demand to attack various kinds of graph embedding models\nwith black-box driven. We investigate the theoretical connections between graph\nsignal processing and graph embedding models and formulate the graph embedding\nmodel as a general graph signal process with a corresponding graph filter.\nTherefore, we design a generalized adversarial attacker: GF-Attack. Without\naccessing any labels and model predictions, GF-Attack can perform the attack\ndirectly on the graph filter in a black-box fashion. We further prove that\nGF-Attack can perform an effective attack without knowing the number of layers\nof graph embedding models. To validate the generalization of GF-Attack, we\nconstruct the attacker on four popular graph embedding models. Extensive\nexperiments validate the effectiveness of GF-Attack on several benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:18:58 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chang", "Heng", ""], ["Rong", "Yu", ""], ["Xu", "Tingyang", ""], ["Huang", "Wenbing", ""], ["Zhang", "Honglei", ""], ["Cui", "Peng", ""], ["Wang", "Xin", ""], ["Zhu", "Wenwu", ""], ["Huang", "Junzhou", ""]]}, {"id": "2105.12427", "submitter": "Alex Serban", "authors": "Alex Serban, Erik Poll and Joost Visser", "title": "Deep Repulsive Prototypes for Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While many defences against adversarial examples have been proposed, finding\nrobust machine learning models is still an open problem. The most compelling\ndefence to date is adversarial training and consists of complementing the\ntraining data set with adversarial examples. Yet adversarial training severely\nimpacts training time and depends on finding representative adversarial\nsamples. In this paper we propose to train models on output spaces with large\nclass separation in order to gain robustness without adversarial training. We\nintroduce a method to partition the output space into class prototypes with\nlarge separation and train models to preserve it. Experimental results shows\nthat models trained with these prototypes -- which we call deep repulsive\nprototypes -- gain robustness competitive with adversarial training, while also\npreserving more accuracy on natural samples. Moreover, the models are more\nresilient to large perturbation sizes. For example, we obtained over 50%\nrobustness for CIFAR-10, with 92% accuracy on natural samples and over 20%\nrobustness for CIFAR-100, with 71% accuracy on natural samples without\nadversarial training. For both data sets, the models preserved robustness\nagainst large perturbations better than adversarially trained models.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:30:28 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Serban", "Alex", ""], ["Poll", "Erik", ""], ["Visser", "Joost", ""]]}, {"id": "2105.12433", "submitter": "Michael Morris", "authors": "Michael Morris, Peter Hayes, Ingemar J. Cox, Vasileios Lampos", "title": "Estimating the Uncertainty of Neural Network Forecasts for Influenza\n  Prevalence Using Web Search Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Influenza is an infectious disease with the potential to become a pandemic,\nand hence, forecasting its prevalence is an important undertaking for planning\nan effective response. Research has found that web search activity can be used\nto improve influenza models. Neural networks (NN) can provide state-of-the-art\nforecasting accuracy but do not commonly incorporate uncertainty in their\nestimates, something essential for using them effectively during decision\nmaking. In this paper, we demonstrate how Bayesian Neural Networks (BNNs) can\nbe used to both provide a forecast and a corresponding uncertainty without\nsignificant loss in forecasting accuracy compared to traditional NNs. Our\nmethod accounts for two sources of uncertainty: data and model uncertainty,\narising due to measurement noise and model specification, respectively.\nExperiments are conducted using 14 years of data for England, assessing the\nmodel's accuracy over the last 4 flu seasons in this dataset. We evaluate the\nperformance of different models including competitive baselines with\nconventional metrics as well as error functions that incorporate uncertainty\nestimates. Our empirical analysis indicates that considering both sources of\nuncertainty simultaneously is superior to considering either one separately. We\nalso show that a BNN with recurrent layers that models both sources of\nuncertainty yields superior accuracy for these metrics for forecasting horizons\ngreater than 7 days.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:45:23 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Morris", "Michael", ""], ["Hayes", "Peter", ""], ["Cox", "Ingemar J.", ""], ["Lampos", "Vasileios", ""]]}, {"id": "2105.12434", "submitter": "Mohamad Wehbi", "authors": "Mohamad Wehbi, Tim Hamann, Jens Barth, Peter Kaempf, Dario Zanca, and\n  Bjoern Eskofier", "title": "Towards an IMU-based Pen Online Handwriting Recognizer", "comments": "Accepted at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most online handwriting recognition systems require the use of specific\nwriting surfaces to extract positional data. In this paper we present a online\nhandwriting recognition system for word recognition which is based on inertial\nmeasurement units (IMUs) for digitizing text written on paper. This is obtained\nby means of a sensor-equipped pen that provides acceleration, angular velocity,\nand magnetic forces streamed via Bluetooth. Our model combines convolutional\nand bidirectional LSTM networks, and is trained with the Connectionist Temporal\nClassification loss that allows the interpretation of raw sensor data into\nwords without the need of sequence segmentation. We use a dataset of words\ncollected using multiple sensor-enhanced pens and evaluate our model on\ndistinct test sets of seen and unseen words achieving a character error rate of\n17.97% and 17.08%, respectively, without the use of a dictionary or language\nmodel\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:47:19 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Wehbi", "Mohamad", ""], ["Hamann", "Tim", ""], ["Barth", "Jens", ""], ["Kaempf", "Peter", ""], ["Zanca", "Dario", ""], ["Eskofier", "Bjoern", ""]]}, {"id": "2105.12436", "submitter": "Chi Zhang", "authors": "Chi Zhang (1), Christian Berger (1), Marco Dozza (2) ((1) Department\n  of Computer Science and Engineering, University of Gothenburg, Gothenburg,\n  Sweden, (2) Department of Maritime Sciences and Mechanics, Chalmers\n  University of Technology, Gothenburg, Sweden)", "title": "Social-IWSTCNN: A Social Interaction-Weighted Spatio-Temporal\n  Convolutional Neural Network for Pedestrian Trajectory Prediction in Urban\n  Traffic Scenarios", "comments": "8 pages, 4 figures. Accepted in IEEE Intelligent Vehicles Symposium\n  (IV), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pedestrian trajectory prediction in urban scenarios is essential for\nautomated driving. This task is challenging because the behavior of pedestrians\nis influenced by both their own history paths and the interactions with others.\nPrevious research modeled these interactions with pooling mechanisms or\naggregating with hand-crafted attention weights. In this paper, we present the\nSocial Interaction-Weighted Spatio-Temporal Convolutional Neural Network\n(Social-IWSTCNN), which includes both the spatial and the temporal features. We\npropose a novel design, namely the Social Interaction Extractor, to learn the\nspatial and social interaction features of pedestrians. Most previous works\nused ETH and UCY datasets which include five scenes but do not cover urban\ntraffic scenarios extensively for training and evaluation. In this paper, we\nuse the recently released large-scale Waymo Open Dataset in urban traffic\nscenarios, which includes 374 urban training scenes and 76 urban testing scenes\nto analyze the performance of our proposed algorithm in comparison to the\nstate-of-the-art (SOTA) models. The results show that our algorithm outperforms\nSOTA algorithms such as Social-LSTM, Social-GAN, and Social-STGCNN on both\nAverage Displacement Error (ADE) and Final Displacement Error (FDE).\nFurthermore, our Social-IWSTCNN is 54.8 times faster in data pre-processing\nspeed, and 4.7 times faster in total test speed than the current best SOTA\nalgorithm Social-STGCNN.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:53:19 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhang", "Chi", ""], ["Berger", "Christian", ""], ["Dozza", "Marco", ""]]}, {"id": "2105.12441", "submitter": "Akis Linardos", "authors": "Akis Linardos, Matthias K\\\"ummerer, Ori Press, Matthias Bethge", "title": "Calibrated prediction in and out-of-domain for state-of-the-art saliency\n  modeling", "comments": "Joint first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since 2014 transfer learning has become the key driver for the improvement of\nspatial saliency prediction; however, with stagnant progress in the last 3-5\nyears. We conduct a large-scale transfer learning study which tests different\nImageNet backbones, always using the same read out architecture and learning\nprotocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze\nII with ResNet50 features we improve the performance on saliency prediction\nfrom 78% to 85%. However, as we continue to test better ImageNet models as\nbackbones (such as EfficientNetB5) we observe no additional improvement on\nsaliency prediction. By analyzing the backbones further, we find that\ngeneralization to other datasets differs substantially, with models being\nconsistently overconfident in their fixation predictions. We show that by\ncombining multiple backbones in a principled manner a good confidence\ncalibration on unseen datasets can be achieved. This yields a significant leap\nin benchmark performance in and out-of-domain with a 15 percent point\nimprovement over DeepGaze II to 93% on MIT1003, marking a new state of the art\non the MIT/Tuebingen Saliency Benchmark in all available metrics (AUC: 88.3%,\nsAUC: 79.4%, CC: 82.4%).\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:59:56 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 15:21:50 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Linardos", "Akis", ""], ["K\u00fcmmerer", "Matthias", ""], ["Press", "Ori", ""], ["Bethge", "Matthias", ""]]}, {"id": "2105.12478", "submitter": "Tyler McCormick", "authors": "Tyler McCormick", "title": "The \"given data\" paradigm undermines both cultures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breiman organizes \"Statistical modeling: The two cultures\" around a simple\nvisual. Data, to the far right, are compelled into a \"black box\" with an arrow\nand then catapulted left by a second arrow, having been transformed into an\noutput. Breiman then posits two interpretations of this visual as encapsulating\na distinction between two cultures in statistics. The divide, he argues is\nabout what happens in the \"black box.\" In this comment, I argue for a broader\nperspective on statistics and, in doing so, elevate questions from \"before\" and\n\"after\" the box as fruitful areas for statistical innovation and practice.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:22:06 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["McCormick", "Tyler", ""]]}, {"id": "2105.12479", "submitter": "Celia Cintas", "authors": "Celia Cintas, Skyler Speakman, Girmaw Abebe Tadesse, Victor Akinwande,\n  Edward McFowland III, Komminist Weldemariam", "title": "Pattern Detection in the Activation Space for Identifying Synthesized\n  Content", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently achieved unprecedented\nsuccess in photo-realistic image synthesis from low-dimensional random noise.\nThe ability to synthesize high-quality content at a large scale brings\npotential risks as the generated samples may lead to misinformation that can\ncreate severe social, political, health, and business hazards. We propose\nSubsetGAN to identify generated content by detecting a subset of anomalous\nnode-activations in the inner layers of pre-trained neural networks. These\nnodes, as a group, maximize a non-parametric measure of divergence away from\nthe expected distribution of activations created from real data. This enable us\nto identify synthesised images without prior knowledge of their distribution.\nSubsetGAN efficiently scores subsets of nodes and returns the group of nodes\nwithin the pre-trained classifier that contributed to the maximum score. The\nclassifier can be a general fake classifier trained over samples from multiple\nsources or the discriminator network from different GANs. Our approach shows\nconsistently higher detection power than existing detection methods across\nseveral state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different\nproportions of generated content.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:28:36 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 08:40:27 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Cintas", "Celia", ""], ["Speakman", "Skyler", ""], ["Tadesse", "Girmaw Abebe", ""], ["Akinwande", "Victor", ""], ["McFowland", "Edward", "III"], ["Weldemariam", "Komminist", ""]]}, {"id": "2105.12485", "submitter": "Chen Lyu", "authors": "Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, Lei Lyu", "title": "TreeBERT: A Tree-Based Pre-Trained Model for Programming Language", "comments": "Accepted by UAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source code can be parsed into the abstract syntax tree (AST) based on\ndefined syntax rules. However, in pre-training, little work has considered the\nincorporation of tree structure into the learning process. In this paper, we\npresent TreeBERT, a tree-based pre-trained model for improving programming\nlanguage-oriented generation tasks. To utilize tree structure, TreeBERT\nrepresents the AST corresponding to the code as a set of composition paths and\nintroduces node position embedding. The model is trained by tree masked\nlanguage modeling (TMLM) and node order prediction (NOP) with a hybrid\nobjective. TMLM uses a novel masking strategy designed according to the tree's\ncharacteristics to help the model understand the AST and infer the missing\nsemantics of the AST. With NOP, TreeBERT extracts the syntactical structure by\nlearning the order constraints of nodes in AST. We pre-trained TreeBERT on\ndatasets covering multiple programming languages. On code summarization and\ncode documentation tasks, TreeBERT outperforms other pre-trained models and\nstate-of-the-art models designed for these tasks. Furthermore, TreeBERT\nperforms well when transferred to the pre-trained unseen programming language.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:36:43 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 02:34:01 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Jiang", "Xue", ""], ["Zheng", "Zhuoran", ""], ["Lyu", "Chen", ""], ["Li", "Liang", ""], ["Lyu", "Lei", ""]]}, {"id": "2105.12486", "submitter": "Petra Poklukar", "authors": "Petra Poklukar, Anastasia Varava, Danica Kragic", "title": "GeomCA: Geometric Evaluation of Data Representations", "comments": "ICML2021 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the quality of learned representations without relying on a\ndownstream task remains one of the challenges in representation learning. In\nthis work, we present Geometric Component Analysis (GeomCA) algorithm that\nevaluates representation spaces based on their geometric and topological\nproperties. GeomCA can be applied to representations of any dimension,\nindependently of the model that generated them. We demonstrate its\napplicability by analyzing representations obtained from a variety of\nscenarios, such as contrastive learning models, generative models and\nsupervised learning models.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:41:40 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Poklukar", "Petra", ""], ["Varava", "Anastasia", ""], ["Kragic", "Danica", ""]]}, {"id": "2105.12497", "submitter": "Ali Raza", "authors": "Ali Raza, Kim Phuc Tran, Ludovic Koehl and Shujun Li", "title": "Designing ECG Monitoring Healthcare System with Federated Transfer\n  Learning and Explainable AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning play a vital role in classifying different arrhythmias using\nthe electrocardiography (ECG) data. Nevertheless, training deep learning models\nnormally requires a large amount of data and it can lead to privacy concerns.\nUnfortunately, a large amount of healthcare data cannot be easily collected\nfrom a single silo. Additionally, deep learning models are like black-box, with\nno explainability of the predicted results, which is often required in clinical\nhealthcare. This limits the application of deep learning in real-world health\nsystems. In this paper, we design a new explainable artificial intelligence\n(XAI) based deep learning framework in a federated setting for ECG-based\nhealthcare applications. The federated setting is used to solve issues such as\ndata availability and privacy concerns. Furthermore, the proposed framework\nsetting effectively classifies arrhythmia's using an autoencoder and a\nclassifier, both based on a convolutional neural network (CNN). Additionally,\nwe propose an XAI-based module on top of the proposed classifier to explain the\nclassification results, which help clinical practitioners make quick and\nreliable decisions. The proposed framework was trained and tested using the\nMIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98%\nfor arrhythmia detection using noisy and clean data, respectively, with\nfive-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:59:44 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Raza", "Ali", ""], ["Tran", "Kim Phuc", ""], ["Koehl", "Ludovic", ""], ["Li", "Shujun", ""]]}, {"id": "2105.12508", "submitter": "Francesco Croce", "authors": "Francesco Croce, Matthias Hein", "title": "Adversarial robustness against multiple $l_p$-threat models at the price\n  of one and how to quickly fine-tune robust models to another threat model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) in order to achieve adversarial robustness wrt\nsingle $l_p$-threat models has been discussed extensively. However, for\nsafety-critical systems adversarial robustness should be achieved wrt all\n$l_p$-threat models simultaneously. In this paper we develop a simple and\nefficient training scheme to achieve adversarial robustness against the union\nof $l_p$-threat models. Our novel $l_1+l_\\infty$-AT scheme is based on\ngeometric considerations of the different $l_p$-balls and costs as much as\nnormal adversarial training against a single $l_p$-threat model. Moreover, we\nshow that using our $l_1+l_\\infty$-AT scheme one can fine-tune with just 3\nepochs any $l_p$-robust model (for $p \\in \\{1,2,\\infty\\}$) and achieve multiple\nnorm adversarial robustness. In this way we boost the previous state-of-the-art\nreported for multiple-norm robustness by more than $6\\%$ on CIFAR-10 and report\nup to our knowledge the first ImageNet models with multiple norm robustness.\nMoreover, we study the general transfer of adversarial robustness between\ndifferent threat models and in this way boost the previous SOTA\n$l_1$-robustness on CIFAR-10 by almost $10\\%$.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 12:20:47 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Croce", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "2105.12524", "submitter": "Caglar Demir", "authors": "Caglar Demir and Axel-Cyrille Ngonga Ngomo", "title": "Out-of-Vocabulary Entities in Link Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Knowledge graph embedding techniques are key to making knowledge graphs\namenable to the plethora of machine learning approaches based on vector\nrepresentations. Link prediction is often used as a proxy to evaluate the\nquality of these embeddings. Given that the creation of benchmarks for link\nprediction is a time-consuming endeavor, most work on the subject matter uses\nonly a few benchmarks. As benchmarks are crucial for the fair comparison of\nalgorithms, ensuring their quality is tantamount to providing a solid ground\nfor developing better solutions to link prediction and ipso facto embedding\nknowledge graphs. First studies of benchmarks pointed to limitations pertaining\nto information leaking from the development to the test fragments of some\nbenchmark datasets. We spotted a further common limitation of three of the\nbenchmarks commonly used for evaluating link prediction approaches:\nout-of-vocabulary entities in the test and validation sets. We provide an\nimplementation of an approach for spotting and removing such entities and\nprovide corrected versions of the datasets WN18RR, FB15K-237, and YAGO3-10. Our\nexperiments on the corrected versions of WN18RR, FB15K-237, and YAGO3-10\nsuggest that the measured performance of state-of-the-art approaches is altered\nsignificantly with p-values <1%, <1.4%, and <1%, respectively. Overall,\nstate-of-the-art approaches gain on average absolute $3.29 \\pm 0.24\\%$ in all\nmetrics on WN18RR. This means that some of the conclusions achieved in previous\nworks might need to be revisited. We provide an open-source implementation of\nour experiments and corrected datasets at at\nhttps://github.com/dice-group/OOV-In-Link-Prediction.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 12:58:18 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Demir", "Caglar", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "2105.12540", "submitter": "Zaiwei Chen", "authors": "Zaiwei Chen, Sajad Khodadadian, Siva Theja Maguluri", "title": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic with Linear\n  Function Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we develop a novel variant of off-policy natural actor-critic\nalgorithm with linear function approximation and we establish a sample\ncomplexity of $\\mathcal{O}(\\epsilon^{-3})$, outperforming all the previously\nknown convergence bounds of such algorithms. In order to overcome the\ndivergence due to deadly triad in off-policy policy evaluation under function\napproximation, we develop a critic that employs $n$-step TD-learning algorithm\nwith a properly chosen $n$. We present finite-sample convergence bounds on this\ncritic under both constant and diminishing step sizes, which are of independent\ninterest. Furthermore, we develop a variant of natural policy gradient under\nfunction approximation, with an improved convergence rate of $\\mathcal{O}(1/T)$\nafter $T$ iterations. Combining the finite sample error bounds of actor and the\ncritic, we obtain the $\\mathcal{O}(\\epsilon^{-3})$ sample complexity. We derive\nour sample complexity bounds solely based on the assumption that the behavior\npolicy sufficiently explores all the states and actions, which is a much\nlighter assumption compared to the related literature.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:35:42 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chen", "Zaiwei", ""], ["Khodadadian", "Sajad", ""], ["Maguluri", "Siva Theja", ""]]}, {"id": "2105.12545", "submitter": "Chang Tian", "authors": "Chang Tian, An Liu, Guang Huang and Wu Luo", "title": "Successive Convex Approximation Based Off-Policy Optimization for\n  Constrained Reinforcement Learning", "comments": "submitted to Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a successive convex approximation based off-policy optimization\n(SCAOPO) algorithm to solve the general constrained reinforcement learning\nproblem, which is formulated as a constrained Markov decision process (CMDP) in\nthe context of average cost. The SCAOPO is based on solving a sequence of\nconvex objective/feasibility optimization problems obtained by replacing the\nobjective and constraint functions in the original problems with convex\nsurrogate functions. At each iteration, the convex surrogate problem can be\nefficiently solved by Lagrange dual method even the policy is parameterized by\na high-dimensional function. Moreover, the SCAOPO enables to reuse old\nexperiences from previous updates, thereby significantly reducing the\nimplementation cost when deployed in the real-world engineering systems that\nneed to online learn the environment. In spite of the time-varying state\ndistribution and the stochastic bias incurred by the off-policy learning, the\nSCAOPO with a feasible initial point can still provably converge to a\nKarush-Kuhn-Tucker (KKT) point of the original problem almost surely.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:52:39 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Tian", "Chang", ""], ["Liu", "An", ""], ["Huang", "Guang", ""], ["Luo", "Wu", ""]]}, {"id": "2105.12564", "submitter": "Rushabh Patel", "authors": "Rushabh Patel", "title": "Predicting invasive ductal carcinoma using a Reinforcement Sample\n  Learning Strategy using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invasive ductal carcinoma is a prevalent, potentially deadly disease\nassociated with a high rate of morbidity and mortality. Its malignancy is the\nsecond leading cause of death from cancer in women. The mammogram is an\nextremely useful resource for mass detection and invasive ductal carcinoma\ndiagnosis. We are proposing a method for Invasive ductal carcinoma that will\nuse convolutional neural networks (CNN) on mammograms to assist radiologists in\ndiagnosing the disease. Due to the varying image clarity and structure of\ncertain mammograms, it is difficult to observe major cancer characteristics\nsuch as microcalcification and mass, and it is often difficult to interpret and\ndiagnose these attributes. The aim of this study is to establish a novel method\nfor fully automated feature extraction and classification in invasive ductal\ncarcinoma computer-aided diagnosis (CAD) systems. This article presents a tumor\nclassification algorithm that makes novel use of convolutional neural networks\non breast mammogram images to increase feature extraction and training speed.\nThe algorithm makes two contributions.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 14:14:45 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Patel", "Rushabh", ""]]}, {"id": "2105.12584", "submitter": "Fanzhen Liu", "authors": "Xing Su, Shan Xue, Fanzhen Liu, Jia Wu, Jian Yang, Chuan Zhou, Wenbin\n  Hu, Cecile Paris, Surya Nepal, Di Jin, Quan Z. Sheng, Philip S. Yu", "title": "A Comprehensive Survey on Community Detection with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A community reveals the features and connections of its members that are\ndifferent from those in other communities in a network. Detecting communities\nis of great significance in network analysis. Despite the classical spectral\nclustering and statistical inference methods, we notice a significant\ndevelopment of deep learning techniques for community detection in recent years\nwith their advantages in handling high dimensional network data. Hence, a\ncomprehensive overview of community detection's latest progress through deep\nlearning is timely to both academics and practitioners. This survey devises and\nproposes a new taxonomy covering different categories of the state-of-the-art\nmethods, including deep learning-based models upon deep neural networks, deep\nnonnegative matrix factorization and deep sparse filtering. The main category,\ni.e., deep neural networks, is further divided into convolutional networks,\ngraph attention networks, generative adversarial networks and autoencoders. The\nsurvey also summarizes the popular benchmark data sets, model evaluation\nmetrics, and open-source implementations to address experimentation settings.\nWe then discuss the practical applications of community detection in various\ndomains and point to implementation scenarios. Finally, we outline future\ndirections by suggesting challenging topics in this fast-growing deep learning\nfield.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 14:37:07 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Su", "Xing", ""], ["Xue", "Shan", ""], ["Liu", "Fanzhen", ""], ["Wu", "Jia", ""], ["Yang", "Jian", ""], ["Zhou", "Chuan", ""], ["Hu", "Wenbin", ""], ["Paris", "Cecile", ""], ["Nepal", "Surya", ""], ["Jin", "Di", ""], ["Sheng", "Quan Z.", ""], ["Yu", "Philip S.", ""]]}, {"id": "2105.12598", "submitter": "Michael Scholkemper", "authors": "Michael Scholkemper and Michael T. Schaub", "title": "Local, global and scale-dependent node roles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper re-examines the concept of node equivalences like structural\nequivalence or automorphic equivalence, which have originally emerged in social\nnetwork analysis to characterize the role an actor plays within a social\nsystem, but have since then been of independent interest for graph-based\nlearning tasks. Traditionally, such exact node equivalences have been defined\neither in terms of the one hop neighborhood of a node, or in terms of the\nglobal graph structure. Here we formalize exact node roles with a\nscale-parameter, describing up to what distance the ego network of a node\nshould be considered when assigning node roles - motivated by the idea that\nthere can be local roles of a node that should not be determined by nodes\narbitrarily far away in the network. We present numerical experiments that show\nhow already \"shallow\" roles of depth 3 or 4 carry sufficient information to\nperform node classification tasks with high accuracy. These findings\ncorroborate the success of recent graph-learning approaches that compute\napproximate node roles in terms of embeddings, by nonlinearly aggregating node\nfeatures in an (un)supervised manner over relatively small neighborhood sizes.\nIndeed, based on our ideas we can construct a shallow classifier achieving on\npar results with recent graph neural network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 14:54:26 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Scholkemper", "Michael", ""], ["Schaub", "Michael T.", ""]]}, {"id": "2105.12626", "submitter": "Sergio Altares", "authors": "Sergio Altares-L\\'opez, Angela Ribeiro, Juan Jos\\'e Garc\\'ia-Ripoll", "title": "Automatic design of quantum feature maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new technique for the automatic generation of optimal ad-hoc\nans\\\"atze for classification by using quantum support vector machine (QSVM).\nThis efficient method is based on NSGA-II multiobjective genetic algorithms\nwhich allow both maximize the accuracy and minimize the ansatz size. It is\ndemonstrated the validity of the technique by a practical example with a\nnon-linear dataset, interpreting the resulting circuit and its outputs. We also\nshow other application fields of the technique that reinforce the validity of\nthe method, and a comparison with classical classifiers in order to understand\nthe advantages of using quantum machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:31:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Altares-L\u00f3pez", "Sergio", ""], ["Ribeiro", "Angela", ""], ["Garc\u00eda-Ripoll", "Juan Jos\u00e9", ""]]}, {"id": "2105.12628", "submitter": "Yujia Bao", "authors": "Yujia Bao, Shiyu Chang, Regina Barzilay", "title": "Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose Predict then Interpolate (PI), a simple algorithm for learning\ncorrelations that are stable across environments. The algorithm follows from\nthe intuition that when using a classifier trained on one environment to make\npredictions on examples from another environment, its mistakes are informative\nas to which correlations are unstable. In this work, we prove that by\ninterpolating the distributions of the correct predictions and the wrong\npredictions, we can uncover an oracle distribution where the unstable\ncorrelation vanishes. Since the oracle interpolation coefficients are not\naccessible, we use group distributionally robust optimization to minimize the\nworst-case risk across all such interpolations. We evaluate our method on both\ntext classification and image classification. Empirical results demonstrate\nthat our algorithm is able to learn robust classifiers (outperforms IRM by\n23.85% on synthetic environments and 12.41% on natural environments). Our code\nand data are available at https://github.com/YujiaBao/Predict-then-Interpolate.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:37:48 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bao", "Yujia", ""], ["Chang", "Shiyu", ""], ["Barzilay", "Regina", ""]]}, {"id": "2105.12638", "submitter": "Gihan Panapitiya", "authors": "Gihan Panapitiya, Michael Girard, Aaron Hollas, Vijay Murugesan, Wei\n  Wang, Emily Saldanha", "title": "Predicting Aqueous Solubility of Organic Molecules Using Deep Learning\n  Models with Varied Molecular Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the aqueous solubility of molecules is a vital step in many\npharmaceutical, environmental, and energy storage applications. Despite efforts\nmade over decades, there are still challenges associated with developing a\nsolubility prediction model with satisfactory accuracy for many of these\napplications. The goal of this study is to develop a general model capable of\npredicting the solubility of a broad range of organic molecules. Using the\nlargest currently available solubility dataset, we implement deep\nlearning-based models to predict solubility from molecular structure and\nexplore several different molecular representations including molecular\ndescriptors, simplified molecular-input line-entry system (SMILES) strings,\nmolecular graphs, and three-dimensional (3D) atomic coordinates using four\ndifferent neural network architectures - fully connected neural networks\n(FCNNs), recurrent neural networks (RNNs), graph neural networks (GNNs), and\nSchNet. We find that models using molecular descriptors achieve the best\nperformance, with GNN models also achieving good performance. We perform\nextensive error analysis to understand the molecular properties that influence\nmodel performance, perform feature analysis to understand which information\nabout molecular structure is most valuable for prediction, and perform a\ntransfer learning and data size study to understand the impact of data\navailability on model performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:54:54 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 01:03:43 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Panapitiya", "Gihan", ""], ["Girard", "Michael", ""], ["Hollas", "Aaron", ""], ["Murugesan", "Vijay", ""], ["Wang", "Wei", ""], ["Saldanha", "Emily", ""]]}, {"id": "2105.12639", "submitter": "Namuk Park", "authors": "Namuk Park, Songkuk Kim", "title": "Blurs Make Results Clearer: Spatial Smoothings to Improve Accuracy,\n  Uncertainty, and Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian neural networks (BNNs) have shown success in the areas of\nuncertainty estimation and robustness. However, a crucial challenge prohibits\ntheir use in practice: Bayesian NNs require a large number of predictions to\nproduce reliable results, leading to a significant increase in computational\ncost. To alleviate this issue, we propose spatial smoothing, a method that\nensembles neighboring feature map points of CNNs. By simply adding a few blur\nlayers to the models, we empirically show that the spatial smoothing improves\naccuracy, uncertainty estimation, and robustness of BNNs across a whole range\nof ensemble sizes. In particular, BNNs incorporating the spatial smoothing\nachieve high predictive performance merely with a handful of ensembles.\nMoreover, this method also can be applied to canonical deterministic neural\nnetworks to improve the performances. A number of evidences suggest that the\nimprovements can be attributed to the smoothing and flattening of the loss\nlandscape. In addition, we provide a fundamental explanation for prior works -\nnamely, global average pooling, pre-activation, and ReLU6 - by addressing to\nthem as special cases of the spatial smoothing. These not only enhance\naccuracy, but also improve uncertainty estimation and robustness by making the\nloss landscape smoother in the same manner as the spatial smoothing. The code\nis available at https://github.com/xxxnell/spatial-smoothing.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:58:11 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Park", "Namuk", ""], ["Kim", "Songkuk", ""]]}, {"id": "2105.12676", "submitter": "Zhaoxia Deng", "authors": "Zhaoxia (Summer) Deng, Jongsoo Park, Ping Tak Peter Tang, Haixin Liu,\n  Jie (Amy) Yang, Hector Yuen, Jianyu Huang, Daya Khudia, Xiaohan Wei, Ellie\n  Wen, Dhruv Choudhary, Raghuraman Krishnamoorthi, Carole-Jean Wu, Satish\n  Nadathur, Changkyu Kim, Maxim Naumov, Sam Naghshineh, Mikhail Smelyanskiy", "title": "Low-Precision Hardware Architectures Meet Recommendation Model Inference\n  at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.IR cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous success of machine learning (ML) and the unabated growth in ML\nmodel complexity motivated many ML-specific designs in both CPU and accelerator\narchitectures to speed up the model inference. While these architectures are\ndiverse, highly optimized low-precision arithmetic is a component shared by\nmost. Impressive compute throughputs are indeed often exhibited by these\narchitectures on benchmark ML models. Nevertheless, production models such as\nrecommendation systems important to Facebook's personalization services are\ndemanding and complex: These systems must serve billions of users per month\nresponsively with low latency while maintaining high prediction accuracy,\nnotwithstanding computations with many tens of billions parameters per\ninference. Do these low-precision architectures work well with our production\nrecommendation systems? They do. But not without significant effort. We share\nin this paper our search strategies to adapt reference recommendation models to\nlow-precision hardware, our optimization of low-precision compute kernels, and\nthe design and development of tool chain so as to maintain our models' accuracy\nthroughout their lifespan during which topic trends and users' interests\ninevitably evolve. Practicing these low-precision technologies helped us save\ndatacenter capacities while deploying models with up to 5X complexity that\nwould otherwise not be deployed on traditional general-purpose CPUs. We believe\nthese lessons from the trenches promote better co-design between hardware\narchitecture and software engineering and advance the state of the art of ML in\nindustry.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:42:33 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhaoxia", "", "", "Summer"], ["Deng", "", "", "Amy"], ["Park", "Jongsoo", "", "Amy"], ["Tang", "Ping Tak Peter", "", "Amy"], ["Liu", "Haixin", "", "Amy"], ["Jie", "", "", "Amy"], ["Yang", "", ""], ["Yuen", "Hector", ""], ["Huang", "Jianyu", ""], ["Khudia", "Daya", ""], ["Wei", "Xiaohan", ""], ["Wen", "Ellie", ""], ["Choudhary", "Dhruv", ""], ["Krishnamoorthi", "Raghuraman", ""], ["Wu", "Carole-Jean", ""], ["Nadathur", "Satish", ""], ["Kim", "Changkyu", ""], ["Naumov", "Maxim", ""], ["Naghshineh", "Sam", ""], ["Smelyanskiy", "Mikhail", ""]]}, {"id": "2105.12686", "submitter": "Lizeth Gonzalez Carabarin", "authors": "Lizeth Gonzalez-Carabarin, Iris A.M. Huijben, Bastiaan S. Veeling,\n  Alexandre Schmid, Ruud J.G. van Sloun", "title": "Dynamic Probabilistic Pruning: A general framework for\n  hardware-constrained pruning at different granularities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured neural network pruning algorithms have achieved impressive\ncompression rates. However, the resulting - typically irregular - sparse\nmatrices hamper efficient hardware implementations, leading to additional\nmemory usage and complex control logic that diminishes the benefits of\nunstructured pruning. This has spurred structured coarse-grained pruning\nsolutions that prune entire filters or even layers, enabling efficient\nimplementation at the expense of reduced flexibility. Here we propose a\nflexible new pruning mechanism that facilitates pruning at different\ngranularities (weights, kernels, filters/feature maps), while retaining\nefficient memory organization (e.g. pruning exactly k-out-of-n weights for\nevery output neuron, or pruning exactly k-out-of-n kernels for every feature\nmap). We refer to this algorithm as Dynamic Probabilistic Pruning (DPP). DPP\nleverages the Gumbel-softmax relaxation for differentiable k-out-of-n sampling,\nfacilitating end-to-end optimization. We show that DPP achieves competitive\ncompression rates and classification accuracy when pruning common deep learning\nmodels trained on different benchmark datasets for image classification.\nRelevantly, the non-magnitude-based nature of DPP allows for joint optimization\nof pruning and weight quantization in order to even further compress the\nnetwork, which we show as well. Finally, we propose novel information theoretic\nmetrics that show the confidence and pruning diversity of pruning masks within\na layer.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:01:52 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Gonzalez-Carabarin", "Lizeth", ""], ["Huijben", "Iris A. M.", ""], ["Veeling", "Bastiaan S.", ""], ["Schmid", "Alexandre", ""], ["van Sloun", "Ruud J. G.", ""]]}, {"id": "2105.12697", "submitter": "Matej Zecevic", "authors": "Matej Ze\\v{c}evi\\'c, Devendra Singh Dhami and Kristian Kersting", "title": "Intriguing Parameters of Structural Causal Models", "comments": "Main paper: 9 pages, References: 2 pages, Supplement: 2 pages. Main\n  paper: 3 figures, Supplement: 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a lot of focus on adversarial attacks,\nespecially on deep neural networks. Here, we argue that they are more general\nin nature and can easily affect a larger class of models, e.g., any\ndifferentiable perturbed optimizers. We further show that such attacks can be\ndetermined by the hidden confounders in a domain, thus drawing a novel\nconnection between such attacks and causality. Establishing this causal\nperspective is characterized by the influence of the structural causal model's\ndata generating process on the subsequent optimization thereby exhibiting\nintriguing parameters of the former. We reveal the existence of such parameters\nfor three combinatorial optimization problems, namely linear assignment,\nshortest path and a real world problem of energy systems. Our empirical\nexamination also unveils worrisome consequences of these attacks on\ndifferentiable perturbed optimizers thereby highlighting the criticality of our\nfindings.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:19:22 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 09:13:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ze\u010devi\u0107", "Matej", ""], ["Dhami", "Devendra Singh", ""], ["Kersting", "Kristian", ""]]}, {"id": "2105.12700", "submitter": "Luka Murn", "authors": "Luka Murn, Marc Gorriz Blanch, Maria Santamaria, Fiona Rivera, Marta\n  Mrak", "title": "Towards Transparent Application of Machine Learning in Video Processing", "comments": "International Broadcasting Convention, 11-14 Sep 2020, Amsterdam,\n  Netherlands (Technical Paper section, Virtual)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning techniques for more efficient video compression and video\nenhancement have been developed thanks to breakthroughs in deep learning. The\nnew techniques, considered as an advanced form of Artificial Intelligence (AI),\nbring previously unforeseen capabilities. However, they typically come in the\nform of resource-hungry black-boxes (overly complex with little transparency\nregarding the inner workings). Their application can therefore be unpredictable\nand generally unreliable for large-scale use (e.g. in live broadcast). The aim\nof this work is to understand and optimise learned models in video processing\napplications so systems that incorporate them can be used in a more trustworthy\nmanner. In this context, the presented work introduces principles for\nsimplification of learned models targeting improved transparency in\nimplementing machine learning for video production and distribution\napplications. These principles are demonstrated on video compression examples,\nshowing how bitrate savings and reduced complexity can be achieved by\nsimplifying relevant deep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:24:23 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 09:35:54 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Murn", "Luka", ""], ["Blanch", "Marc Gorriz", ""], ["Santamaria", "Maria", ""], ["Rivera", "Fiona", ""], ["Mrak", "Marta", ""]]}, {"id": "2105.12703", "submitter": "Rodrigo Alves Randel", "authors": "Rodrigo Randel and Daniel Aloise and Alain Hertz", "title": "Exploring dual information in distance metric learning for clustering", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distance metric learning algorithms aim to appropriately measure similarities\nand distances between data points. In the context of clustering, metric\nlearning is typically applied with the assist of side-information provided by\nexperts, most commonly expressed in the form of cannot-link and must-link\nconstraints. In this setting, distance metric learning algorithms move closer\npairs of data points involved in must-link constraints, while pairs of points\ninvolved in cannot-link constraints are moved away from each other. For these\nalgorithms to be effective, it is important to use a distance metric that\nmatches the expert knowledge, beliefs, and expectations, and the\ntransformations made to stick to the side-information should preserve\ngeometrical properties of the dataset. Also, it is interesting to filter the\nconstraints provided by the experts to keep only the most useful and reject\nthose that can harm the clustering process. To address these issues, we propose\nto exploit the dual information associated with the pairwise constraints of the\nsemi-supervised clustering problem. Experiments clearly show that distance\nmetric learning algorithms benefit from integrating this dual information.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:33:23 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Randel", "Rodrigo", ""], ["Aloise", "Daniel", ""], ["Hertz", "Alain", ""]]}, {"id": "2105.12722", "submitter": "Pak Hei Yeung", "authors": "Pak-Hei Yeung, Ana I.L. Namburete, Weidi Xie", "title": "Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised\n  Learning", "comments": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this work is to segment any arbitrary structures of interest\n(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D\nsegmentation). We show that high accuracy can be achieved by simply propagating\nthe 2D slice segmentation with an affinity matrix between consecutive slices,\nwhich can be learnt in a self-supervised manner, namely slice reconstruction.\nSpecifically, we compare the proposed framework, termed as Sli2Vol, with\nsupervised approaches and two other unsupervised/ self-supervised slice\nregistration approaches, on 8 public datasets (both CT and MRI scans), spanning\n9 different SOIs. Without any parameter-tuning, the same model achieves\nsuperior performance with Dice scores (0-100 scale) of over 80 for most of the\nbenchmarks, including the ones that are unseen during training. Our results\nshow generalizability of the proposed approach across data from different\nmachines and with different SOIs: a major use case of semi-automatic\nsegmentation methods where fully supervised approaches would normally struggle.\nThe source code will be made publicly available at\nhttps://github.com/pakheiyeung/Sli2Vol.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:56:39 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 18:28:19 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Yeung", "Pak-Hei", ""], ["Namburete", "Ana I. L.", ""], ["Xie", "Weidi", ""]]}, {"id": "2105.12724", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Yuhang Hu, Lianfeng Li, Sara Cummings, Hod Lipson", "title": "Smile Like You Mean It: Driving Animatronic Robotic Face with Learned\n  Models", "comments": "ICRA 2021. Website:http://www.cs.columbia.edu/~bchen/aiface/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ability to generate intelligent and generalizable facial expressions is\nessential for building human-like social robots. At present, progress in this\nfield is hindered by the fact that each facial expression needs to be\nprogrammed by humans. In order to adapt robot behavior in real time to\ndifferent situations that arise when interacting with human subjects, robots\nneed to be able to train themselves without requiring human labels, as well as\nmake fast action decisions and generalize the acquired knowledge to diverse and\nnew contexts. We addressed this challenge by designing a physical animatronic\nrobotic face with soft skin and by developing a vision-based self-supervised\nlearning framework for facial mimicry. Our algorithm does not require any\nknowledge of the robot's kinematic model, camera calibration or predefined\nexpression set. By decomposing the learning process into a generative model and\nan inverse model, our framework can be trained using a single motor babbling\ndataset. Comprehensive evaluations show that our method enables accurate and\ndiverse face mimicry across diverse human subjects. The project website is at\nhttp://www.cs.columbia.edu/~bchen/aiface/\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:57:19 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chen", "Boyuan", ""], ["Hu", "Yuhang", ""], ["Li", "Lianfeng", ""], ["Cummings", "Sara", ""], ["Lipson", "Hod", ""]]}, {"id": "2105.12769", "submitter": "Alexander Jung", "authors": "Yasmin SarcheshmehPour, Yu Tian, Linli Zhang, Alexander Jung", "title": "Networked Federated Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many important application domains generate distributed collections of\nheterogeneous local datasets. These local datasets are often related via an\nintrinsic network structure that arises from domain-specific notions of\nsimilarity between local datasets. Different notions of similarity are induced\nby spatiotemporal proximity, statistical dependencies, or functional relations.\nWe use this network structure to adaptively pool similar local datasets into\nnearly homogenous training sets for learning tailored models. Our main\nconceptual contribution is to formulate networked federated learning using the\nconcept of generalized total variation (GTV) minimization as a regularizer.\nThis formulation is highly flexible and can be combined with almost any\nparametric model including Lasso or deep neural networks. We unify and\nconsiderably extend some well-known approaches to federated multi-task\nlearning. Our main algorithmic contribution is a novel federated learning\nalgorithm that is well suited for distributed computing environments such as\nedge computing over wireless networks. This algorithm is robust against model\nmisspecification and numerical errors arising from limited computational\nresources including processing time or wireless channel bandwidth. As our main\ntechnical contribution, we offer precise conditions on the local models as well\non their network structure such that our algorithm learns nearly optimal local\nmodels. Our analysis reveals an interesting interplay between the\n(information-) geometry of local models and the (cluster-) geometry of their\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:07:19 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["SarcheshmehPour", "Yasmin", ""], ["Tian", "Yu", ""], ["Zhang", "Linli", ""], ["Jung", "Alexander", ""]]}, {"id": "2105.12770", "submitter": "Xun Jiao", "authors": "Rahul Thapa, Dongning Ma, Xun Jiao", "title": "HDXplore: Automated Blackbox Testing of Brain-Inspired Hyperdimensional\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the way human brain works, the emerging hyperdimensional\ncomputing (HDC) is getting more and more attention. HDC is an emerging\ncomputing scheme based on the working mechanism of brain that computes with\ndeep and abstract patterns of neural activity instead of actual numbers.\nCompared with traditional ML algorithms such as DNN, HDC is more\nmemory-centric, granting it advantages such as relatively smaller model size,\nless computation cost, and one-shot learning, making it a promising candidate\nin low-cost computing platforms. However, the robustness of HDC models have not\nbeen systematically studied. In this paper, we systematically expose the\nunexpected or incorrect behaviors of HDC models by developing HDXplore, a\nblackbox differential testing-based framework. We leverage multiple HDC models\nwith similar functionality as cross-referencing oracles to avoid manual\nchecking or labeling the original input. We also propose different perturbation\nmechanisms in HDXplore. HDXplore automatically finds thousands of incorrect\ncorner case behaviors of the HDC model. We propose two retraining mechanisms\nand using the corner cases generated by HDXplore to retrain the HDC model, we\ncan improve the model accuracy by up to 9%.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:08:52 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Thapa", "Rahul", ""], ["Ma", "Dongning", ""], ["Jiao", "Xun", ""]]}, {"id": "2105.12774", "submitter": "Sabyasachi Sahoo", "authors": "Prashant Kumar, Sabyasachi Sahoo, Vanshil Shah, Vineetha Kondameedi,\n  Abhinav Jain, Akshaj Verma, Chiranjib Bhattacharyya, Vinay Viswanathan", "title": "DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially\n  Trained Autoencoder", "comments": "17 pages, 15 figures, Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate reconstruction of static environments from LiDAR scans of scenes\ncontaining dynamic objects, which we refer to as Dynamic to Static Translation\n(DST), is an important area of research in Autonomous Navigation. This problem\nhas been recently explored for visual SLAM, but to the best of our knowledge no\nwork has been attempted to address DST for LiDAR scans. The problem is of\ncritical importance due to wide-spread adoption of LiDAR in Autonomous\nVehicles. We show that state-of the art methods developed for the visual domain\nwhen adapted for LiDAR scans perform poorly.\n  We develop DSLR, a deep generative model which learns a mapping between\ndynamic scan to its static counterpart through an adversarially trained\nautoencoder. Our model yields the first solution for DST on LiDAR that\ngenerates static scans without using explicit segmentation labels. DSLR cannot\nalways be applied to real world data due to lack of paired dynamic-static\nscans. Using Unsupervised Domain Adaptation, we propose DSLR-UDA for transfer\nto real world data and experimentally show that this performs well in real\nworld settings. Additionally, if segmentation information is available, we\nextend DSLR to DSLR-Seg to further improve the reconstruction quality.\n  DSLR gives the state of the art performance on simulated and real-world\ndatasets and also shows at least 4x improvement. We show that DSLR, unlike the\nexisting baselines, is a practically viable model with its reconstruction\nquality within the tolerable limits for tasks pertaining to autonomous\nnavigation like SLAM in dynamic environments.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:19:21 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kumar", "Prashant", ""], ["Sahoo", "Sabyasachi", ""], ["Shah", "Vanshil", ""], ["Kondameedi", "Vineetha", ""], ["Jain", "Abhinav", ""], ["Verma", "Akshaj", ""], ["Bhattacharyya", "Chiranjib", ""], ["Viswanathan", "Vinay", ""]]}, {"id": "2105.12781", "submitter": "Supreeth Mysore Shivanandamurthy", "authors": "Supreeth Mysore Shivanandamurthy, Ishan. G. Thakkar, Sayed Ahmad\n  Salehi", "title": "ATRIA: A Bit-Parallel Stochastic Arithmetic Based Accelerator for\n  In-DRAM CNN Processing", "comments": "Preprint accepted in ISVLSI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the rapidly growing use of Convolutional Neural Networks (CNNs) in\nreal-world applications related to machine learning and Artificial Intelligence\n(AI), several hardware accelerator designs for CNN inference and training have\nbeen proposed recently. In this paper, we present ATRIA, a novel bit-pArallel\nsTochastic aRithmetic based In-DRAM Accelerator for energy-efficient and\nhigh-speed inference of CNNs. ATRIA employs light-weight modifications in DRAM\ncell arrays to implement bit-parallel stochastic arithmetic based acceleration\nof multiply-accumulate (MAC) operations inside DRAM. ATRIA significantly\nimproves the latency, throughput, and efficiency of processing CNN inferences\nby performing 16 MAC operations in only five consecutive memory operation\ncycles. We mapped the inference tasks of four benchmark CNNs on ATRIA to\ncompare its performance with five state-of-the-art in-DRAM CNN accelerators\nfrom prior work. The results of our analysis show that ATRIA exhibits only 3.5%\ndrop in CNN inference accuracy and still achieves improvements of up to 3.2x in\nframes-per-second (FPS) and up to 10x in efficiency (FPS/W/mm2), compared to\nthe best-performing in-DRAM accelerator from prior work.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:36:01 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shivanandamurthy", "Supreeth Mysore", ""], ["Thakkar", "Ishan. G.", ""], ["Salehi", "Sayed Ahmad", ""]]}, {"id": "2105.12787", "submitter": "Miltiadis Allamanis", "authors": "Miltiadis Allamanis, Henry Jackson-Flux, Marc Brockschmidt", "title": "Self-Supervised Bug Detection and Repair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning-based program analyses have recently shown the promise of\nintegrating formal and probabilistic reasoning towards aiding software\ndevelopment. However, in the absence of large annotated corpora, training these\nanalyses is challenging. Towards addressing this, we present BugLab, an\napproach for self-supervised learning of bug detection and repair. BugLab\nco-trains two models: (1) a detector model that learns to detect and repair\nbugs in code, (2) a selector model that learns to create buggy code for the\ndetector to use as training data. A Python implementation of BugLab improves by\nup to 30% upon baseline methods on a test dataset of 2374 real-life bugs and\nfinds 19 previously unknown bugs in open-source software.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:41:05 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Allamanis", "Miltiadis", ""], ["Jackson-Flux", "Henry", ""], ["Brockschmidt", "Marc", ""]]}, {"id": "2105.12791", "submitter": "Roberto Calandra", "authors": "Mike Lambeta, Huazhe Xu, Jingwei Xu, Po-Wei Chou, Shaoxiong Wang,\n  Trevor Darrell, Roberto Calandra", "title": "PyTouch: A Machine Learning Library for Touch Processing", "comments": "7 pages. Accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased availability of rich tactile sensors, there is an equally\nproportional need for open-source and integrated software capable of\nefficiently and effectively processing raw touch measurements into high-level\nsignals that can be used for control and decision-making. In this paper, we\npresent PyTouch -- the first machine learning library dedicated to the\nprocessing of touch sensing signals. PyTouch, is designed to be modular,\neasy-to-use and provides state-of-the-art touch processing capabilities as a\nservice with the goal of unifying the tactile sensing community by providing a\nlibrary for building scalable, proven, and performance-validated modules over\nwhich applications and research can be built upon. We evaluate PyTouch on\nreal-world data from several tactile sensors on touch processing tasks such as\ntouch detection, slip and object pose estimations. PyTouch is open-sourced at\nhttps://github.com/facebookresearch/pytouch .\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:55:18 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Lambeta", "Mike", ""], ["Xu", "Huazhe", ""], ["Xu", "Jingwei", ""], ["Chou", "Po-Wei", ""], ["Wang", "Shaoxiong", ""], ["Darrell", "Trevor", ""], ["Calandra", "Roberto", ""]]}, {"id": "2105.12806", "submitter": "Mark Sellke", "authors": "S\\'ebastien Bubeck, Mark Sellke", "title": "A Universal Law of Robustness via Isoperimetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classically, data interpolation with a parametrized model class is possible\nas long as the number of parameters is larger than the number of equations to\nbe satisfied. A puzzling phenomenon in deep learning is that models are trained\nwith many more parameters than what this classical theory would suggest. We\npropose a theoretical explanation for this phenomenon. We prove that for a\nbroad class of data distributions and model classes, overparametrization is\nnecessary if one wants to interpolate the data smoothly. Namely we show that\nsmooth interpolation requires $d$ times more parameters than mere\ninterpolation, where $d$ is the ambient data dimension. We prove this universal\nlaw of robustness for any smoothly parametrized function class with polynomial\nsize weights, and any covariate distribution verifying isoperimetry. In the\ncase of two-layers neural networks and Gaussian covariates, this law was\nconjectured in prior work by Bubeck, Li and Nagaraj. We also give an\ninterpretation of our result as an improved generalization bound for model\nclasses consisting of smooth functions.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 19:49:47 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 21:10:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Sellke", "Mark", ""]]}, {"id": "2105.12807", "submitter": "Xiaoyu Zhang", "authors": "Eloise Withnell, Xiaoyu Zhang, Kai Sun, Yike Guo", "title": "XOmiVAE: an interpretable deep learning model for cancer classification\n  using high-dimensional omics data", "comments": "12 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of explainability is one of the most prominent disadvantages of deep\nlearning applications in omics. This \"black box\" problem can undermine the\ncredibility and limit the practical implementation of biomedical deep learning\nmodels. Here we present XOmiVAE, a variational autoencoder (VAE) based\ninterpretable deep learning model for cancer classification using\nhigh-dimensional omics data. XOmiVAE is capable of revealing the contribution\nof each gene and latent dimension for each classification prediction, and the\ncorrelation between each gene and each latent dimension. It is also\ndemonstrated that XOmiVAE can explain not only the supervised classification\nbut the unsupervised clustering results from the deep learning network. To the\nbest of our knowledge, XOmiVAE is one of the first activation level-based\ninterpretable deep learning models explaining novel clusters generated by VAE.\nThe explainable results generated by XOmiVAE were validated by both the\nperformance of downstream tasks and the biomedical knowledge. In our\nexperiments, XOmiVAE explanations of deep learning based cancer classification\nand clustering aligned with current domain knowledge including biological\nannotation and academic literature, which shows great potential for novel\nbiomedical knowledge discovery from deep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 19:55:12 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 08:50:15 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Withnell", "Eloise", ""], ["Zhang", "Xiaoyu", ""], ["Sun", "Kai", ""], ["Guo", "Yike", ""]]}, {"id": "2105.12810", "submitter": "Md Hasib Zunair", "authors": "Hasib Zunair, Aimon Rahman, and Nabeel Mohammed", "title": "ViPTT-Net: Video pretraining of spatio-temporal model for tuberculosis\n  type classification from chest CT scans", "comments": "Under review at CLEF 2021. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretraining has sparked groundswell of interest in deep learning workflows to\nlearn from limited data and improve generalization. While this is common for 2D\nimage classification tasks, its application to 3D medical imaging tasks like\nchest CT interpretation is limited. We explore the idea of whether pretraining\na model on realistic videos could improve performance rather than training the\nmodel from scratch, intended for tuberculosis type classification from chest CT\nscans. To incorporate both spatial and temporal features, we develop a hybrid\nconvolutional neural network (CNN) and recurrent neural network (RNN) model,\nwhere the features are extracted from each axial slice of the CT scan by a CNN,\nthese sequence of image features are input to a RNN for classification of the\nCT scan. Our model termed as ViPTT-Net, was trained on over 1300 video clips\nwith labels of human activities, and then fine-tuned on chest CT scans with\nlabels of tuberculosis type. We find that pretraining the model on videos lead\nto better representations and significantly improved model validation\nperformance from a kappa score of 0.17 to 0.35, especially for\nunder-represented class samples. Our best method achieved 2nd place in the\nImageCLEF 2021 Tuberculosis - TBT classification task with a kappa score of\n0.20 on the final test set with only image information (without using clinical\nmeta-data). All codes and models are made available.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 20:00:31 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zunair", "Hasib", ""], ["Rahman", "Aimon", ""], ["Mohammed", "Nabeel", ""]]}, {"id": "2105.12818", "submitter": "Jacinto Carrasco", "authors": "Jacinto Carrasco, Irina Markova, David L\\'opez, Ignacio Aguilera,\n  Diego Garc\\'ia, Marta Garc\\'ia-Barzana, Manuel Arias-Rodil, Juli\\'an Luengo,\n  Francisco Herrera", "title": "Anomaly Detection in Predictive Maintenance: A New Evaluation Framework\n  for Temporal Unsupervised Anomaly Detection Algorithms", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The research in anomaly detection lacks a unified definition of what\nrepresents an anomalous instance. Discrepancies in the nature itself of an\nanomaly lead to multiple paradigms of algorithms design and experimentation.\nPredictive maintenance is a special case, where the anomaly represents a\nfailure that must be prevented. Related time-series research as outlier and\nnovelty detection or time-series classification does not apply to the concept\nof an anomaly in this field, because they are not single points which have not\nbeen seen previously and may not be precisely annotated. Moreover, due to the\nlack of annotated anomalous data, many benchmarks are adapted from supervised\nscenarios.\n  To address these issues, we generalise the concept of positive and negative\ninstances to intervals to be able to evaluate unsupervised anomaly detection\nalgorithms. We also preserve the imbalance scheme for evaluation through the\nproposal of the Preceding Window ROC, a generalisation for the calculation of\nROC curves for time-series scenarios. We also adapt the mechanism from a\nestablished time-series anomaly detection benchmark to the proposed\ngeneralisations to reward early detection. Therefore, the proposal represents a\nflexible evaluation framework for the different scenarios. To show the\nusefulness of this definition, we include a case study of Big Data algorithms\nwith a real-world time-series problem provided by the company ArcelorMittal,\nand compare the proposal with an evaluation method.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 20:15:40 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Carrasco", "Jacinto", ""], ["Markova", "Irina", ""], ["L\u00f3pez", "David", ""], ["Aguilera", "Ignacio", ""], ["Garc\u00eda", "Diego", ""], ["Garc\u00eda-Barzana", "Marta", ""], ["Arias-Rodil", "Manuel", ""], ["Luengo", "Juli\u00e1n", ""], ["Herrera", "Francisco", ""]]}, {"id": "2105.12823", "submitter": "Alireza Shamsoshoara", "authors": "Alireza Shamsoshoara, Fatemeh Afghah, Erik Blasch, Jonathan Ashdown,\n  Mehdi Bennis", "title": "UAV-Assisted Communication in Remote Disaster Areas using Imitation\n  Learning", "comments": "15 pages, 14 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The damage to cellular towers during natural and man-made disasters can\ndisturb the communication services for cellular users. One solution to the\nproblem is using unmanned aerial vehicles to augment the desired communication\nnetwork. The paper demonstrates the design of a UAV-Assisted Imitation Learning\n(UnVAIL) communication system that relays the cellular users' information to a\nneighbor base station. Since the user equipment (UEs) are equipped with buffers\nwith limited capacity to hold packets, UnVAIL alternates between different UEs\nto reduce the chance of buffer overflow, positions itself optimally close to\nthe selected UE to reduce service time, and uncovers a network pathway by\nacting as a relay node. UnVAIL utilizes Imitation Learning (IL) as a\ndata-driven behavioral cloning approach to accomplish an optimal scheduling\nsolution. Results demonstrate that UnVAIL performs similar to a human expert\nknowledge-based planning in communication timeliness, position accuracy, and\nenergy consumption with an accuracy of 97.52% when evaluated on a developed\nsimulator to train the UAV.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:26:44 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shamsoshoara", "Alireza", ""], ["Afghah", "Fatemeh", ""], ["Blasch", "Erik", ""], ["Ashdown", "Jonathan", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2105.12828", "submitter": "Qi Zheng", "authors": "Qi Zheng", "title": "Pouring Dynamics Estimation Using Gated Recurrent Units", "comments": "7 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most commonly performed manipulation in a human's daily life is\npouring. Many factors have an effect on target accuracy, including pouring\nvelocity, rotation angle, geometric of the source, and the receiving\ncontainers. This paper presents an approach to increase the repeatability and\naccuracy of the robotic manipulator by estimating the change in the amount of\nwater of the pouring cup to a sequence of pouring actions using multiple layers\nof the deep recurrent neural network, especially gated recurrent units (GRU).\nThe proposed GRU model achieved a validation mean squared error as low as 1e-4\n(lbf) for the predicted value of weight f(t). This paper contains a\ncomprehensive evaluation and analysis of numerous experiments with various\ndesigns of recurrent neural networks and hyperparameters fine-tuning.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 00:30:31 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zheng", "Qi", ""]]}, {"id": "2105.12833", "submitter": "Ayaan Haque", "authors": "Sajiv Shah, Ayaan Haque, Fei Liu", "title": "Simulated Data Generation Through Algorithmic Force Coefficient\n  Estimation for AI-Based Robotic Projectile Launch Modeling", "comments": "ACIRS 2021; First two authors contributed equally, order arbitrarily\n  assigned", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling of non-rigid object launching and manipulation is complex\nconsidering the wide range of dynamics affecting trajectory, many of which may\nbe unknown. Using physics models can be inaccurate because they cannot account\nfor unknown factors and the effects of the deformation of the object as it is\nlaunched; moreover, deriving force coefficients for these models is not\npossible without extensive experimental testing. Recently, advancements in\ndata-powered artificial intelligence methods have allowed learnable models and\nsystems to emerge. It is desirable to train a model for launch prediction on a\nrobot, as deep neural networks can account for immeasurable dynamics. However,\nthe inability to collect large amounts of experimental data decreases\nperformance of deep neural networks. Through estimating force coefficients, the\naccepted physics models can be leveraged to produce adequate supplemental data\nto artificially increase the size of the training set, yielding improved neural\nnetworks. In this paper, we introduce a new framework for algorithmic\nestimation of force coefficients for non-rigid object launching, which can be\ngeneralized to other domains, in order to generate large datasets. We implement\na novel training algorithm and objective for our deep neural network to\naccurately model launch trajectory of non-rigid objects and predict whether\nthey will hit a series of targets. Our experimental results demonstrate the\neffectiveness of using simulated data from force coefficient estimation and\nshows the importance of simulated data for training an effective neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 18:47:45 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 02:03:32 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 21:30:40 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Shah", "Sajiv", ""], ["Haque", "Ayaan", ""], ["Liu", "Fei", ""]]}, {"id": "2105.12837", "submitter": "Hubert Baniecki", "authors": "Hubert Baniecki, Wojciech Kretowicz, Przemyslaw Biecek", "title": "Fooling Partial Dependence via Data Poisoning", "comments": "Code for this work is available at\n  https://github.com/MI2DataLab/fooling-partial-dependence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been developed to understand complex predictive models and\nhigh expectations are placed on post-hoc model explainability. It turns out\nthat such explanations are not robust nor trustworthy, and they can be fooled.\nThis paper presents techniques for attacking Partial Dependence (plots,\nprofiles, PDP), which are among the most popular methods of explaining any\npredictive model trained on tabular data. We showcase that PD can be\nmanipulated in an adversarial manner, which is alarming, especially in\nfinancial or medical applications where auditability became a must-have trait\nsupporting black-box models. The fooling is performed via poisoning the data to\nbend and shift explanations in the desired direction using genetic and gradient\nalgorithms. To the best of our knowledge, this is the first work performing\nattacks on variable dependence explanations. The novel approach of using a\ngenetic algorithm for doing so is highly transferable as it generalizes both\nways: in a model-agnostic and an explanation-agnostic manner.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 20:58:04 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 17:10:23 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Baniecki", "Hubert", ""], ["Kretowicz", "Wojciech", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "2105.12841", "submitter": "David Shriver", "authors": "David Shriver, Sebastian Elbaum, Matthew B. Dwyer", "title": "DNNV: A Framework for Deep Neural Network Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the large number of sophisticated deep neural network (DNN)\nverification algorithms, DNN verifier developers, users, and researchers still\nface several challenges. First, verifier developers must contend with the\nrapidly changing DNN field to support new DNN operations and property types.\nSecond, verifier users have the burden of selecting a verifier input format to\nspecify their problem. Due to the many input formats, this decision can greatly\nrestrict the verifiers that a user may run. Finally, researchers face\ndifficulties in re-using benchmarks to evaluate and compare verifiers, due to\nthe large number of input formats required to run different verifiers. Existing\nbenchmarks are rarely in formats supported by verifiers other than the one for\nwhich the benchmark was introduced. In this work we present DNNV, a framework\nfor reducing the burden on DNN verifier researchers, developers, and users.\nDNNV standardizes input and output formats, includes a simple yet expressive\nDSL for specifying DNN properties, and provides powerful simplification and\nreduction operations to facilitate the application, development, and comparison\nof DNN verifiers. We show how DNNV increases the support of verifiers for\nexisting benchmarks from 30% to 74%.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:08:17 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shriver", "David", ""], ["Elbaum", "Sebastian", ""], ["Dwyer", "Matthew B.", ""]]}, {"id": "2105.12842", "submitter": "Dan Zhang", "authors": "Dan Zhang, Safeen Huda, Ebrahim Songhori, Quoc Le, Anna Goldie, Azalia\n  Mirhoseini", "title": "A Full-stack Accelerator Search Technique for Vision Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly-changing ML model landscape presents a unique opportunity for\nbuilding hardware accelerators optimized for specific datacenter-scale\nworkloads. We propose Full-stack Accelerator Search Technique (FAST), a\nhardware accelerator search framework that defines a broad optimization\nenvironment covering key design decisions within the hardware-software stack,\nincluding hardware datapath, software scheduling, and compiler passes such as\noperation fusion and tensor padding. Although FAST can be used on any number\nand type of deep learning workload, in this paper we focus on optimizing for a\nsingle or small set of vision models, resulting in significantly faster and\nmore power-efficient designs relative to a general purpose ML accelerator. When\nevaluated on EfficientNet, ResNet50v2, and OCR inference performance relative\nto a TPU-v3, designs generated by FAST optimized for single workloads can\nimprove Perf/TDP (peak power) by over 6x in the best case and 4x on average. On\na limited workload subset, FAST improves Perf/TDP 2.85x on average, with a\nreduction to 2.35x for a single design optimized over the set of workloads. In\naddition, we demonstrate a potential 1.8x speedup opportunity for TPU-v3 with\nimproved scheduling.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:10:20 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhang", "Dan", ""], ["Huda", "Safeen", ""], ["Songhori", "Ebrahim", ""], ["Le", "Quoc", ""], ["Goldie", "Anna", ""], ["Mirhoseini", "Azalia", ""]]}, {"id": "2105.12849", "submitter": "Yun Zeng", "authors": "Chun-Ta Lu, Yun Zeng, Da-Cheng Juan, Yicheng Fan, Zhe Li, Jan Dlabal,\n  Yi-Ting Chen, Arjun Gopalan, Allan Heydon, Chun-Sung Ferng, Reah Miyara,\n  Ariel Fuxman, Futang Peng, Zhen Li, Tom Duerig, Andrew Tomkins", "title": "CARLS: Cross-platform Asynchronous Representation Learning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we propose CARLS, a novel framework for augmenting the capacity\nof existing deep learning frameworks by enabling multiple components -- model\ntrainers, knowledge makers and knowledge banks -- to concertedly work together\nin an asynchronous fashion across hardware platforms. The proposed CARLS is\nparticularly suitable for learning paradigms where model training benefits from\nadditional knowledge inferred or discovered during training, such as node\nembeddings for graph neural networks or reliable pseudo labels from model\npredictions. We also describe three learning paradigms -- semi-supervised\nlearning, curriculum learning and multimodal learning -- as examples that can\nbe scaled up efficiently by CARLS. One version of CARLS has been open-sourced\nand available for download at:\nhttps://github.com/tensorflow/neural-structured-learning/tree/master/research/carls\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:19:02 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Lu", "Chun-Ta", ""], ["Zeng", "Yun", ""], ["Juan", "Da-Cheng", ""], ["Fan", "Yicheng", ""], ["Li", "Zhe", ""], ["Dlabal", "Jan", ""], ["Chen", "Yi-Ting", ""], ["Gopalan", "Arjun", ""], ["Heydon", "Allan", ""], ["Ferng", "Chun-Sung", ""], ["Miyara", "Reah", ""], ["Fuxman", "Ariel", ""], ["Peng", "Futang", ""], ["Li", "Zhen", ""], ["Duerig", "Tom", ""], ["Tomkins", "Andrew", ""]]}, {"id": "2105.12866", "submitter": "Xiaoliang Wan", "authors": "Xiaoliang Wan and Kejun Tang", "title": "Augmented KRnet for density estimation and approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we have proposed augmented KRnets including both discrete and\ncontinuous models. One difficulty in flow-based generative modeling is to\nmaintain the invertibility of the transport map, which is often a trade-off\nbetween effectiveness and robustness. The exact invertibility has been achieved\nin the real NVP using a specific pattern to exchange information between two\nseparated groups of dimensions. KRnet has been developed to enhance the\ninformation exchange among data dimensions by incorporating the\nKnothe-Rosenblatt rearrangement into the structure of the transport map. Due to\nthe maintenance of exact invertibility, a full nonlinear update of all data\ndimensions needs three iterations in KRnet. To alleviate this issue, we will\nadd augmented dimensions that act as a channel for communications among the\ndata dimensions. In the augmented KRnet, a fully nonlinear update is achieved\nin two iterations. We also show that the augmented KRnet can be reformulated as\nthe discretization of a neural ODE, where the exact invertibility is kept such\nthat the adjoint method can be formulated with respect to the discretized ODE\nto obtain the exact gradient. Numerical experiments have been implemented to\ndemonstrate the effectiveness of our models.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 22:20:16 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 00:08:27 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wan", "Xiaoliang", ""], ["Tang", "Kejun", ""]]}, {"id": "2105.12876", "submitter": "Pratik K. Biswas", "authors": "Pratik K. Biswas, Songlin Liu", "title": "A Hybrid Recommender System for Recommending Smartphones to Prospective\n  Customers", "comments": "Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems are a subclass of machine learning systems that employ\nsophisticated information filtering strategies to reduce the search time and\nsuggest the most relevant items to any particular user. Hybrid recommender\nsystems combine multiple recommendation strategies in different ways to benefit\nfrom their complementary advantages. Some hybrid recommender systems have\ncombined collaborative filtering and content-based approaches to build systems\nthat are more robust. In this paper, we propose a hybrid recommender system,\nwhich combines Alternative Least Squares (ALS) based collaborative filtering\nwith deep learning to enhance recommendation performance as well as overcome\nthe limitations associated with the collaborative filtering approach,\nespecially concerning its cold start problem. In essence, we use the outputs\nfrom ALS (collaborative filtering) to influence the recommendations from a Deep\nNeural Network (DNN), which combines characteristic, contextual, structural and\nsequential information, in a big data processing framework. We have conducted\nseveral experiments in testing the efficacy of the proposed hybrid architecture\nin recommending smartphones to prospective customers and compared its\nperformance with other open-source recommenders. The results have shown that\nthe proposed system has outperformed several existing hybrid recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 23:10:51 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Biswas", "Pratik K.", ""], ["Liu", "Songlin", ""]]}, {"id": "2105.12893", "submitter": "Yuanlu Bai", "authors": "Yuanlu Bai and Tucker Balch and Haoxian Chen and Danial Dervovic and\n  Henry Lam and Svitlana Vyetrenko", "title": "Calibrating Over-Parametrized Simulation Models: A Framework via\n  Eligibility Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulation aims to compute output performance for complex models\nthat lack analytical tractability. To ensure accurate prediction, the model\nneeds to be calibrated and validated against real data. Conventional methods\napproach these tasks by assessing the model-data match via simple hypothesis\ntests or distance minimization in an ad hoc fashion, but they can encounter\nchallenges arising from non-identifiability and high dimensionality. In this\npaper, we investigate a framework to develop calibration schemes that satisfy\nrigorous frequentist statistical guarantees, via a basic notion that we call\neligibility set designed to bypass non-identifiability via a set-based\nestimation. We investigate a feature extraction-then-aggregation approach to\nconstruct these sets that target at multivariate outputs. We demonstrate our\nmethodology on several numerical examples, including an application to\ncalibration of a limit order book market simulator (ABIDES).\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 00:59:29 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Bai", "Yuanlu", ""], ["Balch", "Tucker", ""], ["Chen", "Haoxian", ""], ["Dervovic", "Danial", ""], ["Lam", "Henry", ""], ["Vyetrenko", "Svitlana", ""]]}, {"id": "2105.12894", "submitter": "Chaofan Huang", "authors": "Chaofan Huang, Simin Ma, Shihao Yang", "title": "MAGI-X: Manifold-Constrained Gaussian Process Inference for Unknown\n  System Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equations (ODEs), commonly used to characterize the\ndynamic systems, are difficult to propose in closed-form for many complicated\nscientific applications, even with the help of domain expert. We propose a fast\nand accurate data-driven method, MAGI-X, to learn the unknown dynamic from the\nobservation data in a non-parametric fashion, without the need of any domain\nknowledge. Unlike the existing methods that mainly rely on the costly numerical\nintegration, MAGI-X utilizes the powerful functional approximator of neural\nnetwork to learn the unknown nonlinear dynamic within the MAnifold-constrained\nGaussian process Inference (MAGI) framework that completely circumvents the\nnumerical integration. Comparing against the state-of-the-art methods on three\nrealistic examples, MAGI-X achieves competitive accuracy in both fitting and\nforecasting while only taking a fraction of computational time. Moreover,\nMAGI-X provides practical solution for the inference of partial observed\nsystems, which no previous method is able to handle.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 01:01:40 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:28:33 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Huang", "Chaofan", ""], ["Ma", "Simin", ""], ["Yang", "Shihao", ""]]}, {"id": "2105.12898", "submitter": "Duong Dung", "authors": "Tri Dung Duong, Qian Li, Guandong Xu", "title": "Stochastic Intervention for Causal Effect Estimation", "comments": "Accepted in IJCNN 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal inference methods are widely applied in various decision-making\ndomains such as precision medicine, optimal policy and economics. Central to\nthese applications is the treatment effect estimation of intervention\nstrategies. Current estimation methods are mostly restricted to the\ndeterministic treatment, which however, is unable to address the stochastic\nspace treatment policies. Moreover, previous methods can only make binary\nyes-or-no decisions based on the treatment effect, lacking the capability of\nproviding fine-grained effect estimation degree to explain the process of\ndecision making. In our study, we therefore advance the causal inference\nresearch to estimate stochastic intervention effect by devising a new\nstochastic propensity score and stochastic intervention effect estimator (SIE).\nMeanwhile, we design a customized genetic algorithm specific to stochastic\nintervention effect (Ge-SIO) with the aim of providing causal evidence for\ndecision making. We provide the theoretical analysis and conduct an empirical\nstudy to justify that our proposed measures and algorithms can achieve a\nsignificant performance lift in comparison with state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 01:12:03 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Duong", "Tri Dung", ""], ["Li", "Qian", ""], ["Xu", "Guandong", ""]]}, {"id": "2105.12903", "submitter": "MIngchao Liang", "authors": "Mingchao Liang, Florian Meyer", "title": "Neural Enhanced Belief Propagation for Cooperative Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.RO eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location-aware networks will introduce innovative services and applications\nfor modern convenience, applied ocean sciences, and public safety. In this\npaper, we establish a hybrid method for model-based and data-driven inference.\nWe consider a cooperative localization (CL) scenario where the mobile agents in\na wireless network aim to localize themselves by performing pairwise\nobservations with other agents and by exchanging location information. A\ntraditional method for distributed CL in large agent networks is belief\npropagation (BP) which is completely model-based and is known to suffer from\nproviding inconsistent (overconfident) estimates. The proposed approach\naddresses these limitations by complementing BP with learned information\nprovided by a graph neural network (GNN). We demonstrate numerically that our\nmethod can improve estimation accuracy and avoid overconfident beliefs, while\nits computational complexity remains comparable to BP. Notably, more consistent\nbeliefs are obtained by not explicitly addressing overconfidence in the loss\nfunction used for training of the GNN.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 01:42:54 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Liang", "Mingchao", ""], ["Meyer", "Florian", ""]]}, {"id": "2105.12909", "submitter": "Shahine Bouabid", "authors": "Siu Lun Chau, Shahine Bouabid, Dino Sejdinovic", "title": "Deconditional Downscaling with Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Refining low-resolution (LR) spatial fields with high-resolution (HR)\ninformation is challenging as the diversity of spatial datasets often prevents\ndirect matching of observations. Yet, when LR samples are modeled as aggregate\nconditional means of HR samples with respect to a mediating variable that is\nglobally observed, the recovery of the underlying fine-grained field can be\nframed as taking an \"inverse\" of the conditional expectation, namely a\ndeconditioning problem. In this work, we introduce conditional mean processes\n(CMP), a new class of Gaussian Processes describing conditional means. By\ntreating CMPs as inter-domain features of the underlying field, a posterior for\nthe latent field can be established as a solution to the deconditioning\nproblem. Furthermore, we show that this solution can be viewed as a two-staged\nvector-valued kernel ridge regressor and show that it has a minimax optimal\nconvergence rate under mild assumptions. Lastly, we demonstrate its proficiency\nin a synthetic and a real-world atmospheric field downscaling problem, showing\nsubstantial improvements over existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 02:10:22 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 19:32:40 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chau", "Siu Lun", ""], ["Bouabid", "Shahine", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "2105.12916", "submitter": "Hubert Banville", "authors": "Hubert Banville, Sean U.N. Wood, Chris Aimone, Denis-Alexander\n  Engemann and Alexandre Gramfort", "title": "Robust learning from corrupted EEG with dynamic spatial filtering", "comments": "42 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building machine learning models using EEG recorded outside of the laboratory\nsetting requires methods robust to noisy data and randomly missing channels.\nThis need is particularly great when working with sparse EEG montages (1-6\nchannels), often encountered in consumer-grade or mobile EEG devices. Neither\nclassical machine learning models nor deep neural networks trained end-to-end\non EEG are typically designed or tested for robustness to corruption, and\nespecially to randomly missing channels. While some studies have proposed\nstrategies for using data with missing channels, these approaches are not\npractical when sparse montages are used and computing power is limited (e.g.,\nwearables, cell phones). To tackle this problem, we propose dynamic spatial\nfiltering (DSF), a multi-head attention module that can be plugged in before\nthe first layer of a neural network to handle missing EEG channels by learning\nto focus on good channels and to ignore bad ones. We tested DSF on public EEG\ndata encompassing ~4,000 recordings with simulated channel corruption and on a\nprivate dataset of ~100 at-home recordings of mobile EEG with natural\ncorruption. Our proposed approach achieves the same performance as baseline\nmodels when no noise is applied, but outperforms baselines by as much as 29.4%\naccuracy when significant channel corruption is present. Moreover, DSF outputs\nare interpretable, making it possible to monitor channel importance in\nreal-time. This approach has the potential to enable the analysis of EEG in\nchallenging settings where channel corruption hampers the reading of brain\nsignals.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 02:33:16 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Banville", "Hubert", ""], ["Wood", "Sean U. N.", ""], ["Aimone", "Chris", ""], ["Engemann", "Denis-Alexander", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "2105.12918", "submitter": "Likang Wu", "authors": "Likang Wu, Zhi Li, Hongke Zhao, Qi Liu, Enhong Chen", "title": "Estimating Fund-Raising Performance for Start-up Projects from a Market\n  Graph Perspective", "comments": "accepted by Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online innovation market, the fund-raising performance of the start-up\nproject is a concerning issue for creators, investors and platforms.\nUnfortunately, existing studies always focus on modeling the fund-raising\nprocess after the publishment of a project but the predicting of a project\nattraction in the market before setting up is largely unexploited. Usually,\nthis prediction is always with great challenges to making a comprehensive\nunderstanding of both the start-up project and market environment. To that end,\nin this paper, we present a focused study on this important problem from a\nmarket graph perspective. Specifically, we propose a Graph-based Market\nEnvironment (GME) model for predicting the fund-raising performance of the\nunpublished project by exploiting the market environment. In addition, we\ndiscriminatively model the project competitiveness and market preferences by\ndesigning two graph-based neural network architectures and incorporating them\ninto a joint optimization stage. Furthermore, to explore the information\npropagation problem with dynamic environment in a large-scale market graph, we\nextend the GME model with parallelizing competitiveness quantification and\nhierarchical propagation algorithm. Finally, we conduct extensive experiments\non real-world data. The experimental results clearly demonstrate the\neffectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 02:39:30 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wu", "Likang", ""], ["Li", "Zhi", ""], ["Zhao", "Hongke", ""], ["Liu", "Qi", ""], ["Chen", "Enhong", ""]]}, {"id": "2105.12920", "submitter": "Dusan Stosic", "authors": "Darko Stosic, Dusan Stosic", "title": "Search Spaces for Neural Model Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While larger neural models are pushing the boundaries of what deep learning\ncan do, often more weights are needed to train models rather than to run\ninference for tasks. This paper seeks to understand this behavior using search\nspaces -- adding weights creates extra degrees of freedom that form new paths\nfor optimization (or wider search spaces) rendering neural model training more\neffective. We then show how we can augment search spaces to train sparse models\nattaining competitive scores across dozens of deep learning workloads. They are\nalso are tolerant of structures targeting current hardware, opening avenues for\ntraining and inference acceleration. Our work encourages research to explore\nbeyond massive neural models being used today.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 02:50:08 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Stosic", "Darko", ""], ["Stosic", "Dusan", ""]]}, {"id": "2105.12923", "submitter": "Tianqi Wang", "authors": "Tianqi Wang, Dong Eui Chang", "title": "Robust Navigation for Racing Drones based on Imitation Learning and\n  Modularization", "comments": "Published at the 2021 International Conference on Robotics and\n  Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a vision-based modularized drone racing navigation system\nthat uses a customized convolutional neural network (CNN) for the perception\nmodule to produce high-level navigation commands and then leverages a\nstate-of-the-art planner and controller to generate low-level control commands,\nthus exploiting the advantages of both data-based and model-based approaches.\nUnlike the state-of-the-art method which only takes the current camera image as\nthe CNN input, we further add the latest three drone states as part of the\ninputs. Our method outperforms the state-of-the-art method in various track\nlayouts and offers two switchable navigation behaviors with a single trained\nnetwork. The CNN-based perception module is trained to imitate an expert policy\nthat automatically generates ground truth navigation commands based on the\npre-computed global trajectories. Owing to the extensive randomization and our\nmodified dataset aggregation (DAgger) policy during data collection, our\nnavigation system, which is purely trained in simulation with synthetic\ntextures, successfully operates in environments with randomly-chosen\nphotorealistic textures without further fine-tuning.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 03:26:40 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wang", "Tianqi", ""], ["Chang", "Dong Eui", ""]]}, {"id": "2105.12937", "submitter": "Dong Li", "authors": "Ruoming Jin and Dong Li and Jing Gao and Zhi Liu and Li Chen and Yang\n  Zhou", "title": "Towards a Better Understanding of Linear Models for Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/3447548.3467428", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, linear regression models, such as EASE and SLIM, have shown to\noften produce rather competitive results against more sophisticated deep\nlearning models. On the other side, the (weighted) matrix factorization\napproaches have been popular choices for recommendation in the past and widely\nadopted in the industry. In this work, we aim to theoretically understand the\nrelationship between these two approaches, which are the cornerstones of\nmodel-based recommendations. Through the derivation and analysis of the\nclosed-form solutions for two basic regression and matrix factorization\napproaches, we found these two approaches are indeed inherently related but\nalso diverge in how they \"scale-down\" the singular values of the original\nuser-item interaction matrix. This analysis also helps resolve the questions\nrelated to the regularization parameter range and model complexities. We\nfurther introduce a new learning algorithm in searching (hyper)parameters for\nthe closed-form solution and utilize it to discover the nearby models of the\nexisting solutions. The experimental results demonstrate that the basic models\nand their closed-form solutions are indeed quite competitive against the\nstate-of-the-art models, thus, confirming the validity of studying the basic\nmodels. The effectiveness of exploring the nearby models are also\nexperimentally validated.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 04:17:04 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 03:38:09 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Jin", "Ruoming", ""], ["Li", "Dong", ""], ["Gao", "Jing", ""], ["Liu", "Zhi", ""], ["Chen", "Li", ""], ["Zhou", "Yang", ""]]}, {"id": "2105.12941", "submitter": "Jilei Yang", "authors": "Jilei Yang, Diana Negoescu, Parvez Ahammad", "title": "Intellige: A User-Facing Model Explainer for Narrative Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predictive machine learning models often lack interpretability, resulting in\nlow trust from model end users despite having high predictive performance.\nWhile many model interpretation approaches return top important features to\nhelp interpret model predictions, these top features may not be well-organized\nor intuitive to end users, which limits model adoption rates. In this paper, we\npropose Intellige, a user-facing model explainer that creates user-digestible\ninterpretations and insights reflecting the rationale behind model predictions.\nIntellige builds an end-to-end pipeline from machine learning platforms to end\nuser platforms, and provides users with an interface for implementing model\ninterpretation approaches and for customizing narrative insights. Intellige is\na platform consisting of four components: Model Importer, Model Interpreter,\nNarrative Generator, and Narrative Exporter. We describe these components, and\nthen demonstrate the effectiveness of Intellige through use cases at LinkedIn.\nQuantitative performance analyses indicate that Intellige's narrative insights\nlead to lifts in adoption rates of predictive model recommendations, as well as\nto increases in downstream key metrics such as revenue when compared to\nprevious approaches, while qualitative analyses indicate positive feedback from\nend users.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 05:11:47 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Yang", "Jilei", ""], ["Negoescu", "Diana", ""], ["Ahammad", "Parvez", ""]]}, {"id": "2105.12991", "submitter": "Taisuke Kobayashi", "authors": "Taisuke Kobayashi", "title": "Optimistic Reinforcement Learning by Forward Kullback-Leibler Divergence\n  Optimization", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a new interpretation of reinforcement learning (RL) as\nreverse Kullback-Leibler (KL) divergence optimization, and derives a new\noptimization method using forward KL divergence. Although RL originally aims to\nmaximize return indirectly through optimization of policy, the recent work by\nLevine has proposed a different derivation process with explicit consideration\nof optimality as stochastic variable. This paper follows this concept and\nformulates the traditional learning laws for both value function and policy as\nthe optimization problems with reverse KL divergence including optimality.\nFocusing on the asymmetry of KL divergence, the new optimization problems with\nforward KL divergence are derived. Remarkably, such new optimization problems\ncan be regarded as optimistic RL. That optimism is intuitively specified by a\nhyperparameter converted from an uncertainty parameter. In addition, it can be\nenhanced when it is integrated with prioritized experience replay and\neligibility traces, both of which accelerate learning. The effects of this\nexpected optimism was investigated through learning tendencies on numerical\nsimulations using Pybullet. As a result, moderate optimism accelerated learning\nand yielded higher rewards. In a realistic robotic simulation, the proposed\nmethod with the moderate optimism outperformed one of the state-of-the-art RL\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:24:51 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kobayashi", "Taisuke", ""]]}, {"id": "2105.12995", "submitter": "Thomas Dopierre", "authors": "Thomas Dopierre, Christophe Gravier, Wilfried Logerais", "title": "ProtAugment: Unsupervised diverse short-texts paraphrasing for intent\n  detection meta-learning", "comments": "Accepted at the 59th Annual Meeting of the Association for\n  Computational Linguistics (ACL2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research considers few-shot intent detection as a meta-learning\nproblem: the model is learning to learn from a consecutive set of small tasks\nnamed episodes. In this work, we propose ProtAugment, a meta-learning algorithm\nfor short texts classification (the intent detection task). ProtAugment is a\nnovel extension of Prototypical Networks, that limits overfitting on the bias\nintroduced by the few-shots classification objective at each episode. It relies\non diverse paraphrasing: a conditional language model is first fine-tuned for\nparaphrasing, and diversity is later introduced at the decoding stage at each\nmeta-learning episode. The diverse paraphrasing is unsupervised as it is\napplied to unlabelled data, and then fueled to the Prototypical Network\ntraining objective as a consistency loss. ProtAugment is the state-of-the-art\nmethod for intent detection meta-learning, at no extra labeling efforts and\nwithout the need to fine-tune a conditional language model on a given\napplication domain.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:31:27 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dopierre", "Thomas", ""], ["Gravier", "Christophe", ""], ["Logerais", "Wilfried", ""]]}, {"id": "2105.13001", "submitter": "Shuo Yang", "authors": "Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, Tongliang\n  Liu", "title": "Estimating Instance-dependent Label-noise Transition Matrix using DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In label-noise learning, estimating the transition matrix is a hot topic as\nthe matrix plays an important role in building statistically consistent\nclassifiers. Traditionally, the transition from clean distribution to noisy\ndistribution (i.e., clean label transition matrix) has been widely exploited to\nlearn a clean label classifier by employing the noisy data. Motivated by that\nclassifiers mostly output Bayes optimal labels for prediction, in this paper,\nwe study to directly model the transition from Bayes optimal distribution to\nnoisy distribution (i.e., Bayes label transition matrix) and learn a Bayes\noptimal label classifier. Note that given only noisy data, it is ill-posed to\nestimate either the clean label transition matrix or the Bayes label transition\nmatrix. But favorably, Bayes optimal labels are less uncertain compared with\nthe clean labels, i.e., the class posteriors of Bayes optimal labels are\none-hot vectors while those of clean labels are not. This enables two\nadvantages to estimate the Bayes label transition matrix, i.e., (a) we could\ntheoretically recover a set of Bayes optimal labels under mild conditions; (b)\nthe feasible solution space is much smaller. By exploiting the advantages, we\nestimate the Bayes label transition matrix by employing a deep neural network\nin a parameterized way, leading to better generalization and superior\nclassification performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:36:54 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Yang", "Shuo", ""], ["Yang", "Erkun", ""], ["Han", "Bo", ""], ["Liu", "Yang", ""], ["Xu", "Min", ""], ["Niu", "Gang", ""], ["Liu", "Tongliang", ""]]}, {"id": "2105.13003", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Yongfeng Huang", "title": "Rethinking InfoNCE: How Many Negative Samples Do You Need?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  InfoNCE loss is a widely used loss function for contrastive model training.\nIt aims to estimate the mutual information between a pair of variables by\ndiscriminating between each positive pair and its associated $K$ negative\npairs. It is proved that when the sample labels are clean, the lower bound of\nmutual information estimation is tighter when more negative samples are\nincorporated, which usually yields better model performance. However, in many\nreal-world tasks the labels often contain noise, and incorporating too many\nnoisy negative samples for model training may be suboptimal. In this paper, we\nstudy how many negative samples are optimal for InfoNCE in different scenarios\nvia a semi-quantitative theoretical framework. More specifically, we first\npropose a probabilistic model to analyze the influence of the negative sampling\nratio $K$ on training sample informativeness. Then, we design a training\neffectiveness function to measure the overall influence of training samples on\nmodel learning based on their informativeness. We estimate the optimal negative\nsampling ratio using the $K$ value that maximizes the training effectiveness\nfunction. Based on our framework, we further propose an adaptive negative\nsampling method that can dynamically adjust the negative sampling ratio to\nimprove InfoNCE based model training. Extensive experiments on different\nreal-world datasets show our framework can accurately predict the optimal\nnegative sampling ratio in different tasks, and our proposed adaptive negative\nsampling method can achieve better performance than the commonly used fixed\nnegative sampling ratio strategy.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:38:29 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2105.13010", "submitter": "Yunfei Yang", "authors": "Jian Huang, Yuling Jiao, Zhen Li, Shiao Liu, Yang Wang, Yunfei Yang", "title": "An error analysis of generative adversarial networks for learning\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how well generative adversarial networks (GANs) learn\nprobability distributions from finite samples. Our main results establish the\nconvergence rates of GANs under a collection of integral probability metrics\ndefined through H\\\"older classes, including the Wasserstein distance as a\nspecial case. We also show that GANs are able to adaptively learn data\ndistributions with low-dimensional structures or have H\\\"older densities, when\nthe network architectures are chosen properly. In particular, for distributions\nconcentrated around a low-dimensional set, we show that the learning rates of\nGANs do not depend on the high ambient dimension, but on the lower intrinsic\ndimension. Our analysis is based on a new oracle inequality decomposing the\nestimation error into the generator and discriminator approximation error and\nthe statistical error, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:55:19 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 07:28:00 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 02:19:06 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 03:03:30 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Li", "Zhen", ""], ["Liu", "Shiao", ""], ["Wang", "Yang", ""], ["Yang", "Yunfei", ""]]}, {"id": "2105.13011", "submitter": "Subhayan De", "authors": "Subhayan De and Alireza Doostan", "title": "Neural Network Training Using $\\ell_1$-Regularization and Bi-fidelity\n  Data", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the capability of accurately representing a functional relationship\nbetween the inputs of a physical system's model and output quantities of\ninterest, neural networks have become popular for surrogate modeling in\nscientific applications. However, as these networks are over-parameterized,\ntheir training often requires a large amount of data. To prevent overfitting\nand improve generalization error, regularization based on, e.g., $\\ell_1$- and\n$\\ell_2$-norms of the parameters is applied. Similarly, multiple connections of\nthe network may be pruned to increase sparsity in the network parameters. In\nthis paper, we explore the effects of sparsity promoting\n$\\ell_1$-regularization on training neural networks when only a small training\ndataset from a high-fidelity model is available. As opposed to standard\n$\\ell_1$-regularization that is known to be inadequate, we consider two\nvariants of $\\ell_1$-regularization informed by the parameters of an identical\nnetwork trained using data from lower-fidelity models of the problem at hand.\nThese bi-fidelity strategies are generalizations of transfer learning of neural\nnetworks that uses the parameters learned from a large low-fidelity dataset to\nefficiently train networks for a small high-fidelity dataset. We also compare\nthe bi-fidelity strategies with two $\\ell_1$-regularization methods that only\nuse the high-fidelity dataset. Three numerical examples for propagating\nuncertainty through physical systems are used to show that the proposed\nbi-fidelity $\\ell_1$-regularization strategies produce errors that are one\norder of magnitude smaller than those of networks trained only using datasets\nfrom the high-fidelity models.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:56:17 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 19:54:39 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["De", "Subhayan", ""], ["Doostan", "Alireza", ""]]}, {"id": "2105.13017", "submitter": "Junwen Yang", "authors": "Junwen Yang, Vincent Y. F. Tan", "title": "Towards Minimax Optimal Best Arm Identification in Linear Bandits", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of best arm identification in linear bandits in the\nfixed-budget setting. By leveraging properties of the G-optimal design and\nincorporating it into the arm allocation rule, we design a parameter-free\nalgorithm, Optimal Design-based Linear Best Arm Identification (OD-LinBAI). We\nprovide a theoretical analysis of the failure probability of OD-LinBAI. While\nthe performances of existing methods (e.g., BayesGap) depend on all the\noptimality gaps, OD-LinBAI depends on the gaps of the top $d$ arms, where $d$\nis the effective dimension of the linear bandit instance. Furthermore, we\npresent a minimax lower bound for this problem. The upper and lower bounds show\nthat OD-LinBAI is minimax optimal up to multiplicative factors in the exponent.\nFinally, numerical experiments corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 09:19:10 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Yang", "Junwen", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2105.13031", "submitter": "Dario  Izzo", "authors": "Dario Izzo and Pablo G\\'omez", "title": "Geodesy of irregular small bodies via neural density fields: geodesyNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach based on artificial neural networks, so-called\ngeodesyNets, and present compelling evidence of their ability to serve as\naccurate geodetic models of highly irregular bodies using minimal prior\ninformation on the body. The approach does not rely on the body shape\ninformation but, if available, can harness it. GeodesyNets learn a\nthree-dimensional, differentiable, function representing the body density,\nwhich we call neural density field. The body shape, as well as other geodetic\nproperties, can easily be recovered. We investigate six different shapes\nincluding the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and\n25143 Itokawa for which shape models developed during close proximity surveys\nare available. Both heterogeneous and homogeneous mass distributions are\nconsidered. The gravitational acceleration computed from the trained\ngeodesyNets models, as well as the inferred body shape, show great accuracy in\nall cases with a relative error on the predicted acceleration smaller than 1\\%\neven close to the asteroid surface. When the body shape information is\navailable, geodesyNets can seamlessly exploit it and be trained to represent a\nhigh-fidelity neural density field able to give insights into the internal\nstructure of the body. This work introduces a new unexplored approach to\ngeodesy, adding a powerful tool to consolidated ones based on spherical\nharmonics, mascon models and polyhedral gravity.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 09:56:12 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Izzo", "Dario", ""], ["G\u00f3mez", "Pablo", ""]]}, {"id": "2105.13036", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "P. Gloor, A. Fronzetti Colladon, J. M. de Oliveira, P. Rovelli", "title": "Put your money where your mouth is: Using deep learning to identify\n  consumer tribes from word usage", "comments": null, "journal-ref": "International Journal of Information Management 51, 101924 (2020)", "doi": "10.1016/j.ijinfomgt.2019.03.011", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Internet and social media offer firms novel ways of managing their marketing\nstrategy and gain competitive advantage. The groups of users expressing\nthemselves on the Internet about a particular topic, product, or brand are\nfrequently called a virtual tribe or E-tribe. However, there are no automatic\ntools for identifying and studying the characteristics of these virtual tribes.\nTowards this aim, this paper presents Tribefinder, a system to reveal Twitter\nusers' tribal affiliations, by analyzing their tweets and language use. To show\nthe potential of this instrument, we provide an example considering three\nspecific tribal macro-categories: alternative realities, lifestyle, and\nrecreation. In addition, we discuss the different characteristics of each\nidentified tribe, in terms of use of language and social interaction metrics.\nTribefinder illustrates the importance of adopting a new lens for studying\nvirtual tribes, which is crucial for firms to properly design their marketing\nstrategy, and for scholars to extend prior marketing research.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:06:56 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Gloor", "P.", ""], ["Colladon", "A. Fronzetti", ""], ["de Oliveira", "J. M.", ""], ["Rovelli", "P.", ""]]}, {"id": "2105.13052", "submitter": "Nicolas Boull\\'e", "authors": "Nicolas Boull\\'e, Alex Townsend", "title": "A generalization of the randomized singular value decomposition", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized singular value decomposition (SVD) is a popular and effective\nalgorithm for computing a near-best rank $k$ approximation of a matrix $A$\nusing matrix-vector products with standard Gaussian vectors. Here, we\ngeneralize the theory of randomized SVD to multivariable Gaussian vectors,\nallowing one to incorporate prior knowledge of $A$ into the algorithm. This\nenables us to explore the continuous analogue of the randomized SVD for\nHilbert--Schmidt (HS) operators using operator-function products with functions\ndrawn from a Gaussian process (GP). We then construct a new covariance kernel\nfor GPs, based on weighted Jacobi polynomials, which allows us to rapidly\nsample the GP and control the smoothness of the randomly generated functions.\nNumerical examples on matrices and HS operators demonstrate the applicability\nof the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:39:37 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Boull\u00e9", "Nicolas", ""], ["Townsend", "Alex", ""]]}, {"id": "2105.13071", "submitter": "Daniel Stan", "authors": "Oliver Markgraf, Daniel Stan, and Anthony W. Lin", "title": "Learning Union of Integer Hypercubes with Queries (Technical Report)", "comments": "Extended version (technical report) of a paper published in CAV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a finite union of integer (axis-aligned)\nhypercubes over the d-dimensional integer lattice, i.e., whose edges are\nparallel to the coordinate axes. This is a natural generalization of the\nclassic problem in the computational learning theory of learning rectangles. We\nprovide a learning algorithm with access to a minimally adequate teacher (i.e.\nmembership and equivalence oracles) that solves this problem in\npolynomial-time, for any fixed dimension d. Over a non-fixed dimension, the\nproblem subsumes the problem of learning DNF boolean formulas, a central open\nproblem in the field. We have also provided extensions to handle infinite\nhypercubes in the union, as well as showing how subset queries could improve\nthe performance of the learning algorithm in practice. Our problem has a\nnatural application to the problem of monadic decomposition of quantifier-free\ninteger linear arithmetic formulas, which has been actively studied in recent\nyears. In particular, a finite union of integer hypercubes correspond to a\nfinite disjunction of monadic predicates over integer linear arithmetic\n(without modulo constraints). Our experiments suggest that our learning\nalgorithms substantially outperform the existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:39:10 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Markgraf", "Oliver", ""], ["Stan", "Daniel", ""], ["Lin", "Anthony W.", ""]]}, {"id": "2105.13093", "submitter": "Mary Phuong", "authors": "Mary Phuong, Christoph H. Lampert", "title": "Towards Understanding Knowledge Distillation", "comments": "ICML'19. Post-edited to add related work. arXiv admin note: text\n  overlap with arXiv:2003.13438 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation, i.e., one classifier being trained on the outputs of\nanother classifier, is an empirically very successful technique for knowledge\ntransfer between classifiers. It has even been observed that classifiers learn\nmuch faster and more reliably if trained with the outputs of another classifier\nas soft labels, instead of from ground truth data. So far, however, there is no\nsatisfactory theoretical explanation of this phenomenon. In this work, we\nprovide the first insights into the working mechanisms of distillation by\nstudying the special case of linear and deep linear classifiers. Specifically,\nwe prove a generalization bound that establishes fast convergence of the\nexpected risk of a distillation-trained linear classifier. From the bound and\nits proof we extract three key factors that determine the success of\ndistillation: * data geometry -- geometric properties of the data distribution,\nin particular class separation, has a direct influence on the convergence speed\nof the risk; * optimization bias -- gradient descent optimization finds a very\nfavorable minimum of the distillation objective; and * strong monotonicity --\nthe expected risk of the student classifier always decreases when the size of\nthe training set grows.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:45:08 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Phuong", "Mary", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "2105.13099", "submitter": "Nicolas Keriven", "authors": "Nicolas Keriven, Alberto Bietti, Samuel Vaiter", "title": "On the Universality of Graph Neural Networks on Large Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximation power of Graph Neural Networks (GNNs) on latent\nposition random graphs. In the large graph limit, GNNs are known to converge to\ncertain \"continuous\" models known as c-GNNs, which directly enables a study of\ntheir approximation power on random graph models. In the absence of input node\nfeatures however, just as GNNs are limited by the Weisfeiler-Lehman isomorphism\ntest, c-GNNs will be severely limited on simple random graph models. For\ninstance, they will fail to distinguish the communities of a well-separated\nStochastic Block Model (SBM) with constant degree function. Thus, we consider\nrecently proposed architectures that augment GNNs with unique node identifiers,\nreferred to as Structural GNNs here (SGNNs). We study the convergence of SGNNs\nto their continuous counterpart (c-SGNNs) in the large random graph limit,\nunder new conditions on the node identifiers. We then show that c-SGNNs are\nstrictly more powerful than c-GNNs in the continuous limit, and prove their\nuniversality on several random graph models of interest, including most SBMs\nand a large class of random geometric graphs. Our results cover both\npermutation-invariant and permutation-equivariant architectures.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:52:36 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:23:31 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Keriven", "Nicolas", ""], ["Bietti", "Alberto", ""], ["Vaiter", "Samuel", ""]]}, {"id": "2105.13102", "submitter": "Andrea Serani", "authors": "Danny D'Agostino, Andrea Serani, Frederick Stern, Matteo Diez", "title": "Recurrent-type Neural Networks for Real-time Short-term Prediction of\n  Ship Motions in High Sea State", "comments": "10 pages, 5 figures, to be published in conference proceeding of IX\n  International Conference on Computer Methods in Marine Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction capability of recurrent-type neural networks is investigated\nfor real-time short-term prediction (nowcasting) of ship motions in high sea\nstate. Specifically, the performance of recurrent neural networks, long-short\nterm memory, and gated recurrent units models are assessed and compared using a\ndata set coming from computational fluid dynamics simulations of a\nself-propelled destroyer-type vessel in stern-quartering sea state 7. Time\nseries of incident wave, ship motions, rudder angle, as well as immersion\nprobes, are used as variables for a nowcasting problem. The objective is to\nobtain about 20 s ahead prediction. Overall, the three methods provide\npromising and comparable results.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:58:15 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["D'Agostino", "Danny", ""], ["Serani", "Andrea", ""], ["Stern", "Frederick", ""], ["Diez", "Matteo", ""]]}, {"id": "2105.13114", "submitter": "Walt Woods", "authors": "Walt Woods", "title": "RL-GRIT: Reinforcement Learning for Grammar Inference", "comments": "13 pages, published at IEEE LangSec 2021\n  (https://langsec.org/spw21/papers.html). ArXiv version: lacking correct\n  'minted' package behavior, so some atoms may look a little off", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working to understand usage of a data format, examples of the data\nformat are often more representative than the format's specification. For\nexample, two different applications might use very different JSON\nrepresentations, or two PDF-writing applications might make use of very\ndifferent areas of the PDF specification to realize the same rendered content.\nThe complexity arising from these distinct origins can lead to large,\ndifficult-to-understand attack surfaces, presenting a security concern when\nconsidering both exfiltration and data schizophrenia. Grammar inference can aid\nin describing the practical language generator behind examples of a data\nformat. However, most grammar inference research focuses on natural language,\nnot data formats, and fails to support crucial features such as type recursion.\nWe propose a novel set of mechanisms for grammar inference, RL-GRIT, and apply\nthem to understanding de facto data formats. After reviewing existing grammar\ninference solutions, it was determined that a new, more flexible scaffold could\nbe found in Reinforcement Learning (RL). Within this work, we lay out the many\nalgorithmic changes required to adapt RL from its traditional, sequential-time\nenvironment to the highly interdependent environment of parsing. The result is\nan algorithm which can demonstrably learn recursive control structures in\nsimple data formats, and can extract meaningful structure from fragments of the\nPDF format. Whereas prior work in grammar inference focused on either regular\nlanguages or constituency parsing, we show that RL can be used to surpass the\nexpressiveness of both classes, and offers a clear path to learning\ncontext-sensitive languages. The proposed algorithm can serve as a building\nblock for understanding the ecosystems of de facto data formats.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:48:39 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Woods", "Walt", ""]]}, {"id": "2105.13120", "submitter": "Fuzhao Xue", "authors": "Shenggui Li, Fuzhao Xue, Yongbin Li, Yang You", "title": "Sequence Parallelism: Making 4D Parallelism Possible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within Transformer, self-attention is the key module to learn powerful\ncontext-aware representations. However, self-attention suffers from quadratic\nmemory requirements with respect to the sequence length, which limits us to\nprocess longer sequence on GPU. In this work, we propose sequence parallelism,\na memory efficient parallelism method to help us break input sequence length\nlimitation and train with longer sequence on GPUs. Compared with existing\nparallelism, our approach no longer requires a single device to hold the whole\nsequence. Specifically, we split the input sequence into multiple chunks and\nfeed each chunk into its corresponding device (i.e. GPU). To compute the\nattention output, we communicate attention embeddings among GPUs. Inspired by\nring all-reduce, we integrated ring-style communication with self-attention\ncalculation and proposed Ring Self-Attention (RSA). Our implementation is fully\nbased on PyTorch. Without extra compiler or library changes, our approach is\ncompatible with data parallelism and pipeline parallelism. Experiments show\nthat sequence parallelism performs well when scaling with batch size and\nsequence length. Compared with tensor parallelism, our approach achieved\n$13.7\\times$ and $3.0\\times$ maximum batch size and sequence length\nrespectively when scaling up to 64 NVIDIA P100 GPUs. We plan to integrate our\nsequence parallelism with data, pipeline and tensor parallelism to further\ntrain large-scale models with 4D parallelism in our future work.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:40:58 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Li", "Shenggui", ""], ["Xue", "Fuzhao", ""], ["Li", "Yongbin", ""], ["You", "Yang", ""]]}, {"id": "2105.13121", "submitter": "Shuangjia Zheng", "authors": "Shuangjia Zheng, Tao Zeng, Chengtao Li, Binghong Chen, Connor W.\n  Coley, Yuedong Yang, Ruibo Wu", "title": "BioNavi-NP: Biosynthesis Navigator for Natural Products", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nature, a synthetic master, creates more than 300,000 natural products (NPs)\nwhich are the major constituents of FDA-proved drugs owing to the vast chemical\nspace of NPs. To date, there are fewer than 30,000 validated NPs compounds\ninvolved in about 33,000 known enzyme catalytic reactions, and even fewer\nbiosynthetic pathways are known with complete cascade-connected enzyme\ncatalysis. Therefore, it is valuable to make computer-aided bio-retrosynthesis\npredictions. Here, we develop BioNavi-NP, a navigable and user-friendly\ntoolkit, which is capable of predicting the biosynthetic pathways for NPs and\nNP-like compounds through a novel (AND-OR Tree)-based planning algorithm, an\nenhanced molecular Transformer neural network, and a training set that combines\ngeneral organic transformations and biosynthetic steps. Extensive evaluations\nreveal that BioNavi-NP generalizes well to identifying the reported\nbiosynthetic pathways for 90% of test compounds and recovering the verified\nbuilding blocks for 73%, significantly outperforming conventional rule-based\napproaches. Moreover, BioNavi-NP also shows an outstanding capacity of\nbiologically plausible pathways enumeration. In this sense, BioNavi-NP is a\nleading-edge toolkit to redesign complex biosynthetic pathways of natural\nproducts with applications to total or semi-synthesis and pathway elucidation\nor reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 12:04:38 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zheng", "Shuangjia", ""], ["Zeng", "Tao", ""], ["Li", "Chengtao", ""], ["Chen", "Binghong", ""], ["Coley", "Connor W.", ""], ["Yang", "Yuedong", ""], ["Wu", "Ruibo", ""]]}, {"id": "2105.13125", "submitter": "Gang Mei", "authors": "Zhengjing Ma, Gang Mei, Salvatore Cuomo, Francesco Piccialli", "title": "Heterogeneous Data Fusion Considering Spatial Correlations using Graph\n  Convolutional Networks and its Application in Air Quality Prediction", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heterogeneous data are commonly adopted as the inputs for some models that\npredict the future trends of some observations. Existing predictive models\ntypically ignore the inconsistencies and imperfections in heterogeneous data\nwhile also failing to consider the (1) spatial correlations among monitoring\npoints or (2) predictions for the entire study area. To address the above\nproblems, this paper proposes a deep learning method for fusing heterogeneous\ndata collected from multiple monitoring points using graph convolutional\nnetworks (GCNs) to predict the future trends of some observations and evaluates\nits effectiveness by applying it in an air quality predictions scenario. The\nessential idea behind the proposed method is to (1) fuse the collected\nheterogeneous data based on the locations of the monitoring points with regard\nto their spatial correlations and (2) perform prediction based on global\ninformation rather than local information. In the proposed method, first, we\nassemble a fusion matrix using the proposed RBF-based fusion approach; second,\nbased on the fused data, we construct spatially and temporally correlated data\nas inputs for the predictive model; finally, we employ the spatiotemporal graph\nconvolutional network (STGCN) to predict the future trends of some\nobservations. In the application scenario of air quality prediction, it is\nobserved that (1) the fused data derived from the RBF-based fusion approach\nachieve satisfactory consistency; (2) the performances of the prediction models\nbased on fused data are better than those based on raw data; and (3) the STGCN\nmodel achieves the best performance when compared to those of all baseline\nmodels. The proposed method is applicable for similar scenarios where\ncontinuous heterogeneous data are collected from multiple monitoring points\nscattered across a study area.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:57:31 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ma", "Zhengjing", ""], ["Mei", "Gang", ""], ["Cuomo", "Salvatore", ""], ["Piccialli", "Francesco", ""]]}, {"id": "2105.13127", "submitter": "Lorenzo Pellegrini", "authors": "Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Graffieti, Davide\n  Maltoni", "title": "Continual Learning at the Edge: Real-Time Training on Smartphone Devices", "comments": "6 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-device training for personalized learning is a challenging research\nproblem. Being able to quickly adapt deep prediction models at the edge is\nnecessary to better suit personal user needs. However, adaptation on the edge\nposes some questions on both the efficiency and sustainability of the learning\nprocess and on the ability to work under shifting data distributions. Indeed,\nnaively fine-tuning a prediction model only on the newly available data results\nin catastrophic forgetting, a sudden erasure of previously acquired knowledge.\nIn this paper, we detail the implementation and deployment of a hybrid\ncontinual learning strategy (AR1*) on a native Android application for\nreal-time on-device personalization without forgetting. Our benchmark, based on\nan extension of the CORe50 dataset, shows the efficiency and effectiveness of\nour solution.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:00:31 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Pellegrini", "Lorenzo", ""], ["Lomonaco", "Vincenzo", ""], ["Graffieti", "Gabriele", ""], ["Maltoni", "Davide", ""]]}, {"id": "2105.13131", "submitter": "Soumyajit Chatterjee Mr.", "authors": "Ratna Mandal, Prasenjit Karmakar, Soumyajit Chatterjee, Debaleen Das\n  Spandan, Shouvit Pradhan, Sujoy Saha, Sandip Chakraborty and Subrata Nandi", "title": "Exploiting Multi-modal Contextual Sensing for City-bus's Stay Location\n  Characterization: Towards Sub-60 Seconds Accurate Arrival Time Prediction", "comments": "20 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent city transportation systems are one of the core infrastructures\nof a smart city. The true ingenuity of such an infrastructure lies in providing\nthe commuters with real-time information about citywide transports like public\nbuses, allowing her to pre-plan the travel. However, providing prior\ninformation for transportation systems like public buses in real-time is\ninherently challenging because of the diverse nature of different\nstay-locations that a public bus stops. Although straightforward factors stay\nduration, extracted from unimodal sources like GPS, at these locations look\nerratic, a thorough analysis of public bus GPS trails for 720km of bus travels\nat the city of Durgapur, a semi-urban city in India, reveals that several other\nfine-grained contextual features can characterize these locations accurately.\nAccordingly, we develop BuStop, a system for extracting and characterizing the\nstay locations from multi-modal sensing using commuters' smartphones. Using\nthis multi-modal information BuStop extracts a set of granular contextual\nfeatures that allow the system to differentiate among the different\nstay-location types. A thorough analysis of BuStop using the collected dataset\nindicates that the system works with high accuracy in identifying different\nstay locations like regular bus stops, random ad-hoc stops, stops due to\ntraffic congestion stops at traffic signals, and stops at sharp turns.\nAdditionally, we also develop a proof-of-concept setup on top of BuStop to\nanalyze the potential of the framework in predicting expected arrival time, a\ncritical piece of information required to pre-plan travel, at any given bus\nstop. Subsequent analysis of the PoC framework, through simulation over the\ntest dataset, shows that characterizing the stay-locations indeed helps make\nmore accurate arrival time predictions with deviations less than 60s from the\nground-truth arrival time.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:47:10 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Mandal", "Ratna", ""], ["Karmakar", "Prasenjit", ""], ["Chatterjee", "Soumyajit", ""], ["Spandan", "Debaleen Das", ""], ["Pradhan", "Shouvit", ""], ["Saha", "Sujoy", ""], ["Chakraborty", "Sandip", ""], ["Nandi", "Subrata", ""]]}, {"id": "2105.13132", "submitter": "Yang Li", "authors": "Yang Li, Zinc Zhang, Hutchin Huang", "title": "Enhance Multimodal Model Performance with Data Augmentation: Facebook\n  Hateful Meme Challenge Solution", "comments": "Our code is available at:\n  https://github.com/yangland/hatefulchallenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hateful content detection is one of the areas where deep learning can and\nshould make a significant difference. The Hateful Memes Challenge from Facebook\nhelps fulfill such potential by challenging the contestants to detect hateful\nspeech in multi-modal memes using deep learning algorithms. In this paper, we\nutilize multi-modal, pre-trained models VilBERT and Visual BERT. We improved\nmodels' performance by adding training datasets generated from data\naugmentation. Enlarging the training data set helped us get a more than 2%\nboost in terms of AUROC with the Visual BERT model. Our approach achieved\n0.7439 AUROC along with an accuracy of 0.7037 on the challenge's test set,\nwhich revealed remarkable progress.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:07:09 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 02:05:41 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Li", "Yang", ""], ["Zhang", "Zinc", ""], ["Huang", "Hutchin", ""]]}, {"id": "2105.13135", "submitter": "Jinbae Im", "authors": "Jinbae Im, Moonki Kim, Hoyeop Lee, Hyunsouk Cho, Sehee Chung", "title": "Self-Supervised Multimodal Opinion Summarization", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, opinion summarization, which is the generation of a summary from\nmultiple reviews, has been conducted in a self-supervised manner by considering\na sampled review as a pseudo summary. However, non-text data such as image and\nmetadata related to reviews have been considered less often. To use the\nabundant information contained in non-text data, we propose a self-supervised\nmultimodal opinion summarization framework called MultimodalSum. Our framework\nobtains a representation of each modality using a separate encoder for each\nmodality, and the text decoder generates a summary. To resolve the inherent\nheterogeneity of multimodal data, we propose a multimodal training pipeline. We\nfirst pretrain the text encoder--decoder based solely on text modality data.\nSubsequently, we pretrain the non-text modality encoders by considering the\npretrained text decoder as a pivot for the homogeneous representation of\nmultimodal data. Finally, to fuse multimodal representations, we train the\nentire framework in an end-to-end manner. We demonstrate the superiority of\nMultimodalSum by conducting experiments on Yelp and Amazon datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:29:05 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Im", "Jinbae", ""], ["Kim", "Moonki", ""], ["Lee", "Hoyeop", ""], ["Cho", "Hyunsouk", ""], ["Chung", "Sehee", ""]]}, {"id": "2105.13136", "submitter": "Teeratorn Kadeethum", "authors": "Teeratorn Kadeethum, Daniel O'Malley, Jan Niklas Fuhg, Youngsoo Choi,\n  Jonghyun Lee, Hari S. Viswanathan, Nikolaos Bouklas", "title": "A framework for data-driven solution and parameter estimation of PDEs\n  using conditional generative adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work is the first to employ and adapt the image-to-image translation\nconcept based on conditional generative adversarial networks (cGAN) towards\nlearning a forward and an inverse solution operator of partial differential\nequations (PDEs). Even though the proposed framework could be applied as a\nsurrogate model for the solution of any PDEs, here we focus on steady-state\nsolutions of coupled hydro-mechanical processes in heterogeneous porous media.\nStrongly heterogeneous material properties, which translate to the\nheterogeneity of coefficients of the PDEs and discontinuous features in the\nsolutions, require specialized techniques for the forward and inverse solution\nof these problems. Additionally, parametrization of the spatially heterogeneous\ncoefficients is excessively difficult by using standard reduced order modeling\ntechniques. In this work, we overcome these challenges by employing the\nimage-to-image translation concept to learn the forward and inverse solution\noperators and utilize a U-Net generator and a patch-based discriminator. Our\nresults show that the proposed data-driven reduced order model has competitive\npredictive performance capabilities in accuracy and computational efficiency as\nwell as training time requirements compared to state-of-the-art data-driven\nmethods for both forward and inverse problems.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:30:17 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kadeethum", "Teeratorn", ""], ["O'Malley", "Daniel", ""], ["Fuhg", "Jan Niklas", ""], ["Choi", "Youngsoo", ""], ["Lee", "Jonghyun", ""], ["Viswanathan", "Hari S.", ""], ["Bouklas", "Nikolaos", ""]]}, {"id": "2105.13137", "submitter": "David Ahmedt-Aristizabal", "authors": "David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon Denman, Clinton\n  Fookes, Lars Petersson", "title": "Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past,\n  Present and Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances of data-driven machine learning research, a wide variety of\nprediction problems have been tackled. It has become critical to explore how\nmachine learning and specifically deep learning methods can be exploited to\nanalyse healthcare data. A major limitation of existing methods has been the\nfocus on grid-like data; however, the structure of physiological recordings are\noften irregular and unordered which makes it difficult to conceptualise them as\na matrix. As such, graph neural networks have attracted significant attention\nby exploiting implicit information that resides in a biological system, with\ninteractive nodes connected by edges whose weights can be either temporal\nassociations or anatomical junctions. In this survey, we thoroughly review the\ndifferent types of graph architectures and their applications in healthcare. We\nprovide an overview of these methods in a systematic manner, organized by their\ndomain of application including functional connectivity, anatomical structure\nand electrical-based analysis. We also outline the limitations of existing\ntechniques and discuss potential directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:32:45 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ahmedt-Aristizabal", "David", ""], ["Armin", "Mohammad Ali", ""], ["Denman", "Simon", ""], ["Fookes", "Clinton", ""], ["Petersson", "Lars", ""]]}, {"id": "2105.13144", "submitter": "Varun Chandrasekaran", "authors": "Varun Chandrasekaran, Darren Edge, Somesh Jha, Amit Sharma, Cheng\n  Zhang, Shruti Tople", "title": "Causally Constrained Data Synthesis for Private Data Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making evidence based decisions requires data. However for real-world\napplications, the privacy of data is critical. Using synthetic data which\nreflects certain statistical properties of the original data preserves the\nprivacy of the original data. To this end, prior works utilize differentially\nprivate data release mechanisms to provide formal privacy guarantees. However,\nsuch mechanisms have unacceptable privacy vs. utility trade-offs. We propose\nincorporating causal information into the training process to favorably modify\nthe aforementioned trade-off. We theoretically prove that generative models\ntrained with additional causal knowledge provide stronger differential privacy\nguarantees. Empirically, we evaluate our solution comparing different models\nbased on variational auto-encoders (VAEs), and show that causal information\nimproves resilience to membership inference, with improvements in downstream\nutility.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:46:57 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Chandrasekaran", "Varun", ""], ["Edge", "Darren", ""], ["Jha", "Somesh", ""], ["Sharma", "Amit", ""], ["Zhang", "Cheng", ""], ["Tople", "Shruti", ""]]}, {"id": "2105.13189", "submitter": "Zhiyong Zhou", "authors": "Zhiyong Zhou", "title": "Sparse recovery based on the generalized error function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel sparse recovery method based on the\ngeneralized error function. The penalty function introduced involves both the\nshape and the scale parameters, making it very flexible. The theoretical\nanalysis results in terms of the null space property, the spherical section\nproperty and the restricted invertibility factor are established for both\nconstrained and unconstrained models. The practical algorithms via both the\niteratively reweighted $\\ell_1$ and the difference of convex functions\nalgorithms are presented. Numerical experiments are conducted to illustrate the\nimprovement provided by the proposed approach in various scenarios. Its\npractical application in magnetic resonance imaging (MRI) reconstruction is\nstudied as well.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:36:01 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 01:57:48 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhou", "Zhiyong", ""]]}, {"id": "2105.13191", "submitter": "Veljko Pejovic", "authors": "Alina L. Machidon and Veljko Pejovic", "title": "Deep Learning Techniques for Compressive Sensing-Based Reconstruction\n  and Inference -- A Ubiquitous Systems Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) is a mathematically elegant tool for reducing the\nsampling rate, potentially bringing context-awareness to a wider range of\ndevices. Nevertheless, practical issues with the sampling and reconstruction\nalgorithms prevent further proliferation of CS in real world domains,\nespecially among heterogeneous ubiquitous devices. Deep learning (DL) naturally\ncomplements CS for adapting the sampling matrix, reconstructing the signal, and\nlearning form the compressed samples. While the CS-DL integration has received\nsubstantial research interest recently, it has not yet been thoroughly\nsurveyed, nor has the light been shed on practical issues towards bringing the\nCS-DL to real world implementations in the ubicomp domain. In this paper we\nidentify main possible ways in which CS and DL can interplay, extract key ideas\nfor making CS-DL efficient, identify major trends in CS-DL research space, and\nderive guidelines for future evolution of CS-DL within the ubicomp domain.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 14:04:04 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Machidon", "Alina L.", ""], ["Pejovic", "Veljko", ""]]}, {"id": "2105.13203", "submitter": "Julien Grand-Cl\\'ement", "authors": "Julien Grand-Cl\\'ement, Christian Kroer", "title": "Conic Blackwell Algorithm: Parameter-Free Convex-Concave Saddle-Point\n  Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop new parameter and scale-free algorithms for solving convex-concave\nsaddle-point problems. Our results are based on a new simple regret minimizer,\nthe Conic Blackwell Algorithm$^+$ (CBA$^+$), which attains $O(1/\\sqrt{T})$\naverage regret. Intuitively, our approach generalizes to other decision sets of\ninterest ideas from the Counterfactual Regret minimization (CFR$^+$) algorithm,\nwhich has very strong practical performance for solving sequential games on\nsimplexes. We show how to implement CBA$^+$ for the simplex, $\\ell_{p}$ norm\nballs, and ellipsoidal confidence regions in the simplex, and we present\nnumerical experiments for solving matrix games and distributionally robust\noptimization problems. Our empirical results show that CBA$^+$ is a simple\nalgorithm that outperforms state-of-the-art methods on synthetic data and real\ndata instances, without the need for any choice of step sizes or other\nalgorithmic parameters.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:50:31 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 12:27:34 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Grand-Cl\u00e9ment", "Julien", ""], ["Kroer", "Christian", ""]]}, {"id": "2105.13205", "submitter": "Clara Luc\\'ia Galimberti", "authors": "Clara Luc\\'ia Galimberti, Luca Furieri, Liang Xu, Giancarlo\n  Ferrari-Trecate", "title": "Hamiltonian Deep Neural Networks Guaranteeing Non-vanishing Gradients by\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) training can be difficult due to vanishing and\nexploding gradients during weight optimization through backpropagation. To\naddress this problem, we propose a general class of Hamiltonian DNNs (H-DNNs)\nthat stem from the discretization of continuous-time Hamiltonian systems and\ninclude several existing architectures based on ordinary differential\nequations. Our main result is that a broad set of H-DNNs ensures non-vanishing\ngradients by design for an arbitrary network depth. This is obtained by proving\nthat, using a semi-implicit Euler discretization scheme, the backward\nsensitivity matrices involved in gradient computations are symplectic. We also\nprovide an upper bound to the magnitude of sensitivity matrices, and show that\nexploding gradients can be either controlled through regularization or avoided\nfor special architectures. Finally, we enable distributed implementations of\nbackward and forward propagation algorithms in H-DNNs by characterizing\nappropriate sparsity constraints on the weight matrices. The good performance\nof H-DNNs is demonstrated on benchmark classification problems, including image\nclassification with the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:52:22 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Galimberti", "Clara Luc\u00eda", ""], ["Furieri", "Luca", ""], ["Xu", "Liang", ""], ["Ferrari-Trecate", "Giancarlo", ""]]}, {"id": "2105.13218", "submitter": "Runzhe Wan", "authors": "Runzhe Wan, Sheng Zhang, Chengchun Shi, Shikai Luo and Rui Song", "title": "Pattern Transfer Learning for Reinforcement Learning in Order\n  Dispatching", "comments": "Spotlight paper, RL4ITS, IJCAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Order dispatch is one of the central problems to ride-sharing platforms.\nRecently, value-based reinforcement learning algorithms have shown promising\nperformance on this problem. However, in real-world applications, the\nnon-stationarity of the demand-supply system poses challenges to re-utilizing\ndata generated in different time periods to learn the value function. In this\nwork, motivated by the fact that the relative relationship between the values\nof some states is largely stable across various environments, we propose a\npattern transfer learning framework for value-based reinforcement learning in\nthe order dispatch problem. Our method efficiently captures the value patterns\nby incorporating a concordance penalty. The superior performance of the\nproposed method is supported by experiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:08:34 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 21:25:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wan", "Runzhe", ""], ["Zhang", "Sheng", ""], ["Shi", "Chengchun", ""], ["Luo", "Shikai", ""], ["Song", "Rui", ""]]}, {"id": "2105.13220", "submitter": "Ibnu Daqiqil Id", "authors": "Ibnu Daqiqil Id, Masanobu Abe, Sunao Hara", "title": "Evaluation of concept drift adaptation for acoustic scene classifier\n  based on Kernel Density Drift Detection and Combine Merge Gaussian Mixture\n  Model", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.4595866", "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Based on the experimental results, all concepts drift types have their\nrespective hyperparameter configurations. Simple and gradual concept drift have\nsimilar pattern which requires a smaller {\\alpha} value than recurring concept\ndrift because, in this type of drift, a new concept appear continuously, so it\nneeds a high-frequency model adaptation. However, in recurring concepts, the\nnew concept may repeat in the future, so the lower frequency adaptation is\nbetter. Furthermore, high-frequency model adaptation could lead to an\noverfitting problem. Implementing CMGMM component pruning mechanism help to\ncontrol the number of the active component and improve model performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:09:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Id", "Ibnu Daqiqil", ""], ["Abe", "Masanobu", ""], ["Hara", "Sunao", ""]]}, {"id": "2105.13228", "submitter": "Zenan Ling", "authors": "Xingyu Xie, Qiuhao Wang, Zenan Ling, Xia Li, Yisen Wang, Guangcan Liu,\n  Zhouchen Lin", "title": "Optimization Induced Equilibrium Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit equilibrium models, i.e., deep neural networks (DNNs) defined by\nimplicit equations, have been becoming more and more attractive recently. In\nthis paper, we investigate an emerging question: can an implicit equilibrium\nmodel's equilibrium point be regarded as the solution of an optimization\nproblem? To this end, we first decompose DNNs into a new class of unit layer\nthat is the proximal operator of an implicit convex function while keeping its\noutput unchanged. Then, the equilibrium model of the unit layer can be derived,\nnamed Optimization Induced Equilibrium Networks (OptEq), which can be easily\nextended to deep layers. The equilibrium point of OptEq can be theoretically\nconnected to the solution of its corresponding convex optimization problem with\nexplicit objectives. Based on this, we can flexibly introduce prior properties\nto the equilibrium points: 1) modifying the underlying convex problems\nexplicitly so as to change the architectures of OptEq; and 2) merging the\ninformation into the fixed point iteration, which guarantees to choose the\ndesired equilibrium point when the fixed point set is non-singleton. We show\nthat deep OptEq outperforms previous implicit models even with fewer\nparameters. This work establishes the first step towards the\noptimization-guided design of deep models.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:17:41 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:48:54 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 07:56:14 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xie", "Xingyu", ""], ["Wang", "Qiuhao", ""], ["Ling", "Zenan", ""], ["Li", "Xia", ""], ["Wang", "Yisen", ""], ["Liu", "Guangcan", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2105.13231", "submitter": "Daniel Toyama", "authors": "Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici,\n  Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad and Doina Precup", "title": "AndroidEnv: A Reinforcement Learning Platform for Android", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce AndroidEnv, an open-source platform for Reinforcement Learning\n(RL) research built on top of the Android ecosystem. AndroidEnv allows RL\nagents to interact with a wide variety of apps and services commonly used by\nhumans through a universal touchscreen interface. Since agents train on a\nrealistic simulation of an Android device, they have the potential to be\ndeployed on real devices. In this report, we give an overview of the\nenvironment, highlighting the significant features it provides for research,\nand we present an empirical evaluation of some popular reinforcement learning\nagents on a set of tasks built on this platform.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:20:14 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Toyama", "Daniel", ""], ["Hamel", "Philippe", ""], ["Gergely", "Anita", ""], ["Comanici", "Gheorghe", ""], ["Glaese", "Amelia", ""], ["Ahmed", "Zafarali", ""], ["Jackson", "Tyler", ""], ["Mourad", "Shibl", ""], ["Precup", "Doina", ""]]}, {"id": "2105.13240", "submitter": "Haoyu Li", "authors": "Haoyu Li and Han-Wei Shen", "title": "Time Varying Particle Data Feature Extraction and Tracking with Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Analyzing particle data plays an important role in many scientific\napplications such as fluid simulation, cosmology simulation and molecular\ndynamics. While there exist methods that can perform feature extraction and\ntracking for volumetric data, performing those tasks for particle data is more\nchallenging because of the lack of explicit connectivity information. Although\none may convert the particle data to volume first, this approach is at risk of\nincurring error and increasing the size of the data. In this paper, we take a\ndeep learning approach to create feature representations for scientific\nparticle data to assist feature extraction and tracking. We employ a deep\nlearning model, which produces latent vectors to represent the relation between\nspatial locations and physical attributes in a local neighborhood. With the\nlatent vectors, features can be extracted by clustering these vectors. To\nachieve fast feature tracking, the mean-shift tracking algorithm is applied in\nthe feature space, which only requires inference of the latent vector for\nselected regions of interest. We validate our approach using two datasets and\ncompare our method with other existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:38:14 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Li", "Haoyu", ""], ["Shen", "Han-Wei", ""]]}, {"id": "2105.13245", "submitter": "Juan Ungredda", "authors": "Juan Ungredda and Juergen Branke", "title": "Bayesian Optimisation for Constrained Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world optimisation problems such as hyperparameter tuning in\nmachine learning or simulation-based optimisation can be formulated as\nexpensive-to-evaluate black-box functions. A popular approach to tackle such\nproblems is Bayesian optimisation (BO), which builds a response surface model\nbased on the data collected so far, and uses the mean and uncertainty predicted\nby the model to decide what information to collect next. In this paper, we\npropose a novel variant of the well-known Knowledge Gradient acquisition\nfunction that allows it to handle constraints. We empirically compare the new\nalgorithm with four other state-of-the-art constrained Bayesian optimisation\nalgorithms and demonstrate its superior performance. We also prove theoretical\nconvergence in the infinite budget limit.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:43:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ungredda", "Juan", ""], ["Branke", "Juergen", ""]]}, {"id": "2105.13251", "submitter": "T. Mitchell Roddenberry", "authors": "T. Mitchell Roddenberry, Yu Zhu, Santiago Segarra", "title": "An Impossibility Theorem for Node Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing popularity of graph-based methods for dimensionality\nreduction and representation learning, node embedding functions have become\nimportant objects of study in the literature. In this paper, we take an\naxiomatic approach to understanding node embedding methods, first stating three\nproperties for embedding dissimilarity networks, then proving that all three\ncannot be satisfied simultaneously by any node embedding method. Similar to\nexisting results on the impossibility of clustering under certain axiomatic\nassumptions, this points to fundamental difficulties inherent to node embedding\ntasks. Once these difficulties are identified, we then relax these axioms to\nallow for certain node embedding methods to be admissible in our framework.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:48:41 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Roddenberry", "T. Mitchell", ""], ["Zhu", "Yu", ""], ["Segarra", "Santiago", ""]]}, {"id": "2105.13255", "submitter": "Jie Huang", "authors": "Jie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-mei Hwu", "title": "Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical\n  Core-Fringe Approach", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to measure fine-grained domain relevance - the degree that a term\nis relevant to a broad (e.g., computer science) or narrow (e.g., deep learning)\ndomain. Such measurement is crucial for many downstream tasks in natural\nlanguage processing. To handle long-tail terms, we build a core-anchored\nsemantic graph, which uses core terms with rich description information to\nbridge the vast remaining fringe terms semantically. To support a fine-grained\ndomain without relying on a matching corpus for supervision, we develop\nhierarchical core-fringe learning, which learns core and fringe terms jointly\nin a semi-supervised manner contextualized in the hierarchy of the domain. To\nreduce expensive human efforts, we employ automatic annotation and hierarchical\npositive-unlabeled learning. Our approach applies to big or small domains,\ncovers head or tail terms, and requires little human effort. Extensive\nexperiments demonstrate that our methods outperform strong baselines and even\nsurpass professional human performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:52:34 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Huang", "Jie", ""], ["Chang", "Kevin Chen-Chuan", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2105.13258", "submitter": "Yujun Lin", "authors": "Yujun Lin, Mengtian Yang and Song Han", "title": "NAAS: Neural Accelerator Architecture Search", "comments": "Accepted by DAC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven, automatic design space exploration of neural accelerator\narchitecture is desirable for specialization and productivity. Previous\nframeworks focus on sizing the numerical architectural hyper-parameters while\nneglect searching the PE connectivities and compiler mappings. To tackle this\nchallenge, we propose Neural Accelerator Architecture Search (NAAS) which\nholistically searches the neural network architecture, accelerator\narchitecture, and compiler mapping in one optimization loop. NAAS composes\nhighly matched architectures together with efficient mapping. As a data-driven\napproach, NAAS rivals the human design Eyeriss by 4.4x EDP reduction with 2.7%\naccuracy improvement on ImageNet under the same computation resource, and\noffers 1.4x to 3.5x EDP reduction than only sizing the architectural\nhyper-parameters.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:56:41 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Lin", "Yujun", ""], ["Yang", "Mengtian", ""], ["Han", "Song", ""]]}, {"id": "2105.13262", "submitter": "Harideep Nair", "authors": "Harideep Nair, John Paul Shen and James E. Smith", "title": "A Microarchitecture Implementation Framework for Online Learning with\n  Temporal Neural Networks", "comments": "To be published in ISVLSI 2021. arXiv admin note: substantial text\n  overlap with arXiv:2009.00457", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Neural Networks (TNNs) are spiking neural networks that use time as\na resource to represent and process information, similar to the mammalian\nneocortex. In contrast to compute-intensive deep neural networks that employ\nseparate training and inference phases, TNNs are capable of extremely efficient\nonline incremental/continual learning and are excellent candidates for building\nedge-native sensory processing units. This work proposes a microarchitecture\nframework for implementing TNNs using standard CMOS. Gate-level implementations\nof three key building blocks are presented: 1) multi-synapse neurons, 2)\nmulti-neuron columns, and 3) unsupervised and supervised online learning\nalgorithms based on Spike Timing Dependent Plasticity (STDP). The proposed\nmicroarchitecture is embodied in a set of characteristic scaling equations for\nassessing the gate count, area, delay and power for any TNN design.\nPost-synthesis results (in 45nm CMOS) for the proposed designs are presented,\nand their online incremental learning capability is demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:59:54 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 21:51:41 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Nair", "Harideep", ""], ["Shen", "John Paul", ""], ["Smith", "James E.", ""]]}, {"id": "2105.13271", "submitter": "Nicola Bastianello", "authors": "Nicola Bastianello, Andrea Simonetto, Emiliano Dall'Anese", "title": "OpReg-Boost: Learning to Accelerate Online Algorithms with Operator\n  Regression", "comments": "Code available here https://github.com/nicola-bastianello/reg4opt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new regularization approach -- termed OpReg-Boost -- to\nboost the convergence and lessen the asymptotic error of online optimization\nand learning algorithms. In particular, the paper considers online algorithms\nfor optimization problems with a time-varying (weakly) convex composite cost.\nFor a given online algorithm, OpReg-Boost learns the closest algorithmic map\nthat yields linear convergence; to this end, the learning procedure hinges on\nthe concept of operator regression. We show how to formalize the operator\nregression problem and propose a computationally-efficient Peaceman-Rachford\nsolver that exploits a closed-form solution of simple quadratically-constrained\nquadratic programs (QCQPs). Simulation results showcase the superior properties\nof OpReg-Boost w.r.t. the more classical forward-backward algorithm, FISTA, and\nAnderson acceleration, and with respect to its close relative\nconvex-regression-boost (CvxReg-Boost) which is also novel but less performing.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:17:38 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 09:57:53 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Bastianello", "Nicola", ""], ["Simonetto", "Andrea", ""], ["Dall'Anese", "Emiliano", ""]]}, {"id": "2105.13277", "submitter": "Yotam Erel", "authors": "Amir Barda, Yotam Erel, Amit H. Bermano", "title": "MeshCNN Fundamentals: Geometric Learning through a Reconstructable\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mesh-based learning is one of the popular approaches nowadays to learn\nshapes. The most established backbone in this field is MeshCNN. In this paper,\nwe propose infusing MeshCNN with geometric reasoning to achieve higher quality\nlearning. Through careful analysis of the way geometry is represented\nthrough-out the network, we submit that this representation should be rigid\nmotion invariant, and should allow reconstructing the original geometry.\nAccordingly, we introduce the first and second fundamental forms as an\nedge-centric, rotation and translation invariant, reconstructable\nrepresentation. In addition, we update the originally proposed pooling scheme\nto be more geometrically driven. We validate our analysis through\nexperimentation, and present consistent improvement upon the MeshCNN baseline,\nas well as other more elaborate state-of-the-art architectures. Furthermore, we\ndemonstrate this fundamental forms-based representation opens the door to\naccessible generative machine learning over meshes.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:22:44 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Barda", "Amir", ""], ["Erel", "Yotam", ""], ["Bermano", "Amit H.", ""]]}, {"id": "2105.13278", "submitter": "Juan Ungredda", "authors": "Juan Ungredda, Mariapia Marchi, Teresa Montrone and Juergen Branke", "title": "One Step Preference Elicitation in Multi-Objective Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-objective optimization problem with objective functions\nthat are expensive to evaluate. The decision maker (DM) has unknown\npreferences, and so the standard approach is to generate an approximation of\nthe Pareto front and let the DM choose from the generated non-dominated\ndesigns. However, especially for expensive to evaluate problems where the\nnumber of designs that can be evaluated is very limited, the true best solution\naccording to the DM's unknown preferences is unlikely to be among the small set\nof non-dominated solutions found, even if these solutions are truly Pareto\noptimal. We address this issue by using a multi-objective Bayesian optimization\nalgorithm and allowing the DM to select a preferred solution from a predicted\ncontinuous Pareto front just once before the end of the algorithm rather than\nselecting a solution after the end. This allows the algorithm to understand the\nDM's preferences and make a final attempt to identify a more preferred\nsolution. We demonstrate the idea using ParEGO, and show empirically that the\nfound solutions are significantly better in terms of true DM preferences than\nif the DM would simply pick a solution at the end.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:23:29 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ungredda", "Juan", ""], ["Marchi", "Mariapia", ""], ["Montrone", "Teresa", ""], ["Branke", "Juergen", ""]]}, {"id": "2105.13281", "submitter": "Dominik Baumann", "authors": "Dominik Baumann and Alonso Marco and Matteo Turchetta and Sebastian\n  Trimpe", "title": "GoSafe: Globally Optimal Safe Robot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning policies for robotic systems from data, safety is a major\nconcern, as violation of safety constraints may cause hardware damage. SafeOpt\nis an efficient Bayesian optimization (BO) algorithm that can learn policies\nwhile guaranteeing safety with high probability. However, its search space is\nlimited to an initially given safe region. We extend this method by exploring\noutside the initial safe area while still guaranteeing safety with high\nprobability. This is achieved by learning a set of initial conditions from\nwhich we can recover safely using a learned backup controller in case of a\npotential failure. We derive conditions for guaranteed convergence to the\nglobal optimum and validate GoSafe in hardware experiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:27:47 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Baumann", "Dominik", ""], ["Marco", "Alonso", ""], ["Turchetta", "Matteo", ""], ["Trimpe", "Sebastian", ""]]}, {"id": "2105.13283", "submitter": "Lara Hoffmann", "authors": "Lara Hoffmann and Clemens Elster", "title": "Deep Ensembles from a Bayesian Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep ensembles can be seen as the current state-of-the-art for uncertainty\nquantification in deep learning. While the approach was originally proposed as\nan non-Bayesian technique, arguments towards its Bayesian footing have been put\nforward as well. We show that deep ensembles can be viewed as an approximate\nBayesian method by specifying the corresponding assumptions. Our finding leads\nto an improved approximation which results in an increased epistemic part of\nthe uncertainty. Numerical examples suggest that the improved approximation can\nlead to more reliable uncertainties. Analytical derivations ensure easy\ncalculation of results.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:30:52 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Hoffmann", "Lara", ""], ["Elster", "Clemens", ""]]}, {"id": "2105.13284", "submitter": "David Biagioni", "authors": "Erotokritos Skordilis, Yi Hou, Charles Tripp, Matthew Moniot, Peter\n  Graf, David Biagioni", "title": "A Modular and Transferable Reinforcement Learning Framework for the\n  Fleet Rebalancing Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.MA cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobility on demand (MoD) systems show great promise in realizing flexible and\nefficient urban transportation. However, significant technical challenges arise\nfrom operational decision making associated with MoD vehicle dispatch and fleet\nrebalancing. For this reason, operators tend to employ simplified algorithms\nthat have been demonstrated to work well in a particular setting. To help\nbridge the gap between novel and existing methods, we propose a modular\nframework for fleet rebalancing based on model-free reinforcement learning (RL)\nthat can leverage an existing dispatch method to minimize system cost. In\nparticular, by treating dispatch as part of the environment dynamics, a\ncentralized agent can learn to intermittently direct the dispatcher to\nreposition free vehicles and mitigate against fleet imbalance. We formulate RL\nstate and action spaces as distributions over a grid partitioning of the\noperating area, making the framework scalable and avoiding the complexities\nassociated with multiagent RL. Numerical experiments, using real-world trip and\nnetwork data, demonstrate that this approach has several distinct advantages\nover baseline methods including: improved system cost; high degree of\nadaptability to the selected dispatch method; and the ability to perform\nscale-invariant transfer learning between problem instances with similar\nvehicle and request distributions.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:32:28 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Skordilis", "Erotokritos", ""], ["Hou", "Yi", ""], ["Tripp", "Charles", ""], ["Moniot", "Matthew", ""], ["Graf", "Peter", ""], ["Biagioni", "David", ""]]}, {"id": "2105.13289", "submitter": "Li Yang", "authors": "Li Yang, Abdallah Moubayed, Abdallah Shami", "title": "MTH-IDS: A Multi-Tiered Hybrid Intrusion Detection System for Internet\n  of Vehicles", "comments": "Accepted and to appear in IEEE Internet of Things Journal; Code is\n  available at Github link:\n  https://github.com/Western-OC2-Lab/Intrusion-Detection-System-Using-Machine-Learning", "journal-ref": null, "doi": "10.1109/JIOT.2021.3084796", "report-no": null, "categories": "cs.CR cs.AI cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern vehicles, including connected vehicles and autonomous vehicles,\nnowadays involve many electronic control units connected through intra-vehicle\nnetworks to implement various functionalities and perform actions. Modern\nvehicles are also connected to external networks through vehicle-to-everything\ntechnologies, enabling their communications with other vehicles,\ninfrastructures, and smart devices. However, the improving functionality and\nconnectivity of modern vehicles also increase their vulnerabilities to\ncyber-attacks targeting both intra-vehicle and external networks due to the\nlarge attack surfaces. To secure vehicular networks, many researchers have\nfocused on developing intrusion detection systems (IDSs) that capitalize on\nmachine learning methods to detect malicious cyber-attacks. In this paper, the\nvulnerabilities of intra-vehicle and external networks are discussed, and a\nmulti-tiered hybrid IDS that incorporates a signature-based IDS and an\nanomaly-based IDS is proposed to detect both known and unknown attacks on\nvehicular networks. Experimental results illustrate that the proposed system\ncan detect various types of known attacks with 99.99% accuracy on the\nCAN-intrusion-dataset representing the intra-vehicle network data and 99.88%\naccuracy on the CICIDS2017 dataset illustrating the external vehicular network\ndata. For the zero-day attack detection, the proposed system achieves high\nF1-scores of 0.963 and 0.800 on the above two datasets, respectively. The\naverage processing time of each data packet on a vehicle-level machine is less\nthan 0.6 ms, which shows the feasibility of implementing the proposed system in\nreal-time vehicle systems. This emphasizes the effectiveness and efficiency of\nthe proposed IDS.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:36:35 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Yang", "Li", ""], ["Moubayed", "Abdallah", ""], ["Shami", "Abdallah", ""]]}, {"id": "2105.13290", "submitter": "Ming Ding", "authors": "Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin,\n  Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, Jie Tang", "title": "CogView: Mastering Text-to-Image Generation via Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-Image generation in the general domain has long been an open problem,\nwhich requires both a powerful generative model and cross-modal understanding.\nWe propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to\nadvance this problem. We also demonstrate the finetuning strategies for various\ndownstream tasks, e.g. style learning, super-resolution, text-image ranking and\nfashion design, and methods to stabilize pretraining, e.g. eliminating NaN\nlosses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS\nCOCO, outperforms previous GAN-based models and a recent similar work DALL-E.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:52:53 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 18:05:31 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ding", "Ming", ""], ["Yang", "Zhuoyi", ""], ["Hong", "Wenyi", ""], ["Zheng", "Wendi", ""], ["Zhou", "Chang", ""], ["Yin", "Da", ""], ["Lin", "Junyang", ""], ["Zou", "Xu", ""], ["Shao", "Zhou", ""], ["Yang", "Hongxia", ""], ["Tang", "Jie", ""]]}, {"id": "2105.13302", "submitter": "Zhiqi Bu", "authors": "Zhiqi Bu, Jason Klusowski, Cynthia Rush, Weijie J. Su", "title": "Characterizing the SLOPE Trade-off: A Variational Perspective and the\n  Donoho-Tanner Limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG eess.SP math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted l1 regularization has been incorporated into many methods for solving\nhigh-dimensional statistical estimation problems, including the SLOPE estimator\nin linear regression. In this paper, we study how this relatively new\nregularization technique improves variable selection by characterizing the\noptimal SLOPE trade-off between the false discovery proportion (FDP) and true\npositive proportion (TPP) or, equivalently, between measures of type I error\nand power. Assuming a regime of linear sparsity and working under Gaussian\nrandom designs, we obtain an upper bound on the optimal trade-off for SLOPE,\nshowing its capability of breaking the Donoho-Tanner power limit. To put it\ninto perspective, this limit is the highest possible power that the Lasso,\nwhich is perhaps the most popular l1-based method, can achieve even with\narbitrarily strong effect sizes. Next, we derive a tight lower bound that\ndelineates the fundamental limit of sorted l1 regularization in optimally\ntrading the FDP off for the TPP. Finally, we show that on any problem instance,\nSLOPE with a certain regularization sequence outperforms the Lasso, in the\nsense of having a smaller FDP, larger TPP and smaller l2 estimation risk\nsimultaneously. Our proofs are based on a novel technique that reduces a\nvariational calculus problem to a class of infinite-dimensional convex\noptimization problems and a very recent result from approximate message passing\ntheory.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:56:42 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Bu", "Zhiqi", ""], ["Klusowski", "Jason", ""], ["Rush", "Cynthia", ""], ["Su", "Weijie J.", ""]]}, {"id": "2105.13304", "submitter": "Gia-Wei Chern", "authors": "Sheng Zhang, Puhan Zhang, Gia-Wei Chern", "title": "Anomalous phase separation and hidden coarsening of super-clusters in\n  the Falicov-Kimball model", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el cond-mat.dis-nn cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the celebrated Falicov-Kimball model exhibits rich and\nintriguing phase-ordering dynamics. Applying modern machine learning methods to\nenable large-scale quantum kinetic Monte Carlo simulations, we uncover an\nunusual phase-separation scenario in which the growth of charge checkerboard\nclusters competes with domain coarsening related to a hidden symmetry-breaking.\nA self-trapping mechanism as a result of this competition gives rise to\narrested growth of checkerboard patterns and their super-clusters. Glassy\nbehaviors similar to the one reported in this work could be generic for other\ncorrelated electron systems.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:57:46 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhang", "Sheng", ""], ["Zhang", "Puhan", ""], ["Chern", "Gia-Wei", ""]]}, {"id": "2105.13309", "submitter": "Fernando E. Casado", "authors": "Fernando E. Casado, Dylan Lema, Marcos F. Criado, Roberto Iglesias,\n  Carlos V. Regueiro, Sen\\'en Barro", "title": "Concept drift detection and adaptation for federated and continual\n  learning", "comments": null, "journal-ref": null, "doi": "10.1007/s11042-021-11219-x", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Smart devices, such as smartphones, wearables, robots, and others, can\ncollect vast amounts of data from their environment. This data is suitable for\ntraining machine learning models, which can significantly improve their\nbehavior, and therefore, the user experience. Federated learning is a young and\npopular framework that allows multiple distributed devices to train deep\nlearning models collaboratively while preserving data privacy. Nevertheless,\nthis approach may not be optimal for scenarios where data distribution is\nnon-identical among the participants or changes over time, causing what is\nknown as concept drift. Little research has yet been done in this field, but\nthis kind of situation is quite frequent in real life and poses new challenges\nto both continual and federated learning. Therefore, in this work, we present a\nnew method, called Concept-Drift-Aware Federated Averaging (CDA-FedAvg). Our\nproposal is an extension of the most popular federated algorithm, Federated\nAveraging (FedAvg), enhancing it for continual adaptation under concept drift.\nWe empirically demonstrate the weaknesses of regular FedAvg and prove that\nCDA-FedAvg outperforms it in this type of scenario.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:01:58 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 11:48:22 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Casado", "Fernando E.", ""], ["Lema", "Dylan", ""], ["Criado", "Marcos F.", ""], ["Iglesias", "Roberto", ""], ["Regueiro", "Carlos V.", ""], ["Barro", "Sen\u00e9n", ""]]}, {"id": "2105.13320", "submitter": "Timothy DeLise", "authors": "Timothy DeLise", "title": "Neural Options Pricing", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF cs.CE cs.LG q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research investigates pricing financial options based on the traditional\nmartingale theory of arbitrage pricing applied to neural SDEs. We treat neural\nSDEs as universal It\\^o process approximators. In this way we can lift all\nassumptions on the form of the underlying price process, and compute\ntheoretical option prices numerically. We propose a variation of the SDE-GAN\napproach by implementing the Wasserstein distance metric as a loss function for\ntraining. Furthermore, it is conjectured that the error of the option price\nimplied by the learnt model can be bounded by the very Wasserstein distance\nmetric that was used to fit the empirical data.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:22:30 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["DeLise", "Timothy", ""]]}, {"id": "2105.13325", "submitter": "Christopher Briggs", "authors": "Christopher Briggs, Zhong Fan, Peter Andras", "title": "Federated Learning for Short-term Residential Energy Demand Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Energy demand forecasting is an essential task performed within the energy\nindustry to help balance supply with demand and maintain a stable load on the\nelectricity grid. As supply transitions towards less reliable renewable energy\ngeneration, smart meters will prove a vital component to aid these forecasting\ntasks. However, smart meter take-up is low among privacy-conscious consumers\nthat fear intrusion upon their fine-grained consumption data. In this work we\npropose and explore a federated learning (FL) based approach for training\nforecasting models in a distributed, collaborative manner whilst retaining the\nprivacy of the underlying data. We compare two approaches: FL, and a clustered\nvariant, FL+HC against a non-private, centralised learning approach and a fully\nprivate, localised learning approach. Within these approaches, we measure model\nperformance using RMSE and computational efficiency via the number of samples\nrequired to train models under each scenario. In addition, we suggest the FL\nstrategies are followed by a personalisation step and show that model\nperformance can be improved by doing so. We show that FL+HC followed by\npersonalisation can achieve a $\\sim$5% improvement in model performance with a\n$\\sim$10x reduction in computation compared to localised learning. Finally we\nprovide advice on private aggregation of predictions for building a private\nend-to-end energy demand forecasting application.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:33:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Briggs", "Christopher", ""], ["Fan", "Zhong", ""], ["Andras", "Peter", ""]]}, {"id": "2105.13327", "submitter": "Murray Shanahan", "authors": "Murray Shanahan and Christos Kaplanis and Jovana Mitrovi\\'c", "title": "Encoders and Ensembles for Task-Free Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an architecture that is effective for continual learning in an\nespecially demanding setting, where task boundaries do not exist or are\nunknown. Our architecture comprises an encoder, pre-trained on a separate\ndataset, and an ensemble of simple one-layer classifiers. Two main innovations\nare required to make this combination work. First, the provision of suitably\ngeneric pre-trained encoders has been made possible thanks to recent progress\nin self-supervised training methods. Second, pairing each classifier in the\nensemble with a key, where the key-space is identical to the latent space of\nthe encoder, allows them to be used collectively, yet selectively, via\nk-nearest neighbour lookup. We show that models trained with the\nencoders-and-ensembles architecture are state-of-the-art for the task-free\nsetting on standard image classification continual learning benchmarks, and\nimprove on prior state-of-the-art by a large margin in the most challenging\ncases. We also show that the architecture learns well in a fully incremental\nsetting, where one class is learned at a time, and we demonstrate its\neffectiveness in this setting with up to 100 classes. Finally, we show that the\narchitecture works in a task-free continual learning context where the data\ndistribution changes gradually, and existing approaches requiring knowledge of\ntask boundaries cannot be applied.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:34:31 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 15:07:13 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 10:30:31 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Shanahan", "Murray", ""], ["Kaplanis", "Christos", ""], ["Mitrovi\u0107", "Jovana", ""]]}, {"id": "2105.13331", "submitter": "Pierre-Emmanuel Novac", "authors": "Pierre-Emmanuel Novac (1), Ghouthi Boukli Hacene (2 and 3), Alain\n  Pegatoquet (1), Beno\\^it Miramond (1), Vincent Gripon (2) ((1) Universit\\'e\n  C\\^ote d'Azur, CNRS, LEAT, Sophia Antipolis, France, (2) IMT Atlantique,\n  Brest, France, (3) MILA, Montreal, Canada)", "title": "Quantization and Deployment of Deep Neural Networks on Microcontrollers", "comments": "36 pages, 14 figures. Published in MDPI Sensors 2021, special issue\n  \"Embedded Artificial Intelligence (AI) for Smart Sensing and IoT\n  Applications\": https://www.mdpi.com/1424-8220/21/9/2984", "journal-ref": "Sensors 2021, 21, 2984", "doi": "10.3390/s21092984", "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Embedding Artificial Intelligence onto low-power devices is a challenging\ntask that has been partly overcome with recent advances in machine learning and\nhardware design. Presently, deep neural networks can be deployed on embedded\ntargets to perform different tasks such as speech recognition,object detection\nor Human Activity Recognition. However, there is still room for optimization of\ndeep neural networks onto embedded devices. These optimizations mainly address\npower consumption,memory and real-time constraints, but also an easier\ndeployment at the edge. Moreover, there is still a need for a better\nunderstanding of what can be achieved for different use cases. This work\nfocuses on quantization and deployment of deep neural networks onto low-power\n32-bit microcontrollers. The quantization methods, relevant in the context of\nan embedded execution onto a microcontroller, are first outlined. Then, a new\nframework for end-to-end deep neural networks training, quantization and\ndeployment is presented. This framework, called MicroAI, is designed as an\nalternative to existing inference engines (TensorFlow Lite for Microcontrollers\nand STM32Cube.AI). Our framework can indeed be easily adjusted and/or extended\nfor specific use cases. Execution using single precision 32-bit floating-point\nas well as fixed-point on 8- and 16-bit integers are supported. The proposed\nquantization method is evaluated with three different datasets (UCI-HAR, Spoken\nMNIST and GTSRB). Finally, a comparison study between MicroAI and both existing\nembedded inference engines is provided in terms of memory and power efficiency.\nOn-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq\nApollo3 and STM32L452RE).\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:39:06 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Novac", "Pierre-Emmanuel", "", "2 and 3"], ["Hacene", "Ghouthi Boukli", "", "2 and 3"], ["Pegatoquet", "Alain", ""], ["Miramond", "Beno\u00eet", ""], ["Gripon", "Vincent", ""]]}, {"id": "2105.13336", "submitter": "Kaixin Zhang", "authors": "Kaixin Zhang, Hongzhi Wang, Tongxin Li, Han Hu, Jiye Qiu, Songling Zou", "title": "TENSILE: A Tensor granularity dynamic GPU memory scheduler method\n  towards multiple dynamic workloads system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has been an area of intense researching. However, as\na kind of computing intensive task, deep learning highly relies on the the\nscale of the GPU memory, which is usually expensive and scarce. Although there\nare some extensive works have been proposed for dynamic GPU memory management,\nthey are hard to be applied to systems with multitasking dynamic workloads,\nsuch as in-database machine learning system.\n  In this paper, we demonstrated TENSILE, a method of managing GPU memory in\ntensor granularity to reduce the GPU memory peak, with taking the multitasking\ndynamic workloads into consideration. As far as we know, TENSILE is the first\nmethod which is designed to manage multiple workloads' GPU memory using. We\nimplement TENSILE on our own deep learning framework, and evaluated its\nperformance. The experiment results shows that our method can achieve less time\noverhead than prior works with more GPU memory saved.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:46:16 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:31:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Kaixin", ""], ["Wang", "Hongzhi", ""], ["Li", "Tongxin", ""], ["Hu", "Han", ""], ["Qiu", "Jiye", ""], ["Zou", "Songling", ""]]}, {"id": "2105.13343", "submitter": "Stanislav Fort", "authors": "Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, Samuel L.\n  Smith", "title": "Drawing Multiple Augmentation Samples Per Image During Training\n  Efficiently Decreases Test Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, it is standard practice to draw a single sample from the\ndata augmentation procedure for each unique image in the mini-batch, however it\nis not clear whether this choice is optimal for generalization. In this work,\nwe provide a detailed empirical evaluation of how the number of augmentation\nsamples per unique image influences performance on held out data. Remarkably,\nwe find that drawing multiple samples per image consistently enhances the test\naccuracy achieved for both small and large batch training, despite reducing the\nnumber of unique training examples in each mini-batch. This benefit arises even\nwhen different augmentation multiplicities perform the same number of parameter\nupdates and gradient evaluations. Our results suggest that, although the\nvariance in the gradient estimate arising from subsampling the dataset has an\nimplicit regularization benefit, the variance which arises from the data\naugmentation process harms test accuracy. By applying augmentation multiplicity\nto the recently proposed NFNet model family, we achieve a new ImageNet state of\nthe art of 86.8$\\%$ top-1 w/o extra data.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:51:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Fort", "Stanislav", ""], ["Brock", "Andrew", ""], ["Pascanu", "Razvan", ""], ["De", "Soham", ""], ["Smith", "Samuel L.", ""]]}, {"id": "2105.13345", "submitter": "Ishan Durugkar", "authors": "Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone", "title": "Adversarial Intrinsic Motivation for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with an objective to minimize the mismatch with a reference\ndistribution has been shown to be useful for generative modeling and imitation\nlearning. In this paper, we investigate whether one such objective, the\nWasserstein-1 distance between a policy's state visitation distribution and a\ntarget distribution, can be utilized effectively for reinforcement learning\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\nlearning where the idealized (unachievable) target distribution has full\nmeasure at the goal. We introduce a quasimetric specific to Markov Decision\nProcesses (MDPs), and show that the policy that minimizes the Wasserstein-1\ndistance of its state visitation distribution to this target distribution under\nthis quasimetric is the policy that reaches the goal in as few steps as\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\nestimates this Wasserstein-1 distance through its dual objective and uses it to\ncompute a supplemental reward function. Our experiments show that this reward\nfunction changes smoothly with respect to transitions in the MDP and assists\nthe agent in learning. Additionally, we combine AIM with Hindsight Experience\nReplay (HER) and show that the resulting algorithm accelerates learning\nsignificantly on several simulated robotics tasks when compared to HER with a\nsparse positive reward at the goal state.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:51:34 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 22:13:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Durugkar", "Ishan", ""], ["Tec", "Mauricio", ""], ["Niekum", "Scott", ""], ["Stone", "Peter", ""]]}, {"id": "2105.13348", "submitter": "Yu-Guan Hsieh", "authors": "Yu-Guan Hsieh, Franck Iutzeler, J\\'er\\^ome Malick, Panayotis\n  Mertikopoulos", "title": "Optimization in Open Networks via Dual Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In networks of autonomous agents (e.g., fleets of vehicles, scattered\nsensors), the problem of minimizing the sum of the agents' local functions has\nreceived a lot of interest. We tackle here this distributed optimization\nproblem in the case of open networks when agents can join and leave the network\nat any time. Leveraging recent online optimization techniques, we propose and\nanalyze the convergence of a decentralized asynchronous optimization method for\nopen networks.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:52:48 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Hsieh", "Yu-Guan", ""], ["Iutzeler", "Franck", ""], ["Malick", "J\u00e9r\u00f4me", ""], ["Mertikopoulos", "Panayotis", ""]]}, {"id": "2105.13383", "submitter": "Vishrant Tripathi", "authors": "Vishrant Tripathi, Eytan Modiano", "title": "An Online Learning Approach to Optimizing Time-Varying Costs of AoI", "comments": "Accepted to ACM Mobihoc '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider systems that require timely monitoring of sources over a\ncommunication network, where the cost of delayed information is unknown,\ntime-varying and possibly adversarial. For the single source monitoring\nproblem, we design algorithms that achieve sublinear regret compared to the\nbest fixed policy in hindsight. For the multiple source scheduling problem, we\ndesign a new online learning algorithm called\nFollow-the-Perturbed-Whittle-Leader and show that it has low regret compared to\nthe best fixed scheduling policy in hindsight, while remaining computationally\nfeasible. The algorithm and its regret analysis are novel and of independent\ninterest to the study of online restless multi-armed bandit problems. We\nfurther design algorithms that achieve sublinear regret compared to the best\ndynamic policy when the environment is slowly varying. Finally, we apply our\nalgorithms to a mobility tracking problem. We consider non-stationary and\nadversarial mobility models and illustrate the performance benefit of using our\nonline learning algorithms compared to an oblivious scheduling policy.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:10:56 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tripathi", "Vishrant", ""], ["Modiano", "Eytan", ""]]}, {"id": "2105.13392", "submitter": "Sangwook Park", "authors": "Sangwook Park, David K. Han, Mounya Elhilali", "title": "Cross-Referencing Self-Training Network for Sound Event Detection in\n  Audio Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sound event detection is an important facet of audio tagging that aims to\nidentify sounds of interest and define both the sound category and time\nboundaries for each sound event in a continuous recording. With advances in\ndeep neural networks, there has been tremendous improvement in the performance\nof sound event detection systems, although at the expense of costly data\ncollection and labeling efforts. In fact, current state-of-the-art methods\nemploy supervised training methods that leverage large amounts of data samples\nand corresponding labels in order to facilitate identification of sound\ncategory and time stamps of events. As an alternative, the current study\nproposes a semi-supervised method for generating pseudo-labels from\nunsupervised data using a student-teacher scheme that balances self-training\nand cross-training. Additionally, this paper explores post-processing which\nextracts sound intervals from network prediction, for further improvement in\nsound event detection performance. The proposed approach is evaluated on sound\nevent detection task for the DCASE2020 challenge. The results of these methods\non both \"validation\" and \"public evaluation\" sets of DESED database show\nsignificant improvement compared to the state-of-the art systems in\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:46:59 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Park", "Sangwook", ""], ["Han", "David K.", ""], ["Elhilali", "Mounya", ""]]}, {"id": "2105.13393", "submitter": "Sebastian Dorn", "authors": "Philipp Joppich, Sebastian Dorn, Oliver De Candido, Wolfgang Utschick,\n  Jakob Knollm\\\"uller", "title": "Classification and Uncertainty Quantification of Corrupted Data using\n  Semi-Supervised Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric and non-parametric classifiers often have to deal with real-world\ndata, where corruptions like noise, occlusions, and blur are unavoidable -\nposing significant challenges. We present a probabilistic approach to classify\nstrongly corrupted data and quantify uncertainty, despite the model only having\nbeen trained with uncorrupted data. A semi-supervised autoencoder trained on\nuncorrupted data is the underlying architecture. We use the decoding part as a\ngenerative model for realistic data and extend it by convolutions, masking, and\nadditive Gaussian noise to describe imperfections. This constitutes a\nstatistical inference task in terms of the optimal latent space activations of\nthe underlying uncorrupted datum. We solve this problem approximately with\nMetric Gaussian Variational Inference (MGVI). The supervision of the\nautoencoder's latent space allows us to classify corrupted data directly under\nuncertainty with the statistically inferred latent space activations.\nFurthermore, we demonstrate that the model uncertainty strongly depends on\nwhether the classification is correct or wrong, setting a basis for a\nstatistical \"lie detector\" of the classification. Independent of that, we show\nthat the generative model can optimally restore the uncorrupted datum by\ndecoding the inferred latent space activations.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:47:55 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Joppich", "Philipp", ""], ["Dorn", "Sebastian", ""], ["De Candido", "Oliver", ""], ["Utschick", "Wolfgang", ""], ["Knollm\u00fcller", "Jakob", ""]]}, {"id": "2105.13399", "submitter": "Yuqing Hu", "authors": "Yuqing Hu, Xiaoyuan Cheng, Suhang Wang, Jianli Chen, Tianxiang Zhao,\n  Enyan Dai", "title": "Times Series Forecasting for Urban Building Energy Consumption Based on\n  Graph Convolutional Network", "comments": "22 pages, 10 figures, submitted to applied energy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is increasingly urbanizing and the building industry accounts for\nmore than 40% of energy consumption in the United States. To improve urban\nsustainability, many cities adopt ambitious energy-saving strategies through\nretrofitting existing buildings and constructing new communities. In this\nsituation, an accurate urban building energy model (UBEM) is the foundation to\nsupport the design of energy-efficient communities. However, current UBEM are\nlimited in their abilities to capture the inter-building interdependency due to\ntheir dynamic and non-linear characteristics. Those models either ignored or\noversimplified these building interdependencies, which can substantially affect\nthe accuracy of urban energy modeling. To fill the research gap, this study\nproposes a novel data-driven UBEM synthesizing the solar-based building\ninterdependency and spatial-temporal graph convolutional network (ST-GCN)\nalgorithm. Especially, we took a university campus located in downtown Atlanta\nas an example to predict the hourly energy consumption. Furthermore, we tested\nthe feasibility of the proposed model by comparing the performance of the\nST-GCN model with other common time-series machine learning models. The results\nindicate that the ST-GCN model overall outperforms all others. In addition, the\nphysical knowledge embedded in the model is well interpreted. After discussion,\nit is found that data-driven models integrated engineering or physical\nknowledge can significantly improve the urban building energy simulation.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:02:04 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hu", "Yuqing", ""], ["Cheng", "Xiaoyuan", ""], ["Wang", "Suhang", ""], ["Chen", "Jianli", ""], ["Zhao", "Tianxiang", ""], ["Dai", "Enyan", ""]]}, {"id": "2105.13418", "submitter": "Masoumeh Shafieinejad", "authors": "Masoumeh Shafieinejad and Huseyin Inan and Marcello Hasegawa and\n  Robert Sim", "title": "On Privacy and Confidentiality of Communications in Organizational\n  Graphs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learned models trained on organizational communication data, such as\nemails in an enterprise, carry unique risks of breaching confidentiality, even\nif the model is intended only for internal use. This work shows how\nconfidentiality is distinct from privacy in an enterprise context, and aims to\nformulate an approach to preserving confidentiality while leveraging principles\nfrom differential privacy. The goal is to perform machine learning tasks, such\nas learning a language model or performing topic analysis, using interpersonal\ncommunications in the organization, while not learning about confidential\ninformation shared in the organization. Works that apply differential privacy\ntechniques to natural language processing tasks usually assume independently\ndistributed data, and overlook potential correlation among the records.\nIgnoring this correlation results in a fictional promise of privacy. Naively\nextending differential privacy techniques to focus on group privacy instead of\nrecord-level privacy is a straightforward approach to mitigate this issue. This\napproach, although providing a more realistic privacy-guarantee, is\nover-cautious and severely impacts model utility. We show this gap between\nthese two extreme measures of privacy over two language tasks, and introduce a\nmiddle-ground solution. We propose a model that captures the correlation in the\nsocial network graph, and incorporates this correlation in the privacy\ncalculations through Pufferfish privacy principles.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:45:56 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Shafieinejad", "Masoumeh", ""], ["Inan", "Huseyin", ""], ["Hasegawa", "Marcello", ""], ["Sim", "Robert", ""]]}, {"id": "2105.13420", "submitter": "Zhenwen Dai", "authors": "Zhenwen Dai, Praveen Chandar, Ghazal Fazelnia, Ben Carterette, Mounia\n  Lalmas-Roelleke", "title": "Model Selection for Production System via Automated Online Experiments", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A challenge that machine learning practitioners in the industry face is the\ntask of selecting the best model to deploy in production. As a model is often\nan intermediate component of a production system, online controlled experiments\nsuch as A/B tests yield the most reliable estimation of the effectiveness of\nthe whole system, but can only compare two or a few models due to budget\nconstraints. We propose an automated online experimentation mechanism that can\nefficiently perform model selection from a large pool of models with a small\nnumber of online experiments. We derive the probability distribution of the\nmetric of interest that contains the model uncertainty from our Bayesian\nsurrogate model trained using historical logs. Our method efficiently\nidentifies the best model by sequentially selecting and deploying a list of\nmodels from the candidate set that balance exploration-exploitation. Using\nsimulations based on real data, we demonstrate the effectiveness of our method\non two different tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:48:23 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Dai", "Zhenwen", ""], ["Chandar", "Praveen", ""], ["Fazelnia", "Ghazal", ""], ["Carterette", "Ben", ""], ["Lalmas-Roelleke", "Mounia", ""]]}, {"id": "2105.13424", "submitter": "Christina Delimitrou", "authors": "Yanqi Zhang, Weizhe Hua, Zhuangzhuang Zhou, Edward Suh, Christina\n  Delimitrou", "title": "Sinan: Data-Driven, QoS-Aware Cluster Management for Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud applications are increasingly shifting from large monolithic services,\nto large numbers of loosely-coupled, specialized microservices. Despite their\nadvantages in terms of facilitating development, deployment, modularity, and\nisolation, microservices complicate resource management, as dependencies\nbetween them introduce backpressure effects and cascading QoS violations.\n  We present Sinan, a data-driven cluster manager for interactive cloud\nmicroservices that is online and QoS-aware. Sinan leverages a set of scalable\nand validated machine learning models to determine the performance impact of\ndependencies between microservices, and allocate appropriate resources per tier\nin a way that preserves the end-to-end tail latency target. We evaluate Sinan\nboth on dedicated local clusters and large-scale deployments on Google Compute\nEngine (GCE) across representative end-to-end applications built with\nmicroservices, such as social networks and hotel reservation sites. We show\nthat Sinan always meets QoS, while also maintaining cluster utilization high,\nin contrast to prior work which leads to unpredictable performance or\nsacrifices resource efficiency. Furthermore, the techniques in Sinan are\nexplainable, meaning that cloud operators can yield insights from the ML models\non how to better deploy and design their applications to reduce unpredictable\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:57:51 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Yanqi", ""], ["Hua", "Weizhe", ""], ["Zhou", "Zhuangzhuang", ""], ["Suh", "Edward", ""], ["Delimitrou", "Christina", ""]]}, {"id": "2105.13429", "submitter": "Ghasem Akbari", "authors": "Ghasem Akbari, Nader Montazerin", "title": "Flow based features and validation metric for machine learning\n  reconstruction of PIV data", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstruction of flow field from real sparse data by a physics-oriented\napproach is a current challenge for fluid scientists in the AI community. The\nproblem includes feature recognition and implementation of AI algorithms that\nlink data to a physical feature space in order to produce reconstructed data.\nThe present article applies machine learning approach to study contribution of\ndifferent flow-based features with practical fluid mechanics applications for\nreconstruction of the missing data of turbomachinery PIV measurements. Support\nvector regression (SVR) and multi-layer perceptron (MLP) are selected as two\nrobust regressors capable of modelling non-linear fluid flow phenomena. The\nproposed flow-based features are optimally scaled and filtered to extract the\nbest configuration. In addition to conventional data-based validation of the\nregressors, a metric is proposed that reflects mass conservation law as an\nimportant requirement for a physical flow reproduction. For a velocity field\nincluding 25% of clustered missing data, the reconstruction accuracy achieved\nby SVR in terms of R2-score is as high as 0.993 for the in-plane velocity\nvectors in comparison with that obtained by MLP which is up to 0.981. In terms\nof mass conservation metric, the SVR model by R2-score up to 0.96 is\nconsiderably more accurate than the MLP estimator. For extremely sparse data\nwith a gappiness of 75%, vector and contour plots from SVR and MLP were\nconsistent with those of the original field.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:05:41 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Akbari", "Ghasem", ""], ["Montazerin", "Nader", ""]]}, {"id": "2105.13430", "submitter": "Marina Sokolova", "authors": "YuanZheng Hu and Marina Sokolova", "title": "Explainable Multi-class Classification of the CAMH COVID-19 Mental\n  Health Data", "comments": "22 pages, including Appendixes; 7 tables and 5 figures in the main\n  text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Application of Machine Learning algorithms to the medical domain is an\nemerging trend that helps to advance medical knowledge. At the same time, there\nis a significant a lack of explainable studies that promote informed,\ntransparent, and interpretable use of Machine Learning algorithms. In this\npaper, we present explainable multi-class classification of the Covid-19 mental\nhealth data. In Machine Learning study, we aim to find the potential factors to\ninfluence a personal mental health during the Covid-19 pandemic. We found that\nRandom Forest (RF) and Gradient Boosting (GB) have scored the highest accuracy\nof 68.08% and 68.19% respectively, with LIME prediction accuracy 65.5% for RF\nand 61.8% for GB. We then compare a Post-hoc system (Local Interpretable\nModel-Agnostic Explanations, or LIME) and an Ante-hoc system (Gini Importance)\nin their ability to explain the obtained Machine Learning results. To the best\nof these authors knowledge, our study is the first explainable Machine Learning\nstudy of the mental health data collected during Covid-19 pandemics.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:08:58 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hu", "YuanZheng", ""], ["Sokolova", "Marina", ""]]}, {"id": "2105.13431", "submitter": "Giorgio Angelotti", "authors": "Giorgio Angelotti, Nicolas Drougard, Caroline Ponzoni Carvalho Chanel", "title": "Exploitation vs Caution: Risk-sensitive Policies for Offline Learning", "comments": "Preprint, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline model learning for planning is a branch of machine learning that\ntrains agents to perform actions in an unknown environment using a fixed batch\nof previously collected experiences. The limited size of the data set hinders\nthe estimate of the Value function of the relative Markov Decision Process\n(MDP), bounding the performance of the obtained policy in the real world. In\nthis context, recent works showed that planning with a discount factor lower\nthan the one used during the evaluation phase yields more performing policies.\nHowever, the optimal discount factor is finally chosen by cross-validation. Our\naim is to show that looking for a sub-optimal solution of a Bayesian MDP might\nlead to better performances with respect to the current baselines that work in\nthe offline setting. Hence, we propose Exploitation vs Caution (EvC), an\nalgorithm that automatically selects the policy that solves a Risk-sensitive\nBayesian MDP in a set of policies obtained by solving several MDPs\ncharacterized by different discount factors and transition dynamics. On one\nhand, the Bayesian formalism elegantly includes model uncertainty and on\nanother hand the introduction of a risk-sensitive utility function guarantees\nrobustness. We evaluated the proposed approach in different discrete simple\nenvironments offering a fair variety of MDP classes. We also compared the\nobtained results with state-of-the-art offline learning for planning baselines\nsuch as MOPO and MOReL. In the tested scenarios EvC is more robust than the\nsaid approaches suggesting that sub-optimally solving an Offline Risk-sensitive\nBayesian MDP (ORBMDP) could define a sound framework for planning under model\nuncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:12:20 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Angelotti", "Giorgio", ""], ["Drougard", "Nicolas", ""], ["Chanel", "Caroline Ponzoni Carvalho", ""]]}, {"id": "2105.13434", "submitter": "Surya Selvam", "authors": "Surya Selvam, Vinod Ganesan and Pratyush Kumar", "title": "FuSeConv: Fully Separable Convolutions for Fast Inference on Systolic\n  Arrays", "comments": "To appear in the Proceedings of the Design, Automation & Test in\n  Europe (DATE), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Both efficient neural networks and hardware accelerators are being explored\nto speed up DNN inference on edge devices. For example, MobileNet uses\ndepthwise separable convolution to achieve much lower latency, while systolic\narrays provide much higher performance per watt. Interestingly however, the\ncombination of these two ideas is inefficient: The computational patterns of\ndepth-wise separable convolution are not systolic and lack data reuse to\nsaturate the systolic array's constrained dataflow. In this paper, we propose\nFuSeConv (Fully-Separable Convolution) as a drop-in replacement for depth-wise\nseparable convolution. FuSeConv generalizes the decomposition of convolutions\nfully to separable 1D convolutions along spatial and depth dimensions. The\nresultant computation is systolic and efficiently utilizes the systolic array\nwith a slightly modified dataflow. With FuSeConv, we achieve a significant\nspeed-up of 3x-7x with the MobileNet family of networks on a systolic array of\nsize 64x64, with comparable accuracy on the ImageNet dataset. The high speed-up\nmotivates exploration of hardware-aware Neural Operator Search (NOS) in\ncomplement to ongoing efforts on Neural Architecture Search (NAS).\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:19:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Selvam", "Surya", ""], ["Ganesan", "Vinod", ""], ["Kumar", "Pratyush", ""]]}, {"id": "2105.13440", "submitter": "Peter Carbonetto", "authors": "Peter Carbonetto, Abhishek Sarkar, Zihao Wang and Matthew Stephens", "title": "Non-negative matrix factorization algorithms greatly improve topic model\n  fits", "comments": "Submitted to Advances in Neural Information Processing Systems 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We report on the potential for using algorithms for non-negative matrix\nfactorization (NMF) to improve parameter estimation in topic models. While\nseveral papers have studied connections between NMF and topic models, none have\nsuggested leveraging these connections to develop new algorithms for fitting\ntopic models. Importantly, NMF avoids the \"sum-to-one\" constraints on the topic\nmodel parameters, resulting in an optimization problem with simpler structure\nand more efficient computations. Building on recent advances in optimization\nalgorithms for NMF, we show that first solving the NMF problem then recovering\nthe topic model fit can produce remarkably better fits, and in less time, than\nstandard algorithms for topic models. While we focus primarily on maximum\nlikelihood estimation, we show that this approach also has the potential to\nimprove variational inference for topic models. Our methods are implemented in\nthe R package fastTopics.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:34:46 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Carbonetto", "Peter", ""], ["Sarkar", "Abhishek", ""], ["Wang", "Zihao", ""], ["Stephens", "Matthew", ""]]}, {"id": "2105.13448", "submitter": "Jitendra Parmar", "authors": "Jitendra Parmar, Satyendra Singh Chouhan and Santosh Singh Rathore", "title": "Open-world Machine Learning: Applications, Challenges, and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traditional machine learning especially supervised learning follows the\nassumptions of closed-world learning i.e., for each testing class a training\nclass is available. However, such machine learning models fail to identify the\nclasses which were not available during training time. These classes can be\nreferred to as unseen classes. Whereas, open-world machine learning deals with\narbitrary inputs (data with unseen classes) to machine learning systems.\nMoreover, traditional machine learning is static learning which is not\nappropriate for an active environment where the perspective and sources, and/or\nvolume of data are changing rapidly. In this paper, first, we present an\noverview of open-world learning with importance to the real-world context.\nNext, different dimensions of open-world learning are explored and discussed.\nThe area of open-world learning gained the attention of the research community\nin the last decade only. We have searched through different online digital\nlibraries and scrutinized the work done in the last decade. This paper presents\na systematic review of various techniques for open-world machine learning. It\nalso presents the research gaps, challenges, and future directions in\nopen-world learning. This paper will help researchers to understand the\ncomprehensive developments of open-world learning and the likelihoods to extend\nthe research in suitable areas. It will also help to select applicable\nmethodologies and datasets to explore this further.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 21:05:10 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Parmar", "Jitendra", ""], ["Chouhan", "Satyendra Singh", ""], ["Rathore", "Santosh Singh", ""]]}, {"id": "2105.13461", "submitter": "Enpeng Yuan", "authors": "Enpeng Yuan, Pascal Van Hentenryck", "title": "Learning Model-Based Vehicle-Relocation Decisions for Real-Time\n  Ride-Sharing: Hybridizing Learning and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale ride-sharing systems combine real-time dispatching and routing\noptimization over a rolling time horizon with a model predictive control (MPC)\ncomponent that relocates idle vehicles to anticipate the demand. The MPC\noptimization operates over a longer time horizon to compensate for the inherent\nmyopic nature of the real-time dispatching. These longer time horizons are\nbeneficial for the quality of relocation decisions but increase computational\ncomplexity. Consequently, the ride-sharing operators are often forced to use a\nrelatively short time horizon. To address this computational challenge, this\npaper proposes a hybrid approach that combines machine learning and\noptimization. The machine-learning component learns the optimal solution to the\nMPC on the aggregated level to overcome the sparsity and high-dimensionality of\nthe solution. The optimization component transforms the machine-learning\nprediction back to the original granularity through a tractable transportation\nmodel. As a consequence, the original NP-hard MPC problem is reduced to a\npolynomial time prediction and optimization, which allows the ride-sharing\noperators to consider a longer time horizon. Experimental results show that the\nhybrid approach achieves significantly better service quality than the MPC\noptimization in terms of average rider waiting time, due to its ability to\nmodel a longer horizon.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 21:48:05 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 16:39:16 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Yuan", "Enpeng", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "2105.13462", "submitter": "Chao Ma", "authors": "Chao Ma, Lexing Ying", "title": "The Sobolev Regularization Effect of Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The multiplicative structure of parameters and input data in the first layer\nof neural networks is explored to build connection between the landscape of the\nloss function with respect to parameters and the landscape of the model\nfunction with respect to input data. By this connection, it is shown that flat\nminima regularize the gradient of the model function, which explains the good\ngeneralization performance of flat minima. Then, we go beyond the flatness and\nconsider high-order moments of the gradient noise, and show that Stochastic\nGradient Dascent (SGD) tends to impose constraints on these moments by a linear\nstability analysis of SGD around global minima. Together with the\nmultiplicative structure, we identify the Sobolev regularization effect of SGD,\ni.e. SGD regularizes the Sobolev seminorms of the model function with respect\nto the input data. Finally, bounds for generalization error and adversarial\nrobustness are provided for solutions found by SGD under assumptions of the\ndata distribution.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 21:49:21 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ma", "Chao", ""], ["Ying", "Lexing", ""]]}, {"id": "2105.13464", "submitter": "Shreyas Saxena", "authors": "Shreyas Saxena, Nidhi Vyas, Dennis DeCoste", "title": "Training With Data Dependent Dynamic Learning Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently many first and second order variants of SGD have been proposed to\nfacilitate training of Deep Neural Networks (DNNs). A common limitation of\nthese works stem from the fact that they use the same learning rate across all\ninstances present in the dataset. This setting is widely adopted under the\nassumption that loss functions for each instance are similar in nature, and\nhence, a common learning rate can be used. In this work, we relax this\nassumption and propose an optimization framework which accounts for difference\nin loss function characteristics across instances. More specifically, our\noptimizer learns a dynamic learning rate for each instance present in the\ndataset. Learning a dynamic learning rate for each instance allows our\noptimization framework to focus on different modes of training data during\noptimization. When applied to an image classification task, across different\nCNN architectures, learning dynamic learning rates leads to consistent gains\nover standard optimizers. When applied to a dataset containing corrupt\ninstances, our framework reduces the learning rates on noisy instances, and\nimproves over the state-of-the-art. Finally, we show that our optimization\nframework can be used for personalization of a machine learning model towards a\nknown targeted data distribution.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 21:52:29 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Saxena", "Shreyas", ""], ["Vyas", "Nidhi", ""], ["DeCoste", "Dennis", ""]]}, {"id": "2105.13485", "submitter": "Saurabh Gore", "authors": "Manuel Ntumba, Saurabh Gore", "title": "Avancee-1 Mission and SaDoD Method: LiDAR-based stimulated atomic\n  disintegration of space debris (SaDoD) using Optical Neural Networks", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.34878.82240", "report-no": null, "categories": "physics.ins-det cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The surface degradation of satellites in Low Earth Orbit (LEO) is affected by\nAtomic Oxygen (AO) and varies depending on the spacecraft orbital parameters.\nAtomic oxygen initiates several chemical and physical reactions with materials\nand produces erosion and self-disintegration of the debris at high energy. This\npaper discusses Avancee-1 Mission, LiDAR-based space debris removal using\nOptical Neural Networks (ONN) to optimize debris detection and mission\naccuracy. The SaDoD Method is a Stimulated Atomic Disintegration of Orbital\nDebris, which in this case has been achieved using LiDAR technology and Optical\nNeural Networks. We propose Optical Neural Network algorithms with a high\nability of image detection and classification. The results show that orbital\ndebris has a higher chance of disintegration when the laser beam is coming from\nGeostationary Orbit (GEO) satellites and in the presence of high solar\nactivities. This paper proposes a LiDAR-based space debris removal method\ndepending on the variation of atomic oxygen erosion with orbital parameters and\nsolar energy levels. The results obtained show that orbital debris undergoes\nthe most intense degradation at low altitudes and higher temperatures. The\nsatellites in GEO use Optical Neural Network algorithms for object detection\nbefore sending the laser beams to achieve self-disintegration. The SaDoD Method\ncan be implemented with other techniques, but especially for the Avancee-1\nMission, the SaDoD was implemented with LiDAR technologies and Optical Neural\nNetwork algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:44:28 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ntumba", "Manuel", ""], ["Gore", "Saurabh", ""]]}, {"id": "2105.13493", "submitter": "Patrick Kidger", "authors": "Patrick Kidger and James Foster and Xuechen Li and Terry Lyons", "title": "Efficient and Accurate Gradients for Neural SDEs", "comments": "Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural SDEs combine many of the best qualities of both RNNs and SDEs: memory\nefficient training, high-capacity function approximation, and strong priors on\nmodel space. This makes them a natural choice for modelling many types of\ntemporal dynamics. Training a Neural SDE (either as a VAE or as a GAN) requires\nbackpropagating through an SDE solve. This may be done by solving a\nbackwards-in-time SDE whose solution is the desired parameter gradients.\nHowever, this has previously suffered from severe speed and accuracy issues,\ndue to high computational cost and numerical truncation errors. Here, we\novercome these issues through several technical innovations. First, we\nintroduce the \\textit{reversible Heun method}. This is a new SDE solver that is\n\\textit{algebraically reversible}: eliminating numerical gradient errors, and\nthe first such solver of which we are aware. Moreover it requires half as many\nfunction evaluations as comparable solvers, giving up to a $1.98\\times$\nspeedup. Second, we introduce the \\textit{Brownian Interval}: a new, fast,\nmemory efficient, and exact way of sampling \\textit{and reconstructing}\nBrownian motion. With this we obtain up to a $10.6\\times$ speed improvement\nover previous techniques, which in contrast are both approximate and relatively\nslow. Third, when specifically training Neural SDEs as GANs (Kidger et al.\n2021), we demonstrate how SDE-GANs may be trained through careful weight\nclipping and choice of activation function. This reduces computational cost\n(giving up to a $1.87\\times$ speedup) and removes the numerical truncation\nerrors associated with gradient penalty. Altogether, we outperform the\nstate-of-the-art by substantial margins, with respect to training speed, and\nwith respect to classification, prediction, and MMD test metrics. We have\ncontributed implementations of all of our techniques to the torchsde library to\nhelp facilitate their adoption.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:59:36 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 17:34:54 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kidger", "Patrick", ""], ["Foster", "James", ""], ["Li", "Xuechen", ""], ["Lyons", "Terry", ""]]}, {"id": "2105.13495", "submitter": "Byung-Hoon Kim M.D. Ph.D.", "authors": "Byung-Hoon Kim, Jong Chul Ye, Jae-Jin Kim", "title": "Learning Dynamic Graph Representation of Brain Connectome with\n  Spatio-Temporal Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connectivity (FC) between regions of the brain can be assessed by\nthe degree of temporal correlation measured with functional neuroimaging\nmodalities. Based on the fact that these connectivities build a network,\ngraph-based approaches for analyzing the brain connectome have provided\ninsights into the functions of the human brain. The development of graph neural\nnetworks (GNNs) capable of learning representation from graph structured data\nhas led to increased interest in learning the graph representation of the brain\nconnectome. Although recent attempts to apply GNN to the FC network have shown\npromising results, there is still a common limitation that they usually do not\nincorporate the dynamic characteristics of the FC network which fluctuates over\ntime. In addition, a few studies that have attempted to use dynamic FC as an\ninput for the GNN reported a reduction in performance compared to static FC\nmethods, and did not provide temporal explainability. Here, we propose STAGIN,\na method for learning dynamic graph representation of the brain connectome with\nspatio-temporal attention. Specifically, a temporal sequence of brain graphs is\ninput to the STAGIN to obtain the dynamic graph representation, while novel\nREADOUT functions and the Transformer encoder provide spatial and temporal\nexplainability with attention, respectively. Experiments on the HCP-Rest and\nthe HCP-Task datasets demonstrate exceptional performance of our proposed\nmethod. Analysis of the spatio-temporal attention also provide concurrent\ninterpretation with the neuroscientific knowledge, which further validates our\nmethod. Code is available at https://github.com/egyptdj/stagin\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 23:06:50 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Kim", "Byung-Hoon", ""], ["Ye", "Jong Chul", ""], ["Kim", "Jae-Jin", ""]]}, {"id": "2105.13502", "submitter": "Poojan Oza", "authors": "Poojan Oza, Vishwanath A. Sindagi, Vibashan VS, Vishal M. Patel", "title": "Unsupervised Domain Adaptation of Object Detectors: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have led to the development of accurate and\nefficient models for various computer vision applications such as\nclassification, segmentation, and detection. However, learning highly accurate\nmodels relies on the availability of large-scale annotated datasets. Due to\nthis, model performance drops drastically when evaluated on label-scarce\ndatasets having visually distinct images, termed as domain adaptation problem.\nThere is a plethora of works to adapt classification and segmentation models to\nlabel-scarce target datasets through unsupervised domain adaptation.\nConsidering that detection is a fundamental task in computer vision, many\nrecent works have focused on developing novel domain adaptive detection\ntechniques. Here, we describe in detail the domain adaptation problem for\ndetection and present an extensive survey of the various methods. Furthermore,\nwe highlight strategies proposed and the associated shortcomings. Subsequently,\nwe identify multiple aspects of the problem that are most promising for future\nresearch. We believe that this survey shall be valuable to the pattern\nrecognition experts working in the fields of computer vision, biometrics,\nmedical imaging, and autonomous navigation by introducing them to the problem,\nand familiarizing them with the current status of the progress while providing\npromising directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 23:34:06 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 04:56:32 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Oza", "Poojan", ""], ["Sindagi", "Vishwanath A.", ""], ["VS", "Vibashan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2105.13504", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla, Yi Yu, Alessandro Rinaldo", "title": "Lattice partition recovery with dyadic CART", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study piece-wise constant signals corrupted by additive Gaussian noise\nover a $d$-dimensional lattice. Data of this form naturally arise in a host of\napplications, and the tasks of signal detection or testing, de-noising and\nestimation have been studied extensively in the statistical and signal\nprocessing literature. In this paper we consider instead the problem of\npartition recovery, i.e.~of estimating the partition of the lattice induced by\nthe constancy regions of the unknown signal, using the\ncomputationally-efficient dyadic classification and regression tree (DCART)\nmethodology proposed by \\citep{donoho1997cart}. We prove that, under\nappropriate regularity conditions on the shape of the partition elements, a\nDCART-based procedure consistently estimates the underlying partition at a rate\nof order $\\sigma^2 k^* \\log (N)/\\kappa^2$, where $k^*$ is the minimal number of\nrectangular sub-graphs obtained using recursive dyadic partitions supporting\nthe signal partition, $\\sigma^2$ is the noise variance, $\\kappa$ is the minimal\nmagnitude of the signal difference among contiguous elements of the partition\nand $N$ is the size of the lattice. Furthermore, under stronger assumptions,\nour method attains a sharper estimation error of order\n$\\sigma^2\\log(N)/\\kappa^2$, independent of $ k^*$, which we show to be minimax\nrate optimal. Our theoretical guarantees further extend to the partition\nestimator based on the optimal regression tree estimator (ORT) of\n\\cite{chatterjee2019adaptive} and to the one obtained through an NP-hard\nexhaustive search method. We corroborate our theoretical findings and the\neffectiveness of DCART for partition recovery in simulations.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 23:41:01 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Yu", "Yi", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "2105.13514", "submitter": "Duong Dung", "authors": "Tri Dung Duong, Qian Li, Guandong Xu", "title": "Stochastic Intervention for Causal Inference via Reinforcement Learning", "comments": "Under review for Neurocomputiong. arXiv admin note: substantial text\n  overlap with arXiv:2105.12898", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal inference methods are widely applied in various decision-making\ndomains such as precision medicine, optimal policy and economics. Central to\ncausal inference is the treatment effect estimation of intervention strategies,\nsuch as changes in drug dosing and increases in financial aid. Existing methods\nare mostly restricted to the deterministic treatment and compare outcomes under\ndifferent treatments. However, they are unable to address the substantial\nrecent interest of treatment effect estimation under stochastic treatment,\ne.g., \"how all units health status change if they adopt 50\\% dose reduction\".\nIn other words, they lack the capability of providing fine-grained treatment\neffect estimation to support sound decision-making. In our study, we advance\nthe causal inference research by proposing a new effective framework to\nestimate the treatment effect on stochastic intervention. Particularly, we\ndevelop a stochastic intervention effect estimator (SIE) based on nonparametric\ninfluence function, with the theoretical guarantees of robustness and fast\nconvergence rates. Additionally, we construct a customised reinforcement\nlearning algorithm based on the random search solver which can effectively find\nthe optimal policy to produce the greatest expected outcomes for the\ndecision-making process. Finally, we conduct an empirical study to justify that\nour framework can achieve significant performance in comparison with\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 00:11:22 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Duong", "Tri Dung", ""], ["Li", "Qian", ""], ["Xu", "Guandong", ""]]}, {"id": "2105.13524", "submitter": "Suyoung Lee", "authors": "Suyoung Lee and Sae-Young Chung", "title": "Improving Generalization in Meta-RL with Imaginary Tasks from Latent\n  Dynamics Mixture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalization ability of most meta-reinforcement learning (meta-RL)\nmethods is largely limited to test tasks that are sampled from the same\ndistribution used to sample training tasks. To overcome the limitation, we\npropose Latent Dynamics Mixture (LDM) that trains a reinforcement learning\nagent with imaginary tasks generated from mixtures of learned latent dynamics.\nBy training a policy on mixture tasks along with original training tasks, LDM\nallows the agent to prepare for unseen test tasks during training and prevents\nthe agent from overfitting the training tasks. LDM significantly outperforms\nstandard meta-RL methods in test returns on the gridworld navigation and MuJoCo\ntasks where we strictly separate the training task distribution and the test\ntask distribution.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 00:46:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lee", "Suyoung", ""], ["Chung", "Sae-Young", ""]]}, {"id": "2105.13530", "submitter": "George Kesidis", "authors": "Xi Li, David J. Miller, Zhen Xiang, George Kesidis", "title": "A BIC based Mixture Model Defense against Data Poisoning Attacks on\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Poisoning (DP) is an effective attack that causes trained classifiers to\nmisclassify their inputs.DP attacks significantly degrade a classifier's\naccuracy by covertly injecting attack samples into the training set. Broadly\napplicable to different classifier structures, without strong assumptions about\nthe attacker, we herein propose a novel Bayesian Information Criterion\n(BIC)-based mixture model defense against DP attacks that: 1) applies a mixture\nmodel both to well-fit potentially multi-modal class distributions and to\ncapture adversarial samples within a small subset of mixture components; 2)\njointly identifies poisoned components and samples by minimizing the BIC cost\nover all classes, with the identified poisoned data removed prior to classifier\ntraining. Our experimental results, for various classifier structures,\ndemonstrate the effectiveness and universality of our defense under strong DP\nattacks, as well as the superiority over other works.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 01:06:09 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Li", "Xi", ""], ["Miller", "David J.", ""], ["Xiang", "Zhen", ""], ["Kesidis", "George", ""]]}, {"id": "2105.13533", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad, Naimul Khan", "title": "Inertial Sensor Data To Image Encoding For Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) are successful deep learning models in\nthe field of computer vision. To get the maximum advantage of CNN model for\nHuman Action Recognition (HAR) using inertial sensor data, in this paper, we\nuse 4 types of spatial domain methods for transforming inertial sensor data to\nactivity images, which are then utilized in a novel fusion framework. These\nfour types of activity images are Signal Images (SI), Gramian Angular Field\n(GAF) Images, Markov Transition Field (MTF) Images and Recurrence Plot (RP)\nImages. Furthermore, for creating a multimodal fusion framework and to exploit\nactivity image, we made each type of activity images multimodal by convolving\nwith two spatial domain filters : Prewitt filter and High-boost filter.\nResnet-18, a CNN model, is used to learn deep features from multi-modalities.\nLearned features are extracted from the last pooling layer of each ReNet and\nthen fused by canonical correlation based fusion (CCF) for improving the\naccuracy of human action recognition. These highly informative features are\nserved as input to a multiclass Support Vector Machine (SVM). Experimental\nresults on three publicly available inertial datasets show the superiority of\nthe proposed method over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 01:22:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Khan", "Naimul", ""]]}, {"id": "2105.13536", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad, Anika Tabassum, Naimul Khan, Ling Guan", "title": "ECG Heart-beat Classification Using Multimodal Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel Image Fusion Model (IFM) for ECG heart-beat\nclassification to overcome the weaknesses of existing machine learning\ntechniques that rely either on manual feature extraction or direct utilization\nof 1D raw ECG signal. At the input of IFM, we first convert the heart beats of\nECG into three different images using Gramian Angular Field (GAF), Recurrence\nPlot (RP) and Markov Transition Field (MTF) and then fuse these images to\ncreate a single imaging modality. We use AlexNet for feature extraction and\nclassification and thus employ end to end deep learning. We perform experiments\non PhysioNet MIT-BIH dataset for five different arrhythmias in accordance with\nthe AAMI EC57 standard and on PTB diagnostics dataset for myocardial infarction\n(MI) classification. We achieved an state of an art results in terms of\nprediction accuracy, precision and recall.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 01:31:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Tabassum", "Anika", ""], ["Khan", "Naimul", ""], ["Guan", "Ling", ""]]}, {"id": "2105.13553", "submitter": "Alexander Siemenn", "authors": "Alexander E. Siemenn, Evyatar Shaulsky, Matthew Beveridge, Tonio\n  Buonassisi, Sara M. Hashmi, Iddo Drori", "title": "Autonomous Optimization of Fluid Systems at Varying Length Scales", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.mtrl-sci physics.flu-dyn", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous optimization is a process by which hardware conditions are\ndiscovered that generate an optimized experimental product without the guidance\nof a domain expert. We design an autonomous optimization framework to discover\nthe experimental conditions within fluid systems that generate discrete and\nuniform droplet patterns. Generating discrete and uniform droplets requires\nhigh-precision control over the experimental conditions of a fluid system.\nFluid stream instabilities, such as Rayleigh-Plateau instability and capillary\ninstability, drive the separation of a flow into individual droplets. However,\nbecause this phenomenon leverages an instability, by nature the hardware must\nbe precisely tuned to achieve uniform, repeatable droplets. Typically this\nrequires a domain expert in the loop and constant re-tuning depending on the\nhardware configuration and liquid precursor selection. Herein, we propose a\ncomputer vision-driven Bayesian optimization framework to discover the precise\nhardware conditions that generate uniform, reproducible droplets with the\ndesired features, leveraging flow instability without a domain expert in the\nloop. This framework is validated on two fluid systems, at the micrometer and\nmillimeter length scales, using microfluidic and inkjet systems, respectively,\nindicating the application breadth of this approach.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 02:08:03 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 16:08:41 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 16:47:36 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Siemenn", "Alexander E.", ""], ["Shaulsky", "Evyatar", ""], ["Beveridge", "Matthew", ""], ["Buonassisi", "Tonio", ""], ["Hashmi", "Sara M.", ""], ["Drori", "Iddo", ""]]}, {"id": "2105.13556", "submitter": "Carlos Carrion", "authors": "Carlos Carrion, Zenan Wang, Harikesh Nair, Xianghong Luo, Yulin Lei,\n  Xiliang Lin, Wenlong Chen, Qiyu Hu, Changping Peng, Yongjun Bao and Weipeng\n  Yan", "title": "Blending Advertising with Organic Content in E-Commerce: A Virtual Bids\n  Optimization Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In e-commerce platforms, sponsored and non-sponsored content are jointly\ndisplayed to users and both may interactively influence their engagement\nbehavior. The former content helps advertisers achieve their marketing goals\nand provides a stream of ad revenue to the platform. The latter content\ncontributes to users' engagement with the platform, which is key to its\nlong-term health. A burning issue for e-commerce platform design is how to\nblend advertising with content in a way that respects these interactions and\nbalances these multiple business objectives. This paper describes a system\ndeveloped for this purpose in the context of blending personalized sponsored\ncontent with non-sponsored content on the product detail pages of JD.COM, an\ne-commerce company. This system has three key features: (1) Optimization of\nmultiple competing business objectives through a new virtual bids approach and\nthe expressiveness of the latent, implicit valuation of the platform for the\nmultiple objectives via these virtual bids. (2) Modeling of users' click\nbehavior as a function of their characteristics, the individual characteristics\nof each sponsored content and the influence exerted by other sponsored and\nnon-sponsored content displayed alongside through a deep learning approach; (3)\nConsideration of externalities in the allocation of ads, thereby making it\ndirectly compatible with a Vickrey-Clarke-Groves (VCG) auction scheme for the\ncomputation of payments in the presence of these externalities. The system is\ncurrently deployed and serving all traffic through JD.COM's mobile application.\nExperiments demonstrating the performance and advantages of the system are\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 02:27:31 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Carrion", "Carlos", ""], ["Wang", "Zenan", ""], ["Nair", "Harikesh", ""], ["Luo", "Xianghong", ""], ["Lei", "Yulin", ""], ["Lin", "Xiliang", ""], ["Chen", "Wenlong", ""], ["Hu", "Qiyu", ""], ["Peng", "Changping", ""], ["Bao", "Yongjun", ""], ["Yan", "Weipeng", ""]]}, {"id": "2105.13557", "submitter": "Jingyun Jia", "authors": "Jingyun Jia, Philip K. Chan", "title": "Self-supervised Detransformation Autoencoder for Representation Learning\n  in Open Set Recognition", "comments": "arXiv admin note: text overlap with arXiv:2006.15117", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of Open set recognition (OSR) is to learn a classifier that can\nreject the unknown samples while classifying the known classes accurately. In\nthis paper, we propose a self-supervision method, Detransformation Autoencoder\n(DTAE), for the OSR problem. This proposed method engages in learning\nrepresentations that are invariant to the transformations of the input data.\nExperiments on several standard image datasets indicate that the pre-training\nprocess significantly improves the model performance in the OSR tasks.\nMeanwhile, our proposed self-supervision method achieves significant gains in\ndetecting the unknown class and classifying the known classes. Moreover, our\nanalysis indicates that DTAE can yield representations that contain more target\nclass information and less transformation information than RotNet.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 02:45:57 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Jia", "Jingyun", ""], ["Chan", "Philip K.", ""]]}, {"id": "2105.13559", "submitter": "Hao Su", "authors": "Hao Su", "title": "One-shot Learning with Absolute Generalization", "comments": "8 pages, 41 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One-shot learning is proposed to make a pretrained classifier workable on a\nnew dataset based on one labeled samples from each pattern. However, few of\nresearchers consider whether the dataset itself supports one-shot learning. In\nthis paper, we propose a set of definitions to explain what kind of datasets\ncan support one-shot learning and propose the concept \"absolute\ngeneralization\". Based on these definitions, we proposed a method to build an\nabsolutely generalizable classifier. The proposed method concatenates two\nsamples as a new single sample, and converts a classification problem to an\nidentity identification problem or a similarity metric problem. Experiments\ndemonstrate that the proposed method is superior to baseline on one-shot\nlearning datasets and artificial datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 02:52:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Su", "Hao", ""]]}, {"id": "2105.13570", "submitter": "Jiayu Shang", "authors": "Jiayu Shang and Yanni Sun", "title": "Detecting the hosts of bacteriophages using GCN-based semi-supervised\n  learning", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Motivation: Bacteriophages (aka phages) are viruses that infect bacteria and\narchaea. Thus, they play important regulatory roles in natural and\nhost-associated ecosystems. As the most abundant and diverse biological\nentities in the biosphere, phages have received increased attention in their\nresearch and applications. In particular, identifying their hosts provides key\nknowledge for their usages as antibiotics. High-throughput sequencing and its\napplication to the microbiome have offered new opportunities for phage host\ndetection. However, there are two main challenges for computational host\nprediction. First, the known phage-host relationships are very limited compared\nto sequenced phages. Second, although the sequence similarity between phages\nand bacteria has been used as a major feature for host prediction, the\nalignment is either missing or ambiguous for accurate host prediction. Thus,\nthere is still a need to improve the accuracy of host prediction. Results: In\nthis work, we present a semi-supervised learning model, named HostG, to conduct\nhost prediction for novel phages. We construct a knowledge graph by utilizing\nboth phage-phage protein similarity and phage-host DNA sequence similarity.\nThen graph convolutional network (GCN) is adopted to exploit phages with or\nwithout known hosts in training to enhance the learning ability. During the GCN\ntraining, we minimize the expected calibrated error (ECE) to ensure the\nconfidence of the predictions. We tested HostG on both simulated and real\nsequencing data and the results demonstrated that it competes favorably against\nthe state-of-the-art pipelines.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 03:29:31 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Shang", "Jiayu", ""], ["Sun", "Yanni", ""]]}, {"id": "2105.13573", "submitter": "El Moatez Billah Nagoudi", "authors": "El Moatez Billah Nagoudi, AbdelRahim Elmadany, Muhammad Abdul-Mageed", "title": "Investigating Code-Mixed Modern Standard Arabic-Egyptian to English\n  Machine Translation", "comments": "CALCS2021, colocated with NAACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent progress in neural machine translation (NMT) has made it possible to\ntranslate successfully between monolingual language pairs where large parallel\ndata exist, with pre-trained models improving performance even further.\nAlthough there exists work on translating in code-mixed settings (where one of\nthe pairs includes text from two or more languages), it is still unclear what\nrecent success in NMT and language modeling exactly means for translating\ncode-mixed text. We investigate one such context, namely MT from code-mixed\nModern Standard Arabic and Egyptian Arabic (MSAEA) into English. We develop\nmodels under different conditions, employing both (i) standard end-to-end\nsequence-to-sequence (S2S) Transformers trained from scratch and (ii)\npre-trained S2S language models (LMs). We are able to acquire reasonable\nperformance using only MSA-EN parallel data with S2S models trained from\nscratch. We also find LMs fine-tuned on data from various Arabic dialects to\nhelp the MSAEA-EN task. Our work is in the context of the Shared Task on\nMachine Translation in Code-Switching. Our best model achieves $\\bf25.72$ BLEU,\nplacing us first on the official shared task evaluation for MSAEA-EN.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 03:38:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Nagoudi", "El Moatez Billah", ""], ["Elmadany", "AbdelRahim", ""], ["Abdul-Mageed", "Muhammad", ""]]}, {"id": "2105.13591", "submitter": "Guangyin Jin", "authors": "Guangyin Jin, Huan Yan, Fuxian Li, Jincai Huang, Yong Li", "title": "Spatio-Temporal Dual Graph Neural Networks for Travel Time Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Travel time estimation is one of the core tasks for the development of\nintelligent transportation systems. Most previous works model the road segments\nor intersections separately by learning their spatio-temporal characteristics\nto estimate travel time. However, due to the continuous alternations of the\nroad segments and intersections in a path, the dynamic features are supposed to\nbe coupled and interactive. Therefore, modeling one of them limits further\nimprovement in accuracy of estimating travel time. To address the above\nproblems, a novel graph-based deep learning framework for travel time\nestimation is proposed in this paper, namely Spatio-Temporal Dual Graph Neural\nNetworks (STDGNN). Specifically, we first establish the node-wise and edge-wise\ngraphs to respectively characterize the adjacency relations of intersections\nand that of road segments. In order to extract the joint spatio-temporal\ncorrelations of the intersections and road segments, we adopt the\nspatio-temporal dual graph learning approach that incorporates multiple\nspatial-temporal dual graph learning modules with multi-scale network\narchitectures for capturing multi-level spatial-temporal information from the\ndual graph. Finally, we employ the multi-task learning approach to estimate the\ntravel time of a given whole route, each road segment and intersection\nsimultaneously. We conduct extensive experiments to evaluate our proposed model\non three real-world trajectory datasets, and the experimental results show that\nSTDGNN significantly outperforms several state-of-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 05:15:45 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 16:09:05 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jin", "Guangyin", ""], ["Yan", "Huan", ""], ["Li", "Fuxian", ""], ["Huang", "Jincai", ""], ["Li", "Yong", ""]]}, {"id": "2105.13599", "submitter": "Cheng-Wen Hsu", "authors": "Shin-Hung Chang, Cheng-Wen Hsu, Hsing-Ying Li, Wei-Sheng Zeng,\n  Jan-Ming Ho", "title": "Short-Term Stock Price-Trend Prediction Using Meta-Learning", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although conventional machine learning algorithms have been widely adopted\nfor stock-price predictions in recent years, the massive volume of specific\nlabeled data required are not always available. In contrast, meta-learning\ntechnology uses relatively small amounts of training data, called fast\nlearners. Such methods are beneficial under conditions of limited data\navailability, which often obtain for trend prediction based on time-series data\nlimited by sparse information. In this study, we consider short-term stock\nprice prediction using a meta-learning framework with several convolutional\nneural networks, including the temporal convolution network, fully\nconvolutional network, and residual neural network. We propose a sliding time\nhorizon to label stocks according to their predicted price trends, referred to\nas called dynamic k-average labeling, using prediction labels including \"rise\nplus\", \"rise\", \"fall\", and \"fall plus\". The effectiveness of the proposed\nmeta-learning framework was evaluated by application to the S&P500. The\nexperimental results show that the inclusion of the proposed meta-learning\nframework significantly improved both regular and balanced prediction accuracy\nand profitability.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:03:05 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chang", "Shin-Hung", ""], ["Hsu", "Cheng-Wen", ""], ["Li", "Hsing-Ying", ""], ["Zeng", "Wei-Sheng", ""], ["Ho", "Jan-Ming", ""]]}, {"id": "2105.13608", "submitter": "Ehsan Kamalloo", "authors": "Ehsan Kamalloo, Mehdi Rezagholizadeh, Peyman Passban, Ali Ghodsi", "title": "Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data\n  Augmentation via MiniMax", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Natural Language Processing (NLP), finding data augmentation techniques\nthat can produce high-quality human-interpretable examples has always been\nchallenging. Recently, leveraging kNN such that augmented examples are\nretrieved from large repositories of unlabelled sentences has made a step\ntoward interpretable augmentation. Inspired by this paradigm, we introduce\nMinimax-kNN, a sample efficient data augmentation strategy tailored for\nKnowledge Distillation (KD). We exploit a semi-supervised approach based on KD\nto train a model on augmented data. In contrast to existing kNN augmentation\ntechniques that blindly incorporate all samples, our method dynamically selects\na subset of augmented samples that maximizes KL-divergence between the teacher\nand student models. This step aims to extract the most efficient samples to\nensure our augmented data covers regions in the input space with maximum loss\nvalue. We evaluated our technique on several text classification tasks and\ndemonstrated that Minimax-kNN consistently outperforms strong baselines. Our\nresults show that Minimax-kNN requires fewer augmented examples and less\ncomputation to achieve superior performance over the state-of-the-art kNN-based\naugmentation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:32:32 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:18:33 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kamalloo", "Ehsan", ""], ["Rezagholizadeh", "Mehdi", ""], ["Passban", "Peyman", ""], ["Ghodsi", "Ali", ""]]}, {"id": "2105.13609", "submitter": "Vektor Dewanto", "authors": "Vektor Dewanto, Marcus Gallagher", "title": "A nearly Blackwell-optimal policy gradient method", "comments": "26 pages (9-page main content), refined the appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For continuing environments, reinforcement learning methods commonly maximize\na discounted reward criterion with discount factor close to 1 in order to\napproximate the steady-state reward (the gain). However, such a criterion only\nconsiders the long-run performance, ignoring the transient behaviour. In this\nwork, we develop a policy gradient method that optimizes the gain, then the\nbias (which indicates the transient performance and is important to capably\nselect from policies with equal gain). We derive expressions that enable\nsampling for the gradient of the bias, and its preconditioning Fisher matrix.\nWe further propose an algorithm that solves the corresponding bi-level\noptimization using a logarithmic barrier. Experimental results provide insights\ninto the fundamental mechanisms of our proposal.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:37:02 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 00:44:49 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Dewanto", "Vektor", ""], ["Gallagher", "Marcus", ""]]}, {"id": "2105.13618", "submitter": "Jia Yan", "authors": "Jia Yan, Suzhi Bi, Ying-Jun Angela Zhang", "title": "Optimal Model Placement and Online Model Splitting for Device-Edge\n  Co-Inference", "comments": "This paper has been submitted to IEEE Transactions on Wireless\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Device-edge co-inference opens up new possibilities for resource-constrained\nwireless devices (WDs) to execute deep neural network (DNN)-based applications\nwith heavy computation workloads. In particular, the WD executes the first few\nlayers of the DNN and sends the intermediate features to the edge server that\nprocesses the remaining layers of the DNN. By adapting the model splitting\ndecision, there exists a tradeoff between local computation cost and\ncommunication overhead. In practice, the DNN model is re-trained and updated\nperiodically at the edge server. Once the DNN parameters are regenerated, part\nof the updated model must be placed at the WD to facilitate on-device\ninference. In this paper, we study the joint optimization of the model\nplacement and online model splitting decisions to minimize the energy-and-time\ncost of device-edge co-inference in presence of wireless channel fading. The\nproblem is challenging because the model placement and model splitting\ndecisions are strongly coupled, while involving two different time scales. We\nfirst tackle online model splitting by formulating an optimal stopping problem,\nwhere the finite horizon of the problem is determined by the model placement\ndecision. In addition to deriving the optimal model splitting rule based on\nbackward induction, we further investigate a simple one-stage look-ahead rule,\nfor which we are able to obtain analytical expressions of the model splitting\ndecision. The analysis is useful for us to efficiently optimize the model\nplacement decision in a larger time scale. In particular, we obtain a\nclosed-form model placement solution for the fully-connected multilayer\nperceptron with equal neurons. Simulation results validate the superior\nperformance of the joint optimal model placement and splitting with various DNN\nstructures.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:55:04 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yan", "Jia", ""], ["Bi", "Suzhi", ""], ["Zhang", "Ying-Jun Angela", ""]]}, {"id": "2105.13619", "submitter": "Jingyi Liu", "authors": "Jingyi Liu, Zhongyu Li, Xiayue Fan, Jintao Yan, Bolin Li, Xuemeng Hu,\n  Qing Xia, and Yue Wu", "title": "CRT-Net: A Generalized and Scalable Framework for the Computer-Aided\n  Diagnosis of Electrocardiogram Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrocardiogram (ECG) signals play critical roles in the clinical screening\nand diagnosis of many types of cardiovascular diseases. Despite deep neural\nnetworks that have been greatly facilitated computer-aided diagnosis (CAD) in\nmany clinical tasks, the variability and complexity of ECG in the clinic still\npose significant challenges in both diagnostic performance and clinical\napplications. In this paper, we develop a robust and scalable framework for the\nclinical recognition of ECG. Considering the fact that hospitals generally\nrecord ECG signals in the form of graphic waves of 2-D images, we first extract\nthe graphic waves of 12-lead images into numerical 1-D ECG signals by a\nproposed bi-directional connectivity method. Subsequently, a novel deep neural\nnetwork, namely CRT-Net, is designed for the fine-grained and comprehensive\nrepresentation and recognition of 1-D ECG signals. The CRT-Net can well explore\nwaveform features, morphological characteristics and time domain features of\nECG by embedding convolution neural network(CNN), recurrent neural\nnetwork(RNN), and transformer module in a scalable deep model, which is\nespecially suitable in clinical scenarios with different lengths of ECG signals\ncaptured from different devices. The proposed framework is first evaluated on\ntwo widely investigated public repositories, demonstrating the superior\nperformance of ECG recognition in comparison with state-of-the-art. Moreover,\nwe validate the effectiveness of our proposed bi-directional connectivity and\nCRT-Net on clinical ECG images collected from the local hospital, including 258\npatients with chronic kidney disease (CKD), 351 patients with Type-2 Diabetes\n(T2DM), and around 300 patients in the control group. In the experiments, our\nmethods can achieve excellent performance in the recognition of these two types\nof disease.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:56:06 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Liu", "Jingyi", ""], ["Li", "Zhongyu", ""], ["Fan", "Xiayue", ""], ["Yan", "Jintao", ""], ["Li", "Bolin", ""], ["Hu", "Xuemeng", ""], ["Xia", "Qing", ""], ["Wu", "Yue", ""]]}, {"id": "2105.13623", "submitter": "Siyuan Guo", "authors": "Siyuan Guo, Lixin Zou, Yiding Liu, Wenwen Ye, Suqi Cheng, Shuaiqiang\n  Wang, Hechang Chen, Dawei Yin, Yi Chang", "title": "Enhanced Doubly Robust Learning for Debiasing Post-click Conversion Rate\n  Estimation", "comments": "10 pages, 3 figures, accepted by SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462917", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-click conversion, as a strong signal indicating the user preference, is\nsalutary for building recommender systems. However, accurately estimating the\npost-click conversion rate (CVR) is challenging due to the selection bias,\ni.e., the observed clicked events usually happen on users' preferred items.\nCurrently, most existing methods utilize counterfactual learning to debias\nrecommender systems. Among them, the doubly robust (DR) estimator has achieved\ncompetitive performance by combining the error imputation based (EIB) estimator\nand the inverse propensity score (IPS) estimator in a doubly robust way.\nHowever, inaccurate error imputation may result in its higher variance than the\nIPS estimator. Worse still, existing methods typically use simple\nmodel-agnostic methods to estimate the imputation error, which are not\nsufficient to approximate the dynamically changing model-correlated target\n(i.e., the gradient direction of the prediction model). To solve these\nproblems, we first derive the bias and variance of the DR estimator. Based on\nit, a more robust doubly robust (MRDR) estimator has been proposed to further\nreduce its variance while retaining its double robustness. Moreover, we propose\na novel double learning approach for the MRDR estimator, which can convert the\nerror imputation into the general CVR estimation. Besides, we empirically\nverify that the proposed learning scheme can further eliminate the high\nvariance problem of the imputation learning. To evaluate its effectiveness,\nextensive experiments are conducted on a semi-synthetic dataset and two\nreal-world datasets. The results demonstrate the superiority of the proposed\napproach over the state-of-the-art methods. The code is available at\nhttps://github.com/guosyjlu/MRDR-DL.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:59:49 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Guo", "Siyuan", ""], ["Zou", "Lixin", ""], ["Liu", "Yiding", ""], ["Ye", "Wenwen", ""], ["Cheng", "Suqi", ""], ["Wang", "Shuaiqiang", ""], ["Chen", "Hechang", ""], ["Yin", "Dawei", ""], ["Chang", "Yi", ""]]}, {"id": "2105.13636", "submitter": "Akinori F Ebihara", "authors": "Taiki Miyagawa and Akinori F. Ebihara", "title": "The Power of Log-Sum-Exp: Sequential Density Ratio Matrix Estimation for\n  Speed-Accuracy Optimization", "comments": "Accepted to International Conference on Machine Learning (ICML) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a model for multiclass classification of time series to make a\nprediction as early and as accurate as possible. The matrix sequential\nprobability ratio test (MSPRT) is known to be asymptotically optimal for this\nsetting, but contains a critical assumption that hinders broad real-world\napplications; the MSPRT requires the underlying probability density. To address\nthis problem, we propose to solve density ratio matrix estimation (DRME), a\nnovel type of density ratio estimation that consists of estimating matrices of\nmultiple density ratios with constraints and thus is more challenging than the\nconventional density ratio estimation. We propose a log-sum-exp-type loss\nfunction (LSEL) for solving DRME and prove the following: (i) the LSEL provides\nthe true density ratio matrix as the sample size of the training set increases\n(consistency); (ii) it assigns larger gradients to harder classes (hard class\nweighting effect); and (iii) it provides discriminative scores even on\nclass-imbalanced datasets (guess-aversion). Our overall architecture for early\nclassification, MSPRT-TANDEM, statistically significantly outperforms baseline\nmodels on four datasets including action recognition, especially in the early\nstage of sequential observations. Our code and datasets are publicly available\nat: https://github.com/TaikiMiyagawa/MSPRT-TANDEM.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:21:58 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 01:44:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Miyagawa", "Taiki", ""], ["Ebihara", "Akinori F.", ""]]}, {"id": "2105.13637", "submitter": "Zhou Lu", "authors": "Daogao Liu, Zhou Lu", "title": "Curse of Dimensionality in Unconstrained Private Convex ERM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the lower bounds of differentially private empirical risk\nminimization for general convex functions in this paper. For convex generalized\nlinear models (GLMs), the well-known tight bound of DP-ERM in the constrained\ncase is $\\tilde{\\Theta}(\\frac{\\sqrt{p}}{\\epsilon n})$, while recently,\n\\cite{sstt21} find the tight bound of DP-ERM in the unconstrained case is\n$\\tilde{\\Theta}(\\frac{\\sqrt{\\text{rank}}}{\\epsilon n})$ where $p$ is the\ndimension, $n$ is the sample size and $\\text{rank}$ is the rank of the feature\nmatrix of the GLM objective function. As $\\text{rank}\\leq \\min\\{n,p\\}$, a\nnatural and important question arises that whether we can evade the curse of\ndimensionality for over-parameterized models where $n\\ll p$, for more general\nconvex functions beyond GLM. We answer this question negatively by giving the\nfirst and tight lower bound of unconstrained private ERM for the general convex\nfunction, matching the current upper bound\n$\\tilde{O}(\\frac{\\sqrt{p}}{n\\epsilon})$ for unconstrained private ERM. We also\ngive an $\\Omega(\\frac{p}{n\\epsilon})$ lower bound for unconstrained pure-DP ERM\nwhich recovers the result in the constrained case.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:28:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Liu", "Daogao", ""], ["Lu", "Zhou", ""]]}, {"id": "2105.13645", "submitter": "Zeren Huang", "authors": "Zeren Huang, Kerong Wang, Furui Liu, Hui-ling Zhen, Weinan Zhang,\n  Mingxuan Yuan, Jianye Hao, Yong Yu, Jun Wang", "title": "Learning to Select Cuts for Efficient Mixed-Integer Programming", "comments": "30 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutting plane methods play a significant role in modern solvers for tackling\nmixed-integer programming (MIP) problems. Proper selection of cuts would remove\ninfeasible solutions in the early stage, thus largely reducing the\ncomputational burden without hurting the solution accuracy. However, the major\ncut selection approaches heavily rely on heuristics, which strongly depend on\nthe specific problem at hand and thus limit their generalization capability. In\nthis paper, we propose a data-driven and generalizable cut selection approach,\nnamed Cut Ranking, in the settings of multiple instance learning. To measure\nthe quality of the candidate cuts, a scoring function, which takes the\ninstance-specific cut features as inputs, is trained and applied in cut ranking\nand selection. In order to evaluate our method, we conduct extensive\nexperiments on both synthetic datasets and real-world datasets. Compared with\ncommonly used heuristics for cut selection, the learning-based policy has shown\nto be more effective, and is capable of generalizing over multiple problems\nwith different properties. Cut Ranking has been deployed in an industrial\nsolver for large-scale MIPs. In the online A/B testing of the product planning\nproblems with more than $10^7$ variables and constraints daily, Cut Ranking has\nachieved the average speedup ratio of 12.42% over the production solver without\nany accuracy loss of solution.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:48:34 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Huang", "Zeren", ""], ["Wang", "Kerong", ""], ["Liu", "Furui", ""], ["Zhen", "Hui-ling", ""], ["Zhang", "Weinan", ""], ["Yuan", "Mingxuan", ""], ["Hao", "Jianye", ""], ["Yu", "Yong", ""], ["Wang", "Jun", ""]]}, {"id": "2105.13649", "submitter": "Ori Lahav", "authors": "Ori Lahav, Guy Katz", "title": "Pruning and Slicing Neural Networks using Formal Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) play an increasingly important role in various\ncomputer systems. In order to create these networks, engineers typically\nspecify a desired topology, and then use an automated training algorithm to\nselect the network's weights. While training algorithms have been studied\nextensively and are well understood, the selection of topology remains a form\nof art, and can often result in networks that are unnecessarily large - and\nconsequently are incompatible with end devices that have limited memory,\nbattery or computational power. Here, we propose to address this challenge by\nharnessing recent advances in DNN verification. We present a framework and a\nmethodology for discovering redundancies in DNNs - i.e., for finding neurons\nthat are not needed, and can be removed in order to reduce the size of the DNN.\nBy using sound verification techniques, we can formally guarantee that our\nsimplified network is equivalent to the original, either completely, or up to a\nprescribed tolerance. Further, we show how to combine our technique with\nslicing, which results in a family of very small DNNs, which are together\nequivalent to the original. Our approach can produce DNNs that are\nsignificantly smaller than the original, rendering them suitable for deployment\non additional kinds of systems, and even more amenable to subsequent formal\nverification. We provide a proof-of-concept implementation of our approach, and\nuse it to evaluate our techniques on several real-world DNNs.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:53:50 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lahav", "Ori", ""], ["Katz", "Guy", ""]]}, {"id": "2105.13655", "submitter": "Dabeen Lee", "authors": "Dabeen Lee, Milan Vojnovic", "title": "Learning to Schedule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a learning and scheduling algorithm to minimize the\nexpected cumulative holding cost incurred by jobs, where statistical parameters\ndefining their individual holding costs are unknown a priori. In each time\nslot, the server can process a job while receiving the realized random holding\ncosts of the jobs remaining in the system. Our algorithm is a learning-based\nvariant of the $c\\mu$ rule for scheduling: it starts with a preemption period\nof fixed length which serves as a learning phase, and after accumulating enough\ndata about individual jobs, it switches to nonpreemptive scheduling mode. The\nalgorithm is designed to handle instances with large or small gaps in jobs'\nparameters and achieves near-optimal performance guarantees. The performance of\nour algorithm is captured by its regret, where the benchmark is the minimum\npossible cost attained when the statistical parameters of jobs are fully known.\nWe prove upper bounds on the regret of our algorithm, and we derive a regret\nlower bound that is almost matching the proposed upper bounds. Our numerical\nresults demonstrate the effectiveness of our algorithm and show that our\ntheoretical regret analysis is nearly tight.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:04:06 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lee", "Dabeen", ""], ["Vojnovic", "Milan", ""]]}, {"id": "2105.13669", "submitter": "Bernt Ivar Utst{\\o}l N{\\o}dland", "authors": "Bernt Ivar Utst{\\o}l N{\\o}dland", "title": "Measuring global properties of neural generative model outputs via\n  generating mathematical objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train deep generative models on datasets of reflexive polytopes. This\nenables us to compare how well the models have picked up on various global\nproperties of generated samples. Our datasets are complete in the sense that\nevery single example, up to changes of coordinate, is included in the dataset.\nUsing this property we also perform tests checking to what extent the models\nare merely memorizing the data. We also train models on the same dataset\nrepresented in two different ways, enabling us to measure which form is easiest\nto learn from. We use these experiments to show that deep generative models can\nlearn to generate geometric objects with non-trivial global properties, and\nthat the models learn some underlying properties of the objects rather than\nsimply memorizing the data.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:38:18 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["N\u00f8dland", "Bernt Ivar Utst\u00f8l", ""]]}, {"id": "2105.13670", "submitter": "Hieu Nguyen", "authors": "Nguyen Quang Hieu, Dinh Thai Hoang, Dusit Niyato, Ping Wang, Dong In\n  Kim, and Chau Yuen", "title": "Transferable Deep Reinforcement Learning Framework for Autonomous\n  Vehicles with Joint Radar-Data Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous Vehicles (AVs) are required to operate safely and efficiently in\ndynamic environments. For this, the AVs equipped with Joint\nRadar-Communications (JRC) functions can enhance the driving safety by\nutilizing both radar detection and data communication functions. However,\noptimizing the performance of the AV system with two different functions under\nuncertainty and dynamic of surrounding environments is very challenging. In\nthis work, we first propose an intelligent optimization framework based on the\nMarkov Decision Process (MDP) to help the AV make optimal decisions in\nselecting JRC operation functions under the dynamic and uncertainty of the\nsurrounding environment. We then develop an effective learning algorithm\nleveraging recent advances of deep reinforcement learning techniques to find\nthe optimal policy for the AV without requiring any prior information about\nsurrounding environment. Furthermore, to make our proposed framework more\nscalable, we develop a Transfer Learning (TL) mechanism that enables the AV to\nleverage valuable experiences for accelerating the training process when it\nmoves to a new environment. Extensive simulations show that the proposed\ntransferable deep reinforcement learning framework reduces the obstacle miss\ndetection probability by the AV up to 67% compared to other conventional deep\nreinforcement learning approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:45:37 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hieu", "Nguyen Quang", ""], ["Hoang", "Dinh Thai", ""], ["Niyato", "Dusit", ""], ["Wang", "Ping", ""], ["Kim", "Dong In", ""], ["Yuen", "Chau", ""]]}, {"id": "2105.13697", "submitter": "Mingfu Xue", "authors": "Mingfu Xue, Zhiyu Wu, Jian Wang, Yushu Zhang, Weiqiang Liu", "title": "AdvParams: An Active DNN Intellectual Property Protection Technique via\n  Adversarial Perturbation Based Parameter Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-trained DNN model can be regarded as an intellectual property (IP) of\nthe model owner. To date, many DNN IP protection methods have been proposed,\nbut most of them are watermarking based verification methods where model owners\ncan only verify their ownership passively after the copyright of DNN models has\nbeen infringed. In this paper, we propose an effective framework to actively\nprotect the DNN IP from infringement. Specifically, we encrypt the DNN model's\nparameters by perturbing them with well-crafted adversarial perturbations. With\nthe encrypted parameters, the accuracy of the DNN model drops significantly,\nwhich can prevent malicious infringers from using the model. After the\nencryption, the positions of encrypted parameters and the values of the added\nadversarial perturbations form a secret key. Authorized user can use the secret\nkey to decrypt the model. Compared with the watermarking methods which only\npassively verify the ownership after the infringement occurs, the proposed\nmethod can prevent infringement in advance. Moreover, compared with most of the\nexisting active DNN IP protection methods, the proposed method does not require\nadditional training process of the model, which introduces low computational\noverhead. Experimental results show that, after the encryption, the test\naccuracy of the model drops by 80.65%, 81.16%, and 87.91% on Fashion-MNIST,\nCIFAR-10, and GTSRB, respectively. Moreover, the proposed method only needs to\nencrypt an extremely low number of parameters, and the proportion of the\nencrypted parameters of all the model's parameters is as low as 0.000205%. The\nexperimental results also indicate that, the proposed method is robust against\nmodel fine-tuning attack and model pruning attack. Moreover, for the adaptive\nattack where attackers know the detailed steps of the proposed method, the\nproposed method is also demonstrated to be robust.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 09:42:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Xue", "Mingfu", ""], ["Wu", "Zhiyu", ""], ["Wang", "Jian", ""], ["Zhang", "Yushu", ""], ["Liu", "Weiqiang", ""]]}, {"id": "2105.13727", "submitter": "Kieran Wood", "authors": "Kieran Wood, Stephen Roberts, Stefan Zohren", "title": "Slow Momentum with Fast Reversion: A Trading Strategy Using Deep\n  Learning and Changepoint Detection", "comments": "minor corrections, strategy comparison for 2015-2020 made more robust\n  by repeated trials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Momentum strategies are an important part of alternative investments and are\nat the heart of commodity trading advisors (CTAs). These strategies have\nhowever been found to have difficulties adjusting to rapid changes in market\nconditions, such as during the 2020 market crash. In particular, immediately\nafter momentum turning points, where a trend reverses from an uptrend\n(downtrend) to a downtrend (uptrend), time-series momentum (TSMOM) strategies\nare prone to making bad bets. To improve the response to regime change, we\nintroduce a novel approach, where we insert an online change-point detection\n(CPD) module into a Deep Momentum Network (DMN) [1904.04912] pipeline, which\nuses an LSTM deep-learning architecture to simultaneously learn both trend\nestimation and position sizing. Furthermore, our model is able to optimise the\nway in which it balances 1) a slow momentum strategy which exploits persisting\ntrends, but does not overreact to localised price moves, and 2) a fast\nmean-reversion strategy regime by quickly flipping its position, then swapping\nit back again to exploit localised price moves. Our CPD module outputs a\nchangepoint location and severity score, allowing our model to learn to respond\nto varying degrees of disequilibrium, or smaller and more localised\nchangepoints, in a data driven manner. Using a portfolio of 50, liquid,\ncontinuous futures contracts over the period 1990-2020, the addition of the CPD\nmodule leads to an improvement in Sharpe ratio of one-third. Even more notably,\nthis module is especially beneficial in periods of significant nonstationarity,\nand in particular, over the most recent years tested (2015-2020) the\nperformance boost is approximately two-thirds. This is especially interesting\nas traditional momentum strategies have been underperforming in this period.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 10:46:53 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:51:25 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wood", "Kieran", ""], ["Roberts", "Stephen", ""], ["Zohren", "Stefan", ""]]}, {"id": "2105.13731", "submitter": "Yongtao Hu", "authors": "Zhuming Zhang, Yongtao Hu, Guoxing Yu, Jingwen Dai", "title": "DeepTag: A General Framework for Fiducial Marker Design and Detection", "comments": "preprint in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fiducial marker system usually consists of markers, a detection algorithm,\nand a coding system. The appearance of markers and the detection robustness are\ngenerally limited by the existing detection algorithms, which are hand-crafted\nwith traditional low-level image processing techniques. Furthermore, a\nsophisticatedly designed coding system is required to overcome the shortcomings\nof both markers and detection algorithms. To improve the flexibility and\nrobustness in various applications, we propose a general deep learning based\nframework, DeepTag, for fiducial marker design and detection. DeepTag not only\nsupports detection of a wide variety of existing marker families, but also\nmakes it possible to design new marker families with customized local patterns.\nMoreover, we propose an effective procedure to synthesize training data on the\nfly without manual annotations. Thus, DeepTag can easily adapt to existing and\nnewly-designed marker families. To validate DeepTag and existing methods,\nbeside existing datasets, we further collect a new large and challenging\ndataset where markers are placed in different view distances and angles.\nExperiments show that DeepTag well supports different marker families and\ngreatly outperforms the existing methods in terms of both detection robustness\nand pose accuracy. Both code and dataset are available at\n\\url{https://herohuyongtao.github.io/research/publications/deep-tag/}.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 10:54:59 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Zhuming", ""], ["Hu", "Yongtao", ""], ["Yu", "Guoxing", ""], ["Dai", "Jingwen", ""]]}, {"id": "2105.13745", "submitter": "Yaowei Zheng", "authors": "Xiaohui Guo, Richong Zhang, Yaowei Zheng, Yongyi Mao", "title": "Robust Regularization with Adversarial Labelling of Perturbed Samples", "comments": "Accepted to IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches have suggested that the predictive accuracy of neural\nnetwork may contend with its adversarial robustness. This presents challenges\nin designing effective regularization schemes that also provide strong\nadversarial robustness. Revisiting Vicinal Risk Minimization (VRM) as a\nunifying regularization principle, we propose Adversarial Labelling of\nPerturbed Samples (ALPS) as a regularization scheme that aims at improving the\ngeneralization ability and adversarial robustness of the trained model. ALPS\ntrains neural networks with synthetic samples formed by perturbing each\nauthentic input sample towards another one along with an adversarially assigned\nlabel. The ALPS regularization objective is formulated as a min-max problem, in\nwhich the outer problem is minimizing an upper-bound of the VRM loss, and the\ninner problem is L$_1$-ball constrained adversarial labelling on perturbed\nsample. The analytic solution to the induced inner maximization problem is\nelegantly derived, which enables computational efficiency. Experiments on the\nSVHN, CIFAR-10, CIFAR-100 and Tiny-ImageNet datasets show that the ALPS has a\nstate-of-the-art regularization performance while also serving as an effective\nadversarial training scheme.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 11:26:49 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Guo", "Xiaohui", ""], ["Zhang", "Richong", ""], ["Zheng", "Yaowei", ""], ["Mao", "Yongyi", ""]]}, {"id": "2105.13746", "submitter": "Javier Maroto", "authors": "Javier Maroto, G\\'er\\^ome Bovet and Pascal Frossard", "title": "SafeAMC: Adversarial training for robust modulation recognition models", "comments": "arXiv admin note: text overlap with arXiv:2103.14977", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In communication systems, there are many tasks, like modulation recognition,\nwhich rely on Deep Neural Networks (DNNs) models. However, these models have\nbeen shown to be susceptible to adversarial perturbations, namely imperceptible\nadditive noise crafted to induce misclassification. This raises questions about\nthe security but also the general trust in model predictions. We propose to use\nadversarial training, which consists of fine-tuning the model with adversarial\nperturbations, to increase the robustness of automatic modulation recognition\n(AMC) models. We show that current state-of-the-art models benefit from\nadversarial training, which mitigates the robustness issues for some families\nof modulations. We use adversarial perturbations to visualize the features\nlearned, and we found that in robust models the signal symbols are shifted\ntowards the nearest classes in constellation space, like maximum likelihood\nmethods. This confirms that robust models not only are more secure, but also\nmore interpretable, building their decisions on signal statistics that are\nrelevant to modulation recognition.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 11:29:04 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Maroto", "Javier", ""], ["Bovet", "G\u00e9r\u00f4me", ""], ["Frossard", "Pascal", ""]]}, {"id": "2105.13762", "submitter": "Ioannis Kontoyiannis", "authors": "Ioannis Kontoyiannis and Lawrence Tray", "title": "Inferring community characteristics in labelled networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Labelled networks form a very common and important class of data, naturally\nappearing in numerous applications in science and engineering. A typical\ninference goal is to determine how the vertex labels(or {\\em features}) affect\nthe network's graph structure. A standard approach has been to partition the\nnetwork into blocks grouped by distinct values of the feature of interest. A\nblock-based random graph model -- typically a variant of the stochastic block\nmodel -- is then used to test for evidence of asymmetric behaviour within these\nfeature-based communities. Nevertheless, the resulting communities often do not\nproduce a natural partition of the graph. In this work, we introduce a new\ngenerative model, the feature-first block model (FFBM), which is more effective\nat describing vertex-labelled undirected graphs and also facilitates the use of\nricher queries on labelled networks. We develop a Bayesian framework for\ninference with this model, and we present a method to efficiently sample from\nthe posterior distribution of the FFBM parameters. The FFBM's structure is kept\ndeliberately simple to retain easy interpretability of the parameter values. We\napply the proposed methods to a variety of network data to extract the most\nimportant features along which the vertices are partitioned. The main\nadvantages of the proposed approach are that the whole feature-space is used\nautomatically, and features can be rank-ordered implicitly according to impact.\nAny features that do not significantly impact the high-level structure can be\ndiscarded to reduce the problem dimension. In cases where the vertex features\navailable do not readily explain the community structure in the resulting\nnetwork, the approach detects this and is protected against over-fitting.\nResults on several real-world datasets illustrate the performance of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:07:10 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kontoyiannis", "Ioannis", ""], ["Tray", "Lawrence", ""]]}, {"id": "2105.13765", "submitter": "Yasir K{\\i}l{\\i}\\c{c}", "authors": "Yasir Kilic", "title": "Exploiting Transductive Property of Graph Convolutional Neural Networks\n  with Less Labeling Effort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, machine learning approaches on Graph data have become very popular.\nIt was observed that significant results were obtained by including implicit or\nexplicit logical connections between data samples that make up the data to the\nmodel. In this context, the developing GCN model has made significant\nexperimental contributions with Convolution filters applied to graph data. This\nmodel follows Transductive and Semi-Supervised Learning approach. Due to its\ntransductive property, all of the data samples, which is partially labeled, are\ngiven as input to the model. Labeling, which is a cost, is very important.\nWithin the scope of this study, the following research question is tried to be\nanswered: If at least how many samples are labeled, the optimum model success\nis achieved? In addition, some experimental contributions have been made on the\naccuracy of the model, whichever sampling approach is used with fixed labeling\neffort. According to the experiments, the success of the model can be increased\nby using the local centrality metric.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 05:33:31 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Kilic", "Yasir", ""]]}, {"id": "2105.13778", "submitter": "Jan Van Haaren", "authors": "Jan Van Haaren", "title": "\"Why Would I Trust Your Numbers?\" On the Explainability of Expected\n  Values in Soccer", "comments": "Paper accepted for presentation at the AI for Sports Analytics\n  workshop at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many different approaches have been proposed to quantify the\nperformances of soccer players. Since player performances are challenging to\nquantify directly due to the low-scoring nature of soccer, most approaches\nestimate the expected impact of the players' on-the-ball actions on the\nscoreline. While effective, these approaches are yet to be widely embraced by\nsoccer practitioners. The soccer analytics community has primarily focused on\nimproving the accuracy of the models, while the explainability of the produced\nmetrics is often much more important to practitioners.\n  To help bridge the gap between scientists and practitioners, we introduce an\nexplainable Generalized Additive Model that estimates the expected value for\nshots. Unlike existing models, our model leverages features corresponding to\nwidespread soccer concepts. To this end, we represent the locations of shots by\nfuzzily assigning the shots to designated zones on the pitch that practitioners\nare familiar with. Our experimental evaluation shows that our model is as\naccurate as existing models, while being easier to explain to soccer\npractitioners.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:05:00 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Van Haaren", "Jan", ""]]}, {"id": "2105.13783", "submitter": "Carlos Mougan", "authors": "Carlos Mougan, David Masip, Jordi Nin, Oriol Pujol", "title": "Quantile Encoder: Tackling High Cardinality Categorical Features in\n  Regression Problems", "comments": "Accepted at The 18th International Conference on Modeling Decisions\n  for Artificial Intelligence (MDAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression problems have been widely studied in machinelearning literature\nresulting in a plethora of regression models and performance measures. However,\nthere are few techniques specially dedicated to solve the problem of how to\nincorporate categorical features to regression problems. Usually, categorical\nfeature encoders are general enough to cover both classification and regression\nproblems. This lack of specificity results in underperforming regression\nmodels. In this paper,we provide an in-depth analysis of how to tackle high\ncardinality categor-ical features with the quantile. Our proposal outperforms\nstate-of-the-encoders, including the traditional statistical mean target\nencoder, when considering the Mean Absolute Error, especially in the presence\nof long-tailed or skewed distributions. Besides, to deal with possible\noverfitting when there are categories with small support, our encoder benefits\nfrom additive smoothing. Finally, we describe how to expand the encoded values\nby creating a set of features with different quantiles. This expanded encoder\nprovides a more informative output about the categorical feature in question,\nfurther boosting the performance of the regression model.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:56:13 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 18:51:42 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Mougan", "Carlos", ""], ["Masip", "David", ""], ["Nin", "Jordi", ""], ["Pujol", "Oriol", ""]]}, {"id": "2105.13787", "submitter": "Katarzyna Wo\\'znica", "authors": "Katarzyna Wo\\'znica, Katarzyna P\\k{e}kala, Hubert Baniecki, Wojciech\n  Kretowicz, El\\.zbieta Sienkiewicz and Przemys{\\l}aw Biecek", "title": "Do not explain without context: addressing the blind spot of model\n  explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing number of regulations and expectations of predictive machine\nlearning models, such as so called right to explanation, has led to a large\nnumber of methods promising greater interpretability. High demand has led to a\nwidespread adoption of XAI techniques like Shapley values, Partial Dependence\nprofiles or permutational variable importance. However, we still do not know\nenough about their properties and how they manifest in the context in which\nexplanations are created by analysts, reviewed by auditors, and interpreted by\nvarious stakeholders. This paper highlights a blind spot which, although\ncritical, is often overlooked when monitoring and auditing machine learning\nmodels: the effect of the reference data on the explanation calculation. We\ndiscuss that many model explanations depend directly or indirectly on the\nchoice of the referenced data distribution. We showcase examples where small\nchanges in the distribution lead to drastic changes in the explanations, such\nas a change in trend or, alarmingly, a conclusion. Consequently, we postulate\nthat obtaining robust and useful explanations always requires supporting them\nwith a broader context.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:48:40 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Wo\u017anica", "Katarzyna", ""], ["P\u0119kala", "Katarzyna", ""], ["Baniecki", "Hubert", ""], ["Kretowicz", "Wojciech", ""], ["Sienkiewicz", "El\u017cbieta", ""], ["Biecek", "Przemys\u0142aw", ""]]}, {"id": "2105.13789", "submitter": "Maxwell Hogan Mr", "authors": "Maxwell Hogan, Duarte Rondao, Nabil Aouf, and Olivier Dubois-Matra", "title": "Using Convolutional Neural Networks for Relative Pose Estimation of a\n  Non-Cooperative Spacecraft with Thermal Infrared Imagery", "comments": "14 pages; 11 figures; European Space Agency Guidance, Navigation and\n  Control Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent interest in on-orbit servicing and Active Debris Removal (ADR)\nmissions have driven the need for technologies to enable non-cooperative\nrendezvous manoeuvres. Such manoeuvres put heavy burden on the perception\ncapabilities of a chaser spacecraft. This paper demonstrates Convolutional\nNeural Networks (CNNs) capable of providing an initial coarse pose estimation\nof a target from a passive thermal infrared camera feed. Thermal cameras offer\na promising alternative to visible cameras, which struggle in low light\nconditions and are susceptible to overexposure. Often, thermal information on\nthe target is not available a priori; this paper therefore proposes using\nvisible images to train networks. The robustness of the models is demonstrated\non two different targets, first on synthetic data, and then in a laboratory\nenvironment for a realistic scenario that might be faced during an ADR mission.\nGiven that there is much concern over the use of CNN in critical applications\ndue to their black box nature, we use innovative techniques to explain what is\nimportant to our network and fault conditions.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:51:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hogan", "Maxwell", ""], ["Rondao", "Duarte", ""], ["Aouf", "Nabil", ""], ["Dubois-Matra", "Olivier", ""]]}, {"id": "2105.13792", "submitter": "Tianxiang Sun", "authors": "Tianxiang Sun, Yunhua Zhou, Xiangyang Liu, Xinyu Zhang, Hao Jiang,\n  Zhao Cao, Xuanjing Huang, Xipeng Qiu", "title": "Early Exiting with Ensemble Internal Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a simple technique to accelerate inference of large-scale pre-trained\nmodels, early exiting has gained much attention in the NLP community. It allows\nsamples to exit early at internal classifiers without passing through the\nentire model. Most existing work usually trains the internal classifiers\nindependently and employs an exiting strategy to decide whether or not to exit\nbased on the confidence of the current internal classifier. However, none of\nthese works takes full advantage of the fact that the internal classifiers are\ntrained to solve the same task therefore can be used to construct an ensemble.\nIn this paper, we show that a novel objective function for the training of the\nensemble internal classifiers can be naturally induced from the perspective of\nensemble learning and information theory. The proposed training objective\nconsists of two terms: one for accuracy and the other for the diversity of the\ninternal classifiers. In contrast, the objective used in prior work is exactly\nthe accuracy term of our training objective therefore only optimizes the\naccuracy but not diversity. Further, we propose a simple voting-based strategy\nthat considers predictions of all the past internal classifiers to infer the\ncorrect label and decide whether to exit. Experimental results on various NLP\ntasks show that our proposed objective function and voting-based strategy can\nachieve better accuracy-speed trade-offs.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:54:11 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sun", "Tianxiang", ""], ["Zhou", "Yunhua", ""], ["Liu", "Xiangyang", ""], ["Zhang", "Xinyu", ""], ["Jiang", "Hao", ""], ["Cao", "Zhao", ""], ["Huang", "Xuanjing", ""], ["Qiu", "Xipeng", ""]]}, {"id": "2105.13795", "submitter": "Mengying Jiang", "authors": "Mengying Jiang, Guizhong Liu, Yuanchao Su, Xinliang Wu", "title": "GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs\n  under Heterophily", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In representation learning on the graph-structured data, under heterophily\n(or low homophily), many popular GNNs may fail to capture long-range\ndependencies, which leads to their performance degradation. To solve the\nabove-mentioned issue, we propose a graph convolutional networks with structure\nlearning (GCN-SL), and furthermore, the proposed approach can be applied to\nnode classification. The proposed GCN-SL contains two improvements:\ncorresponding to node features and edges, respectively. In the aspect of node\nfeatures, we propose an efficient-spectral-clustering (ESC) and an ESC with\nanchors (ESC-ANCH) algorithms to efficiently aggregate feature representations\nfrom all similar nodes. In the aspect of edges, we build a re-connected\nadjacency matrix by using a special data preprocessing technique and similarity\nlearning, and the re-connected adjacency matrix can be optimized directly along\nwith GCN-SL parameters. Considering that the original adjacency matrix may\nprovide misleading information for aggregation in GCN, especially the graphs\nbeing with a low level of homophily. The proposed GCN-SL can aggregate feature\nrepresentations from nearby nodes via re-connected adjacency matrix and is\napplied to graphs with various levels of homophily. Experimental results on a\nwide range of benchmark datasets illustrate that the proposed GCN-SL\noutperforms the stateof-the-art GNN counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:00:38 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 02:17:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Jiang", "Mengying", ""], ["Liu", "Guizhong", ""], ["Su", "Yuanchao", ""], ["Wu", "Xinliang", ""]]}, {"id": "2105.13797", "submitter": "Guangye Chen", "authors": "Guangye Chen, Luis Chac\\'on, Truong B. Nguyen", "title": "An unsupervised machine-learning checkpoint-restart algorithm using\n  Gaussian mixtures for particle-in-cell simulations", "comments": "Extended abstract for Supercheck21. arXiv admin note: substantial\n  text overlap with arXiv:2007.12273", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised machine-learning checkpoint-restart (CR) lossy\nalgorithm for particle-in-cell (PIC) algorithms using Gaussian mixtures (GM).\nThe algorithm features a particle compression stage and a particle\nreconstruction stage, where a continuum particle distribution function is\nconstructed and resampled, respectively. To guarantee fidelity of the CR\nprocess, we ensure the exact preservation of charge, momentum, and energy for\nboth compression and reconstruction stages, everywhere on the mesh. We also\nensure the preservation of Gauss' law after particle reconstruction. As a\nresult, the GM CR algorithm is shown to provide a clean, conservative restart\ncapability while potentially affording orders of magnitude savings in\ninput/output requirements. We demonstrate the algorithm using a recently\ndeveloped exactly energy- and charge-conserving PIC algorithm on physical\nproblems of interest, with compression factors $\\gtrsim75$ with no appreciable\nimpact on the quality of the restarted dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 01:38:02 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chen", "Guangye", ""], ["Chac\u00f3n", "Luis", ""], ["Nguyen", "Truong B.", ""]]}, {"id": "2105.13802", "submitter": "Neil Zeghidour", "authors": "Neil Zeghidour, Olivier Teboul and David Grangier", "title": "DIVE: End-to-end Speech Diarization via Iterative Speaker Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DIVE, an end-to-end speaker diarization algorithm. Our neural\nalgorithm presents the diarization task as an iterative process: it repeatedly\nbuilds a representation for each speaker before predicting the voice activity\nof each speaker conditioned on the extracted representations. This strategy\nintrinsically resolves the speaker ordering ambiguity without requiring the\nclassical permutation invariant training loss. In contrast with prior work, our\nmodel does not rely on pretrained speaker representations and optimizes all\nparameters of the system with a multi-speaker voice activity loss. Importantly,\nour loss explicitly excludes unreliable speaker turn boundaries from training,\nwhich is adapted to the standard collar-based Diarization Error Rate (DER)\nevaluation. Overall, these contributions yield a system redefining the\nstate-of-the-art on the standard CALLHOME benchmark, with 6.7% DER compared to\n7.8% for the best alternative.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:15:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zeghidour", "Neil", ""], ["Teboul", "Olivier", ""], ["Grangier", "David", ""]]}, {"id": "2105.13807", "submitter": "Shengyi Huang", "authors": "Shengyi Huang, Santiago Onta\\~n\\'on, Chris Bamford, Lukasz Grela", "title": "Gym-$\\mu$RTS: Toward Affordable Full Game Real-time Strategy Games\n  Research with Deep Reinforcement Learning", "comments": "Accepted to IEEE Conference of Games (COG) 2021. See the blog post at\n  https://wandb.ai/vwxyzjn/gym-microrts-paper/reports/Gym-RTS-Toward-Affordable-Deep-Reinforcement-Learning-Research-in-Real-Time-Strategy-Games--Vmlldzo2MDIzMTg\n  and the source code at https://github.com/vwxyzjn/gym-microrts-paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, researchers have achieved great success in applying Deep\nReinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games,\ncreating strong autonomous agents that could defeat professional players in\nStarCraft~II. However, existing approaches to tackle full games have high\ncomputational costs, usually requiring the use of thousands of GPUs and CPUs\nfor weeks. This paper has two main contributions to address this issue: 1) We\nintroduce Gym-$\\mu$RTS (pronounced \"gym-micro-RTS\") as a fast-to-run RL\nenvironment for full-game RTS research and 2) we present a collection of\ntechniques to scale DRL to play full-game $\\mu$RTS as well as ablation studies\nto demonstrate their empirical importance. Our best-trained bot can defeat\nevery $\\mu$RTS bot we tested from the past $\\mu$RTS competitions when working\nin a single-map setting, resulting in a state-of-the-art DRL agent while only\ntaking about 60 hours of training using a single machine (one GPU, three vCPU,\n16GB RAM). See the blog post at\nhttps://wandb.ai/vwxyzjn/gym-microrts-paper/reports/Gym-RTS-Toward-Affordable-Deep-Reinforcement-Learning-Research-in-Real-Time-Strategy-Games--Vmlldzo2MDIzMTg\nand the source code at https://github.com/vwxyzjn/gym-microrts-paper\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:13:35 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 02:45:05 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 15:28:08 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Huang", "Shengyi", ""], ["Onta\u00f1\u00f3n", "Santiago", ""], ["Bamford", "Chris", ""], ["Grela", "Lukasz", ""]]}, {"id": "2105.13809", "submitter": "Xiaofeng Cao", "authors": "Xiaofeng Cao and Ivor W. Tsang", "title": "Distribution Matching for Machine Teaching", "comments": "Black-box Machine Teaching", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine teaching is an inverse problem of machine learning that aims at\nsteering the student learner towards its target hypothesis, in which the\nteacher has already known the student's learning parameters. Previous studies\non machine teaching focused on balancing the teaching risk and cost to find\nthose best teaching examples deriving the student model. This optimization\nsolver is in general ineffective when the student learner does not disclose any\ncue of the learning parameters. To supervise such a teaching scenario, this\npaper presents a distribution matching-based machine teaching strategy.\nSpecifically, this strategy backwardly and iteratively performs the halving\noperation on the teaching cost to find a desired teaching set. Technically, our\nstrategy can be expressed as a cost-controlled optimization process that finds\nthe optimal teaching examples without further exploring in the parameter\ndistribution of the student learner. Then, given any a limited teaching cost,\nthe training examples will be closed-form. Theoretical analysis and experiment\nresults demonstrate this strategy.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 09:32:57 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Cao", "Xiaofeng", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "2105.13810", "submitter": "Benjamin Maschler", "authors": "Benjamin Lindemann, Benjamin Maschler, Nada Sahlab, and Michael\n  Weyrich", "title": "A Survey on Anomaly Detection for Technical Systems using LSTM Networks", "comments": "14 pages, 6 figures, 4 tables. Accepted for publication by Computers\n  in Industry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Anomalies represent deviations from the intended system operation and can\nlead to decreased efficiency as well as partial or complete system failure. As\nthe causes of anomalies are often unknown due to complex system dynamics,\nefficient anomaly detection is necessary. Conventional detection approaches\nrely on statistical and time-invariant methods that fail to address the complex\nand dynamic nature of anomalies. With advances in artificial intelligence and\nincreasing importance for anomaly detection and prevention in various domains,\nartificial neural network approaches enable the detection of more complex\nanomaly types while considering temporal and contextual characteristics. In\nthis article, a survey on state-of-the-art anomaly detection using deep neural\nand especially long short-term memory networks is conducted. The investigated\napproaches are evaluated based on the application scenario, data and anomaly\ntypes as well as further metrics. To highlight the potential of upcoming\nanomaly detection techniques, graph-based and transfer learning approaches are\nalso included in the survey, enabling the analysis of heterogeneous data as\nwell as compensating for its shortage and improving the handling of dynamic\nprocesses.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:24:40 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lindemann", "Benjamin", ""], ["Maschler", "Benjamin", ""], ["Sahlab", "Nada", ""], ["Weyrich", "Michael", ""]]}, {"id": "2105.13813", "submitter": "Daniel Pitchforth", "authors": "Daniel J Pitchforth, Timothy J Rogers, Ulf T Tygesen, Elizabeth J\n  Cross", "title": "Grey-box models for wave loading prediction", "comments": null, "journal-ref": "Mechanical Systems and Signal Processing, Volume 159, 2021", "doi": "10.1016/j.ymssp.2021.107741", "report-no": null, "categories": "cs.LG eess.SP physics.flu-dyn", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The quantification of wave loading on offshore structures and components is a\ncrucial element in the assessment of their useful remaining life. In many\napplications the well-known Morison's equation is employed to estimate the\nforcing from waves with assumed particle velocities and accelerations. This\npaper develops a grey-box modelling approach to improve the predictions of the\nforce on structural members. A grey-box model intends to exploit the enhanced\npredictive capabilities of data-based modelling whilst retaining physical\ninsight into the behaviour of the system; in the context of the work carried\nout here, this can be considered as physics-informed machine learning. There\nare a number of possible approaches to establish a grey-box model. This paper\ndemonstrates two means of combining physics (white box) and data-based (black\nbox) components; one where the model is a simple summation of the two\ncomponents, the second where the white-box prediction is fed into the black box\nas an additional input. Here Morison's equation is used as the physics-based\ncomponent in combination with a data-based Gaussian process NARX - a dynamic\nvariant of the more well-known Gaussian process regression. Two key challenges\nwith employing the GP-NARX formulation that are addressed here are the\nselection of appropriate lag terms and the proper treatment of uncertainty\npropagation within the dynamic GP. The best performing grey-box model, the\nresidual modelling GP-NARX, was able to achieve a 29.13\\% and 5.48\\% relative\nreduction in NMSE over Morison's Equation and a black-box GP-NARX respectively,\nalongside significant benefits in extrapolative capabilities of the model, in\ncircumstances of low dataset coverage.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:10:33 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 15:15:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Pitchforth", "Daniel J", ""], ["Rogers", "Timothy J", ""], ["Tygesen", "Ulf T", ""], ["Cross", "Elizabeth J", ""]]}, {"id": "2105.13817", "submitter": "Marco Scutari", "authors": "Marco Scutari and Manuel Proissl", "title": "Achieving Fairness with a Simple Ridge Penalty", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a fair linear regression model subject to a user-defined level of\nfairness can be achieved by solving a non-convex quadratic programming\noptimisation problem with quadratic constraints. In this work we propose an\nalternative, more flexible approach to this task that enforces a user-defined\nlevel of fairness by means of a ridge penalty. Our proposal addresses three\nlimitations of the former approach: it produces regression coefficient\nestimates that are more intuitive to interpret; it is mathematically simpler,\nwith a solution that is partly in closed form; and it is easier to extend\nbeyond linear regression. We evaluate both approaches empirically on five\ndifferent data sets, and we find that our proposal provides better goodness of\nfit and better predictive accuracy while being equally effective at achieving\nthe desired fairness level. In addition we highlight a source of bias in the\noriginal experimental evaluation of the non-convex quadratic approach, and we\ndiscuss how our proposal can be extended to a wide range of models.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:43:57 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Scutari", "Marco", ""], ["Proissl", "Manuel", ""]]}, {"id": "2105.13831", "submitter": "Fan Wu", "authors": "Fan Wu and Patrick Rebeschini", "title": "Implicit Regularization in Matrix Sensing via Mirror Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study discrete-time mirror descent applied to the unregularized empirical\nrisk in matrix sensing. In both the general case of rectangular matrices and\nthe particular case of positive semidefinite matrices, a simple potential-based\nanalysis in terms of the Bregman divergence allows us to establish convergence\nof mirror descent -- with different choices of the mirror maps -- to a matrix\nthat, among all global minimizers of the empirical risk, minimizes a quantity\nexplicitly related to the nuclear norm, the Frobenius norm, and the von Neumann\nentropy. In both cases, this characterization implies that mirror descent, a\nfirst-order algorithm minimizing the unregularized empirical risk, recovers\nlow-rank matrices under the same set of assumptions that are sufficient to\nguarantee recovery for nuclear-norm minimization. When the sensing matrices are\nsymmetric and commute, we show that gradient descent with full-rank factorized\nparametrization is a first-order approximation to mirror descent, in which case\nwe obtain an explicit characterization of the implicit bias of gradient flow as\na by-product.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:46:47 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Wu", "Fan", ""], ["Rebeschini", "Patrick", ""]]}, {"id": "2105.13841", "submitter": "Huiqi Deng", "authors": "Huiqi Deng, Na Zou, Mengnan Du, Weifu Chen, Guocan Feng, Xia Hu", "title": "A General Taylor Framework for Unifying and Revisiting Attribution\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribution methods provide an insight into the decision-making process of\nmachine learning models, especially deep neural networks, by assigning\ncontribution scores to each individual feature. However, the attribution\nproblem has not been well-defined, which lacks a unified guideline to the\ncontribution assignment process. Furthermore, existing attribution methods\noften built upon various empirical intuitions and heuristics. There still lacks\na general theoretical framework that not only can offer a good description of\nthe attribution problem, but also can be applied to unifying and revisiting\nexisting attribution methods. To bridge the gap, in this paper, we propose a\nTaylor attribution framework, which models the attribution problem as how to\ndecide individual payoffs in a coalition. Then, we reformulate fourteen\nmainstream attribution methods into the Taylor framework and analyze these\nattribution methods in terms of rationale, fidelity, and limitation in the\nframework. Moreover, we establish three principles for a good attribution in\nthe Taylor attribution framework, i.e., low approximation error, correct Taylor\ncontribution assignment, and unbiased baseline selection. Finally, we\nempirically validate the Taylor reformulations and reveal a positive\ncorrelation between the attribution performance and the number of principles\nfollowed by the attribution method via benchmarking on real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:57:16 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Deng", "Huiqi", ""], ["Zou", "Na", ""], ["Du", "Mengnan", ""], ["Chen", "Weifu", ""], ["Feng", "Guocan", ""], ["Hu", "Xia", ""]]}, {"id": "2105.13843", "submitter": "Weiyu Guo", "authors": "Weiyu Guo, Zhijiang Yang, Shu Wu, Fu Chen", "title": "Explainable Enterprise Credit Rating via Deep Feature Crossing Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the powerful learning ability on high-rank and non-linear features,\ndeep neural networks (DNNs) are being applied to data mining and machine\nlearning in various fields, and exhibit higher discrimination performance than\nconventional methods. However, the applications based on DNNs are rare in\nenterprise credit rating tasks because most of DNNs employ the \"end-to-end\"\nlearning paradigm, which outputs the high-rank representations of objects and\npredictive results without any explanations. Thus, users in the financial\nindustry cannot understand how these high-rank representations are generated,\nwhat do they mean and what relations exist with the raw inputs. Then users\ncannot determine whether the predictions provided by DNNs are reliable, and not\ntrust the predictions providing by such \"black box\" models. Therefore, in this\npaper, we propose a novel network to explicitly model the enterprise credit\nrating problem using DNNs and attention mechanisms. The proposed model realizes\nexplainable enterprise credit ratings. Experimental results obtained on\nreal-world enterprise datasets verify that the proposed approach achieves\nhigher performance than conventional methods, and provides insights into\nindividual rating results and the reliability of model training.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 02:41:50 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Guo", "Weiyu", ""], ["Yang", "Zhijiang", ""], ["Wu", "Shu", ""], ["Chen", "Fu", ""]]}, {"id": "2105.13850", "submitter": "Michael Kirchhof", "authors": "Michael Kirchhof and Lena Schmid and Christopher Reining and Michael\n  ten Hompel and Markus Pauly", "title": "pRSL: Interpretable Multi-label Stacking by Learning Probabilistic Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key task in multi-label classification is modeling the structure between\nthe involved classes. Modeling this structure by probabilistic and\ninterpretable means enables application in a broad variety of tasks such as\nzero-shot learning or learning from incomplete data. In this paper, we present\nthe probabilistic rule stacking learner (pRSL) which uses probabilistic\npropositional logic rules and belief propagation to combine the predictions of\nseveral underlying classifiers. We derive algorithms for exact and approximate\ninference and learning, and show that pRSL reaches state-of-the-art performance\non various benchmark datasets.\n  In the process, we introduce a novel multicategorical generalization of the\nnoisy-or gate. Additionally, we report simulation results on the quality of\nloopy belief propagation algorithms for approximate inference in bipartite\nnoisy-or networks.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:06:21 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kirchhof", "Michael", ""], ["Schmid", "Lena", ""], ["Reining", "Christopher", ""], ["Hompel", "Michael ten", ""], ["Pauly", "Markus", ""]]}, {"id": "2105.13854", "submitter": "Andriy Temko Dr", "authors": "Alison O'Shea, Gordon Lightbody, Geraldine Boylan, Andriy Temko", "title": "Neonatal seizure detection from raw multi-channel EEG using a fully\n  convolutional architecture", "comments": null, "journal-ref": "Neural Networks (2020)", "doi": "10.1016/j.neunet.2019.11.023", "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A deep learning classifier for detecting seizures in neonates is proposed.\nThis architecture is designed to detect seizure events from raw\nelectroencephalogram (EEG) signals as opposed to the state-of-the-art hand\nengineered feature-based representation employed in traditional machine\nlearning based solutions. The seizure detection system utilises only\nconvolutional layers in order to process the multichannel time domain signal\nand is designed to exploit the large amount of weakly labelled data in the\ntraining stage. The system performance is assessed on a large database of\ncontinuous EEG recordings of 834h in duration; this is further validated on a\nheld-out publicly available dataset and compared with two baseline SVM based\nsystems.\n  The developed system achieves a 56% relative improvement with respect to a\nfeature-based state-of-the art baseline, reaching an AUC of 98.5%; this also\ncompares favourably both in terms of performance and run-time. The effect of\nvarying architectural parameters is thoroughly studied. The performance\nimprovement is achieved through novel architecture design which allows more\nefficient usage of available training data and end-to-end optimisation from the\nfront-end feature extraction to the back-end classification. The proposed\narchitecture opens new avenues for the application of deep learning to neonatal\nEEG, where the performance becomes a function of the amount of training data\nwith less dependency on the availability of precise clinical labels.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:08:36 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["O'Shea", "Alison", ""], ["Lightbody", "Gordon", ""], ["Boylan", "Geraldine", ""], ["Temko", "Andriy", ""]]}, {"id": "2105.13859", "submitter": "Vinicius Luiz Santos Silva", "authors": "Vinicius L. S. Silva, Claire E. Heaney, Christopher C. Pain", "title": "GAN for time series prediction, data assimilation and uncertainty\n  quantification", "comments": "arXiv admin note: text overlap with arXiv:2105.07729", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method in which a generative adversarial network (GAN) is\nused to quantify the uncertainty of forward simulations in the presence of\nobserved data. Previously, a method has been developed which enables GANs to\nmake time series predictions and data assimilation by training a GAN with\nunconditional simulations of a high-fidelity numerical model. After training,\nthe GAN can be used to predict the evolution of the spatial distribution of the\nsimulation states and observed data is assimilated. In this paper, we describe\nthe process required in order to quantify uncertainty, during which no\nadditional simulations of the high-fidelity numerical model are required. These\nmethods take advantage of the adjoint-like capabilities of generative models\nand the ability to simulate forwards and backwards in time. Set within a\nreduced-order model framework for efficiency, we apply these methods to a\ncompartmental model in epidemiology to predict the spread of COVID-19 in an\nidealised town. The results show that the proposed method can efficiently\nquantify uncertainty in the presence of measurements using only unconditional\nsimulations of the high-fidelity numerical model.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:12:45 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 15:05:18 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Silva", "Vinicius L. S.", ""], ["Heaney", "Claire E.", ""], ["Pain", "Christopher C.", ""]]}, {"id": "2105.13864", "submitter": "Ziyu Jia", "authors": "Ziyu Jia, Youfang Lin, Jing Wang, Xuehui Wang, Peiyi Xie and Yingbin\n  Zhang", "title": "SalientSleepNet: Multimodal Salient Wave Detection Network for Sleep\n  Staging", "comments": "Accepted by IJCAI 2021. The SOLE copyright holder is IJCAI\n  (International Joint Conferences on Artificial Intelligence), all rights\n  reserved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep staging is fundamental for sleep assessment and disease diagnosis.\nAlthough previous attempts to classify sleep stages have achieved high\nclassification performance, several challenges remain open: 1) How to\neffectively extract salient waves in multimodal sleep data; 2) How to capture\nthe multi-scale transition rules among sleep stages; 3) How to adaptively seize\nthe key role of specific modality for sleep staging. To address these\nchallenges, we propose SalientSleepNet, a multimodal salient wave detection\nnetwork for sleep staging. Specifically, SalientSleepNet is a temporal fully\nconvolutional network based on the $\\rm U^2$-Net architecture that is\noriginally proposed for salient object detection in computer vision. It is\nmainly composed of two independent $\\rm U^2$-like streams to extract the\nsalient features from multimodal data, respectively. Meanwhile, the multi-scale\nextraction module is designed to capture multi-scale transition rules among\nsleep stages. Besides, the multimodal attention module is proposed to\nadaptively capture valuable information from multimodal data for the specific\nsleep stage. Experiments on the two datasets demonstrate that SalientSleepNet\noutperforms the state-of-the-art baselines. It is worth noting that this model\nhas the least amount of parameters compared with the existing deep neural\nnetwork models.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:32:09 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Jia", "Ziyu", ""], ["Lin", "Youfang", ""], ["Wang", "Jing", ""], ["Wang", "Xuehui", ""], ["Xie", "Peiyi", ""], ["Zhang", "Yingbin", ""]]}, {"id": "2105.13880", "submitter": "Yujia Qin", "authors": "Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang,\n  Yusheng Su, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou", "title": "Knowledge Inheritance for Pre-trained Language Models", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent explorations of large-scale pre-trained language models (PLMs) such as\nGPT-3 have revealed the power of PLMs with huge amounts of parameters, setting\noff a wave of training ever-larger PLMs. However, training a large-scale PLM\nrequires tremendous amounts of computational resources, which is time-consuming\nand expensive. In addition, existing large-scale PLMs are mainly trained from\nscratch individually, ignoring the availability of many existing well-trained\nPLMs. To this end, we explore the question that how can previously trained PLMs\nbenefit training larger PLMs in future. Specifically, we introduce a novel\npre-training framework named \"knowledge inheritance\" (KI), which combines both\nself-learning and teacher-guided learning to efficiently train larger PLMs.\nSufficient experimental results demonstrate the feasibility of our KI\nframework. We also conduct empirical analyses to explore the effects of teacher\nPLMs' pre-training settings, including model architecture, pre-training data,\netc. Finally, we show that KI can well support lifelong learning and knowledge\ntransfer.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:43:26 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Qin", "Yujia", ""], ["Lin", "Yankai", ""], ["Yi", "Jing", ""], ["Zhang", "Jiajie", ""], ["Han", "Xu", ""], ["Zhang", "Zhengyan", ""], ["Su", "Yusheng", ""], ["Liu", "Zhiyuan", ""], ["Li", "Peng", ""], ["Sun", "Maosong", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.13889", "submitter": "Aur\\'elien Decelle", "authors": "Aur\\'elien Decelle, Cyril Furtlehner, Beatriz Seoane", "title": "Equilibrium and non-Equilibrium regimes in the learning of Restricted\n  Boltzmann Machines", "comments": "12 page, 3 figures. submitted to Neurips 2021 Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Restricted Boltzmann Machines (RBMs) has been challenging for a long\ntime due to the difficulty of computing precisely the log-likelihood gradient.\nOver the past decades, many works have proposed more or less successful\ntraining recipes but without studying the crucial quantity of the problem: the\nmixing time i.e. the number of Monte Carlo iterations needed to sample new\nconfigurations from a model. In this work, we show that this mixing time plays\na crucial role in the dynamics and stability of the trained model, and that\nRBMs operate in two well-defined regimes, namely equilibrium and\nout-of-equilibrium, depending on the interplay between this mixing time of the\nmodel and the number of steps, $k$, used to approximate the gradient. We\nfurther show empirically that this mixing time increases with the learning,\nwhich often implies a transition from one regime to another as soon as $k$\nbecomes smaller than this time. In particular, we show that using the popular\n$k$ (persistent) contrastive divergence approaches, with $k$ small, the\ndynamics of the learned model are extremely slow and often dominated by strong\nout-of-equilibrium effects. On the contrary, RBMs trained in equilibrium\ndisplay faster dynamics, and a smooth convergence to dataset-like\nconfigurations during the sampling. Finally we discuss how to exploit in\npractice both regimes depending on the task one aims to fulfill: (i) short $k$s\ncan be used to generate convincing samples in short times, (ii) large $k$ (or\nincreasingly large) must be used to learn the correct equilibrium distribution\nof the RBM.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:52:11 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 09:57:55 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Decelle", "Aur\u00e9lien", ""], ["Furtlehner", "Cyril", ""], ["Seoane", "Beatriz", ""]]}, {"id": "2105.13890", "submitter": "Yukuan Yang", "authors": "Yukuan Yang, Xiaowei Chi, Lei Deng, Tianyi Yan, Feng Gao, Guoqi Li", "title": "Towards Efficient Full 8-bit Integer DNN Online Training on\n  Resource-limited Devices without Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Huge computational costs brought by convolution and batch normalization (BN)\nhave caused great challenges for the online training and corresponding\napplications of deep neural networks (DNNs), especially in resource-limited\ndevices. Existing works only focus on the convolution or BN acceleration and no\nsolution can alleviate both problems with satisfactory performance. Online\ntraining has gradually become a trend in resource-limited devices like mobile\nphones while there is still no complete technical scheme with acceptable model\nperformance, processing speed, and computational cost. In this research, an\nefficient online-training quantization framework termed EOQ is proposed by\ncombining Fixup initialization and a novel quantization scheme for DNN model\ncompression and acceleration. Based on the proposed framework, we have\nsuccessfully realized full 8-bit integer network training and removed BN in\nlarge-scale DNNs. Especially, weight updates are quantized to 8-bit integers\nfor the first time. Theoretical analyses of EOQ utilizing Fixup initialization\nfor removing BN have been further given using a novel Block Dynamical Isometry\ntheory with weaker assumptions. Benefiting from rational quantization\nstrategies and the absence of BN, the full 8-bit networks based on EOQ can\nachieve state-of-the-art accuracy and immense advantages in computational cost\nand processing speed. What is more, the design of deep learning chips can be\nprofoundly simplified for the absence of unfriendly square root operations in\nBN. Beyond this, EOQ has been evidenced to be more advantageous in small-batch\nonline training with fewer batch samples. In summary, the EOQ framework is\nspecially designed for reducing the high cost of convolution and BN in network\ntraining, demonstrating a broad application prospect of online training in\nresource-limited devices.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:58:04 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yang", "Yukuan", ""], ["Chi", "Xiaowei", ""], ["Deng", "Lei", ""], ["Yan", "Tianyi", ""], ["Gao", "Feng", ""], ["Li", "Guoqi", ""]]}, {"id": "2105.13892", "submitter": "Kai Fong Ernest Chong", "authors": "Jingyi Xu, Tony Q. S. Quek, Kai Fong Ernest Chong", "title": "Training Classifiers that are Universally Robust to All Label Noise\n  Levels", "comments": "IJCNN 2021 paper, 8 pages, 3 figures. Code available:\n  https://github.com/Xu-Jingyi/PUDistill", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For classification tasks, deep neural networks are prone to overfitting in\nthe presence of label noise. Although existing methods are able to alleviate\nthis problem at low noise levels, they encounter significant performance\nreduction at high noise levels, or even at medium noise levels when the label\nnoise is asymmetric. To train classifiers that are universally robust to all\nnoise levels, and that are not sensitive to any variation in the noise model,\nwe propose a distillation-based framework that incorporates a new subcategory\nof Positive-Unlabeled learning. In particular, we shall assume that a small\nsubset of any given noisy dataset is known to have correct labels, which we\ntreat as \"positive\", while the remaining noisy subset is treated as\n\"unlabeled\". Our framework consists of the following two components: (1) We\nshall generate, via iterative updates, an augmented clean subset with\nadditional reliable \"positive\" samples filtered from \"unlabeled\" samples; (2)\nWe shall train a teacher model on this larger augmented clean set. With the\nguidance of the teacher model, we then train a student model on the whole\ndataset. Experiments were conducted on the CIFAR-10 dataset with synthetic\nlabel noise at multiple noise levels for both symmetric and asymmetric noise.\nThe results show that our framework generally outperforms at medium to high\nnoise levels. We also evaluated our framework on Clothing1M, a real-world noisy\ndataset, and we achieved 2.94% improvement in accuracy over existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:49:31 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Xu", "Jingyi", ""], ["Quek", "Tony Q. S.", ""], ["Chong", "Kai Fong Ernest", ""]]}, {"id": "2105.13898", "submitter": "Jaydip Sen", "authors": "Jaydip Sen, Sidra Mehtab, Abhishek Dutta", "title": "Volatility Modeling of Stocks from Selected Sectors of the Indian\n  Economy Using GARCH", "comments": "This paper is the accepted version of our paper in the IEEE Asian\n  Conference on Innovation Technology (IEEE ASIANCON'2021) which will be\n  organized in Pune, INDIA during August 28 - 29, 2021. The paper consists of 8\n  pages and it contains 13 figures and 22 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volatility clustering is an important characteristic that has a significant\neffect on the behavior of stock markets. However, designing robust models for\naccurate prediction of future volatilities of stock prices is a very\nchallenging research problem. We present several volatility models based on\ngeneralized autoregressive conditional heteroscedasticity (GARCH) framework for\nmodeling the volatility of ten stocks listed in the national stock exchange\n(NSE) of India. The stocks are selected from the auto sector and the banking\nsector of the Indian economy, and they have a significant impact on the\nsectoral index of their respective sectors in the NSE. The historical stock\nprice records from Jan 1, 2010, to Apr 30, 2021, are scraped from the Yahoo\nFinance website using the DataReader API of the Pandas module in the Python\nprogramming language. The GARCH modules are built and fine-tuned on the\ntraining data and then tested on the out-of-sample data to evaluate the\nperformance of the models. The analysis of the results shows that asymmetric\nGARCH models yield more accurate forecasts on the future volatility of stocks.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:59:40 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sen", "Jaydip", ""], ["Mehtab", "Sidra", ""], ["Dutta", "Abhishek", ""]]}, {"id": "2105.13901", "submitter": "Leonardo Antonio Ayala Menjivar", "authors": "Leonardo Ayala, Sebastian Wirkert, Anant Vemuri, Tim Adler, Silvia\n  Seidlitz, Sebastian Pirmann, Christina Engels, Dogu Teber, Lena Maier-Hein", "title": "Video-rate multispectral imaging in laparoscopic surgery: First-in-human\n  application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multispectral and hyperspectral imaging (MSI/HSI) can provide clinically\nrelevant information on morphological and functional tissue properties.\nApplication in the operating room (OR), however, has so far been limited by\ncomplex hardware setups and slow acquisition times. To overcome these\nlimitations, we propose a novel imaging system for video-rate spectral imaging\nin the clinical workflow. The system integrates a small snapshot multispectral\ncamera with a standard laparoscope and a clinically commonly used light source,\nenabling the recording of multispectral images with a spectral dimension of 16\nat a frame rate of 25 Hz. An ongoing in patient study shows that multispectral\nrecordings from this system can help detect perfusion changes in partial\nnephrectomy surgery, thus opening the doors to a wide range of clinical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:03:54 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ayala", "Leonardo", ""], ["Wirkert", "Sebastian", ""], ["Vemuri", "Anant", ""], ["Adler", "Tim", ""], ["Seidlitz", "Silvia", ""], ["Pirmann", "Sebastian", ""], ["Engels", "Christina", ""], ["Teber", "Dogu", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2105.13904", "submitter": "Mohammed Elbtity", "authors": "Mohammed Elbtity, Abhishek Singh, Brendan Reidy, Xiaochen Guo, Ramtin\n  Zand", "title": "An In-Memory Analog Computing Co-Processor for Energy-Efficient CNN\n  Inference on Mobile Devices", "comments": "6 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2012.02695, arXiv:2006.01238", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an in-memory analog computing (IMAC) architecture\nrealizing both synaptic behavior and activation functions within non-volatile\nmemory arrays. Spin-orbit torque magnetoresistive random-access memory\n(SOT-MRAM) devices are leveraged to realize sigmoidal neurons as well as\nbinarized synapses. First, it is shown the proposed IMAC architecture can be\nutilized to realize a multilayer perceptron (MLP) classifier achieving orders\nof magnitude performance improvement compared to previous mixed-signal and\ndigital implementations. Next, a heterogeneous mixed-signal and mixed-precision\nCPU-IMAC architecture is proposed for convolutional neural networks (CNNs)\ninference on mobile processors, in which IMAC is designed as a co-processor to\nrealize fully-connected (FC) layers whereas convolution layers are executed in\nCPU. Architecture-level analytical models are developed to evaluate the\nperformance and energy consumption of the CPU-IMAC architecture. Simulation\nresults exhibit 6.5% and 10% energy savings for CPU-IMAC based realizations of\nLeNet and VGG CNN models, for MNIST and CIFAR-10 pattern recognition tasks,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 23:01:36 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Elbtity", "Mohammed", ""], ["Singh", "Abhishek", ""], ["Reidy", "Brendan", ""], ["Guo", "Xiaochen", ""], ["Zand", "Ramtin", ""]]}, {"id": "2105.13905", "submitter": "Jinhui Yuan", "authors": "Jinhui Yuan and Fei Pan and Chunting Zhou and Tao Qin and Tie-Yan Liu", "title": "Learning Structures for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on the unsupervised setting for structure learning of\ndeep neural networks and propose to adopt the efficient coding principle,\nrooted in information theory and developed in computational neuroscience, to\nguide the procedure of structure learning without label information. This\nprinciple suggests that a good network structure should maximize the mutual\ninformation between inputs and outputs, or equivalently maximize the entropy of\noutputs under mild assumptions. We further establish connections between this\nprinciple and the theory of Bayesian optimal classification, and empirically\nverify that larger entropy of the outputs of a deep neural network indeed\ncorresponds to a better classification accuracy. Then as an implementation of\nthe principle, we show that sparse coding can effectively maximize the entropy\nof the output signals, and accordingly design an algorithm based on global\ngroup sparse coding to automatically learn the inter-layer connection and\ndetermine the depth of a neural network. Our experiments on a public image\nclassification dataset demonstrate that using the structure learned from\nscratch by our proposed algorithm, one can achieve a classification accuracy\ncomparable to the best expert-designed structure (i.e., convolutional neural\nnetworks (CNN)). In addition, our proposed algorithm successfully discovers the\nlocal connectivity (corresponding to local receptive fields in CNN) and\ninvariance structure (corresponding to pulling in CNN), as well as achieves a\ngood tradeoff between marginal performance gain and network depth.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:27:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yuan", "Jinhui", ""], ["Pan", "Fei", ""], ["Zhou", "Chunting", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2105.13913", "submitter": "Mathieu Besan\\c{c}on", "authors": "Alejandro Carderera and Mathieu Besan\\c{c}on and Sebastian Pokutta", "title": "Simple steps are all you need: Frank-Wolfe and generalized\n  self-concordant functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized self-concordance is a key property present in the objective\nfunction of many important learning problems. We establish the convergence rate\nof a simple Frank-Wolfe variant that uses the open-loop step size strategy\n$\\gamma_t = 2/(t+2)$, obtaining a $\\mathcal{O}(1/t)$ convergence rate for this\nclass of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the\niteration count. This avoids the use of second-order information or the need to\nestimate local smoothness parameters of previous work. We also show improved\nconvergence rates for various common cases, e.g., when the feasible region\nunder consideration is uniformly convex or polyhedral.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:26:36 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 07:18:01 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Carderera", "Alejandro", ""], ["Besan\u00e7on", "Mathieu", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "2105.13921", "submitter": "Oleg Smirnov", "authors": "Oleg Smirnov", "title": "TensorFlow RiemOpt: a library for optimization on Riemannian manifolds", "comments": "The library code is available at\n  https://github.com/master/tensorflow-riemopt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The adoption of neural networks and deep learning in non-Euclidean domains\nhas been hindered until recently by the lack of scalable and efficient learning\nframeworks. Existing toolboxes in this space were mainly motivated by research\nand education use cases, whereas practical aspects, such as deploying and\nmaintaining machine learning models, were often overlooked.\n  We attempt to bridge this gap by proposing TensorFlow RiemOpt, a Python\nlibrary for optimization on Riemannian manifolds in TensorFlow. The library is\ndesigned with the aim for a seamless integration with the TensorFlow ecosystem,\ntargeting not only research, but also streamlining production machine learning\npipelines.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:42:09 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 18:50:43 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Smirnov", "Oleg", ""]]}, {"id": "2105.13922", "submitter": "Mihaela Rosca", "authors": "Mihaela Rosca and Yan Wu and Benoit Dherin and David G. T. Barrett", "title": "Discretization Drift in Two-Player Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradient-based methods for two-player games produce rich dynamics that can\nsolve challenging problems, yet can be difficult to stabilize and understand.\nPart of this complexity originates from the discrete update steps given by\nsimultaneous or alternating gradient descent, which causes each player to drift\naway from the continuous gradient flow -- a phenomenon we call discretization\ndrift. Using backward error analysis, we derive modified continuous dynamical\nsystems that closely follow the discrete dynamics. These modified dynamics\nprovide an insight into the notorious challenges associated with zero-sum\ngames, including Generative Adversarial Networks. In particular, we identify\ndistinct components of the discretization drift that can alter performance and\nin some cases destabilize the game. Finally, quantifying discretization drift\nallows us to identify regularizers that explicitly cancel harmful forms of\ndrift or strengthen beneficial forms of drift, and thus improve performance of\nGAN training.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:38:34 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 18:26:15 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Rosca", "Mihaela", ""], ["Wu", "Yan", ""], ["Dherin", "Benoit", ""], ["Barrett", "David G. T.", ""]]}, {"id": "2105.13926", "submitter": "Oscar Carlsson", "authors": "Jan E. Gerken, Jimmy Aronsson, Oscar Carlsson, Hampus Linander,\n  Fredrik Ohlsson, Christoffer Petersson, Daniel Persson", "title": "Geometric Deep Learning and Equivariant Neural Networks", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey the mathematical foundations of geometric deep learning, focusing\non group equivariant and gauge equivariant neural networks. We develop gauge\nequivariant convolutional neural networks on arbitrary manifolds $\\mathcal{M}$\nusing principal bundles with structure group $K$ and equivariant maps between\nsections of associated vector bundles. We also discuss group equivariant neural\nnetworks for homogeneous spaces $\\mathcal{M}=G/K$, which are instead\nequivariant with respect to the global symmetry $G$ on $\\mathcal{M}$. Group\nequivariant layers can be interpreted as intertwiners between induced\nrepresentations of $G$, and we show their relation to gauge equivariant\nconvolutional layers. We analyze several applications of this formalism,\nincluding semantic segmentation and object detection networks. We also discuss\nthe case of spherical networks in great detail, corresponding to the case\n$\\mathcal{M}=S^2=\\mathrm{SO}(3)/\\mathrm{SO}(2)$. Here we emphasize the use of\nFourier analysis involving Wigner matrices, spherical harmonics and\nClebsch-Gordan coefficients for $G=\\mathrm{SO}(3)$, illustrating the power of\nrepresentation theory for deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:41:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Gerken", "Jan E.", ""], ["Aronsson", "Jimmy", ""], ["Carlsson", "Oscar", ""], ["Linander", "Hampus", ""], ["Ohlsson", "Fredrik", ""], ["Petersson", "Christoffer", ""], ["Persson", "Daniel", ""]]}, {"id": "2105.13928", "submitter": "Guang Chen", "authors": "Guang Chen", "title": "DMInet: An Accurate and Highly Flexible Deep Learning Framework for Drug\n  Discovery with Membrane Selectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drug membrane interaction is a very significant bioprocess to consider in\ndrug discovery. Here, we propose a novel deep learning framework coined DMInet\nto study drug-membrane interactions that leverages large-scale Martini\ncoarse-grained molecular simulations of permeation of drug-like molecules\nacross six different lipid membranes. The network of DMInet receives three\ninputs, viz, the drug-like molecule, membrane type and spatial distance across\nmembrane thickness, and predicts the potential of mean force with structural\nresolution across the lipid membrane and membrane selectivity. Inheriting from\ncoarse-grained Martini representation of organic molecules and combined with\ndeep learning, DMInet has the potential for more accelerated high throughput\nscreening in drug discovery across a much larger chemical space than that can\nbe explored by physics-based simulations alone. Moreover, DMInet is highly\nflexible in its nature and holds the possibilities for other properties\nprediction without significant change of the architecture. Last but not least,\nthe architecture of DMInet is general and can be applied to other membrane\nproblems involving permeation and selection.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:45:27 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chen", "Guang", ""]]}, {"id": "2105.13929", "submitter": "Fan Mo", "authors": "Fan Mo, Anastasia Borovykh, Mohammad Malekzadeh, Hamed Haddadi,\n  Soteris Demetriou", "title": "Quantifying Information Leakage from Gradients", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing deep neural networks' gradients instead of training data could\nfacilitate data privacy in collaborative learning. In practice however,\ngradients can disclose both private latent attributes and original data.\nMathematical metrics are needed to quantify both original and latent\ninformation leakages from gradients computed over the training data. In this\nwork, we first use an adaptation of the empirical $\\mathcal{V}$-information to\npresent an information-theoretic justification for the attack success rates in\na layer-wise manner. We then move towards a deeper understanding of gradient\nleakages and propose more general and efficient metrics, using sensitivity and\nsubspace distance to quantify the gradient changes w.r.t. original and latent\ninformation, respectively. Our empirical results, on six datasets and four\nmodels, reveal that gradients of the first layers contain the highest amount of\noriginal information, while the classifier/fully-connected layers placed after\nthe feature extractor contain the highest latent information. Further, we show\nhow training hyperparameters such as gradient aggregation can decrease\ninformation leakages. Our characterization provides a new understanding on\ngradient-based information leakages using the gradients' sensitivity w.r.t.\nchanges in private information, and portends possible defenses such as\nlayer-based protection or strong aggregation.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:47:44 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Mo", "Fan", ""], ["Borovykh", "Anastasia", ""], ["Malekzadeh", "Mohammad", ""], ["Haddadi", "Hamed", ""], ["Demetriou", "Soteris", ""]]}, {"id": "2105.13937", "submitter": "Sotirios Sabanis", "authors": "Dong-Young Lim and Sotirios Sabanis", "title": "Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient\n  adaptive algorithms for neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new class of adaptive stochastic optimization algorithms, which\novercomes many of the known shortcomings of popular adaptive optimizers that\nare currently used for the fine tuning of artificial neural networks (ANNs).\nIts underpinning theory relies on advances of Euler's polygonal approximations\nfor stochastic differential equations (SDEs) with monotone coefficients. As a\nresult, it inherits the stability properties of tamed algorithms, while it\naddresses other known issues, e.g. vanishing gradients in ANNs. In particular,\nwe provide an nonasymptotic analysis and full theoretical guarantees for the\nconvergence properties of an algorithm of this novel class, which we named\nTH$\\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments\nare presented with different types of ANNs, which show the superior performance\nof TheoPouLa over many popular adaptive optimization algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:58:48 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lim", "Dong-Young", ""], ["Sabanis", "Sotirios", ""]]}, {"id": "2105.13939", "submitter": "Christophe Roux", "authors": "Christophe Roux, Elias Wirth, Sebastian Pokutta, Thomas Kerdreux", "title": "Efficient Online-Bandit Strategies for Minimax Learning Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several learning problems involve solving min-max problems, e.g., empirical\ndistributional robust learning or learning with non-standard aggregated losses.\nMore specifically, these problems are convex-linear problems where the\nminimization is carried out over the model parameters $w\\in\\mathcal{W}$ and the\nmaximization over the empirical distribution $p\\in\\mathcal{K}$ of the training\nset indexes, where $\\mathcal{K}$ is the simplex or a subset of it. To design\nefficient methods, we let an online learning algorithm play against a\n(combinatorial) bandit algorithm. We argue that the efficiency of such\napproaches critically depends on the structure of $\\mathcal{K}$ and propose two\nproperties of $\\mathcal{K}$ that facilitate designing efficient algorithms. We\nfocus on a specific family of sets $\\mathcal{S}_{n,k}$ encompassing various\nlearning applications and provide high-probability convergence guarantees to\nthe minimax values.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:01:42 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 14:38:53 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Roux", "Christophe", ""], ["Wirth", "Elias", ""], ["Pokutta", "Sebastian", ""], ["Kerdreux", "Thomas", ""]]}, {"id": "2105.13942", "submitter": "Joachim Schreurs", "authors": "Joachim Schreurs, Micha\\\"el Fanuel and Johan A.K. Suykens", "title": "Towards Deterministic Diverse Subset Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are well known models for diverse subset\nselection problems, including recommendation tasks, document summarization and\nimage search. In this paper, we discuss a greedy deterministic adaptation of\nk-DPP. Deterministic algorithms are interesting for many applications, as they\nprovide interpretability to the user by having no failure probability and\nalways returning the same results. First, the ability of the method to yield\nlow-rank approximations of kernel matrices is evaluated by comparing the\naccuracy of the Nystr\\\"om approximation on multiple datasets. Afterwards, we\ndemonstrate the usefulness of the model on an image search task.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:05:58 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Schreurs", "Joachim", ""], ["Fanuel", "Micha\u00ebl", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "2105.13945", "submitter": "Andrew Blance", "authors": "Steve Abel, Andrew Blance and Michael Spannowsky", "title": "Quantum Optimisation of Complex Systems with a Quantum Annealer", "comments": "24 pages, 19 figures, V3 (fixed typo on page 5)", "journal-ref": null, "doi": null, "report-no": "IPPP/20/106", "categories": "quant-ph cond-mat.stat-mech cs.LG hep-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform an in-depth comparison of quantum annealing with several classical\noptimisation techniques, namely thermal annealing, Nelder-Mead, and gradient\ndescent. We begin with a direct study of the 2D Ising model on a quantum\nannealer, and compare its properties directly with those of the thermal 2D\nIsing model. These properties include an Ising-like phase transition that can\nbe induced by either a change in 'quantum-ness' of the theory, or by a scaling\nthe Ising couplings up or down. This behaviour is in accord with what is\nexpected from the physical understanding of the quantum system. We then go on\nto demonstrate the efficacy of the quantum annealer at minimising several\nincreasingly hard two dimensional potentials. For all the potentials we find\nthe general behaviour that Nelder-Mead and gradient descent methods are very\nsusceptible to becoming trapped in false minima, while the thermal anneal\nmethod is somewhat better at discovering the true minimum. However, and despite\ncurrent limitations on its size, the quantum annealer performs a minimisation\nvery markedly better than any of these classical techniques. A quantum anneal\ncan be designed so that the system almost never gets trapped in a false\nminimum, and rapidly and successfully minimises the potentials.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:08:48 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 11:58:57 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 18:48:26 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Abel", "Steve", ""], ["Blance", "Andrew", ""], ["Spannowsky", "Michael", ""]]}, {"id": "2105.13949", "submitter": "Joachim Schreurs", "authors": "David Winant, Joachim Schreurs and Johan A.K. Suykens", "title": "Latent Space Exploration Using Generative Kernel PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel PCA is a powerful feature extractor which recently has seen a\nreformulation in the context of Restricted Kernel Machines (RKMs). These RKMs\nallow for a representation of kernel PCA in terms of hidden and visible units\nsimilar to Restricted Boltzmann Machines. This connection has led to insights\non how to use kernel PCA in a generative procedure, called generative kernel\nPCA. In this paper, the use of generative kernel PCA for exploring latent\nspaces of datasets is investigated. New points can be generated by gradually\nmoving in the latent space, which allows for an interpretation of the\ncomponents. Firstly, examples of this feature space exploration on three\ndatasets are shown with one of them leading to an interpretable representation\nof ECG signals. Afterwards, the use of the tool in combination with novelty\ndetection is shown, where the latent space around novel patterns in the data is\nexplored. This helps in the interpretation of why certain points are considered\nas novel.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:17:37 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Winant", "David", ""], ["Schreurs", "Joachim", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "2105.13954", "submitter": "Mirai Tanaka Dr.", "authors": "Ryo Sato, Mirai Tanaka, Akiko Takeda", "title": "A Gradient Method for Multilevel Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although application examples of multilevel optimization have already been\ndiscussed since the '90s, the development of solution methods was almost\nlimited to bilevel cases due to the difficulty of the problem. In recent years,\nin machine learning, Franceschi et al. have proposed a method for solving\nbilevel optimization problems by replacing their lower-level problems with the\n$T$ steepest descent update equations with some prechosen iteration number $T$.\nIn this paper, we have developed a gradient-based algorithm for multilevel\noptimization with $n$ levels based on their idea and proved that our\nreformulation with $n T$ variables asymptotically converges to the original\nmultilevel problem. As far as we know, this is one of the first algorithms with\nsome theoretical guarantee for multilevel optimization. Numerical experiments\nshow that a trilevel hyperparameter learning model considering data poisoning\nproduces more stable prediction results than an existing bilevel hyperparameter\nlearning model in noisy data settings.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:22:10 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sato", "Ryo", ""], ["Tanaka", "Mirai", ""], ["Takeda", "Akiko", ""]]}, {"id": "2105.13967", "submitter": "Zhengchun Liu", "authors": "Zhengchun Liu, Ahsan Ali, Peter Kenesei, Antonino Miceli, Hemant\n  Sharma, Nicholas Schwarz, Dennis Trujillo, Hyunseung Yoo, Ryan Coffee, Ryan\n  Herbst, Jana Thayer, Chun Hong Yoon, Ian Foster", "title": "Bridge Data Center AI Systems with Edge Computing for Actionable\n  Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extremely high data rates at modern synchrotron and X-ray free-electron\nlasers (XFELs) light source beamlines motivate the use of machine learning\nmethods for data reduction, feature detection, and other purposes. Regardless\nof the application, the basic concept is the same: data collected in early\nstages of an experiment, data from past similar experiments, and/or data\nsimulated for the upcoming experiment are used to train machine learning models\nthat, in effect, learn specific characteristics of those data; these models are\nthen used to process subsequent data more efficiently than would\ngeneral-purpose models that lack knowledge of the specific dataset or data\nclass. Thus, a key challenge is to be able to train models with sufficient\nrapidity that they can be deployed and used within useful timescales. We\ndescribe here how specialized data center AI systems can be used for this\npurpose.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:47:01 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Liu", "Zhengchun", ""], ["Ali", "Ahsan", ""], ["Kenesei", "Peter", ""], ["Miceli", "Antonino", ""], ["Sharma", "Hemant", ""], ["Schwarz", "Nicholas", ""], ["Trujillo", "Dennis", ""], ["Yoo", "Hyunseung", ""], ["Coffee", "Ryan", ""], ["Herbst", "Ryan", ""], ["Thayer", "Jana", ""], ["Yoon", "Chun Hong", ""], ["Foster", "Ian", ""]]}, {"id": "2105.13975", "submitter": "Veronika Thost", "authors": "Arthur Feeney and Rishabh Gupta and Veronika Thost and Rico Angell and\n  Gayathri Chandu and Yash Adhikari and Tengfei Ma", "title": "Relation Matters in Sampling: A Scalable Multi-Relational Graph Neural\n  Network for Drug-Drug Interaction Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is an established technique to scale graph neural networks to large\ngraphs. Current approaches however assume the graphs to be homogeneous in terms\nof relations and ignore relation types, critically important in biomedical\ngraphs. Multi-relational graphs contain various types of relations that usually\ncome with variable frequency and have different importance for the problem at\nhand. We propose an approach to modeling the importance of relation types for\nneighborhood sampling in graph neural networks and show that we can learn the\nright balance: relation-type probabilities that reflect both frequency and\nimportance. Our experiments on drug-drug interaction prediction show that\nstate-of-the-art graph neural networks profit from relation-dependent sampling\nin terms of both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:55:09 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Feeney", "Arthur", ""], ["Gupta", "Rishabh", ""], ["Thost", "Veronika", ""], ["Angell", "Rico", ""], ["Chandu", "Gayathri", ""], ["Adhikari", "Yash", ""], ["Ma", "Tengfei", ""]]}, {"id": "2105.13977", "submitter": "Vudtiwat Ngampruetikorn", "authors": "Vudtiwat Ngampruetikorn, David J. Schwab", "title": "Perturbation Theory for the Information Bottleneck", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT physics.data-an", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting relevant information from data is crucial for all forms of\nlearning. The information bottleneck (IB) method formalizes this, offering a\nmathematically precise and conceptually appealing framework for understanding\nlearning phenomena. However the nonlinearity of the IB problem makes it\ncomputationally expensive and analytically intractable in general. Here we\nderive a perturbation theory for the IB method and report the first complete\ncharacterization of the learning onset, the limit of maximum relevant\ninformation per bit extracted from data. We test our results on synthetic\nprobability distributions, finding good agreement with the exact numerical\nsolution near the onset of learning. We explore the difference and subtleties\nin our derivation and previous attempts at deriving a perturbation theory for\nthe learning onset and attribute the discrepancy to a flawed assumption. Our\nwork also provides a fresh perspective on the intimate relationship between the\nIB method and the strong data processing inequality.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:59:01 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ngampruetikorn", "Vudtiwat", ""], ["Schwab", "David J.", ""]]}, {"id": "2105.13984", "submitter": "Pierce Burke", "authors": "Pierce Burke and Richard Klein", "title": "Confident in the Crowd: Bayesian Inference to Improve Data Labelling in\n  Crowdsourcing", "comments": "6 pages, 4 figures", "journal-ref": "2020 International SAUPEC/RobMech/PRASA Conference, 2020, pp. 1-6", "doi": "10.1109/SAUPEC/RobMech/PRASA48453.2020.9041099", "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased interest in machine learning and big data problems, the\nneed for large amounts of labelled data has also grown. However, it is often\ninfeasible to get experts to label all of this data, which leads many\npractitioners to crowdsourcing solutions. In this paper, we present new\ntechniques to improve the quality of the labels while attempting to reduce the\ncost. The naive approach to assigning labels is to adopt a majority vote\nmethod, however, in the context of data labelling, this is not always ideal as\ndata labellers are not equally reliable. One might, instead, give higher\npriority to certain labellers through some kind of weighted vote based on past\nperformance. This paper investigates the use of more sophisticated methods,\nsuch as Bayesian inference, to measure the performance of the labellers as well\nas the confidence of each label. The methods we propose follow an iterative\nimprovement algorithm which attempts to use the least amount of workers\nnecessary to achieve the desired confidence in the inferred label. This paper\nexplores simulated binary classification problems with simulated workers and\nquestions to test the proposed methods. Our methods outperform the standard\nvoting methods in both cost and accuracy while maintaining higher reliability\nwhen there is disagreement within the crowd.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:09:45 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Burke", "Pierce", ""], ["Klein", "Richard", ""]]}, {"id": "2105.13986", "submitter": "Caleb Bowyer", "authors": "Caleb M. Bowyer", "title": "Improving Generalization in Mountain Car Through the Partitioned\n  Parameterized Policy Approach via Quasi-Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The reinforcement learning problem of finding a control policy that minimizes\nthe minimum time objective for the Mountain Car environment is considered.\nParticularly, a class of parameterized nonlinear feedback policies is optimized\nover to reach the top of the highest mountain peak in minimum time. The\noptimization is carried out using quasi-Stochastic Gradient Descent (qSGD)\nmethods. In attempting to find the optimal minimum time policy, a new\nparameterized policy approach is considered that seeks to learn an optimal\npolicy parameter for different regions of the state space, rather than rely on\na single macroscopic policy parameter for the entire state space. This\npartitioned parameterized policy approach is shown to outperform the uniform\nparameterized policy approach and lead to greater generalization than prior\nmethods, where the Mountain Car became trapped in circular trajectories in the\nstate space.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:11:10 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Bowyer", "Caleb M.", ""]]}, {"id": "2105.13987", "submitter": "Jingzhao Hu", "authors": "Jingzhao Hu, Chen Wang, Qiaomei Jia, Qirong Bu, Jun Feng", "title": "ScalingNet: extracting features from raw EEG data for emotion\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks(CNNs) has achieved remarkable performance\nbreakthrough in a variety of tasks. Recently, CNNs based methods that are fed\nwith hand-extracted EEG features gradually produce a powerful performance on\nthe EEG data based emotion recognition task. In this paper, we propose a novel\nconvolutional layer allowing to adaptively extract effective data-driven\nspectrogram-like features from raw EEG signals, which we reference as scaling\nlayer. Further, it leverages convolutional kernels scaled from one data-driven\npattern to exposed a frequency-like dimension to address the shortcomings of\nprior methods requiring hand-extracted features or their approximations. The\nproposed neural network architecture based on the scaling layer, references as\nScalingNet, has achieved the state-of-the-art result across the established\nDEAP benchmark dataset.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 08:54:27 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hu", "Jingzhao", ""], ["Wang", "Chen", ""], ["Jia", "Qiaomei", ""], ["Bu", "Qirong", ""], ["Feng", "Jun", ""]]}, {"id": "2105.13988", "submitter": "Emanuele Guidotti", "authors": "Emanuele Guidotti, Alfio Ferrara", "title": "An Explainable Probabilistic Classifier for Categorical Data Inspired to\n  Quantum Physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents Sparse Tensor Classifier (STC), a supervised\nclassification algorithm for categorical data inspired by the notion of\nsuperposition of states in quantum physics. By regarding an observation as a\nsuperposition of features, we introduce the concept of wave-particle duality in\nmachine learning and propose a generalized framework that unifies the classical\nand the quantum probability. We show that STC possesses a wide range of\ndesirable properties not available in most other machine learning methods but\nit is at the same time exceptionally easy to comprehend and use. Empirical\nevaluation of STC on structured data and text classification demonstrates that\nour methodology achieves state-of-the-art performances compared to both\nstandard classifiers and deep learning, at the additional benefit of requiring\nminimal data pre-processing and hyper-parameter tuning. Moreover, STC provides\na native explanation of its predictions both for single instances and for each\ntarget label globally.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:41:30 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Guidotti", "Emanuele", ""], ["Ferrara", "Alfio", ""]]}, {"id": "2105.13993", "submitter": "Xuzhe Zhang", "authors": "Xuzhe Zhang, Xinzi He, Jia Guo, Nabil Ettehadi, Natalie Aw, David\n  Semanek, Jonathan Posner, Andrew Laine, Yun Wang", "title": "PTNet: A High-Resolution Infant MRI Synthesizer Based on Transformer", "comments": "arXiv Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) noninvasively provides critical information\nabout how human brain structures develop across stages of life. Developmental\nscientists are particularly interested in the first few years of\nneurodevelopment. Despite the success of MRI collection and analysis for\nadults, it is a challenge for researchers to collect high-quality multimodal\nMRIs from developing infants mainly because of their irregular sleep pattern,\nlimited attention, inability to follow instructions to stay still, and a lack\nof analysis approaches. These challenges often lead to a significant reduction\nof usable data. To address this issue, researchers have explored various\nsolutions to replace corrupted scans through synthesizing realistic MRIs. Among\nthem, the convolution neural network (CNN) based generative adversarial network\nhas demonstrated promising results and achieves state-of-the-art performance.\nHowever, adversarial training is unstable and may need careful tuning of\nregularization terms to stabilize the training. In this study, we introduced a\nnovel MRI synthesis framework - Pyramid Transformer Net (PTNet). PTNet consists\nof transformer layers, skip-connections, and multi-scale pyramid\nrepresentation. Compared with the most widely used CNN-based conditional GAN\nmodels (namely pix2pix and pix2pixHD), our model PTNet shows superior\nperformance in terms of synthesis accuracy and model size. Notably, PTNet does\nnot require any type of adversarial training and can be easily trained using\nthe simple mean squared error loss.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:20:19 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Xuzhe", ""], ["He", "Xinzi", ""], ["Guo", "Jia", ""], ["Ettehadi", "Nabil", ""], ["Aw", "Natalie", ""], ["Semanek", "David", ""], ["Posner", "Jonathan", ""], ["Laine", "Andrew", ""], ["Wang", "Yun", ""]]}, {"id": "2105.14005", "submitter": "Fabiana Zollo", "authors": "Matteo Cinelli, Andra\\v{z} Pelicon, Igor Mozeti\\v{c}, Walter\n  Quattrociocchi, Petra Kralj Novak, Fabiana Zollo", "title": "Online Hate: Behavioural Dynamics and Relationship with Misinformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Online debates are often characterised by extreme polarisation and heated\ndiscussions among users. The presence of hate speech online is becoming\nincreasingly problematic, making necessary the development of appropriate\ncountermeasures. In this work, we perform hate speech detection on a corpus of\nmore than one million comments on YouTube videos through a machine learning\nmodel fine-tuned on a large set of hand-annotated data. Our analysis shows that\nthere is no evidence of the presence of \"serial haters\", intended as active\nusers posting exclusively hateful comments. Moreover, coherently with the echo\nchamber hypothesis, we find that users skewed towards one of the two categories\nof video channels (questionable, reliable) are more prone to use inappropriate,\nviolent, or hateful language within their opponents community. Interestingly,\nusers loyal to reliable sources use on average a more toxic language than their\ncounterpart. Finally, we find that the overall toxicity of the discussion\nincreases with its length, measured both in terms of number of comments and\ntime. Our results show that, coherently with Godwin's law, online debates tend\nto degenerate towards increasingly toxic exchanges of views.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:30:51 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Cinelli", "Matteo", ""], ["Pelicon", "Andra\u017e", ""], ["Mozeti\u010d", "Igor", ""], ["Quattrociocchi", "Walter", ""], ["Novak", "Petra Kralj", ""], ["Zollo", "Fabiana", ""]]}, {"id": "2105.14016", "submitter": "Bingyan Wang", "authors": "Bingyan Wang, Yuling Yan, Jianqing Fan", "title": "Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs\n  with a Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The curse of dimensionality is a widely known issue in reinforcement learning\n(RL). In the tabular setting where the state space $\\mathcal{S}$ and the action\nspace $\\mathcal{A}$ are both finite, to obtain a nearly optimal policy with\nsampling access to a generative model, the minimax optimal sample complexity\nscales linearly with $|\\mathcal{S}|\\times|\\mathcal{A}|$, which can be\nprohibitively large when $\\mathcal{S}$ or $\\mathcal{A}$ is large. This paper\nconsiders a Markov decision process (MDP) that admits a set of state-action\nfeatures, which can linearly express (or approximate) its probability\ntransition kernel. We show that a model-based approach (resp.$~$Q-learning)\nprovably learns an $\\varepsilon$-optimal policy (resp.$~$Q-function) with high\nprobability as soon as the sample size exceeds the order of\n$\\frac{K}{(1-\\gamma)^{3}\\varepsilon^{2}}$\n(resp.$~$$\\frac{K}{(1-\\gamma)^{4}\\varepsilon^{2}}$), up to some logarithmic\nfactor. Here $K$ is the feature dimension and $\\gamma\\in(0,1)$ is the discount\nfactor of the MDP. Both sample complexity bounds are provably tight, and our\nresult for the model-based approach matches the minimax lower bound. Our\nresults show that for arbitrarily large-scale MDP, both the model-based\napproach and Q-learning are sample-efficient when $K$ is relatively small, and\nhence the title of this paper.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:49:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Wang", "Bingyan", ""], ["Yan", "Yuling", ""], ["Fan", "Jianqing", ""]]}, {"id": "2105.14024", "submitter": "Scott Sussex", "authors": "Scott Sussex (1), Andreas Krause (1), Caroline Uhler (2) ((1)\n  Department of Computer Science, ETH Z\\\"urich, (2) Laboratory for Information\n  & Decision Systems, Massachusetts Institute of Technology)", "title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure\n  Learning", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal structure learning is a key problem in many domains. Causal structures\ncan be learnt by performing experiments on the system of interest. We address\nthe largely unexplored problem of designing experiments that simultaneously\nintervene on multiple variables. While potentially more informative than the\ncommonly considered single-variable interventions, selecting such interventions\nis algorithmically much more challenging, due to the doubly-exponential\ncombinatorial search space over sets of composite interventions. In this paper,\nwe develop efficient algorithms for optimizing different objective functions\nquantifying the informativeness of experiments. By establishing novel\nsubmodularity properties of these objectives, we provide approximation\nguarantees for our algorithms. Our algorithms empirically perform superior to\nboth random interventions and algorithms that only select single-variable\ninterventions.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:00:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sussex", "Scott", ""], ["Krause", "Andreas", ""], ["Uhler", "Caroline", ""]]}, {"id": "2105.14033", "submitter": "Heng Yang", "authors": "Heng Yang, Ling Liang, Kim-Chuan Toh, Luca Carlone", "title": "STRIDE along Spectrahedral Vertices for Solving Large-Scale Rank-One\n  Semidefinite Relaxations", "comments": "9 pages main context, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider solving high-order semidefinite programming (SDP) relaxations of\nnonconvex polynomial optimization problems (POPs) that admit rank-one optimal\nsolutions. Existing approaches, which solve the SDP independently from the POP,\neither cannot scale to large problems or suffer from slow convergence due to\nthe typical degeneracy of such SDPs. We propose a new algorithmic framework,\ncalled SpecTrahedral pRoximal gradIent Descent along vErtices (STRIDE), that\nblends fast local search on the nonconvex POP with global descent on the convex\nSDP. Specifically, STRIDE follows a globally convergent trajectory driven by a\nproximal gradient method (PGM) for solving the SDP, while simultaneously\nprobing long, but safeguarded, rank-one \"strides\", generated by fast nonlinear\nprogramming algorithms on the POP, to seek rapid descent. We prove STRIDE has\nglobal convergence. To solve the subproblem of projecting a given point onto\nthe feasible set of the SDP, we reformulate the projection step as a\ncontinuously differentiable unconstrained optimization and apply a\nlimited-memory BFGS method to achieve both scalability and accuracy. We conduct\nnumerical experiments on solving second-order SDP relaxations arising from two\nimportant applications in machine learning and computer vision. STRIDE\ndominates a diverse set of five existing SDP solvers and is the only solver\nthat can solve degenerate rank-one SDPs to high accuracy (e.g., KKT residuals\nbelow 1e-9), even in the presence of millions of equality constraints.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:07:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yang", "Heng", ""], ["Liang", "Ling", ""], ["Toh", "Kim-Chuan", ""], ["Carlone", "Luca", ""]]}, {"id": "2105.14035", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang and Johannes Lederer", "title": "DeepMoM: Robust Deep Learning With Median-of-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data used in deep learning is notoriously problematic. For example, data are\nusually combined from diverse sources, rarely cleaned and vetted thoroughly,\nand sometimes corrupted on purpose. Intentional corruption that targets the\nweak spots of algorithms has been studied extensively under the label of\n\"adversarial attacks.\" In contrast, the arguably much more common case of\ncorruption that reflects the limited quality of data has been studied much\nless. Such \"random\" corruptions are due to measurement errors, unreliable\nsources, convenience sampling, and so forth. These kinds of corruption are\ncommon in deep learning, because data are rarely collected according to strict\nprotocols -- in strong contrast to the formalized data collection in some parts\nof classical statistics. This paper concerns such corruption. We introduce an\napproach motivated by very recent insights into median-of-means and Le Cam's\nprinciple, we show that the approach can be readily implemented, and we\ndemonstrate that it performs very well in practice. In conclusion, we believe\nthat our approach is a very promising alternative to standard parameter\ntraining based on least-squares and cross-entropy loss.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:07:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Shih-Ting", ""], ["Lederer", "Johannes", ""]]}, {"id": "2105.14038", "submitter": "Xuechen Li", "authors": "Xuechen Li, Chris J. Maddison, Daniel Tarlow", "title": "Learning to Extend Program Graphs to Work-in-Progress Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Source code spends most of its time in a broken or incomplete state during\nsoftware development. This presents a challenge to machine learning for code,\nsince high-performing models typically rely on graph structured representations\nof programs derived from traditional program analyses. Such analyses may be\nundefined for broken or incomplete code. We extend the notion of program graphs\nto work-in-progress code by learning to predict edge relations between tokens,\ntraining on well-formed code before transferring to work-in-progress code. We\nconsider the tasks of code completion and localizing and repairing variable\nmisuse in a work-in-process scenario. We demonstrate that training\nrelation-aware models with fine-tuned edges consistently leads to improved\nperformance on both tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:12:22 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Xuechen", ""], ["Maddison", "Chris J.", ""], ["Tarlow", "Daniel", ""]]}, {"id": "2105.14039", "submitter": "Andrew Lampinen", "authors": "Andrew Kyle Lampinen, Stephanie C.Y. Chan, Andrea Banino, Felix Hill", "title": "Towards mental time travel: a hierarchical memory for reinforcement\n  learning agents", "comments": "10 pages main text; 22 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning agents often forget details of the past, especially\nafter delays or distractor tasks. Agents with common memory architectures\nstruggle to recall and integrate across multiple timesteps of a past event, or\neven to recall the details of a single timestep that is followed by distractor\ntasks. To address these limitations, we propose a Hierarchical Transformer\nMemory (HTM), which helps agents to remember the past in detail. HTM stores\nmemories by dividing the past into chunks, and recalls by first performing\nhigh-level attention over coarse summaries of the chunks, and then performing\ndetailed attention within only the most relevant chunks. An agent with HTM can\ntherefore \"mentally time-travel\" -- remember past events in detail without\nattending to all intervening events. We show that agents with HTM substantially\noutperform agents with other memory architectures at tasks requiring long-term\nrecall, retention, or reasoning over memory. These include recalling where an\nobject is hidden in a 3D environment, rapidly learning to navigate efficiently\nin a new neighborhood, and rapidly learning and retaining new object names.\nAgents with HTM can extrapolate to task sequences an order of magnitude longer\nthan they were trained on, and can even generalize zero-shot from a\nmeta-learning setting to maintaining knowledge across episodes. HTM improves\nagent sample efficiency, generalization, and generality (by solving tasks that\npreviously required specialized architectures). Our work is a step towards\nagents that can learn, interact, and adapt in complex and temporally-extended\nenvironments.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:12:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lampinen", "Andrew Kyle", ""], ["Chan", "Stephanie C. Y.", ""], ["Banino", "Andrea", ""], ["Hill", "Felix", ""]]}, {"id": "2105.14044", "submitter": "Xavier Gitiaux", "authors": "Xavier Gitiaux, Huzefa Rangwala", "title": "Fair Representations by Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organizations that collect and sell data face increasing scrutiny for the\ndiscriminatory use of data. We propose a novel unsupervised approach to\ntransform data into a compressed binary representation independent of sensitive\nattributes. We show that in an information bottleneck framework, a parsimonious\nrepresentation should filter out information related to sensitive attributes if\nthey are provided directly to the decoder. Empirical results show that the\nproposed method, \\textbf{FBC}, achieves state-of-the-art accuracy-fairness\ntrade-off. Explicit control of the entropy of the representation bit stream\nallows the user to move smoothly and simultaneously along both rate-distortion\nand rate-fairness curves. \\end{abstract}\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:22:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gitiaux", "Xavier", ""], ["Rangwala", "Huzefa", ""]]}, {"id": "2105.14052", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang and Johannes Lederer", "title": "Targeted Deep Learning: Framework, Methods, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning systems are typically designed to perform for a wide range of\ntest inputs. For example, deep learning systems in autonomous cars are supposed\nto deal with traffic situations for which they were not specifically trained.\nIn general, the ability to cope with a broad spectrum of unseen test inputs is\ncalled generalization. Generalization is definitely important in applications\nwhere the possible test inputs are known but plentiful or simply unknown, but\nthere are also cases where the possible inputs are few and unlabeled but known\nbeforehand. For example, medicine is currently interested in targeting\ntreatments to individual patients; the number of patients at any given time is\nusually small (typically one), their diagnoses/responses/... are still unknown,\nbut their general characteristics (such as genome information, protein levels\nin the blood, and so forth) are known before the treatment. We propose to call\ndeep learning in such applications targeted deep learning. In this paper, we\nintroduce a framework for targeted deep learning, and we devise and test an\napproach for adapting standard pipelines to the requirements of targeted deep\nlearning. The approach is very general yet easy to use: it can be implemented\nas a simple data-preprocessing step. We demonstrate on a variety of real-world\ndata that our approach can indeed render standard deep learning faster and more\naccurate when the test inputs are known beforehand.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:37:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Shih-Ting", ""], ["Lederer", "Johannes", ""]]}, {"id": "2105.14058", "submitter": "Francesco Farina", "authors": "Francesco Farina, Emma Slade", "title": "Symmetry-driven graph neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploiting symmetries and invariance in data is a powerful, yet not fully\nexploited, way to achieve better generalisation with more efficiency. In this\npaper, we introduce two graph network architectures that are equivariant to\nseveral types of transformations affecting the node coordinates. First, we\nbuild equivariance to any transformation in the coordinate embeddings that\npreserves the distance between neighbouring nodes, allowing for equivariance to\nthe Euclidean group. Then, we introduce angle attributes to build equivariance\nto any angle preserving transformation - thus, to the conformal group. Thanks\nto their equivariance properties, the proposed models can be vastly more data\nefficient with respect to classical graph architectures, intrinsically equipped\nwith a better inductive bias and better at generalising. We demonstrate these\ncapabilities on a synthetic dataset composed of $n$-dimensional geometric\nobjects. Additionally, we provide examples of their limitations when (the\nright) symmetries are not present in the data.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:54:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Farina", "Francesco", ""], ["Slade", "Emma", ""]]}, {"id": "2105.14070", "submitter": "Mikko Lehtimaki", "authors": "Mikko Lehtim\\\"aki, Lassi Paunonen, Marja-Leena Linne", "title": "Accelerating Neural ODEs Using Model Order Reduction", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding nonlinear dynamical systems into artificial neural networks is a\npowerful new formalism for machine learning. By parameterizing ordinary\ndifferential equations (ODEs) as neural network layers, these Neural ODEs are\nmemory-efficient to train, process time-series naturally and incorporate\nknowledge of physical systems into deep learning models. However, the practical\napplications of Neural ODEs are limited due to long inference times, because\nthe outputs of the embedded ODE layers are computed numerically with\ndifferential equation solvers that can be computationally demanding. Here we\nshow that mathematical model order reduction methods can be used for\ncompressing and accelerating Neural ODEs by accurately simulating the\ncontinuous nonlinear dynamics in low-dimensional subspaces. We implement our\nnovel compression method by developing Neural ODEs that integrate the necessary\nsubspace-projection and interpolation operations as layers of the neural\nnetwork. We validate our model reduction approach by comparing it to two\nestablished acceleration methods from the literature in two classification\nasks. In compressing convolutional and recurrent Neural ODE architectures, we\nachieve the best balance between speed and accuracy when compared to the other\ntwo acceleration methods. Based on our results, our integration of model order\nreduction with Neural ODEs can facilitate efficient, dynamical system-driven\ndeep learning in resource-constrained applications.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:27:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lehtim\u00e4ki", "Mikko", ""], ["Paunonen", "Lassi", ""], ["Linne", "Marja-Leena", ""]]}, {"id": "2105.14071", "submitter": "Soumick Chatterjee", "authors": "Soumick Chatterjee, Faraz Ahmed Nizamani, Andreas N\\\"urnberger and\n  Oliver Speck", "title": "Classification of Brain Tumours in MR Images using Deep Spatiospatial\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain tumour is a mass or cluster of abnormal cells in the brain, which has\nthe possibility of becoming life-threatening because of its ability to invade\nneighbouring tissues and also form metastases. An accurate diagnosis is\nessential for successful treatment planning and magnetic resonance imaging is\nthe principal imaging modality for diagnostic of brain tumours and their\nextent. Deep Learning methods in computer vision applications have shown\nsignificant improvement in recent years, most of which can be credited to the\nfact that a sizeable amount of data is available to train models on, and the\nimprovements in the model architectures yielding better approximations in a\nsupervised setting. Classifying tumours using such deep learning methods has\nmade significant progress with the availability of open datasets with reliable\nannotations. Typically those methods are either 3D models, which use 3D\nvolumetric MRIs or even 2D models considering each slice separately. However,\nby treating the slice spatial dimension separately, spatiotemporal models can\nbe employed as spatiospatial models for this task. These models have the\ncapabilities of learning specific spatial and temporal relationship, while\nreducing computational costs. This paper uses two spatiotemporal models, ResNet\n(2+1)D and ResNet Mixed Convolution, to classify different types of brain\ntumours. It was observed that both these models performed superior to the pure\n3D convolutional model, ResNet18. Furthermore, it was also observed that\npre-training the models on a different, even unrelated dataset before training\nthem for the task of tumour classification improves the performance. Finally,\nPre-trained ResNet Mixed Convolution was observed to be the best model in these\nexperiments, achieving a macro F1-score of 0.93 and a test accuracy of 96.98\\%,\nwhile at the same time being the model with the least computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:27:51 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chatterjee", "Soumick", ""], ["Nizamani", "Faraz Ahmed", ""], ["N\u00fcrnberger", "Andreas", ""], ["Speck", "Oliver", ""]]}, {"id": "2105.14073", "submitter": "Murat Cubuktepe", "authors": "Franck Djeumou, Murat Cubuktepe, Craig Lennon, Ufuk Topcu", "title": "Task-Guided Inverse Reinforcement Learning Under Partial Information", "comments": "Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of inverse reinforcement learning (IRL), where the\nlearning agent recovers a reward function using expert demonstrations. Most of\nthe existing IRL techniques make the often unrealistic assumption that the\nagent has access to full information about the environment. We remove this\nassumption by developing an algorithm for IRL in partially observable Markov\ndecision processes (POMDPs), where an agent cannot directly observe the current\nstate of the POMDP. The algorithm addresses several limitations of existing\ntechniques that do not take the \\emph{information asymmetry} between the expert\nand the agent into account. First, it adopts causal entropy as the measure of\nthe likelihood of the expert demonstrations as opposed to entropy in most\nexisting IRL techniques and avoids a common source of algorithmic complexity.\nSecond, it incorporates task specifications expressed in temporal logic into\nIRL. Such specifications may be interpreted as side information available to\nthe learner a priori in addition to the demonstrations, and may reduce the\ninformation asymmetry between the expert and the agent. Nevertheless, the\nresulting formulation is still nonconvex due to the intrinsic nonconvexity of\nthe so-called \\emph{forward problem}, i.e., computing an optimal policy given a\nreward function, in POMDPs. We address this nonconvexity through sequential\nconvex programming and introduce several extensions to solve the forward\nproblem in a scalable manner. This scalability allows computing policies that\nincorporate memory at the expense of added computational cost yet also achieves\nhigher performance compared to memoryless policies. We demonstrate that, even\nwith severely limited data, the algorithm learns reward functions and policies\nthat satisfy the task and induce a similar behavior to the expert by leveraging\nthe side information and incorporating memory into the policy.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:36:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Djeumou", "Franck", ""], ["Cubuktepe", "Murat", ""], ["Lennon", "Craig", ""], ["Topcu", "Ufuk", ""]]}, {"id": "2105.14074", "submitter": "Rohan Chitnis", "authors": "Rohan Chitnis, Tom Silver, Joshua B. Tenenbaum, Tomas Lozano-Perez,\n  Leslie Pack Kaelbling", "title": "Learning Neuro-Symbolic Relational Transition Models for Bilevel\n  Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent, independent progress in model-based reinforcement learning\nand integrated symbolic-geometric robotic planning, synthesizing these\ntechniques remains challenging because of their disparate assumptions and\nstrengths. In this work, we take a step toward bridging this gap with\nNeuro-Symbolic Relational Transition Models (NSRTs), a novel class of\ntransition models that are data-efficient to learn, compatible with powerful\nrobotic planning methods, and generalizable over objects. NSRTs have both\nsymbolic and neural components, enabling a bilevel planning scheme where\nsymbolic AI planning in an outer loop guides continuous planning with neural\nmodels in an inner loop. Experiments in four robotic planning domains show that\nNSRTs can be learned after only tens or hundreds of training episodes, and then\nused for fast planning in new tasks that require up to 60 actions to reach the\ngoal and involve many more objects than were seen during training. Video:\nhttps://tinyurl.com/chitnis-nsrts\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:37:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chitnis", "Rohan", ""], ["Silver", "Tom", ""], ["Tenenbaum", "Joshua B.", ""], ["Lozano-Perez", "Tomas", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "2105.14077", "submitter": "George Cazenavette V", "authors": "George Cazenavette, Simon Lucey", "title": "On the Bias Against Inductive Biases", "comments": "Under Review at NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Borrowing from the transformer models that revolutionized the field of\nnatural language processing, self-supervised feature learning for visual tasks\nhas also seen state-of-the-art success using these extremely deep, isotropic\nnetworks. However, the typical AI researcher does not have the resources to\nevaluate, let alone train, a model with several billion parameters and\nquadratic self-attention activations. To facilitate further research, it is\nnecessary to understand the features of these huge transformer models that can\nbe adequately studied by the typical researcher. One interesting characteristic\nof these transformer models is that they remove most of the inductive biases\npresent in classical convolutional networks. In this work, we analyze the\neffect of these and more inductive biases on small to moderately-sized\nisotropic networks used for unsupervised visual feature learning and show that\ntheir removal is not always ideal.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:41:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cazenavette", "George", ""], ["Lucey", "Simon", ""]]}, {"id": "2105.14080", "submitter": "Alexia Jolicoeur-Martineau", "authors": "Alexia Jolicoeur-Martineau, Ke Li, R\\'emi Pich\\'e-Taillefer, Tal\n  Kachman, Ioannis Mitliagkas", "title": "Gotta Go Fast When Generating Data with Score-Based Models", "comments": "Code is available on\n  https://github.com/AlexiaJM/score_sde_fast_sampling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-based (denoising diffusion) generative models have recently gained a\nlot of success in generating realistic and diverse data. These approaches\ndefine a forward diffusion process for transforming data to noise and generate\ndata by reversing it (thereby going from noise to data). Unfortunately, current\nscore-based models generate data very slowly due to the sheer number of score\nnetwork evaluations required by numerical SDE solvers.\n  In this work, we aim to accelerate this process by devising a more efficient\nSDE solver. Existing approaches rely on the Euler-Maruyama (EM) solver, which\nuses a fixed step size. We found that naively replacing it with other SDE\nsolvers fares poorly - they either result in low-quality samples or become\nslower than EM. To get around this issue, we carefully devise an SDE solver\nwith adaptive step sizes tailored to score-based generative models piece by\npiece. Our solver requires only two score function evaluations, rarely rejects\nsamples, and leads to high-quality samples. Our approach generates data 2 to 10\ntimes faster than EM while achieving better or equal sample quality. For\nhigh-resolution images, our method leads to significantly higher quality\nsamples than all other methods tested. Our SDE solver has the benefit of\nrequiring no step size tuning.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:48:51 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jolicoeur-Martineau", "Alexia", ""], ["Li", "Ke", ""], ["Pich\u00e9-Taillefer", "R\u00e9mi", ""], ["Kachman", "Tal", ""], ["Mitliagkas", "Ioannis", ""]]}, {"id": "2105.14083", "submitter": "Glenn Dawson", "authors": "Glenn Dawson, Robi Polikar", "title": "Rethinking Noisy Label Models: Labeler-Dependent Noise with Adversarial\n  Awareness", "comments": "9 pages, 3 figures, 3 algorithms. Currently under blind review at\n  NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most studies on learning from noisy labels rely on unrealistic models of\ni.i.d. label noise, such as class-conditional transition matrices. More recent\nwork on instance-dependent noise models are more realistic, but assume a single\ngenerative process for label noise across the entire dataset. We propose a more\nprincipled model of label noise that generalizes instance-dependent noise to\nmultiple labelers, based on the observation that modern datasets are typically\nannotated using distributed crowdsourcing methods. Under our labeler-dependent\nmodel, label noise manifests itself under two modalities: natural error of\ngood-faith labelers, and adversarial labels provided by malicious actors. We\npresent two adversarial attack vectors that more accurately reflect the label\nnoise that may be encountered in real-world settings, and demonstrate that\nunder our multimodal noisy labels model, state-of-the-art approaches for\nlearning from noisy labels are defeated by adversarial label attacks. Finally,\nwe propose a multi-stage, labeler-aware, model-agnostic framework that reliably\nfilters noisy labels by leveraging knowledge about which data partitions were\nlabeled by which labeler, and show that our proposed framework remains robust\neven in the presence of extreme adversarial label noise.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:58:18 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 01:40:37 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dawson", "Glenn", ""], ["Polikar", "Robi", ""]]}, {"id": "2105.14084", "submitter": "Clayton Sanford", "authors": "Navid Ardeshir, Clayton Sanford, Daniel Hsu", "title": "Support vector machines and linear regression coincide with very\n  high-dimensional features", "comments": "32 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The support vector machine (SVM) and minimum Euclidean norm least squares\nregression are two fundamentally different approaches to fitting linear models,\nbut they have recently been connected in models for very high-dimensional data\nthrough a phenomenon of support vector proliferation, where every training\nexample used to fit an SVM becomes a support vector. In this paper, we explore\nthe generality of this phenomenon and make the following contributions. First,\nwe prove a super-linear lower bound on the dimension (in terms of sample size)\nrequired for support vector proliferation in independent feature models,\nmatching the upper bounds from previous works. We further identify a sharp\nphase transition in Gaussian feature models, bound the width of this\ntransition, and give experimental support for its universality. Finally, we\nhypothesize that this phase transition occurs only in much higher-dimensional\nsettings in the $\\ell_1$ variant of the SVM, and we present a new geometric\ncharacterization of the problem that may elucidate this phenomenon for the\ngeneral $\\ell_p$ case.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:06:21 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ardeshir", "Navid", ""], ["Sanford", "Clayton", ""], ["Hsu", "Daniel", ""]]}, {"id": "2105.14094", "submitter": "Justin Dong", "authors": "Mark Ainsworth and Justin Dong", "title": "Galerkin Neural Networks: A Framework for Approximating Variational\n  Equations with Error Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new approach to using neural networks to approximate the\nsolutions of variational equations, based on the adaptive construction of a\nsequence of finite-dimensional subspaces whose basis functions are realizations\nof a sequence of neural networks. The finite-dimensional subspaces are then\nused to define a standard Galerkin approximation of the variational equation.\nThis approach enjoys a number of advantages, including: the sequential nature\nof the algorithm offers a systematic approach to enhancing the accuracy of a\ngiven approximation; the sequential enhancements provide a useful indicator for\nthe error that can be used as a criterion for terminating the sequential\nupdates; the basic approach is largely oblivious to the nature of the partial\ndifferential equation under consideration; and, some basic theoretical results\nare presented regarding the convergence (or otherwise) of the method which are\nused to formulate basic guidelines for applying the method.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:25:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ainsworth", "Mark", ""], ["Dong", "Justin", ""]]}, {"id": "2105.14095", "submitter": "Shuxiao Chen", "authors": "Shuxiao Chen, Koby Crammer, Hangfeng He, Dan Roth, Weijie J. Su", "title": "Weighted Training for Cross-Task Learning", "comments": "21 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted\ntraining algorithm for cross-task learning based on minimizing a\nrepresentation-based task distance between the source and target tasks. We show\nthat TAWT is easy to implement, is computationally efficient, requires little\nhyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees.\nThe effectiveness of TAWT is corroborated through extensive experiments with\nBERT on four sequence tagging tasks in natural language processing (NLP),\nincluding part-of-speech (PoS) tagging, chunking, predicate detection, and\nnamed entity recognition (NER). As a byproduct, the proposed\nrepresentation-based task distance allows one to reason in a theoretically\nprincipled way about several critical aspects of cross-task learning, such as\nthe choice of the source data and the impact of fine-tuning\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:27:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Shuxiao", ""], ["Crammer", "Koby", ""], ["He", "Hangfeng", ""], ["Roth", "Dan", ""], ["Su", "Weijie J.", ""]]}, {"id": "2105.14097", "submitter": "Pawe{\\l} Wawrzy\\'nski", "authors": "Grzegorz Rype\\'s\\'c, {\\L}ukasz Lepak, Pawe{\\l} Wawrzy\\'nski", "title": "Reinforcement Learning for on-line Sequence Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of problems in the processing of sound and natural language, as well\nas in other areas, can be reduced to simultaneously reading an input sequence\nand writing an output sequence of generally different length. There are well\ndeveloped methods that produce the output sequence based on the entirely known\ninput. However, efficient methods that enable such transformations on-line do\nnot exist. In this paper we introduce an architecture that learns with\nreinforcement to make decisions about whether to read a token or write another\ntoken. This architecture is able to transform potentially infinite sequences\non-line. In an experimental study we compare it with state-of-the-art methods\nfor neural machine translation. While it produces slightly worse translations\nthan Transformer, it outperforms the autoencoder with attention, even though\nour architecture translates texts on-line thereby solving a more difficult\nproblem than both reference methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:31:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Rype\u015b\u0107", "Grzegorz", ""], ["Lepak", "\u0141ukasz", ""], ["Wawrzy\u0144ski", "Pawe\u0142", ""]]}, {"id": "2105.14099", "submitter": "Nan Ding", "authors": "Nan Ding, Xi Chen, Tomer Levinboim, Sebastian Goodman, Radu Soricut", "title": "Bridging the Gap Between Practice and PAC-Bayes Theory in Few-Shot\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent advances in its theoretical understanding, there still remains\na significant gap in the ability of existing PAC-Bayesian theories on\nmeta-learning to explain performance improvements in the few-shot learning\nsetting, where the number of training examples in the target tasks is severely\nlimited. This gap originates from an assumption in the existing theories which\nsupposes that the number of training examples in the observed tasks and the\nnumber of training examples in the target tasks follow the same distribution,\nan assumption that rarely holds in practice. By relaxing this assumption, we\ndevelop two PAC-Bayesian bounds tailored for the few-shot learning setting and\nshow that two existing meta-learning algorithms (MAML and Reptile) can be\nderived from our bounds, thereby bridging the gap between practice and\nPAC-Bayesian theories. Furthermore, we derive a new computationally-efficient\nPACMAML algorithm, and show it outperforms existing meta-learning algorithms on\nseveral few-shot benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:40:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ding", "Nan", ""], ["Chen", "Xi", ""], ["Levinboim", "Tomer", ""], ["Goodman", "Sebastian", ""], ["Soricut", "Radu", ""]]}, {"id": "2105.14103", "submitter": "Hanlin Goh", "authors": "Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin\n  Goh, Ruixiang Zhang, Josh Susskind", "title": "An Attention Free Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Attention Free Transformer (AFT), an efficient variant of\nTransformers that eliminates the need for dot product self attention. In an AFT\nlayer, the key and value are first combined with a set of learned position\nbiases, the result of which is multiplied with the query in an element-wise\nfashion. This new operation has a memory complexity linear w.r.t. both the\ncontext size and the dimension of features, making it compatible to both large\ninput and model sizes. We also introduce AFT-local and AFT-conv, two model\nvariants that take advantage of the idea of locality and spatial weight sharing\nwhile maintaining global connectivity. We conduct extensive experiments on two\nautoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image\nrecognition task (ImageNet-1K classification). We show that AFT demonstrates\ncompetitive performance on all the benchmarks, while providing excellent\nefficiency at the same time.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:45:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Talbott", "Walter", ""], ["Srivastava", "Nitish", ""], ["Huang", "Chen", ""], ["Goh", "Hanlin", ""], ["Zhang", "Ruixiang", ""], ["Susskind", "Josh", ""]]}, {"id": "2105.14105", "submitter": "Dominik Schildknecht", "authors": "Dominik Schildknecht, Anastasia N. Popova, Jack Stellwagen, Matt\n  Thomson", "title": "Reinforcement Learning reveals fundamental limits on the mixing of\n  active particles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The control of far-from-equilibrium physical systems, including active\nmaterials, has emerged as an important area for the application of\nreinforcement learning (RL) strategies to derive control policies for physical\nsystems. In active materials, non-linear dynamics and long-range interactions\nbetween particles prohibit closed-form descriptions of the system's dynamics\nand prevent explicit solutions to optimal control problems. Due to fundamental\nchallenges in solving for explicit control strategies, RL has emerged as an\napproach to derive control strategies for far-from-equilibrium active matter\nsystems. However, an important open question is how the mathematical structure\nand the physical properties of the active matter systems determine the\ntractability of RL for learning control policies. In this work, we show that RL\ncan only find good strategies to the canonical active matter task of mixing for\nsystems that combine attractive and repulsive particle interactions. Using\nmathematical results from dynamical systems theory, we relate the availability\nof both interaction types with the existence of hyperbolic dynamics and the\nability of RL to find homogeneous mixing strategies. In particular, we show\nthat for drag-dominated translational-invariant particle systems, hyperbolic\ndynamics and, therefore, mixing requires combining attractive and repulsive\ninteractions. Broadly, our work demonstrates how fundamental physical and\nmathematical properties of dynamical systems can enable or constrain\nreinforcement learning-based control.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:04:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Schildknecht", "Dominik", ""], ["Popova", "Anastasia N.", ""], ["Stellwagen", "Jack", ""], ["Thomson", "Matt", ""]]}, {"id": "2105.14111", "submitter": "Jack Koch", "authors": "Jack Koch, Lauro Langosco, Jacob Pfau, James Le, Lee Sharkey", "title": "Objective Robustness in Deep Reinforcement Learning", "comments": "small revisions, corrected figure for ablation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study objective robustness failures, a type of out-of-distribution\nrobustness failure in reinforcement learning (RL). Objective robustness\nfailures occur when an RL agent retains its capabilities out-of-distribution\nyet pursues the wrong objective. This kind of failure presents different risks\nthan the robustness problems usually considered in the literature, since it\ninvolves agents that leverage their capabilities to pursue the wrong objective\nrather than simply failing to do anything useful. We provide the first explicit\nempirical demonstrations of objective robustness failures and present a partial\ncharacterization of its causes.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:13:34 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 21:48:02 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Koch", "Jack", ""], ["Langosco", "Lauro", ""], ["Pfau", "Jacob", ""], ["Le", "James", ""], ["Sharkey", "Lee", ""]]}, {"id": "2105.14114", "submitter": "Matias M\\\"uller", "authors": "Matias I. M\\\"uller and Cristian R. Rojas", "title": "Asymptotically Optimal Bandits under Weighted Information", "comments": "9 content pages, 3 references pages, 22 appendix pages, 4 figures, 34\n  total pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of regret minimization in a multi-armed bandit setup\nwhere the agent is allowed to play multiple arms at each round by spreading the\nresources usually allocated to only one arm. At each iteration the agent\nselects a normalized power profile and receives a Gaussian vector as outcome,\nwhere the unknown variance of each sample is inversely proportional to the\npower allocated to that arm. The reward corresponds to a linear combination of\nthe power profile and the outcomes, resembling a linear bandit. By spreading\nthe power, the agent can choose to collect information much faster than in a\ntraditional multi-armed bandit at the price of reducing the accuracy of the\nsamples. This setup is fundamentally different from that of a linear bandit --\nthe regret is known to scale as $\\Theta(\\sqrt{T})$ for linear bandits, while in\nthis setup the agent receives a much more detailed feedback, for which we\nderive a tight $\\log(T)$ problem-dependent lower-bound. We propose a\nThompson-Sampling-based strategy, called Weighted Thompson Sampling (\\WTS),\nthat designs the power profile as its posterior belief of each arm being the\nbest arm, and show that its upper bound matches the derived logarithmic lower\nbound. Finally, we apply this strategy to a problem of control and system\nidentification, where the goal is to estimate the maximum gain (also called\n$\\mathcal{H}_\\infty$-norm) of a linear dynamical system based on batches of\ninput-output samples.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:28:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["M\u00fcller", "Matias I.", ""], ["Rojas", "Cristian R.", ""]]}, {"id": "2105.14116", "submitter": "Daniel Steinberg", "authors": "Daniel Steinberg, Paul Munro", "title": "Visualizing Representations of Adversarially Perturbed Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that deep learning models are vulnerable to adversarial\nattacks. We seek to further understand the consequence of such attacks on the\nintermediate activations of neural networks. We present an evaluation metric,\nPOP-N, which scores the effectiveness of projecting data to N dimensions under\nthe context of visualizing representations of adversarially perturbed inputs.\nWe conduct experiments on CIFAR-10 to compare the POP-2 score of several\ndimensionality reduction algorithms across various adversarial attacks.\nFinally, we utilize the 2D data corresponding to high POP-2 scores to generate\nexample visualizations.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:34:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Steinberg", "Daniel", ""], ["Munro", "Paul", ""]]}, {"id": "2105.14119", "submitter": "Adam Kalai", "authors": "Adam Tauman Kalai, Varun Kanade", "title": "Towards optimally abstaining from prediction", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common challenge across all areas of machine learning is that training data\nis not distributed like test data, due to natural shifts, \"blind spots,\" or\nadversarial examples. We consider a model where one may abstain from\npredicting, at a fixed cost. In particular, our transductive abstention\nalgorithm takes labeled training examples and unlabeled test examples as input,\nand provides predictions with optimal prediction loss guarantees. The loss\nbounds match standard generalization bounds when test examples are i.i.d. from\nthe training distribution, but add an additional term that is the cost of\nabstaining times the statistical distance between the train and test\ndistribution (or the fraction of adversarial examples). For linear regression,\nwe give a polynomial-time algorithm based on Celis-Dennis-Tapia optimization\nalgorithms. For binary classification, we show how to efficiently implement it\nusing a proper agnostic learner (i.e., an Empirical Risk Minimizer) for the\nclass of interest. Our work builds on a recent abstention algorithm of\nGoldwasser, Kalais, and Montasser (2020) for transductive binary\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:44:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kalai", "Adam Tauman", ""], ["Kanade", "Varun", ""]]}, {"id": "2105.14125", "submitter": "Vaneet Aggarwal", "authors": "Qinbo Bai and Mridul Agarwal and Vaneet Aggarwal", "title": "Joint Optimization of Multi-Objective Reinforcement Learning with Policy\n  Gradient Based Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many engineering problems have multiple objectives, and the overall aim is to\noptimize a non-linear function of these objectives. In this paper, we formulate\nthe problem of maximizing a non-linear concave function of multiple long-term\nobjectives. A policy-gradient based model-free algorithm is proposed for the\nproblem. To compute an estimate of the gradient, a biased estimator is\nproposed. The proposed algorithm is shown to achieve convergence to within an\n$\\epsilon$ of the global optima after sampling\n$\\mathcal{O}(\\frac{M^4\\sigma^2}{(1-\\gamma)^8\\epsilon^4})$ trajectories where\n$\\gamma$ is the discount factor and $M$ is the number of the agents, thus\nachieving the same dependence on $\\epsilon$ as the policy gradient algorithm\nfor the standard reinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 22:20:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bai", "Qinbo", ""], ["Agarwal", "Mridul", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "2105.14127", "submitter": "Michael Gimelfarb Mr.", "authors": "Michael Gimelfarb, Andr\\'e Barreto, Scott Sanner, Chi-Guhn Lee", "title": "Risk-Aware Transfer in Reinforcement Learning using Successor Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample efficiency and risk-awareness are central to the development of\npractical reinforcement learning (RL) for complex decision-making. The former\ncan be addressed by transfer learning and the latter by optimizing some utility\nfunction of the return. However, the problem of transferring skills in a\nrisk-aware manner is not well-understood. In this paper, we address the problem\nof risk-aware policy transfer between tasks in a common domain that differ only\nin their reward functions, in which risk is measured by the variance of reward\nstreams. Our approach begins by extending the idea of generalized policy\nimprovement to maximize entropic utilities, thus extending policy improvement\nvia dynamic programming to sets of policies and levels of risk-aversion. Next,\nwe extend the idea of successor features (SF), a value function representation\nthat decouples the environment dynamics from the rewards, to capture the\nvariance of returns. Our resulting risk-aware successor features (RaSF)\nintegrate seamlessly within the RL framework, inherit the superior task\ngeneralization ability of SFs, and incorporate risk-awareness into the\ndecision-making. Experiments on a discrete navigation domain and control of a\nsimulated robotic arm demonstrate the ability of RaSFs to outperform\nalternative methods including SFs, when taking the risk of the learned policies\ninto account.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 22:22:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gimelfarb", "Michael", ""], ["Barreto", "Andr\u00e9", ""], ["Sanner", "Scott", ""], ["Lee", "Chi-Guhn", ""]]}, {"id": "2105.14139", "submitter": "Sergey Ketkov", "authors": "Sergey S. Ketkov, Andrei S. Shilov, Oleg A. Prokopyev", "title": "On a class of data-driven combinatorial optimization problems under\n  uncertainty: a distributionally robust approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this study we analyze linear combinatorial optimization problems where the\ncost vector is not known a priori, but is only observable through a finite data\nset. In contrast to the related studies, we presume that the number of\nobservations with respect to particular components of the cost vector may vary.\nThe goal is to find a procedure that transforms the data set into an estimate\nof the expected value of the objective function (which is referred to as a\nprediction rule) and a procedure that retrieves a candidate decision (which is\nreferred to as a prescription rule). We aim at finding the least conservative\nprediction and prescription rules, which satisfy some specified asymptotic\nguarantees. We demonstrate that the resulting vector optimization problems\nadmit a weakly optimal solution, which can be obtained by solving a particular\ndistributionally robust optimization problem. Specifically, the decision-maker\nmay optimize the worst-case expected loss across all probability distributions\nwith given component-wise relative entropy distances from the empirical\nmarginal distributions. Finally, we perform numerical experiments to analyze\nthe out-of-sample performance of the proposed solution approach.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 23:17:35 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 20:32:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ketkov", "Sergey S.", ""], ["Shilov", "Andrei S.", ""], ["Prokopyev", "Oleg A.", ""]]}, {"id": "2105.14141", "submitter": "Alek Dimitriev", "authors": "Alek Dimitriev and Mingyuan Zhou", "title": "ARMS: Antithetic-REINFORCE-Multi-Sample Gradient for Binary Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the gradients for binary variables is a task that arises\nfrequently in various domains, such as training discrete latent variable\nmodels. What has been commonly used is a REINFORCE based Monte Carlo estimation\nmethod that uses either independent samples or pairs of negatively correlated\nsamples. To better utilize more than two samples, we propose ARMS, an\nAntithetic REINFORCE-based Multi-Sample gradient estimator. ARMS uses a copula\nto generate any number of mutually antithetic samples. It is unbiased, has low\nvariance, and generalizes both DisARM, which we show to be ARMS with two\nsamples, and the leave-one-out REINFORCE (LOORF) estimator, which is ARMS with\nuncorrelated samples. We evaluate ARMS on several datasets for training\ngenerative models, and our experimental results show that it outperforms\ncompeting methods. We also develop a version of ARMS for optimizing the\nmulti-sample variational bound, and show that it outperforms both VIMCO and\nDisARM. The code is publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 23:19:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Dimitriev", "Alek", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2105.14146", "submitter": "Hongjing Zhang", "authors": "Hongjing Zhang, Ian Davidson", "title": "Deep Fair Discriminative Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep clustering has the potential to learn a strong representation and hence\nbetter clustering performance compared to traditional clustering methods such\nas $k$-means and spectral clustering. However, this strong representation\nlearning ability may make the clustering unfair by discovering surrogates for\nprotected information which we empirically show in our experiments. In this\nwork, we study a general notion of group-level fairness for both binary and\nmulti-state protected status variables (PSVs). We begin by formulating the\ngroup-level fairness problem as an integer linear programming formulation whose\ntotally unimodular constraint matrix means it can be efficiently solved via\nlinear programming. We then show how to inject this solver into a\ndiscriminative deep clustering backbone and hence propose a refinement learning\nalgorithm to combine the clustering goal with the fairness objective to learn\nfair clusters adaptively. Experimental results on real-world datasets\ndemonstrate that our model consistently outperforms state-of-the-art fair\nclustering algorithms. Our framework shows promising results for novel\nclustering tasks including flexible fairness constraints, multi-state PSVs and\npredictive clustering.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 23:50:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Hongjing", ""], ["Davidson", "Ian", ""]]}, {"id": "2105.14156", "submitter": "Kaustubh Shivdikar", "authors": "Kaustubh Shivdikar", "title": "SMASH: Sparse Matrix Atomic Scratchpad Hashing", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.17515.87840", "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse matrices, more specifically SpGEMM kernels, are commonly found in a\nwide range of applications, spanning graph-based path-finding to machine\nlearning algorithms (e.g., neural networks). A particular challenge in\nimplementing SpGEMM kernels has been the pressure placed on DRAM memory. One\napproach to tackle this problem is to use an inner product method for the\nSpGEMM kernel implementation. While the inner product produces fewer\nintermediate results, it can end up saturating the memory bandwidth, given the\nhigh number of redundant fetches of the input matrix elements. Using an outer\nproduct-based SpGEMM kernel can reduce redundant fetches, but at the cost of\nincreased overhead due to extra computation and memory accesses for\nproducing/managing partial products.\n  In this thesis, we introduce a novel SpGEMM kernel implementation based on\nthe row-wise product approach. We leverage atomic instructions to merge\nintermediate partial products as they are generated. The use of atomic\ninstructions eliminates the need to create partial product matrices.\n  To evaluate our row-wise product approach, we map an optimized SpGEMM kernel\nto a custom accelerator designed to accelerate graph-based applications. The\ntargeted accelerator is an experimental system named PIUMA, being developed by\nIntel. PIUMA provides several attractive features, including fast context\nswitching, user-configurable caches, globally addressable memory, non-coherent\ncaches, and asynchronous pipelines. We tailor our SpGEMM kernel to exploit many\nof the features of the PIUMA fabric.\n  This thesis compares our SpGEMM implementation against prior solutions, all\nmapped to the PIUMA framework. We briefly describe some of the PIUMA\narchitecture features and then delve into the details of our optimized SpGEMM\nkernel. Our SpGEMM kernel can achieve 9.4x speedup as compared to competing\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:22:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shivdikar", "Kaustubh", ""]]}, {"id": "2105.14162", "submitter": "Zhibo Zhang", "authors": "Ruiwen Li (co-first author), Zhibo Zhang (co-first author), Jiani Li,\n  Scott Sanner, Jongseong Jang, Yeonjeong Jeong, Dongsub Shim", "title": "EDDA: Explanation-driven Data Augmentation to Improve Model and\n  Explanation Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent years have seen the introduction of a range of methods for post-hoc\nexplainability of image classifier predictions. However, these post-hoc\nexplanations may not always align perfectly with classifier predictions, which\nposes a significant challenge when attempting to debug models based on such\nexplanations. To this end, we seek a methodology that can improve alignment\nbetween model predictions and explanation method that is both agnostic to the\nmodel and explanation classes and which does not require ground truth\nexplanations. We achieve this through a novel explanation-driven data\naugmentation (EDDA) method that augments the training data with occlusions of\nexisting data stemming from model-explanations; this is based on the simple\nmotivating principle that occluding salient regions for the model prediction\nshould decrease the model confidence in the prediction, while occluding\nnon-salient regions should not change the prediction -- if the model and\nexplainer are aligned. To verify that this augmentation method improves model\nand explainer alignment, we evaluate the methodology on a variety of datasets,\nimage classification models, and explanation methods. We verify in all cases\nthat our explanation-driven data augmentation method improves alignment of the\nmodel and explanation in comparison to no data augmentation and non-explanation\ndriven data augmentation methods. In conclusion, this approach provides a novel\nmodel- and explainer-agnostic methodology for improving alignment between model\npredictions and explanations, which we see as a critical step forward for\npractical deployment and debugging of image classification models.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:42:42 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 00:01:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Ruiwen", "", "co-first author"], ["Zhang", "Zhibo", "", "co-first author"], ["Li", "Jiani", ""], ["Sanner", "Scott", ""], ["Jang", "Jongseong", ""], ["Jeong", "Yeonjeong", ""], ["Shim", "Dongsub", ""]]}, {"id": "2105.14163", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Patrik Gerber, Chen Lu, Thibaut Le Gouic, Philippe\n  Rigollet", "title": "The query complexity of sampling from strongly log-concave distributions\n  in one dimension", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the first tight lower bound of $\\Omega(\\log\\log\\kappa)$ on the\nquery complexity of sampling from the class of strongly log-concave and\nlog-smooth distributions with condition number $\\kappa$ in one dimension.\nWhereas existing guarantees for MCMC-based algorithms scale polynomially in\n$\\kappa$, we introduce a novel algorithm based on rejection sampling that\ncloses this doubly exponential gap.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:51:17 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 18:32:57 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chewi", "Sinho", ""], ["Gerber", "Patrik", ""], ["Lu", "Chen", ""], ["Gouic", "Thibaut Le", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2105.14166", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Patrik Gerber, Chen Lu, Thibaut Le Gouic, Philippe\n  Rigollet", "title": "Rejection sampling from shape-constrained distributions in sublinear\n  time", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of generating exact samples from a target distribution,\nknown up to normalization, over a finite alphabet. The classical algorithm for\nthis task is rejection sampling, and although it has been used in practice for\ndecades, there is surprisingly little study of its fundamental limitations. In\nthis work, we study the query complexity of rejection sampling in a minimax\nframework for various classes of discrete distributions. Our results provide\nnew algorithms for sampling whose complexity scales sublinearly with the\nalphabet size. When applied to adversarial bandits, we show that a slight\nmodification of the Exp3 algorithm reduces the per-iteration complexity from\n$\\mathcal O(K)$ to $\\mathcal O(\\log^2 K)$, where $K$ is the number of arms.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 01:00:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chewi", "Sinho", ""], ["Gerber", "Patrik", ""], ["Lu", "Chen", ""], ["Gouic", "Thibaut Le", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2105.14171", "submitter": "Weishen Pan", "authors": "Weishen Pan, Changshui Zhang", "title": "The Definitions of Interpretability and Learning of Interpretable Models", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning algorithms getting adopted in an ever-increasing number\nof applications, interpretation has emerged as a crucial desideratum. In this\npaper, we propose a mathematical definition for the human-interpretable model.\nIn particular, we define interpretability between two information process\nsystems. If a prediction model is interpretable by a human recognition system\nbased on the above interpretability definition, the prediction model is defined\nas a completely human-interpretable model. We further design a practical\nframework to train a completely human-interpretable model by user interactions.\nExperiments on image datasets show the advantages of our proposed model in two\naspects: 1) The completely human-interpretable model can provide an entire\ndecision-making process that is human-understandable; 2) The completely\nhuman-interpretable model is more robust against adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 01:44:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pan", "Weishen", ""], ["Zhang", "Changshui", ""]]}, {"id": "2105.14172", "submitter": "Suyun Liu", "authors": "Suyun Liu, Luis Nunes Vicente", "title": "A Stochastic Alternating Balance $k$-Means Algorithm for Fair Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the application of data clustering to human-centric decision-making\nsystems, such as loan applications and advertisement recommendations, the\nclustering outcome might discriminate against people across different\ndemographic groups, leading to unfairness. A natural conflict occurs between\nthe cost of clustering (in terms of distance to cluster centers) and the\nbalance representation of all demographic groups across the clusters, leading\nto a bi-objective optimization problem that is nonconvex and nonsmooth. To\ndetermine the complete trade-off between these two competing goals, we design a\nnovel stochastic alternating balance fair $k$-means (SAfairKM) algorithm, which\nconsists of alternating classical mini-batch $k$-means updates and group swap\nupdates. The number of $k$-means updates and the number of swap updates\nessentially parameterize the weight put on optimizing each objective function.\nOur numerical experiments show that the proposed SAfairKM algorithm is robust\nand computationally efficient in constructing well-spread and high-quality\nPareto fronts both on synthetic and real datasets. Moreover, we propose a novel\ncompanion algorithm, the stochastic alternating bi-objective gradient descent\n(SA2GD) algorithm, which can handle a smooth version of the considered\nbi-objective fair $k$-means problem, more amenable for analysis. A sublinear\nconvergence rate of $\\mathcal{O}(1/T)$ is established under strong convexity\nfor the determination of a stationary point of a weighted sum of the two\nfunctions parameterized by the number of steps or updates on each function.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 01:47:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Suyun", ""], ["Vicente", "Luis Nunes", ""]]}, {"id": "2105.14188", "submitter": "Liyi Guo", "authors": "Liyi Guo, Junqi Jin, Haoqi Zhang, Zhenzhe Zheng, Zhiye Yang, Zhizhuang\n  Xing, Fei Pan, Lvyin Niu, Fan Wu, Haiyang Xu, Chuan Yu, Yuning Jiang,\n  Xiaoqiang Zhu", "title": "We Know What You Want: An Advertising Strategy Recommender System for\n  Online Advertising", "comments": null, "journal-ref": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD '21), August 14--18, 2021, Virtual Event,\n  Singapore", "doi": "10.1145/3447548.3467175", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advertising expenditures have become the major source of revenue for\ne-commerce platforms. Providing good advertising experiences for advertisers by\nreducing their costs of trial and error in discovering the optimal advertising\nstrategies is crucial for the long-term prosperity of online advertising. To\nachieve this goal, the advertising platform needs to identify the advertiser's\noptimization objectives, and then recommend the corresponding strategies to\nfulfill the objectives. In this work, we first deploy a prototype of strategy\nrecommender system on Taobao display advertising platform, which indeed\nincreases the advertisers' performance and the platform's revenue, indicating\nthe effectiveness of strategy recommendation for online advertising. We further\naugment this prototype system by explicitly learning the advertisers'\npreferences over various advertising performance indicators and then\noptimization objectives through their adoptions of different recommending\nadvertising strategies. We use contextual bandit algorithms to efficiently\nlearn the advertisers' preferences and maximize the recommendation adoption,\nsimultaneously. Simulation experiments based on Taobao online bidding data show\nthat the designed algorithms can effectively optimize the strategy adoption\nrate of advertisers.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:06:59 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 12:28:10 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 07:34:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Guo", "Liyi", ""], ["Jin", "Junqi", ""], ["Zhang", "Haoqi", ""], ["Zheng", "Zhenzhe", ""], ["Yang", "Zhiye", ""], ["Xing", "Zhizhuang", ""], ["Pan", "Fei", ""], ["Niu", "Lvyin", ""], ["Wu", "Fan", ""], ["Xu", "Haiyang", ""], ["Yu", "Chuan", ""], ["Jiang", "Yuning", ""], ["Zhu", "Xiaoqiang", ""]]}, {"id": "2105.14191", "submitter": "Thomas Haugland Johansen", "authors": "Thomas Haugland Johansen, Steffen Aagaard S{\\o}rensen, Kajsa\n  M{\\o}llersen, Fred Godtliebsen", "title": "Instance Segmentation of Microscopic Foraminifera", "comments": "18 pages, 14 figures. Submitted to Applied Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Foraminifera are single-celled marine organisms that construct shells that\nremain as fossils in the marine sediments. Classifying and counting these\nfossils are important in e.g. paleo-oceanographic and -climatological research.\nHowever, the identification and counting process has been performed manually\nsince the 1800s and is laborious and time-consuming. In this work, we present a\ndeep learning-based instance segmentation model for classifying, detecting, and\nsegmenting microscopic foraminifera. Our model is based on the Mask R-CNN\narchitecture, using model weight parameters that have learned on the COCO\ndetection dataset. We use a fine-tuning approach to adapt the parameters on a\nnovel object detection dataset of more than 7000 microscopic foraminifera and\nsediment grains. The model achieves a (COCO-style) average precision of $0.78\n\\pm 0.00$ on the classification and detection task, and $0.80 \\pm 0.00$ on the\nsegmentation task. When the model is evaluated without challenging sediment\ngrain images, the average precision for both tasks increases to $0.84 \\pm 0.00$\nand $0.86 \\pm 0.00$, respectively. Prediction results are analyzed both\nquantitatively and qualitatively and discussed. Based on our findings we\npropose several directions for future work, and conclude that our proposed\nmodel is an important step towards automating the identification and counting\nof microscopic foraminifera.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 10:46:22 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Johansen", "Thomas Haugland", ""], ["S\u00f8rensen", "Steffen Aagaard", ""], ["M\u00f8llersen", "Kajsa", ""], ["Godtliebsen", "Fred", ""]]}, {"id": "2105.14194", "submitter": "Nikolay Ivanov", "authors": "Nikolay Ivanov and Qiben Yan", "title": "Constraint-Based Inference of Heuristics for Foreign Exchange Trade\n  Model Optimization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Foreign Exchange (Forex) is a large decentralized market, on which\ntrading analysis and algorithmic trading are popular. Research efforts have\nbeen focusing on proof of efficiency of certain technical indicators. We\ndemonstrate, however, that the values of indicator functions are not\nreproducible and often reduce the number of trade opportunities, compared to\nprice-action trading.\n  In this work, we develop two dataset-agnostic Forex trading heuristic\ntemplates with high rate of trading signals. In order to determine most optimal\nparameters for the given heuristic prototypes, we perform a machine learning\nsimulation of 10 years of Forex price data over three low-margin instruments\nand 6 different OHLC granularities. As a result, we develop a specific and\nreproducible list of most optimal trade parameters found for each\ninstrument-granularity pair, with 118 pips of average daily profit for the\noptimized configuration.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 00:36:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ivanov", "Nikolay", ""], ["Yan", "Qiben", ""]]}, {"id": "2105.14196", "submitter": "Qi Zheng", "authors": "Qi Zheng", "title": "Classifying States of Cooking Objects Using Convolutional Neural Network", "comments": "6 pages,9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated cooking machine is a goal for the future. The main aim is to make\nthe cooking process easier, safer, and create human welfare. To allow robots to\naccurately perform the cooking activities, it is important for them to\nunderstand the cooking environment and recognize the objects, especially\ncorrectly identifying the state of the cooking objects. This will significantly\nimprove the correctness of the following cooking recipes. In this project,\nseveral parts of the experiment were conducted to design a robust deep\nconvolutional neural network for classifying the state of the cooking objects\nfrom scratch. The model is evaluated by using various techniques, such as\nadjusting architecture layers, tuning key hyperparameters, and using different\noptimization techniques to maximize the accuracy of state classification.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:26:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zheng", "Qi", ""]]}, {"id": "2105.14201", "submitter": "Qianren Mao", "authors": "Xi Li, Qianren Mao, Hao Peng, Hongdong Zhu, Jianxin Li, Zheng Wang", "title": "Automated Timeline Length Selection for Flexible Timeline Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By producing summaries for long-running events, timeline summarization (TLS)\nunderpins many information retrieval tasks. Successful TLS requires identifying\nan appropriate set of key dates (the timeline length) to cover. However, doing\nso is challenging as the right length can change from one topic to another.\nExisting TLS solutions either rely on an event-agnostic fixed length or an\nexpert-supplied setting. Neither of the strategies is desired for real-life TLS\nscenarios. A fixed, event-agnostic setting ignores the diversity of events and\ntheir development and hence can lead to low-quality TLS. Relying on\nexpert-crafted settings is neither scalable nor sustainable for processing many\ndynamically changing events. This paper presents a better TLS approach for\nautomatically and dynamically determining the TLS timeline length. We achieve\nthis by employing the established elbow method from the machine learning\ncommunity to automatically find the minimum number of dates within the time\nseries to generate concise and informative summaries. We applied our approach\nto four TLS datasets of English and Chinese and compared them against three\nprior methods. Experimental results show that our approach delivers comparable\nor even better summaries over state-of-art TLS methods, but it achieves this\nwithout expert involvement.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 03:47:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Li", "Xi", ""], ["Mao", "Qianren", ""], ["Peng", "Hao", ""], ["Zhu", "Hongdong", ""], ["Li", "Jianxin", ""], ["Wang", "Zheng", ""]]}, {"id": "2105.14203", "submitter": "Zhifeng Kong", "authors": "Zhifeng Kong, Kamalika Chaudhuri", "title": "Understanding Instance-based Interpretability of Variational\n  Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance-based interpretation methods have been widely studied for supervised\nlearning methods as they help explain how black box neural networks predict.\nHowever, instance-based interpretations remain ill-understood in the context of\nunsupervised learning. In this paper, we investigate influence functions [20],\na popular instance-based interpretation method, for a class of deep generative\nmodels called variational auto-encoders (VAE). We formally frame the\ncounter-factual question answered by influence functions in this setting, and\nthrough theoretical analysis, examine what they reveal about the impact of\ntraining samples on classical unsupervised learning methods. We then introduce\nVAE-TracIn, a computationally efficient and theoretically sound solution based\non Pruthi et al. [28], for VAEs. Finally, we evaluate VAE-TracIn on several\nreal world datasets with extensive quantitative and qualitative analysis.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 04:03:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kong", "Zhifeng", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "2105.14210", "submitter": "Chen Zhang", "authors": "Fang Ma, Chen Zhang, Dawei Song", "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification", "comments": "7 pages, 2 figures, 4 tables, accepted to Findings of ACL 2021. Repo:\n  https://github.com/BD-MF/POS4ASC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect sentiment classification (ASC) aims at determining sentiments\nexpressed towards different aspects in a sentence. While state-of-the-art ASC\nmodels have achieved remarkable performance, they are recently shown to suffer\nfrom the issue of robustness. Particularly in two common scenarios: when\ndomains of test and training data are different (out-of-domain scenario) or\ntest data is adversarially perturbed (adversarial scenario), ASC models may\nattend to irrelevant words and neglect opinion expressions that truly describe\ndiverse aspects. To tackle the challenge, in this paper, we hypothesize that\nposition bias (i.e., the words closer to a concerning aspect would carry a\nhigher degree of importance) is crucial for building more robust ASC models by\nreducing the probability of mis-attending. Accordingly, we propose two\nmechanisms for capturing position bias, namely position-biased weight and\nposition-biased dropout, which can be flexibly injected into existing models to\nenhance representations for classification. Experiments conducted on\nout-of-domain and adversarial datasets demonstrate that our proposed approaches\nlargely improve the robustness and effectiveness of current models.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 04:41:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ma", "Fang", ""], ["Zhang", "Chen", ""], ["Song", "Dawei", ""]]}, {"id": "2105.14214", "submitter": "Qingfeng Lan", "authors": "Qingfeng Lan, Luke Kumar, Martha White, Alona Fyshe", "title": "Predictive Representation Learning for Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To effectively perform the task of next-word prediction, long short-term\nmemory networks (LSTMs) must keep track of many types of information. Some\ninformation is directly related to the next word's identity, but some is more\nsecondary (e.g. discourse-level features or features of downstream words).\nCorrelates of secondary information appear in LSTM representations even though\nthey are not part of an \\emph{explicitly} supervised prediction task. In\ncontrast, in reinforcement learning (RL), techniques that explicitly supervise\nrepresentations to predict secondary information have been shown to be\nbeneficial. Inspired by that success, we propose Predictive Representation\nLearning (PRL), which explicitly constrains LSTMs to encode specific\npredictions, like those that might need to be learned implicitly. We show that\nPRL 1) significantly improves two strong language modeling methods, 2)\nconverges more quickly, and 3) performs better when data is limited. Our work\nshows that explicitly encoding a simple predictive task facilitates the search\nfor a more effective language model.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:03:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lan", "Qingfeng", ""], ["Kumar", "Luke", ""], ["White", "Martha", ""], ["Fyshe", "Alona", ""]]}, {"id": "2105.14216", "submitter": "Jiahao Xie", "authors": "Jiahao Xie, Chao Zhang, Yunsong Zhang, Zebang Shen, Hui Qian", "title": "A Federated Learning Framework for Nonconvex-PL Minimax Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general class of nonconvex-PL minimax problems in the\ncross-device federated learning setting. Although nonconvex-PL minimax problems\nhave received a lot of interest in recent years, existing algorithms do not\napply to the cross-device federated learning setting which is substantially\ndifferent from conventional distributed settings and poses new challenges. To\nbridge this gap, we propose an algorithmic framework named FedSGDA. FedSGDA\nperforms multiple local update steps on a subset of active clients in each\nround and leverages global gradient estimates to correct the bias in local\nupdate directions. By incorporating FedSGDA with two representative global\ngradient estimators, we obtain two specific algorithms. We establish\nconvergence rates of the proposed algorithms by using novel potential\nfunctions. Experimental results on synthetic and real data corroborate our\ntheory and demonstrate the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:18:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xie", "Jiahao", ""], ["Zhang", "Chao", ""], ["Zhang", "Yunsong", ""], ["Shen", "Zebang", ""], ["Qian", "Hui", ""]]}, {"id": "2105.14218", "submitter": "Fei Ye", "authors": "Fei Ye, Shen Zhang, Pin Wang, and Ching-Yao Chan", "title": "A Survey of Deep Reinforcement Learning Algorithms for Motion Planning\n  and Control of Autonomous Vehicles", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this survey, we systematically summarize the current literature on studies\nthat apply reinforcement learning (RL) to the motion planning and control of\nautonomous vehicles. Many existing contributions can be attributed to the\npipeline approach, which consists of many hand-crafted modules, each with a\nfunctionality selected for the ease of human interpretation. However, this\napproach does not automatically guarantee maximal performance due to the lack\nof a system-level optimization. Therefore, this paper also presents a growing\ntrend of work that falls into the end-to-end approach, which typically offers\nbetter performance and smaller system scales. However, their performance also\nsuffers from the lack of expert data and generalization issues. Finally, the\nremaining challenges applying deep RL algorithms on autonomous driving are\nsummarized, and future research directions are also presented to tackle these\nchallenges.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:27:07 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 03:50:52 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Ye", "Fei", ""], ["Zhang", "Shen", ""], ["Wang", "Pin", ""], ["Chan", "Ching-Yao", ""]]}, {"id": "2105.14219", "submitter": "Francesc Wilhelmi", "authors": "Francesc Wilhelmi, David G\\'oez, Paola Soto, Ramon Vall\\'es, Mohammad\n  Alfaifi, Abdulrahman Algunayah, Jorge Martin-P\\'erez, Luigi Girletti,\n  Rajasekar Mohan, K Venkat Ramnan, Boris Bellalta", "title": "Machine Learning for Performance Prediction of Channel Bonding in\n  Next-Generation IEEE 802.11 WLANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of Artificial Intelligence (AI)-empowered communications,\nindustry, academia, and standardization organizations are progressing on the\ndefinition of mechanisms and procedures to address the increasing complexity of\nfuture 5G and beyond communications. In this context, the International\nTelecommunication Union (ITU) organized the first AI for 5G Challenge to bring\nindustry and academia together to introduce and solve representative problems\nrelated to the application of Machine Learning (ML) to networks. In this paper,\nwe present the results gathered from Problem Statement~13 (PS-013), organized\nby Universitat Pompeu Fabra (UPF), which primary goal was predicting the\nperformance of next-generation Wireless Local Area Networks (WLANs) applying\nChannel Bonding (CB) techniques. In particular, we overview the ML models\nproposed by participants (including Artificial Neural Networks, Graph Neural\nNetworks, Random Forest regression, and gradient boosting) and analyze their\nperformance on an open dataset generated using the IEEE 802.11ax-oriented\nKomondor network simulator. The accuracy achieved by the proposed methods\ndemonstrates the suitability of ML for predicting the performance of WLANs.\nMoreover, we discuss the importance of abstracting WLAN interactions to achieve\nbetter results, and we argue that there is certainly room for improvement in\nthroughput prediction through ML.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:33:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wilhelmi", "Francesc", ""], ["G\u00f3ez", "David", ""], ["Soto", "Paola", ""], ["Vall\u00e9s", "Ramon", ""], ["Alfaifi", "Mohammad", ""], ["Algunayah", "Abdulrahman", ""], ["Martin-P\u00e9rez", "Jorge", ""], ["Girletti", "Luigi", ""], ["Mohan", "Rajasekar", ""], ["Ramnan", "K Venkat", ""], ["Bellalta", "Boris", ""]]}, {"id": "2105.14224", "submitter": "Fan Hu", "authors": "Fan Hu, Lei Wang, Yishen Hu, Dongqi Wang, Weijie Wang, Jianbing Jiang,\n  Nan Li and Peng Yin", "title": "A Novel Framework Integrating AI Model and Enzymological Experiments\n  Promotes Identification of SARS-CoV-2 3CL Protease Inhibitors and\n  Activity-based Probe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of protein-ligand interaction plays a key role in\nbiochemical research and drug discovery. Although deep learning has recently\nshown great promise in discovering new drugs, there remains a gap between deep\nlearning-based and experimental approaches. Here we propose a novel framework,\nnamed AIMEE, integrating AI Model and Enzymology Experiments, to identify\ninhibitors against 3CL protease of SARS-CoV-2, which has taken a significant\ntoll on people across the globe. From a bioactive chemical library, we have\nconducted two rounds of experiments and identified six novel inhibitors with a\nhit rate of 29.41%, and four of them showed an IC50 value less than 3 {\\mu}M.\nMoreover, we explored the interpretability of the central model in AIMEE,\nmapping the deep learning extracted features to domain knowledge of chemical\nproperties. Based on this knowledge, a commercially available compound was\nselected and proven to be an activity-based probe of 3CLpro. This work\nhighlights the great potential of combining deep learning models and\nbiochemical experiments for intelligent iteration and expanding the boundaries\nof drug discovery.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 06:23:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hu", "Fan", ""], ["Wang", "Lei", ""], ["Hu", "Yishen", ""], ["Wang", "Dongqi", ""], ["Wang", "Weijie", ""], ["Jiang", "Jianbing", ""], ["Li", "Nan", ""], ["Yin", "Peng", ""]]}, {"id": "2105.14244", "submitter": "Hongteng Xu", "authors": "Hongteng Xu, Peilin Zhao, Junzhou Huang, Dixin Luo", "title": "Learning Graphon Autoencoders for Generative Graph Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphon is a nonparametric model that generates graphs with arbitrary sizes\nand can be induced from graphs easily. Based on this model, we propose a novel\nalgorithmic framework called \\textit{graphon autoencoder} to build an\ninterpretable and scalable graph generative model. This framework treats\nobserved graphs as induced graphons in functional space and derives their\nlatent representations by an encoder that aggregates Chebshev graphon filters.\nA linear graphon factorization model works as a decoder, leveraging the latent\nrepresentations to reconstruct the induced graphons (and the corresponding\nobserved graphs). We develop an efficient learning algorithm to learn the\nencoder and the decoder, minimizing the Wasserstein distance between the model\nand data distributions. This algorithm takes the KL divergence of the graph\ndistributions conditioned on different graphons as the underlying distance and\nleads to a reward-augmented maximum likelihood estimation. The graphon\nautoencoder provides a new paradigm to represent and generate graphs, which has\ngood generalizability and transferability.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 08:11:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Hongteng", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""], ["Luo", "Dixin", ""]]}, {"id": "2105.14250", "submitter": "Mikhail Usvyatsov", "authors": "Mikhail Usvyatsov, Anastasia Makarova, Rafael Ballester-Ripoll, Maxim\n  Rakhuba, Andreas Krause, Konrad Schindler", "title": "Cherry-Picking Gradients: Learning Low-Rank Embeddings of Visual Data\n  via Differentiable Cross-Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an end-to-end trainable framework that processes large-scale\nvisual data tensors by looking \\emph{at a fraction of their entries only}. Our\nmethod combines a neural network encoder with a \\emph{tensor train\ndecomposition} to learn a low-rank latent encoding, coupled with\ncross-approximation (CA) to learn the representation through a subset of the\noriginal samples. CA is an adaptive sampling algorithm that is native to tensor\ndecompositions and avoids working with the full high-resolution data\nexplicitly. Instead, it actively selects local representative samples that we\nfetch out-of-core and on-demand. The required number of samples grows only\nlogarithmically with the size of the input. Our implicit representation of the\ntensor in the network enables processing large grids that could not be\notherwise tractable in their uncompressed form. The proposed approach is\nparticularly useful for large-scale multidimensional grid data (e.g., 3D\ntomography), and for tasks that require context over a large receptive field\n(e.g., predicting the medical condition of entire organs). The code will be\navailable at https://github.com/aelphy/c-pic\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 08:39:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Usvyatsov", "Mikhail", ""], ["Makarova", "Anastasia", ""], ["Ballester-Ripoll", "Rafael", ""], ["Rakhuba", "Maxim", ""], ["Krause", "Andreas", ""], ["Schindler", "Konrad", ""]]}, {"id": "2105.14257", "submitter": "Korbinian Abstreiter", "authors": "Korbinian Abstreiter, Stefan Bauer, Arash Mehrjou", "title": "Representation Learning in Continuous-Time Score-Based Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Score-based methods represented as stochastic differential equations on a\ncontinuous time domain have recently proven successful as a non-adversarial\ngenerative model. Training such models relies on denoising score matching,\nwhich can be seen as multi-scale denoising autoencoders. Here, we augment the\ndenoising score-matching framework to enable representation learning without\nany supervised signal. GANs and VAEs learn representations by directly\ntransforming latent codes to data samples. In contrast, score-based\nrepresentation learning relies on a new formulation of the denoising\nscore-matching objective and thus encodes information needed for denoising. We\nshow how this difference allows for manual control of the level of detail\nencoded in the representation.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 09:26:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Abstreiter", "Korbinian", ""], ["Bauer", "Stefan", ""], ["Mehrjou", "Arash", ""]]}, {"id": "2105.14260", "submitter": "Houshuang Chen", "authors": "Houshuang Chen (1), Zengfeng Huang (2), Shuai Li (1) and Chihao Zhang\n  (1) ((1) Shanghai Jiao Tong University, (2) Fudan University)", "title": "Understanding Bandits with Graph Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bandit problem with graph feedback, proposed in [Mannor and Shamir,\nNeurIPS 2011], is modeled by a directed graph $G=(V,E)$ where $V$ is the\ncollection of bandit arms, and once an arm is triggered, all its incident arms\nare observed. A fundamental question is how the structure of the graph affects\nthe min-max regret. We propose the notions of the fractional weak domination\nnumber $\\delta^*$ and the $k$-packing independence number capturing upper bound\nand lower bound for the regret respectively. We show that the two notions are\ninherently connected via aligning them with the linear program of the weakly\ndominating set and its dual -- the fractional vertex packing set respectively.\nBased on this connection, we utilize the strong duality theorem to prove a\ngeneral regret upper bound $O\\left(\\left( \\delta^*\\log\n|V|\\right)^{\\frac{1}{3}}T^{\\frac{2}{3}}\\right)$ and a lower bound\n$\\Omega\\left(\\left(\\delta^*/\\alpha\\right)^{\\frac{1}{3}}T^{\\frac{2}{3}}\\right)$\nwhere $\\alpha$ is the integrality gap of the dual linear program. Therefore,\nour bounds are tight up to a $\\left(\\log |V|\\right)^{\\frac{1}{3}}$ factor on\ngraphs with bounded integrality gap for the vertex packing problem including\ntrees and graphs with bounded degree. Moreover, we show that for several\nspecial families of graphs, we can get rid of the $\\left(\\log\n|V|\\right)^{\\frac{1}{3}}$ factor and establish optimal regret.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 09:35:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Houshuang", "", "Shanghai Jiao Tong University"], ["Huang", "Zengfeng", "", "Fudan University"], ["Li", "Shuai", "", "Shanghai Jiao Tong University"], ["Zhang", "Chihao", "", "Shanghai Jiao Tong University"]]}, {"id": "2105.14267", "submitter": "Botao Hao", "authors": "Botao Hao, Tor Lattimore, Wei Deng", "title": "Information Directed Sampling for Sparse Linear Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic sparse linear bandits offer a practical model for high-dimensional\nonline decision-making problems and have a rich information-regret structure.\nIn this work we explore the use of information-directed sampling (IDS), which\nnaturally balances the information-regret trade-off. We develop a class of\ninformation-theoretic Bayesian regret bounds that nearly match existing lower\nbounds on a variety of problem instances, demonstrating the adaptivity of IDS.\nTo efficiently implement sparse IDS, we propose an empirical Bayesian approach\nfor sparse posterior sampling using a spike-and-slab Gaussian-Laplace prior.\nNumerical results demonstrate significant regret reductions by sparse IDS\nrelative to several baselines.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 10:26:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hao", "Botao", ""], ["Lattimore", "Tor", ""], ["Deng", "Wei", ""]]}, {"id": "2105.14275", "submitter": "Aleksei Tiulpin", "authors": "Aleksei Tiulpin and Matthew B. Blaschko", "title": "Greedy Bayesian Posterior Approximation with Deep Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensembles of independently trained neural networks are a state-of-the-art\napproach to estimate predictive uncertainty in Deep Learning, and can be\ninterpreted as an approximation of the posterior distribution via a mixture of\ndelta functions. The training of ensembles relies on non-convexity of the loss\nlandscape and random initialization of their individual members, making the\nresulting posterior approximation uncontrolled. This paper proposes a novel and\nprincipled method to tackle this limitation, minimizing an $f$-divergence\nbetween the true posterior and a kernel density estimator in a function space.\nWe analyze this objective from a combinatorial point of view, and show that it\nis submodular with respect to mixture components for any $f$. Subsequently, we\nconsider the problem of greedy ensemble construction, and from the marginal\ngain of the total objective, we derive a novel diversity term for ensemble\nmethods. The performance of our approach is demonstrated on computer vision\nout-of-distribution benchmarks in a range of architectures trained on multiple\ndatasets. The source code of our method is publicly available at\nhttps://github.com/MIPT-Oulu/greedy_ensembles_training.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 11:35:27 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 07:29:42 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Tiulpin", "Aleksei", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "2105.14276", "submitter": "Rossella Arcucci Dr", "authors": "Robin Hendrickx, Rossella Arcucci, Julio Amador D{\\i}az Lopez, Yi-Ke\n  Guo, and Mark Kennedy", "title": "Correcting public opinion trends through Bayesian data assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring public opinion is a key focus during democratic elections, enabling\ncandidates to gauge their popularity and alter their campaign strategies\naccordingly. Traditional survey polling remains the most popular estimation\ntechnique, despite its cost and time intensity, measurement errors, lack of\nreal-time capabilities and lagged representation of public opinion. In recent\nyears, Twitter opinion mining has attempted to combat these issues. Despite\nachieving promising results, it experiences its own set of shortcomings such as\nan unrepresentative sample population and a lack of long term stability. This\npaper aims to merge data from both these techniques using Bayesian data\nassimilation to arrive at a more accurate estimate of true public opinion for\nthe Brexit referendum. This paper demonstrates the effectiveness of the\nproposed approach using Twitter opinion data and survey data from trusted\npollsters. Firstly, the possible existence of a time gap of 16 days between the\ntwo data sets is identified. This gap is subsequently incorporated into a\nproposed assimilation architecture. This method was found to adequately\nincorporate information from both sources and measure a strong upward trend in\nLeave support leading up to the Brexit referendum. The proposed technique\nprovides useful estimates of true opinion, which is essential to future opinion\nmeasurement and forecasting research.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 11:39:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hendrickx", "Robin", ""], ["Arcucci", "Rossella", ""], ["Lopez", "Julio Amador D\u0131az", ""], ["Guo", "Yi-Ke", ""], ["Kennedy", "Mark", ""]]}, {"id": "2105.14278", "submitter": "Navid Ghassemi", "authors": "Afshin Shoeibi, Navid Ghassemi, Marjane Khodatars, Mahboobeh Jafari,\n  Parisa Moridian, Roohallah Alizadehsani, Ali Khadem, Yinan Kong, Assef Zare,\n  Juan Manuel Gorriz, Javier Ram\\'irez, Maryam Panahiazar, Abbas Khosravi,\n  Saeid Nahavandi", "title": "Applications of Epileptic Seizures Detection in Neuroimaging Modalities\n  Using Deep Learning Techniques: Methods, Challenges, and Future Works", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Epileptic seizures are a type of neurological disorder that affect many\npeople worldwide. Specialist physicians and neurologists take advantage of\nstructural and functional neuroimaging modalities to diagnose various types of\nepileptic seizures. Neuroimaging modalities assist specialist physicians\nconsiderably in analyzing brain tissue and the changes made in it. One method\nto accelerate the accurate and fast diagnosis of epileptic seizures is to\nemploy computer aided diagnosis systems (CADS) based on artificial intelligence\n(AI) and functional and structural neuroimaging modalities. AI encompasses a\nvariety of areas, and one of its branches is deep learning (DL). Not long ago,\nand before the rise of DL algorithms, feature extraction was an essential part\nof every conventional machine learning method, yet handcrafting features limit\nthese models' performances to the knowledge of system designers. DL methods\nresolved this issue entirely by automating the feature extraction and\nclassification process; applications of these methods in many fields of\nmedicine, such as the diagnosis of epileptic seizures, have made notable\nimprovements. In this paper, a comprehensive overview of the types of DL\nmethods exploited to diagnose epileptic seizures from various neuroimaging\nmodalities has been studied. Additionally, rehabilitation systems and cloud\ncomputing in epileptic seizures diagnosis applications have been exactly\ninvestigated using various modalities.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 12:00:39 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shoeibi", "Afshin", ""], ["Ghassemi", "Navid", ""], ["Khodatars", "Marjane", ""], ["Jafari", "Mahboobeh", ""], ["Moridian", "Parisa", ""], ["Alizadehsani", "Roohallah", ""], ["Khadem", "Ali", ""], ["Kong", "Yinan", ""], ["Zare", "Assef", ""], ["Gorriz", "Juan Manuel", ""], ["Ram\u00edrez", "Javier", ""], ["Panahiazar", "Maryam", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2105.14280", "submitter": "Wei Wu", "authors": "Wei Wu, Bin Li, Chuan Luo, Wolfgang Nejdl", "title": "Hashing-Accelerated Graph Neural Networks for Link Prediction", "comments": null, "journal-ref": "The Web Conference 2021", "doi": "10.1145/3442381.3449884", "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are ubiquitous in the real world. Link prediction, as one of the key\nproblems for network-structured data, aims to predict whether there exists a\nlink between two nodes. The traditional approaches are based on the explicit\nsimilarity computation between the compact node representation by embedding\neach node into a low-dimensional space. In order to efficiently handle the\nintensive similarity computation in link prediction, the hashing technique has\nbeen successfully used to produce the node representation in the Hamming space.\nHowever, the hashing-based link prediction algorithms face accuracy loss from\nthe randomized hashing techniques or inefficiency from the learning to hash\ntechniques in the embedding process. Currently, the Graph Neural Network (GNN)\nframework has been widely applied to the graph-related tasks in an end-to-end\nmanner, but it commonly requires substantial computational resources and memory\ncosts due to massive parameter learning, which makes the GNN-based algorithms\nimpractical without the help of a powerful workhorse. In this paper, we propose\na simple and effective model called #GNN, which balances the trade-off between\naccuracy and efficiency. #GNN is able to efficiently acquire node\nrepresentation in the Hamming space for link prediction by exploiting the\nrandomized hashing technique to implement message passing and capture\nhigh-order proximity in the GNN framework. Furthermore, we characterize the\ndiscriminative power of #GNN in probability. The extensive experimental results\ndemonstrate that the proposed #GNN algorithm achieves accuracy comparable to\nthe learning-based algorithms and outperforms the randomized algorithm, while\nrunning significantly faster than the learning-based algorithms. Also, the\nproposed algorithm shows excellent scalability on a large-scale network with\nthe limited resources.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 12:04:27 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wu", "Wei", ""], ["Li", "Bin", ""], ["Luo", "Chuan", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "2105.14301", "submitter": "Haozhe Shan", "authors": "Haozhe Shan and Blake Bordelon", "title": "Rapid Feature Evolution Accelerates Learning in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network (NN) training and generalization in the infinite-width limit\nare well-characterized by kernel methods with a neural tangent kernel (NTK)\nthat is stationary in time. However, finite-width NNs consistently outperform\ncorresponding kernel methods, suggesting the importance of feature learning,\nwhich manifests as the time evolution of NTKs. Here, we analyze the phenomenon\nof kernel alignment of the NTK with the target functions during gradient\ndescent. We first provide a mechanistic explanation for why alignment between\ntask and kernel occurs in deep linear networks. We then show that this behavior\noccurs more generally if one optimizes the feature map over time to accelerate\nlearning while constraining how quickly the features evolve. Empirically,\ngradient descent undergoes a feature learning phase, during which top\neigenfunctions of the NTK quickly align with the target function and the loss\ndecreases faster than power law in time; it then enters a kernel gradient\ndescent (KGD) phase where the alignment does not improve significantly and the\ntraining loss decreases in power law. We show that feature evolution is faster\nand more dramatic in deeper networks. We also found that networks with multiple\noutput nodes develop separate, specialized kernels for each output channel, a\nphenomenon we termed kernel specialization. We show that this class-specific\nalignment is does not occur in linear networks.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 13:50:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shan", "Haozhe", ""], ["Bordelon", "Blake", ""]]}, {"id": "2105.14318", "submitter": "Shenhao Wang", "authors": "Da Zhang, Qingyi Wang, Shaojie Song, Simiao Chen, Mingwei Li, Lu Shen,\n  Siqi Zheng, Bofeng Cai, Shenhao Wang", "title": "Estimating air quality co-benefits of energy transition using machine\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.LG q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating health benefits of reducing fossil fuel use from improved air\nquality provides important rationales for carbon emissions abatement.\nSimulating pollution concentration is a crucial step of the estimation, but\ntraditional approaches often rely on complicated chemical transport models that\nrequire extensive expertise and computational resources. In this study, we\ndevelop a novel and succinct machine learning framework that is able to provide\nprecise and robust annual average fine particle (PM2.5) concentration\nestimations directly from a high-resolution fossil energy use data set. The\naccessibility and applicability of this framework show great potentials of\nmachine learning approaches for integrated assessment studies. Applications of\nthe framework with Chinese data reveal highly heterogeneous health benefits of\nreducing fossil fuel use in different sectors and regions in China with a mean\nof \\$34/tCO2 and a standard deviation of \\$84/tCO2. Reducing rural and\nresidential coal use offers the highest co-benefits with a mean of \\$360/tCO2.\nOur findings prompt careful policy designs to maximize cost-effectiveness in\nthe transition towards a carbon-neutral energy system.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 14:52:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Da", ""], ["Wang", "Qingyi", ""], ["Song", "Shaojie", ""], ["Chen", "Simiao", ""], ["Li", "Mingwei", ""], ["Shen", "Lu", ""], ["Zheng", "Siqi", ""], ["Cai", "Bofeng", ""], ["Wang", "Shenhao", ""]]}, {"id": "2105.14328", "submitter": "Ye Tian", "authors": "Ye Tian and Yang Feng", "title": "Transfer Learning under High-dimensional Generalized Linear Models", "comments": "52 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the transfer learning problem under high-dimensional\ngeneralized linear models (GLMs), which aim to improve the fit on target data\nby borrowing information from useful source data. Given which sources to\ntransfer, we propose an oracle algorithm and derive its $\\ell_2$-estimation\nerror bounds. The theoretical analysis shows that under certain conditions,\nwhen the target and source are sufficiently close to each other, the estimation\nerror bound could be improved over that of the classical penalized estimator\nusing only target data. When we don't know which sources to transfer, an\nalgorithm-free transferable source detection approach is introduced to detect\ninformative sources. The detection consistency is proved under the\nhigh-dimensional GLM transfer learning setting. Extensive simulations and a\nreal-data experiment verify the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 15:39:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tian", "Ye", ""], ["Feng", "Yang", ""]]}, {"id": "2105.14329", "submitter": "Gerrit Gro{\\ss}mann", "authors": "Gerrit Gro{\\ss}mann, Julian Zimmerlin, Michael Backenk\\\"ohler, Verena\n  Wolf", "title": "GINA: Neural Relational Inference From Independent Snapshots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.MA physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamical systems in which local interactions among agents give rise to\ncomplex emerging phenomena are ubiquitous in nature and society. This work\nexplores the problem of inferring the unknown interaction structure\n(represented as a graph) of such a system from measurements of its constituent\nagents or individual components (represented as nodes). We consider a setting\nwhere the underlying dynamical model is unknown and where different\nmeasurements (i.e., snapshots) may be independent (e.g., may stem from\ndifferent experiments). We propose GINA (Graph Inference Network Architecture),\na graph neural network (GNN) to simultaneously learn the latent interaction\ngraph and, conditioned on the interaction graph, the prediction of a node's\nobservable state based on adjacent vertices. GINA is based on the hypothesis\nthat the ground truth interaction graph -- among all other potential graphs --\nallows to predict the state of a node, given the states of its neighbors, with\nthe highest accuracy. We test this hypothesis and demonstrate GINA's\neffectiveness on a wide range of interaction graphs and dynamical processes.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 15:42:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gro\u00dfmann", "Gerrit", ""], ["Zimmerlin", "Julian", ""], ["Backenk\u00f6hler", "Michael", ""], ["Wolf", "Verena", ""]]}, {"id": "2105.14333", "submitter": "Dinesh J", "authors": "Dinesh J and Mohammed Rhithick A", "title": "Covid-19 diagnosis from x-ray using neural networks", "comments": "3 Graphs, 1 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Corona virus or COVID-19 is a pandemic illness, which has influenced more\nthan million of causalities worldwide and infected a few large number of\nindividuals .Innovative instrument empowering quick screening of the COVID-19\ncontamination with high precision can be critically useful to the medical care\nexperts. The primary clinical device presently being used for the analysis of\nCOVID-19 is the Reverse record polymerase chain response as known as RT-PCR,\nwhich is costly, less-delicate and requires specific clinical work force. X-Ray\nimaging is an effectively available apparatus that can be a great option in the\nCOVID-19 conclusion. This exploration was taken to examine the utility of\ncomputerized reasoning in the quick and exact recognition of COVID-19 from\nchest X-Ray pictures. The point of this paper is to propose a procedure for\nprogrammed recognition of COVID-19 from advanced chest X-Ray images applying\npre-prepared profound learning calculations while boosting the discovery\nexactness. The point is to give over-focused on clinical experts a second pair\nof eyes through a learning picture characterization models. We distinguish an\nappropriate Convolutional Neural Network-CNN model through beginning similar\ninvestigation of a few mainstream CNN models.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 16:12:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["J", "Dinesh", ""], ["A", "Mohammed Rhithick", ""]]}, {"id": "2105.14337", "submitter": "Diego Gonz\\'alez-S\\'anchez", "authors": "D\\'avid Terj\\'ek (1) and Diego Gonz\\'alez-S\\'anchez (1) ((1) Alfr\\'ed\n  R\\'enyi Institute of Mathematics)", "title": "Optimal transport with $f$-divergence regularization and generalized\n  Sinkhorn algorithm", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropic regularization provides a generalization of the original optimal\ntransport problem. It introduces a penalty term defined by the Kullback-Leibler\ndivergence, making the problem more tractable via the celebrated Sinkhorn\nalgorithm. Replacing the Kullback-Leibler divergence with a general\n$f$-divergence leads to a natural generalization. Using convex analysis, we\nextend the theory developed so far to include $f$-divergences defined by\nfunctions of Legendre type, and prove that under some mild conditions, strong\nduality holds, optimums in both the primal and dual problems are attained, the\ngeneralization of the $c$-transform is well-defined, and we give sufficient\nconditions for the generalized Sinkhorn algorithm to converge to an optimal\nsolution. We propose a practical algorithm for computing the regularized\noptimal transport cost and its gradient via the generalized Sinkhorn algorithm.\nFinally, we present experimental results on synthetic 2-dimensional data,\ndemonstrating the effects of using different $f$-divergences for\nregularization, which influences convergence speed, numerical stability and\nsparsity of the optimal coupling.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 16:37:31 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Terj\u00e9k", "D\u00e1vid", ""], ["Gonz\u00e1lez-S\u00e1nchez", "Diego", ""]]}, {"id": "2105.14351", "submitter": "Mohsen Shahhosseini", "authors": "Mohsen Shahhosseini, Guiping Hu, Saeed Khaki, Sotirios V. Archontoulis", "title": "Corn Yield Prediction with Ensemble CNN-DNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We investigate the predictive performance of two novel CNN-DNN machine\nlearning ensemble models in predicting county-level corn yields across the US\nCorn Belt (12 states). The developed data set is a combination of management,\nenvironment, and historical corn yields from 1980-2019. Two scenarios for\nensemble creation are considered: homogenous and heterogeneous ensembles. In\nhomogenous ensembles, the base CNN-DNN models are all the same, but they are\ngenerated with a bagging procedure to ensure they exhibit a certain level of\ndiversity. Heterogenous ensembles are created from different base CNN-DNN\nmodels which share the same architecture but have different levels of depth.\nThree types of ensemble creation methods were used to create several ensembles\nfor either of the scenarios: Basic Ensemble Method (BEM), Generalized Ensemble\nMethod (GEM), and stacked generalized ensembles. Results indicated that both\ndesigned ensemble types (heterogenous and homogenous) outperform the ensembles\ncreated from five individual ML models (linear regression, LASSO, random\nforest, XGBoost, and LightGBM). Furthermore, by introducing improvements over\nthe heterogeneous ensembles, the homogenous ensembles provide the most accurate\nyield predictions across US Corn Belt states. This model could make 2019 yield\npredictions with a root mean square error of 866 kg/ha, equivalent to 8.5%\nrelative root mean square, and could successfully explain about 77% of the\nspatio-temporal variation in the corn grain yields. The significant predictive\npower of this model can be leveraged for designing a reliable tool for corn\nyield prediction which will, in turn, assist agronomic decision-makers.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 18:25:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shahhosseini", "Mohsen", ""], ["Hu", "Guiping", ""], ["Khaki", "Saeed", ""], ["Archontoulis", "Sotirios V.", ""]]}, {"id": "2105.14363", "submitter": "Niladri Chatterji", "authors": "Niladri S. Chatterji, Aldo Pacchiano, Peter L. Bartlett, Michael I.\n  Jordan", "title": "On the Theory of Reinforcement Learning with Once-per-Episode Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a theory of reinforcement learning (RL) in which the learner\nreceives binary feedback only once at the end of an episode. While this is an\nextreme test case for theory, it is also arguably more representative of\nreal-world applications than the traditional requirement in RL practice that\nthe learner receive feedback at every time step. Indeed, in many real-world\napplications of reinforcement learning, such as self-driving cars and robotics,\nit is easier to evaluate whether a learner's complete trajectory was either\n\"good\" or \"bad,\" but harder to provide a reward signal at each step. To show\nthat learning is possible in this more challenging setting, we study the case\nwhere trajectory labels are generated by an unknown parametric model, and\nprovide a statistically and computationally efficient algorithm that achieves\nsub-linear regret.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 19:48:51 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 05:16:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chatterji", "Niladri S.", ""], ["Pacchiano", "Aldo", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2105.14364", "submitter": "Corinna Coupette", "authors": "Corinna Coupette, Jilles Vreeken", "title": "Graph Similarity Description: How Are These Graphs Similar?", "comments": "27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n  (KDD 2021); 9+2 pages, 9+4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How do social networks differ across platforms? How do information networks\nchange over time? Answering questions like these requires us to compare two or\nmore graphs. This task is commonly treated as a measurement problem, but\nnumerical answers give limited insight. Here, we argue that if the goal is to\ngain understanding, we should treat graph similarity assessment as a\ndescription problem instead. We formalize this problem as a model selection\ntask using the Minimum Description Length principle, capturing the similarity\nof the input graphs in a common model and the differences between them in\ntransformations to individual models. To discover good models, we propose Momo,\nwhich breaks the problem into two parts and introduces efficient algorithms for\neach. Through an extensive set of experiments on a wide range of synthetic and\nreal-world graphs, we confirm that Momo works well in practice.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 19:55:27 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Coupette", "Corinna", ""], ["Vreeken", "Jilles", ""]]}, {"id": "2105.14367", "submitter": "Mazharul Islam", "authors": "Bing Chen, Mazharul Islam, Lin Wang, Jisuo Gao and Jeff Orchard", "title": "Deconvolutional Density Network: Free-Form Conditional Density\n  Estimation", "comments": "11 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional density estimation is the task of estimating the probability of\nan event, conditioned on some inputs. A neural network can be used to compute\nthe output distribution explicitly. For such a task, there are many ways to\nrepresent a continuous-domain distribution using the output of a neural\nnetwork, but each comes with its own limitations for what distributions it can\naccurately render. If the family of functions is too restrictive, it will not\nbe appropriate for many datasets. In this paper, we demonstrate the benefits of\nmodeling free-form distributions using deconvolution. It has the advantage of\nbeing flexible, but also takes advantage of the topological smoothness offered\nby the deconvolution layers. We compare our method to a number of other\ndensity-estimation approaches, and show that our Deconvolutional Density\nNetwork (DDN) outperforms the competing methods on many artificial and real\ntasks, without committing to a restrictive parametric model.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 20:09:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Bing", ""], ["Islam", "Mazharul", ""], ["Wang", "Lin", ""], ["Gao", "Jisuo", ""], ["Orchard", "Jeff", ""]]}, {"id": "2105.14368", "submitter": "Mikhail Belkin", "authors": "Mikhail Belkin", "title": "Fit without fear: remarkable mathematical phenomena of deep learning\n  through the prism of interpolation", "comments": "A version of this paper will appear in Acta Numerica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past decade the mathematical theory of machine learning has lagged far\nbehind the triumphs of deep neural networks on practical challenges. However,\nthe gap between theory and practice is gradually starting to close. In this\npaper I will attempt to assemble some pieces of the remarkable and still\nincomplete mathematical mosaic emerging from the efforts to understand the\nfoundations of deep learning. The two key themes will be interpolation, and its\nsibling, over-parameterization. Interpolation corresponds to fitting data, even\nnoisy data, exactly. Over-parameterization enables interpolation and provides\nflexibility to select a right interpolating model.\n  As we will see, just as a physical prism separates colors mixed within a ray\nof light, the figurative prism of interpolation helps to disentangle\ngeneralization and optimization properties within the complex picture of modern\nMachine Learning. This article is written with belief and hope that clearer\nunderstanding of these issues brings us a step closer toward a general theory\nof deep learning and machine learning.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 20:15:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Belkin", "Mikhail", ""]]}, {"id": "2105.14370", "submitter": "Ma Bing", "authors": "Deng Yongqiang, Wang Dengjiang, Cao Gang, Ma Bing, Guan Xijia, Wang\n  Yajun, Liu Jianchao, Fang Yanming, Li Juanjuan", "title": "BAAI-VANJEE Roadside Dataset: Towards the Connected Automated Vehicle\n  Highway technologies in Challenging Environments of China", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the roadside perception plays an increasingly significant role in the\nConnected Automated Vehicle Highway(CAVH) technologies, there are immediate\nneeds of challenging real-world roadside datasets for bench marking and\ntraining various computer vision tasks such as 2D/3D object detection and\nmulti-sensor fusion. In this paper, we firstly introduce a challenging\nBAAI-VANJEE roadside dataset which consist of LiDAR data and RGB images\ncollected by VANJEE smart base station placed on the roadside about 4.5m high.\nThis dataset contains 2500 frames of LiDAR data, 5000 frames of RGB images,\nincluding 20% collected at the same time. It also contains 12 classes of\nobjects, 74K 3D object annotations and 105K 2D object annotations. By providing\na real complex urban intersections and highway scenes, we expect the\nBAAI-VANJEE roadside dataset will actively assist the academic and industrial\ncircles to accelerate the innovation research and achievement transformation in\nthe field of intelligent transportation in big data era.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 20:40:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yongqiang", "Deng", ""], ["Dengjiang", "Wang", ""], ["Gang", "Cao", ""], ["Bing", "Ma", ""], ["Xijia", "Guan", ""], ["Yajun", "Wang", ""], ["Jianchao", "Liu", ""], ["Yanming", "Fang", ""], ["Juanjuan", "Li", ""]]}, {"id": "2105.14372", "submitter": "Anthony Gitter", "authors": "Benjamin D. Lee, Anthony Gitter, Casey S. Greene, Sebastian Raschka,\n  Finlay Maguire, Alexander J. Titus, Michael D. Kessler, Alexandra J. Lee,\n  Marc G. Chevrette, Paul Allen Stewart, Thiago Britto-Borges, Evan M. Cofer,\n  Kun-Hsing Yu, Juan Jose Carmona, Elana J. Fertig, Alexandr A. Kalinin, Beth\n  Signal, Benjamin J. Lengerich, Timothy J. Triche Jr, Simina M. Boca", "title": "Ten Quick Tips for Deep Learning in Biology", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning is a modern approach to problem-solving and task automation.\nIn particular, machine learning is concerned with the development and\napplications of algorithms that can recognize patterns in data and use them for\npredictive modeling. Artificial neural networks are a particular class of\nmachine learning algorithms and models that evolved into what is now described\nas deep learning. Given the computational advances made in the last decade,\ndeep learning can now be applied to massive data sets and in innumerable\ncontexts. Therefore, deep learning has become its own subfield of machine\nlearning. In the context of biological research, it has been increasingly used\nto derive novel insights from high-dimensional biological data. To make the\nbiological applications of deep learning more accessible to scientists who have\nsome experience with machine learning, we solicited input from a community of\nresearchers with varied biological and deep learning interests. These\nindividuals collaboratively contributed to this manuscript's writing using the\nGitHub version control platform and the Manubot manuscript generation toolset.\nThe goal was to articulate a practical, accessible, and concise set of\nguidelines and suggestions to follow when using deep learning. In the course of\nour discussions, several themes became clear: the importance of understanding\nand applying machine learning fundamentals as a baseline for utilizing deep\nlearning, the necessity for extensive model comparisons with careful\nevaluation, and the need for critical thought in interpreting results generated\nby deep learning, among others.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 21:02:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lee", "Benjamin D.", ""], ["Gitter", "Anthony", ""], ["Greene", "Casey S.", ""], ["Raschka", "Sebastian", ""], ["Maguire", "Finlay", ""], ["Titus", "Alexander J.", ""], ["Kessler", "Michael D.", ""], ["Lee", "Alexandra J.", ""], ["Chevrette", "Marc G.", ""], ["Stewart", "Paul Allen", ""], ["Britto-Borges", "Thiago", ""], ["Cofer", "Evan M.", ""], ["Yu", "Kun-Hsing", ""], ["Carmona", "Juan Jose", ""], ["Fertig", "Elana J.", ""], ["Kalinin", "Alexandr A.", ""], ["Signal", "Beth", ""], ["Lengerich", "Benjamin J.", ""], ["Triche", "Timothy J.", "Jr"], ["Boca", "Simina M.", ""]]}, {"id": "2105.14385", "submitter": "Youbang Sun", "authors": "Youbang Sun, Mahyar Fazlyab, Shahin Shahrampour", "title": "On Centralized and Distributed Mirror Descent: Exponential Convergence\n  Analysis Using Quadratic Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mirror descent (MD) is a powerful first-order optimization technique that\nsubsumes several optimization algorithms including gradient descent (GD). In\nthis work, we study the exact convergence rate of MD in both centralized and\ndistributed cases for strongly convex and smooth problems. We view MD with a\ndynamical system lens and leverage quadratic constraints (QCs) to provide\nconvergence guarantees based on the Lyapunov stability. For centralized MD, we\nestablish a semi-definite programming (SDP) that certifies exponentially fast\nconvergence of MD subject to a linear matrix inequality (LMI). We prove that\nthe SDP always has a feasible solution that recovers the optimal GD rate. Next,\nwe analyze the exponential convergence of distributed MD and characterize the\nrate using two LMIs. To the best of our knowledge, the exact (exponential) rate\nof distributed MD has not been previously explored in the literature. We\npresent numerical results as a verification of our theory and observe that the\nrichness of the Lyapunov function entails better (worst-case) convergence rates\ncompared to existing works on distributed GD.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 23:05:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sun", "Youbang", ""], ["Fazlyab", "Mahyar", ""], ["Shahrampour", "Shahin", ""]]}, {"id": "2105.14396", "submitter": "Carlos Magno C. O. Valle", "authors": "Carlos Magno C. O. Valle, Sami Haddadin", "title": "SyReNets: Symbolic Residual Neural Networks", "comments": "11 pages, 3 figures, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite successful seminal works on passive systems in the literature,\nlearning free-form physical laws for controlled dynamical systems given\nexperimental data is still an open problem. For decades, symbolic mathematical\nequations and system identification were the golden standards. Unfortunately, a\nset of assumptions about the properties of the underlying system is required,\nwhich makes the model very rigid and unable to adapt to unforeseen changes in\nthe physical system. Neural networks, on the other hand, are known universal\nfunction approximators but are prone to over-fit, limited accuracy, and bias\nproblems, which makes them alone unreliable candidates for such tasks. In this\npaper, we propose SyReNets, an approach that leverages neural networks for\nlearning symbolic relations to accurately describe dynamic physical systems\nfrom data. It explores a sequence of symbolic layers that build, in a residual\nmanner, mathematical relations that describes a given desired output from input\nvariables. We apply it to learn the symbolic equation that describes the\nLagrangian of a given physical system. We do this by only observing random\nsamples of position, velocity, and acceleration as input and torque as output.\nTherefore, using the Lagrangian as a latent representation from which we derive\ntorque using the Euler-Lagrange equations. The approach is evaluated using a\nsimulated controlled double pendulum and compared with neural networks, genetic\nprogramming, and traditional system identification. The results demonstrate\nthat, compared to neural networks and genetic programming, SyReNets converges\nto representations that are more accurate and precise throughout the state\nspace. Despite having slower convergence than traditional system\nidentification, similar to neural networks, the approach remains flexible\nenough to adapt to an unforeseen change in the physical system structure.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:30:27 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Valle", "Carlos Magno C. O.", ""], ["Haddadin", "Sami", ""]]}, {"id": "2105.14398", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, Ivan Vuli\\'c, Anna Korhonen, Nigel Collier", "title": "Learning Domain-Specialised Representations for Cross-Lingual Biomedical\n  Entity Linking", "comments": "ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Injecting external domain-specific knowledge (e.g., UMLS) into pretrained\nlanguage models (LMs) advances their capability to handle specialised in-domain\ntasks such as biomedical entity linking (BEL). However, such abundant expert\nknowledge is available only for a handful of languages (e.g., English). In this\nwork, by proposing a novel cross-lingual biomedical entity linking task\n(XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically\ndiverse languages, we first investigate the ability of standard\nknowledge-agnostic as well as knowledge-enhanced monolingual and multilingual\nLMs beyond the standard monolingual English BEL task. The scores indicate large\ngaps to English performance. We then address the challenge of transferring\ndomain-specific knowledge in resource-rich languages to resource-poor ones. To\nthis end, we propose and evaluate a series of cross-lingual transfer methods\nfor the XL-BEL task, and demonstrate that general-domain bitext helps propagate\nthe available English knowledge to languages with little to no in-domain data.\nRemarkably, we show that our proposed domain-specific transfer methods yield\nconsistent gains across all target languages, sometimes up to 20 Precision@1\npoints, without any in-domain knowledge in the target language, and without any\nin-domain parallel data.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:50:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Fangyu", ""], ["Vuli\u0107", "Ivan", ""], ["Korhonen", "Anna", ""], ["Collier", "Nigel", ""]]}, {"id": "2105.14399", "submitter": "David Mac\\^edo", "authors": "David Mac\\^edo, Teresa Ludermir", "title": "Improving Entropic Out-of-Distribution Detection using Isometric\n  Distances and the Minimum Distance Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Code available at\n$\\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\\text{entropic\nout-of-distribution detection}}$.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:55:03 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:18:07 GMT"}, {"version": "v3", "created": "Sun, 18 Jul 2021 04:02:03 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mac\u00eado", "David", ""], ["Ludermir", "Teresa", ""]]}, {"id": "2105.14403", "submitter": "Ryoma Sato", "authors": "Ryoma Sato, Makoto Yamada, Hisashi Kashima", "title": "Re-evaluating Word Mover's Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The word mover's distance (WMD) is a fundamental technique for measuring the\nsimilarity of two documents. As the crux of WMD, it can take advantage of the\nunderlying geometry of the word space by employing an optimal transport\nformulation. The original study on WMD reported that WMD outperforms classical\nbaselines such as bag-of-words (BOW) and TF-IDF by significant margins in\nvarious datasets. In this paper, we point out that the evaluation in the\noriginal study could be misleading. We re-evaluate the performances of WMD and\nthe classical baselines and find that the classical baselines are competitive\nwith WMD if we employ an appropriate preprocessing, i.e., L1 normalization.\nHowever, this result is not intuitive. WMD should be superior to BOW because\nWMD can take the underlying geometry into account, whereas BOW cannot. Our\nanalysis shows that this is due to the high-dimensional nature of the\nunderlying metric. We find that WMD in high-dimensional spaces behaves more\nsimilarly to BOW than in low-dimensional spaces due to the curse of\ndimensionality.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 01:35:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sato", "Ryoma", ""], ["Yamada", "Makoto", ""], ["Kashima", "Hisashi", ""]]}, {"id": "2105.14409", "submitter": "Niharika S. D'Souza", "authors": "Niharika Shimona D'Souza, Mary Beth Nebel, Deana Crocetti, Nicholas\n  Wymbs, Joshua Robinson, Stewart Mostofsky, Archana Venkataraman", "title": "A Matrix Autoencoder Framework to Align the Functional and Structural\n  Connectivity Manifolds as Guided by Behavioral Phenotypes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel matrix autoencoder to map functional connectomes from\nresting state fMRI (rs-fMRI) to structural connectomes from Diffusion Tensor\nImaging (DTI), as guided by subject-level phenotypic measures. Our specialized\nautoencoder infers a low dimensional manifold embedding for the rs-fMRI\ncorrelation matrices that mimics a canonical outer-product decomposition. The\nembedding is simultaneously used to reconstruct DTI tractography matrices via a\nsecond manifold alignment decoder and to predict inter-subject phenotypic\nvariability via an artificial neural network. We validate our framework on a\ndataset of 275 healthy individuals from the Human Connectome Project database\nand on a second clinical dataset consisting of 57 subjects with Autism Spectrum\nDisorder. We demonstrate that the model reliably recovers structural\nconnectivity patterns across individuals, while robustly extracting predictive\nand interpretable brain biomarkers in a cross-validated setting. Finally, our\nframework outperforms several baselines at predicting behavioral phenotypes in\nboth real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 02:06:12 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 21:59:34 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["D'Souza", "Niharika Shimona", ""], ["Nebel", "Mary Beth", ""], ["Crocetti", "Deana", ""], ["Wymbs", "Nicholas", ""], ["Robinson", "Joshua", ""], ["Mostofsky", "Stewart", ""], ["Venkataraman", "Archana", ""]]}, {"id": "2105.14410", "submitter": "Juntao Huang", "authors": "Juntao Huang, Yingda Cheng, Andrew J. Christlieb, Luke F. Roberts,\n  Wen-An Yong", "title": "Machine learning moment closure models for the radiative transfer\n  equation II: enforcing global hyperbolicity in gradient based closures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is the second paper in a series in which we develop machine learning\n(ML) moment closure models for the radiative transfer equation (RTE). In our\nprevious work \\cite{huang2021gradient}, we proposed an approach to directly\nlearn the gradient of the unclosed high order moment, which performs much\nbetter than learning the moment itself and the conventional $P_N$ closure.\nHowever, the ML moment closure model in \\cite{huang2021gradient} is not able to\nguarantee hyperbolicity and long time stability. We propose in this paper a\nmethod to enforce the global hyperbolicity of the ML closure model. The main\nidea is to seek a symmetrizer (a symmetric positive definite matrix) for the\nclosure system, and derive constraints such that the system is globally\nsymmetrizable hyperbolic. It is shown that the new ML closure system inherits\nthe dissipativeness of the RTE and preserves the correct diffusion limit as the\nKnunsden number goes to zero. Several benchmark tests including the Gaussian\nsource problem and the two-material problem show the good accuracy, long time\nstability and generalizability of our globally hyperbolic ML closure model.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 02:10:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Juntao", ""], ["Cheng", "Yingda", ""], ["Christlieb", "Andrew J.", ""], ["Roberts", "Luke F.", ""], ["Yong", "Wen-An", ""]]}, {"id": "2105.14412", "submitter": "Andrei Velichko", "authors": "Hanif Heidari and Andrei Velichko", "title": "An improved LogNNet classifier for IoT application", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet of things devices suffer of low memory while good accuracy is\nneeded. Designing suitable algorithms is vital in this subject. This paper\nproposes a feed forward LogNNet neural network which uses a semi-linear Henon\ntype discrete chaotic map to classify MNIST-10 dataset. The model is composed\nof reservoir part and trainable classifier. The aim of reservoir part is\ntransforming the inputs to maximize the classification accuracy using a special\nmatrix filing method and a time series generated by the chaotic map. The\nparameters of the chaotic map are optimized using particle swarm optimization\nwith random immigrants. The results show that the proposed LogNNet/Henon\nclassifier has higher accuracy and same RAM saving comparable to the original\nversion of LogNNet and has broad prospects for implementation in IoT devices.\nIn addition, the relation between the entropy and accuracy of the\nclassification is investigated. It is shown that there exists a direct relation\nbetween the value of entropy and accuracy of the classification.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 02:12:45 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Heidari", "Hanif", ""], ["Velichko", "Andrei", ""]]}, {"id": "2105.14417", "submitter": "Zhiyan Ding", "authors": "Zhiyan Ding and Shi Chen and Qin Li and Stephen Wright", "title": "Overparameterization of deep ResNet: zero loss and mean-field analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding parameters in a deep neural network (NN) that fit training data is a\nnonconvex optimization problem, but a basic first-order optimization method\n(gradient descent) finds a global solution with perfect fit in many practical\nsituations. We examine this phenomenon for the case of Residual Neural Networks\n(ResNet) with smooth activation functions in a limiting regime in which both\nthe number of layers (depth) and the number of neurons in each layer (width) go\nto infinity. First, we use a mean-field-limit argument to prove that the\ngradient descent for parameter training becomes a partial differential equation\n(PDE) that characterizes gradient flow for a probability distribution in the\nlarge-NN limit. Next, we show that the solution to the PDE converges in the\ntraining time to a zero-loss solution. Together, these results imply that\ntraining of the ResNet also gives a near-zero loss if the Resnet is large\nenough. We give estimates of the depth and width needed to reduce the loss\nbelow a given threshold, with high probability.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 02:46:09 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 18:57:16 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ding", "Zhiyan", ""], ["Chen", "Shi", ""], ["Li", "Qin", ""], ["Wright", "Stephen", ""]]}, {"id": "2105.14422", "submitter": "Hengrui Cai", "authors": "Hengrui Cai, Zhihao Cen, Ling Leng, Rui Song", "title": "Periodic-GP: Learning Periodic World with Gaussian Process Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sequential decision optimization on the periodic environment,\nthat occurs in a wide variety of real-world applications when the data involves\nseasonality, such as the daily demand of drivers in ride-sharing and dynamic\ntraffic patterns in transportation. In this work, we focus on learning the\nstochastic periodic world by leveraging this seasonal law. To deal with the\ngeneral action space, we use the bandit based on Gaussian process (GP) as the\nbase model due to its flexibility and generality, and propose the Periodic-GP\nmethod with a temporal periodic kernel based on the upper confidence bound.\nTheoretically, we provide a new regret bound of the proposed method, by\nexplicitly characterizing the periodic kernel in the periodic stationary model.\nEmpirically, the proposed algorithm significantly outperforms the existing\nmethods in both synthetic data experiments and a real data application on\nMadrid traffic pollution.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 03:40:16 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 20:14:01 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 03:09:01 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cai", "Hengrui", ""], ["Cen", "Zhihao", ""], ["Leng", "Ling", ""], ["Song", "Rui", ""]]}, {"id": "2105.14444", "submitter": "Jin Xu", "authors": "Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, Tie-Yan\n  Liu", "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural\n  Architecture Search", "comments": "Accepted by KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467262", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While pre-trained language models (e.g., BERT) have achieved impressive\nresults on different natural language processing tasks, they have large numbers\nof parameters and suffer from big computational and memory costs, which make\nthem difficult for real-world deployment. Therefore, model compression is\nnecessary to reduce the computation and memory cost of pre-trained models. In\nthis work, we aim to compress BERT and address the following two challenging\npractical issues: (1) The compression algorithm should be able to output\nmultiple compressed models with different sizes and latencies, in order to\nsupport devices with different memory and latency limitations; (2) The\nalgorithm should be downstream task agnostic, so that the compressed models are\ngenerally applicable for different downstream tasks. We leverage techniques in\nneural architecture search (NAS) and propose NAS-BERT, an efficient method for\nBERT compression. NAS-BERT trains a big supernet on a search space containing a\nvariety of architectures and outputs multiple compressed models with adaptive\nsizes and latency. Furthermore, the training of NAS-BERT is conducted on\nstandard self-supervised pre-training tasks (e.g., masked language model) and\ndoes not depend on specific downstream tasks. Thus, the compressed models can\nbe used across various downstream tasks. The technical challenge of NAS-BERT is\nthat training a big supernet on the pre-training task is extremely costly. We\nemploy several techniques including block-wise search, search space pruning,\nand performance approximation to improve search efficiency and accuracy.\nExtensive experiments on GLUE and SQuAD benchmark datasets demonstrate that\nNAS-BERT can find lightweight models with better accuracy than previous\napproaches, and can be directly applied to different downstream tasks with\nadaptive model sizes for different requirements of memory or latency.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 07:20:27 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Jin", ""], ["Tan", "Xu", ""], ["Luo", "Renqian", ""], ["Song", "Kaitao", ""], ["Li", "Jian", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2105.14450", "submitter": "Yang You", "authors": "Zhengda Bian and Qifan Xu and Boxiang Wang and Yang You", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks", "comments": "Technical Report of NUS HPC-AI Lab (https://ai.comp.nus.edu.sg). The\n  leading two authors have equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent Natural Language Processing techniques have been refreshing the\nstate-of-the-art performance at an incredible speed. Training huge language\nmodels is therefore an imperative demand in both industry and academy. However,\nhuge language models impose challenges to both hardware and software. Graphical\nprocessing units (GPUs) are iterated frequently to meet the exploding demand,\nand a variety of ASICs like TPUs are spawned. However, there is still a tension\nbetween the fast growth of the extremely huge models and the fact that Moore's\nlaw is approaching the end. To this end, many model parallelism techniques are\nproposed to distribute the model parameters to multiple devices, so as to\nalleviate the tension on both memory and computation. Our work is the first to\nintroduce a 3-dimensional model parallelism for expediting huge language\nmodels. By reaching a perfect load balance, our approach presents smaller\nmemory and communication cost than existing state-of-the-art 1-D and 2-D model\nparallelism. Our experiments on 64 TACC's V100 GPUs show that our 3-D\nparallelism outperforms the 1-D and 2-D parallelism with 2.32x and 1.57x\nspeedup, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 07:41:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bian", "Zhengda", ""], ["Xu", "Qifan", ""], ["Wang", "Boxiang", ""], ["You", "Yang", ""]]}, {"id": "2105.14476", "submitter": "Xianyuan Zhan", "authors": "Huiling Qin, Xianyuan Zhan, Yu Zheng", "title": "CSCAD: Correlation Structure-based Collective Anomaly Detection in\n  Complex System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting anomalies in large complex systems is a critical and challenging\ntask. The difficulties arise from several aspects. First, collecting ground\ntruth labels or prior knowledge for anomalies is hard in real-world systems,\nwhich often lead to limited or no anomaly labels in the dataset. Second,\nanomalies in large systems usually occur in a collective manner due to the\nunderlying dependency structure among devices or sensors. Lastly, real-time\nanomaly detection for high-dimensional data requires efficient algorithms that\nare capable of handling different types of data (i.e. continuous and discrete).\nWe propose a correlation structure-based collective anomaly detection (CSCAD)\nmodel for high-dimensional anomaly detection problem in large systems, which is\nalso generalizable to semi-supervised or supervised settings. Our framework\nutilize graph convolutional network combining a variational autoencoder to\njointly exploit the feature space correlation and reconstruction deficiency of\nsamples to perform anomaly detection. We propose an extended mutual information\n(EMI) metric to mine the internal correlation structure among different data\nfeatures, which enhances the data reconstruction capability of CSCAD. The\nreconstruction loss and latent standard deviation vector of a sample obtained\nfrom reconstruction network can be perceived as two natural anomalous degree\nmeasures. An anomaly discriminating network can then be trained using low\nanomalous degree samples as positive samples, and high anomalous degree samples\nas negative samples. Experimental results on five public datasets demonstrate\nthat our approach consistently outperforms all the competing baselines.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:28:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Qin", "Huiling", ""], ["Zhan", "Xianyuan", ""], ["Zheng", "Yu", ""]]}, {"id": "2105.14490", "submitter": "Ailing Zeng", "authors": "Ailing Zeng, Minhao Liu, Zhiwei Liu, Ruiyuan Gao, Qiang Xu", "title": "Hop-Aware Dimension Optimization for Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Graph Neural Networks (GNNs), the embedding of each node is obtained by\naggregating information with its direct and indirect neighbors. As the messages\npassed among nodes contain both information and noise, the critical issue in\nGNN representation learning is how to retrieve information effectively while\nsuppressing noise. Generally speaking, interactions with distant nodes usually\nintroduce more noise for a particular node than those with close nodes.\nHowever, in most existing works, the messages being passed among nodes are\nmingled together, which is inefficient from a communication perspective. Mixing\nthe information from clean sources (low-order neighbors) and noisy sources\n(high-order neighbors) makes discriminative feature extraction challenging.\nMotivated by the above, we propose a simple yet effective ladder-style GNN\narchitecture, namely LADDER-GNN. Specifically, we separate messages from\ndifferent hops and assign different dimensions for them before concatenating\nthem to obtain the node representation. Such disentangled representations\nfacilitate extracting information from messages passed from different hops, and\ntheir corresponding dimensions are determined with a reinforcement\nlearning-based neural architecture search strategy. The resulted hop-aware\nrepresentations generally contain more dimensions for low-order neighbors and\nfewer dimensions for high-order neighbors, leading to a ladder-style\naggregation scheme. We verify the proposed LADDER-GNN on several\nsemi-supervised node classification datasets. Experimental results show that\nthe proposed simple hop-aware representation learning solution can achieve\nstate-of-the-art performance on most datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 10:12:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zeng", "Ailing", ""], ["Liu", "Minhao", ""], ["Liu", "Zhiwei", ""], ["Gao", "Ruiyuan", ""], ["Xu", "Qiang", ""]]}, {"id": "2105.14491", "submitter": "Shaked Brody", "authors": "Shaked Brody, Uri Alon, Eran Yahav", "title": "How Attentive are Graph Attention Networks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Attention Networks (GATs) are one of the most popular GNN architectures\nand are considered as the state-of-the-art architecture for representation\nlearning with graphs. In GAT, every node attends to its neighbors given its own\nrepresentation as the query. However, in this paper we show that GATs can only\ncompute a restricted kind of attention where the ranking of attended nodes is\nunconditioned on the query node. We formally define this restricted kind of\nattention as static attention and distinguish it from a strictly more\nexpressive dynamic attention. Because GATs use a static attention mechanism,\nthere are simple graph problems that GAT cannot express: in a controlled\nproblem, we show that static attention hinders GAT from even fitting the\ntraining data. To remove this limitation, we introduce a simple fix by\nmodifying the order of operations and propose GATv2: a dynamic graph attention\nvariant that is strictly more expressive than GAT. We perform an extensive\nevaluation and show that GATv2 outperforms GAT across 11 OGB and other\nbenchmarks while we match their parametric costs. Our code is available at\nhttps://github.com/tech-srl/how_attentive_are_gats .\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 10:17:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Brody", "Shaked", ""], ["Alon", "Uri", ""], ["Yahav", "Eran", ""]]}, {"id": "2105.14493", "submitter": "Tamer Olmez", "authors": "Nuri Korkan, Tamer Olmez, Zumray Dokur", "title": "Generating Ten BCI Commands Using Four Simple Motor Imageries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The brain computer interface (BCI) systems are utilized for transferring\ninformation among humans and computers by analyzing electroencephalogram (EEG)\nrecordings.The process of mentally previewing a motor movement without\ngenerating the corporal output can be described as motor imagery (MI).In this\nemerging research field, the number of commands is also limited in relation to\nthe number of MI tasks; in the current literature, mostly two or four commands\n(classes) are studied. As a solution to this problem, it is recommended to use\nmental tasks as well as MI tasks. Unfortunately, the use of this approach\nreduces the classification performance of MI EEG signals. The fMRI analyses\nshow that the resources in the brain associated with the motor imagery can be\nactivated independently. It is assumed that the brain activity induced by the\nMI of the combination of body parts corresponds to the superposition of the\nactivities generated during each body parts's simple MI. In this study, in\norder to create more than four BCI commands, we suggest to generate combined MI\nEEG signals artificially by using left hand, right hand, tongue, and feet motor\nimageries in pairs. A maximum of ten different BCI commands can be generated by\nusing four motor imageries in pairs.This study aims to achieve high\nclassification performances for BCI commands produced from four motor imageries\nby implementing a small-sized deep neural network (DNN).The presented method is\nevaluated on the four-class datasets of BCI Competitions III and IV, and an\naverage classification performance of 81.8% is achieved for ten classes. The\nabove assumption is also validated on a different dataset which consists of\nsimple and combined MI EEG signals acquired in real time. Trained with the\nartificially generated combined MI EEG signals, DivFE resulted in an average of\n76.5% success rate for the combined MI EEG signals acquired in real-time.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 10:34:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Korkan", "Nuri", ""], ["Olmez", "Tamer", ""], ["Dokur", "Zumray", ""]]}, {"id": "2105.14500", "submitter": "Boxiang Wang", "authors": "Boxiang Wang, Qifan Xu, Zhengda Bian, Yang You", "title": "2.5-dimensional distributed model training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallelism does a good job in speeding up the training. However, when\nit comes to the case when the memory of a single device can not host a whole\nmodel, data parallelism would not have the chance to do anything. Another\noption is to split the model by operator, or horizontally. Megatron-LM\nintroduced a 1-Dimensional distributed method to use GPUs to speed up the\ntraining process. Optimus is a 2D solution for distributed tensor parallelism.\nHowever, these methods have a high communication overhead and a low scaling\nefficiency on large-scale computing clusters. To solve this problem, we\ninvestigate the 2.5-Dimensional distributed tensor parallelism.Introduced by\nSolomonik et al., 2.5-Dimensional Matrix Multiplication developed an effective\nmethod to perform multiple Cannon's algorithm at the same time to increase the\nefficiency. With many restrictions of Cannon's Algorithm and a huge amount of\nshift operation, we need to invent a new method of 2.5-dimensional matrix\nmultiplication to enhance the performance. Absorbing the essence from both\nSUMMA and 2.5-Dimensional Matrix Multiplication, we introduced SUMMA2.5-LM for\nlanguage models to overcome the abundance of unnecessary transmission loss\nresult from the increasing size of language model parallelism. Compared to\nprevious 1D and 2D model parallelization of language models, our SUMMA2.5-LM\nmanaged to reduce the transmission cost on each layer, which could get a 1.45X\nefficiency according to our weak scaling result between 2.5-D [4,4,4]\narrangement and 2-D [8,8,1] arrangement.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 11:06:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Boxiang", ""], ["Xu", "Qifan", ""], ["Bian", "Zhengda", ""], ["You", "Yang", ""]]}, {"id": "2105.14506", "submitter": "Jivitesh Sharma", "authors": "Jivitesh Sharma, Rohan Yadav, Ole-Christoffer Granmo and Lei Jiao", "title": "Human Interpretable AI: Enhancing Tsetlin Machine Stochasticity with\n  Drop Clause", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, we introduce a novel variant of the Tsetlin machine (TM)\nthat randomly drops clauses, the key learning elements of a TM. In effect, TM\nwith drop clause ignores a random selection of the clauses in each epoch,\nselected according to a predefined probability. In this way, additional\nstochasticity is introduced in the learning phase of TM. Along with producing\nmore distinct and well-structured patterns that improve the performance, we\nalso show that dropping clauses increases learning robustness. To explore the\neffects clause dropping has on accuracy, training time, and interpretability,\nwe conduct extensive experiments on various benchmark datasets in natural\nlanguage processing (NLP) (IMDb and SST2) as well as computer vision (MNIST and\nCIFAR10). In brief, we observe from +2% to +4% increase in accuracy and 2x to\n4x faster learning. We further employ the Convolutional TM to document\ninterpretable results on the CIFAR10 dataset. To the best of our knowledge,\nthis is the first time an interpretable machine learning algorithm has been\nused to produce pixel-level human-interpretable results on CIFAR10. Also,\nunlike previous interpretable methods that focus on attention visualisation or\ngradient interpretability, we show that the TM is a more general interpretable\nmethod. That is, by producing rule-based propositional logic expressions that\nare \\emph{human}-interpretable, the TM can explain how it classifies a\nparticular instance at the pixel level for computer vision and at the word\nlevel for NLP.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 11:29:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sharma", "Jivitesh", ""], ["Yadav", "Rohan", ""], ["Granmo", "Ole-Christoffer", ""], ["Jiao", "Lei", ""]]}, {"id": "2105.14519", "submitter": "Xiongshi Deng", "authors": "Xiongshi Deng, Min Li, Lei Wang, Qikang Wan", "title": "RFCBF: enhance the performance and stability of Fast Correlation-Based\n  Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a preprocessing step which plays a crucial role in the\ndomain of machine learning and data mining. Feature selection methods have been\nshown to be effctive in removing redundant and irrelevant features, improving\nthe learning algorithm's prediction performance. Among the various methods of\nfeature selection based on redundancy, the fast correlation-based filter (FCBF)\nis one of the most effective. In this paper, we proposed a novel extension of\nFCBF, called RFCBF, which combines resampling technique to improve\nclassification accuracy. We performed comprehensive experiments to compare the\nRFCBF with other state-of-the-art feature selection methods using the KNN\nclassifier on 12 publicly available data sets. The experimental results show\nthat the RFCBF algorithm yields significantly better results than previous\nstate-of-the-art methods in terms of classification accuracy and runtime.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 12:36:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Deng", "Xiongshi", ""], ["Li", "Min", ""], ["Wang", "Lei", ""], ["Wan", "Qikang", ""]]}, {"id": "2105.14524", "submitter": "Jiwei Li", "authors": "Chun Fan, Yuxian Meng, Xiaofei Sun, Fei Wu, Tianwei Zhang, Jiwei Li", "title": "Parameter Estimation for the SEIR Model Using Recurrent Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The standard way to estimate the parameters $\\Theta_\\text{SEIR}$ (e.g., the\ntransmission rate $\\beta$) of an SEIR model is to use grid search, where\nsimulations are performed on each set of parameters, and the parameter set\nleading to the least $L_2$ distance between predicted number of infections and\nobserved infections is selected. This brute-force strategy is not only time\nconsuming, as simulations are slow when the population is large, but also\ninaccurate, since it is impossible to enumerate all parameter combinations. To\naddress these issues, in this paper, we propose to transform the\nnon-differentiable problem of finding optimal $\\Theta_\\text{SEIR}$ to a\ndifferentiable one, where we first train a recurrent net to fit a small number\nof simulation data. Next, based on this recurrent net that is able to\ngeneralize SEIR simulations, we are able to transform the objective to a\ndifferentiable one with respect to $\\Theta_\\text{SEIR}$, and straightforwardly\nobtain its optimal value. The proposed strategy is both time efficient as it\nonly relies on a small number of SEIR simulations, and accurate as we are able\nto find the optimal $\\Theta_\\text{SEIR}$ based on the differentiable objective.\nOn two COVID-19 datasets, we observe that the proposed strategy leads to\nsignificantly better parameter estimations with a smaller number of\nsimulations.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 12:51:45 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Fan", "Chun", ""], ["Meng", "Yuxian", ""], ["Sun", "Xiaofei", ""], ["Wu", "Fei", ""], ["Zhang", "Tianwei", ""], ["Li", "Jiwei", ""]]}, {"id": "2105.14526", "submitter": "Nikhil Iyer", "authors": "Nikhil Iyer, V Thejas, Nipun Kwatra, Ramachandran Ramjee, Muthian\n  Sivathanu", "title": "LRTuner: A Learning Rate Tuner for Deep Neural Networks", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One very important hyperparameter for training deep neural networks is the\nlearning rate schedule of the optimizer. The choice of learning rate schedule\ndetermines the computational cost of getting close to a minima, how close you\nactually get to the minima, and most importantly the kind of local minima\n(wide/narrow) attained. The kind of minima attained has a significant impact on\nthe generalization accuracy of the network. Current systems employ hand tuned\nlearning rate schedules, which are painstakingly tuned for each network and\ndataset. Given that the state space of schedules is huge, finding a\nsatisfactory learning rate schedule can be very time consuming. In this paper,\nwe present LRTuner, a method for tuning the learning rate as training proceeds.\nOur method works with any optimizer, and we demonstrate results on SGD with\nMomentum, and Adam optimizers.\n  We extensively evaluate LRTuner on multiple datasets, models, and across\noptimizers. We compare favorably against standard learning rate schedules for\nthe given dataset and models, including ImageNet on Resnet-50, Cifar-10 on\nResnet-18, and SQuAD fine-tuning on BERT. For example on ImageNet with\nResnet-50, LRTuner shows up to 0.2% absolute gains in test accuracy compared to\nthe hand-tuned baseline schedule. Moreover, LRTuner can achieve the same\naccuracy as the baseline schedule in 29% less optimization steps.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:06:26 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Iyer", "Nikhil", ""], ["Thejas", "V", ""], ["Kwatra", "Nipun", ""], ["Ramjee", "Ramachandran", ""], ["Sivathanu", "Muthian", ""]]}, {"id": "2105.14529", "submitter": "Changjian Shui", "authors": "Changjian Shui, Boyu Wang, Christian Gagn\\'e", "title": "On the benefits of representation regularization in invariance based\n  domain generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A crucial aspect in reliable machine learning is to design a deployable\nsystem in generalizing new related but unobserved environments. Domain\ngeneralization aims to alleviate such a prediction gap between the observed and\nunseen environments. Previous approaches commonly incorporated learning\ninvariant representation for achieving good empirical performance. In this\npaper, we reveal that merely learning invariant representation is vulnerable to\nthe unseen environment. To this end, we derive novel theoretical analysis to\ncontrol the unseen test environment error in the representation learning, which\nhighlights the importance of controlling the smoothness of representation. In\npractice, our analysis further inspires an efficient regularization method to\nimprove the robustness in domain generalization. Our regularization is\northogonal to and can be straightforwardly adopted in existing domain\ngeneralization algorithms for invariant representation learning. Empirical\nresults show that our algorithm outperforms the base versions in various\ndataset and invariance criteria.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:13:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shui", "Changjian", ""], ["Wang", "Boyu", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "2105.14548", "submitter": "Gal Metzer", "authors": "Gal Metzer, Rana Hanocka, Raja Giryes, Niloy J. Mitra, Daniel Cohen-Or", "title": "Z2P: Instant Rendering of Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for rendering point clouds using a neural network.\nExisting point rendering techniques either use splatting, or first reconstruct\na surface mesh that can then be rendered. Both of these techniques require\nsolving for global point normal orientation, which is a challenging problem on\nits own. Furthermore, splatting techniques result in holes and overlaps,\nwhereas mesh reconstruction is particularly challenging, especially in the\ncases of thin surfaces and sheets.\n  We cast the rendering problem as a conditional image-to-image translation\nproblem. In our formulation, Z2P, i.e., depth-augmented point features as\nviewed from target camera view, are directly translated by a neural network to\nrendered images, conditioned on control variables (e.g., color, light). We\navoid inevitable issues with splatting (i.e., holes and overlaps), and bypass\nsolving the notoriously challenging surface reconstruction problem or\nestimating oriented normals. Yet, our approach results in a rendered image as\nif a surface mesh was reconstructed. We demonstrate that our framework produces\na plausible image, and can effectively handle noise, non-uniform sampling, thin\nsurfaces / sheets, and is fast.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:58:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Metzer", "Gal", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Mitra", "Niloy J.", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2105.14557", "submitter": "Chengbin Hou", "authors": "Chengbin Hou, Guoji Fu, Peng Yang, Shan He, Ke Tang", "title": "Robust Dynamic Network Embedding via Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Network Embedding (DNE) has recently attracted considerable attention\ndue to the advantage of network embedding in various applications and the\ndynamic nature of many real-world networks. For dynamic networks, the degree of\nchanges, i.e., defined as the averaged number of changed edges between\nconsecutive snapshots spanning a dynamic network, could be very different in\nreal-world scenarios. Although quite a few DNE methods have been proposed, it\nstill remains unclear that whether and to what extent the existing DNE methods\nare robust to the degree of changes, which is however an important factor in\nboth academic research and industrial applications. In this work, we\ninvestigate the robustness issue of DNE methods w.r.t. the degree of changes\nfor the first time and accordingly, propose a robust DNE method. Specifically,\nthe proposed method follows the notion of ensembles where the base learner\nadopts an incremental Skip-Gram neural embedding approach. To further boost the\nperformance, a novel strategy is proposed to enhance the diversity among base\nlearners at each timestep by capturing different levels of local-global\ntopology. Extensive experiments demonstrate the benefits of special designs in\nthe proposed method, and the superior performance of the proposed method\ncompared to state-of-the-art methods. The comparative study also reveals the\nrobustness issue of some DNE methods. The source code is available at\nhttps://github.com/houchengbin/SG-EDNE\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 14:44:26 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hou", "Chengbin", ""], ["Fu", "Guoji", ""], ["Yang", "Peng", ""], ["He", "Shan", ""], ["Tang", "Ke", ""]]}, {"id": "2105.14559", "submitter": "Jae Oh Woo", "authors": "Jae Oh Woo", "title": "BABA: Beta Approximation for Bayesian Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a new acquisition function under the Bayesian active\nlearning framework, namely BABA. It is motivated by previously well-established\nworks BALD, and BatchBALD which capture the mutual information between the\nmodel parameters and the predictive outputs of the data. Our proposed measure,\nBABA, endeavors to quantify the normalized mutual information by approximating\nthe stochasticity of predictive probabilities using Beta distributions. BABA\noutperforms the well-known family of acquisition functions, including BALD and\nBatchBALD. We demonstrate this by showing extensive experimental results\nobtained from MNIST and EMNIST datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 14:49:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Woo", "Jae Oh", ""]]}, {"id": "2105.14568", "submitter": "Ronald Pereira", "authors": "Ronald D. R. Pereira and Fabr\\'icio Murai", "title": "How effective are Graph Neural Networks in Fraud Detection for Network\n  Data?", "comments": "12 pages, in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph-based Neural Networks (GNNs) are recent models created for learning\nrepresentations of nodes (and graphs), which have achieved promising results\nwhen detecting patterns that occur in large-scale data relating different\nentities. Among these patterns, financial fraud stands out for its\nsocioeconomic relevance and for presenting particular challenges, such as the\nextreme imbalance between the positive (fraud) and negative (legitimate\ntransactions) classes, and the concept drift (i.e., statistical properties of\nthe data change over time). Since GNNs are based on message propagation, the\nrepresentation of a node is strongly impacted by its neighbors and by the\nnetwork's hubs, amplifying the imbalance effects. Recent works attempt to adapt\nundersampling and oversampling strategies for GNNs in order to mitigate this\neffect without, however, accounting for concept drift. In this work, we conduct\nexperiments to evaluate existing techniques for detecting network fraud,\nconsidering the two previous challenges. For this, we use real data sets,\ncomplemented by synthetic data created from a new methodology introduced here.\nBased on this analysis, we propose a series of improvement points that should\nbe investigated in future research.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 15:17:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pereira", "Ronald D. R.", ""], ["Murai", "Fabr\u00edcio", ""]]}, {"id": "2105.14573", "submitter": "Zhongwang Zhang", "authors": "Yaoyu Zhang, Zhongwang Zhang, Tao Luo, Zhi-Qin John Xu", "title": "Embedding Principle of Loss Landscape of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the structure of loss landscape of deep neural networks\n(DNNs)is obviously important. In this work, we prove an embedding principle\nthat the loss landscape of a DNN \"contains\" all the critical points of all the\nnarrower DNNs. More precisely, we propose a critical embedding such that any\ncritical point, e.g., local or global minima, of a narrower DNN can be embedded\nto a critical point/hyperplane of the target DNN with higher degeneracy and\npreserving the DNN output function. The embedding structure of critical points\nis independent of loss function and training data, showing a stark difference\nfrom other nonconvex problems such as protein-folding. Empirically, we find\nthat a wide DNN is often attracted by highly-degenerate critical points that\nare embedded from narrow DNNs. The embedding principle provides an explanation\nfor the general easy optimization of wide DNNs and unravels a potential\nimplicit low-complexity regularization during the training. Overall, our work\nprovides a skeleton for the study of loss landscape of DNNs and its\nimplication, by which a more exact and comprehensive understanding can be\nanticipated in the near\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 15:32:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Yaoyu", ""], ["Zhang", "Zhongwang", ""], ["Luo", "Tao", ""], ["Xu", "Zhi-Qin John", ""]]}, {"id": "2105.14574", "submitter": "Aristeidis Panos", "authors": "Aristeidis Panos, Ioannis Kosmidis, Petros Dellaportas", "title": "Scalable and Interpretable Marked Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel inferential framework for marked point processes that\nenjoys both scalability and interpretability. The framework is based on\nvariational inference and it aims to speed up inference for a flexible family\nof marked point processes where the joint distribution of times and marks can\nbe specified in terms of the conditional distribution of times given the\nprocess filtration, and of the conditional distribution of marks given the\nprocess filtration and the current time. We assess the predictive ability of\nour proposed method over four real-world datasets where results show its\ncompetitive performance against other baselines. The attractiveness of our\nframework for the modelling of marked point processes is illustrated through a\ncase study of association football data where scalability and interpretability\nare exploited for extracting useful informative patterns.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 15:37:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Panos", "Aristeidis", ""], ["Kosmidis", "Ioannis", ""], ["Dellaportas", "Petros", ""]]}, {"id": "2105.14584", "submitter": "Gunhee Nam", "authors": "Gunhee Nam, Miran Heo, Seoung Wug Oh, Joon-Young Lee, Seon Joo Kim", "title": "Polygonal Point Set Tracking", "comments": "14 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel learning-based polygonal point set tracking\nmethod. Compared to existing video object segmentation~(VOS) methods that\npropagate pixel-wise object mask information, we propagate a polygonal point\nset over frames.\n  Specifically, the set is defined as a subset of points in the target contour,\nand our goal is to track corresponding points on the target contour. Those\noutputs enable us to apply various visual effects such as motion tracking, part\ndeformation, and texture mapping. To this end, we propose a new method to track\nthe corresponding points between frames by the global-local alignment with\ndelicately designed losses and regularization terms. We also introduce a novel\nlearning strategy using synthetic and VOS datasets that makes it possible to\ntackle the problem without developing the point correspondence dataset. Since\nthe existing datasets are not suitable to validate our method, we build a new\npolygonal point set tracking dataset and demonstrate the superior performance\nof our method over the baselines and existing contour-based VOS methods. In\naddition, we present visual-effects applications of our method on part\ndistortion and text mapping.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 17:12:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Nam", "Gunhee", ""], ["Heo", "Miran", ""], ["Oh", "Seoung Wug", ""], ["Lee", "Joon-Young", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2105.14586", "submitter": "Hardhik Mohanty", "authors": "Gourab Ghatak, Hardhik Mohanty, Aniq Ur Rahman", "title": "Kolmogorov-Smirnov Test-Based Actively-Adaptive Thompson Sampling for\n  Non-Stationary Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-stationary multi-armed bandit (MAB) framework and propose\na Kolmogorov-Smirnov (KS) test based Thompson Sampling (TS) algorithm named\nTS-KS, that actively detects change points and resets the TS parameters once a\nchange is detected. In particular, for the two-armed bandit case, we derive\nbounds on the number of samples of the reward distribution to detect the change\nonce it occurs. Consequently, we show that the proposed algorithm has\nsub-linear regret. Contrary to existing works, our algorithm is able to detect\na change when the underlying reward distribution changes even though the mean\nreward remains the same. Finally, to test the efficacy of the proposed\nalgorithm, we employ it in two case-studies: i) task-offloading scenario in\nwireless edge-computing, and ii) portfolio optimization. Our results show that\nthe proposed TS-KS algorithm outperforms not only the static TS algorithm but\nalso it performs better than other bandit algorithms designed for\nnon-stationary environments. Moreover, the performance of TS-KS is at par with\nthe state-of-the-art forecasting algorithms such as Facebook-PROPHET and ARIMA.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 17:28:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ghatak", "Gourab", ""], ["Mohanty", "Hardhik", ""], ["Rahman", "Aniq Ur", ""]]}, {"id": "2105.14594", "submitter": "Yingzhen Li", "authors": "Hippolyt Ritter, Martin Kukla, Cheng Zhang, Yingzhen Li", "title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian neural networks and deep ensembles represent two modern paradigms of\nuncertainty quantification in deep learning. Yet these approaches struggle to\nscale mainly due to memory inefficiency issues, since they require parameter\nstorage several times higher than their deterministic counterparts. To address\nthis, we augment the weight matrix of each layer with a small number of\ninducing weights, thereby projecting the uncertainty quantification into such\nlow dimensional spaces. We further extend Matheron's conditional Gaussian\nsampling rule to enable fast weight sampling, which enables our inference\nmethod to maintain reasonable run-time as compared with ensembles. Importantly,\nour approach achieves competitive performance to the state-of-the-art in\nprediction and uncertainty estimation tasks with fully connected neural\nnetworks and ResNets, while reducing the parameter size to $\\leq 24.3\\%$ of\nthat of a $single$ neural network.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 18:17:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ritter", "Hippolyt", ""], ["Kukla", "Martin", ""], ["Zhang", "Cheng", ""], ["Li", "Yingzhen", ""]]}, {"id": "2105.14602", "submitter": "SueYeon Chung", "authors": "Cory Stephenson, Suchismita Padhy, Abhinav Ganesh, Yue Hui, Hanlin\n  Tang and SueYeon Chung", "title": "On the geometry of generalization and memorization in deep neural\n  networks", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how large neural networks avoid memorizing training data is key\nto explaining their high generalization performance. To examine the structure\nof when and where memorization occurs in a deep network, we use a recently\ndeveloped replica-based mean field theoretic geometric analysis method. We find\nthat all layers preferentially learn from examples which share features, and\nlink this behavior to generalization performance. Memorization predominately\noccurs in the deeper layers, due to decreasing object manifolds' radius and\ndimension, whereas early layers are minimally affected. This predicts that\ngeneralization can be restored by reverting the final few layer weights to\nearlier epochs before significant memorization occurred, which is confirmed by\nthe experiments. Additionally, by studying generalization under different model\nsizes, we reveal the connection between the double descent phenomenon and the\nunderlying model geometry. Finally, analytical analysis shows that networks\navoid memorization early in training because close to initialization, the\ngradient contribution from permuted examples are small. These findings provide\nquantitative evidence for the structure of memorization across layers of a deep\nneural network, the drivers for such structure, and its connection to manifold\ngeometric properties.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 19:07:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Stephenson", "Cory", ""], ["Padhy", "Suchismita", ""], ["Ganesh", "Abhinav", ""], ["Hui", "Yue", ""], ["Tang", "Hanlin", ""], ["Chung", "SueYeon", ""]]}, {"id": "2105.14620", "submitter": "Yicong He", "authors": "Yicong He, George K. Atia", "title": "Non-local Patch-based Low-rank Tensor Ring Completion for Visual Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tensor completion is the problem of estimating the missing entries of a\npartially observed tensor with a certain low-rank structure. It improves on\nmatrix completion for image and video data by capturing additional structural\ninformation intrinsic to such data. % With more inherent information involving\nin tensor structure than matrix, tensor completion has shown better performance\ncompared with matrix completion especially in image and video data. Traditional\ncompletion algorithms treat the entire visual data as a tensor, which may not\nalways work well especially when camera or object motion exists. In this paper,\nwe develop a novel non-local patch-based tensor ring completion algorithm. In\nthe proposed approach, similar patches are extracted for each reference patch\nalong both the spatial and temporal domains of the visual data. The collected\npatches are then formed into a high-order tensor and a tensor ring completion\nalgorithm is proposed to recover the completed tensor. A novel interval\nsampling-based block matching (ISBM) strategy and a hybrid completion strategy\nare also proposed to improve efficiency and accuracy. Further, we develop an\nonline patch-based completion algorithm to deal with streaming video data. An\nefficient online tensor ring completion algorithm is proposed to reduce the\ntime cost. Extensive experimental results demonstrate the superior performance\nof the proposed algorithms compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 20:33:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["He", "Yicong", ""], ["Atia", "George K.", ""]]}, {"id": "2105.14625", "submitter": "Thomas Bartz-Beielstein", "authors": "Thomas Bartz-Beielstein and Frederik Rehbach and Amrita Sen and Martin\n  Zaefferer", "title": "Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT", "comments": "version 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A surrogate model based hyperparameter tuning approach for deep learning is\npresented. This article demonstrates how the architecture-level parameters\n(hyperparameters) of deep learning models that were implemented in\nKeras/tensorflow can be optimized. The implementation of the tuning procedure\nis 100% accessible from R, the software environment for statistical computing.\nWith a few lines of code, existing R packages (tfruns and SPOT) can be combined\nto perform hyperparameter tuning. An elementary hyperparameter tuning task\n(neural network and the MNIST data) is used to exemplify this approach\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 21:16:51 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 14:48:40 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 22:04:14 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bartz-Beielstein", "Thomas", ""], ["Rehbach", "Frederik", ""], ["Sen", "Amrita", ""], ["Zaefferer", "Martin", ""]]}, {"id": "2105.14629", "submitter": "Li Chen", "authors": "Li Chen, Richard Peng, and Di Wang", "title": "$\\ell_2$-norm Flow Diffusion in Near-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion is a fundamental graph procedure and has been a basic building\nblock in a wide range of theoretical and empirical applications such as graph\npartitioning and semi-supervised learning on graphs. In this paper, we study\ncomputationally efficient diffusion primitives beyond random walk.\n  We design an $\\widetilde{O}(m)$-time randomized algorithm for the\n$\\ell_2$-norm flow diffusion problem, a recently proposed diffusion model based\non network flow with demonstrated graph clustering related applications both in\ntheory and in practice. Examples include finding locally-biased low conductance\ncuts. Using a known connection between the optimal dual solution of the flow\ndiffusion problem and the local cut structure, our algorithm gives an\nalternative approach for finding such cuts in nearly linear time.\n  From a technical point of view, our algorithm contributes a novel way of\ndealing with inequality constraints in graph optimization problems. It adapts\nthe high-level algorithmic framework of nearly linear time Laplacian system\nsolvers, but requires several new tools: vertex elimination under constraints,\na new family of graph ultra-sparsifiers, and accelerated proximal gradient\nmethods with inexact proximal mapping computation.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 21:27:58 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 21:02:24 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chen", "Li", ""], ["Peng", "Richard", ""], ["Wang", "Di", ""]]}, {"id": "2105.14636", "submitter": "Zhewei Yao", "authors": "Zhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, Michael W. Mahoney", "title": "MLPruning: A Multilevel Structured Pruning Framework for\n  Transformer-based Models", "comments": "20 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning is an effective method to reduce the memory footprint and\ncomputational cost associated with large natural language processing models.\nHowever, current approaches either only explore head pruning, which has a\nlimited pruning ratio, or only focus on unstructured pruning, which has\nnegligible effects on the real inference time and/or power consumption. To\naddress these challenges, we develop a novel MultiLevel structured Pruning\n(MLPruning) framework, which uses three different levels of structured pruning:\nhead pruning, row pruning, and block-wise sparse pruning. We propose using a\nlearnable Top-k threshold, which employs an adaptive regularization to adjust\nthe regularization magnitude adaptively, to select appropriate pruning ratios\nfor different weight matrices. We also propose a two-step pipeline to combine\nblock-wise pruning with head/row pruning to achieve high structured pruning\nratios with minimum accuracy degradation. Our empirical results show that for\n\\bertbase, with \\textapprox20\\% of remaining weights, \\OURS can achieve an\naccuracy that is comparable to the full model on QQP/MNLI/\\squad, with up to\n\\textapprox3.69x speedup. Our framework has been open sourced~\\cite{codebase}.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 22:00:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yao", "Zhewei", ""], ["Ma", "Linjian", ""], ["Shen", "Sheng", ""], ["Keutzer", "Kurt", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2105.14638", "submitter": "Johannes Otterbach", "authors": "Samuel von Bau{\\ss}nern, Johannes Otterbach, Adrian Loy, Mathieu\n  Salzmann, Thomas Wollmann", "title": "DAAIN: Detection of Anomalous and Adversarial Input using Normalizing\n  Flows", "comments": "14 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite much recent work, detecting out-of-distribution (OOD) inputs and\nadversarial attacks (AA) for computer vision models remains a challenge. In\nthis work, we introduce a novel technique, DAAIN, to detect OOD inputs and AA\nfor image segmentation in a unified setting. Our approach monitors the inner\nworkings of a neural network and learns a density estimator of the activation\ndistribution. We equip the density estimator with a classification head to\ndiscriminate between regular and anomalous inputs. To deal with the\nhigh-dimensional activation-space of typical segmentation networks, we\nsubsample them to obtain a homogeneous spatial and layer-wise coverage. The\nsubsampling pattern is chosen once per monitored model and kept fixed for all\ninputs. Since the attacker has access to neither the detection model nor the\nsampling key, it becomes harder for them to attack the segmentation network, as\nthe attack cannot be backpropagated through the detector. We demonstrate the\neffectiveness of our approach using an ESPNet trained on the Cityscapes dataset\nas segmentation model, an affine Normalizing Flow as density estimator and use\nblue noise to ensure homogeneous sampling. Our model can be trained on a single\nGPU making it compute efficient and deployable without requiring specialized\naccelerators.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 22:07:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["von Bau\u00dfnern", "Samuel", ""], ["Otterbach", "Johannes", ""], ["Loy", "Adrian", ""], ["Salzmann", "Mathieu", ""], ["Wollmann", "Thomas", ""]]}, {"id": "2105.14639", "submitter": "Kiran Lekkala", "authors": "Kiran Lekkala, Laurent Itti", "title": "Shaped Policy Search for Evolutionary Strategies using Waypoints", "comments": "Presented at the International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we try to improve exploration in Blackbox methods,\nparticularly Evolution strategies (ES), when applied to Reinforcement Learning\n(RL) problems where intermediate waypoints/subgoals are available. Since\nEvolutionary strategies are highly parallelizable, instead of extracting just a\nscalar cumulative reward, we use the state-action pairs from the trajectories\nobtained during rollouts/evaluations, to learn the dynamics of the agent. The\nlearnt dynamics are then used in the optimization procedure to speed-up\ntraining. Lastly, we show how our proposed approach is universally applicable\nby presenting results from experiments conducted on Carla driving and UR5\nrobotic arm simulators.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 22:15:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lekkala", "Kiran", ""], ["Itti", "Laurent", ""]]}, {"id": "2105.14644", "submitter": "Florian Jaeckle", "authors": "Florian Jaeckle and M. Pawan Kumar", "title": "Generating Adversarial Examples with Graph Neural Networks", "comments": "To be published in UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent years have witnessed the deployment of adversarial attacks to evaluate\nthe robustness of Neural Networks. Past work in this field has relied on\ntraditional optimization algorithms that ignore the inherent structure of the\nproblem and data, or generative methods that rely purely on learning and often\nfail to generate adversarial examples where they are hard to find. To alleviate\nthese deficiencies, we propose a novel attack based on a graph neural network\n(GNN) that takes advantage of the strengths of both approaches; we call it\nAdvGNN. Our GNN architecture closely resembles the network we wish to attack.\nDuring inference, we perform forward-backward passes through the GNN layers to\nguide an iterative procedure towards adversarial examples. During training, its\nparameters are estimated via a loss function that encourages the efficient\ncomputation of adversarial examples over a time horizon. We show that our\nmethod beats state-of-the-art adversarial attacks, including PGD-attack,\nMI-FGSM, and Carlini and Wagner attack, reducing the time required to generate\nadversarial examples with small perturbation norms by over 65\\%. Moreover,\nAdvGNN achieves good generalization performance on unseen networks. Finally, we\nprovide a new challenging dataset specifically designed to allow for a more\nillustrative comparison of adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 22:46:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jaeckle", "Florian", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "2105.14645", "submitter": "Akshay Dave", "authors": "Akshay J. Dave (1), Jiankai Yu (1), Jarod Wilson (1), Bren Phillips\n  (1), Kaichao Sun (1), Benoit Forget (1) ((1) Massachusetts Institute of\n  Technology)", "title": "Empirical Models for Multidimensional Regression of Fission Systems", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of next-generation autonomous control of fission systems,\nsuch as nuclear power plants, will require leveraging advancements in machine\nlearning. For fission systems, accurate prediction of nuclear transport is\nimportant to quantify the safety margin and optimize performance. The\nstate-of-the-art approach to this problem is costly Monte Carlo (MC)\nsimulations to approximate solutions of the neutron transport equation. Such an\napproach is feasible for offline calculations e.g., for design or licensing,\nbut is precluded from use as a model-based controller. In this work, we explore\nthe use of Artificial Neural Networks (ANN), Gradient Boosting Regression\n(GBR), Gaussian Process Regression (GPR) and Support Vector Regression (SVR) to\ngenerate empirical models. The empirical model can then be deployed, e.g., in a\nmodel predictive controller. Two fission systems are explored: the subcritical\nMIT Graphite Exponential Pile (MGEP), and the critical MIT Research Reactor\n(MITR).\n  Findings from this work establish guidelines for developing empirical models\nfor multidimensional regression of neutron transport. An assessment of the\naccuracy and precision finds that the SVR, followed closely by ANN, performs\nthe best. For both MGEP and MITR, the optimized SVR model exhibited a\ndomain-averaged, test, mean absolute percentage error of 0.17 %. A spatial\ndistribution of performance metrics indicates that physical regions of poor\nperformance coincide with locations of largest neutron flux perturbation --\nthis outcome is mitigated by ANN and SVR. Even at local maxima, ANN and SVR\nbias is within experimental uncertainty bounds. A comparison of the performance\nvs. training dataset size found that SVR is more data-efficient than ANN. Both\nANN and SVR achieve a greater than 7 order reduction in evaluation time vs. a\nMC simulation.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 22:53:39 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Dave", "Akshay J.", ""], ["Yu", "Jiankai", ""], ["Wilson", "Jarod", ""], ["Phillips", "Bren", ""], ["Sun", "Kaichao", ""], ["Forget", "Benoit", ""]]}, {"id": "2105.14648", "submitter": "Jesse Geneson", "authors": "Jesse Geneson", "title": "Sharper bounds for online learning of smooth functions of a single\n  variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the generalization of the mistake-bound model to continuous\nreal-valued single variable functions. Let $\\mathcal{F}_q$ be the class of\nabsolutely continuous functions $f: [0, 1] \\rightarrow \\mathbb{R}$ with\n$||f'||_q \\le 1$, and define $opt_p(\\mathcal{F}_q)$ as the best possible bound\non the worst-case sum of the $p^{th}$ powers of the absolute prediction errors\nover any number of trials. Kimber and Long (Theoretical Computer Science, 1995)\nproved for $q \\ge 2$ that $opt_p(\\mathcal{F}_q) = 1$ when $p \\ge 2$ and\n$opt_p(\\mathcal{F}_q) = \\infty$ when $p = 1$. For $1 < p < 2$ with $p =\n1+\\epsilon$, the only known bound was $opt_p(\\mathcal{F}_{q}) =\nO(\\epsilon^{-1})$ from the same paper. We show for all $\\epsilon \\in (0, 1)$\nand $q \\ge 2$ that $opt_{1+\\epsilon}(\\mathcal{F}_q) =\n\\Theta(\\epsilon^{-\\frac{1}{2}})$, where the constants in the bound do not\ndepend on $q$. We also show that $opt_{1+\\epsilon}(\\mathcal{F}_{\\infty}) =\n\\Theta(\\epsilon^{-\\frac{1}{2}})$.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 23:06:21 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Geneson", "Jesse", ""]]}, {"id": "2105.14655", "submitter": "Zhuoran Qiao", "authors": "Zhuoran Qiao, Anders S. Christensen, Matthew Welborn, Frederick R.\n  Manby, Anima Anandkumar, Thomas F. Miller III", "title": "UNiTE: Unitary N-body Tensor Equivariant Network with Applications to\n  Quantum Chemistry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equivariant neural networks have been successful in incorporating various\ntypes of symmetries, but are mostly limited to vector representations of\ngeometric objects. Despite the prevalence of higher-order tensors in various\napplication domains, e.g. in quantum chemistry, equivariant neural networks for\ngeneral tensors remain unexplored. Previous strategies for learning equivariant\nfunctions on tensors mostly rely on expensive tensor factorization which is not\nscalable when the dimensionality of the problem becomes large. In this work, we\npropose unitary $N$-body tensor equivariant neural network (UNiTE), an\narchitecture for a general class of symmetric tensors called $N$-body tensors.\nThe proposed neural network is equivariant with respect to the actions of a\nunitary group, such as the group of 3D rotations. Furthermore, it has a linear\ntime complexity with respect to the number of non-zero elements in the tensor.\nWe also introduce a normalization method, viz., Equivariant Normalization, to\nimprove generalization of the neural network while preserving symmetry. When\napplied to quantum chemistry, UNiTE outperforms all state-of-the-art machine\nlearning methods of that domain with over 110% average improvements on multiple\nbenchmarks. Finally, we show that UNiTE achieves a robust zero-shot\ngeneralization performance on diverse down stream chemistry tasks, while being\nthree orders of magnitude faster than conventional numerical methods with\ncompetitive accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 00:48:18 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 02:47:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Qiao", "Zhuoran", ""], ["Christensen", "Anders S.", ""], ["Welborn", "Matthew", ""], ["Manby", "Frederick R.", ""], ["Anandkumar", "Anima", ""], ["Miller", "Thomas F.", "III"]]}, {"id": "2105.14656", "submitter": "Arash Mohammadi", "authors": "Parnian Afshar, Moezedin Javad Rafiee, Farnoosh Naderkhani, Shahin\n  Heidarian, Nastaran Enshaei, Anastasia Oikonomou, Faranak Babaki Fard, Reut\n  Anconina, Keyvan Farahani, Konstantinos N. Plataniotis, and Arash Mohammadi", "title": "Human-level COVID-19 Diagnosis from Low-dose CT Scans Using a Two-stage\n  Time-distributed Capsule Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reverse transcription-polymerase chain reaction (RT-PCR) is currently the\ngold standard in COVID-19 diagnosis. It can, however, take days to provide the\ndiagnosis, and false negative rate is relatively high. Imaging, in particular\nchest computed tomography (CT), can assist with diagnosis and assessment of\nthis disease. Nevertheless, it is shown that standard dose CT scan gives\nsignificant radiation burden to patients, especially those in need of multiple\nscans. In this study, we consider low-dose and ultra-low-dose (LDCT and ULDCT)\nscan protocols that reduce the radiation exposure close to that of a single\nX-Ray, while maintaining an acceptable resolution for diagnosis purposes. Since\nthoracic radiology expertise may not be widely available during the pandemic,\nwe develop an Artificial Intelligence (AI)-based framework using a collected\ndataset of LDCT/ULDCT scans, to study the hypothesis that the AI model can\nprovide human-level performance. The AI model uses a two stage capsule network\narchitecture and can rapidly classify COVID-19, community acquired pneumonia\n(CAP), and normal cases, using LDCT/ULDCT scans. The AI model achieves COVID-19\nsensitivity of 89.5% +\\- 0.11, CAP sensitivity of 95% +\\- 0.11, normal cases\nsensitivity (specificity) of 85.7% +\\- 0.16, and accuracy of 90% +\\- 0.06. By\nincorporating clinical data (demographic and symptoms), the performance further\nimproves to COVID-19 sensitivity of 94.3% +\\- pm 0.05, CAP sensitivity of 96.7%\n+\\- 0.07, normal cases sensitivity (specificity) of 91% +\\- 0.09 , and accuracy\nof 94.1% +\\- 0.03. The proposed AI model achieves human-level diagnosis based\non the LDCT/ULDCT scans with reduced radiation exposure. We believe that the\nproposed AI model has the potential to assist the radiologists to accurately\nand promptly diagnose COVID-19 infection and help control the transmission\nchain during the pandemic.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 00:49:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Afshar", "Parnian", ""], ["Rafiee", "Moezedin Javad", ""], ["Naderkhani", "Farnoosh", ""], ["Heidarian", "Shahin", ""], ["Enshaei", "Nastaran", ""], ["Oikonomou", "Anastasia", ""], ["Fard", "Faranak Babaki", ""], ["Anconina", "Reut", ""], ["Farahani", "Keyvan", ""], ["Plataniotis", "Konstantinos N.", ""], ["Mohammadi", "Arash", ""]]}, {"id": "2105.14659", "submitter": "Dinh Nguyen", "authors": "Dinh C. Nguyen, Ming Ding, Pubudu N. Pathirana, Aruna Seneviratne, Jun\n  Li, Dusit Niyato, H. Vincent Poor", "title": "Federated Learning for Industrial Internet of Things in Future\n  Industries", "comments": "Accepted at IEEE Wireless Communications Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Industrial Internet of Things (IIoT) offers promising opportunities to\ntransform the operation of industrial systems and becomes a key enabler for\nfuture industries. Recently, artificial intelligence (AI) has been widely\nutilized for realizing intelligent IIoT applications where AI techniques\nrequire centralized data collection and processing. However, this is not always\nfeasible in realistic scenarios due to the high scalability of modern IIoT\nnetworks and growing industrial data confidentiality. Federated Learning (FL),\nas an emerging collaborative AI approach, is particularly attractive for\nintelligent IIoT networks by coordinating multiple IIoT devices and machines to\nperform AI training at the network edge while helping protect user privacy. In\nthis article, we provide a detailed overview and discussions of the emerging\napplications of FL in key IIoT services and applications. A case study is also\nprovided to demonstrate the feasibility of FL in IIoT. Finally, we highlight a\nrange of interesting open research topics that need to be addressed for the\nfull realization of FL-IIoT in industries.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 01:02:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Nguyen", "Dinh C.", ""], ["Ding", "Ming", ""], ["Pathirana", "Pubudu N.", ""], ["Seneviratne", "Aruna", ""], ["Li", "Jun", ""], ["Niyato", "Dusit", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2105.14669", "submitter": "Yuekai Zhao", "authors": "Yuekai Zhao, Li Dong, Yelong Shen, Zhihua Zhang, Furu Wei, Weizhu Chen", "title": "Memory-Efficient Differentiable Transformer Architecture Search", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentiable architecture search (DARTS) is successfully applied in many\nvision tasks. However, directly using DARTS for Transformers is\nmemory-intensive, which renders the search process infeasible. To this end, we\npropose a multi-split reversible network and combine it with DARTS.\nSpecifically, we devise a backpropagation-with-reconstruction algorithm so that\nwe only need to store the last layer's outputs. By relieving the memory burden\nfor DARTS, it allows us to search with larger hidden size and more candidate\noperations. We evaluate the searched architecture on three sequence-to-sequence\ndatasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14\nEnglish-Czech. Experimental results show that our network consistently\noutperforms standard Transformers across the tasks. Moreover, our method\ncompares favorably with big-size Evolved Transformers, reducing search\ncomputation by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 01:52:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhao", "Yuekai", ""], ["Dong", "Li", ""], ["Shen", "Yelong", ""], ["Zhang", "Zhihua", ""], ["Wei", "Furu", ""], ["Chen", "Weizhu", ""]]}, {"id": "2105.14673", "submitter": "Waheed Bajwa", "authors": "Batoul Taki, Mohsen Ghassemi, Anand D. Sarwate, and Waheed U. Bajwa", "title": "A Minimax Lower Bound for Low-Rank Matrix-Variate Logistic Regression", "comments": "8 pages; preprint of a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of matrix-variate logistic regression. The\nfundamental error threshold on estimating coefficient matrices in the logistic\nregression problem is found by deriving a lower bound on the minimax risk. The\nfocus of this paper is on derivation of a minimax risk lower bound for low-rank\ncoefficient matrices. The bound depends explicitly on the dimensions and\ndistribution of the covariates, the rank and energy of the coefficient matrix,\nand the number of samples. The resulting bound is proportional to the intrinsic\ndegrees of freedom in the problem, which suggests the sample complexity of the\nlow-rank matrix logistic regression problem can be lower than that for\nvectorized logistic regression. \\color{red}\\color{black} The proof techniques\nutilized in this work also set the stage for development of minimax lower\nbounds for tensor-variate logistic regression problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 02:06:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Taki", "Batoul", ""], ["Ghassemi", "Mohsen", ""], ["Sarwate", "Anand D.", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "2105.14675", "submitter": "Huanle Zhang", "authors": "Huanle Zhang, Jeonghoon Kim", "title": "Towards a Federated Learning Framework for Heterogeneous Devices of\n  Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) has received a significant amount of attention in the\nindustry and research community due to its capability of keeping data on local\ndevices. To aggregate the gradients of local models to train the global model,\nexisting works require that the global model and the local models are the same.\nHowever, Internet of Things (IoT) devices are inherently diverse regarding\ncomputation speed and onboard memory. In this paper, we propose an FL framework\ntargeting the heterogeneity of IoT devices. Specifically, local models are\ncompressed from the global model, and the gradients of the compressed local\nmodels are used to update the global model. We conduct preliminary experiments\nto illustrate that our framework can facilitate the design of IoT-aware FL.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 02:08:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Huanle", ""], ["Kim", "Jeonghoon", ""]]}, {"id": "2105.14676", "submitter": "Jingfeng Zhang", "authors": "Jingfeng Zhang, Xilie Xu, Bo Han, Tongliang Liu, Gang Niu, Lizhen Cui,\n  Masashi Sugiyama", "title": "NoiLIn: Do Noisy Labels Always Hurt Adversarial Training?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) based on minimax optimization is a popular learning\nstyle that enhances the model's adversarial robustness. Noisy labels (NL)\ncommonly undermine the learning and hurt the model's performance.\nInterestingly, both research directions hardly crossover and hit sparks. In\nthis paper, we raise an intriguing question -- Does NL always hurt AT? Firstly,\nwe find that NL injection in inner maximization for generating adversarial data\naugments natural data implicitly, which benefits AT's generalization. Secondly,\nwe find NL injection in outer minimization for the learning serves as\nregularization that alleviates robust overfitting, which benefits AT's\nrobustness. To enhance AT's adversarial robustness, we propose \"NoiLIn\" that\ngradually increases \\underline{Noi}sy \\underline{L}abels \\underline{In}jection\nover the AT's training process. Empirically, NoiLIn answers the previous\nquestion negatively -- the adversarial robustness can be indeed enhanced by NL\ninjection. Philosophically, we provide a new perspective of the learning with\nNL: NL should not always be deemed detrimental, and even in the absence of NL\nin the training set, we may consider injecting it deliberately.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 02:17:51 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Jingfeng", ""], ["Xu", "Xilie", ""], ["Han", "Bo", ""], ["Liu", "Tongliang", ""], ["Niu", "Gang", ""], ["Cui", "Lizhen", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2105.14686", "submitter": "Weize Chen", "authors": "Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li,\n  Maosong Sun, Jie Zhou", "title": "Fully Hyperbolic Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperbolic neural networks have shown great potential for modeling complex\ndata. However, existing hyperbolic networks are not completely hyperbolic, as\nthey encode features in a hyperbolic space yet formalize most of their\noperations in the tangent space (a Euclidean subspace) at the origin of the\nhyperbolic space. This hybrid method greatly limits the modeling ability of\nnetworks. In this paper, we propose a fully hyperbolic framework to build\nhyperbolic networks based on the Lorentz model by adapting the Lorentz\ntransformations (including boost and rotation) to formalize essential\noperations of neural networks. Moreover, we also prove that linear\ntransformation in tangent spaces used by existing hyperbolic networks is a\nrelaxation of the Lorentz rotation and does not include the boost, implicitly\nlimiting the capabilities of existing hyperbolic networks. The experimental\nresults on four NLP tasks show that our method has better performance for\nbuilding both shallow and deep networks. Our code will be released to\nfacilitate follow-up research.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 03:36:49 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 15:24:28 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chen", "Weize", ""], ["Han", "Xu", ""], ["Lin", "Yankai", ""], ["Zhao", "Hexu", ""], ["Liu", "Zhiyuan", ""], ["Li", "Peng", ""], ["Sun", "Maosong", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.14693", "submitter": "Junghoon Seo", "authors": "Chaehyeon Lee, Junghoon Seo, Heechul Jung", "title": "Training Domain-invariant Object Detector Faster with Feature Replay and\n  Slow Learner", "comments": "2021 CVPR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning-based object detection on remote sensing domain, nuisance\nfactors, which affect observed variables while not affecting predictor\nvariables, often matters because they cause domain changes. Previously,\nnuisance disentangled feature transformation (NDFT) was proposed to build\ndomain-invariant feature extractor with with knowledge of nuisance factors.\nHowever, NDFT requires enormous time in a training phase, so it has been\nimpractical. In this paper, we introduce our proposed method, A-NDFT, which is\nan improvement to NDFT. A-NDFT utilizes two acceleration techniques, feature\nreplay and slow learner. Consequently, on a large-scale UAVDT benchmark, it is\nshown that our framework can reduce the training time of NDFT from 31 hours to\n3 hours while still maintaining the performance. The code will be made publicly\navailable online.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 04:09:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lee", "Chaehyeon", ""], ["Seo", "Junghoon", ""], ["Jung", "Heechul", ""]]}, {"id": "2105.14694", "submitter": "Jing An", "authors": "Jing An, Lexing Ying", "title": "Combining resampling and reweighting for faithful stochastic\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many machine learning and data science tasks require solving non-convex\noptimization problems. When the loss function is a sum of multiple terms, a\npopular method is stochastic gradient descent. Viewed as a process for sampling\nthe loss function landscape, the stochastic gradient descent is known to prefer\nflat local minimums. Though this is desired for certain optimization problems\nsuch as in deep learning, it causes issues when the goal is to find the global\nminimum, especially if the global minimum resides in a sharp valley.\n  Illustrated with a simple motivating example, we show that the fundamental\nreason is that the difference in the Lipschitz constants of multiple terms in\nthe loss function causes stochastic gradient descent to experience different\nvariances at different minimums. In order to mitigate this effect and perform\nfaithful optimization, we propose a combined resampling-reweighting scheme to\nbalance the variance at local minimums and extend to general loss functions. We\nalso explain from the stochastic asymptotics perspective how the proposed\nscheme is more likely to select the true global minimum when compared with the\nvanilla stochastic gradient descent. Experiments from robust statistics,\ncomputational chemistry, and neural network training are provided to\ndemonstrate the theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 04:21:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["An", "Jing", ""], ["Ying", "Lexing", ""]]}, {"id": "2105.14706", "submitter": "Zsolt Zombori", "authors": "Zsolt Zombori, Josef Urban, Miroslav Ol\\v{s}\\'ak", "title": "The Role of Entropy in Guiding a Connection Prover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study how to learn good algorithms for selecting reasoning\nsteps in theorem proving. We explore this in the connection tableau calculus\nimplemented by leanCoP where the partial tableau provides a clean and compact\nnotion of a state to which a limited number of inferences can be applied. We\nstart by incorporating a state-of-the-art learning algorithm -- a graph neural\nnetwork (GNN) -- into the plCoP theorem prover. Then we use it to observe the\nsystem's behaviour in a reinforcement learning setting, i.e., when learning\ninference guidance from successful Monte-Carlo tree searches on many problems.\nDespite its better pattern matching capability, the GNN initially performs\nworse than a simpler previously used learning algorithm. We observe that the\nsimpler algorithm is less confident, i.e., its recommendations have higher\nentropy. This leads us to explore how the entropy of the inference selection\nimplemented via the neural network influences the proof search. This is related\nto research in human decision-making under uncertainty, and in particular the\nprobability matching theory. Our main result shows that a proper entropy\nregularisation, i.e., training the GNN not to be overconfident, greatly\nimproves plCoP's performance on a large mathematical corpus.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 04:57:44 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 12:43:12 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Zombori", "Zsolt", ""], ["Urban", "Josef", ""], ["Ol\u0161\u00e1k", "Miroslav", ""]]}, {"id": "2105.14710", "submitter": "Ameya Patil", "authors": "Ameya D. Patil, Michael Tuttle, Alexander G. Schwing, Naresh R.\n  Shanbhag", "title": "Robustifying $\\ell_\\infty$ Adversarial Training to the Union of\n  Perturbation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical adversarial training (AT) frameworks are designed to achieve high\nadversarial accuracy against a single attack type, typically $\\ell_\\infty$\nnorm-bounded perturbations. Recent extensions in AT have focused on defending\nagainst the union of multiple perturbations but this benefit is obtained at the\nexpense of a significant (up to $10\\times$) increase in training complexity\nover single-attack $\\ell_\\infty$ AT. In this work, we expand the capabilities\nof widely popular single-attack $\\ell_\\infty$ AT frameworks to provide\nrobustness to the union of ($\\ell_\\infty, \\ell_2, \\ell_1$) perturbations while\npreserving their training efficiency. Our technique, referred to as Shaped\nNoise Augmented Processing (SNAP), exploits a well-established byproduct of\nsingle-attack AT frameworks -- the reduction in the curvature of the decision\nboundary of networks. SNAP prepends a given deep net with a shaped noise\naugmentation layer whose distribution is learned along with network parameters\nusing any standard single-attack AT. As a result, SNAP enhances adversarial\naccuracy of ResNet-18 on CIFAR-10 against the union of ($\\ell_\\infty, \\ell_2,\n\\ell_1$) perturbations by 14%-to-20% for four state-of-the-art (SOTA)\nsingle-attack $\\ell_\\infty$ AT frameworks, and, for the first time, establishes\na benchmark for ResNet-50 and ResNet-101 on ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 05:18:42 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 23:12:47 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 22:36:08 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Patil", "Ameya D.", ""], ["Tuttle", "Michael", ""], ["Schwing", "Alexander G.", ""], ["Shanbhag", "Naresh R.", ""]]}, {"id": "2105.14730", "submitter": "Benjamin Maschler", "authors": "Benjamin Maschler, Timo M\\\"uller, Andreas L\\\"ocklin and Michael\n  Weyrich", "title": "Transfer Learning as an Enhancement for Reconfiguration Management of\n  Cyber-Physical Production Systems", "comments": "6 pages, 4 figures, 1 table. Submitted for publication at CIRP ICME\n  2021", "journal-ref": null, "doi": "10.13140/RG.2.2.14077.69606", "report-no": null, "categories": "cs.LG cs.AI cs.SE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reconfiguration demand is increasing due to frequent requirement changes for\nmanufacturing systems. Recent approaches aim at investigating feasible\nconfiguration alternatives from which they select the optimal one. This relies\non processes whose behavior is not reliant on e.g. the production sequence.\nHowever, when machine learning is used, components' behavior depends on the\nprocess' specifics, requiring additional concepts to successfully conduct\nreconfiguration management. Therefore, we propose the enhancement of the\ncomprehensive reconfiguration management with transfer learning. This provides\nthe ability to assess the machine learning dependent behavior of the different\nCPPS configurations with reduced effort and further assists the recommissioning\nof the chosen one. A real cyber-physical production system from the discrete\nmanufacturing domain is utilized to demonstrate the aforementioned proposal.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 06:50:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Maschler", "Benjamin", ""], ["M\u00fcller", "Timo", ""], ["L\u00f6cklin", "Andreas", ""], ["Weyrich", "Michael", ""]]}, {"id": "2105.14732", "submitter": "Chenxin Li", "authors": "Chenxin Li, Wenao Ma, Liyan Sun, Xinghao Ding, Yue Huang, Guisheng\n  Wang, Yizhou Yu", "title": "Hierarchical Deep Network with Uncertainty-aware Semi-supervised\n  Learning for Vessel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of organ vessels is essential for computer-aided diagnosis and\nsurgical planning. But it is not a easy task since the fine-detailed connected\nregions of organ vessel bring a lot of ambiguity in vessel segmentation and\nsub-type recognition, especially for the low-contrast capillary regions.\nFurthermore, recent two-staged approaches would accumulate and even amplify\nthese inaccuracies from the first-stage whole vessel segmentation into the\nsecond-stage sub-type vessel pixel-wise classification. Moreover, the scarcity\nof manual annotation in organ vessels poses another challenge. In this paper,\nto address the above issues, we propose a hierarchical deep network where an\nattention mechanism localizes the low-contrast capillary regions guided by the\nwhole vessels, and enhance the spatial activation in those areas for the\nsub-type vessels. In addition, we propose an uncertainty-aware semi-supervised\ntraining framework to alleviate the annotation-hungry limitation of deep\nmodels. The proposed method achieves the state-of-the-art performance in the\nbenchmarks of both retinal artery/vein segmentation in fundus images and liver\nportal/hepatic vessel segmentation in CT images.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 06:55:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Chenxin", ""], ["Ma", "Wenao", ""], ["Sun", "Liyan", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Wang", "Guisheng", ""], ["Yu", "Yizhou", ""]]}, {"id": "2105.14737", "submitter": "Jin-Hwa Kim", "authors": "Jin-Hwa Kim, Do-Hyeong Kim, Saehoon Yi, Taehoon Lee", "title": "Semi-orthogonal Embedding for Efficient Unsupervised Anomaly\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the efficiency of semi-orthogonal embedding for unsupervised\nanomaly segmentation. The multi-scale features from pre-trained CNNs are\nrecently used for the localized Mahalanobis distances with significant\nperformance. However, the increased feature size is problematic to scale up to\nthe bigger CNNs, since it requires the batch-inverse of multi-dimensional\ncovariance tensor. Here, we generalize an ad-hoc method, random feature\nselection, into semi-orthogonal embedding for robust approximation, cubically\nreducing the computational cost for the inverse of multi-dimensional covariance\ntensor. With the scrutiny of ablation studies, the proposed method achieves a\nnew state-of-the-art with significant margins for the MVTec AD, KolektorSDD,\nKolektorSDD2, and mSTC datasets. The theoretical and empirical analyses offer\ninsights and verification of our straightforward yet cost-effective approach.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:02:20 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kim", "Jin-Hwa", ""], ["Kim", "Do-Hyeong", ""], ["Yi", "Saehoon", ""], ["Lee", "Taehoon", ""]]}, {"id": "2105.14742", "submitter": "Dominik Linzner", "authors": "Dominik Linzner and Heinz Koeppl", "title": "Active Learning of Continuous-time Bayesian Networks through\n  Interventions", "comments": "Accepted at ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning structures and parameters of\nContinuous-time Bayesian Networks (CTBNs) from time-course data under minimal\nexperimental resources. In practice, the cost of generating experimental data\nposes a bottleneck, especially in the natural and social sciences. A popular\napproach to overcome this is Bayesian optimal experimental design (BOED).\nHowever, BOED becomes infeasible in high-dimensional settings, as it involves\nintegration over all possible experimental outcomes. We propose a novel\ncriterion for experimental design based on a variational approximation of the\nexpected information gain. We show that for CTBNs, a semi-analytical expression\nfor this criterion can be calculated for structure and parameter learning. By\ndoing so, we can replace sampling over experimental outcomes by solving the\nCTBNs master-equation, for which scalable approximations exist. This alleviates\nthe computational burden of sampling possible experimental outcomes in\nhigh-dimensions. We employ this framework in order to recommend interventional\nsequences. In this context, we extend the CTBN model to conditional CTBNs in\norder to incorporate interventions. We demonstrate the performance of our\ncriterion on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:13:50 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 06:10:06 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Linzner", "Dominik", ""], ["Koeppl", "Heinz", ""]]}, {"id": "2105.14750", "submitter": "Siyuan Li", "authors": "Siyuan Li, Jin Zhang, Jianhao Wang, Chongjie Zhang", "title": "Efficient Hierarchical Exploration with Stable Subgoal Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal-conditioned hierarchical reinforcement learning (HRL) serves as a\nsuccessful approach to solving complex and temporally extended tasks. Recently,\nits success has been extended to more general settings by concurrently learning\nhierarchical policies and subgoal representations. However, online subgoal\nrepresentation learning exacerbates the non-stationary issue of HRL and\nintroduces challenges for exploration in high-level policy learning. In this\npaper, we propose a state-specific regularization that stabilizes subgoal\nembeddings in well-explored areas while allowing representation updates in less\nexplored state regions. Benefiting from this stable representation, we design\nmeasures of novelty and potential for subgoals, and develop an efficient\nhierarchical exploration strategy that actively seeks out new promising\nsubgoals and states. Experimental results show that our method significantly\noutperforms state-of-the-art baselines in continuous control tasks with sparse\nrewards and further demonstrate the stability and efficiency of the subgoal\nrepresentation learning of this work, which promotes superior policy learning.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:28:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Siyuan", ""], ["Zhang", "Jin", ""], ["Wang", "Jianhao", ""], ["Zhang", "Chongjie", ""]]}, {"id": "2105.14753", "submitter": "Am\\'elie Gruel", "authors": "Am\\'elie Gruel and Jean Martinet", "title": "Bio-inspired visual attention for silicon retinas based on spiking\n  neural networks applied to pattern classification", "comments": "6 pages, 3 figures. To be published in Content-Based Multimedia\n  Indexing (CBMI) 2021, Lille, France. This work was supported by the European\n  Union's ERA-NET CHIST-ERA 2018 research and innovation programme under grant\n  agreement ANR-19-CHR3-0008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual attention can be defined as the behavioral and cognitive process of\nselectively focusing on a discrete aspect of sensory cues while disregarding\nother perceivable information. This biological mechanism, more specifically\nsaliency detection, has long been used in multimedia indexing to drive the\nanalysis only on relevant parts of images or videos for further processing.\n  The recent advent of silicon retinas (or event cameras -- sensors that\nmeasure pixel-wise changes in brightness and output asynchronous events\naccordingly) raises the question of how to adapt attention and saliency to the\nunconventional type of such sensors' output. Silicon retina aims to reproduce\nthe biological retina behaviour. In that respect, they produce punctual events\nin time that can be construed as neural spikes and interpreted as such by a\nneural network.\n  In particular, Spiking Neural Networks (SNNs) represent an asynchronous type\nof artificial neural network closer to biology than traditional artificial\nnetworks, mainly because they seek to mimic the dynamics of neural membrane and\naction potentials over time. SNNs receive and process information in the form\nof spike trains. Therefore, they make for a suitable candidate for the\nefficient processing and classification of incoming event patterns measured by\nsilicon retinas. In this paper, we review the biological background behind the\nattentional mechanism, and introduce a case study of event videos\nclassification with SNNs, using a biology-grounded low-level computational\nattention mechanism, with interesting preliminary results.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:34:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gruel", "Am\u00e9lie", ""], ["Martinet", "Jean", ""]]}, {"id": "2105.14761", "submitter": "Guangsheng Bao", "authors": "Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen and Weihua Luo", "title": "G-Transformer for Document-level Machine Translation", "comments": "Accepted by ACL2021 main track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level MT models are still far from satisfactory. Existing work\nextend translation unit from single sentence to multiple sentences. However,\nstudy shows that when we further enlarge the translation unit to a whole\ndocument, supervised training of Transformer can fail. In this paper, we find\nsuch failure is not caused by overfitting, but by sticking around local minima\nduring training. Our analysis shows that the increased complexity of\ntarget-to-source attention is a reason for the failure. As a solution, we\npropose G-Transformer, introducing locality assumption as an inductive bias\ninto Transformer, reducing the hypothesis space of the attention from target to\nsource. Experiments show that G-Transformer converges faster and more stably\nthan Transformer, achieving new state-of-the-art BLEU scores for both\nnon-pretraining and pre-training settings on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:47:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bao", "Guangsheng", ""], ["Zhang", "Yue", ""], ["Teng", "Zhiyang", ""], ["Chen", "Boxing", ""], ["Luo", "Weihua", ""]]}, {"id": "2105.14772", "submitter": "Anis Elgabli", "authors": "Anis Elgabli, Chaouki Ben Issaid, Amrit S. Bedi, Mehdi Bennis, Vaneet\n  Aggarwal", "title": "Energy-Efficient and Federated Meta-Learning via Projected Stochastic\n  Gradient Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an energy-efficient federated meta-learning\nframework. The objective is to enable learning a meta-model that can be\nfine-tuned to a new task with a few number of samples in a distributed setting\nand at low computation and communication energy consumption. We assume that\neach task is owned by a separate agent, so a limited number of tasks is used to\ntrain a meta-model. Assuming each task was trained offline on the agent's local\ndata, we propose a lightweight algorithm that starts from the local models of\nall agents, and in a backward manner using projected stochastic gradient ascent\n(P-SGA) finds a meta-model. The proposed method avoids complex computations\nsuch as computing hessian, double looping, and matrix inversion, while\nachieving high performance at significantly less energy consumption compared to\nthe state-of-the-art methods such as MAML and iMAML on conducted experiments\nfor sinusoid regression and image classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:15:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Elgabli", "Anis", ""], ["Issaid", "Chaouki Ben", ""], ["Bedi", "Amrit S.", ""], ["Bennis", "Mehdi", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "2105.14780", "submitter": "Matthias M\\\"uller-Brockhausen", "authors": "Matthias M\\\"uller-Brockhausen, Mike Preuss, Aske Plaat", "title": "Procedural Content Generation: Better Benchmarks for Transfer\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The idea of transfer in reinforcement learning (TRL) is intriguing: being\nable to transfer knowledge from one problem to another problem without learning\neverything from scratch. This promises quicker learning and learning more\ncomplex methods. To gain an insight into the field and to detect emerging\ntrends, we performed a database search. We note a surprisingly late adoption of\ndeep learning that starts in 2018. The introduction of deep learning has not\nyet solved the greatest challenge of TRL: generalization. Transfer between\ndifferent domains works well when domains have strong similarities (e.g.\nMountainCar to Cartpole), and most TRL publications focus on different tasks\nwithin the same domain that have few differences. Most TRL applications we\nencountered compare their improvements against self-defined baselines, and the\nfield is still missing unified benchmarks. We consider this to be a\ndisappointing situation. For the future, we note that: (1) A clear measure of\ntask similarity is needed. (2) Generalization needs to improve. Promising\napproaches merge deep learning with planning via MCTS or introduce memory\nthrough LSTMs. (3) The lack of benchmarking tools will be remedied to enable\nmeaningful comparison and measure progress. Already Alchemy and Meta-World are\nemerging as interesting benchmark suites. We note that another development, the\nincrease in procedural content generation (PCG), can improve both benchmarking\nand generalization in TRL.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:21:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["M\u00fcller-Brockhausen", "Matthias", ""], ["Preuss", "Mike", ""], ["Plaat", "Aske", ""]]}, {"id": "2105.14784", "submitter": "Shen Cai", "authors": "Siyu Zhang, Hui Cao, Yuqi Liu, Shen Cai, Yanting Zhang, Yuanzhan Li,\n  Xiaoyu Chi", "title": "SN-Graph: a Minimalist 3D Object Representation for Classification", "comments": "ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using deep learning techniques to process 3D objects has achieved many\nsuccesses. However, few methods focus on the representation of 3D objects,\nwhich could be more effective for specific tasks than traditional\nrepresentations, such as point clouds, voxels, and multi-view images. In this\npaper, we propose a Sphere Node Graph (SN-Graph) to represent 3D objects.\nSpecifically, we extract a certain number of internal spheres (as nodes) from\nthe signed distance field (SDF), and then establish connections (as edges)\namong the sphere nodes to construct a graph, which is seamlessly suitable for\n3D analysis using graph neural network (GNN). Experiments conducted on the\nModelNet40 dataset show that when there are fewer nodes in the graph or the\ntested objects are rotated arbitrarily, the classification accuracy of SN-Graph\nis significantly higher than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:24:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Siyu", ""], ["Cao", "Hui", ""], ["Liu", "Yuqi", ""], ["Cai", "Shen", ""], ["Zhang", "Yanting", ""], ["Li", "Yuanzhan", ""], ["Chi", "Xiaoyu", ""]]}, {"id": "2105.14785", "submitter": "Tianyu Pang", "authors": "Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen,\n  Jun Zhu, Tie-Yan Liu", "title": "Adversarial Training with Rectified Rejection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) is one of the most effective strategies for\npromoting model robustness, whereas even the state-of-the-art adversarially\ntrained models struggle to exceed 60% robust test accuracy on CIFAR-10 without\nadditional data, which is far from practical. A natural way to break this\naccuracy bottleneck is to introduce a rejection option, where confidence is a\ncommonly used certainty proxy. However, the vanilla confidence can overestimate\nthe model certainty if the input is wrongly classified. To this end, we propose\nto use true confidence (T-Con) (i.e., predicted probability of the true class)\nas a certainty oracle, and learn to predict T-Con by rectifying confidence. We\nprove that under mild conditions, a rectified confidence (R-Con) rejector and a\nconfidence rejector can be coupled to distinguish any wrongly classified input\nfrom correctly classified ones, even under adaptive attacks. We also quantify\nthat training R-Con to be aligned with T-Con could be an easier task than\nlearning robust classifiers. In our experiments, we evaluate our rectified\nrejection (RR) module on CIFAR-10, CIFAR-10-C, and CIFAR-100 under several\nattacks, and demonstrate that the RR module is well compatible with different\nAT frameworks on improving robustness, with little extra computation.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:24:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pang", "Tianyu", ""], ["Zhang", "Huishuai", ""], ["He", "Di", ""], ["Dong", "Yinpeng", ""], ["Su", "Hang", ""], ["Chen", "Wei", ""], ["Zhu", "Jun", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2105.14802", "submitter": "Yafu Li", "authors": "Yafu Li, Yongjing Yin, Yulong Chen and Yue Zhang", "title": "On Compositional Generalization of Neural Machine Translation", "comments": "To appear at the ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern neural machine translation (NMT) models have achieved competitive\nperformance in standard benchmarks such as WMT. However, there still exist\nsignificant issues such as robustness, domain generalization, etc. In this\npaper, we study NMT models from the perspective of compositional generalization\nby building a benchmark dataset, CoGnition, consisting of 216k clean and\nconsistent sentence pairs. We quantitatively analyze effects of various factors\nusing compound translation error rate, then demonstrate that the NMT model\nfails badly on compositional generalization, although it performs remarkably\nwell under traditional metrics.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:04:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Yafu", ""], ["Yin", "Yongjing", ""], ["Chen", "Yulong", ""], ["Zhang", "Yue", ""]]}, {"id": "2105.14803", "submitter": "Manish Shukla", "authors": "Rosni K Vasu, Sanjay Seetharaman, Shubham Malaviya, Manish Shukla,\n  Sachin Lodha", "title": "Gradient-based Data Subversion Attack Against Binary Classifiers", "comments": "26 pages, 3 Figures, 8 tables, adversarial attacks, data poisoning\n  attacks, label contamination, transferability of attack, susceptibility", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Machine learning based data-driven technologies have shown impressive\nperformances in a variety of application domains. Most enterprises use data\nfrom multiple sources to provide quality applications. The reliability of the\nexternal data sources raises concerns for the security of the machine learning\ntechniques adopted. An attacker can tamper the training or test datasets to\nsubvert the predictions of models generated by these techniques. Data poisoning\nis one such attack wherein the attacker tries to degrade the performance of a\nclassifier by manipulating the training data.\n  In this work, we focus on label contamination attack in which an attacker\npoisons the labels of data to compromise the functionality of the system. We\ndevelop Gradient-based Data Subversion strategies to achieve model degradation\nunder the assumption that the attacker has limited-knowledge of the victim\nmodel. We exploit the gradients of a differentiable convex loss function\n(residual errors) with respect to the predicted label as a warm-start and\nformulate different strategies to find a set of data instances to contaminate.\nFurther, we analyze the transferability of attacks and the susceptibility of\nbinary classifiers. Our experiments show that the proposed approach outperforms\nthe baselines and is computationally efficient.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:04:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Vasu", "Rosni K", ""], ["Seetharaman", "Sanjay", ""], ["Malaviya", "Shubham", ""], ["Shukla", "Manish", ""], ["Lodha", "Sachin", ""]]}, {"id": "2105.14815", "submitter": "Thiemo Wambsganss", "authors": "Thiemo Wambsganss, Christina Niklaus, Matthias S\\\"ollner, Siegfried\n  Handschuh and Jan Marco Leimeister", "title": "Supporting Cognitive and Emotional Empathic Writing of Students", "comments": "to be published in The Joint Conference of the 59th Annual Meeting of\n  the Association for Computational Linguistics and the 11th International\n  Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present an annotation approach to capturing emotional and cognitive\nempathy in student-written peer reviews on business models in German. We\npropose an annotation scheme that allows us to model emotional and cognitive\nempathy scores based on three types of review components. Also, we conducted an\nannotation study with three annotators based on 92 student essays to evaluate\nour annotation scheme. The obtained inter-rater agreement of {\\alpha}=0.79 for\nthe components and the multi-{\\pi}=0.41 for the empathy scores indicate that\nthe proposed annotation scheme successfully guides annotators to a substantial\nto moderate agreement. Moreover, we trained predictive models to detect the\nannotated empathy structures and embedded them in an adaptive writing support\nsystem for students to receive individual empathy feedback independent of an\ninstructor, time, and location. We evaluated our tool in a peer learning\nexercise with 58 students and found promising results for perceived empathy\nskill learning, perceived feedback accuracy, and intention to use. Finally, we\npresent our freely available corpus of 500 empathy-annotated, student-written\npeer reviews on business models and our annotation guidelines to encourage\nfuture research on the design and development of empathy support systems.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:18:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wambsganss", "Thiemo", ""], ["Niklaus", "Christina", ""], ["S\u00f6llner", "Matthias", ""], ["Handschuh", "Siegfried", ""], ["Leimeister", "Jan Marco", ""]]}, {"id": "2105.14820", "submitter": "Pierre Blanchart", "authors": "Pierre Blanchart", "title": "An exact counterfactual-example-based approach to tree-ensemble models\n  interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Explaining the decisions of machine learning models is becoming a necessity\nin many areas where trust in ML models decision is key to their\naccreditation/adoption. The ability to explain models decisions also allows to\nprovide diagnosis in addition to the model decision, which is highly valuable\nin scenarios such as fault detection. Unfortunately, high-performance models do\nnot exhibit the necessary transparency to make their decisions fully\nunderstandable. And the black-boxes approaches, which are used to explain such\nmodel decisions, suffer from a lack of accuracy in tracing back the exact cause\nof a model decision regarding a given input. Indeed, they do not have the\nability to explicitly describe the decision regions of the model around that\ninput, which is necessary to determine what influences the model towards one\ndecision or the other. We thus asked ourselves the question: is there a\ncategory of high-performance models among the ones currently used for which we\ncould explicitly and exactly characterise the decision regions in the input\nfeature space using a geometrical characterisation? Surprisingly we came out\nwith a positive answer for any model that enters the category of tree ensemble\nmodels, which encompasses a wide range of high-performance models such as\nXGBoost, LightGBM, random forests ... We could derive an exact geometrical\ncharacterisation of their decision regions under the form of a collection of\nmultidimensional intervals. This characterisation makes it straightforward to\ncompute the optimal counterfactual (CF) example associated with a query point.\nWe demonstrate several possibilities of the approach, such as computing the CF\nexample based only on a subset of features. This allows to obtain more\nplausible explanations by adding prior knowledge about which variables the user\ncan control. An adaptation to CF reasoning on regression problems is also\nenvisaged.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:32:46 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Blanchart", "Pierre", ""]]}, {"id": "2105.14824", "submitter": "Thomas Baumhauer", "authors": "Thomas Baumhauer and Djordje Slijepcevic and Matthias Zeppelzauer", "title": "Bounded logit attention: Learning to explain image classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable artificial intelligence is the attempt to elucidate the workings\nof systems too complex to be directly accessible to human cognition through\nsuitable side-information referred to as \"explanations\". We present a trainable\nexplanation module for convolutional image classifiers we call bounded logit\nattention (BLA). The BLA module learns to select a subset of the convolutional\nfeature map for each input instance, which then serves as an explanation for\nthe classifier's prediction. BLA overcomes several limitations of the\ninstancewise feature selection method \"learning to explain\" (L2X) introduced by\nChen et al. (2018): 1) BLA scales to real-world sized image classification\nproblems, and 2) BLA offers a canonical way to learn explanations of variable\nsize. Due to its modularity BLA lends itself to transfer learning setups and\ncan also be employed as a post-hoc add-on to trained classifiers. Beyond\nexplainability, BLA may serve as a general purpose method for differentiable\napproximation of subset selection. In a user study we find that BLA\nexplanations are preferred over explanations generated by the popular\n(Grad-)CAM method.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:36:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Baumhauer", "Thomas", ""], ["Slijepcevic", "Djordje", ""], ["Zeppelzauer", "Matthias", ""]]}, {"id": "2105.14829", "submitter": "Stephen James", "authors": "Stephen James and Andrew J. Davison", "title": "Q-attention: Enabling Efficient Learning for Vision-based Robotic\n  Manipulation", "comments": "Videos and code found at: https://sites.google.com/view/q-attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of reinforcement learning methods, they have yet to have\ntheir breakthrough moment when applied to a broad range of robotic manipulation\ntasks. This is partly due to the fact that reinforcement learning algorithms\nare notoriously difficult and time consuming to train, which is exacerbated\nwhen training from images rather than full-state inputs. As humans perform\nmanipulation tasks, our eyes closely monitor every step of the process with our\ngaze focusing sequentially on the objects being manipulated. With this in mind,\nwe present our Attention-driven Robotic Manipulation (ARM) algorithm, which is\na general manipulation algorithm that can be applied to a range of\nsparse-rewarded tasks, given only a small number of demonstrations. ARM splits\nthe complex task of manipulation into a 3 stage pipeline: (1) a Q-attention\nagent extracts interesting pixel locations from RGB and point cloud inputs, (2)\na next-best pose agent that accepts crops from the Q-attention agent and\noutputs poses, and (3) a control agent that takes the goal pose and outputs\njoint actions. We show that current learning algorithms fail on a range of\nRLBench tasks, whilst ARM is successful.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:44:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["James", "Stephen", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2105.14835", "submitter": "Christoph Hertrich", "authors": "Christoph Hertrich, Amitabh Basu, Marco Di Summa, Martin Skutella", "title": "Towards Lower Bounds on the Depth of ReLU Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.NE math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute to a better understanding of the class of functions that is\nrepresented by a neural network with ReLU activations and a given architecture.\nUsing techniques from mixed-integer optimization, polyhedral theory, and\ntropical geometry, we provide a mathematical counterbalance to the universal\napproximation theorems which suggest that a single hidden layer is sufficient\nfor learning tasks. In particular, we investigate whether the class of exactly\nrepresentable functions strictly increases by adding more layers (with no\nrestrictions on size). This problem has potential impact on algorithmic and\nstatistical aspects because of the insight it provides into the class of\nfunctions represented by neural hypothesis classes. However, to the best of our\nknowledge, this question has not been investigated in the neural network\nliterature. We also present upper bounds on the sizes of neural networks\nrequired to represent functions in these neural hypothesis classes.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:49:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hertrich", "Christoph", ""], ["Basu", "Amitabh", ""], ["Di Summa", "Marco", ""], ["Skutella", "Martin", ""]]}, {"id": "2105.14848", "submitter": "Huy Trinh Quoc", "authors": "Quoc-Huy Trinh, Minh-Van Nguyen, Thiet-Gia Huynh, Minh-Triet Tran", "title": "Refined Deep Neural Network and U-Net for Polyps Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Medico: Multimedia Task 2020 focuses on developing an efficient and\naccurate computer-aided diagnosis system for automatic segmentation [3]. We\nparticipate in task 1, Polyps segmentation task, which is to develop algorithms\nfor segmenting polyps on a comprehensive dataset. In this task, we propose\nmethods combining Residual module, Inception module, Adaptive Convolutional\nneural network with U-Net model, and PraNet for semantic segmentation of\nvarious types of polyps in endoscopic images. We select 5 runs with different\narchitecture and parameters in our methods. Our methods show potential results\nin accuracy and efficiency through multiple experiments, and our team is in the\nTop 3 best results with a Jaccard index of 0.765.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:02:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Trinh", "Quoc-Huy", ""], ["Nguyen", "Minh-Van", ""], ["Huynh", "Thiet-Gia", ""], ["Tran", "Minh-Triet", ""]]}, {"id": "2105.14849", "submitter": "Albert Zeyer", "authors": "Albert Zeyer and Ralf Schl\\\"uter and Hermann Ney", "title": "Why does CTC result in peaky behavior?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE cs.SD eess.AS math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The peaky behavior of CTC models is well known experimentally. However, an\nunderstanding about why peaky behavior occurs is missing, and whether this is a\ngood property. We provide a formal analysis of the peaky behavior and gradient\ndescent convergence properties of the CTC loss and related training criteria.\nOur analysis provides a deep understanding why peaky behavior occurs and when\nit is suboptimal. On a simple example which should be trivial to learn for any\nmodel, we prove that a feed-forward neural network trained with CTC from\nuniform initialization converges towards peaky behavior with a 100% error rate.\nOur analysis further explains why CTC only works well together with the blank\nlabel. We further demonstrate that peaky behavior does not occur on other\nrelated losses including a label prior model, and that this improves\nconvergence.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:03:14 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 21:44:23 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2105.14850", "submitter": "Lin Zheng", "authors": "Lin Zheng, Zhiyong Wu, Lingpeng Kong", "title": "Cascaded Head-colliding Attention", "comments": "ACL 2021 Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformers have advanced the field of natural language processing (NLP) on\na variety of important tasks. At the cornerstone of the Transformer\narchitecture is the multi-head attention (MHA) mechanism which models pairwise\ninteractions between the elements of the sequence. Despite its massive success,\nthe current framework ignores interactions among different heads, leading to\nthe problem that many of the heads are redundant in practice, which greatly\nwastes the capacity of the model. To improve parameter efficiency, we\nre-formulate the MHA as a latent variable model from a probabilistic\nperspective. We present cascaded head-colliding attention (CODA) which\nexplicitly models the interactions between attention heads through a\nhierarchical variational distribution. We conduct extensive experiments and\ndemonstrate that CODA outperforms the transformer baseline, by $0.6$ perplexity\non \\texttt{Wikitext-103} in language modeling, and by $0.6$ BLEU on\n\\texttt{WMT14 EN-DE} in machine translation, due to its improvements on the\nparameter efficiency.\\footnote{Our implementation is publicly available at\n\\url{https://github.com/LZhengisme/CODA}.}\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:06:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zheng", "Lin", ""], ["Wu", "Zhiyong", ""], ["Kong", "Lingpeng", ""]]}, {"id": "2105.14859", "submitter": "Samarth Sinha", "authors": "Samarth Sinha, Adji B. Dieng", "title": "Consistency Regularization for Variational Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Variational auto-encoders (VAEs) are a powerful approach to unsupervised\nlearning. They enable scalable approximate posterior inference in\nlatent-variable models using variational inference (VI). A VAE posits a\nvariational family parameterized by a deep neural network called an encoder\nthat takes data as input. This encoder is shared across all the observations,\nwhich amortizes the cost of inference. However the encoder of a VAE has the\nundesirable property that it maps a given observation and a\nsemantics-preserving transformation of it to different latent representations.\nThis \"inconsistency\" of the encoder lowers the quality of the learned\nrepresentations, especially for downstream tasks, and also negatively affects\ngeneralization. In this paper, we propose a regularization method to enforce\nconsistency in VAEs. The idea is to minimize the Kullback-Leibler (KL)\ndivergence between the variational distribution when conditioning on the\nobservation and the variational distribution when conditioning on a random\nsemantic-preserving transformation of this observation. This regularization is\napplicable to any VAE. In our experiments we apply it to four different VAE\nvariants on several benchmark datasets and found it always improves the quality\nof the learned representations but also leads to better generalization. In\nparticular, when applied to the Nouveau Variational Auto-Encoder (NVAE), our\nregularization method yields state-of-the-art performance on MNIST and\nCIFAR-10. We also applied our method to 3D data and found it learns\nrepresentations of superior quality as measured by accuracy on a downstream\nclassification task.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:26:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sinha", "Samarth", ""], ["Dieng", "Adji B.", ""]]}, {"id": "2105.14866", "submitter": "Alexander Camuto", "authors": "Alexander Camuto, Matthew Willetts", "title": "Variational Autoencoders: A Harmonic Perspective", "comments": "18 pages including Appendix, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we study Variational Autoencoders (VAEs) from the perspective of\nharmonic analysis. By viewing a VAE's latent space as a Gaussian Space, a\nvariety of measure space, we derive a series of results that show that the\nencoder variance of a VAE controls the frequency content of the functions\nparameterised by the VAE encoder and decoder neural networks. In particular we\ndemonstrate that larger encoder variances reduce the high frequency content of\nthese functions. Our analysis allows us to show that increasing this variance\neffectively induces a soft Lipschitz constraint on the decoder network of a\nVAE, which is a core contributor to the adversarial robustness of VAEs. We\nfurther demonstrate that adding Gaussian noise to the input of a VAE allows us\nto more finely control the frequency content and the Lipschitz constant of the\nVAE encoder networks. To support our theoretical analysis we run experiments\nwith VAEs with small fully-connected neural networks and with larger\nconvolutional networks, demonstrating empirically that our theory holds for a\nvariety of neural network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:39:25 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 08:59:58 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:09:44 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Camuto", "Alexander", ""], ["Willetts", "Matthew", ""]]}, {"id": "2105.14875", "submitter": "Jakaria Rabbi", "authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, MD.\n  Kamrul Hasan, Mohammed Baz, Mehedi Masud, Md. Abdul Awal, Awal Ahmed Fime,\n  Md. Tahmid Hasan Fuad, Delowar Sikder, and MD. Akil Raihan Iftee", "title": "Bangla Natural Language Processing: A Comprehensive Review of Classical,\n  Machine Learning, and Deep Learning Based Methods", "comments": "This preprint will be submitted to IEEE Access Journal and it\n  contains total of 43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive study of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough review of 71 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. We discuss Classical, Machine Learning\nand Deep Learning approaches with different datasets while addressing the\nlimitations and current and future trends of the BNLP.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:58:58 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:40:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sen", "Ovishake", ""], ["Fuad", "Mohtasim", ""], ["Islam", "MD. Nazrul", ""], ["Rabbi", "Jakaria", ""], ["Hasan", "MD. Kamrul", ""], ["Baz", "Mohammed", ""], ["Masud", "Mehedi", ""], ["Awal", "Md. Abdul", ""], ["Fime", "Awal Ahmed", ""], ["Fuad", "Md. Tahmid Hasan", ""], ["Sikder", "Delowar", ""], ["Iftee", "MD. Akil Raihan", ""]]}, {"id": "2105.14876", "submitter": "Nestor Cabello", "authors": "Nestor Cabello, Elham Naghizade, Jianzhong Qi, Lars Kulik", "title": "Fast, Accurate and Interpretable Time Series Classification Through\n  Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series classification (TSC) aims to predict the class label of a given\ntime series, which is critical to a rich set of application areas such as\neconomics and medicine. State-of-the-art TSC methods have mostly focused on\nclassification accuracy and efficiency, without considering the\ninterpretability of their classifications, which is an important property\nrequired by modern applications such as appliance modeling and legislation such\nas the European General Data Protection Regulation. To address this gap, we\npropose a novel TSC method - the Randomized-Supervised Time Series Forest\n(r-STSF). r-STSF is highly efficient, achieves state-of-the-art classification\naccuracy and enables interpretability. r-STSF takes an efficient interval-based\napproach to classify time series according to aggregate values of\ndiscriminatory sub-series (intervals). To achieve state-of-the-art accuracy,\nr-STSF builds an ensemble of randomized trees using the discriminatory\nsub-series. It uses four time series representations, nine aggregation\nfunctions and a supervised binary-inspired search combined with a feature\nranking metric to identify highly discriminatory sub-series. The discriminatory\nsub-series enable interpretable classifications. Experiments on extensive\ndatasets show that r-STSF achieves state-of-the-art accuracy while being orders\nof magnitude faster than most existing TSC methods. It is the only classifier\nfrom the state-of-the-art group that enables interpretability. Our findings\nalso highlight that r-STSF is the best TSC method when classifying complex time\nseries datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:59:11 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cabello", "Nestor", ""], ["Naghizade", "Elham", ""], ["Qi", "Jianzhong", ""], ["Kulik", "Lars", ""]]}, {"id": "2105.14877", "submitter": "Thanh Vinh Vo", "authors": "Thanh Vinh Vo, Pengfei Wei, Trong Nghia Hoang, Tze-Yun Leong", "title": "Adaptive Multi-Source Causal Inference", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scarcity is a tremendous challenge in causal effect estimation. In this\npaper, we propose to exploit additional data sources to facilitate estimating\ncausal effects in the target population. Specifically, we leverage additional\nsource datasets which share similar causal mechanisms with the target\nobservations to help infer causal effects of the target population. We propose\nthree levels of knowledge transfer, through modelling the outcomes, treatments,\nand confounders. To achieve consistent positive transfer, we introduce\nlearnable parametric transfer factors to adaptively control the transfer\nstrength, and thus achieving a fair and balanced knowledge transfer between the\nsources and the target. The proposed method can infer causal effects in the\ntarget population without prior knowledge of data discrepancy between the\nadditional data sources and the target. Experiments on both synthetic and\nreal-world datasets show the effectiveness of the proposed method as compared\nwith recent baselines.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:02:37 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Vo", "Thanh Vinh", ""], ["Wei", "Pengfei", ""], ["Hoang", "Trong Nghia", ""], ["Leong", "Tze-Yun", ""]]}, {"id": "2105.14890", "submitter": "Kulin Shah", "authors": "Kulin Shah, Pooja Gupta, Amit Deshpande, Chiranjib Bhattacharyya", "title": "Rawlsian Fair Adaptation of Deep Learning Classifiers", "comments": "24 figures, 19 figures", "journal-ref": null, "doi": "10.1145/3461702.3462592", "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group-fairness in classification aims for equality of a predictive utility\nacross different sensitive sub-populations, e.g., race or gender. Equality or\nnear-equality constraints in group-fairness often worsen not only the aggregate\nutility but also the utility for the least advantaged sub-population. In this\npaper, we apply the principles of Pareto-efficiency and least-difference to the\nutility being accuracy, as an illustrative example, and arrive at the Rawls\nclassifier that minimizes the error rate on the worst-off sensitive\nsub-population. Our mathematical characterization shows that the Rawls\nclassifier uniformly applies a threshold to an ideal score of features, in the\nspirit of fair equality of opportunity. In practice, such a score or a feature\nrepresentation is often computed by a black-box model that has been useful but\nunfair. Our second contribution is practical Rawlsian fair adaptation of any\ngiven black-box deep learning model, without changing the score or feature\nrepresentation it computes. Given any score function or feature representation\nand only its second-order statistics on the sensitive sub-populations, we seek\na threshold classifier on the given score or a linear threshold classifier on\nthe given feature representation that achieves the Rawls error rate restricted\nto this hypothesis class. Our technical contribution is to formulate the above\nproblems using ambiguous chance constraints, and to provide efficient\nalgorithms for Rawlsian fair adaptation, along with provable upper bounds on\nthe Rawls error rate. Our empirical results show significant improvement over\nstate-of-the-art group-fair algorithms, even without retraining for fairness.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:31:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shah", "Kulin", ""], ["Gupta", "Pooja", ""], ["Deshpande", "Amit", ""], ["Bhattacharyya", "Chiranjib", ""]]}, {"id": "2105.14898", "submitter": "Igor Mozeti\\v{c}", "authors": "Bojan Evkoski, Andraz Pelicon, Igor Mozetic, Nikola Ljubesic, Petra\n  Kralj Novak", "title": "Retweet communities reveal the main sources of hate speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a challenging problem of identifying main sources of hate speech\non Twitter. On one hand, we carefully annotate a large set of tweets for hate\nspeech, and deploy advanced deep learning to produce high quality hate speech\nclassification models. On the other hand, we create retweet networks, detect\ncommunities and monitor their evolution through time. This combined approach is\napplied to three years of Slovenian Twitter data. We report a number of\ninteresting results. Hate speech is dominated by offensive tweets, related to\npolitical and ideological issues. The share of unacceptable tweets is\nmoderately increasing with time, from the initial 20% to 30% by the end of\n2020. Unacceptable tweets are retweeted significantly more often than\nacceptable tweets. About 60% of unacceptable tweets are produced by a single\nright-wing community of only moderate size. Institutional Twitter accounts and\nmedia accounts post significantly less unacceptable tweets than individual\naccounts. However, the main sources of unacceptable tweets are anonymous\naccounts, and accounts that were suspended or closed during the last three\nyears.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:43:19 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Evkoski", "Bojan", ""], ["Pelicon", "Andraz", ""], ["Mozetic", "Igor", ""], ["Ljubesic", "Nikola", ""], ["Novak", "Petra Kralj", ""]]}, {"id": "2105.14900", "submitter": "Paavo Parmas", "authors": "Paavo Parmas and Masashi Sugiyama", "title": "A unified view of likelihood ratio and reparameterization gradients", "comments": "AISTATS2021; Earlier paper was split in two (arXiv:1910.06419). Refer\n  to the current paper for the unified view, but see the earlier paper for\n  discussion on an importance sampling technique", "journal-ref": "In International Conference on Artificial Intelligence and\n  Statistics (pp. 4078-4086). PMLR (2021, March)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reparameterization (RP) and likelihood ratio (LR) gradient estimators are\nused to estimate gradients of expectations throughout machine learning and\nreinforcement learning; however, they are usually explained as simple\nmathematical tricks, with no insight into their nature. We use a first\nprinciples approach to explain that LR and RP are alternative methods of\nkeeping track of the movement of probability mass, and the two are connected\nvia the divergence theorem. Moreover, we show that the space of all possible\nestimators combining LR and RP can be completely parameterized by a flow field\n$u(x)$ and an importance sampling distribution $q(x)$. We prove that there\ncannot exist a single-sample estimator of this type outside our characterized\nspace, thus, clarifying where we should be searching for better Monte Carlo\ngradient estimators.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:53:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Parmas", "Paavo", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2105.14927", "submitter": "Mikael Jacquemont", "authors": "Mika\\\"el Jacquemont (LAPP), Thomas Vuillaume (LAPP), Alexandre Benoit\n  (LISTIC), Gilles Maurin (LAPP), Patrick Lambert (LISTIC), Giovanni Lamanna\n  (LAPP)", "title": "First Full-Event Reconstruction from Imaging Atmospheric Cherenkov\n  Telescope Real Data with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cherenkov Telescope Array is the future of ground-based gamma-ray\nastronomy. Its first prototype telescope built on-site, the Large Size\nTelescope 1, is currently under commissioning and taking its first scientific\ndata. In this paper, we present for the first time the development of a\nfull-event reconstruction based on deep convolutional neural networks and its\napplication to real data. We show that it outperforms the standard analysis,\nboth on simulated and on real data, thus validating the deep approach for the\nCTA data analysis. This work also illustrates the difficulty of moving from\nsimulated data to actual data.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 12:51:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jacquemont", "Mika\u00ebl", "", "LAPP"], ["Vuillaume", "Thomas", "", "LAPP"], ["Benoit", "Alexandre", "", "LISTIC"], ["Maurin", "Gilles", "", "LAPP"], ["Lambert", "Patrick", "", "LISTIC"], ["Lamanna", "Giovanni", "", "LAPP"]]}, {"id": "2105.14931", "submitter": "Jian Chen", "authors": "Meng Ling and Jian Chen and Torsten M\\\"oller and Petra Isenberg and\n  Tobias Isenberg and Michael Sedlmair and Robert S. Laramee and Han-Wei Shen\n  and Jian Wu and C. Lee Giles", "title": "Document Domain Randomization for Deep Learning Document Layout\n  Extraction", "comments": "Main paper to appear in ICDAR 2021 (16th International Conference on\n  Document Analysis and Recognition). This version contains additional\n  materials. The associated test data is hosted on IEEE Data Port:\n  http://doi.org/10.21227/326q-bf39", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present document domain randomization (DDR), the first successful transfer\nof convolutional neural networks (CNNs) trained only on graphically rendered\npseudo-paper pages to real-world document segmentation. DDR renders\npseudo-document pages by modeling randomized textual and non-textual contents\nof interest, with user-defined layout and font styles to support joint learning\nof fine-grained classes. We demonstrate competitive results using our DDR\napproach to extract nine document classes from the benchmark CS-150 and papers\npublished in two domains, namely annual meetings of Association for\nComputational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to\nconditions of style mismatch, fewer or more noisy samples that are more easily\nobtained in the real world. We show that high-fidelity semantic information is\nnot necessary to label semantic classes but style mismatch between train and\ntest can lower model accuracy. Using smaller training samples had a slightly\ndetrimental effect. Finally, network models still achieved high test accuracy\nwhen correct labels are diluted towards confusing labels; this behavior hold\nacross several classes.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:16:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ling", "Meng", ""], ["Chen", "Jian", ""], ["M\u00f6ller", "Torsten", ""], ["Isenberg", "Petra", ""], ["Isenberg", "Tobias", ""], ["Sedlmair", "Michael", ""], ["Laramee", "Robert S.", ""], ["Shen", "Han-Wei", ""], ["Wu", "Jian", ""], ["Giles", "C. Lee", ""]]}, {"id": "2105.14933", "submitter": "Thabang Lebese", "authors": "Thabang Lebese, Bruce Mellado, Xifeng Ruan", "title": "The use of Generative Adversarial Networks to characterise new physics\n  in multi-lepton final states at the LHC", "comments": "18 pages, 5 figures, 1 table, journal (JHEP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.LG hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-supervision in Machine Learning can be used in searches for new physics\nwhere the signal plus background regions are not labelled. This strongly\nreduces model dependency in the search for signals Beyond the Standard Model.\nThis approach displays the drawback in that over-fitting can give rise to fake\nsignals. Tossing toy Monte Carlo (MC) events can be used to estimate the\ncorresponding trials factor through a frequentist inference. However, MC events\nthat are based on full detector simulations are resource intensive. Generative\nAdversarial Networks (GANs) can be used to mimic MC generators. GANs are\npowerful generative models, but often suffer from training instability. We\nhenceforth show a review of GANs. We advocate the use of Wasserstein GAN (WGAN)\nwith weight clipping and WGAN with gradient penalty (WGAN-GP) where the norm of\ngradient of the critic is penalized with respect to its input. Following the\nemergence of multi-lepton anomalies at the LHC, we apply GANs for the\ngeneration of di-leptons final states in association with b-quarks at the LHC.\nA good agreement between the MC events and the WGAN-GP events is found for the\nobservables selected in the study.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 12:58:17 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lebese", "Thabang", ""], ["Mellado", "Bruce", ""], ["Ruan", "Xifeng", ""]]}, {"id": "2105.14937", "submitter": "Wanxin Jin", "authors": "Wanxin Jin, Shaoshuai Mou, George J. Pappas", "title": "Safe Pontryagin Differentiable Programming", "comments": "We have developed the implementation codes of Safe PDP as a\n  stand-alone package, available at https://github.com/wanxinjin/Safe-PDP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Safe Pontryagin Differentiable Programming (Safe PDP)\nmethodology, which establishes a theoretical and algorithmic safe\ndifferentiable framework to solve a broad class of safety-critical learning and\ncontrol tasks -- problems that require the guarantee of both immediate and\nlong-term constraint satisfaction at any stage of the learning and control\nprogress. In the spirit of interior-point methods, Safe PDP handles different\ntypes of state and input constraints by incorporating them into the cost and\nloss through barrier functions. We prove the following fundamental features of\nSafe PDP: first, both the constrained solution and its gradient in backward\npass can be approximated by solving a more efficient unconstrained counterpart;\nsecond, the approximation for both the solution and its gradient can be\ncontrolled for arbitrary accuracy using a barrier parameter; and third,\nimportantly, any intermediate results throughout the approximation and\noptimization are strictly respecting all constraints, thus guaranteeing safety\nthroughout the entire learning and control process. We demonstrate the\ncapabilities of Safe PDP in solving various safe learning and control tasks,\nincluding safe policy optimization, safe motion planning, and learning MPCs\nfrom demonstrations, on different challenging control systems such as 6-DoF\nmaneuvering quadrotor and 6-DoF rocket powered landing.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:03:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jin", "Wanxin", ""], ["Mou", "Shaoshuai", ""], ["Pappas", "George J.", ""]]}, {"id": "2105.14940", "submitter": "Zae Myung Kim", "authors": "Zae Myung Kim, Laurent Besacier, Vassilina Nikoulina, Didier Schwab", "title": "Do Multilingual Neural Machine Translation Models Contain Language Pair\n  Specific Attention Heads?", "comments": "10 pages, accepted at Findings of ACL 2021 (short)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on the analysis of the multilingual representations focus on\nidentifying whether there is an emergence of language-independent\nrepresentations, or whether a multilingual model partitions its weights among\ndifferent languages. While most of such work has been conducted in a\n\"black-box\" manner, this paper aims to analyze individual components of a\nmultilingual neural translation (NMT) model. In particular, we look at the\nencoder self-attention and encoder-decoder attention heads (in a many-to-one\nNMT model) that are more specific to the translation of a certain language pair\nthan others by (1) employing metrics that quantify some aspects of the\nattention weights such as \"variance\" or \"confidence\", and (2) systematically\nranking the importance of attention heads with respect to translation quality.\nExperimental results show that surprisingly, the set of most important\nattention heads are very similar across the language pairs and that it is\npossible to remove nearly one-third of the less important heads without hurting\nthe translation quality greatly.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:15:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kim", "Zae Myung", ""], ["Besacier", "Laurent", ""], ["Nikoulina", "Vassilina", ""], ["Schwab", "Didier", ""]]}, {"id": "2105.14953", "submitter": "Noseong Park", "authors": "Sheo Yon Jhin, Minju Jo, Taeyong Kong, Jinsung Jeon, Noseong Park", "title": "ACE-NODE: Attentive Co-Evolving Neural Ordinary Differential Equations", "comments": "Accepted by KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neural ordinary differential equations (NODEs) presented a new paradigm to\nconstruct (continuous-time) neural networks. While showing several good\ncharacteristics in terms of the number of parameters and the flexibility in\nconstructing neural networks, they also have a couple of well-known\nlimitations: i) theoretically NODEs learn homeomorphic mapping functions only,\nand ii) sometimes NODEs show numerical instability in solving integral\nproblems. To handle this, many enhancements have been proposed. To our\nknowledge, however, integrating attention into NODEs has been overlooked for a\nwhile. To this end, we present a novel method of attentive dual co-evolving\nNODE (ACE-NODE): one main NODE for a downstream machine learning task and the\nother for providing attention to the main NODE. Our ACE-NODE supports both\npairwise and elementwise attention. In our experiments, our method outperforms\nexisting NODE-based and non-NODE-based baselines in almost all cases by\nnon-trivial margins.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:39:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jhin", "Sheo Yon", ""], ["Jo", "Minju", ""], ["Kong", "Taeyong", ""], ["Jeon", "Jinsung", ""], ["Park", "Noseong", ""]]}, {"id": "2105.14969", "submitter": "Noseong Park", "authors": "Jayoung Kim, Jinsung Jeon, Jaehoon Lee, Jihyeon Hyeong, Noseong Park", "title": "OCT-GAN: Neural ODE-based Conditional Tabular GANs", "comments": "Accepted by WWW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Synthesizing tabular data is attracting much attention these days for various\npurposes. With sophisticate synthetic data, for instance, one can augment its\ntraining data. For the past couple of years, tabular data synthesis techniques\nhave been greatly improved. Recent work made progress to address many problems\nin synthesizing tabular data, such as the imbalanced distribution and\nmultimodality problems. However, the data utility of state-of-the-art methods\nis not satisfactory yet. In this work, we significantly improve the utility by\ndesigning our generator and discriminator based on neural ordinary differential\nequations (NODEs). After showing that NODEs have theoretically preferred\ncharacteristics for generating tabular data, we introduce our designs. The\nNODE-based discriminator performs a hidden vector evolution trajectory-based\nclassification rather than classifying with a hidden vector at the last layer\nonly. Our generator also adopts an ODE layer at the very beginning of its\narchitecture to transform its initial input vector (i.e., the concatenation of\na noisy vector and a condition vector in our case) onto another latent vector\nspace suitable for the generation process. We conduct experiments with 13\ndatasets, including but not limited to insurance fraud detection, online news\narticle prediction, and so on, and our presented method outperforms other\nstate-of-the-art tabular data synthesis methods in many cases of our\nclassification, regression, and clustering experiments.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:58:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kim", "Jayoung", ""], ["Jeon", "Jinsung", ""], ["Lee", "Jaehoon", ""], ["Hyeong", "Jihyeon", ""], ["Park", "Noseong", ""]]}, {"id": "2105.14975", "submitter": "Shuai Wang", "authors": "Shuai Wang, Kun Zhang, Le Wu, Haiping Ma, Richang Hong, Meng Wang", "title": "Privileged Graph Distillation for Cold Start Recommendation", "comments": "10 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cold start problem in recommender systems is a long-standing challenge,\nwhich requires recommending to new users (items) based on attributes without\nany historical interaction records. In these recommendation systems, warm users\n(items) have privileged collaborative signals of interaction records compared\nto cold start users (items), and these Collaborative Filtering (CF) signals are\nshown to have competing performance for recommendation. Many researchers\nproposed to learn the correlation between collaborative signal embedding space\nand the attribute embedding space to improve the cold start recommendation, in\nwhich user and item categorical attributes are available in many online\nplatforms. However, the cold start recommendation is still limited by two\nembedding spaces modeling and simple assumptions of space transformation. As\nuser-item interaction behaviors and user (item) attributes naturally form a\nheterogeneous graph structure, in this paper, we propose a privileged graph\ndistillation model~(PGD). The teacher model is composed of a heterogeneous\ngraph structure for warm users and items with privileged CF links. The student\nmodel is composed of an entity-attribute graph without CF links. Specifically,\nthe teacher model can learn better embeddings of each entity by injecting\ncomplex higher-order relationships from the constructed heterogeneous graph.\nThe student model can learn the distilled output with privileged CF embeddings\nfrom the teacher embeddings. Our proposed model is generally applicable to\ndifferent cold start scenarios with new user, new item, or new user-new item.\nFinally, extensive experimental results on the real-world datasets clearly show\nthe effectiveness of our proposed model on different types of cold start\nproblems, with average $6.6\\%, 5.6\\%, $ and $17.1\\%$ improvement over\nstate-of-the-art baselines on three datasets, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:05:27 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Shuai", ""], ["Zhang", "Kun", ""], ["Wu", "Le", ""], ["Ma", "Haiping", ""], ["Hong", "Richang", ""], ["Wang", "Meng", ""]]}, {"id": "2105.14980", "submitter": "Xin Zhang", "authors": "Xin Zhang, Guangwei Xu, Yueheng Sun, Meishan Zhang, Pengjun Xie", "title": "Crowdsourcing Learning as Domain Adaptation: A Case Study on Named\n  Entity Recognition", "comments": "Accepted by ACL-IJCNLP 2021 (long paper), accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is regarded as one prospective solution for effective\nsupervised learning, aiming to build large-scale annotated training data by\ncrowd workers. Previous studies focus on reducing the influences from the\nnoises of the crowdsourced annotations for supervised models. We take a\ndifferent point in this work, regarding all crowdsourced annotations as\ngold-standard with respect to the individual annotators. In this way, we find\nthat crowdsourcing could be highly similar to domain adaptation, and then the\nrecent advances of cross-domain methods can be almost directly applied to\ncrowdsourcing. Here we take named entity recognition (NER) as a study case,\nsuggesting an annotator-aware representation learning model that inspired by\nthe domain adaptation methods which attempt to capture effective domain-aware\nfeatures. We investigate both unsupervised and supervised crowdsourcing\nlearning, assuming that no or only small-scale expert annotations are\navailable. Experimental results on a benchmark crowdsourced NER dataset show\nthat our method is highly effective, leading to a new state-of-the-art\nperformance. In addition, under the supervised setting, we can achieve\nimpressive performance gains with only a very small scale of expert\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:11:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Xin", ""], ["Xu", "Guangwei", ""], ["Sun", "Yueheng", ""], ["Zhang", "Meishan", ""], ["Xie", "Pengjun", ""]]}, {"id": "2105.14986", "submitter": "Mohammad Eslami", "authors": "Mohammad Eslami, Solale Tabarestani, Malek Adjouadi", "title": "Feasibility Assessment of Multitasking in MRI Neuroimaging Analysis:\n  Tissue Segmentation, Cross-Modality Conversion and Bias correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging is essential in brain studies for the diagnosis and\nidentification of disease, structure, and function of the brain in its healthy\nand disease states. Literature shows that there are advantages of multitasking\nwith some deep learning (DL) schemes in challenging neuroimaging applications.\nThis study examines the feasibility of using multitasking in three different\napplications, including tissue segmentation, cross-modality conversion, and\nbias-field correction. These applications reflect five different scenarios in\nwhich multitasking is explored and 280 training and testing sessions conducted\nfor empirical evaluations. Two well-known networks, U-Net as a well-known\nconvolutional neural network architecture, and a closed architecture based on\nthe conditional generative adversarial network are implemented. Different\nmetrics such as the normalized cross-correlation coefficient and Dice scores\nare used for comparison of methods and results of the different experiments.\nStatistical analysis is also provided by paired t-test. The present study\nexplores the pros and cons of these methods and their practical impacts on\nmultitasking in different implementation scenarios. This investigation shows\nthat bias correction and cross-modality conversion applications are\nsignificantly easier than the segmentation application, and having multitasking\nwith segmentation is not reasonable if one of them is identified as the main\ntarget application. However, when the main application is the segmentation of\ntissues, multitasking with cross-modality conversion is beneficial, especially\nfor the U-net architecture.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:16:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Eslami", "Mohammad", ""], ["Tabarestani", "Solale", ""], ["Adjouadi", "Malek", ""]]}, {"id": "2105.14989", "submitter": "Ziping Xu", "authors": "Ziping Xu and Ambuj Tewari", "title": "Representation Learning Beyond Linear Prediction Functions", "comments": "1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent papers on the theory of representation learning has shown the\nimportance of a quantity called diversity when generalizing from a set of\nsource tasks to a target task. Most of these papers assume that the function\nmapping shared representations to predictions is linear, for both source and\ntarget tasks. In practice, researchers in deep learning use different numbers\nof extra layers following the pretrained model based on the difficulty of the\nnew task. This motivates us to ask whether diversity can be achieved when\nsource tasks and the target task use different prediction function spaces\nbeyond linear functions. We show that diversity holds even if the target task\nuses a neural network with multiple layers, as long as source tasks use linear\nfunctions. If source tasks use nonlinear prediction functions, we provide a\nnegative result by showing that depth-1 neural networks with ReLu activation\nfunction need exponentially many source tasks to achieve diversity. For a\ngeneral function class, we find that eluder dimension gives a lower bound on\nthe number of tasks required for diversity. Our theoretical results imply that\nsimpler tasks generalize better. Though our theoretical results are shown for\nthe global minimizer of empirical risks, their qualitative predictions still\nhold true for gradient-based optimization algorithms as verified by our\nsimulations on deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:21:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Ziping", ""], ["Tewari", "Ambuj", ""]]}, {"id": "2105.14995", "submitter": "Shuhao Cao", "authors": "Shuhao Cao", "title": "Choose a Transformer: Fourier or Galerkin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply the self-attention from the state-of-the-art\nTransformer in Attention Is All You Need the first time to a data-driven\noperator learning problem related to partial differential equations. We put\ntogether an effort to explain the heuristics of, and improve the efficacy of\nthe self-attention by demonstrating that the softmax normalization in the\nscaled dot-product attention is sufficient but not necessary, and have proved\nthe approximation capacity of a linear variant as a Petrov-Galerkin projection.\nA new layer normalization scheme is proposed to allow a scaling to propagate\nthrough attention layers, which helps the model achieve remarkable accuracy in\noperator learning tasks with unnormalized data. Finally, we present three\noperator learning experiments, including the viscid Burgers' equation, an\ninterface Darcy flow, and an inverse interface coefficient identification\nproblem. All experiments validate the improvements of the newly proposed simple\nattention-based operator learner over their softmax-normalized counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:30:53 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 16:06:10 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Cao", "Shuhao", ""]]}, {"id": "2105.15004", "submitter": "Florent Krzakala", "authors": "Hugo Cui, Bruno Loureiro, Florent Krzakala, Lenka Zdeborov\\'a", "title": "Generalization Error Rates in Kernel Regression: The Crossover from the\n  Noiseless to Noisy Regime", "comments": "22 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this manuscript we consider Kernel Ridge Regression (KRR) under the\nGaussian design. Exponents for the decay of the excess generalization error of\nKRR have been reported in various works under the assumption of power-law decay\nof eigenvalues of the features co-variance. These decays were, however,\nprovided for sizeably different setups, namely in the noiseless case with\nconstant regularization and in the noisy optimally regularized case.\nIntermediary settings have been left substantially uncharted. In this work, we\nunify and extend this line of work, providing characterization of all regimes\nand excess error decay rates that can be observed in terms of the interplay of\nnoise and regularization. In particular, we show the existence of a transition\nin the noisy setting between the noiseless exponents to its noisy values as the\nsample complexity is increased. Finally, we illustrate how this crossover can\nalso be observed on real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:39:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cui", "Hugo", ""], ["Loureiro", "Bruno", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2105.15007", "submitter": "Anamay Chaturvedi", "authors": "Anamay Chaturvedi, Matthew Jones, Huy L. Nguyen", "title": "Locally Private $k$-Means Clustering with Constant Multiplicative\n  Approximation and Near-Optimal Additive Error", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a data set of size $n$ in $d'$-dimensional Euclidean space, the\n$k$-means problem asks for a set of $k$ points (called centers) so that the sum\nof the $\\ell_2^2$-distances between points of a given data set of size $n$ and\nthe set of $k$ centers is minimized. Recent work on this problem in the locally\nprivate setting achieves constant multiplicative approximation with additive\nerror $\\tilde{O} (n^{1/2 + a} \\cdot k \\cdot \\max \\{\\sqrt{d}, \\sqrt{k} \\})$ and\nproves a lower bound of $\\Omega(\\sqrt{n})$ on the additive error for any\nsolution with a constant number of rounds. In this work we bridge the gap\nbetween the exponents of $n$ in the upper and lower bounds on the additive\nerror with two new algorithms. Given any $\\alpha>0$, our first algorithm\nachieves a multiplicative approximation guarantee which is at most a\n$(1+\\alpha)$ factor greater than that of any non-private $k$-means clustering\nalgorithm with $k^{\\tilde{O}(1/\\alpha^2)} \\sqrt{d' n} \\mbox{poly}\\log n$\nadditive error. Given any $c>\\sqrt{2}$, our second algorithm achieves $O(k^{1 +\n\\tilde{O}(1/(2c^2-1))} \\sqrt{d' n} \\mbox{poly} \\log n)$ additive error with\nconstant multiplicative approximation. Both algorithms go beyond the\n$\\Omega(n^{1/2 + a})$ factor that occurs in the additive error for arbitrarily\nsmall parameters $a$ in previous work, and the second algorithm in particular\nshows for the first time that it is possible to solve the locally private\n$k$-means problem in a constant number of rounds with constant factor\nmultiplicative approximation and polynomial dependence on $k$ in the additive\nerror arbitrarily close to linear.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:41:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chaturvedi", "Anamay", ""], ["Jones", "Matthew", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "2105.15010", "submitter": "Sizhe Chen", "authors": "Sizhe Chen, Zhehao Huang, Qinghua Tao, Xiaolin Huang", "title": "QueryNet: An Efficient Attack Framework with Surrogates Carrying\n  Multiple Identities", "comments": "QueryNet reduces queries by about an order of magnitude against SOTA\n  black-box attacks. 21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are acknowledged as vulnerable to adversarial\nattacks, while the existing black-box attacks require extensive queries on the\nvictim DNN to achieve high success rates. For query-efficiency, surrogate\nmodels of the victim are adopted as transferable attackers in consideration of\ntheir Gradient Similarity (GS), i.e., surrogates' attack gradients are similar\nto the victim's ones to some extent. However, it is generally neglected to\nexploit their similarity on outputs, namely the Prediction Similarity (PS), to\nfilter out inefficient queries. To jointly utilize and also optimize\nsurrogates' GS and PS, we develop QueryNet, an efficient attack network that\ncan significantly reduce queries. QueryNet crafts several transferable\nAdversarial Examples (AEs) by surrogates, and then decides also by surrogates\non the most promising AE, which is then sent to query the victim. That is to\nsay, in QueryNet, surrogates are not only exploited as transferable attackers,\nbut also as transferability evaluators for AEs. The AEs are generated using\nsurrogates' GS and evaluated based on their FS, and therefore, the query\nresults could be back-propagated to optimize surrogates' parameters and also\ntheir architectures, enhancing both the GS and the FS. QueryNet has significant\nquery-efficiency, i.e., reduces queries by averagely about an order of\nmagnitude compared to recent SOTA methods according to our comprehensive and\nreal-world experiments: 11 victims (including 2 commercial models) on\nMNIST/CIFAR10/ImageNet, allowing only 8-bit image queries, and no access to the\nvictim's training data.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:45:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Sizhe", ""], ["Huang", "Zhehao", ""], ["Tao", "Qinghua", ""], ["Huang", "Xiaolin", ""]]}, {"id": "2105.15012", "submitter": "Noseong Park", "authors": "Duanshun Li, Jing Liu, Jinsung Jeon, Seoyoung Hong, Thai Le, Dongwon\n  Lee, Noseong Park", "title": "Large-Scale Data-Driven Airline Market Influence Maximization", "comments": "Accepted by KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a prediction-driven optimization framework to maximize the market\ninfluence in the US domestic air passenger transportation market by adjusting\nflight frequencies. At the lower level, our neural networks consider a wide\nvariety of features, such as classical air carrier performance features and\ntransportation network features, to predict the market influence. On top of the\nprediction models, we define a budget-constrained flight frequency optimization\nproblem to maximize the market influence over 2,262 routes. This problem falls\ninto the category of the non-linear optimization problem, which cannot be\nsolved exactly by conventional methods. To this end, we present a novel\nadaptive gradient ascent (AGA) method. Our prediction models show two to eleven\ntimes better accuracy in terms of the median root-mean-square error (RMSE) over\nbaselines. In addition, our AGA optimization method runs 690 times faster with\na better optimization result (in one of our largest scale experiments) than a\ngreedy algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:48:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Duanshun", ""], ["Liu", "Jing", ""], ["Jeon", "Jinsung", ""], ["Hong", "Seoyoung", ""], ["Le", "Thai", ""], ["Lee", "Dongwon", ""], ["Park", "Noseong", ""]]}, {"id": "2105.15013", "submitter": "Jianhong Wang", "authors": "Jianhong Wang, Jinxin Wang, Yuan Zhang, Yunjie Gu, Tae-Kyun Kim", "title": "SHAQ: Incorporating Shapley Value Theory into Q-Learning for Multi-Agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value factorisation proves to be a very useful technique in multi-agent\nreinforcement learning (MARL), but the underlying mechanism is not yet fully\nunderstood. This paper explores a theoretic basis for value factorisation. We\ngeneralise the Shapley value in the coalitional game theory to a Markov convex\ngame (MCG) and use it to guide value factorisation in MARL. We show that the\ngeneralised Shapley value possesses several features such as (1) accurate\nestimation of the maximum global value, (2) fairness in the factorisation of\nthe global value, and (3) being sensitive to dummy agents. The proposed theory\nyields a new learning algorithm called Sharpley Q-learning (SHAQ), which\ninherits the important merits of ordinary Q-learning but extends it to MARL. In\ncomparison with prior-arts, SHAQ has a much weaker assumption (MCG) that is\nmore compatible with real-world problems, but has superior explainability and\nperformance in many cases. We demonstrated SHAQ and verified the theoretic\nclaims on Predator-Prey and StarCraft Multi-Agent Challenge (SMAC).\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:50:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Jianhong", ""], ["Wang", "Jinxin", ""], ["Zhang", "Yuan", ""], ["Gu", "Yunjie", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2105.15018", "submitter": "Giambattista Albora", "authors": "Giambattista Albora, Luciano Pietronero, Andrea Tacchella, Andrea\n  Zaccaria", "title": "Product Progression: a machine learning approach to forecasting\n  industrial upgrading", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic complexity methods, and in particular relatedness measures, lack a\nsystematic evaluation and comparison framework. We argue that out-of-sample\nforecast exercises should play this role, and we compare various machine\nlearning models to set the prediction benchmark. We find that the key object to\nforecast is the activation of new products, and that tree-based algorithms\nclearly overperform both the quite strong auto-correlation benchmark and the\nother supervised algorithms. Interestingly, we find that the best results are\nobtained in a cross-validation setting, when data about the predicted country\nwas excluded from the training set. Our approach has direct policy\nimplications, providing a quantitative and scientifically tested measure of the\nfeasibility of introducing a new product in a given country.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:59:37 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Albora", "Giambattista", ""], ["Pietronero", "Luciano", ""], ["Tacchella", "Andrea", ""], ["Zaccaria", "Andrea", ""]]}, {"id": "2105.15022", "submitter": "Anum Talpur", "authors": "Anum Talpur and Mohan Gurusamy", "title": "Reinforcement Learning-based Dynamic Service Placement in Vehicular\n  Networks", "comments": "Accepted and presented in IEEE 93rd Vehicular Technology Conference\n  VTC2021-Spring", "journal-ref": null, "doi": "10.1109/VTC2021-Spring51267.2021.9448645", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of technologies such as 5G and mobile edge computing has\nenabled provisioning of different types of services with different resource and\nservice requirements to the vehicles in a vehicular network.The growing\ncomplexity of traffic mobility patterns and dynamics in the requests for\ndifferent types of services has made service placement a challenging task. A\ntypical static placement solution is not effective as it does not consider the\ntraffic mobility and service dynamics. In this paper, we propose a\nreinforcement learning-based dynamic (RL-Dynamic) service placement framework\nto find the optimal placement of services at the edge servers while considering\nthe vehicle's mobility and dynamics in the requests for different types of\nservices. We use SUMO and MATLAB to carry out simulation experiments. In our\nlearning framework, for the decision module, we consider two alternative\nobjective functions-minimizing delay and minimizing edge server utilization. We\ndeveloped an ILP based problem formulation for the two objective functions. The\nexperimental results show that 1) compared to static service placement,\nRL-based dynamic service placement achieves fair utilization of edge server\nresources and low service delay, and 2) compared to delay-optimized placement,\nserver utilization optimized placement utilizes resources more effectively,\nachieving higher fairness with lower edge-server utilization.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:01:35 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 13:38:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Talpur", "Anum", ""], ["Gurusamy", "Mohan", ""]]}, {"id": "2105.15024", "submitter": "Thanh Binh Nguyen", "authors": "Binh T. Nguyen, Duy M. Nguyen, Lam Si Tung Ho, Vu Dinh", "title": "OASIS: An Active Framework for Set Inversion", "comments": "13 pages, 8 figures", "journal-ref": "Frontiers in Artificial Intelligence and Applications, 2018", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we introduce a novel method for solving the set inversion\nproblem by formulating it as a binary classification problem. Aiming to develop\na fast algorithm that can work effectively with high-dimensional and\ncomputationally expensive nonlinear models, we focus on active learning, a\nfamily of new and powerful techniques which can achieve the same level of\naccuracy with fewer data points compared to traditional learning methods.\nSpecifically, we propose OASIS, an active learning framework using Support\nVector Machine algorithms for solving the problem of set inversion. Our method\nworks well in high dimensions and its computational cost is relatively robust\nto the increase of dimension. We illustrate the performance of OASIS by several\nsimulation studies and show that our algorithm outperforms VISIA, the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:04:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Nguyen", "Binh T.", ""], ["Nguyen", "Duy M.", ""], ["Ho", "Lam Si Tung", ""], ["Dinh", "Vu", ""]]}, {"id": "2105.15029", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "P. A. Gloor, A. Fronzetti Colladon, F. Grippa, P. Budner, J. Eirich", "title": "Aristotle Said \"Happiness is a State of Activity\" -- Predicting Mood\n  through Body Sensing with Smartwatches", "comments": null, "journal-ref": "Journal of Systems Science and Systems Engineering 27(5), 586-612\n  (2018)", "doi": "10.1007/s11518-018-5383-7", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We measure and predict states of Activation and Happiness using a body\nsensing application connected to smartwatches. Through the sensors of\ncommercially available smartwatches we collect individual mood states and\ncorrelate them with body sensing data such as acceleration, heart rate, light\nlevel data, and location, through the GPS sensor built into the smartphone\nconnected to the smartwatch. We polled users on the smartwatch for seven weeks\nfour times per day asking for their mood state. We found that both Happiness\nand Activation are negatively correlated with heart beats and with the levels\nof light. People tend to be happier when they are moving more intensely and are\nfeeling less activated during weekends. We also found that people with a lower\nConscientiousness and Neuroticism and higher Agreeableness tend to be happy\nmore frequently. In addition, more Activation can be predicted by lower\nOpenness to experience and higher Agreeableness and Conscientiousness. Lastly,\nwe find that tracking people's geographical coordinates might play an important\nrole in predicting Happiness and Activation. The methodology we propose is a\nfirst step towards building an automated mood tracking system, to be used for\nbetter teamwork and in combination with social network analysis studies.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:14:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gloor", "P. A.", ""], ["Colladon", "A. Fronzetti", ""], ["Grippa", "F.", ""], ["Budner", "P.", ""], ["Eirich", "J.", ""]]}, {"id": "2105.15034", "submitter": "Fei Tang", "authors": "Fei Tang, Michael Kopp", "title": "A remark on a paper of Krotov and Hopfield [arXiv:2008.06996]", "comments": "1 page, 8 formulae", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In their recent paper titled \"Large Associative Memory Problem in\nNeurobiology and Machine Learning\" [arXiv:2008.06996] the authors gave a\nbiologically plausible microscopic theory from which one can recover many dense\nassociative memory models discussed in the literature. We show that the layers\nof the recent \"MLP-mixer\" [arXiv:2105.01601] as well as the essentially\nequivalent model in [arXiv:2105.02723] are amongst them.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:13:00 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 07:14:13 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Tang", "Fei", ""], ["Kopp", "Michael", ""]]}, {"id": "2105.15035", "submitter": "Anum Talpur", "authors": "Anum Talpur and Mohan Gurusamy", "title": "Machine Learning for Security in Vehicular Networks: A Comprehensive\n  Survey", "comments": "Submitted in IEEE Communications Surveys & Tutorials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has emerged as an attractive and viable technique to\nprovide effective solutions for a wide range of application domains. An\nimportant application domain is vehicular networks wherein ML-based approaches\nare found to be very useful to address various problems. The use of wireless\ncommunication between vehicular nodes and/or infrastructure makes it vulnerable\nto different types of attacks. In this regard, ML and its variants are gaining\npopularity to detect attacks and deal with different kinds of security issues\nin vehicular communication. In this paper, we present a comprehensive survey of\nML-based techniques for different security issues in vehicular networks. We\nfirst briefly introduce the basics of vehicular networks and different types of\ncommunications. Apart from the traditional vehicular networks, we also consider\nmodern vehicular network architectures. We propose a taxonomy of security\nattacks in vehicular networks and discuss various security challenges and\nrequirements. We classify the ML techniques developed in the literature\naccording to their use in vehicular network applications. We explain the\nsolution approaches and working principles of these ML techniques in addressing\nvarious security challenges and provide insightful discussion. The limitations\nand challenges in using ML-based methods in vehicular networks are discussed.\nFinally, we present observations and lessons learned before we conclude our\nwork.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:15:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Talpur", "Anum", ""], ["Gurusamy", "Mohan", ""]]}, {"id": "2105.15057", "submitter": "Zhixing Ye", "authors": "Zhixing Ye, Shaofei Qin, Sizhe Chen, Xiaolin Huang", "title": "Dominant Patterns: Critical Features Hidden in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we find the existence of critical features hidden in Deep\nNeuralNetworks (DNNs), which are imperceptible but can actually dominate the\noutputof DNNs. We call these features dominant patterns. As the name suggests,\nfor a natural image, if we add the dominant pattern of a DNN to it, the output\nof this DNN is determined by the dominant pattern instead of the original\nimage, i.e., DNN's prediction is the same with the dominant pattern's. We\ndesign an algorithm to find such patterns by pursuing the insensitivity in the\nfeature space. A direct application of the dominant patterns is the Universal\nAdversarial Perturbations(UAPs). Numerical experiments show that the found\ndominant patterns defeat state-of-the-art UAP methods, especially in label-free\nsettings. In addition, dominant patterns are proved to have the potential to\nattack downstream tasks in which DNNs share the same backbone. We claim that\nDNN-specific dominant patterns reveal some essential properties of a DNN and\nare of great importance for its feature analysis and robustness enhancement.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:43:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ye", "Zhixing", ""], ["Qin", "Shaofei", ""], ["Chen", "Sizhe", ""], ["Huang", "Xiaolin", ""]]}, {"id": "2105.15064", "submitter": "William Blanzeisky", "authors": "William Blanzeisky, P\\'adraig Cunningham", "title": "Using Pareto Simulated Annealing to Address Algorithmic Bias in Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithmic Bias can be due to bias in the training data or issues with the\nalgorithm itself. These algorithmic issues typically relate to problems with\nmodel capacity and regularisation. This underestimation bias may arise because\nthe model has been optimised for good generalisation accuracy without any\nexplicit consideration of bias or fairness. In a sense, we should not be\nsurprised that a model might be biased when it hasn't been \"asked\" not to be.\nIn this paper, we consider including bias (underestimation) as an additional\ncriterion in model training. We present a multi-objective optimisation strategy\nusing Pareto Simulated Annealing that optimise for both balanced accuracy and\nunderestimation. We demonstrate the effectiveness of this strategy on one\nsynthetic and two real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:51:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Blanzeisky", "William", ""], ["Cunningham", "P\u00e1draig", ""]]}, {"id": "2105.15069", "submitter": "Alex Nowak-Vila", "authors": "Alex Nowak-Vila, Alessandro Rudi, Francis Bach", "title": "Max-Margin is Dead, Long Live Max-Margin!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The foundational concept of Max-Margin in machine learning is ill-posed for\noutput spaces with more than two labels such as in structured prediction. In\nthis paper, we show that the Max-Margin loss can only be consistent to the\nclassification task under highly restrictive assumptions on the discrete loss\nmeasuring the error between outputs. These conditions are satisfied by\ndistances defined in tree graphs, for which we prove consistency, thus being\nthe first losses shown to be consistent for Max-Margin beyond the binary\nsetting. We finally address these limitations by correcting the concept of\nMax-Margin and introducing the Restricted-Max-Margin, where the maximization of\nthe loss-augmented scores is maintained, but performed over a subset of the\noriginal domain. The resulting loss is also a generalization of the binary\nsupport vector machine and it is consistent under milder conditions on the\ndiscrete loss.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:55:52 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:52:27 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Nowak-Vila", "Alex", ""], ["Rudi", "Alessandro", ""], ["Bach", "Francis", ""]]}, {"id": "2105.15074", "submitter": "Sergio Contreras", "authors": "Vannessa de J. Duarte, Paul Leger, Sergio Contreras and Hiroaki Fukuda", "title": "Detecting Fetal Alcohol Spectrum Disorder in children using Artificial\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fetal alcohol spectrum disorder (FASD) is a syndrome whose only difference\ncompared to other children's conditions is the mother's alcohol consumption\nduring pregnancy. An earlier diagnosis of FASD improving the quality of life of\nchildren and adolescents. For this reason, this study focus on evaluating the\nuse of the artificial neural network (ANN) to classify children with FASD and\nexplore how accurate it is. ANN has been used to diagnose cancer, diabetes, and\nother diseases in the medical area, being a tool that presents good results.\nThe data used is from a battery of tests from children for 5-18 years old\n(include tests of psychometric, saccade eye movement, and diffusion tensor\nimaging (DTI)). We study the different configurations of ANN with dense layers.\nThe first one predicts 75\\% of the outcome correctly for psychometric data. The\nothers models include a feature layer, and we used it to predict FASD using\nevery test individually. The models accurately predict over 70\\% of the cases,\nand psychometric and memory guides predict over 88\\% accuracy. The results\nsuggest that the ANN approach is a competitive and efficient methodology to\ndetect FASD. However, we could be careful in used as a diagnostic technique.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:02:01 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Duarte", "Vannessa de J.", ""], ["Leger", "Paul", ""], ["Contreras", "Sergio", ""], ["Fukuda", "Hiroaki", ""]]}, {"id": "2105.15075", "submitter": "Yulin Wang", "authors": "Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, Gao Huang", "title": "Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with\n  Adaptive Sequence Length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision Transformers (ViT) have achieved remarkable success in large-scale\nimage recognition. They split every 2D image into a fixed number of patches,\neach of which is treated as a token. Generally, representing an image with more\ntokens would lead to higher prediction accuracy, while it also results in\ndrastically increased computational cost. To achieve a decent trade-off between\naccuracy and speed, the number of tokens is empirically set to 16x16. In this\npaper, we argue that every image has its own characteristics, and ideally the\ntoken number should be conditioned on each individual input. In fact, we have\nobserved that there exist a considerable number of \"easy\" images which can be\naccurately predicted with a mere number of 4x4 tokens, while only a small\nfraction of \"hard\" ones need a finer representation. Inspired by this\nphenomenon, we propose a Dynamic Transformer to automatically configure a\nproper number of tokens for each input image. This is achieved by cascading\nmultiple Transformers with increasing numbers of tokens, which are sequentially\nactivated in an adaptive fashion at test time, i.e., the inference is\nterminated once a sufficiently confident prediction is produced. We further\ndesign efficient feature reuse and relationship reuse mechanisms across\ndifferent components of the Dynamic Transformer to reduce redundant\ncomputations. Extensive empirical results on ImageNet, CIFAR-10, and CIFAR-100\ndemonstrate that our method significantly outperforms the competitive baselines\nin terms of both theoretical computational efficiency and practical inference\nspeed.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:04:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Yulin", ""], ["Huang", "Rui", ""], ["Song", "Shiji", ""], ["Huang", "Zeyi", ""], ["Huang", "Gao", ""]]}, {"id": "2105.15077", "submitter": "Fuxiang Tan", "authors": "Fuxiang Tan, YuTing Kong, Yingying Fan, Feng Liu, Daxin Zhou, Hao\n  zhang, Long Chen, Liang Gao and Yurong Qian", "title": "SDNet: mutil-branch for single image deraining using swin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streaks degrade the image quality and seriously affect the performance\nof subsequent computer vision tasks, such as autonomous driving, social\nsecurity, etc. Therefore, removing rain streaks from a given rainy images is of\ngreat significance. Convolutional neural networks(CNN) have been widely used in\nimage deraining tasks, however, the local computational characteristics of\nconvolutional operations limit the development of image deraining tasks.\nRecently, the popular transformer has global computational features that can\nfurther facilitate the development of image deraining tasks. In this paper, we\nintroduce Swin-transformer into the field of image deraining for the first time\nto study the performance and potential of Swin-transformer in the field of\nimage deraining. Specifically, we improve the basic module of Swin-transformer\nand design a three-branch model to implement single-image rain removal. The\nformer implements the basic rain pattern feature extraction, while the latter\nfuses different features to further extract and process the image features. In\naddition, we employ a jump connection to fuse deep features and shallow\nfeatures. In terms of experiments, the existing public dataset suffers from\nimage duplication and relatively homogeneous background. So we propose a new\ndataset Rain3000 to validate our model. Therefore, we propose a new dataset\nRain3000 for validating our model. Experimental results on the publicly\navailable datasets Rain100L, Rain100H and our dataset Rain3000 show that our\nproposed method has performance and inference speed advantages over the current\nmainstream single-image rain streaks removal models.The source code will be\navailable at https://github.com/H-tfx/SDNet.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:06:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tan", "Fuxiang", ""], ["Kong", "YuTing", ""], ["Fan", "Yingying", ""], ["Liu", "Feng", ""], ["Zhou", "Daxin", ""], ["zhang", "Hao", ""], ["Chen", "Long", ""], ["Gao", "Liang", ""], ["Qian", "Yurong", ""]]}, {"id": "2105.15078", "submitter": "Meng-Hao Guo", "authors": "Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, Dun Liang, Ralph R. Martin\n  and Shi-Min Hu", "title": "Can Attention Enable MLPs To Catch Up With CNNs?", "comments": "Computational Visual Media, 2021, accepted. 4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first week of May, 2021, researchers from four different institutions:\nGoogle, Tsinghua University, Oxford University and Facebook, shared their\nlatest work [16, 7, 12, 17] on arXiv.org almost at the same time, each\nproposing new learning architectures, consisting mainly of linear layers,\nclaiming them to be comparable, or even superior to convolutional-based models.\nThis sparked immediate discussion and debate in both academic and industrial\ncommunities as to whether MLPs are sufficient, many thinking that learning\narchitectures are returning to MLPs. Is this true? In this perspective, we give\na brief history of learning architectures, including multilayer perceptrons\n(MLPs), convolutional neural networks (CNNs) and transformers. We then examine\nwhat the four newly proposed architectures have in common. Finally, we give our\nviews on challenges and directions for new learning architectures, hoping to\ninspire future research.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:08:46 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Guo", "Meng-Hao", ""], ["Liu", "Zheng-Ning", ""], ["Mu", "Tai-Jiang", ""], ["Liang", "Dun", ""], ["Martin", "Ralph R.", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2105.15082", "submitter": "An Yang", "authors": "An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang\n  Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren\n  Zhou, Hongxia Yang", "title": "Exploring Sparse Expert Models and Beyond", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:12:44 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 15:28:15 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 13:45:24 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 03:01:55 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Yang", "An", ""], ["Lin", "Junyang", ""], ["Men", "Rui", ""], ["Zhou", "Chang", ""], ["Jiang", "Le", ""], ["Jia", "Xianyan", ""], ["Wang", "Ang", ""], ["Zhang", "Jie", ""], ["Wang", "Jiamang", ""], ["Li", "Yong", ""], ["Zhang", "Di", ""], ["Lin", "Wei", ""], ["Qu", "Lin", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""]]}, {"id": "2105.15094", "submitter": "Michael Horry Mr", "authors": "Michael J. Horry, Subrata Chakraborty, Biswajeet Pradhan, Maryam\n  Fallahpoor, Chegeni Hossein, Manoranjan Paul", "title": "Systematic investigation into generalization of COVID-19 CT deep\n  learning models with Gabor ensemble for lung involvement scoring", "comments": "39 Pages, 8 figures, 14 tables comparing the generalization of\n  COVID-19 CT Deep Learning Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has inspired unprecedented data collection and computer\nvision modelling efforts worldwide, focusing on diagnosis and stratification of\nCOVID-19 from medical images. Despite this large-scale research effort, these\nmodels have found limited practical application due in part to unproven\ngeneralization of these models beyond their source study. This study\ninvestigates the generalizability of key published models using the publicly\navailable COVID-19 Computed Tomography data through cross dataset validation.\nWe then assess the predictive ability of these models for COVID-19 severity\nusing an independent new dataset that is stratified for COVID-19 lung\ninvolvement. Each inter-dataset study is performed using histogram\nequalization, and contrast limited adaptive histogram equalization with and\nwithout a learning Gabor filter. The study shows high variability in the\ngeneralization of models trained on these datasets due to varied sample image\nprovenances and acquisition processes amongst other factors. We show that under\ncertain conditions, an internally consistent dataset can generalize well to an\nexternal dataset despite structural differences between these datasets with f1\nscores up to 86%. Our best performing model shows high predictive accuracy for\nlung involvement score for an independent dataset for which expertly labelled\nlung involvement stratification is available. Creating an ensemble of our best\nmodel for disease positive prediction with our best model for disease negative\nprediction using a min-max function resulted in a superior model for lung\ninvolvement prediction with average predictive accuracy of 75% for zero lung\ninvolvement and 96% for 75-100% lung involvement with almost linear\nrelationship between these stratifications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 03:49:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Horry", "Michael J.", ""], ["Chakraborty", "Subrata", ""], ["Pradhan", "Biswajeet", ""], ["Fallahpoor", "Maryam", ""], ["Hossein", "Chegeni", ""], ["Paul", "Manoranjan", ""]]}, {"id": "2105.15098", "submitter": "Yongxin Liu", "authors": "Yongxin Liu, Jian Wang, Jianqiang Li, Shuteng Niu, Houbing Song", "title": "Zero-bias Deep Learning Enabled Quick and Reliable Abnormality Detection\n  in IoT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormality detection is essential to the performance of safety-critical and\nlatency-constrained systems. However, as systems are becoming increasingly\ncomplicated with a large quantity of heterogeneous data, conventional\nstatistical change point detection methods are becoming less effective and\nefficient. Although Deep Learning (DL) and Deep Neural Networks (DNNs) are\nincreasingly employed to handle heterogeneous data, they still lack theoretic\nassurable performance and explainability. This paper integrates zero-bias DNN\nand Quickest Event Detection algorithms to provide a holistic framework for\nquick and reliable detection of both abnormalities and time-dependent abnormal\nevents in the Internet of Things (IoT). We first use the zero-bias dense layer\nto increase the explainability of DNN. We provide a solution to convert\nzero-bias DNN classifiers into performance assured binary abnormality\ndetectors. Using the converted abnormality detector, we then present a\nsequential quickest detection scheme that provides the theoretically assured\nlowest abnormal event detection delay under false alarm constraints. Finally,\nwe demonstrate the effectiveness of the framework using both massive signal\nrecords from real-world aviation communication systems and simulated data. Code\nand data of our work is available at\n\\url{https://github.com/pcwhy/AbnormalityDetectionInZbDNN}\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 03:31:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Yongxin", ""], ["Wang", "Jian", ""], ["Li", "Jianqiang", ""], ["Niu", "Shuteng", ""], ["Song", "Houbing", ""]]}, {"id": "2105.15103", "submitter": "Krishna Sivalingam", "authors": "Krishna M. Sivalingam", "title": "Applications of Artificial Intelligence, Machine Learning and related\n  techniques for Computer Networking Systems", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article presents a primer/overview of applications of Artificial\nIntelligence and Machine Learning (AI/ML) techniques to address problems in the\ndomain of computer networking. In particular, the techniques have been used to\nsupport efficient and accurate traffic prediction, traffic classification,\nanomaly detection, network management, network security, network resource\nallocation and optimization, network scheduling algorithms, fault diagnosis and\nmany more such applications. The article first summarizes some of the key\nnetworking concepts and a few representative machine learning techniques and\nalgorithms. The article then presents details regarding the availability of\ndata sets for networking applications and machine learning software and\ntoolkits for processing these data sets. Highlights of some of the standards\nactivities, pursued by ITU-T and ETSI, which are related to AI/ML for\nnetworking, are also presented. Finally, the article discusses a small set of\nrepresentative networking problems where AI/ML techniques have been\nsuccessfully applied.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 05:40:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sivalingam", "Krishna M.", ""]]}, {"id": "2105.15106", "submitter": "Shuanghong Shen", "authors": "Qi Liu, Shuanghong Shen, Zhenya Huang, Enhong Chen, and Yonghe Zheng", "title": "A Survey of Knowledge Tracing", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality education is one of the keys to achieving a more sustainable\nworld. The recent COVID-19 epidemic has triggered the outbreak of online\neducation, which has enabled both students and teachers to learn and teach at\nhome. Meanwhile, it is now possible to record and research a large amount of\nlearning data using online learning platforms in order to offer better\nintelligent educational services. Knowledge Tracing (KT), which aims to monitor\nstudents' evolving knowledge state, is a fundamental and crucial task to\nsupport these intelligent services. Therefore, an increasing amount of research\nattention has been paid to this emerging area and considerable progress has\nbeen made. In this survey, we propose a new taxonomy of existing basic KT\nmodels from a technical perspective and provide a comprehensive overview of\nthese models in a systematic manner. In addition, many variants of KT models\nhave been proposed to capture more complete learning process. We then review\nthese variants involved in three phases of the learning process: before,\nduring, and after the student learning, respectively. Moreover, we present\nseveral typical applications of KT in different educational scenarios. Finally,\nwe provide some potential directions for future research in this fast-growing\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:05:55 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 04:43:30 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Liu", "Qi", ""], ["Shen", "Shuanghong", ""], ["Huang", "Zhenya", ""], ["Chen", "Enhong", ""], ["Zheng", "Yonghe", ""]]}, {"id": "2105.15108", "submitter": "Elena Kronberg A.", "authors": "Elena A. Kronberg, Tanveer Hannan, Jens Huthmacher, Marcus M\\\"unzer,\n  Florian Peste, Ziyang Zhou, Max Berrendorf, Evgeniy Faerman, Fabio\n  Gastaldello, Simona Ghizzardi, Philippe Escoubet, Stein Haaland, Artem\n  Smirnov, Nithin Sivadas, Robert C. Allen, Andrea Tiengo, and Raluca Ilie", "title": "Prediction of soft proton intensities in the near-Earth space using\n  machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.space-ph astro-ph.EP astro-ph.SR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The spatial distribution of energetic protons contributes towards the\nunderstanding of magnetospheric dynamics. Based upon 17 years of the\nCluster/RAPID observations, we have derived machine learning-based models to\npredict the proton intensities at energies from 28 to 1,885 keV in the 3D\nterrestrial magnetosphere at radial distances between 6 and 22 RE. We used the\nsatellite location and indices for solar, solar wind and geomagnetic activity\nas predictors. The results demonstrate that the neural network (multi-layer\nperceptron regressor) outperforms baseline models based on the k-Nearest\nNeighbors and historical binning on average by ~80% and ~33\\%, respectively.\nThe average correlation between the observed and predicted data is about 56%,\nwhich is reasonable in light of the complex dynamics of fast-moving energetic\nprotons in the magnetosphere. In addition to a quantitative analysis of the\nprediction results, we also investigate parameter importance in our model. The\nmost decisive parameters for predicting proton intensities are related to the\nlocation: ZGSE direction and the radial distance. Among the activity indices,\nthe solar wind dynamic pressure is the most important. The results have a\ndirect practical application, for instance, for assessing the contamination\nparticle background in the X-Ray telescopes for X-ray astronomy orbiting above\nthe radiation belts. To foster reproducible research and to enable the\ncommunity to build upon our work we publish our complete code, the data, as\nwell as weights of trained models. Further description can be found in the\nGitHub project at https://github.com/Tanveer81/deep_horizon.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:33:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kronberg", "Elena A.", ""], ["Hannan", "Tanveer", ""], ["Huthmacher", "Jens", ""], ["M\u00fcnzer", "Marcus", ""], ["Peste", "Florian", ""], ["Zhou", "Ziyang", ""], ["Berrendorf", "Max", ""], ["Faerman", "Evgeniy", ""], ["Gastaldello", "Fabio", ""], ["Ghizzardi", "Simona", ""], ["Escoubet", "Philippe", ""], ["Haaland", "Stein", ""], ["Smirnov", "Artem", ""], ["Sivadas", "Nithin", ""], ["Allen", "Robert C.", ""], ["Tiengo", "Andrea", ""], ["Ilie", "Raluca", ""]]}, {"id": "2105.15119", "submitter": "Paulo Roberto de Oliveira da Costa", "authors": "Paulo da Costa, Peter Verleijsdonk, Simon Voorberg, Alp Akcay, Stella\n  Kapodistria, Willem van Jaarsveld and Yingqian Zhang", "title": "Policies for the Dynamic Traveling Maintainer Problem with Alerts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Companies require modern capital assets such as wind turbines, trains and\nhospital equipment to experience minimal downtime. Ideally, assets are\nmaintained right before failure to ensure maximum availability at minimum\nmaintenance costs. To this end, two challenges arise: failure times of assets\nare unknown a priori and assets can be part of a larger asset network.\nNowadays, it is common for assets to be equipped with real-time monitoring that\nemits alerts, typically triggered by the first signs of degradation. Thus, it\nbecomes crucial to plan maintenance considering information received via\nalerts, asset locations and maintenance costs. This problem is referred to as\nthe Dynamic Traveling Maintainer Problem with Alerts (DTMPA). We propose a\nmodeling framework for the DTMPA, where the alerts are early and imperfect\nindicators of failures. The objective is to minimize discounted maintenance\ncosts accrued over an infinite time horizon. We propose three methods to solve\nthis problem, leveraging different information levels from the alert signals.\nThe proposed methods comprise various greedy heuristics that rank assets based\non proximity, urgency and economic risk; a Traveling Maintainer Heuristic\nemploying combinatorial optimization to optimize near-future costs; a Deep\nReinforcement Learning (DRL) method trained to minimize the long-term costs\nusing exclusively the history of alerts. In a simulated environment, all\nmethods can approximate optimal policies with access to perfect condition\ninformation for small asset networks. For larger networks, where computing the\noptimal policy is intractable, the proposed methods yield competitive\nmaintenance policies, with DRL consistently achieving the lowest costs.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:35:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["da Costa", "Paulo", ""], ["Verleijsdonk", "Peter", ""], ["Voorberg", "Simon", ""], ["Akcay", "Alp", ""], ["Kapodistria", "Stella", ""], ["van Jaarsveld", "Willem", ""], ["Zhang", "Yingqian", ""]]}, {"id": "2105.15134", "submitter": "Zixin Wen", "authors": "Zixin Wen, Yuanzhi Li", "title": "Toward Understanding the Feature Learning Process of Self-supervised\n  Contrastive Learning", "comments": "V3 corrected related works. Accepted to ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can neural networks trained by contrastive learning extract features from\nthe unlabeled data? Why does contrastive learning usually need much stronger\ndata augmentations than supervised learning to ensure good representations?\nThese questions involve both the optimization and statistical aspects of deep\nlearning, but can hardly be answered by analyzing supervised learning, where\nthe target functions are the highest pursuit. Indeed, in self-supervised\nlearning, it is inevitable to relate to the optimization/generalization of\nneural networks to how they can encode the latent structures in the data, which\nwe refer to as the feature learning process.\n  In this work, we formally study how contrastive learning learns the feature\nrepresentations for neural networks by analyzing its feature learning process.\nWe consider the case where our data are comprised of two types of features: the\nmore semantically aligned sparse features which we want to learn from, and the\nother dense features we want to avoid. Theoretically, we prove that contrastive\nlearning using $\\mathbf{ReLU}$ networks provably learns the desired sparse\nfeatures if proper augmentations are adopted. We present an underlying\nprinciple called $\\textbf{feature decoupling}$ to explain the effects of\naugmentations, where we theoretically characterize how augmentations can reduce\nthe correlations of dense features between positive samples while keeping the\ncorrelations of sparse features intact, thereby forcing the neural networks to\nlearn from the self-supervision of sparse features. Empirically, we verified\nthat the feature decoupling principle matches the underlying mechanism of\ncontrastive learning in practice.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:42:09 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 10:05:33 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 17:48:01 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wen", "Zixin", ""], ["Li", "Yuanzhi", ""]]}, {"id": "2105.15162", "submitter": "Aciel Eshky", "authors": "Aciel Eshky, Joanne Cleland, Manuel Sam Ribeiro, Eleanor Sugden, Korin\n  Richmond, Steve Renals", "title": "Automatic audiovisual synchronisation for ultrasound tongue imaging", "comments": "18 pages, 10 figures. Manuscript accepted at Speech Communication", "journal-ref": null, "doi": "10.1016/j.specom.2021.05.008", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ultrasound tongue imaging is used to visualise the intra-oral articulators\nduring speech production. It is utilised in a range of applications, including\nspeech and language therapy and phonetics research. Ultrasound and speech audio\nare recorded simultaneously, and in order to correctly use this data, the two\nmodalities should be correctly synchronised. Synchronisation is achieved using\nspecialised hardware at recording time, but this approach can fail in practice\nresulting in data of limited usability. In this paper, we address the problem\nof automatically synchronising ultrasound and audio after data collection. We\nfirst investigate the tolerance of expert ultrasound users to synchronisation\nerrors in order to find the thresholds for error detection. We use these\nthresholds to define accuracy scoring boundaries for evaluating our system. We\nthen describe our approach for automatic synchronisation, which is driven by a\nself-supervised neural network, exploiting the correlation between the two\nsignals to synchronise them. We train our model on data from multiple domains\nwith different speaker characteristics, different equipment, and different\nrecording environments, and achieve an accuracy >92.4% on held-out in-domain\ndata. Finally, we introduce a novel resource, the Cleft dataset, which we\ngathered with a new clinical subgroup and for which hardware synchronisation\nproved unreliable. We apply our model to this out-of-domain data, and evaluate\nits performance subjectively with expert users. Results show that users prefer\nour model's output over the original hardware output 79.3% of the time. Our\nresults demonstrate the strength of our approach and its ability to generalise\nto data from new domains.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:11:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Eshky", "Aciel", ""], ["Cleland", "Joanne", ""], ["Ribeiro", "Manuel Sam", ""], ["Sugden", "Eleanor", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "2105.15164", "submitter": "Asma Ghandeharioun", "authors": "Asma Ghandeharioun, Been Kim, Chun-Liang Li, Brendan Jou, Brian Eoff,\n  Rosalind W. Picard", "title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining deep learning model inferences is a promising venue for scientific\nunderstanding, improving safety, uncovering hidden biases, evaluating fairness,\nand beyond, as argued by many scholars. One of the principal benefits of\ncounterfactual explanations is allowing users to explore \"what-if\" scenarios\nthrough what does not and cannot exist in the data, a quality that many other\nforms of explanation such as heatmaps and influence functions are inherently\nincapable of doing. However, most previous work on generative explainability\ncannot disentangle important concepts effectively, produces unrealistic\nexamples, or fails to retain relevant information. We propose a novel approach,\nDISSECT, that jointly trains a generator, a discriminator, and a concept\ndisentangler to overcome such challenges using little supervision. DISSECT\ngenerates Concept Traversals (CTs), defined as a sequence of generated examples\nwith increasing degrees of concepts that influence a classifier's decision. By\ntraining a generative model from a classifier's signal, DISSECT offers a way to\ndiscover a classifier's inherent \"notion\" of distinct concepts automatically\nrather than rely on user-predefined concepts. We show that DISSECT produces CTs\nthat (1) disentangle several concepts, (2) are influential to a classifier's\ndecision and are coupled to its reasoning due to joint training (3), are\nrealistic, (4) preserve relevant information, and (5) are stable across similar\ninputs. We validate DISSECT on several challenging synthetic and realistic\ndatasets where previous methods fall short of satisfying desirable criteria for\ninterpretability and show that it performs consistently well and better than\nexisting methods. Finally, we present experiments showing applications of\nDISSECT for detecting potential biases of a classifier and identifying spurious\nartifacts that impact predictions.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:11:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ghandeharioun", "Asma", ""], ["Kim", "Been", ""], ["Li", "Chun-Liang", ""], ["Jou", "Brendan", ""], ["Eoff", "Brian", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "2105.15168", "submitter": "Jiemin Fang", "authors": "Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, Qi\n  Tian", "title": "MSG-Transformer: Exchanging Local Spatial Information by Manipulating\n  Messenger Tokens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have offered a new methodology of designing neural networks for\nvisual recognition. Compared to convolutional networks, Transformers enjoy the\nability of referring to global features at each stage, yet the attention module\nbrings higher computational overhead that obstructs the application of\nTransformers to process high-resolution visual data. This paper aims to\nalleviate the conflict between efficiency and flexibility, for which we propose\na specialized token for each region that serves as a messenger (MSG). Hence, by\nmanipulating these MSG tokens, one can flexibly exchange visual information\nacross regions and the computational complexity is reduced. We then integrate\nthe MSG token into a multi-scale architecture named MSG-Transformer. In\nstandard image classification and object detection, MSG-Transformer achieves\ncompetitive performance and the inference on both GPU and CPU is accelerated.\nThe code will be available at https://github.com/hustvl/MSG-Transformer.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:16:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Fang", "Jiemin", ""], ["Xie", "Lingxi", ""], ["Wang", "Xinggang", ""], ["Zhang", "Xiaopeng", ""], ["Liu", "Wenyu", ""], ["Tian", "Qi", ""]]}, {"id": "2105.15176", "submitter": "Tianyang Xu", "authors": "Tianyang Xu, Chunyun Zhang", "title": "Reinforced Generative Adversarial Network for Abstractive Text\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models provide a viable new approach to generative\nsummarization, allowing models that are no longer limited to simply selecting\nand recombining sentences from the original text. However, these models have\nthree drawbacks: their grasp of the details of the original text is often\ninaccurate, and the text generated by such models often has repetitions, while\nit is difficult to handle words that are beyond the word list. In this paper,\nwe propose a new architecture that combines reinforcement learning and\nadversarial generative networks to enhance the sequence-to-sequence attention\nmodel. First, we use a hybrid pointer-generator network that copies words\ndirectly from the source text, contributing to accurate reproduction of\ninformation without sacrificing the ability of generators to generate new\nwords. Second, we use both intra-temporal and intra-decoder attention to\npenalize summarized content and thus discourage repetition. We apply our model\nto our own proposed COVID-19 paper title summarization task and achieve close\napproximations to the current model on ROUEG, while bringing better\nreadability.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:34:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Tianyang", ""], ["Zhang", "Chunyun", ""]]}, {"id": "2105.15182", "submitter": "Peter Zhang", "authors": "Runshan Fu, Yangfan Liang, Peter Zhang", "title": "Model Mis-specification and Algorithmic Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Machine learning algorithms are increasingly used to inform critical\ndecisions. There is a growing concern about bias, that algorithms may produce\nuneven outcomes for individuals in different demographic groups. In this work,\nwe measure bias as the difference between mean prediction errors across groups.\nWe show that even with unbiased input data, when a model is mis-specified: (1)\npopulation-level mean prediction error can still be negligible, but group-level\nmean prediction errors can be large; (2) such errors are not equal across\ngroups; and (3) the difference between errors, i.e., bias, can take the\nworst-case realization. That is, when there are two groups of the same size,\nmean prediction errors for these two groups have the same magnitude but\nopposite signs. In closed form, we show such errors and bias are functions of\nthe first and second moments of the joint distribution of features (for linear\nand probit regressions). We also conduct numerical experiments to show similar\nresults in more general settings. Our work provides a first step for decoupling\nthe impact of different causes of bias.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:45:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Fu", "Runshan", ""], ["Liang", "Yangfan", ""], ["Zhang", "Peter", ""]]}, {"id": "2105.15183", "submitter": "Mathieu Blondel", "authors": "Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan\n  Hoyer, Felipe Llinares-L\\'opez, Fabian Pedregosa, Jean-Philippe Vert", "title": "Efficient and Modular Implicit Differentiation", "comments": "V2: some corrections and link to software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic differentiation (autodiff) has revolutionized machine learning. It\nallows expressing complex computations by composing elementary ones in creative\nways and removes the burden of computing their derivatives by hand. More\nrecently, differentiation of optimization problem solutions has attracted\nwidespread attention with applications such as optimization as a layer, and in\nbi-level problems such as hyper-parameter optimization and meta-learning.\nHowever, the formulas for these derivatives often involve case-by-case tedious\nmathematical derivations. In this paper, we propose a unified, efficient and\nmodular approach for implicit differentiation of optimization problems. In our\napproach, the user defines (in Python in the case of our implementation) a\nfunction $F$ capturing the optimality conditions of the problem to be\ndifferentiated. Once this is done, we leverage autodiff of $F$ and implicit\ndifferentiation to automatically differentiate the optimization problem. Our\napproach thus combines the benefits of implicit differentiation and autodiff.\nIt is efficient as it can be added on top of any state-of-the-art solver and\nmodular as the optimality condition specification is decoupled from the\nimplicit differentiation mechanism. We show that seemingly simple principles\nallow to recover many recently proposed implicit differentiation methods and\ncreate new ones easily. We demonstrate the ease of formulating and solving\nbi-level optimization problems using our framework. We also showcase an\napplication to the sensitivity analysis of molecular dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:45:58 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 21:40:01 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Blondel", "Mathieu", ""], ["Berthet", "Quentin", ""], ["Cuturi", "Marco", ""], ["Frostig", "Roy", ""], ["Hoyer", "Stephan", ""], ["Llinares-L\u00f3pez", "Felipe", ""], ["Pedregosa", "Fabian", ""], ["Vert", "Jean-Philippe", ""]]}, {"id": "2105.15186", "submitter": "Shicong Cen", "authors": "Shicong Cen, Yuting Wei, Yuejie Chi", "title": "Fast Policy Extragradient Methods for Competitive Games with Entropy\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of computing the equilibrium of\ncompetitive games, which is often modeled as a constrained saddle-point\noptimization problem with probability simplex constraints. Despite recent\nefforts in understanding the last-iterate convergence of extragradient methods\nin the unconstrained setting, the theoretical underpinnings of these methods in\nthe constrained settings, especially those using multiplicative updates, remain\nhighly inadequate, even when the objective function is bilinear. Motivated by\nthe algorithmic role of entropy regularization in single-agent reinforcement\nlearning and game theory, we develop provably efficient extragradient methods\nto find the quantal response equilibrium (QRE) -- which are solutions to\nzero-sum two-player matrix games with entropy regularization -- at a linear\nrate. The proposed algorithms can be implemented in a decentralized manner,\nwhere each player executes symmetric and multiplicative updates iteratively\nusing its own payoff without observing the opponent's actions directly. In\naddition, by controlling the knob of entropy regularization, the proposed\nalgorithms can locate an approximate Nash equilibrium of the unregularized\nmatrix game at a sublinear rate without assuming the Nash equilibrium to be\nunique. Our methods also lead to efficient policy extragradient algorithms for\nsolving entropy-regularized zero-sum Markov games at a linear rate. All of our\nconvergence rates are nearly dimension-free, which are independent of the size\nof the state and action spaces up to logarithm factors, highlighting the\npositive role of entropy regularization for accelerating convergence.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:51:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cen", "Shicong", ""], ["Wei", "Yuting", ""], ["Chi", "Yuejie", ""]]}, {"id": "2105.15189", "submitter": "Brady Moon", "authors": "Arnav Choudhry, Brady Moon, Jay Patrikar, Constantine Samaras,\n  Sebastian Scherer", "title": "CVaR-based Flight Energy Risk Assessment for Multirotor UAVs using a\n  Deep Energy Model", "comments": "7 pages, 8 figures, Submitted ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy management is a critical aspect of risk assessment for Uncrewed Aerial\nVehicle (UAV) flights, as a depleted battery during a flight brings almost\nguaranteed vehicle damage and a high risk of human injuries or property damage.\nPredicting the amount of energy a flight will consume is challenging as\nrouting, weather, obstacles, and other factors affect the overall consumption.\nWe develop a deep energy model for a UAV that uses Temporal Convolutional\nNetworks to capture the time varying features while incorporating static\ncontextual information. Our energy model is trained on a real world dataset and\ndoes not require segregating flights into regimes. We illustrate an improvement\nin power predictions by $29\\%$ on test flights when compared to a\nstate-of-the-art analytical method. Using the energy model, we can predict the\nenergy usage for a given trajectory and evaluate the risk of running out of\nbattery during flight. We propose using Conditional Value-at-Risk (CVaR) as a\nmetric for quantifying this risk. We show that CVaR captures the risk\nassociated with worst-case energy consumption on a nominal path by transforming\nthe output distribution of Monte Carlo forward simulations into a risk space.\nComputing the CVaR on the risk-space distribution provides a metric that can\nevaluate the overall risk of a flight before take-off. Our energy model and\nrisk evaluation method can improve flight safety and evaluate the coverage area\nfrom a proposed takeoff location.\n  The video and codebase are available at https://youtu.be/PHXGigqilOA and\nhttps://git.io/cvar-risk .\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:51:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Choudhry", "Arnav", ""], ["Moon", "Brady", ""], ["Patrikar", "Jay", ""], ["Samaras", "Constantine", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2105.15191", "submitter": "Siddharth Divi", "authors": "Siddharth Divi, Habiba Farrukh, Berkay Celik", "title": "Unifying Distillation with Personalization in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning (FL) is a decentralized privacy-preserving learning\ntechnique in which clients learn a joint collaborative model through a central\naggregator without sharing their data. In this setting, all clients learn a\nsingle common predictor (FedAvg), which does not generalize well on each\nclient's local data due to the statistical data heterogeneity among clients. In\nthis paper, we address this problem with PersFL, a discrete two-stage\npersonalized learning algorithm. In the first stage, PersFL finds the optimal\nteacher model of each client during the FL training phase. In the second stage,\nPersFL distills the useful knowledge from optimal teachers into each user's\nlocal model. The teacher model provides each client with some rich, high-level\nrepresentation that a client can easily adapt to its local model, which\novercomes the statistical heterogeneity present at different clients. We\nevaluate PersFL on CIFAR-10 and MNIST datasets using three data-splitting\nstrategies to control the diversity between clients' data distributions. We\nempirically show that PersFL outperforms FedAvg and three state-of-the-art\npersonalization methods, pFedMe, Per-FedAvg, and FedPer on majority data-splits\nwith minimal communication cost. Further, we study the performance of PersFL on\ndifferent distillation objectives, how this performance is affected by the\nequitable notion of fairness among clients, and the number of required\ncommunication rounds. PersFL code is available at https://tinyurl.com/hdh5zhxs\nfor public use and validation.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:54:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Divi", "Siddharth", ""], ["Farrukh", "Habiba", ""], ["Celik", "Berkay", ""]]}, {"id": "2105.15197", "submitter": "Rahul Singh", "authors": "Victor Chernozhukov, Whitney K. Newey, Rahul Singh", "title": "A Simple and General Debiased Machine Learning Theorem with Finite\n  Sample Guarantees", "comments": "25 pages. arXiv admin note: text overlap with arXiv:2102.11076", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Debiased machine learning is a meta algorithm based on bias correction and\nsample splitting to calculate confidence intervals for functionals (i.e. scalar\nsummaries) of machine learning algorithms. For example, an analyst may desire\nthe confidence interval for a treatment effect estimated with a neural network.\nWe provide a nonasymptotic debiased machine learning theorem that encompasses\nany global or local functional of any machine learning algorithm that satisfies\na few simple, interpretable conditions. Formally, we prove consistency,\nGaussian approximation, and semiparametric efficiency by finite sample\narguments. The rate of convergence is root-n for global functionals, and it\ndegrades gracefully for local functionals. Our results culminate in a simple\nset of conditions that an analyst can use to translate modern learning theory\nrates into traditional statistical inference. The conditions reveal a new\ndouble robustness property for ill posed inverse problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:57:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Newey", "Whitney K.", ""], ["Singh", "Rahul", ""]]}, {"id": "2105.15203", "submitter": "Enze Xie", "authors": "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez,\n  Ping Luo", "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with\n  Transformers", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SegFormer, a simple, efficient yet powerful semantic segmentation\nframework which unifies Transformers with lightweight multilayer perception\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a\nnovel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the\ninterpolation of positional codes which leads to decreased performance when the\ntesting resolution differs from training. 2) SegFormer avoids complex decoders.\nThe proposed MLP decoder aggregates information from different layers, and thus\ncombining both local attention and global attention to render powerful\nrepresentations. We show that this simple and lightweight design is the key to\nefficient segmentation on Transformers. We scale our approach up to obtain a\nseries of models from SegFormer-B0 to SegFormer-B5, reaching significantly\nbetter performance and efficiency than previous counterparts. For example,\nSegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x\nsmaller and 2.2% better than the previous best method. Our best model,\nSegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows\nexcellent zero-shot robustness on Cityscapes-C. Code will be released at:\ngithub.com/NVlabs/SegFormer.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:59:51 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 22:51:54 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xie", "Enze", ""], ["Wang", "Wenhai", ""], ["Yu", "Zhiding", ""], ["Anandkumar", "Anima", ""], ["Alvarez", "Jose M.", ""], ["Luo", "Ping", ""]]}]