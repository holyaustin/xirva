[{"id": "1408.0017", "submitter": "Walid Krichene", "authors": "Walid Krichene, Benjamin Drigh\\`es and Alexandre M. Bayen", "title": "Learning Nash Equilibria in Congestion Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the repeated congestion game, in which multiple populations of\nplayers share resources, and make, at each iteration, a decentralized decision\non which resources to utilize. We investigate the following question: given a\nmodel of how individual players update their strategies, does the resulting\ndynamics of strategy profiles converge to the set of Nash equilibria of the\none-shot game? We consider in particular a model in which players update their\nstrategies using algorithms with sublinear discounted regret. We show that the\nresulting sequence of strategy profiles converges to the set of Nash equilibria\nin the sense of Ces\\`aro means. However, strong convergence is not guaranteed\nin general. We show that strong convergence can be guaranteed for a class of\nalgorithms with a vanishing upper bound on discounted regret, and which satisfy\nan additional condition. We call such algorithms AREP algorithms, for\nApproximate REPlicator, as they can be interpreted as a discrete-time\napproximation of the replicator equation, which models the continuous-time\nevolution of population strategies, and which is known to converge for the\nclass of congestion games. In particular, we show that the discounted Hedge\nalgorithm belongs to the AREP class, which guarantees its strong convergence.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 20:10:14 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Krichene", "Walid", ""], ["Drigh\u00e8s", "Benjamin", ""], ["Bayen", "Alexandre M.", ""]]}, {"id": "1408.0043", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Learning From Ordered Sets and Applications in Collaborative Ranking", "comments": "JMLR: Workshop and Conference Proceedings 25:1-16, 2012, Asian\n  Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking over sets arise when users choose between groups of items. For\nexample, a group may be of those movies deemed $5$ stars to them, or a\ncustomized tour package. It turns out, to model this data type properly, we\nneed to investigate the general combinatorics problem of partitioning a set and\nordering the subsets. Here we construct a probabilistic log-linear model over a\nset of ordered subsets. Inference in this combinatorial space is highly\nchallenging: The space size approaches $(N!/2)6.93145^{N+1}$ as $N$ approaches\ninfinity. We propose a \\texttt{split-and-merge} Metropolis-Hastings procedure\nthat can explore the state-space efficiently. For discovering hidden aspects in\nthe data, we enrich the model with latent binary variables so that the\nposteriors can be efficiently evaluated. Finally, we evaluate the proposed\nmodel on large-scale collaborative filtering tasks and demonstrate that it is\ncompetitive against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 23:30:37 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.0047", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Cumulative Restricted Boltzmann Machines for Ordinal Matrix Data\n  Analysis", "comments": "JMLR: Workshop and Conference Proceedings 25:1-16, 2012; Asian\n  Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal data is omnipresent in almost all multiuser-generated feedback -\nquestionnaires, preferences etc. This paper investigates modelling of ordinal\ndata with Gaussian restricted Boltzmann machines (RBMs). In particular, we\npresent the model architecture, learning and inference procedures for both\nvector-variate and matrix-variate ordinal data. We show that our model is able\nto capture latent opinion profile of citizens around the world, and is\ncompetitive against state-of-art collaborative filtering techniques on\nlarge-scale public datasets. The model thus has the potential to extend\napplication of RBMs to diverse domains such as recommendation systems, product\nreviews and expert assessments.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 23:54:16 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.0055", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Thurstonian Boltzmann Machines: Learning from Multiple Inequalities", "comments": "Proceedings of the 30 th International Conference on Machine\n  Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture\nthat can naturally incorporate a wide range of data inputs at the same time.\nOur motivation rests in the Thurstonian view that many discrete data types can\nbe considered as being generated from a subset of underlying latent continuous\nvariables, and in the observation that each realisation of a discrete type\nimposes certain inequalities on those variables. Thus learning and inference in\nTBM reduce to making sense of a set of inequalities. Our proposed TBM naturally\nsupports the following types: Gaussian, intervals, censored, binary,\ncategorical, muticategorical, ordinal, (in)-complete rank with and without\nties. We demonstrate the versatility and capacity of the proposed model on\nthree applications of very different natures; namely handwritten digit\nrecognition, collaborative filtering and complex social survey analysis.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 00:32:32 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.0058", "submitter": "Mehrab Norouzitallab", "authors": "Mehrab Norouzitallab, Valiallah Monajjemi, Saeed Shiry Ghidary and\n  Mohammad Bagher Menhaj", "title": "A Framework for learning multi-agent dynamic formation strategy in\n  real-time applications", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formation strategy is one of the most important parts of many multi-agent\nsystems with many applications in real world problems. In this paper, a\nframework for learning this task in a limited domain (restricted environment)\nis proposed. In this framework, agents learn either directly by observing an\nexpert behavior or indirectly by observing other agents or objects behavior.\nFirst, a group of algorithms for learning formation strategy based on limited\nfeatures will be presented. Due to distributed and complex nature of many\nmulti-agent systems, it is impossible to include all features directly in the\nlearning process; thus, a modular scheme is proposed in order to reduce the\nnumber of features. In this method, some important features have indirect\ninfluence in learning instead of directly involving them as input features.\nThis framework has the ability to dynamically assign a group of positions to a\ngroup of agents to improve system performance. In addition, it can change the\nformation strategy when the context changes. Finally, this framework is able to\nautomatically produce many complex and flexible formation strategy algorithms\nwithout directly involving an expert to present and implement such complex\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 01:29:08 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Norouzitallab", "Mehrab", ""], ["Monajjemi", "Valiallah", ""], ["Ghidary", "Saeed Shiry", ""], ["Menhaj", "Mohammad Bagher", ""]]}, {"id": "1408.0096", "submitter": "Jiankou Li", "authors": "Jiankou Li and Wei Zhang", "title": "Conditional Restricted Boltzmann Machines for Cold Start Recommendations", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzman Machines (RBMs) have been successfully used in\nrecommender systems. However, as with most of other collaborative filtering\ntechniques, it cannot solve cold start problems for there is no rating for a\nnew item. In this paper, we first apply conditional RBM (CRBM) which could take\nextra information into account and show that CRBM could solve cold start\nproblem very well, especially for rating prediction task. CRBM naturally\ncombine the content and collaborative data under a single framework which could\nbe fitted effectively. Experiments show that CRBM can be compared favourably\nwith matrix factorization models, while hidden features learned from the former\nmodels are more easy to be interpreted.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 07:51:37 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Li", "Jiankou", ""], ["Zhang", "Wei", ""]]}, {"id": "1408.0193", "submitter": "Zaid Albataineh", "authors": "Zaid Albataineh and Fathi M. Salem", "title": "A RobustICA Based Algorithm for Blind Separation of Convolutive Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a frequency domain method based on robust independent component\nanalysis (RICA) to address the multichannel Blind Source Separation (BSS)\nproblem of convolutive speech mixtures in highly reverberant environments. We\nimpose regularization processes to tackle the ill-conditioning problem of the\ncovariance matrix and to mitigate the performance degradation in the frequency\ndomain. We apply an algorithm to separate the source signals in adverse\nconditions, i.e. high reverberation conditions when short observation signals\nare available. Furthermore, we study the impact of several parameters on the\nperformance of separation, e.g. overlapping ratio and window type of the\nfrequency domain method. We also compare different techniques to solve the\nfrequency-domain permutation ambiguity. Through simulations and real world\nexperiments, we verify the superiority of the presented convolutive algorithm\namong other BSS algorithms, including recursive regularized ICA (RR ICA),\nindependent vector analysis (IVA).\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 14:47:33 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Albataineh", "Zaid", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1408.0196", "submitter": "Zaid Albataineh", "authors": "Zaid Albataineh and Fathi M. Salem", "title": "A Blind Adaptive CDMA Receiver Based on State Space Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code Division Multiple Access (CDMA) is a channel access method, based on\nspread-spectrum technology, used by various radio technologies world-wide. In\ngeneral, CDMA is used as an access method in many mobile standards such as\nCDMA2000 and WCDMA. We address the problem of blind multiuser equalization in\nthe wideband CDMA system, in the noisy multipath propagation environment.\nHerein, we propose three new blind receiver schemes, which are based on state\nspace structures and Independent Component Analysis (ICA). These blind\nstate-space receivers (BSSR) do not require knowledge of the propagation\nparameters or spreading code sequences of the users they primarily exploit the\nnatural assumption of statistical independence among the source signals. We\nalso develop three semi blind adaptive detectors by incorporating the new\nadaptive methods into the standard RAKE receiver structure. Extensive\ncomparative case study, based on Bit error rate (BER) performance of these\nmethods, is carried out for different number of users, symbols per user, and\nsignal to noise ratio (SNR) in comparison with conventional detectors,\nincluding the Blind Multiuser Detectors (BMUD) and Linear Minimum mean squared\nerror (LMMSE). The results show that the proposed methods outperform the other\ndetectors in estimating the symbol signals from the received mixed CDMA\nsignals. Moreover, the new blind detectors mitigate the multi access\ninterference (MAI) in CDMA.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 14:52:47 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 22:26:25 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Albataineh", "Zaid", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1408.0204", "submitter": "Momiao Xiong", "authors": "Nan Lin, Junhai Jiang, Shicheng Guo and Momiao Xiong", "title": "Functional Principal Component Analysis and Randomized Sparse Clustering\n  Algorithm for Medical Image Analysis", "comments": "35 pages, 2 figures, 6 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0132945", "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to advances in sensors, growing large and complex medical image data have\nthe ability to visualize the pathological change in the cellular or even the\nmolecular level or anatomical changes in tissues and organs. As a consequence,\nthe medical images have the potential to enhance diagnosis of disease,\nprediction of clinical outcomes, characterization of disease progression,\nmanagement of health care and development of treatments, but also pose great\nmethodological and computational challenges for representation and selection of\nfeatures in image cluster analysis. To address these challenges, we first\nextend one dimensional functional principal component analysis to the two\ndimensional functional principle component analyses (2DFPCA) to fully capture\nspace variation of image signals. Image signals contain a large number of\nredundant and irrelevant features which provide no additional or no useful\ninformation for cluster analysis. Widely used methods for removing redundant\nand irrelevant features are sparse clustering algorithms using a lasso-type\npenalty to select the features. However, the accuracy of clustering using a\nlasso-type penalty depends on how to select penalty parameters and a threshold\nfor selecting features. In practice, they are difficult to determine. Recently,\nrandomized algorithms have received a great deal of attention in big data\nanalysis. This paper presents a randomized algorithm for accurate feature\nselection in image cluster analysis. The proposed method is applied to ovarian\nand kidney cancer histology image data from the TCGA database. The results\ndemonstrate that the randomized feature selection method coupled with\nfunctional principal component analysis substantially outperforms the current\nsparse clustering algorithms in image cluster analysis.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 15:15:48 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Lin", "Nan", ""], ["Jiang", "Junhai", ""], ["Guo", "Shicheng", ""], ["Xiong", "Momiao", ""]]}, {"id": "1408.0325", "submitter": "Mehrdad Mahdavi", "authors": "Rana Forsati, Mehrdad Mahdavi, Mehrnoush Shamsfard, Mohamed Sarwat", "title": "Matrix Factorization with Explicit Trust and Distrust Relationships", "comments": "ACM Transactions on Information Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of online social networks, recommender systems have became\ncrucial for the success of many online applications/services due to their\nsignificance role in tailoring these applications to user-specific needs or\npreferences. Despite their increasing popularity, in general recommender\nsystems suffer from the data sparsity and the cold-start problems. To alleviate\nthese issues, in recent years there has been an upsurge of interest in\nexploiting social information such as trust relations among users along with\nthe rating data to improve the performance of recommender systems. The main\nmotivation for exploiting trust information in recommendation process stems\nfrom the observation that the ideas we are exposed to and the choices we make\nare significantly influenced by our social context. However, in large user\ncommunities, in addition to trust relations, the distrust relations also exist\nbetween users. For instance, in Epinions the concepts of personal \"web of\ntrust\" and personal \"block list\" allow users to categorize their friends based\non the quality of reviews into trusted and distrusted friends, respectively. In\nthis paper, we propose a matrix factorization based model for recommendation in\nsocial rating networks that properly incorporates both trust and distrust\nrelationships aiming to improve the quality of recommendations and mitigate the\ndata sparsity and the cold-start users issues. Through experiments on the\nEpinions data set, we show that our new algorithm outperforms its standard\ntrust-enhanced or distrust-enhanced counterparts with respect to accuracy,\nthereby demonstrating the positive effect that incorporation of explicit\ndistrust information can have on recommender systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Aug 2014 01:56:10 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Forsati", "Rana", ""], ["Mahdavi", "Mehrdad", ""], ["Shamsfard", "Mehrnoush", ""], ["Sarwat", "Mohamed", ""]]}, {"id": "1408.0553", "submitter": "Majid Janzamin", "authors": "Animashree Anandkumar and Rong Ge and Majid Janzamin", "title": "Sample Complexity Analysis for Learning Overcomplete Latent Variable\n  Models through Tensor Methods", "comments": "Title changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide guarantees for learning latent variable models emphasizing on the\novercomplete regime, where the dimensionality of the latent space can exceed\nthe observed dimensionality. In particular, we consider multiview mixtures,\nspherical Gaussian mixtures, ICA, and sparse coding models. We provide tight\nconcentration bounds for empirical moments through novel covering arguments. We\nanalyze parameter recovery through a simple tensor power update algorithm. In\nthe semi-supervised setting, we exploit the label or prior information to get a\nrough estimate of the model parameters, and then refine it using the tensor\nmethod on unlabeled samples. We establish that learning is possible when the\nnumber of components scales as $k=o(d^{p/2})$, where $d$ is the observed\ndimension, and $p$ is the order of the observed moment employed in the tensor\nmethod. Our concentration bound analysis also leads to minimax sample\ncomplexity for semi-supervised learning of spherical Gaussian mixtures. In the\nunsupervised setting, we use a simple initialization algorithm based on SVD of\nthe tensor slices, and provide guarantees under the stricter condition that\n$k\\le \\beta d$ (where constant $\\beta$ can be larger than $1$), where the\ntensor method recovers the components under a polynomial running time (and\nexponential in $\\beta$). Our analysis establishes that a wide range of\novercomplete latent variable models can be learned efficiently with low\ncomputational and sample complexity through tensor decomposition methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 23:21:33 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 22:21:23 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Ge", "Rong", ""], ["Janzamin", "Majid", ""]]}, {"id": "1408.0838", "submitter": "Bjoern Andres", "authors": "Lizhen Qu and Bjoern Andres", "title": "Estimating Maximally Probable Constrained Relations by Mathematical\n  Programming", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a constrained relation is a fundamental problem in machine\nlearning. Special cases are classification (the problem of estimating a map\nfrom a set of to-be-classified elements to a set of labels), clustering (the\nproblem of estimating an equivalence relation on a set) and ranking (the\nproblem of estimating a linear order on a set). We contribute a family of\nprobability measures on the set of all relations between two finite, non-empty\nsets, which offers a joint abstraction of multi-label classification,\ncorrelation clustering and ranking by linear ordering. Estimating (learning) a\nmaximally probable measure, given (a training set of) related and unrelated\npairs, is a convex optimization problem. Estimating (inferring) a maximally\nprobable relation, given a measure, is a 01-linear program. It is solved in\nlinear time for maps. It is NP-hard for equivalence relations and linear\norders. Practical solutions for all three cases are shown in experiments with\nreal data. Finally, estimating a maximally probable measure and relation\njointly is posed as a mixed-integer nonlinear program. This formulation\nsuggests a mathematical programming approach to semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 23:30:20 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Qu", "Lizhen", ""], ["Andres", "Bjoern", ""]]}, {"id": "1408.0848", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Multilayer bootstrap networks", "comments": "accepted for publication by Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear\nnetwork from bottom up for unsupervised nonlinear dimensionality reduction.\nEach layer of the network is a nonparametric density estimator. It consists of\na group of k-centroids clusterings. Each clustering randomly selects data\npoints with randomly selected features as its centroids, and learns a one-hot\nencoder by one-nearest-neighbor optimization. Geometrically, the nonparametric\ndensity estimator at each layer projects the input data space to a\nuniformly-distributed discrete feature space, where the similarity of two data\npoints in the discrete feature space is measured by the number of the nearest\ncentroids they share in common. The multilayer network gradually reduces the\nnonlinear variations of data from bottom up by building a vast number of\nhierarchical trees implicitly on the original data space. Theoretically, the\nestimation error caused by the nonparametric density estimator is proportional\nto the correlation between the clusterings, both of which are reduced by the\nrandomization steps.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 02:13:50 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 04:26:21 GMT"}, {"version": "v3", "created": "Sun, 1 Feb 2015 03:11:40 GMT"}, {"version": "v4", "created": "Tue, 10 Feb 2015 09:09:46 GMT"}, {"version": "v5", "created": "Mon, 4 Jan 2016 17:50:27 GMT"}, {"version": "v6", "created": "Thu, 7 Jan 2016 08:07:44 GMT"}, {"version": "v7", "created": "Mon, 6 Jun 2016 18:00:32 GMT"}, {"version": "v8", "created": "Tue, 6 Mar 2018 15:59:10 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1408.0853", "submitter": "Masahiro Yukawa", "authors": "Masahiro Yukawa", "title": "Adaptive Learning in Cartesian Product of Reproducing Kernel Hilbert\n  Spaces", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2463261", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive learning algorithm based on iterative orthogonal\nprojections in the Cartesian product of multiple reproducing kernel Hilbert\nspaces (RKHSs). The task is estimating/tracking nonlinear functions which are\nsupposed to contain multiple components such as (i) linear and nonlinear\ncomponents, (ii) high- and low- frequency components etc. In this case, the use\nof multiple RKHSs permits a compact representation of multicomponent functions.\nThe proposed algorithm is where two different methods of the author meet:\nmultikernel adaptive filtering and the algorithm of hyperplane projection along\naffine subspace (HYPASS). In a certain particular case, the sum space of the\nRKHSs is isomorphic to the product space and hence the proposed algorithm can\nalso be regarded as an iterative projection method in the sum space. The\nefficacy of the proposed algorithm is shown by numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 02:31:27 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 04:53:17 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Yukawa", "Masahiro", ""]]}, {"id": "1408.0967", "submitter": "Shaina Race", "authors": "Shaina Race, Carl Meyer, Kevin Valakuzhy", "title": "Determining the Number of Clusters via Iterative Consensus Clustering", "comments": "Proceedings of the 2013 SIAM International Conference on Data Mining", "journal-ref": null, "doi": "10.1137/1.9781611972832.11", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a cluster ensemble to determine the number of clusters, k, in a group\nof data. A consensus similarity matrix is formed from the ensemble using\nmultiple algorithms and several values for k. A random walk is induced on the\ngraph defined by the consensus matrix and the eigenvalues of the associated\ntransition probability matrix are used to determine the number of clusters. For\nnoisy or high-dimensional data, an iterative technique is presented to refine\nthis consensus matrix in way that encourages a block-diagonal form. It is shown\nthat the resulting consensus matrix is generally superior to existing\nsimilarity matrices for this type of spectral analysis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 13:40:03 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Race", "Shaina", ""], ["Meyer", "Carl", ""], ["Valakuzhy", "Kevin", ""]]}, {"id": "1408.0972", "submitter": "Shaina Race Ph.D", "authors": "Shaina Race and Carl Meyer", "title": "A Flexible Iterative Framework for Consensus Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel framework for consensus clustering is presented which has the ability\nto determine both the number of clusters and a final solution using multiple\nalgorithms. A consensus similarity matrix is formed from an ensemble using\nmultiple algorithms and several values for k. A variety of dimension reduction\ntechniques and clustering algorithms are considered for analysis. For noisy or\nhigh-dimensional data, an iterative technique is presented to refine this\nconsensus matrix in way that encourages algorithms to agree upon a common\nsolution. We utilize the theory of nearly uncoupled Markov chains to determine\nthe number, k , of clusters in a dataset by considering a random walk on the\ngraph defined by the consensus matrix. The eigenvalues of the associated\ntransition probability matrix are used to determine the number of clusters.\nThis method succeeds at determining the number of clusters in many datasets\nwhere previous methods fail. On every considered dataset, our consensus method\nprovides a final result with accuracy well above the average of the individual\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 13:54:01 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Race", "Shaina", ""], ["Meyer", "Carl", ""]]}, {"id": "1408.1000", "submitter": "Himanshu Tyagi", "authors": "Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, and Himanshu\n  Tyagi", "title": "Estimating Renyi Entropy of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown that estimating the Shannon entropy $H({\\rm p})$ of a\ndiscrete $k$-symbol distribution ${\\rm p}$ requires $\\Theta(k/\\log k)$ samples,\na number that grows near-linearly in the support size. In many applications\n$H({\\rm p})$ can be replaced by the more general R\\'enyi entropy of order\n$\\alpha$, $H_\\alpha({\\rm p})$. We determine the number of samples needed to\nestimate $H_\\alpha({\\rm p})$ for all $\\alpha$, showing that $\\alpha < 1$\nrequires a super-linear, roughly $k^{1/\\alpha}$ samples, noninteger $\\alpha>1$\nrequires a near-linear $k$ samples, but, perhaps surprisingly, integer\n$\\alpha>1$ requires only $\\Theta(k^{1-1/\\alpha})$ samples. Furthermore,\ndeveloping on a recently established connection between polynomial\napproximation and estimation of additive functions of the form $\\sum_{x} f({\\rm\np}_x)$, we reduce the sample complexity for noninteger values of $\\alpha$ by a\nfactor of $\\log k$ compared to the empirical estimator. The estimators\nachieving these bounds are simple and run in time linear in the number of\nsamples. Our lower bounds provide explicit constructions of distributions with\ndifferent R\\'enyi entropies that are hard to distinguish.\n", "versions": [{"version": "v1", "created": "Sat, 2 Aug 2014 18:52:52 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 10:50:49 GMT"}, {"version": "v3", "created": "Thu, 10 Mar 2016 08:35:51 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Acharya", "Jayadev", ""], ["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1408.1054", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki, Jacek Tabor", "title": "Multithreshold Entropy Linear Classifier", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2015.03.007", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear classifiers separate the data with a hyperplane. In this paper we\nfocus on the novel method of construction of multithreshold linear classifier,\nwhich separates the data with multiple parallel hyperplanes. Proposed model is\nbased on the information theory concepts -- namely Renyi's quadratic entropy\nand Cauchy-Schwarz divergence.\n  We begin with some general properties, including data scale invariance. Then\nwe prove that our method is a multithreshold large margin classifier, which\nshows the analogy to the SVM, while in the same time works with much broader\nclass of hypotheses. What is also interesting, proposed method is aimed at the\nmaximization of the balanced quality measure (such as Matthew's Correlation\nCoefficient) as opposed to very common maximization of the accuracy. This\nfeature comes directly from the optimization problem statement and is further\nconfirmed by the experiments on the UCI datasets.\n  It appears, that our Multithreshold Entropy Linear Classifier (MELC) obtaines\nsimilar or higher scores than the ones given by SVM on both synthetic and real\ndata. We show how proposed approach can be benefitial for the cheminformatics\nin the task of ligands activity prediction, where despite better classification\nresults, MELC gives some additional insight into the data structure (classes of\nunderrepresented chemical compunds).\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 18:01:29 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""], ["Tabor", "Jacek", ""]]}, {"id": "1408.1160", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Mixed-Variate Restricted Boltzmann Machines", "comments": "Originally published in Proceedings of ACML'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern datasets are becoming heterogeneous. To this end, we present in this\npaper Mixed-Variate Restricted Boltzmann Machines for simultaneously modelling\nvariables of multiple types and modalities, including binary and continuous\nresponses, categorical options, multicategorical choices, ordinal assessment\nand category-ranked preferences. Dependency among variables is modeled using\nlatent binary variables, each of which can be interpreted as a particular\nhidden aspect of the data. The proposed model, similar to the standard RBMs,\nallows fast evaluation of the posterior for the latent variables. Hence, it is\nnaturally suitable for many common tasks including, but not limited to, (a) as\na pre-processing step to convert complex input data into a more convenient\nvectorial representation through the latent posteriors, thereby offering a\ndimensionality reduction capacity, (b) as a classifier supporting binary,\nmulticlass, multilabel, and label-ranking outputs, or a regression tool for\ncontinuous outputs and (c) as a data completion tool for multimodal and\nheterogeneous data. We evaluate the proposed model on a large-scale dataset\nusing the world opinion survey results on three tasks: feature extraction and\nvisualization, data completion and prediction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 01:43:05 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.1162", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh, Hung H. Bui", "title": "MCMC for Hierarchical Semi-Markov Conditional Random Fields", "comments": "NIPS'09 Workshop on Deep Learning for Speech Recognition and Related\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep architecture such as hierarchical semi-Markov models is an important\nclass of models for nested sequential data. Current exact inference schemes\neither cost cubic time in sequence length, or exponential time in model depth.\nThese costs are prohibitive for large-scale problems with arbitrary length and\ndepth. In this contribution, we propose a new approximation technique that may\nhave the potential to achieve sub-cubic time complexity in length and linear\ntime depth, at the cost of some loss of quality. The idea is based on two\nwell-known methods: Gibbs sampling and Rao-Blackwellisation. We provide some\nsimulation-based evaluation of the quality of the RGBS with respect to run time\nand sequence length.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 02:04:43 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""], ["Bui", "Hung H.", ""]]}, {"id": "1408.1167", "submitter": "Truyen Tran", "authors": "Truyen Tran, Hung Bui, Svetha Venkatesh", "title": "Boosted Markov Networks for Activity Recognition", "comments": "International Conference on Intelligent Sensors, Sensor Networks and\n  Information Processing (ISSNIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a framework called boosted Markov networks to combine the learning\ncapacity of boosting and the rich modeling semantics of Markov networks and\napplying the framework for video-based activity recognition. Importantly, we\nextend the framework to incorporate hidden variables. We show how the framework\ncan be applied for both model learning and feature selection. We demonstrate\nthat boosted Markov networks with hidden variables perform comparably with the\nstandard maximum likelihood estimation. However, our framework is able to learn\nsparse models, and therefore can provide computational savings when the learned\nmodels are used for classification.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 02:45:51 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Tran", "Truyen", ""], ["Bui", "Hung", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.1292", "submitter": "Ilja Kuzborskij", "authors": "Ilja Kuzborskij, Francesco Orabona, Barbara Caputo", "title": "Scalable Greedy Algorithms for Transfer Learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2016.09.003", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the binary transfer learning problem, focusing on\nhow to select and combine sources from a large pool to yield a good performance\non a target task. Constraining our scenario to real world, we do not assume the\ndirect access to the source data, but rather we employ the source hypotheses\ntrained from them. We propose an efficient algorithm that selects relevant\nsource hypotheses and feature dimensions simultaneously, building on the\nliterature on the best subset selection problem. Our algorithm achieves\nstate-of-the-art results on three computer vision datasets, substantially\noutperforming both transfer learning and popular feature selection baselines in\na small-sample setting. We also present a randomized variant that achieves the\nsame results with the computational cost independent from the number of source\nhypotheses and feature dimensions. Also, we theoretically prove that, under\nreasonable assumptions on the source hypotheses, our algorithm can learn\neffectively from few examples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 14:27:57 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 15:56:53 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2015 10:27:39 GMT"}, {"version": "v4", "created": "Sat, 18 Jun 2016 00:17:50 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Kuzborskij", "Ilja", ""], ["Orabona", "Francesco", ""], ["Caputo", "Barbara", ""]]}, {"id": "1408.1319", "submitter": "Lewis Evans Mr", "authors": "Lewis Evans and Niall M. Adams and Christoforos Anagnostopoulos", "title": "When does Active Learning Work?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Learning (AL) methods seek to improve classifier performance when\nlabels are expensive or scarce. We consider two central questions: Where does\nAL work? How much does it help? To address these questions, a comprehensive\nexperimental simulation study of Active Learning is presented. We consider a\nvariety of tasks, classifiers and other AL factors, to present a broad\nexploration of AL performance in various settings. A precise way to quantify\nperformance is needed in order to know when AL works. Thus we also present a\ndetailed methodology for tackling the complexities of assessing AL performance\nin the context of this experimental study.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 15:27:20 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Evans", "Lewis", ""], ["Adams", "Niall M.", ""], ["Anagnostopoulos", "Christoforos", ""]]}, {"id": "1408.1387", "submitter": "Nihar Shah", "authors": "Nihar B. Shah and Dengyong Zhou", "title": "Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has gained immense popularity in machine learning applications\nfor obtaining large amounts of labeled data. Crowdsourcing is cheap and fast,\nbut suffers from the problem of low-quality data. To address this fundamental\nchallenge in crowdsourcing, we propose a simple payment mechanism to\nincentivize workers to answer only the questions that they are sure of and skip\nthe rest. We show that surprisingly, under a mild and natural \"no-free-lunch\"\nrequirement, this mechanism is the one and only incentive-compatible payment\nmechanism possible. We also show that among all possible incentive-compatible\nmechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the\nsmallest possible payment to spammers. We further extend our results to a more\ngeneral setting in which workers are required to provide a quantized confidence\nfor each question. Interestingly, this unique mechanism takes a\n\"multiplicative\" form. The simplicity of the mechanism is an added benefit. In\npreliminary experiments involving over 900 worker-task pairs, we observe a\nsignificant drop in the error rates under this unique mechanism for the same or\nlower monetary expenditure.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 19:52:28 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 06:29:22 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2015 19:53:47 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Shah", "Nihar B.", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1408.1655", "submitter": "Jonathan Ullman", "authors": "Moritz Hardt and Jonathan Ullman", "title": "Preventing False Discovery in Interactive Data Analysis is Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that, under a standard hardness assumption, there is no\ncomputationally efficient algorithm that given $n$ samples from an unknown\ndistribution can give valid answers to $n^{3+o(1)}$ adaptively chosen\nstatistical queries. A statistical query asks for the expectation of a\npredicate over the underlying distribution, and an answer to a statistical\nquery is valid if it is \"close\" to the correct expectation over the\ndistribution.\n  Our result stands in stark contrast to the well known fact that exponentially\nmany statistical queries can be answered validly and efficiently if the queries\nare chosen non-adaptively (no query may depend on the answers to previous\nqueries). Moreover, a recent work by Dwork et al. shows how to accurately\nanswer exponentially many adaptively chosen statistical queries via a\ncomputationally inefficient algorithm; and how to answer a quadratic number of\nadaptive queries via a computationally efficient algorithm. The latter result\nimplies that our result is tight up to a linear factor in $n.$\n  Conceptually, our result demonstrates that achieving statistical validity\nalone can be a source of computational intractability in adaptive settings. For\nexample, in the modern large collaborative research environment, data analysts\ntypically choose a particular approach based on previous findings. False\ndiscovery occurs if a research finding is supported by the data but not by the\nunderlying distribution. While the study of preventing false discovery in\nStatistics is decades old, to the best of our knowledge our result is the first\nto demonstrate a computational barrier. In particular, our result suggests that\nthe perceived difficulty of preventing false discovery in today's collaborative\nresearch environment may be inherent.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 17:39:56 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["Hardt", "Moritz", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1408.1664", "submitter": "Yetian Chen", "authors": "Yetian Chen, Jin Tian, Olga Nikolova and Srinivas Aluru", "title": "A Parallel Algorithm for Exact Bayesian Structure Discovery in Bayesian\n  Networks", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact Bayesian structure discovery in Bayesian networks requires exponential\ntime and space. Using dynamic programming (DP), the fastest known sequential\nalgorithm computes the exact posterior probabilities of structural features in\n$O(2(d+1)n2^n)$ time and space, if the number of nodes (variables) in the\nBayesian network is $n$ and the in-degree (the number of parents) per node is\nbounded by a constant $d$. Here we present a parallel algorithm capable of\ncomputing the exact posterior probabilities for all $n(n-1)$ edges with optimal\nparallel space efficiency and nearly optimal parallel time efficiency. That is,\nif $p=2^k$ processors are used, the run-time reduces to\n$O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per\nprocessor. Our algorithm is based the observation that the subproblems in the\nsequential DP algorithm constitute a $n$-$D$ hypercube. We take a delicate way\nto coordinate the computation of correlated DP procedures such that large\namount of data exchange is suppressed. Further, we develop parallel techniques\nfor two variants of the well-known \\emph{zeta transform}, which have\napplications outside the context of Bayesian networks. We demonstrate the\ncapability of our algorithm on datasets with up to 33 variables and its\nscalability on up to 2048 processors. We apply our algorithm to a biological\ndata set for discovering the yeast pheromone response pathways.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 17:40:36 GMT"}, {"version": "v2", "created": "Thu, 14 Aug 2014 04:12:09 GMT"}, {"version": "v3", "created": "Sat, 13 Aug 2016 04:25:55 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Chen", "Yetian", ""], ["Tian", "Jin", ""], ["Nikolova", "Olga", ""], ["Aluru", "Srinivas", ""]]}, {"id": "1408.1717", "submitter": "Vassilis Kalofolias", "authors": "Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre\n  Vandergheynst", "title": "Matrix Completion on Graphs", "comments": "Version of NIPS 2014 workshop \"Out of the Box: Robustness in High\n  Dimension\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding the missing values of a matrix given a few of its\nentries, called matrix completion, has gathered a lot of attention in the\nrecent years. Although the problem under the standard low rank assumption is\nNP-hard, Cand\\`es and Recht showed that it can be exactly relaxed if the number\nof observed entries is sufficiently large. In this work, we introduce a novel\nmatrix completion model that makes use of proximity information about rows and\ncolumns by assuming they form communities. This assumption makes sense in\nseveral real-world problems like in recommender systems, where there are\ncommunities of people sharing preferences, while products form clusters that\nreceive similar ratings. Our main goal is thus to find a low-rank solution that\nis structured by the proximities of rows and columns encoded by graphs. We\nborrow ideas from manifold learning to constrain our solution to be smooth on\nthese graphs, in order to implicitly force row and column proximities. Our\nmatrix recovery model is formulated as a convex non-smooth optimization\nproblem, for which a well-posed iterative scheme is provided. We study and\nevaluate the proposed matrix completion on synthetic and real data, showing\nthat the proposed structured low-rank recovery model outperforms the standard\nmatrix completion model in many situations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 21:33:51 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 17:00:14 GMT"}, {"version": "v3", "created": "Thu, 27 Nov 2014 11:12:27 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Kalofolias", "Vassilis", ""], ["Bresson", "Xavier", ""], ["Bronstein", "Michael", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1408.1784", "submitter": "Haiping Huang", "authors": "Haiping Huang and Yoshiyuki Kabashima", "title": "Origin of the computational hardness for learning with binary synapses", "comments": "9 pages, 1 figure", "journal-ref": "Phys. Rev. E 90, 052813 (2014)", "doi": "10.1103/PhysRevE.90.052813", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning in a binary perceptron is able to classify an extensive\nnumber of random patterns by a proper assignment of binary synaptic weights.\nHowever, to find such assignments in practice, is quite a nontrivial task. The\nrelation between the weight space structure and the algorithmic hardness has\nnot yet been fully understood. To this end, we analytically derive the\nFranz-Parisi potential for the binary preceptron problem, by starting from an\nequilibrium solution of weights and exploring the weight space structure around\nit. Our result reveals the geometrical organization of the weight\nspace\\textemdash the weight space is composed of isolated solutions, rather\nthan clusters of exponentially many close-by solutions. The point-like clusters\nfar apart from each other in the weight space explain the previously observed\nglassy behavior of stochastic local search heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 08:13:52 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Huang", "Haiping", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1408.1913", "submitter": "Patrick M. Pilarski", "authors": "Adam S. R. Parker, Ann L. Edwards, and Patrick M. Pilarski", "title": "Using Learned Predictions as Feedback to Improve Control and\n  Communication with an Artificial Limb: Preliminary Findings", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people suffer from the loss of a limb. Learning to get by without an arm\nor hand can be very challenging, and existing prostheses do not yet fulfil the\nneeds of individuals with amputations. One promising solution is to provide\ngreater communication between a prosthesis and its user. Towards this end, we\npresent a simple machine learning interface to supplement the control of a\nrobotic limb with feedback to the user about what the limb will be experiencing\nin the near future. A real-time prediction learner was implemented to predict\nimpact-related electrical load experienced by a robot limb; the learning\nsystem's predictions were then communicated to the device's user to aid in\ntheir interactions with a workspace. We tested this system with five\nable-bodied subjects. Each subject manipulated the robot arm while receiving\ndifferent forms of vibrotactile feedback regarding the arm's contact with its\nworkspace. Our trials showed that communicable predictions could be learned\nquickly during human control of the robot arm. Using these predictions as a\nbasis for feedback led to a statistically significant improvement in task\nperformance when compared to purely reactive feedback from the device. Our\nstudy therefore contributes initial evidence that prediction learning and\nmachine intelligence can benefit not just control, but also feedback from an\nartificial limb. We expect that a greater level of acceptance and ownership can\nbe achieved if the prosthesis itself takes an active role in transmitting\nlearned knowledge about its state and its situation of use.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 16:57:22 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Parker", "Adam S. R.", ""], ["Edwards", "Ann L.", ""], ["Pilarski", "Patrick M.", ""]]}, {"id": "1408.2003", "submitter": "Bo Han", "authors": "Bo Han, Bo He, Rui Nian, Mengmeng Ma, Shujing Zhang, Minghui Li and\n  Amaury Lendasse", "title": "LARSEN-ELM: Selective Ensemble of Extreme Learning Machines using LARS\n  for Blended Data", "comments": "Accepted for publication in Neurocomputing, 01/19/2014", "journal-ref": "Neurocomputing, 2014, Elsevier. Manuscript ID: NEUCOM-D-13-01029", "doi": "10.1016/j.neucom.2014.01.069", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme learning machine (ELM) as a neural network algorithm has shown its\ngood performance, such as fast speed, simple structure etc, but also, weak\nrobustness is an unavoidable defect in original ELM for blended data. We\npresent a new machine learning framework called LARSEN-ELM for overcoming this\nproblem. In our paper, we would like to show two key steps in LARSEN-ELM. In\nthe first step, preprocessing, we select the input variables highly related to\nthe output using least angle regression (LARS). In the second step, training,\nwe employ Genetic Algorithm (GA) based selective ensemble and original ELM. In\nthe experiments, we apply a sum of two sines and four datasets from UCI\nrepository to verify the robustness of our approach. The experimental results\nshow that compared with original ELM and other methods such as OP-ELM,\nGASEN-ELM and LSBoost, LARSEN-ELM significantly improve robustness performance\nwhile keeping a relatively high speed.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 01:31:02 GMT"}, {"version": "v2", "created": "Wed, 27 Aug 2014 02:54:54 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Han", "Bo", ""], ["He", "Bo", ""], ["Nian", "Rui", ""], ["Ma", "Mengmeng", ""], ["Zhang", "Shujing", ""], ["Li", "Minghui", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1408.2004", "submitter": "Bo Han", "authors": "Bo Han, Bo He, Mengmeng Ma, Tingting Sun, Tianhong Yan, Amaury\n  Lendasse", "title": "RMSE-ELM: Recursive Model based Selective Ensemble of Extreme Learning\n  Machines for Robustness Improvement", "comments": "Accepted for publication in Mathematical Problems in Engineering,\n  09/22/2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme learning machine (ELM) as an emerging branch of shallow networks has\nshown its excellent generalization and fast learning speed. However, for\nblended data, the robustness of ELM is weak because its weights and biases of\nhidden nodes are set randomly. Moreover, the noisy data exert a negative\neffect. To solve this problem, a new framework called RMSE-ELM is proposed in\nthis paper. It is a two-layer recursive model. In the first layer, the\nframework trains lots of ELMs in different groups concurrently, then employs\nselective ensemble to pick out an optimal set of ELMs in each group, which can\nbe merged into a large group of ELMs called candidate pool. In the second\nlayer, selective ensemble is recursively used on candidate pool to acquire the\nfinal ensemble. In the experiments, we apply UCI blended datasets to confirm\nthe robustness of our new approach in two key aspects (mean square error and\nstandard deviation). The space complexity of our method is increased to some\ndegree, but the results have shown that RMSE-ELM significantly improves\nrobustness with slightly computational time compared with representative\nmethods (ELM, OP-ELM, GASEN-ELM, GASEN-BP and E-GASEN). It becomes a potential\nframework to solve robustness issue of ELM for high-dimensional blended data in\nthe future.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 01:36:03 GMT"}, {"version": "v2", "created": "Wed, 27 Aug 2014 02:35:11 GMT"}, {"version": "v3", "created": "Tue, 23 Sep 2014 07:48:35 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Han", "Bo", ""], ["He", "Bo", ""], ["Ma", "Mengmeng", ""], ["Sun", "Tingting", ""], ["Yan", "Tianhong", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1408.2025", "submitter": "Cosma Shalizi", "authors": "Cosma Shalizi, Kristina Lisa Klinkner", "title": "Blind Construction of Optimal Nonlinear Recursive Predictors for\n  Discrete Sequences", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-504-511", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for nonlinear prediction of discrete random sequences\nunder minimal structural assumptions. We give a mathematical construction for\noptimal predictors of such processes, in the form of hidden Markov models. We\nthen describe an algorithm, CSSR (Causal-State Splitting Reconstruction), which\napproximates the ideal predictor from data. We discuss the reliability of CSSR,\nits data requirements, and its performance in simulations. Finally, we compare\nour approach to existing methods using variablelength Markov models and\ncross-validated hidden Markov models, and show theoretically and experimentally\nthat our method delivers results superior to the former and at least comparable\nto the latter.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:18:20 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Shalizi", "Cosma", ""], ["Klinkner", "Kristina Lisa", ""]]}, {"id": "1408.2031", "submitter": "Alina Beygelzimer", "authors": "Alina Beygelzimer, John Langford, Yuri Lifshits, Gregory Sorkin,\n  Alexander L. Strehl", "title": "Conditional Probability Tree Estimation Analysis and Algorithms", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-51-58", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the conditional probability of a label\nin time O(log n), where n is the number of possible labels. We analyze a\nnatural reduction of this problem to a set of binary regression problems\norganized in a tree structure, proving a regret bound that scales with the\ndepth of the tree. Motivated by this analysis, we propose the first online\nalgorithm which provably constructs a logarithmic depth tree on the set of\nlabels to solve this problem. We test the algorithm empirically, showing that\nit works succesfully on a dataset with roughly 106 labels.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:25:07 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Langford", "John", ""], ["Lifshits", "Yuri", ""], ["Sorkin", "Gregory", ""], ["Strehl", "Alexander L.", ""]]}, {"id": "1408.2032", "submitter": "Hal Daume III", "authors": "Hal Daume III", "title": "Bayesian Multitask Learning with Latent Hierarchies", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-135-142", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn multiple hypotheses for related tasks under a latent hierarchical\nrelationship between tasks. We exploit the intuition that for domain\nadaptation, we wish to share classifier structure, but for multitask learning,\nwe wish to share covariance structure. Our hierarchical model is seen to\nsubsume several previously proposed multitask learning models and performs well\non three distinct real-world data sets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:26:02 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Daume", "Hal", "III"]]}, {"id": "1408.2033", "submitter": "Michael A. Finegold", "authors": "Michael A. Finegold, Mathias Drton", "title": "Robust Graphical Modeling with t-Distributions", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-169-176", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Gaussian models have proven to be useful tools for exploring\nnetwork structures based on multivariate data. Applications to studies of gene\nexpression have generated substantial interest in these models, and resulting\nrecent progress includes the development of fitting methodology involving\npenalization of the likelihood function. In this paper we advocate the use of\nthe multivariate t and related distributions for more robust inference of\ngraphs. In particular, we demonstrate that penalized likelihood inference\ncombined with an application of the EM algorithm provides a simple and\ncomputationally efficient approach to model selection in the t-distribution\ncase.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:26:59 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Finegold", "Michael A.", ""], ["Drton", "Mathias", ""]]}, {"id": "1408.2035", "submitter": "Kenichi Kurihara", "authors": "Kenichi Kurihara, Shu Tanaka, Seiji Miyashita", "title": "Quantum Annealing for Clustering", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-321-328", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies quantum annealing (QA) for clustering, which can be seen\nas an extension of simulated annealing (SA). We derive a QA algorithm for\nclustering and propose an annealing schedule, which is crucial in practice.\nExperiments show the proposed QA algorithm finds better clustering assignments\nthan SA. Furthermore, QA is as easy as SA to implement.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:31:06 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Kurihara", "Kenichi", ""], ["Tanaka", "Shu", ""], ["Miyashita", "Seiji", ""]]}, {"id": "1408.2036", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Characterizing predictable classes of processes", "comments": "This is a duplicate submission of 0905.4341, made by UAI foundation\n  who had the brilliant idea of flooding arxiv with UAI papers 5 years after\n  the conference, without checking whether these papers were already submitted\n  to arxiv or at least asking the authors. Great job, UAI! The journal\n  (extended) version appears in JMLR, 11: 581-602, 2010; also as\n  arXiv:0912.4883", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-471-478", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem is sequence prediction in the following setting. A sequence\nx1,..., xn,... of discrete-valued observations is generated according to some\nunknown probabilistic law (measure) mu. After observing each outcome, it is\nrequired to give the conditional probabilities of the next observation. The\nmeasure mu belongs to an arbitrary class C of stochastic processes. We are\ninterested in predictors ? whose conditional probabilities converge to the\n'true' mu-conditional probabilities if any mu { C is chosen to generate the\ndata. We show that if such a predictor exists, then a predictor can also be\nobtained as a convex combination of a countably many elements of C. In other\nwords, it can be obtained as a Bayesian predictor whose prior is concentrated\non a countable set. This result is established for two very different measures\nof performance of prediction, one of which is very strong, namely, total\nvariation, and the other is very weak, namely, prediction in expected average\nKullback-Leibler divergence.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:32:03 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 16:08:12 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1408.2037", "submitter": "Issei Sato", "authors": "Issei Sato, Kenichi Kurihara, Shu Tanaka, Hiroshi Nakagawa, Seiji\n  Miyashita", "title": "Quantum Annealing for Variational Bayes Inference", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-479-486", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents studies on a deterministic annealing algorithm based on\nquantum annealing for variational Bayes (QAVB) inference, which can be seen as\nan extension of the simulated annealing for variational Bayes (SAVB) inference.\nQAVB is as easy as SAVB to implement. Experiments revealed QAVB finds a better\nlocal optimum than SAVB in terms of the variational free energy in latent\nDirichlet allocation (LDA).\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:33:21 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Sato", "Issei", ""], ["Kurihara", "Kenichi", ""], ["Tanaka", "Shu", ""], ["Nakagawa", "Hiroshi", ""], ["Miyashita", "Seiji", ""]]}, {"id": "1408.2038", "submitter": "Shohei Shimizu", "authors": "Shohei Shimizu, Aapo Hyvarinen, Yoshinobu Kawahara", "title": "A direct method for estimating a causal ordering in a linear\n  non-Gaussian acyclic model", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-506-513", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models and Bayesian networks have been widely used to\nanalyze causal relations between continuous variables. In such frameworks,\nlinear acyclic models are typically used to model the datagenerating process of\nvariables. Recently, it was shown that use of non-Gaussianity identifies a\ncausal ordering of variables in a linear acyclic model without using any prior\nknowledge on the network structure, which is not the case with conventional\nmethods. However, existing estimation methods are based on iterative search\nalgorithms and may not converge to a correct solution in a finite number of\nsteps. In this paper, we propose a new direct method to estimate a causal\nordering based on non-Gaussianity. In contrast to the previous methods, our\nalgorithm requires no algorithmic parameters and is guaranteed to converge to\nthe right solution within a small fixed number of steps if the data strictly\nfollows the model.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:34:21 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Shimizu", "Shohei", ""], ["Hyvarinen", "Aapo", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1408.2039", "submitter": "Ryan Prescott Adams", "authors": "Ryan Prescott Adams, George E. Dahl, Iain Murray", "title": "Incorporating Side Information in Probabilistic Matrix Factorization\n  with Gaussian Processes", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-1-9", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic matrix factorization (PMF) is a powerful method for modeling\ndata associ- ated with pairwise relationships, Finding use in collaborative\nFiltering, computational bi- ology, and document analysis, among other areas.\nIn many domains, there are additional covariates that can assist in prediction.\nFor example, when modeling movie ratings, we might know when the rating\noccurred, where the user lives, or what actors appear in the movie. It is\ndifficult, however, to incorporate this side information into the PMF model. We\npropose a framework for incorporating side information by coupling together\nmulti- ple PMF problems via Gaussian process priors. We replace scalar latent\nfeatures with func- tions that vary over the covariate space. The GP priors on\nthese functions require them to vary smoothly and share information. We apply\nthis new method to predict the scores of professional basketball games, where\nside information about the venue and date of the game are relevant for the\noutcome.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:35:48 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["Dahl", "George E.", ""], ["Murray", "Iain", ""]]}, {"id": "1408.2040", "submitter": "Alexey Chernov", "authors": "Alexey Chernov, Vladimir Vovk", "title": "Prediction with Advice of Unknown Number of Experts", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-117-125", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of prediction with expert advice, we consider a recently\nintroduced kind of regret bounds: the bounds that depend on the effective\ninstead of nominal number of experts. In contrast to the Normal- Hedge bound,\nwhich mainly depends on the effective number of experts but also weakly depends\non the nominal one, we obtain a bound that does not contain the nominal number\nof experts at all. We use the defensive forecasting method and introduce an\napplication of defensive forecasting to multivalued supermartingales.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:36:41 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Chernov", "Alexey", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1408.2041", "submitter": "Yucheng Low", "authors": "Yucheng Low, Joseph E. Gonzalez, Aapo Kyrola, Danny Bickson, Carlos E.\n  Guestrin, Joseph Hellerstein", "title": "GraphLab: A New Framework For Parallel Machine Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-340-349", "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and implementing efficient, provably correct parallel machine\nlearning (ML) algorithms is challenging. Existing high-level parallel\nabstractions like MapReduce are insufficiently expressive while low-level tools\nlike MPI and Pthreads leave ML experts repeatedly solving the same design\nchallenges. By targeting common patterns in ML, we developed GraphLab, which\nimproves upon abstractions like MapReduce by compactly expressing asynchronous\niterative algorithms with sparse computational dependencies while ensuring data\nconsistency and achieving a high degree of parallel performance. We demonstrate\nthe expressiveness of the GraphLab framework by designing and implementing\nparallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and\nCompressed Sensing. We show that using GraphLab we can achieve excellent\nparallel performance on large scale real-world problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:38:37 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Low", "Yucheng", ""], ["Gonzalez", "Joseph E.", ""], ["Kyrola", "Aapo", ""], ["Bickson", "Danny", ""], ["Guestrin", "Carlos E.", ""], ["Hellerstein", "Joseph", ""]]}, {"id": "1408.2042", "submitter": "Ricardo Silva", "authors": "Ricardo Silva, Robert B. Gramacy", "title": "Gaussian Process Structural Equation Models with Latent Variables", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-537-545", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of disciplines such as social sciences, psychology, medicine and\neconomics, the recorded data are considered to be noisy measurements of latent\nvariables connected by some causal structure. This corresponds to a family of\ngraphical models known as the structural equation model with latent variables.\nWhile linear non-Gaussian variants have been well-studied, inference in\nnonparametric structural equation models is still underdeveloped. We introduce\na sparse Gaussian process parameterization that defines a non-linear structure\nconnecting latent variables, unlike common formulations of Gaussian process\nlatent variable models. The sparse parameterization is given a full Bayesian\ntreatment without compromising Markov chain Monte Carlo efficiency. We compare\nthe stability of the sampling procedure and the predictive ability of the model\nagainst the current practice.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:39:50 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Silva", "Ricardo", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "1408.2044", "submitter": "Ameet Talwalkar", "authors": "Ameet Talwalkar, Afshin Rostamizadeh", "title": "Matrix Coherence and the Nystrom Method", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-572-579", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nystrom method is an efficient technique used to speed up large-scale\nlearning applications by generating low-rank approximations. Crucial to the\nperformance of this technique is the assumption that a matrix can be well\napproximated by working exclusively with a subset of its columns. In this work\nwe relate this assumption to the concept of matrix coherence, connecting\ncoherence to the performance of the Nystrom method. Making use of related work\nin the compressed sensing and the matrix completion literature, we derive novel\ncoherence-based bounds for the Nystrom method in the low-rank setting. We then\npresent empirical results that corroborate these theoretical bounds. Finally,\nwe present more general empirical results for the full-rank setting that\nconvincingly demonstrate the ability of matrix coherence to measure the degree\nto which information can be extracted from a subset of columns.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:40:28 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Talwalkar", "Ameet", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "1408.2045", "submitter": "Konstantin Voevodski", "authors": "Konstantin Voevodski, Maria-Florina Balcan, Heiko Roglin, Shang-Hua\n  Teng, Yu Xia", "title": "Efficient Clustering with Limited Distance Information", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-632-640", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a point set S and an unknown metric d on S, we study the problem of\nefficiently partitioning S into k clusters while querying few distances between\nthe points. In our model we assume that we have access to one versus all\nqueries that given a point s 2 S return the distances between s and all other\npoints. We show that given a natural assumption about the structure of the\ninstance, we can efficiently find an accurate clustering using only O(k)\ndistance queries. We use our algorithm to cluster proteins by sequence\nsimilarity. This setting nicely fits our model because we can use a fast\nsequence database search program to query a sequence against an entire dataset.\nWe conduct an empirical study that shows that even though we query a small\nfraction of the distances between the points, we produce clusterings that are\nclose to a desired clustering given by manual classification.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:41:26 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Voevodski", "Konstantin", ""], ["Balcan", "Maria-Florina", ""], ["Roglin", "Heiko", ""], ["Teng", "Shang-Hua", ""], ["Xia", "Yu", ""]]}, {"id": "1408.2047", "submitter": "Yutian Chen", "authors": "Yutian Chen, Max Welling", "title": "Bayesian Structure Learning for Markov Random Fields with a Spike and\n  Slab Prior", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-174-184", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years a number of methods have been developed for automatically\nlearning the (sparse) connectivity structure of Markov Random Fields. These\nmethods are mostly based on L1-regularized optimization which has a number of\ndisadvantages such as the inability to assess model uncertainty and expensive\ncrossvalidation to find the optimal regularization parameter. Moreover, the\nmodel's predictive performance may degrade dramatically with a suboptimal value\nof the regularization parameter (which is sometimes desirable to induce\nsparseness). We propose a fully Bayesian approach based on a \"spike and slab\"\nprior (similar to L0 regularization) that does not suffer from these\nshortcomings. We develop an approximate MCMC method combining Langevin dynamics\nand reversible jump MCMC to conduct inference in this model. Experiments show\nthat the proposed model learns a good combination of the structure and\nparameter values without the need for separate hyper-parameter tuning.\nMoreover, the model's predictive performance is much more robust than L1-based\nmethods with hyper-parameter settings that induce highly sparse model\nstructures.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:45:11 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Chen", "Yutian", ""], ["Welling", "Max", ""]]}, {"id": "1408.2049", "submitter": "David Duvenaud", "authors": "Ferenc Huszar, David Duvenaud", "title": "Optimally-Weighted Herding is Bayesian Quadrature", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012). This copy was withdrawn since it's a\n  duplicate of arXiv:1204.1664", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-377-386", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herding and kernel herding are deterministic methods of choosing samples\nwhich summarise a probability distribution. A related task is choosing samples\nfor estimating integrals using Bayesian quadrature. We show that the criterion\nminimised when selecting samples in kernel herding is equivalent to the\nposterior variance in Bayesian quadrature. We then show that sequential\nBayesian quadrature can be viewed as a weighted version of kernel herding which\nachieves performance superior to any other weighted herding method. We\ndemonstrate empirically a rate of convergence faster than O(1/N). Our results\nalso imply an upper bound on the empirical error of the Bayesian quadrature\nestimate.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:47:25 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 21:01:52 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Huszar", "Ferenc", ""], ["Duvenaud", "David", ""]]}, {"id": "1408.2051", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer, Jeff A. Bilmes", "title": "Algorithms for Approximate Minimization of the Difference Between\n  Submodular Functions, with Applications", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-407-417", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the work of Narasimhan and Bilmes [30] for minimizing set functions\nrepresentable as a dierence between submodular functions. Similar to [30], our\nnew algorithms are guaranteed to monotonically reduce the objective function at\nevery step. We empirically and theoretically show that the per-iteration cost\nof our algorithms is much less than [30], and our algorithms can be used to\nefficiently minimize a dierence between submodular functions under various\ncombinatorial constraints, a problem not previously addressed. We provide\ncomputational bounds and a hardness result on the multiplicative\ninapproximability of minimizing the dierence between submodular functions. We\nshow, however, that it is possible to give worst-case additive bounds by\nproviding a polynomial time computable lower-bound on the minima. Finally we\nshow how a number of machine learning problems can be modeled as minimizing the\ndierence between submodular functions. We experimentally show the validity of\nour algorithms by testing them on the problem of feature selection with\nsubmodular cost features.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:48:31 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1408.2054", "submitter": "David Wipf", "authors": "David Wipf", "title": "Non-Convex Rank Minimization via an Empirical Bayesian Approach", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-914-923", "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications that require matrix solutions of minimal rank, the\nunderlying cost function is non-convex leading to an intractable, NP-hard\noptimization problem. Consequently, the convex nuclear norm is frequently used\nas a surrogate penalty term for matrix rank. The problem is that in many\npractical scenarios there is no longer any guarantee that we can correctly\nestimate generative low-rank matrices of interest, theoretical special cases\nnotwithstanding. Consequently, this paper proposes an alternative empirical\nBayesian procedure build upon a variational approximation that, unlike the\nnuclear norm, retains the same globally minimizing point estimate as the rank\nfunction under many useful constraints. However, locally minimizing solutions\nare largely smoothed away via marginalization, allowing the algorithm to\nsucceed when standard convex relaxations completely fail. While the proposed\nmethodology is generally applicable to a wide range of low-rank applications,\nwe focus our attention on the robust principal component analysis problem\n(RPCA), which involves estimating an unknown low-rank matrix with unknown\nsparse corruptions. Theoretical and empirical evidence are presented to show\nthat our method is potentially superior to related MAP-based approaches, for\nwhich the convex principle component pursuit (PCP) algorithm (Candes et al.,\n2011) can be viewed as a special case.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:52:02 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Wipf", "David", ""]]}, {"id": "1408.2055", "submitter": "Amy Zhang", "authors": "Amy Zhang, Nadia Fawaz, Stratis Ioannidis, Andrea Montanari", "title": "Guess Who Rated This Movie: Identifying Users Through Subspace\n  Clustering", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-944-953", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often the case that, within an online recommender system, multiple\nusers share a common account. Can such shared accounts be identified solely on\nthe basis of the userprovided ratings? Once a shared account is identified, can\nthe different users sharing it be identified as well? Whenever such user\nidentification is feasible, it opens the way to possible improvements in\npersonalized recommendations, but also raises privacy concerns. We develop a\nmodel for composite accounts based on unions of linear subspaces, and use\nsubspace clustering for carrying out the identification task. We show that a\nsignificant fraction of such accounts is identifiable in a reliable manner, and\nillustrate potential uses for personalized recommendation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:54:49 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Zhang", "Amy", ""], ["Fawaz", "Nadia", ""], ["Ioannidis", "Stratis", ""], ["Montanari", "Andrea", ""]]}, {"id": "1408.2060", "submitter": "Jie Chen", "authors": "Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan\n  Tan, Patrick Jaillet", "title": "Parallel Gaussian Process Regression with Low-Rank Covariance Matrix\n  Approximations", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-152-161", "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are Bayesian non-parametric models that are widely\nused for probabilistic regression. Unfortunately, it cannot scale well with\nlarge data nor perform real-time predictions due to its cubic time cost in the\ndata size. This paper presents two parallel GP regression methods that exploit\nlow-rank covariance matrix approximations for distributing the computational\nload among parallel machines to achieve time efficiency and scalability. We\ntheoretically guarantee the predictive performances of our proposed parallel\nGPs to be equivalent to that of some centralized approximate GP regression\nmethods: The computation of their centralized counterparts can be distributed\namong parallel machines, hence achieving greater time efficiency and\nscalability. We analytically compare the properties of our parallel GPs such as\ntime, space, and communication complexity. Empirical evaluation on two\nreal-world datasets in a cluster of 20 computing nodes shows that our parallel\nGPs are significantly more time-efficient and scalable than their centralized\ncounterparts and exact/full GP while achieving predictive performances\ncomparable to full GP.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:58:33 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Chen", "Jie", ""], ["Cao", "Nannan", ""], ["Low", "Kian Hsiang", ""], ["Ouyang", "Ruofei", ""], ["Tan", "Colin Keng-Yan", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1408.2061", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata, David Duvenaud, Zoubin Ghahramani", "title": "Warped Mixtures for Nonparametric Cluster Shapes", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-311-320", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of Gaussians fit to a single curved or heavy-tailed cluster will\nreport that the data contains many clusters. To produce more appropriate\nclusterings, we introduce a model which warps a latent mixture of Gaussians to\nproduce nonparametric cluster shapes. The possibly low-dimensional latent\nmixture model allows us to summarize the properties of the high-dimensional\nclusters (or density manifolds) describing the data. The number of manifolds,\nas well as the shape and dimension of each manifold is automatically inferred.\nWe derive a simple inference scheme for this model which analytically\nintegrates out both the mixture parameters and the warping function. We show\nthat our model is effective for density estimation, performs better than\ninfinite Gaussian mixture models at recovering the true number of clusters, and\nproduces interpretable summaries of high-dimensional datasets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:00:05 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Duvenaud", "David", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1408.2062", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer, Jeff A. Bilmes", "title": "The Lovasz-Bregman Divergence and connections to rank aggregation,\n  clustering, and web ranking", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-321-330", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the recently introduced theory of Lovasz-Bregman (LB) divergences\n(Iyer & Bilmes 2012) in several ways. We show that they represent a distortion\nbetween a \"score\" and an \"ordering\", thus providing a new view of rank\naggregation and order based clustering with interesting connections to web\nranking. We show how the LB divergences have a number of properties akin to\nmany permutation based metrics, and in fact have as special cases forms very\nsimilar to the Kendall-tau metric. We also show how the LB divergences subsume\na number of commonly used ranking measures in information retrieval, like NDCG\nand AUC. Unlike the traditional permutation based metrics, however, the LB\ndivergence naturally captures a notion of \"confidence\" in the orderings, thus\nproviding a new representation to applications involving aggregating scores as\nopposed to just orderings. We show how a number of recently used web ranking\nmodels are forms of Lovasz-Bregman rank aggregation and also observe that a\nnatural form of Mallow's model using the LB divergence has been used as\nconditional ranking models for the \"Learning to Rank\" problem.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:01:37 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1408.2064", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Bernhard Schoelkopf", "title": "One-Class Support Measure Machines for Group Anomaly Detection", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-449-458", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose one-class support measure machines (OCSMMs) for group anomaly\ndetection which aims at recognizing anomalous aggregate behaviors of data\npoints. The OCSMMs generalize well-known one-class support vector machines\n(OCSVMs) to a space of probability measures. By formulating the problem as\nquantile estimation on distributions, we can establish an interesting\nconnection to the OCSVMs and variable kernel density estimators (VKDEs) over\nthe input space on which the distributions are defined, bridging the gap\nbetween large-margin methods and kernel density estimators. In particular, we\nshow that various types of VKDEs can be considered as solutions to a class of\nregularization problems studied in this paper. Experiments on Sloan Digital Sky\nSurvey dataset and High Energy Particle Physics dataset demonstrate the\nbenefits of the proposed framework in real-world applications.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:04:33 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Muandet", "Krikamol", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1408.2065", "submitter": "Stephane Ross", "authors": "Stephane Ross, Paul Mineiro, John Langford", "title": "Normalized Online Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-537-545", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce online learning algorithms which are independent of feature\nscales, proving regret bounds dependent on the ratio of scales existent in the\ndata rather than the absolute scale. This has several useful effects: there is\nno need to pre-normalize data, the test-time and test-space complexity are\nreduced, and the algorithms are more robust.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:05:51 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Ross", "Stephane", ""], ["Mineiro", "Paul", ""], ["Langford", "John", ""]]}, {"id": "1408.2066", "submitter": "Vikas Sindhwani", "authors": "Vikas Sindhwani, Ha Quang Minh, Aurelie Lozano", "title": "Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear\n  Multivariate Regression and Granger Causality", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-586-595", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general matrix-valued multiple kernel learning framework for\nhigh-dimensional nonlinear multivariate regression problems. This framework\nallows a broad class of mixed norm regularizers, including those that induce\nsparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel\nHilbert Spaces. We develop a highly scalable and eigendecomposition-free\nalgorithm that orchestrates two inexact solvers for simultaneously learning\nboth the input and output components of separable matrix-valued kernels. As a\nkey application enabled by our framework, we show how high-dimensional causal\ninference tasks can be naturally cast as sparse function estimation problems,\nleading to novel nonlinear extensions of a class of Graphical Granger Causality\ntechniques. Our algorithmic developments and extensive empirical studies are\ncomplemented by theoretical analyses in terms of Rademacher generalization\nbounds.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:06:49 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Sindhwani", "Vikas", ""], ["Minh", "Ha Quang", ""], ["Lozano", "Aurelie", ""]]}, {"id": "1408.2067", "submitter": "Aristide Tossou", "authors": "Aristide Tossou, Christos Dimitrakakis", "title": "Probabilistic inverse reinforcement learning in unknown environments", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-635-643", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning by demonstration from agents acting in\nunknown stochastic Markov environments or games. Our aim is to estimate agent\npreferences in order to construct improved policies for the same task that the\nagents are trying to solve. To do so, we extend previous probabilistic\napproaches for inverse reinforcement learning in known MDPs to the case of\nunknown dynamics or opponents. We do this by deriving two simplified\nprobabilistic models of the demonstrator's policy and utility. For\ntractability, we use maximum a posteriori estimation rather than full Bayesian\ninference. Under a flat prior, this results in a convex optimisation problem.\nWe find that the resulting algorithms are highly competitive against a variety\nof other methods for inverse reinforcement learning that do have knowledge of\nthe dynamics.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:07:52 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Tossou", "Aristide", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1408.2156", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan, Martin J. Wainwright, Bin Yu", "title": "Statistical guarantees for the EM algorithm: From population to\n  sample-based analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework for proving rigorous guarantees on the\nperformance of the EM algorithm and a variant known as gradient EM. Our\nanalysis is divided into two parts: a treatment of these algorithms at the\npopulation level (in the limit of infinite data), followed by results that\napply to updates based on a finite set of samples. First, we characterize the\ndomain of attraction of any global maximizer of the population likelihood. This\ncharacterization is based on a novel view of the EM updates as a perturbed form\nof likelihood ascent, or in parallel, of the gradient EM updates as a perturbed\nform of standard gradient ascent. Leveraging this characterization, we then\nprovide non-asymptotic guarantees on the EM and gradient EM algorithms when\napplied to a finite set of samples. We develop consequences of our general\ntheory for three canonical examples of incomplete-data problems: mixture of\nGaussians, mixture of regressions, and linear regression with covariates\nmissing completely at random. In each case, our theory guarantees that with a\nsuitable initialization, a relatively small number of EM (or gradient EM) steps\nwill yield (with high probability) an estimate that is within statistical error\nof the MLE. We provide simulations to confirm this theoretically predicted\nbehavior.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 21:40:15 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1408.2195", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "R-UCB: a Contextual Bandit Algorithm for Risk-Aware Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Context-Aware Recommender Systems can be naturally modelled as an\nexploration/exploitation trade-off (exr/exp) problem, where the system has to\nchoose between maximizing its expected rewards dealing with its current\nknowledge (exploitation) and learning more about the unknown user's preferences\nto improve its knowledge (exploration). This problem has been addressed by the\nreinforcement learning community but they do not consider the risk level of the\ncurrent user's situation, where it may be dangerous to recommend items the user\nmay not desire in her current situation if the risk level is high. We introduce\nin this paper an algorithm named R-UCB that considers the risk level of the\nuser's situation to adaptively balance between exr and exp. The detailed\nanalysis of the experimental results reveals several important discoveries in\nthe exr/exp behaviour.\n", "versions": [{"version": "v1", "created": "Sun, 10 Aug 2014 07:28:20 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1408.2196", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Exponentiated Gradient Exploration for Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning strategies respond to the costly labelling task in a\nsupervised classification by selecting the most useful unlabelled examples in\ntraining a predictive model. Many conventional active learning algorithms focus\non refining the decision boundary, rather than exploring new regions that can\nbe more informative. In this setting, we propose a sequential algorithm named\nEG-Active that can improve any Active learning algorithm by an optimal random\nexploration. Experimental results show a statistically significant and\nappreciable improvement in the performance of our new approach over the\nexisting active feedback methods.\n", "versions": [{"version": "v1", "created": "Sun, 10 Aug 2014 07:47:50 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1408.2327", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa, Francis Bach, Alexandre Gramfort", "title": "On the Consistency of Ordinal Regression Methods", "comments": "Journal of Machine Learning Research 18 (2017)", "journal-ref": "Journal of Machine Learning Research 18 (2017) 1-35", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many of the ordinal regression models that have been proposed in the\nliterature can be seen as methods that minimize a convex surrogate of the\nzero-one, absolute, or squared loss functions. A key property that allows to\nstudy the statistical implications of such approximations is that of Fisher\nconsistency. Fisher consistency is a desirable property for surrogate loss\nfunctions and implies that in the population setting, i.e., if the probability\ndistribution that generates the data were available, then optimization of the\nsurrogate would yield the best possible model. In this paper we will\ncharacterize the Fisher consistency of a rich family of surrogate loss\nfunctions used in the context of ordinal regression, including support vector\nordinal regression, ORBoosting and least absolute deviation. We will see that,\nfor a family of surrogate loss functions that subsumes support vector ordinal\nregression and ORBoosting, consistency can be fully characterized by the\nderivative of a real-valued function at zero, as happens for convex\nmargin-based surrogates in binary classification. We also derive excess risk\nbounds for a surrogate of the absolute error that generalize existing risk\nbounds for binary classification. Finally, our analysis suggests a novel\nsurrogate of the squared error loss. We compare this novel surrogate with\ncompeting approaches on 9 different datasets. Our method shows to be highly\ncompetitive in practice, outperforming the least squares loss on 7 out of 9\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 06:52:46 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 11:26:39 GMT"}, {"version": "v3", "created": "Sun, 7 Dec 2014 15:56:55 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2015 10:38:59 GMT"}, {"version": "v5", "created": "Tue, 15 Sep 2015 10:43:35 GMT"}, {"version": "v6", "created": "Wed, 30 Sep 2015 08:56:31 GMT"}, {"version": "v7", "created": "Fri, 2 Oct 2015 08:44:54 GMT"}, {"version": "v8", "created": "Mon, 19 Jun 2017 18:20:16 GMT"}, {"version": "v9", "created": "Fri, 21 Jul 2017 05:50:43 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Pedregosa", "Fabian", ""], ["Bach", "Francis", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1408.2368", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "On the Complexity of Bandit Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the attainable regret for online linear optimization problems with\nbandit feedback, where unlike the full-information setting, the player can only\nobserve its own loss rather than the full loss vector. We show that the price\nof bandit information in this setting can be as large as $d$, disproving the\nwell-known conjecture that the regret for bandit linear optimization is at most\n$\\sqrt{d}$ times the full-information regret. Surprisingly, this is shown using\n\"trivial\" modifications of standard domains, which have no effect in the\nfull-information setting. This and other results we present highlight some\ninteresting differences between full-information and bandit learning, which\nwere not considered in previous literature.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 10:40:29 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1408.2504", "submitter": "Ping Li", "authors": "Ping Li and Cun-Hui Zhang", "title": "Compressed Sensing with Very Sparse Gaussian Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of very sparse random projections for compressed sensing\n(sparse signal recovery) when the signal entries can be either positive or\nnegative. In our setting, the entries of a Gaussian design matrix are randomly\nsparsified so that only a very small fraction of the entries are nonzero. Our\nproposed decoding algorithm is simple and efficient in that the major cost is\none linear scan of the coordinates. We have developed two estimators: (i) the\n{\\em tie estimator}, and (ii) the {\\em absolute minimum estimator}. Using only\nthe tie estimator, we are able to recover a $K$-sparse signal of length $N$\nusing $1.551 eK \\log K/\\delta$ measurements (where $\\delta\\leq 0.05$ is the\nconfidence). Using only the absolute minimum estimator, we can detect the\nsupport of the signal using $eK\\log N/\\delta$ measurements. For a particular\ncoordinate, the absolute minimum estimator requires fewer measurements (i.e.,\nwith a constant $e$ instead of $1.551e$). Thus, the two estimators can be\ncombined to form an even more practical decoding framework.\n  Prior studies have shown that existing one-scan (or roughly one-scan)\nrecovery algorithms using sparse matrices would require substantially more\n(e.g., one order of magnitude) measurements than L1 decoding by linear\nprogramming, when the nonzero entries of signals can be either negative or\npositive. In this paper, following a known experimental setup, we show that, at\nthe same number of measurements, the recovery accuracies of our proposed method\nare (at least) similar to the standard L1 decoding.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 19:55:11 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1408.2539", "submitter": "Constantinos Daskalakis", "authors": "Yang Cai, Constantinos Daskalakis, Christos H. Papadimitriou", "title": "Optimum Statistical Estimation with Strategic Data Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimum mechanism for providing monetary incentives to the data\nsources of a statistical estimator such as linear regression, so that high\nquality data is provided at low cost, in the sense that the sum of payments and\nestimation error is minimized. The mechanism applies to a broad range of\nestimators, including linear and polynomial regression, kernel regression, and,\nunder some additional assumptions, ridge regression. It also generalizes to\nseveral objectives, including minimizing estimation error subject to budget\nconstraints. Besides our concrete results for regression problems, we\ncontribute a mechanism design framework through which to design and analyze\nstatistical estimators whose examples are supplied by workers with cost for\nlabeling said examples.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 20:13:15 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 16:39:54 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Cai", "Yang", ""], ["Daskalakis", "Constantinos", ""], ["Papadimitriou", "Christos H.", ""]]}, {"id": "1408.2552", "submitter": "Amit Gulab Deshwar", "authors": "Amit G. Deshwar, Shankar Vembu, Quaid Morris", "title": "Comparing Nonparametric Bayesian Tree Priors for Clonal Reconstruction\n  of Tumors", "comments": "Preprint of an article submitted for consideration in the Pacific\n  Symposium on Biocomputing \\c{opyright} 2015; World Scientific Publishing Co.,\n  Singapore, 2015; http://psb.stanford.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine learning methods, especially nonparametric Bayesian\nmethods, have become increasingly popular to infer clonal population structure\nof tumors. Here we describe the treeCRP, an extension of the Chinese restaurant\nprocess (CRP), a popular construction used in nonparametric mixture models, to\ninfer the phylogeny and genotype of major subclonal lineages represented in the\npopulation of cancer cells. We also propose new split-merge updates tailored to\nthe subclonal reconstruction problem that improve the mixing time of Markov\nchains. In comparisons with the tree-structured stick breaking prior used in\nPhyloSub, we demonstrate superior mixing and running time using the treeCRP\nwith our new split-merge procedures. We also show that given the same number of\nsamples, TSSB and treeCRP have similar ability to recover the subclonal\nstructure of a tumor.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 20:39:42 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Deshwar", "Amit G.", ""], ["Vembu", "Shankar", ""], ["Morris", "Quaid", ""]]}, {"id": "1408.2597", "submitter": "Yangyang Xu", "authors": "Yangyang Xu and Wotao Yin", "title": "Block stochastic gradient iteration for convex and nonconvex\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic gradient (SG) method can minimize an objective function\ncomposed of a large number of differentiable functions, or solve a stochastic\noptimization problem, to a moderate accuracy. The block coordinate\ndescent/update (BCD) method, on the other hand, handles problems with multiple\nblocks of variables by updating them one at a time; when the blocks of\nvariables are easier to update individually than together, BCD has a lower\nper-iteration cost. This paper introduces a method that combines the features\nof SG and BCD for problems with many components in the objective and with\nmultiple (blocks of) variables.\n  Specifically, a block stochastic gradient (BSG) method is proposed for\nsolving both convex and nonconvex programs. At each iteration, BSG approximates\nthe gradient of the differentiable part of the objective by randomly sampling a\nsmall set of data or sampling a few functions from the sum term in the\nobjective, and then, using those samples, it updates all the blocks of\nvariables in either a deterministic or a randomly shuffled order. Its\nconvergence for both convex and nonconvex cases are established in different\nsenses. In the convex case, the proposed method has the same order of\nconvergence rate as the SG method. In the nonconvex case, its convergence is\nestablished in terms of the expected violation of a first-order optimality\ncondition. The proposed method was numerically tested on problems including\nstochastic least squares and logistic regression, which are convex, as well as\nlow-rank tensor recovery and bilinear logistic regression, which are nonconvex.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 01:21:42 GMT"}, {"version": "v2", "created": "Tue, 26 Aug 2014 13:14:26 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 04:02:54 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Xu", "Yangyang", ""], ["Yin", "Wotao", ""]]}, {"id": "1408.2764", "submitter": "Harish Ramaswamy", "authors": "Harish G. Ramaswamy and Shivani Agarwal", "title": "Convex Calibration Dimension for Multiclass Loss Matrices", "comments": "Accepted to JMLR, pending editing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study consistency properties of surrogate loss functions for general\nmulticlass learning problems, defined by a general multiclass loss matrix. We\nextend the notion of classification calibration, which has been studied for\nbinary and multiclass 0-1 classification problems (and for certain other\nspecific learning problems), to the general multiclass setting, and derive\nnecessary and sufficient conditions for a surrogate loss to be calibrated with\nrespect to a loss matrix in this setting. We then introduce the notion of\nconvex calibration dimension of a multiclass loss matrix, which measures the\nsmallest `size' of a prediction space in which it is possible to design a\nconvex surrogate that is calibrated with respect to the loss matrix. We derive\nboth upper and lower bounds on this quantity, and use these results to analyze\nvarious loss matrices. In particular, we apply our framework to study various\nsubset ranking losses, and use the convex calibration dimension as a tool to\nshow both the existence and non-existence of various types of convex calibrated\nsurrogates for these losses. Our results strengthen recent results of Duchi et\nal. (2010) and Calauzenes et al. (2012) on the non-existence of certain types\nof convex calibrated surrogates in subset ranking. We anticipate the convex\ncalibration dimension may prove to be a useful tool in the study and design of\nsurrogate losses for general multiclass learning problems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 16:13:50 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2015 18:56:15 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Ramaswamy", "Harish G.", ""], ["Agarwal", "Shivani", ""]]}, {"id": "1408.2803", "submitter": "Jayadeva", "authors": "Jayadeva", "title": "Learning a hyperplane classifier by minimizing an exact bound on the VC\n  dimension", "comments": "Accepted Author Manuscript (Neurocomputing, Elsevier); 10 pages", "journal-ref": "Neurocomputing, Volume 149, Part B, 3 February 2015, Pages\n  683-689, ISSN 0925-2312,", "doi": "10.1016/j.neucom.2014.07.062", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The VC dimension measures the capacity of a learning machine, and a low VC\ndimension leads to good generalization. While SVMs produce state-of-the-art\nlearning performance, it is well known that the VC dimension of a SVM can be\nunbounded; despite good results in practice, there is no guarantee of good\ngeneralization. In this paper, we show how to learn a hyperplane classifier by\nminimizing an exact, or \\boldmath{$\\Theta$} bound on its VC dimension. The\nproposed approach, termed as the Minimal Complexity Machine (MCM), involves\nsolving a simple linear programming problem. Experimental results show, that on\na number of benchmark datasets, the proposed approach learns classifiers with\nerror rates much less than conventional SVMs, while often using fewer support\nvectors. On many benchmark datasets, the number of support vectors is less than\none-tenth the number used by SVMs, indicating that the MCM does indeed learn\nsimpler representations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 18:57:48 GMT"}, {"version": "v2", "created": "Wed, 13 Aug 2014 16:32:30 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Jayadeva", "", ""]]}, {"id": "1408.2869", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki, Jacek Tabor", "title": "Cluster based RBF Kernel for Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical Gaussian SVM classification we use the feature space\nprojection transforming points to normal distributions with fixed covariance\nmatrices (identity in the standard RBF and the covariance of the whole dataset\nin Mahalanobis RBF). In this paper we add additional information to Gaussian\nSVM by considering local geometry-dependent feature space projection. We\nemphasize that our approach is in fact an algorithm for a construction of the\nnew Gaussian-type kernel.\n  We show that better (compared to standard RBF and Mahalanobis RBF)\nclassification results are obtained in the simple case when the space is\npreliminary divided by k-means into two sets and points are represented as\nnormal distributions with a covariances calculated according to the dataset\npartitioning.\n  We call the constructed method C$_k$RBF, where $k$ stands for the amount of\nclusters used in k-means. We show empirically on nine datasets from UCI\nrepository that C$_2$RBF increases the stability of the grid search (measured\nas the probability of finding good parameters).\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 22:30:11 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""], ["Tabor", "Jacek", ""]]}, {"id": "1408.2873", "submitter": "Andrew Maas", "authors": "Awni Y. Hannun, Andrew L. Maas, Daniel Jurafsky, Andrew Y. Ng", "title": "First-Pass Large Vocabulary Continuous Speech Recognition using\n  Bi-Directional Recurrent DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to perform first-pass large vocabulary continuous speech\nrecognition using only a neural network and language model. Deep neural network\nacoustic models are now commonplace in HMM-based speech recognition systems,\nbut building such systems is a complex, domain-specific task. Recent work\ndemonstrated the feasibility of discarding the HMM sequence modeling framework\nby directly predicting transcript text from audio. This paper extends this\napproach in two ways. First, we demonstrate that a straightforward recurrent\nneural network architecture can achieve a high level of accuracy. Second, we\npropose and evaluate a modified prefix-search decoding algorithm. This approach\nto decoding enables first-pass speech recognition with a language model,\ncompletely unaided by the cumbersome infrastructure of HMM-based systems.\nExperiments on the Wall Street Journal corpus demonstrate fairly competitive\nword error rates, and the importance of bi-directional network recurrence.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 22:40:21 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 20:21:52 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Hannun", "Awni Y.", ""], ["Maas", "Andrew L.", ""], ["Jurafsky", "Daniel", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1408.2889", "submitter": "Alceu de Souza Britto Jr", "authors": "Albert H. R. Ko, Robert Sabourin, Alceu S. Britto Jr, Luiz E. S.\n  Oliveira", "title": "A Classifier-free Ensemble Selection Method based on Data Diversity in\n  Random Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ensemble of Classifiers (EoC) has been shown to be effective in improving\nthe performance of single classifiers by combining their outputs, and one of\nthe most important properties involved in the selection of the best EoC from a\npool of classifiers is considered to be classifier diversity. In general,\nclassifier diversity does not occur randomly, but is generated systematically\nby various ensemble creation methods. By using diverse data subsets to train\nclassifiers, these methods can create diverse classifiers for the EoC. In this\nwork, we propose a scheme to measure data diversity directly from random\nsubspaces, and explore the possibility of using it to select the best data\nsubsets for the construction of the EoC. Our scheme is the first ensemble\nselection method to be presented in the literature based on the concept of data\ndiversity. Its main advantage over the traditional framework (ensemble creation\nthen selection) is that it obviates the need for classifier training prior to\nensemble selection. A single Genetic Algorithm (GA) and a Multi-Objective\nGenetic Algorithm (MOGA) were evaluated to search for the best solutions for\nthe classifier-free ensemble selection. In both cases, objective functions\nbased on different clustering diversity measures were implemented and tested.\nAll the results obtained with the proposed classifier-free ensemble selection\nmethod were compared with the traditional classifier-based ensemble selection\nusing Mean Classifier Error (ME) and Majority Voting Error (MVE). The\napplicability of the method is tested on UCI machine learning problems and NIST\nSD19 handwritten numerals.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 00:38:41 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Ko", "Albert H. R.", ""], ["Sabourin", "Robert", ""], ["Britto", "Alceu S.", "Jr"], ["Oliveira", "Luiz E. S.", ""]]}, {"id": "1408.2890", "submitter": "Yang Liu", "authors": "Yang Liu, Bo He, Diya Dong, Yue Shen, Tianhong Yan, Rui Nian, Amaury\n  Lendase", "title": "Robust OS-ELM with a novel selective ensemble based on particle swarm\n  optimization", "comments": "Submitted to Mathematical Problems in Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a robust online sequential extreme learning machine (ROS-ELM)\nis proposed. It is based on the original OS-ELM with an adaptive selective\nensemble framework. Two novel insights are proposed in this paper. First, a\nnovel selective ensemble algorithm referred to as particle swarm optimization\nselective ensemble (PSOSEN) is proposed. Noting that PSOSEN is a general\nselective ensemble method which is applicable to any learning algorithms,\nincluding batch learning and online learning. Second, an adaptive selective\nensemble framework for online learning is designed to balance the robustness\nand complexity of the algorithm. Experiments for both regression and\nclassification problems with UCI data sets are carried out. Comparisons between\nOS-ELM, simple ensemble OS-ELM (EOS-ELM) and the proposed ROS-ELM empirically\nshow that ROS-ELM significantly improves the robustness and stability.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 00:41:01 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Liu", "Yang", ""], ["He", "Bo", ""], ["Dong", "Diya", ""], ["Shen", "Yue", ""], ["Yan", "Tianhong", ""], ["Nian", "Rui", ""], ["Lendase", "Amaury", ""]]}, {"id": "1408.2938", "submitter": "Wenbin Li", "authors": "Wenbin Li, Mario Fritz", "title": "Learning Multi-Scale Representations for Material Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent progress in sparse coding and deep learning has made unsupervised\nfeature learning methods a strong competitor to hand-crafted descriptors. In\ncomputer vision, success stories of learned features have been predominantly\nreported for object recognition tasks. In this paper, we investigate if and how\nfeature learning can be used for material recognition. We propose two\nstrategies to incorporate scale information into the learning procedure\nresulting in a novel multi-scale coding procedure. Our results show that our\nlearned features for material recognition outperform hand-crafted descriptors\non the FMD and the KTH-TIPS2 material classification benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 08:27:12 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Li", "Wenbin", ""], ["Fritz", "Mario", ""]]}, {"id": "1408.3060", "submitter": "Alex Smola J", "authors": "Quoc Viet Le, Tamas Sarlos, Alexander Johannes Smola", "title": "Fastfood: Approximate Kernel Expansions in Loglinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their successes, what makes kernel methods difficult to use in many\nlarge scale problems is the fact that storing and computing the decision\nfunction is typically expensive, especially at prediction time. In this paper,\nwe overcome this difficulty by proposing Fastfood, an approximation that\naccelerates such computation significantly. Key to Fastfood is the observation\nthat Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit\nproperties similar to dense Gaussian random matrices. Yet unlike the latter,\nHadamard and diagonal matrices are inexpensive to multiply and store. These two\nmatrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks\nproposed by Rahimi and Recht (2009) and thereby speeding up the computation for\na large range of kernel functions. Specifically, Fastfood requires O(n log d)\ntime and O(n) storage to compute n non-linear basis functions in d dimensions,\na significant improvement from O(nd) computation and storage, without\nsacrificing accuracy.\n  Our method applies to any translation invariant and any dot-product kernel,\nsuch as the popular RBF kernels and polynomial kernels. We prove that the\napproximation is unbiased and has low variance. Experiments show that we\nachieve similar accuracy to full kernel expansions and Random Kitchen Sinks\nwhile being 100x faster and using 1000x less memory. These improvements,\nespecially in terms of memory usage, make kernel methods more practical for\napplications that have large training sets and/or require real-time prediction.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 17:37:43 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Le", "Quoc Viet", ""], ["Sarlos", "Tamas", ""], ["Smola", "Alexander Johannes", ""]]}, {"id": "1408.3081", "submitter": "Truyen Tran", "authors": "Truyen Tran, Hung Bui, Svetha Venkatesh", "title": "Human Activity Learning and Segmentation using Partially Hidden\n  Discriminative Models", "comments": "HAREM 2005: Proceedings of the International Workshop on Human\n  Activity Recognition and Modelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and understanding the typical patterns in the daily activities and\nroutines of people from low-level sensory data is an important problem in many\napplication domains such as building smart environments, or providing\nintelligent assistance. Traditional approaches to this problem typically rely\non supervised learning and generative models such as the hidden Markov models\nand its extensions. While activity data can be readily acquired from pervasive\nsensors, e.g. in smart environments, providing manual labels to support\nsupervised training is often extremely expensive. In this paper, we propose a\nnew approach based on semi-supervised training of partially hidden\ndiscriminative models such as the conditional random field (CRF) and the\nmaximum entropy Markov model (MEMM). We show that these models allow us to\nincorporate both labeled and unlabeled data for learning, and at the same time,\nprovide us with the flexibility and accuracy of the discriminative framework.\nOur experimental results in the video surveillance domain illustrate that these\nmodels can perform better than their generative counterpart, the partially\nhidden Markov model, even when a substantial amount of labels are unavailable.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 02:35:49 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Tran", "Truyen", ""], ["Bui", "Hung", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.3092", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki", "title": "Convergence rate of Bayesian tensor estimator: Optimal rate without\n  restricted strong convexity", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the statistical convergence rate of a Bayesian\nlow-rank tensor estimator. Our problem setting is the regression problem where\na tensor structure underlying the data is estimated. This problem setting\noccurs in many practical applications, such as collaborative filtering,\nmulti-task learning, and spatio-temporal data analysis. The convergence rate is\nanalyzed in terms of both in-sample and out-of-sample predictive accuracies. It\nis shown that a near optimal rate is achieved without any strong convexity of\nthe observation. Moreover, we show that the method has adaptivity to the\nunknown rank of the true tensor, that is, the near optimal rate depending on\nthe true rank is achieved even if it is not known a priori.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 19:16:29 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Suzuki", "Taiji", ""]]}, {"id": "1408.3115", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Rong Jin, Shenghuo Zhu, Qihang Lin", "title": "On Data Preconditioning for Regularized Loss Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study data preconditioning, a well-known and long-existing\ntechnique, for boosting the convergence of first-order methods for regularized\nloss minimization. It is well understood that the condition number of the\nproblem, i.e., the ratio of the Lipschitz constant to the strong convexity\nmodulus, has a harsh effect on the convergence of the first-order optimization\nmethods. Therefore, minimizing a small regularized loss for achieving good\ngeneralization performance, yielding an ill conditioned problem, becomes the\nbottleneck for big data problems. We provide a theory on data preconditioning\nfor regularized loss minimization. In particular, our analysis exhibits an\nappropriate data preconditioner and characterizes the conditions on the loss\nfunction and on the data under which data preconditioning can reduce the\ncondition number and therefore boost the convergence for minimizing the\nregularized loss. To make the data preconditioning practically useful, we\nendeavor to employ and analyze a random sampling approach to efficiently\ncompute the preconditioned data. The preliminary experiments validate our\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 18:44:18 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 06:29:17 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 15:23:40 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2015 15:35:10 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Yang", "Tianbao", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""], ["Lin", "Qihang", ""]]}, {"id": "1408.3169", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "Indefinitely Oscillating Martingales", "comments": "ALT 2014, extended technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We construct a class of nonnegative martingale processes that oscillate\nindefinitely with high probability. For these processes, we state a uniform\nrate of the number of oscillations and show that this rate is asymptotically\nclose to the theoretical upper bound. These bounds on probability and\nexpectation of the number of upcrossings are compared to classical bounds from\nthe martingale literature. We discuss two applications. First, our results\nimply that the limit of the minimum description length operator may not exist.\nSecond, we give bounds on how often one can change one's belief in a given\nhypothesis when observing a stream of data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 00:19:03 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1408.3218", "submitter": "Babak Saleh", "authors": "Babak Saleh, Kanako Abe, Ravneet Singh Arora, Ahmed Elgammal", "title": "Toward Automated Discovery of Artistic Influence", "comments": "29 pages, 14 figures and 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the huge amount of art pieces that exist, there is valuable\ninformation to be discovered. Examining a painting, an expert can determine its\nstyle, genre, and the time period that the painting belongs. One important task\nfor art historians is to find influences and connections between artists. Is\ninfluence a task that a computer can measure? The contribution of this paper is\nin exploring the problem of computer-automated suggestion of influences between\nartists, a problem that was not addressed before in a general setting. We first\npresent a comparative study of different classification methodologies for the\ntask of fine-art style classification. A two-level comparative study is\nperformed for this classification problem. The first level reviews the\nperformance of discriminative vs. generative models, while the second level\ntouches the features aspect of the paintings and compares semantic-level\nfeatures vs. low-level and intermediate-level features present in the painting.\nThen, we investigate the question \"Who influenced this artist?\" by looking at\nhis masterpieces and comparing them to others. We pose this interesting\nquestion as a knowledge discovery problem. For this purpose, we investigated\nseveral painting-similarity and artist-similarity measures. As a result, we\nprovide a visualization of artists (Map of Artists) based on the similarity\nbetween their works\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 08:42:22 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Saleh", "Babak", ""], ["Abe", "Kanako", ""], ["Arora", "Ravneet Singh", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1408.3264", "submitter": "Mohammad Ali Keyvanrad", "authors": "Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour", "title": "A brief survey on deep belief networks and introducing a new object\n  oriented toolbox (DeeBNet)", "comments": "Technical Report 27 pages, Ver3.0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, this is very popular to use the deep architectures in machine\nlearning. Deep Belief Networks (DBNs) are deep architectures that use stack of\nRestricted Boltzmann Machines (RBM) to create a powerful generative model using\ntraining data. DBNs have many ability like feature extraction and\nclassification that are used in many applications like image processing, speech\nprocessing and etc. This paper introduces a new object oriented MATLAB toolbox\nwith most of abilities needed for the implementation of DBNs. In the new\nversion, the toolbox can be used in Octave. According to the results of the\nexperiments conducted on MNIST (image), ISOLET (speech), and 20 Newsgroups\n(text) datasets, it was shown that the toolbox can learn automatically a good\nrepresentation of the input from unlabeled data with better discrimination\nbetween different classes. Also on all datasets, the obtained classification\nerrors are comparable to those of state of the art classifiers. In addition,\nthe toolbox supports different sampling methods (e.g. Gibbs, CD, PCD and our\nnew FEPCD method), different sparsity methods (quadratic, rate distortion and\nour new normal method), different RBM types (generative and discriminative),\nusing GPU, etc. The toolbox is a user-friendly open source software and is\nfreely available on the website\nhttp://ceit.aut.ac.ir/~keyvanrad/DeeBNet%20Toolbox.html .\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 12:37:57 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 14:44:02 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2015 12:44:01 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2015 13:21:02 GMT"}, {"version": "v5", "created": "Wed, 22 Jul 2015 14:25:13 GMT"}, {"version": "v6", "created": "Mon, 7 Sep 2015 14:44:47 GMT"}, {"version": "v7", "created": "Wed, 6 Jan 2016 13:20:11 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Keyvanrad", "Mohammad Ali", ""], ["Homayounpour", "Mohammad Mehdi", ""]]}, {"id": "1408.3332", "submitter": "Victor Nedelko", "authors": "Victor Nedelko", "title": "Exact and empirical estimation of misclassification probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the problem of risk estimation in the classification problem, with\nspecific focus on finding distributions that maximize the confidence intervals\nof risk estimation. We derived simple analytic approximations for the maximum\nbias of empirical risk for histogram classifier. We carry out a detailed study\non using these analytic estimates for empirical estimation of risk.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 16:29:36 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Nedelko", "Victor", ""]]}, {"id": "1408.3337", "submitter": "Ari Seff", "authors": "Ari Seff, Le Lu, Kevin M. Cherry, Holger Roth, Jiamin Liu, Shijun\n  Wang, Joanne Hoffman, Evrim B. Turkbey, and Ronald M. Summers", "title": "2D View Aggregation for Lymph Node Detection Using a Shallow Hierarchy\n  of Linear Classifiers", "comments": "This article will be presented at MICCAI (Medical Image Computing and\n  Computer-Assisted Intervention) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Enlarged lymph nodes (LNs) can provide important information for cancer\ndiagnosis, staging, and measuring treatment reactions, making automated\ndetection a highly sought goal. In this paper, we propose a new algorithm\nrepresentation of decomposing the LN detection problem into a set of 2D object\ndetection subtasks on sampled CT slices, largely alleviating the curse of\ndimensionality issue. Our 2D detection can be effectively formulated as linear\nclassification on a single image feature type of Histogram of Oriented\nGradients (HOG), covering a moderate field-of-view of 45 by 45 voxels. We\nexploit both simple pooling and sparse linear fusion schemes to aggregate these\n2D detection scores for the final 3D LN detection. In this manner, detection is\nmore tractable and does not need to perform perfectly at instance level (as\nweak hypotheses) since our aggregation process will robustly harness collective\ninformation for LN detection. Two datasets (90 patients with 389 mediastinal\nLNs and 86 patients with 595 abdominal LNs) are used for validation.\nCross-validation demonstrates 78.0% sensitivity at 6 false positives/volume\n(FP/vol.) (86.1% at 10 FP/vol.) and 73.1% sensitivity at 6 FP/vol. (87.2% at 10\nFP/vol.), for the mediastinal and abdominal datasets respectively. Our results\ncompare favorably to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 16:47:34 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Seff", "Ari", ""], ["Lu", "Le", ""], ["Cherry", "Kevin M.", ""], ["Roth", "Holger", ""], ["Liu", "Jiamin", ""], ["Wang", "Shijun", ""], ["Hoffman", "Joanne", ""], ["Turkbey", "Evrim B.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1408.3359", "submitter": "Bing Li", "authors": "Bing Li, Hongyuan Zha, Francesca Chiaromonte", "title": "Linear Contour Learning: A Method for Supervised Dimension Reduction", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-349-356", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to sufficient dimension reduction in regression,\nbased on estimating contour directions of negligible variation for the response\nsurface. These directions span the orthogonal complement of the minimal space\nrelevant for the regression, and can be extracted according to a measure of the\nvariation in the response, leading to General Contour Regression(GCR). In\ncomparison to exiisting sufficient dimension reduction techniques, this\nsontour-based mothology guarantees exhaustive estimation of the central space\nunder ellipticity of the predictoor distribution and very mild additional\nassumptions, while maintaining vn-consisytency and somputational ease.\nMoreover, it proves to be robust to departures from ellipticity. We also\nestablish some useful population properties for GCR. Simulations to compare\nperformance with that of standard techniques such as ordinary least squares,\nsliced inverse regression, principal hessian directions, and sliced average\nvariance estimation confirm the advntages anticipated by theoretical analyses.\nWe also demonstrate the use of contour-based methods on a data set concerning\ngrades of students from Massachusetts colleges.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 05:14:15 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Li", "Bing", ""], ["Zha", "Hongyuan", ""], ["Chiaromonte", "Francesca", ""]]}, {"id": "1408.3382", "submitter": "Kalyan Veeramachaneni", "authors": "Colin Taylor, Kalyan Veeramachaneni, Una-May O'Reilly", "title": "Likely to stop? Predicting Stopout in Massive Open Online Courses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why students stopout will help in understanding how students\nlearn in MOOCs. In this report, part of a 3 unit compendium, we describe how we\nbuild accurate predictive models of MOOC student stopout. We document a\nscalable, stopout prediction methodology, end to end, from raw source data to\nmodel analysis. We attempted to predict stopout for the Fall 2012 offering of\n6.002x. This involved the meticulous and crowd-sourced engineering of over 25\npredictive features extracted for thousands of students, the creation of\ntemporal and non-temporal data representations for use in predictive modeling,\nthe derivation of over 10 thousand models with a variety of state-of-the-art\nmachine learning techniques and the analysis of feature importance by examining\nover 70000 models. We found that stop out prediction is a tractable problem.\nOur models achieved an AUC (receiver operating characteristic\narea-under-the-curve) as high as 0.95 (and generally 0.88) when predicting one\nweek in advance. Even with more difficult prediction problems, such as\npredicting stop out at the end of the course with only one weeks' data, the\nmodels attained AUCs of 0.7.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 18:54:30 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Taylor", "Colin", ""], ["Veeramachaneni", "Kalyan", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "1408.3467", "submitter": "Qianqian Xu", "authors": "Qianqian Xu and Jiechao Xiong and Xiaochun Cao and Qingming Huang and\n  Yuan Yao", "title": "Evaluating Visual Properties via Robust HodgeRank", "comments": "25 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, how to effectively evaluate visual properties has become a popular\ntopic for fine-grained visual comprehension. In this paper we study the problem\nof how to estimate such visual properties from a ranking perspective with the\nhelp of the annotators from online crowdsourcing platforms. The main challenges\nof our task are two-fold. On one hand, the annotations often contain\ncontaminated information, where a small fraction of label flips might ruin the\nglobal ranking of the whole dataset. On the other hand, considering the large\ndata capacity, the annotations are often far from being complete. What is\nworse, there might even exist imbalanced annotations where a small subset of\nsamples are frequently annotated. Facing such challenges, we propose a robust\nranking framework based on the principle of Hodge decomposition of imbalanced\nand incomplete ranking data. According to the HodgeRank theory, we find that\nthe major source of the contamination comes from the cyclic ranking component\nof the Hodge decomposition. This leads us to an outlier detection formulation\nas sparse approximations of the cyclic ranking projection. Taking a step\nfurther, it facilitates a novel outlier detection model as Huber's LASSO in\nrobust statistics. Moreover, simple yet scalable algorithms are developed based\non Linearized Bregman Iteration to achieve an even less biased estimator.\nStatistical consistency of outlier detection is established in both cases under\nnearly the same conditions. Our studies are supported by experiments with both\nsimulated examples and real-world data. The proposed framework provides us a\npromising tool for robust ranking with large scale crowdsourcing data arising\nfrom computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 05:18:19 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 04:06:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Cao", "Xiaochun", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1408.3693", "submitter": "Zaid Towfic", "authors": "Zaid J. Towfic and Ali H. Sayed", "title": "Stability and Performance Limits of Adaptive Primal-Dual Networks", "comments": "16 pages, 9 figures", "journal-ref": "IEEE Transactions on Signal Processing, vol. 63, no. 11, pp.\n  2888-2903, Jun. 2015", "doi": "10.1109/TSP.2015.2415759", "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies distributed primal-dual strategies for adaptation and\nlearning over networks from streaming data. Two first-order methods are\nconsidered based on the Arrow-Hurwicz (AH) and augmented Lagrangian (AL)\ntechniques. Several revealing results are discovered in relation to the\nperformance and stability of these strategies when employed over adaptive\nnetworks. The conclusions establish that the advantages that these methods have\nfor deterministic optimization problems do not necessarily carry over to\nstochastic optimization problems. It is found that they have narrower stability\nranges and worse steady-state mean-square-error performance than primal methods\nof the consensus and diffusion type. It is also found that the AH technique can\nbecome unstable under a partial observation model, while the other techniques\nare able to recover the unknown under this scenario. A method to enhance the\nperformance of AL strategies is proposed by tying the selection of the\nstep-size to their regularization parameter. It is shown that this method\nallows the AL algorithm to approach the performance of consensus and diffusion\nstrategies but that it remains less stable than these other strategies.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 01:52:42 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2015 20:32:43 GMT"}, {"version": "v3", "created": "Wed, 13 May 2015 12:04:30 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Towfic", "Zaid J.", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1408.3727", "submitter": "Kun Li", "authors": "Kun Li, Max Q.-H. Meng", "title": "Inverse Reinforcement Learning with Multi-Relational Chains for\n  Robot-Centered Smart Home", "comments": "This paper has been withdrawn due to incorrect article structure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a robot-centered smart home, the robot observes the home states with its\nown sensors, and then it can change certain object states according to an\noperator's commands for remote operations, or imitate the operator's behaviors\nin the house for autonomous operations. To model the robot's imitation of the\noperator's behaviors in a dynamic indoor environment, we use multi-relational\nchains to describe the changes of environment states, and apply inverse\nreinforcement learning to encoding the operator's behaviors with a learned\nreward function. We implement this approach with a mobile robot, and do five\nexperiments to include increasing training days, object numbers, and action\ntypes. Besides, a baseline method by directly recording the operator's\nbehaviors is also implemented, and comparison is made on the accuracy of home\nstate evaluation and the accuracy of robot action selection. The results show\nthat the proposed approach handles dynamic environment well, and guides the\nrobot's actions in the house more accurately.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 09:20:47 GMT"}, {"version": "v2", "created": "Thu, 28 Aug 2014 07:06:15 GMT"}, {"version": "v3", "created": "Tue, 30 Sep 2014 07:42:53 GMT"}, {"version": "v4", "created": "Fri, 28 Nov 2014 06:07:30 GMT"}, {"version": "v5", "created": "Sat, 18 Apr 2015 01:27:35 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Li", "Kun", ""], ["Meng", "Max Q. -H.", ""]]}, {"id": "1408.3733", "submitter": "Ehtesham Hassan", "authors": "Ehtesham Hassan and Gautam Shroff and Puneet Agarwal", "title": "Multi-Sensor Event Detection using Shape Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicular sensor data consists of multiple time-series arising from a number\nof sensors. Using such multi-sensor data we would like to detect occurrences of\nspecific events that vehicles encounter, e.g., corresponding to particular\nmaneuvers that a vehicle makes or conditions that it encounters. Events are\ncharacterized by similar waveform patterns re-appearing within one or more\nsensors. Further such patterns can be of variable duration. In this work, we\npropose a method for detecting such events in time-series data using a novel\nfeature descriptor motivated by similar ideas in image processing. We define\nthe shape histogram: a constant dimension descriptor that nevertheless captures\npatterns of variable duration. We demonstrate the efficacy of using shape\nhistograms as features to detect events in an SVM-based, multi-sensor,\nsupervised learning scenario, i.e., multiple time-series are used to detect an\nevent. We present results on real-life vehicular sensor data and show that our\ntechnique performs better than available pattern detection implementations on\nour data, and that it can also be used to combine features from multiple\nsensors resulting in better accuracy than using any single sensor. Since\nprevious work on pattern detection in time-series has been in the single series\ncontext, we also present results using our technique on multiple standard\ntime-series datasets and show that it is the most versatile in terms of how it\nranks compared to other published results.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 11:11:59 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Hassan", "Ehtesham", ""], ["Shroff", "Gautam", ""], ["Agarwal", "Puneet", ""]]}, {"id": "1408.3750", "submitter": "S\\'ebastien Ouellet", "authors": "S\\'ebastien Ouellet", "title": "Real-time emotion recognition for gaming using deep convolutional\n  network features", "comments": "6 pages, 8 figures, IEEE style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the present study is to explore the application of deep\nconvolutional network features to emotion recognition. Results indicate that\nthey perform similarly to other published models at a best recognition rate of\n94.4%, and do so with a single still image rather than a video stream. An\nimplementation of an affective feedback game is also described, where a\nclassifier using these features tracks the facial expressions of a player in\nreal-time.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 17:11:44 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Ouellet", "S\u00e9bastien", ""]]}, {"id": "1408.3944", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (IRISA), Sylvie Gibet (IRISA), Clement\n  Reverdy (IRISA)", "title": "Down-Sampling coupled to Elastic Kernel Machines for Efficient\n  Recognition of Isolated Gestures", "comments": "ICPR 2014, International Conference on Pattern Recognition, Stockholm\n  : Sweden (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of gestural action recognition, many studies have focused on\ndimensionality reduction along the spatial axis, to reduce both the variability\nof gestural sequences expressed in the reduced space, and the computational\ncomplexity of their processing. It is noticeable that very few of these methods\nhave explicitly addressed the dimensionality reduction along the time axis.\nThis is however a major issue with regard to the use of elastic distances\ncharacterized by a quadratic complexity. To partially fill this apparent gap,\nwe present in this paper an approach based on temporal down-sampling associated\nto elastic kernel machine learning. We experimentally show, on two data sets\nthat are widely referenced in the domain of human gesture recognition, and very\ndifferent in terms of quality of motion capture, that it is possible to\nsignificantly reduce the number of skeleton frames while maintaining a good\nrecognition rate. The method proves to give satisfactory results at a level\ncurrently reached by state-of-the-art methods on these data sets. The\ncomputational complexity reduction makes this approach eligible for real-time\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 09:18:38 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 16:19:34 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "IRISA"], ["Gibet", "Sylvie", "", "IRISA"], ["Reverdy", "Clement", "", "IRISA"]]}, {"id": "1408.3967", "submitter": "Zhanpeng Zhang", "authors": "Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Learning Deep Representation for Face Alignment with Auxiliary\n  Attributes", "comments": "to be published in the IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2469286", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we show that landmark detection or face alignment task is not\na single and independent problem. Instead, its robustness can be greatly\nimproved with auxiliary information. Specifically, we jointly optimize landmark\ndetection together with the recognition of heterogeneous but subtly correlated\nfacial attributes, such as gender, expression, and appearance attributes. This\nis non-trivial since different attribute inference tasks have different\nlearning difficulties and convergence rates. To address this problem, we\nformulate a novel tasks-constrained deep model, which not only learns the\ninter-task correlation but also employs dynamic task coefficients to facilitate\nthe optimization convergence when learning multiple complex tasks. Extensive\nevaluations show that the proposed task-constrained learning (i) outperforms\nexisting face alignment methods, especially in dealing with faces with severe\nocclusion and pose variation, and (ii) reduces model complexity drastically\ncompared to the state-of-the-art methods based on cascaded deep model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 10:34:29 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 08:02:41 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2015 00:39:18 GMT"}, {"version": "v4", "created": "Tue, 11 Aug 2015 10:08:40 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zhang", "Zhanpeng", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1408.4045", "submitter": "Soledad Villar", "authors": "Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar\n  Krishnaswamy, Soledad Villar, Rachel Ward", "title": "Relax, no need to round: integrality of clustering formulations", "comments": "30 pages, ITCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exact recovery conditions for convex relaxations of point cloud\nclustering problems, focusing on two of the most common optimization problems\nfor unsupervised clustering: $k$-means and $k$-median clustering. Motivations\nfor focusing on convex relaxations are: (a) they come with a certificate of\noptimality, and (b) they are generic tools which are relatively parameter-free,\nnot tailored to specific assumptions over the input. More precisely, we\nconsider the distributional setting where there are $k$ clusters in\n$\\mathbb{R}^m$ and data from each cluster consists of $n$ points sampled from a\nsymmetric distribution within a ball of unit radius. We ask: what is the\nminimal separation distance between cluster centers needed for convex\nrelaxations to exactly recover these $k$ clusters as the optimal integral\nsolution? For the $k$-median linear programming relaxation we show a tight\nbound: exact recovery is obtained given arbitrarily small pairwise separation\n$\\epsilon > 0$ between the balls. In other words, the pairwise center\nseparation is $\\Delta > 2+\\epsilon$. Under the same distributional model, the\n$k$-means LP relaxation fails to recover such clusters at separation as large\nas $\\Delta = 4$. Yet, if we enforce PSD constraints on the $k$-means LP, we get\nexact cluster recovery at center separation $\\Delta > 2\\sqrt2(1+\\sqrt{1/m})$.\nIn contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-means\nalgorithm) can fail to recover clusters in this setting; even with arbitrarily\nlarge cluster separation, k-means++ with overseeding by any constant factor\nfails with high probability at exact cluster recovery. To complement the\ntheoretical analysis, we provide an experimental study of the recovery\nguarantees for these various methods, and discuss several open problems which\nthese experiments suggest.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 15:42:16 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 18:37:34 GMT"}, {"version": "v3", "created": "Wed, 10 Dec 2014 18:07:10 GMT"}, {"version": "v4", "created": "Tue, 10 Feb 2015 16:11:36 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2015 02:11:54 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Bandeira", "Afonso S.", ""], ["Charikar", "Moses", ""], ["Krishnaswamy", "Ravishankar", ""], ["Villar", "Soledad", ""], ["Ward", "Rachel", ""]]}, {"id": "1408.4072", "submitter": "Eugene Wu", "authors": "Leilani Battle, Edward Benson, Aditya Parameswaran, Eugene Wu", "title": "Indexing Cost Sensitive Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models are often used for real-time decision making. However,\ntypical machine learning techniques ignore feature evaluation cost, and focus\nsolely on the accuracy of the machine learning models obtained utilizing all\nthe features available. We develop algorithms and indexes to support\ncost-sensitive prediction, i.e., making decisions using machine learning models\ntaking feature evaluation cost into account. Given an item and a online\ncomputation cost (i.e., time) budget, we present two approaches to return an\nappropriately chosen machine learning model that will run within the specified\ntime on the given item. The first approach returns the optimal machine learning\nmodel, i.e., one with the highest accuracy, that runs within the specified\ntime, but requires significant up-front precomputation time. The second\napproach returns a possibly sub- optimal machine learning model, but requires\nlittle up-front precomputation time. We study these two algorithms in detail\nand characterize the scenarios (using real and synthetic data) in which each\nperforms well. Unlike prior work that focuses on a narrow domain or a specific\nalgorithm, our techniques are very general: they apply to any cost-sensitive\nprediction scenario on any machine learning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 07:21:48 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Battle", "Leilani", ""], ["Benson", "Edward", ""], ["Parameswaran", "Aditya", ""], ["Wu", "Eugene", ""]]}, {"id": "1408.4551", "submitter": "Ankur Kulkarni", "authors": "Bharat Prabhakar, Ankur A. Kulkarni", "title": "Dimensionality Reduction of Affine Variational Inequalities Using Random\n  Projections", "comments": "Submitted to Mathematical Programming Series A. Edited some typos\n  from the previous version. Also added a bound on the lower dimension", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for dimensionality reduction of an affine variational\ninequality (AVI) defined over a compact feasible region. Centered around the\nJohnson Lindenstrauss lemma, our method is a randomized algorithm that produces\nwith high probability an approximate solution for the given AVI by solving a\nlower-dimensional AVI. The algorithm allows the lower dimension to be chosen\nbased on the quality of approximation desired. The algorithm can also be used\nas a subroutine in an exact algorithm for generating an initial point close to\nthe solution. The lower-dimensional AVI is obtained by appropriately projecting\nthe original AVI on a randomly chosen subspace. The lower-dimensional AVI is\nsolved using standard solvers and from this solution an approximate solution to\nthe original AVI is recovered through an inexpensive process. Our numerical\nexperiments corroborate the theoretical results and validate that the algorithm\nprovides a good approximation at low dimensions and substantial savings in time\nfor an exact solution.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 07:29:11 GMT"}, {"version": "v2", "created": "Sat, 8 Nov 2014 12:13:28 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Prabhakar", "Bharat", ""], ["Kulkarni", "Ankur A.", ""]]}, {"id": "1408.4576", "submitter": "Bingchen Gong", "authors": "Sibei Yang and Liangde Tao and Bingchen Gong", "title": "Introduction to Clustering Algorithms and Applications", "comments": "This paper has been withdrawn by the author due to unsuitable content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Data clustering is the process of identifying natural groupings or clusters\nwithin multidimensional data based on some similarity measure. Clustering is a\nfundamental process in many different disciplines. Hence, researchers from\ndifferent fields are actively working on the clustering problem. This paper\nprovides an overview of the different representative clustering methods. In\naddition, application of clustering in different field is briefly introduced.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 09:25:18 GMT"}, {"version": "v2", "created": "Mon, 25 Aug 2014 14:49:56 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Yang", "Sibei", ""], ["Tao", "Liangde", ""], ["Gong", "Bingchen", ""]]}, {"id": "1408.4622", "submitter": "Emmanuel Vazquez", "authors": "Emmanuel Vazquez and Julien Bect", "title": "A new integral loss function for Bayesian optimization", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a real-valued continuous function $f$\nusing a Bayesian approach. Since the early work of Jonas Mockus and Antanas\n\\v{Z}ilinskas in the 70's, the problem of optimization is usually formulated by\nconsidering the loss function $\\max f - M_n$ (where $M_n$ denotes the best\nfunction value observed after $n$ evaluations of $f$). This loss function puts\nemphasis on the value of the maximum, at the expense of the location of the\nmaximizer. In the special case of a one-step Bayes-optimal strategy, it leads\nto the classical Expected Improvement (EI) sampling criterion. This is a\nspecial case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk\nassociated to a certain uncertainty measure (here, the expected loss) on the\nquantity of interest is minimized at each step of the algorithm. In this\narticle, assuming that $f$ is defined over a measure space $(\\mathbb{X},\n\\lambda)$, we propose to consider instead the integral loss function\n$\\int_{\\mathbb{X}} (f - M_n)_{+}\\, d\\lambda$, and we show that this leads, in\nthe case of a Gaussian process prior, to a new numerically tractable sampling\ncriterion that we call $\\rm EI^2$ (for Expected Integrated Expected\nImprovement). A numerical experiment illustrates that a SUR strategy based on\nthis new sampling criterion reduces the error on both the value and the\nlocation of the maximizer faster than the EI-based strategy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 12:16:54 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Vazquez", "Emmanuel", ""], ["Bect", "Julien", ""]]}, {"id": "1408.4673", "submitter": "Ruhollah Majdoddin", "authors": "Ruhollah Majdoddin", "title": "AFP Algorithm and a Canonical Normal Form for Horn Formulas", "comments": "Some minor corrections in the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AFP Algorithm is a learning algorithm for Horn formulas. We show that it does\nnot improve the complexity of AFP Algorithm, if after each negative\ncounterexample more that just one refinements are performed. Moreover, a\ncanonical normal form for Horn formulas is presented, and it is proved that the\noutput formula of AFP Algorithm is in this normal form.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 14:29:11 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 12:50:11 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Majdoddin", "Ruhollah", ""]]}, {"id": "1408.4714", "submitter": "Cong Li", "authors": "Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos", "title": "Conic Multi-Task Classification", "comments": "Accepted by European Conference on Machine Learning and Principles\n  and Practice of Knowledge Discovery in Databases (ECMLPKDD)-2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, Multi-task Learning (MTL) models optimize the average of\ntask-related objective functions, which is an intuitive approach and which we\nwill be referring to as Average MTL. However, a more general framework,\nreferred to as Conic MTL, can be formulated by considering conic combinations\nof the objective functions instead; in this framework, Average MTL arises as a\nspecial case, when all combination coefficients equal 1. Although the advantage\nof Conic MTL over Average MTL has been shown experimentally in previous works,\nno theoretical justification has been provided to date. In this paper, we\nderive a generalization bound for the Conic MTL method, and demonstrate that\nthe tightest bound is not necessarily achieved, when all combination\ncoefficients equal 1; hence, Average MTL may not always be the optimal choice,\nand it is important to consider Conic MTL. As a byproduct of the generalization\nbound, it also theoretically explains the good experimental results of previous\nrelevant works. Finally, we propose a new Conic MTL model, whose conic\ncombination coefficients minimize the generalization bound, instead of choosing\nthem heuristically as has been done in previous methods. The rationale and\nadvantage of our model is demonstrated and verified via a series of experiments\nby comparing with several other methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 16:23:53 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Li", "Cong", ""], ["Georgiopoulos", "Michael", ""], ["Anagnostopoulos", "Georgios C.", ""]]}, {"id": "1408.4966", "submitter": "Jimmy Dubuisson", "authors": "Jimmy Dubuisson, Jean-Pierre Eckmann and Andrea Agazzi", "title": "Diffusion Fingerprints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce, test and discuss a method for classifying and clustering data\nmodeled as directed graphs. The idea is to start diffusion processes from any\nsubset of a data collection, generating corresponding distributions for\nreaching points in the network. These distributions take the form of\nhigh-dimensional numerical vectors and capture essential topological properties\nof the original dataset. We show how these diffusion vectors can be\nsuccessfully applied for getting state-of-the-art accuracies in the problem of\nextracting pathways from metabolic networks. We also provide a guideline to\nillustrate how to use our method for classification problems, and discuss\nimportant details of its implementation. In particular, we present a simple\ndimensionality reduction technique that lowers the computational cost of\nclassifying diffusion vectors, while leaving the predictive power of the\nclassification process substantially unaltered. Although the method has very\nfew parameters, the results we obtain show its flexibility and power. This\nshould make it helpful in many other contexts.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 11:34:37 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 13:48:40 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Dubuisson", "Jimmy", ""], ["Eckmann", "Jean-Pierre", ""], ["Agazzi", "Andrea", ""]]}, {"id": "1408.5093", "submitter": "Yangqing Jia", "authors": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\n  Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell", "title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "comments": "Tech report for the Caffe software at http://github.com/BVLC/Caffe/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caffe provides multimedia scientists and practitioners with a clean and\nmodifiable framework for state-of-the-art deep learning algorithms and a\ncollection of reference models. The framework is a BSD-licensed C++ library\nwith Python and MATLAB bindings for training and deploying general-purpose\nconvolutional neural networks and other deep models efficiently on commodity\narchitectures. Caffe fits industry and internet-scale media needs by CUDA GPU\ncomputation, processing over 40 million images a day on a single K40 or Titan\nGPU ($\\approx$ 2.5 ms per image). By separating model representation from\nactual implementation, Caffe allows experimentation and seamless switching\namong platforms for ease of development and deployment from prototyping\nmachines to cloud environments. Caffe is maintained and developed by the\nBerkeley Vision and Learning Center (BVLC) with the help of an active community\nof contributors on GitHub. It powers ongoing research projects, large-scale\nindustrial applications, and startup prototypes in vision, speech, and\nmultimedia.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 23:00:32 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Jia", "Yangqing", ""], ["Shelhamer", "Evan", ""], ["Donahue", "Jeff", ""], ["Karayev", "Sergey", ""], ["Long", "Jonathan", ""], ["Girshick", "Ross", ""], ["Guadarrama", "Sergio", ""], ["Darrell", "Trevor", ""]]}, {"id": "1408.5099", "submitter": "Cameron Musco", "authors": "Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco,\n  Richard Peng, Aaron Sidford", "title": "Uniform Sampling for Matrix Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sampling has become a critical tool in solving massive matrix\nproblems. For linear regression, a small, manageable set of data rows can be\nrandomly selected to approximate a tall, skinny data matrix, improving\nprocessing time significantly. For theoretical performance guarantees, each row\nmust be sampled with probability proportional to its statistical leverage\nscore. Unfortunately, leverage scores are difficult to compute.\n  A simple alternative is to sample rows uniformly at random. While this often\nworks, uniform sampling will eliminate critical row information for many\nnatural instances. We take a fresh look at uniform sampling by examining what\ninformation it does preserve. Specifically, we show that uniform sampling\nyields a matrix that, in some sense, well approximates a large fraction of the\noriginal. While this weak form of approximation is not enough for solving\nlinear regression directly, it is enough to compute a better approximation.\n  This observation leads to simple iterative row sampling algorithms for matrix\napproximation that run in input-sparsity time and preserve row structure and\nsparsity at all intermediate steps. In addition to an improved understanding of\nuniform sampling, our main proof introduces a structural result of independent\ninterest: we show that every matrix can be made to have low coherence by\nreweighting a small subset of its rows.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 18:32:00 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Cohen", "Michael B.", ""], ["Lee", "Yin Tat", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Peng", "Richard", ""], ["Sidford", "Aaron", ""]]}, {"id": "1408.5241", "submitter": "Nguyen Duc-Hien", "authors": "Duc-Hien Nguyen, Manh-Thanh Le", "title": "A two-stage architecture for stock price forecasting by combining SOM\n  and fuzzy-SVM", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a model to predict the stock price based on combining\nSelf-Organizing Map (SOM) and fuzzy-Support Vector Machines (f-SVM). Extraction\nof fuzzy rules from raw data based on the combining of statistical machine\nlearning models is base of this proposed approach. In the proposed model, SOM\nis used as a clustering algorithm to partition the whole input space into the\nseveral disjoint regions. For each partition, a set of fuzzy rules is extracted\nbased on a f-SVM combining model. Then fuzzy rules sets are used to predict the\ntest data using fuzzy inference algorithms. The performance of the proposed\napproach is compared with other models using four data sets\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 09:33:50 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Nguyen", "Duc-Hien", ""], ["Le", "Manh-Thanh", ""]]}, {"id": "1408.5246", "submitter": "Nguyen Duc-Hien", "authors": "Duc-Hien Nguyen, Manh-Thanh Le", "title": "Improving the Interpretability of Support Vector Machines-based Fuzzy\n  Rules", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) and fuzzy rule systems are functionally\nequivalent under some conditions. Therefore, the learning algorithms developed\nin the field of support vector machines can be used to adapt the parameters of\nfuzzy systems. Extracting fuzzy models from support vector machines has the\ninherent advantage that the model does not need to determine the number of\nrules in advance. However, after the support vector machine learning, the\ncomplexity is usually high, and interpretability is also impaired. This paper\nnot only proposes a complete framework for extracting interpretable SVM-based\nfuzzy modeling, but also provides optimization issues of the models.\nSimulations examples are given to embody the idea of this paper.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 09:55:06 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Nguyen", "Duc-Hien", ""], ["Le", "Manh-Thanh", ""]]}, {"id": "1408.5352", "submitter": "Han Liu", "authors": "Zhaoran Wang, Huanran Lu, Han Liu", "title": "Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in\n  Polynomial Time", "comments": "64 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (PCA) involves nonconvex optimization for\nwhich the global solution is hard to obtain. To address this issue, one popular\napproach is convex relaxation. However, such an approach may produce suboptimal\nestimators due to the relaxation effect. To optimally estimate sparse principal\nsubspaces, we propose a two-stage computational framework named \"tighten after\nrelax\": Within the 'relax' stage, we approximately solve a convex relaxation of\nsparse PCA with early stopping to obtain a desired initial estimator; For the\n'tighten' stage, we propose a novel algorithm called sparse orthogonal\niteration pursuit (SOAP), which iteratively refines the initial estimator by\ndirectly solving the underlying nonconvex problem. A key concept of this\ntwo-stage framework is the basin of attraction. It represents a local region\nwithin which the `tighten' stage has desired computational and statistical\nguarantees. We prove that, the initial estimator obtained from the 'relax'\nstage falls into such a region, and hence SOAP geometrically converges to a\nprincipal subspace estimator which is minimax-optimal within a certain model\nclass. Unlike most existing sparse PCA estimators, our approach applies to the\nnon-spiked covariance models, and adapts to non-Gaussianity as well as\ndependent data settings. Moreover, through analyzing the computational\ncomplexity of the two stages, we illustrate an interesting phenomenon that\nlarger sample size can reduce the total iteration complexity. Our framework\nmotivates a general paradigm for solving many complex statistical problems\nwhich involve nonconvex optimization with provable guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 16:33:09 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Wang", "Zhaoran", ""], ["Lu", "Huanran", ""], ["Liu", "Han", ""]]}, {"id": "1408.5389", "submitter": "Zhensong Qian", "authors": "Zhensong Qian, Oliver Schulte and Yan Sun", "title": "Computing Multi-Relational Sufficient Statistics for Large Databases", "comments": "11pages, 8 figures, 8 tables, CIKM'14,November 3--7, 2014, Shanghai,\n  China", "journal-ref": null, "doi": "10.1145/2661829.2662010", "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases contain information about which relationships do and do not hold\namong entities. To make this information accessible for statistical analysis\nrequires computing sufficient statistics that combine information from\ndifferent database tables. Such statistics may involve any number of {\\em\npositive and negative} relationships. With a naive enumeration approach,\ncomputing sufficient statistics for negative relationships is feasible only for\nsmall databases. We solve this problem with a new dynamic programming algorithm\nthat performs a virtual join, where the requisite counts are computed without\nmaterializing join tables. Contingency table algebra is a new extension of\nrelational algebra, that facilitates the efficient implementation of this\nM\\\"obius virtual join operation. The M\\\"obius Join scales to large datasets\n(over 1M tuples) with complex schemas. Empirical evaluation with seven\nbenchmark datasets showed that information about the presence and absence of\nlinks can be exploited in feature selection, association rule mining, and\nBayesian network learning.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 19:12:19 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Qian", "Zhensong", ""], ["Schulte", "Oliver", ""], ["Sun", "Yan", ""]]}, {"id": "1408.5400", "submitter": "Sebastian Ramos", "authors": "Jiaolong Xu, Sebastian Ramos, David Vazquez, Antonio M. Lopez", "title": "Hierarchical Adaptive Structural SVM for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key topic in classification is the accuracy loss produced when the data\ndistribution in the training (source) domain differs from that in the testing\n(target) domain. This is being recognized as a very relevant problem for many\ncomputer vision tasks such as image classification, object detection, and\nobject category recognition. In this paper, we present a novel domain\nadaptation method that leverages multiple target domains (or sub-domains) in a\nhierarchical adaptation tree. The core idea is to exploit the commonalities and\ndifferences of the jointly considered target domains.\n  Given the relevance of structural SVM (SSVM) classifiers, we apply our idea\nto the adaptive SSVM (A-SSVM), which only requires the target domain samples\ntogether with the existing source-domain classifier for performing the desired\nadaptation. Altogether, we term our proposal as hierarchical A-SSVM (HA-SSVM).\n  As proof of concept we use HA-SSVM for pedestrian detection and object\ncategory recognition. In the former we apply HA-SSVM to the deformable\npart-based model (DPM) while in the latter HA-SSVM is applied to multi-category\nclassifiers. In both cases, we show how HA-SSVM is effective in increasing the\ndetection/recognition accuracy with respect to adaptation strategies that\nignore the structure of the target data. Since, the sub-domains of the target\ndata are not always known a priori, we shown how HA-SSVM can incorporate\nsub-domain structure discovery for object category recognition.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 19:56:14 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Xu", "Jiaolong", ""], ["Ramos", "Sebastian", ""], ["Vazquez", "David", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "1408.5427", "submitter": "Carl Meyer Dr.", "authors": "Daniel Godfrey, Caley Johns, Carl Meyer, Shaina Race, Carol Sadek", "title": "A Case Study in Text Mining: Interpreting Twitter Data From World Cup\n  Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is a field of data analysis that extracts underlying\npatterns in data. One application of cluster analysis is in text-mining, the\nanalysis of large collections of text to find similarities between documents.\nWe used a collection of about 30,000 tweets extracted from Twitter just before\nthe World Cup started. A common problem with real world text data is the\npresence of linguistic noise. In our case it would be extraneous tweets that\nare unrelated to dominant themes. To combat this problem, we created an\nalgorithm that combined the DBSCAN algorithm and a consensus matrix. This way\nwe are left with the tweets that are related to those dominant themes. We then\nused cluster analysis to find those topics that the tweets describe. We\nclustered the tweets using k-means, a commonly used clustering algorithm, and\nNon-Negative Matrix Factorization (NMF) and compared the results. The two\nalgorithms gave similar results, but NMF proved to be faster and provided more\neasily interpreted results. We explored our results using two visualization\ntools, Gephi and Wordle.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 17:58:33 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Godfrey", "Daniel", ""], ["Johns", "Caley", ""], ["Meyer", "Carl", ""], ["Race", "Shaina", ""], ["Sadek", "Carol", ""]]}, {"id": "1408.5449", "submitter": "Kar-Ann Toh", "authors": "Kar-Ann Toh", "title": "Stretchy Polynomial Regression", "comments": "Article created in April and revised in August 2014. Submitted to\n  ICARCV 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel solution for stretchy polynomial regression\nlearning. The solution comes in primal and dual closed-forms similar to that of\nridge regression. Essentially, the proposed solution stretches the covariance\ncomputation via a power term thereby compresses or amplifies the estimation.\nOur experiments on both synthetic data and real-world data show effectiveness\nof the proposed method for compressive learning.\n", "versions": [{"version": "v1", "created": "Sat, 23 Aug 2014 01:23:23 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Toh", "Kar-Ann", ""]]}, {"id": "1408.5456", "submitter": "Houtao Deng", "authors": "Houtao Deng", "title": "Interpreting Tree Ensembles with inTrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree ensembles such as random forests and boosted trees are accurate but\ndifficult to understand, debug and deploy. In this work, we provide the inTrees\n(interpretable trees) framework that extracts, measures, prunes and selects\nrules from a tree ensemble, and calculates frequent variable interactions. An\nrule-based learner, referred to as the simplified tree ensemble learner (STEL),\ncan also be formed and used for future prediction. The inTrees framework can\napplied to both classification and regression problems, and is applicable to\nmany types of tree ensembles, e.g., random forests, regularized random forests,\nand boosted trees. We implemented the inTrees algorithms in the \"inTrees\" R\npackage.\n", "versions": [{"version": "v1", "created": "Sat, 23 Aug 2014 05:06:55 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Deng", "Houtao", ""]]}, {"id": "1408.5544", "submitter": "Daniel Pimentel", "authors": "Daniel L. Pimentel-Alarc\\'on", "title": "To lie or not to lie in a subspace", "comments": "First author mistakenly listed advisors as co-authors in his research\n  proposal. This is corrected in the current version. 59 pages, 19 figures.\n  Subspace clustering, missing data, converse of matrix completion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Give deterministic necessary and sufficient conditions to guarantee that if a\nsubspace fits certain partially observed data from a union of subspaces, it is\nbecause such data really lies in a subspace.\n  Furthermore, Give deterministic necessary and sufficient conditions to\nguarantee that if a subspace fits certain partially observed data, such\nsubspace is unique.\n  Do this by characterizing when and only when a set of incomplete vectors\nbehaves as a single but complete one.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 02:53:57 GMT"}, {"version": "v2", "created": "Wed, 27 Aug 2014 15:21:09 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Pimentel-Alarc\u00f3n", "Daniel L.", ""]]}, {"id": "1408.5574", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Anton van den Hengel", "title": "Supervised Hashing Using Graph Cuts and Boosted Decision Trees", "comments": "15 pages. Appearing in IEEE T. Pattern Analysis & Machine\n  Intelligence. arXiv admin note: text overlap with arXiv:1404.1561", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2404776", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding image features into a binary Hamming space can improve both the\nspeed and accuracy of large-scale query-by-example image retrieval systems.\nSupervised hashing aims to map the original features to compact binary codes in\na manner which preserves the label-based similarities of the original data.\nMost existing approaches apply a single form of hash function, and an\noptimization process which is typically deeply coupled to this specific form.\nThis tight coupling restricts the flexibility of those methods, and can result\nin complex optimization problems that are difficult to solve. In this work we\nproffer a flexible yet simple framework that is able to accommodate different\ntypes of loss functions and hash functions. The proposed framework allows a\nnumber of existing approaches to hashing to be placed in context, and\nsimplifies the development of new problem-specific hashing methods. Our\nframework decomposes the into two steps: binary code (hash bits) learning, and\nhash function learning. The first step can typically be formulated as a binary\nquadratic problem, and the second step can be accomplished by training standard\nbinary classifiers. For solving large-scale binary code inference, we show how\nto ensure that the binary quadratic problems are submodular such that an\nefficient graph cut approach can be used. To achieve efficiency as well as\nefficacy on large-scale high-dimensional data, we propose to use boosted\ndecision trees as the hash functions, which are nonlinear, highly descriptive,\nand very fast to train and evaluate. Experiments demonstrate that our proposed\nmethod significantly outperforms most state-of-the-art methods, especially on\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 07:40:19 GMT"}, {"version": "v2", "created": "Sun, 8 Feb 2015 23:52:38 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1408.5634", "submitter": "R. Sean Bowman", "authors": "R. Sean Bowman and Douglas Heisterkamp and Jesse Johnson and Danielle\n  O'Donnol", "title": "An application of topological graph clustering to protein function\n  prediction", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a semisupervised learning algorithm based on a topological data\nanalysis approach to assign functional categories to yeast proteins using\nsimilarity graphs. This new approach to analyzing biological networks yields\nresults that are as good as or better than state of the art existing\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 20:41:51 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Bowman", "R. Sean", ""], ["Heisterkamp", "Douglas", ""], ["Johnson", "Jesse", ""], ["O'Donnol", "Danielle", ""]]}, {"id": "1408.5661", "submitter": "Keisuke Yamazaki", "authors": "Keisuke Yamazaki", "title": "Asymptotic Accuracy of Bayesian Estimation for a Single Latent Variable", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data science and machine learning, hierarchical parametric models, such as\nmixture models, are often used. They contain two kinds of variables: observable\nvariables, which represent the parts of the data that can be directly measured,\nand latent variables, which represent the underlying processes that generate\nthe data. Although there has been an increase in research on the estimation\naccuracy for observable variables, the theoretical analysis of estimating\nlatent variables has not been thoroughly investigated. In a previous study, we\ndetermined the accuracy of a Bayes estimation for the joint probability of the\nlatent variables in a dataset, and we proved that the Bayes method is\nasymptotically more accurate than the maximum-likelihood method. However, the\naccuracy of the Bayes estimation for a single latent variable remains unknown.\nIn the present paper, we derive the asymptotic expansions of the error\nfunctions, which are defined by the Kullback-Leibler divergence, for two types\nof single-variable estimations when the statistical regularity is satisfied.\nOur results indicate that the accuracies of the Bayes and maximum-likelihood\nmethods are asymptotically equivalent and clarify that the Bayes method is only\nadvantageous for multivariable estimations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 04:44:53 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 04:00:06 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 06:59:26 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Yamazaki", "Keisuke", ""]]}, {"id": "1408.5823", "submitter": "Yingyu Liang", "authors": "Maria-Florina Balcan, Vandana Kanchanapally, Yingyu Liang, David\n  Woodruff", "title": "Improved Distributed Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distributed computing setting in which there are multiple\nservers, each holding a set of points, who wish to compute functions on the\nunion of their point sets. A key task in this setting is Principal Component\nAnalysis (PCA), in which the servers would like to compute a low dimensional\nsubspace capturing as much of the variance of the union of their point sets as\npossible. Given a procedure for approximate PCA, one can use it to\napproximately solve $\\ell_2$-error fitting problems such as $k$-means\nclustering and subspace clustering. The essential properties of an approximate\ndistributed PCA algorithm are its communication cost and computational\nefficiency for a given desired accuracy in downstream applications. We give new\nalgorithms and analyses for distributed PCA which lead to improved\ncommunication and computational costs for $k$-means clustering and related\nproblems. Our empirical study on real world data shows a speedup of orders of\nmagnitude, preserving communication with only a negligible degradation in\nsolution quality. Some of these techniques we develop, such as a general\ntransformation from a constant success probability subspace embedding to a high\nsuccess probability subspace embedding with a dimension and sparsity\nindependent of the success probability, may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 16:24:43 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 01:32:30 GMT"}, {"version": "v3", "created": "Thu, 13 Nov 2014 14:51:43 GMT"}, {"version": "v4", "created": "Sun, 21 Dec 2014 01:13:00 GMT"}, {"version": "v5", "created": "Tue, 23 Dec 2014 03:17:19 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Kanchanapally", "Vandana", ""], ["Liang", "Yingyu", ""], ["Woodruff", "David", ""]]}, {"id": "1408.5845", "submitter": "Reza Arablouei", "authors": "Reza Arablouei, Stefan Werner, Kutluy{\\i}l Do\\u{g}an\\c{c}ay, and\n  Yih-Fang Huang", "title": "Analysis of a Reduced-Communication Diffusion LMS Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In diffusion-based algorithms for adaptive distributed estimation, each node\nof an adaptive network estimates a target parameter vector by creating an\nintermediate estimate and then combining the intermediate estimates available\nwithin its closed neighborhood. We analyze the performance of a\nreduced-communication diffusion least mean-square (RC-DLMS) algorithm, which\nallows each node to receive the intermediate estimates of only a subset of its\nneighbors at each iteration. This algorithm eases the usage of network\ncommunication resources and delivers a trade-off between estimation performance\nand communication cost. We show analytically that the RC-DLMS algorithm is\nstable and convergent in both mean and mean-square senses. We also calculate\nits theoretical steady-state mean-square deviation. Simulation results\ndemonstrate a good match between theory and experiment.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 17:42:41 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 00:40:57 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Arablouei", "Reza", ""], ["Werner", "Stefan", ""], ["Do\u011fan\u00e7ay", "Kutluy\u0131l", ""], ["Huang", "Yih-Fang", ""]]}, {"id": "1408.6027", "submitter": "Xin Geng", "authors": "Xin Geng", "title": "Label Distribution Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although multi-label learning can deal with many problems with label\nambiguity, it does not fit some real applications well where the overall\ndistribution of the importance of the labels matters. This paper proposes a\nnovel learning paradigm named \\emph{label distribution learning} (LDL) for such\nkind of applications. The label distribution covers a certain number of labels,\nrepresenting the degree to which each label describes the instance. LDL is a\nmore general learning framework which includes both single-label and\nmulti-label learning as its special cases. This paper proposes six working LDL\nalgorithms in three ways: problem transformation, algorithm adaptation, and\nspecialized algorithm design. In order to compare the performance of the LDL\nalgorithms, six representative and diverse evaluation measures are selected via\na clustering analysis, and the first batch of label distribution datasets are\ncollected and made publicly available. Experimental results on one artificial\nand fifteen real-world datasets show clear advantages of the specialized\nalgorithms, which indicates the importance of special design for the\ncharacteristics of the LDL problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 06:48:58 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 09:47:09 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Geng", "Xin", ""]]}, {"id": "1408.6032", "submitter": "Daniele Ramazzotti", "authors": "Fabrizio Angaroni, Kevin Chen, Chiara Damiani, Giulio Caravagna, Alex\n  Graudenzi, Daniele Ramazzotti", "title": "PMCE: efficient inference of expressive models of cancer evolution with\n  high prognostic power", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Driver (epi)genomic alterations underlie the positive selection\nof cancer subpopulations, which promotes drug resistance and relapse. Even\nthough substantial heterogeneity is witnessed in most cancer types, mutation\naccumulation patterns can be regularly found and can be exploited to\nreconstruct predictive models of cancer evolution. Yet, available methods\ncannot infer logical formulas connecting events to represent alternative\nevolutionary routes or convergent evolution. Results: We introduce PMCE, an\nexpressive framework that leverages mutational profiles from cross-sectional\nsequencing data to infer probabilistic graphical models of cancer evolution\nincluding arbitrary logical formulas, and which outperforms the\nstate-of-the-art in terms of accuracy and robustness to noise, on simulations.\nThe application of PMCE to 7866 samples from the TCGA database allows us to\nidentify a highly significant correlation between the predicted evolutionary\npaths and the overall survival in 7 tumor types, proving that our approach can\neffectively stratify cancer patients in reliable risk groups. Availability:\nPMCE is freely available at https://github.com/danro9685/HESBCN. The code to\nreplicate all the analyses is available at:\nhttps://github.com/BIMIB-DISCo/PMCE. Contacts: daniele.ramazzotti@unimib.it,\nalex.graudenzi@ibfm.cnr.it\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 07:04:25 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 15:03:28 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Angaroni", "Fabrizio", ""], ["Chen", "Kevin", ""], ["Damiani", "Chiara", ""], ["Caravagna", "Giulio", ""], ["Graudenzi", "Alex", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1408.6141", "submitter": "Reza Arablouei", "authors": "Reza Arablouei, Kutluy{\\i}l Do\\u{g}an\\c{c}ay, and Stefan Werner", "title": "Recursive Total Least-Squares Algorithm Based on Inverse Power Method\n  and Dichotomous Coordinate-Descent Iterations", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2405492", "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a recursive total least-squares (RTLS) algorithm for\nerrors-in-variables system identification utilizing the inverse power method\nand the dichotomous coordinate-descent (DCD) iterations. The proposed\nalgorithm, called DCD-RTLS, outperforms the previously-proposed RTLS\nalgorithms, which are based on the line-search method, with reduced\ncomputational complexity. We perform a comprehensive analysis of the DCD-RTLS\nalgorithm and show that it is asymptotically unbiased as well as being stable\nin the mean. We also find a lower bound for the forgetting factor that ensures\nmean-square stability of the algorithm and calculate the theoretical\nsteady-state mean-square deviation (MSD). We verify the effectiveness of the\nproposed algorithm and the accuracy of the predicted steady-state MSD via\nsimulations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 17:40:44 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 00:44:23 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Arablouei", "Reza", ""], ["Do\u011fan\u00e7ay", "Kutluy\u0131l", ""], ["Werner", "Stefan", ""]]}, {"id": "1408.6214", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "A Methodology for the Diagnostic of Aircraft Engine Based on Indicators\n  Aggregation", "comments": "Proceedings of the 14th Industrial Conference, ICDM 2014, St.\n  Petersburg : Russian Federation (2014)", "journal-ref": null, "doi": "10.1007/978-3-319-08976-8_11", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aircraft engine manufacturers collect large amount of engine related data\nduring flights. These data are used to detect anomalies in the engines in order\nto help companies optimize their maintenance costs. This article introduces and\nstudies a generic methodology that allows one to build automatic early signs of\nanomaly detection in a way that is understandable by human operators who make\nthe final maintenance decision. The main idea of the method is to generate a\nvery large number of binary indicators based on parametric anomaly scores\ndesigned by experts, complemented by simple aggregations of those scores. The\nbest indicators are selected via a classical forward scheme, leading to a much\nreduced number of indicators that are tuned to a data set. We illustrate the\ninterest of the method on simulated data which contain realistic early signs of\nanomalies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 19:15:21 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1408.6515", "submitter": "Yuyu Zhang", "authors": "Yuyu Zhang, Liang Pang, Lei Shi and Bin Wang", "title": "Large Scale Purchase Prediction with Historical User Actions on B2C\n  Online Retail Platform", "comments": "Accepted by 2nd Large Scale Recommender Systems Workshop, RecSys 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the solution of Bazinga Team for Tmall Recommendation\nPrize 2014. With real-world user action data provided by Tmall, one of the\nlargest B2C online retail platforms in China, this competition requires to\npredict future user purchases on Tmall website. Predictions are judged on\nF1Score, which considers both precision and recall for fair evaluation. The\ndata set provided by Tmall contains more than half billion action records from\nover ten million distinct users. Such massive data volume poses a big\nchallenge, and drives competitors to write every single program in MapReduce\nfashion and run it on distributed cluster. We model the purchase prediction\nproblem as standard machine learning problem, and mainly employ regression and\nclassification methods as single models. Individual models are then aggregated\nin a two-stage approach, using linear regression for blending, and finally a\nlinear ensemble of blended models. The competition is approaching the end but\nstill in running during writing this paper. In the end, our team achieves\nF1Score 6.11 and ranks 7th (out of 7,276 teams in total).\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 06:32:21 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 16:30:51 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2015 15:11:18 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Zhang", "Yuyu", ""], ["Pang", "Liang", ""], ["Shi", "Lei", ""], ["Wang", "Bin", ""]]}, {"id": "1408.6617", "submitter": "Chao Zhang", "authors": "Chao Zhang, Dacheng Tao, Tao Hu, Xiang Li", "title": "Task-group Relatedness and Generalization Bounds for Regularized\n  Multi-task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the generalization performance of regularized\nmulti-task learning (RMTL) in a vector-valued framework, where MTL is\nconsidered as a learning process for vector-valued functions. We are mainly\nconcerned with two theoretical questions: 1) under what conditions does RMTL\nperform better with a smaller task sample size than STL? 2) under what\nconditions is RMTL generalizable and can guarantee the consistency of each task\nduring simultaneous learning?\n  In particular, we investigate two types of task-group relatedness: the\nobserved discrepancy-dependence measure (ODDM) and the empirical\ndiscrepancy-dependence measure (EDDM), both of which detect the dependence\nbetween two groups of multiple related tasks (MRTs). We then introduce the\nCartesian product-based uniform entropy number (CPUEN) to measure the\ncomplexities of vector-valued function classes. By applying the specific\ndeviation and the symmetrization inequalities to the vector-valued framework,\nwe obtain the generalization bound for RMTL, which is the upper bound of the\njoint probability of the event that there is at least one task with a large\nempirical discrepancy between the expected and empirical risks. Finally, we\npresent a sufficient condition to guarantee the consistency of each task in the\nsimultaneous learning process, and we discuss how task relatedness affects the\ngeneralization performance of RMTL. Our theoretical findings answer the\naforementioned two questions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 03:27:27 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Zhang", "Chao", ""], ["Tao", "Dacheng", ""], ["Hu", "Tao", ""], ["Li", "Xiang", ""]]}, {"id": "1408.6618", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Falsifiable implies Learnable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper demonstrates that falsifiability is fundamental to learning. We\nprove the following theorem for statistical learning and sequential prediction:\nIf a theory is falsifiable then it is learnable -- i.e. admits a strategy that\npredicts optimally. An analogous result is shown for universal induction.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 03:29:06 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1408.6686", "submitter": "Junxiao Song", "authors": "Junxiao Song, Prabhu Babu, and Daniel P. Palomar", "title": "Sparse Generalized Eigenvalue Problem via Smooth Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2394443", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an $\\ell_{0}$-norm penalized formulation of the\ngeneralized eigenvalue problem (GEP), aimed at extracting the leading sparse\ngeneralized eigenvector of a matrix pair. The formulation involves maximization\nof a discontinuous nonconcave objective function over a nonconvex constraint\nset, and is therefore computationally intractable. To tackle the problem, we\nfirst approximate the $\\ell_{0}$-norm by a continuous surrogate function. Then\nan algorithm is developed via iteratively majorizing the surrogate function by\na quadratic separable function, which at each iteration reduces to a regular\ngeneralized eigenvalue problem. A preconditioned steepest ascent algorithm for\nfinding the leading generalized eigenvector is provided. A systematic way based\non smoothing is proposed to deal with the \"singularity issue\" that arises when\na quadratic function is used to majorize the nondifferentiable surrogate\nfunction. For sparse GEPs with special structure, algorithms that admit a\nclosed-form solution at every iteration are derived. Numerical experiments show\nthat the proposed algorithms match or outperform existing algorithms in terms\nof computational complexity and support recovery.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 11:22:08 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 07:07:13 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Song", "Junxiao", ""], ["Babu", "Prabhu", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1408.6746", "submitter": "Slobodan Beliga", "authors": "Slobodan Beliga, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c", "title": "Non-Standard Words as Features for Text Categorization", "comments": "IEEE 37th International Convention on Information and Communication\n  Technology, Electronics and Microelectronics (MIPRO 2014), pp. 1415-1419,\n  2014", "journal-ref": "IEEE 37th International Convention on Information and\n  Communication Technology, Electronics and Microelectronics (MIPRO 2014), pp.\n  1415-1419, 2014", "doi": "10.1109/MIPRO.2014.6859744", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents categorization of Croatian texts using Non-Standard Words\n(NSW) as features. Non-Standard Words are: numbers, dates, acronyms,\nabbreviations, currency, etc. NSWs in Croatian language are determined\naccording to Croatian NSW taxonomy. For the purpose of this research, 390 text\ndocuments were collected and formed the SKIPEZ collection with 6 classes:\nofficial, literary, informative, popular, educational and scientific. Text\ncategorization experiment was conducted on three different representations of\nthe SKIPEZ collection: in the first representation, the frequencies of NSWs are\nused as features; in the second representation, the statistic measures of NSWs\n(variance, coefficient of variation, standard deviation, etc.) are used as\nfeatures; while the third representation combines the first two feature sets.\nNaive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms\nwere used in text categorization experiments. The best categorization results\nare achieved using the first feature set (NSW frequencies) with the\ncategorization accuracy of 87%. This suggests that the NSWs should be\nconsidered as features in highly inflectional languages, such as Croatian. NSW\nbased features reduce the dimensionality of the feature space without standard\nlemmatization procedures, and therefore the bag-of-NSWs should be considered\nfor further Croatian texts categorization experiments.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 15:06:50 GMT"}, {"version": "v2", "created": "Sun, 16 Nov 2014 21:33:22 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Beliga", "Slobodan", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""]]}, {"id": "1408.6804", "submitter": "Vladimir Kolmogorov", "authors": "Neel Shah, Vladimir Kolmogorov, Christoph H. Lampert", "title": "A Multi-Plane Block-Coordinate Frank-Wolfe Algorithm for Training\n  Structural SVMs with a Costly max-Oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural support vector machines (SSVMs) are amongst the best performing\nmodels for structured computer vision tasks, such as semantic image\nsegmentation or human pose estimation. Training SSVMs, however, is\ncomputationally costly, because it requires repeated calls to a structured\nprediction subroutine (called \\emph{max-oracle}), which has to solve an\noptimization problem itself, e.g. a graph cut.\n  In this work, we introduce a new algorithm for SSVM training that is more\nefficient than earlier techniques when the max-oracle is computationally\nexpensive, as it is frequently the case in computer vision tasks. The main idea\nis to (i) combine the recent stochastic Block-Coordinate Frank-Wolfe algorithm\nwith efficient hyperplane caching, and (ii) use an automatic selection rule for\ndeciding whether to call the exact max-oracle or to rely on an approximate one\nbased on the cached hyperplanes.\n  We show experimentally that this strategy leads to faster convergence to the\noptimum with respect to the number of requires oracle calls, and that this\ntranslates into faster convergence with respect to the total runtime when the\nmax-oracle is slow compared to the other steps of the algorithm.\n  A publicly available C++ implementation is provided at\nhttp://pub.ist.ac.at/~vnk/papers/SVM.html .\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 18:38:24 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 14:43:43 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Shah", "Neel", ""], ["Kolmogorov", "Vladimir", ""], ["Lampert", "Christoph H.", ""]]}]