[{"id": "1002.0383", "submitter": "Dakshina Ranjan Kisku", "authors": "Hunny Mehrotra, Dakshina Ranjan Kisku, V. Bhawani Radhika, Banshidhar\n  Majhi, Phalguni Gupta", "title": "Feature Level Clustering of Large Biometric Database", "comments": "4 pages, 2 figures, IAPR International Conference on Machine Vision\n  Applications, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient technique for partitioning large biometric\ndatabase during identification. In this technique feature vector which\ncomprises of global and local descriptors extracted from offline signature are\nused by fuzzy clustering technique to partition the database. As biometric\nfeatures posses no natural order of sorting, thus it is difficult to index them\nalphabetically or numerically. Hence, some supervised criteria is required to\npartition the search space. At the time of identification the fuzziness\ncriterion is introduced to find the nearest clusters for declaring the identity\nof query sample. The system is tested using bin-miss rate and performs better\nin comparison to traditional k-means approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 02:30:22 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Mehrotra", "Hunny", ""], ["Kisku", "Dakshina Ranjan", ""], ["Radhika", "V. Bhawani", ""], ["Majhi", "Banshidhar", ""], ["Gupta", "Phalguni", ""]]}, {"id": "1002.0416", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Phalguni Gupta, Jamuna Kanta Sing", "title": "Fusion of Multiple Matchers using SVM for Offline Signature\n  Identification", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper uses Support Vector Machines (SVM) to fuse multiple classifiers\nfor an offline signature system. From the signature images, global and local\nfeatures are extracted and the signatures are verified with the help of\nGaussian empirical rule, Euclidean and Mahalanobis distance based classifiers.\nSVM is used to fuse matching scores of these matchers. Finally, recognition of\nquery signatures is done by comparing it with all signatures of the database.\nThe proposed system is tested on a signature database contains 5400 offline\nsignatures of 600 individuals and the results are found to be promising.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 08:15:20 GMT"}], "update_date": "2010-02-04", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Gupta", "Phalguni", ""], ["Sing", "Jamuna Kanta", ""]]}, {"id": "1002.0709", "submitter": "Fedor Zhdanov", "authors": "Fedor Zhdanov, Alexey Chernov and Yuri Kalnishkan", "title": "Aggregating Algorithm competing with Banach lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with on-line regression settings with signals belonging to a\nBanach lattice. Our algorithms work in a semi-online setting where all the\ninputs are known in advance and outcomes are unknown and given step by step. We\napply the Aggregating Algorithm to construct a prediction method whose\ncumulative loss over all the input vectors is comparable with the cumulative\nloss of any linear functional on the Banach lattice. As a by-product we get an\nalgorithm that takes signals from an arbitrary domain. Its cumulative loss is\ncomparable with the cumulative loss of any predictor function from Besov and\nTriebel-Lizorkin spaces. We describe several applications of our setting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2010 11:31:24 GMT"}], "update_date": "2010-02-04", "authors_parsed": [["Zhdanov", "Fedor", ""], ["Chernov", "Alexey", ""], ["Kalnishkan", "Yuri", ""]]}, {"id": "1002.0747", "submitter": "Omer Tamuz", "authors": "Elchanan Mossel and Noah Olsman and Omer Tamuz", "title": "Efficient Bayesian Learning in Social Networks with Gaussian Estimators", "comments": "Added coauthor. Added proofs for fast convergence on trees and\n  distance transitive graphs. Also, now analyzing a notion of privacy", "journal-ref": null, "doi": "10.1109/ALLERTON.2016.7852262", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a group of Bayesian agents who try to estimate a state of the\nworld $\\theta$ through interaction on a social network. Each agent $v$\ninitially receives a private measurement of $\\theta$: a number $S_v$ picked\nfrom a Gaussian distribution with mean $\\theta$ and standard deviation one.\nThen, in each discrete time iteration, each reveals its estimate of $\\theta$ to\nits neighbors, and, observing its neighbors' actions, updates its belief using\nBayes' Law.\n  This process aggregates information efficiently, in the sense that all the\nagents converge to the belief that they would have, had they access to all the\nprivate measurements. We show that this process is computationally efficient,\nso that each agent's calculation can be easily carried out. We also show that\non any graph the process converges after at most $2N \\cdot D$ steps, where $N$\nis the number of agents and $D$ is the diameter of the network. Finally, we\nshow that on trees and on distance transitive-graphs the process converges\nafter $D$ steps, and that it preserves privacy, so that agents learn very\nlittle about the private signal of most other agents, despite the efficient\naggregation of information. Our results extend those in an unpublished\nmanuscript of the first and last authors.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2010 14:11:06 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2010 14:53:07 GMT"}, {"version": "v3", "created": "Sun, 26 Jun 2016 01:23:28 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Mossel", "Elchanan", ""], ["Olsman", "Noah", ""], ["Tamuz", "Omer", ""]]}, {"id": "1002.0757", "submitter": "Wojciech Kot{\\l}owski", "authors": "Peter Gr\\\"unwald, Wojciech Kot{\\l}owski", "title": "Prequential Plug-In Codes that Achieve Optimal Redundancy Rates even if\n  the Model is Wrong", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the prequential plug-in codes relative to one-parameter\nexponential families M. We show that if data are sampled i.i.d. from some\ndistribution outside M, then the redundancy of any plug-in prequential code\ngrows at rate larger than 1/2 ln(n) in the worst case. This means that plug-in\ncodes, such as the Rissanen-Dawid ML code, may behave inferior to other\nimportant universal codes such as the 2-part MDL, Shtarkov and Bayes codes, for\nwhich the redundancy is always 1/2 ln(n) + O(1). However, we also show that a\nslight modification of the ML plug-in code, \"almost\" in the model, does achieve\nthe optimal redundancy even if the the true distribution is outside M.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2010 15:11:21 GMT"}], "update_date": "2010-02-04", "authors_parsed": [["Gr\u00fcnwald", "Peter", ""], ["Kot\u0142owski", "Wojciech", ""]]}, {"id": "1002.1144", "submitter": "Vishal Goyal", "authors": "M. Ramaswami, R. Bhaskaran", "title": "A CHAID Based Performance Prediction Model in Educational Data Mining", "comments": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010,\n  http://ijcsi.org/articles/A-CHAID-Based-Performance-Prediction-Model-in-Educational-Data-Mining.php", "journal-ref": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010,\n  http://ijcsi.org/articles/A-CHAID-Based-Performance-Prediction-Model-in-Educational-Data-Mining.php", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance in higher secondary school education in India is a turning\npoint in the academic lives of all students. As this academic performance is\ninfluenced by many factors, it is essential to develop predictive data mining\nmodel for students' performance so as to identify the slow learners and study\nthe influence of the dominant factors on their academic performance. In the\npresent investigation, a survey cum experimental methodology was adopted to\ngenerate a database and it was constructed from a primary and a secondary\nsource. While the primary data was collected from the regular students, the\nsecondary data was gathered from the school and office of the Chief Educational\nOfficer (CEO). A total of 1000 datasets of the year 2006 from five different\nschools in three different districts of Tamilnadu were collected. The raw data\nwas preprocessed in terms of filling up missing values, transforming values in\none form into another and relevant attribute/ variable selection. As a result,\nwe had 772 student records, which were used for CHAID prediction model\nconstruction. A set of prediction rules were extracted from CHIAD prediction\nmodel and the efficiency of the generated CHIAD prediction model was found. The\naccuracy of the present model was compared with other model and it has been\nfound to be satisfactory.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 08:27:17 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Ramaswami", "M.", ""], ["Bhaskaran", "R.", ""]]}, {"id": "1002.1156", "submitter": "Vishal Goyal", "authors": "M. Babu Reddy, L. S. S. Reddy", "title": "Dimensionality Reduction: An Empirical Study on the Usability of IFE-CF\n  (Independent Feature Elimination- by C-Correlation and F-Correlation)\n  Measures", "comments": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010, http://ijcsi.org", "journal-ref": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010,\n  http://ijcsi.org/articles/Dimensionality-Reduction-An-Empirical-Study-on-the-Usability-of-IFE-CF-(Independent-Feature-Elimination-by-C-Correlation-and-F-Correlation)-Measures.php", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent increase in dimensionality of data has thrown a great challenge to\nthe existing dimensionality reduction methods in terms of their effectiveness.\nDimensionality reduction has emerged as one of the significant preprocessing\nsteps in machine learning applications and has been effective in removing\ninappropriate data, increasing learning accuracy, and improving\ncomprehensibility. Feature redundancy exercises great influence on the\nperformance of classification process. Towards the better classification\nperformance, this paper addresses the usefulness of truncating the highly\ncorrelated and redundant attributes. Here, an effort has been made to verify\nthe utility of dimensionality reduction by applying LVQ (Learning Vector\nQuantization) method on two Benchmark datasets of 'Pima Indian Diabetic\npatients' and 'Lung cancer patients'.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 08:59:05 GMT"}], "update_date": "2010-02-10", "authors_parsed": [["Reddy", "M. Babu", ""], ["Reddy", "L. S. S.", ""]]}, {"id": "1002.1480", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel A. Braun", "title": "A Minimum Relative Entropy Controller for Undiscounted Markov Decision\n  Processes", "comments": "8 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive control problems are notoriously difficult to solve even in the\npresence of plant-specific controllers. One way to by-pass the intractable\ncomputation of the optimal policy is to restate the adaptive control as the\nminimization of the relative entropy of a controller that ignores the true\nplant dynamics from an informed controller. The solution is given by the\nBayesian control rule-a set of equations characterizing a stochastic adaptive\ncontroller for the class of possible plant dynamics. Here, the Bayesian control\nrule is applied to derive BCR-MDP, a controller to solve undiscounted Markov\ndecision processes with finite state and action spaces and unknown dynamics. In\nparticular, we derive a non-parametric conjugate prior distribution over the\npolicy space that encapsulates the agent's whole relevant history and we\npresent a Gibbs sampler to draw random policies from this distribution.\nPreliminary results show that BCR-MDP successfully avoids sub-optimal limit\ncycles due to its built-in mechanism to balance exploration versus\nexploitation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2010 19:58:46 GMT"}], "update_date": "2010-02-09", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1002.1782", "submitter": "Matthew Faulkner", "authors": "Daniel Golovin, Matthew Faulkner and Andreas Krause", "title": "Online Distributed Sensor Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in sensor networks is to decide which sensors to query when, in\norder to obtain the most useful information (e.g., for performing accurate\nprediction), subject to constraints (e.g., on power and bandwidth). In many\napplications the utility function is not known a priori, must be learned from\ndata, and can even change over time. Furthermore for large sensor networks\nsolving a centralized optimization problem to select sensors is not feasible,\nand thus we seek a fully distributed solution. In this paper, we present\nDistributed Online Greedy (DOG), an efficient, distributed algorithm for\nrepeatedly selecting sensors online, only receiving feedback about the utility\nof the selected sensors. We prove very strong theoretical no-regret guarantees\nthat apply whenever the (unknown) utility function satisfies a natural\ndiminishing returns property called submodularity. Our algorithm has extremely\nlow communication requirements, and scales well to large sensor deployments. We\nextend DOG to allow observation-dependent sensor selection. We empirically\ndemonstrate the effectiveness of our algorithm on several real-world sensing\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2010 07:32:59 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2010 02:34:38 GMT"}, {"version": "v3", "created": "Thu, 13 May 2010 03:32:05 GMT"}], "update_date": "2010-05-14", "authors_parsed": [["Golovin", "Daniel", ""], ["Faulkner", "Matthew", ""], ["Krause", "Andreas", ""]]}, {"id": "1002.2044", "submitter": "Benjamin Rubinstein", "authors": "Benjamin I. P. Rubinstein, Aleksandr Simma", "title": "On the Stability of Empirical Risk Minimization in the Presence of\n  Multiple Risk Minimizers", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Kutin and Niyogi investigated several notions of algorithmic\nstability--a property of a learning map conceptually similar to\ncontinuity--showing that training-stability is sufficient for consistency of\nEmpirical Risk Minimization while distribution-free CV-stability is necessary\nand sufficient for having finite VC-dimension. This paper concerns a phase\ntransition in the training stability of ERM, conjectured by the same authors.\nKutin and Niyogi proved that ERM on finite hypothesis spaces containing a\nunique risk minimizer has training stability that scales exponentially with\nsample size, and conjectured that the existence of multiple risk minimizers\nprevents even super-quadratic convergence. We prove this result for the\nstrictly weaker notion of CV-stability, positively resolving the conjecture.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 09:08:56 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Rubinstein", "Benjamin I. P.", ""], ["Simma", "Aleksandr", ""]]}, {"id": "1002.2050", "submitter": "Bo Zhang", "authors": "Mingyu Fan, Nannan Gu, Hong Qiao, Bo Zhang", "title": "Intrinsic dimension estimation of data by principal component analysis", "comments": "8 pages, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating intrinsic dimensionality of data is a classic problem in pattern\nrecognition and statistics. Principal Component Analysis (PCA) is a powerful\ntool in discovering dimensionality of data sets with a linear structure; it,\nhowever, becomes ineffective when data have a nonlinear structure. In this\npaper, we propose a new PCA-based method to estimate intrinsic dimension of\ndata with nonlinear structures. Our method works by first finding a minimal\ncover of the data set, then performing PCA locally on each subset in the cover\nand finally giving the estimation result by checking up the data variance on\nall small neighborhood regions. The proposed method utilizes the whole data set\nto estimate its intrinsic dimension and is convenient for incremental learning.\nIn addition, our new PCA procedure can filter out noise in data and converge to\na stable estimation with the neighborhood region size increasing. Experiments\non synthetic and real world data sets show effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 10:16:57 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Fan", "Mingyu", ""], ["Gu", "Nannan", ""], ["Qiao", "Hong", ""], ["Zhang", "Bo", ""]]}, {"id": "1002.2171", "submitter": "Judith Wiesinger", "authors": "J. Wiesinger, D. Sornette, J. Satinover", "title": "Reverse Engineering Financial Markets with Majority and Minority Games\n  using Genetic Algorithms", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using virtual stock markets with artificial interacting software investors,\naka agent-based models (ABMs), we present a method to reverse engineer\nreal-world financial time series. We model financial markets as made of a large\nnumber of interacting boundedly rational agents. By optimizing the similarity\nbetween the actual data and that generated by the reconstructed virtual stock\nmarket, we obtain parameters and strategies, which reveal some of the inner\nworkings of the target stock market. We validate our approach by out-of-sample\npredictions of directional moves of the Nasdaq Composite Index.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 18:48:43 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Wiesinger", "J.", ""], ["Sornette", "D.", ""], ["Satinover", "J.", ""]]}, {"id": "1002.2240", "submitter": "Joe Suzuki", "authors": "Joe Suzuki", "title": "A Generalization of the Chow-Liu Algorithm and its Application to\n  Statistical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Chow-Liu algorithm for general random variables while the\nprevious versions only considered finite cases. In particular, this paper\napplies the generalization to Suzuki's learning algorithm that generates from\ndata forests rather than trees based on the minimum description length by\nbalancing the fitness of the data to the forest and the simplicity of the\nforest. As a result, we successfully obtain an algorithm when both of the\nGaussian and finite random variables are present.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 23:19:56 GMT"}], "update_date": "2010-02-12", "authors_parsed": [["Suzuki", "Joe", ""]]}, {"id": "1002.2425", "submitter": "Rdv Ijcsis", "authors": "O. J. Oyelade, O. O. Oladipupo, I. C. Obagbuwa", "title": "Application of k Means Clustering algorithm for prediction of Students\n  Academic Performance", "comments": "IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS January 2010, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 7, No. 1, pp. 292-295, January 2010, USA", "doi": null, "report-no": "Journal of Computer Science, ISSN 1947 5500", "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to monitor the progress of students academic performance is a\ncritical issue to the academic community of higher learning. A system for\nanalyzing students results based on cluster analysis and uses standard\nstatistical algorithms to arrange their scores data according to the level of\ntheir performance is described. In this paper, we also implemented k mean\nclustering algorithm for analyzing students result data. The model was combined\nwith the deterministic model to analyze the students results of a private\nInstitution in Nigeria which is a good benchmark to monitor the progression of\nacademic performance of students in higher Institution for the purpose of\nmaking an effective decision by the academic planners.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2010 20:41:28 GMT"}], "update_date": "2010-02-12", "authors_parsed": [["Oyelade", "O. J.", ""], ["Oladipupo", "O. O.", ""], ["Obagbuwa", "I. C.", ""]]}, {"id": "1002.2780", "submitter": "Ruslan Salakhutdinov", "authors": "Ruslan Salakhutdinov, Nathan Srebro", "title": "Collaborative Filtering in a Non-Uniform World: Learning with the\n  Weighted Trace Norm", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that matrix completion with trace-norm regularization can be\nsignificantly hurt when entries of the matrix are sampled non-uniformly. We\nintroduce a weighted version of the trace-norm regularizer that works well also\nwith non-uniform sampling. Our experimental results demonstrate that the\nweighted trace-norm regularization indeed yields significant gains on the\n(highly non-uniformly sampled) Netflix dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2010 16:37:04 GMT"}], "update_date": "2010-02-16", "authors_parsed": [["Salakhutdinov", "Ruslan", ""], ["Srebro", "Nathan", ""]]}, {"id": "1002.3086", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel A. Braun", "title": "Convergence of Bayesian Control Rule", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, new approaches to adaptive control have sought to reformulate the\nproblem as a minimization of a relative entropy criterion to obtain tractable\nsolutions. In particular, it has been shown that minimizing the expected\ndeviation from the causal input-output dependencies of the true plant leads to\na new promising stochastic control rule called the Bayesian control rule. This\nwork proves the convergence of the Bayesian control rule under two sufficient\nassumptions: boundedness, which is an ergodicity condition; and consistency,\nwhich is an instantiation of the sure-thing principle.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2010 14:14:59 GMT"}], "update_date": "2010-02-17", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1002.3174", "submitter": "Mohsen Toorani", "authors": "M. C. Amirani, M. Toorani, A. A. Beheshti", "title": "A new approach to content-based file type detection", "comments": "6 Pages, 5 Figure, 2 Tables", "journal-ref": "Proceedings of the 13th IEEE Symposium on Computers and\n  Communications (ISCC'08), pp.1103-1108, July 2008", "doi": "10.1109/ISCC.2008.4625611", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  File type identification and file type clustering may be difficult tasks that\nhave an increasingly importance in the field of computer and network security.\nClassical methods of file type detection including considering file extensions\nand magic bytes can be easily spoofed. Content-based file type detection is a\nnewer way that is taken into account recently. In this paper, a new\ncontent-based method for the purpose of file type detection and file type\nclustering is proposed that is based on the PCA and neural networks. The\nproposed method has a good accuracy and is fast enough.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 10:18:07 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2011 14:02:13 GMT"}, {"version": "v3", "created": "Fri, 16 Mar 2012 21:31:17 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Amirani", "M. C.", ""], ["Toorani", "M.", ""], ["Beheshti", "A. A.", ""]]}, {"id": "1002.3183", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "A Complete Characterization of Statistical Query Learning with\n  Applications to Evolvability", "comments": "Simplified Lemma 3.8 and it's applications", "journal-ref": "Proceedings of the 44th IEEE Symposium on Foundations of Computer\n  Science, pp 375-384, 2009", "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical query (SQ) learning model of Kearns (1993) is a natural\nrestriction of the PAC learning model in which a learning algorithm is allowed\nto obtain estimates of statistical properties of the examples but cannot see\nthe examples themselves. We describe a new and simple characterization of the\nquery complexity of learning in the SQ learning model. Unlike the previously\nknown bounds on SQ learning our characterization preserves the accuracy and the\nefficiency of learning. The preservation of accuracy implies that that our\ncharacterization gives the first characterization of SQ learning in the\nagnostic learning framework. The preservation of efficiency is achieved using a\nnew boosting technique and allows us to derive a new approach to the design of\nevolutionary algorithms in Valiant's (2006) model of evolvability. We use this\napproach to demonstrate the existence of a large class of monotone evolutionary\nlearning algorithms based on square loss performance estimation. These results\ndiffer significantly from the few known evolutionary algorithms and give\nevidence that evolvability in Valiant's model is a more versatile phenomenon\nthan there had been previous reason to suspect.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2010 22:35:39 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2012 00:43:31 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2013 04:57:18 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1002.3345", "submitter": "Andrew Guillory", "authors": "Andrew Guillory, Jeff Bilmes", "title": "Interactive Submodular Set Cover", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "UWEETR-2010-0001", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a natural generalization of submodular set cover and exact\nactive learning with a finite hypothesis class (query learning). We call this\nnew problem interactive submodular set cover. Applications include advertising\nin social networks with hidden information. We give an approximation guarantee\nfor a novel greedy algorithm and give a hardness of approximation result which\nmatches up to constant factors. We also discuss negative results for simpler\napproaches and present encouraging early experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 18:43:59 GMT"}, {"version": "v2", "created": "Thu, 20 May 2010 23:39:23 GMT"}], "update_date": "2010-05-24", "authors_parsed": [["Guillory", "Andrew", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1002.4007", "submitter": "William Jackson", "authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita\n  Nasipuri, Dipak Kumar Basu", "title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2010 19:48:16 GMT"}], "update_date": "2010-03-25", "authors_parsed": [["Sarkar", "Ram", ""], ["Das", "Nibaran", ""], ["Basu", "Subhadip", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1002.4040", "submitter": "William Jackson", "authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas\n  Kundu, Mita Nasipuri", "title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2010 02:58:49 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2010 06:44:32 GMT"}], "update_date": "2010-03-25", "authors_parsed": [["Das", "Nibaran", ""], ["Das", "Bindaban", ""], ["Sarkar", "Ram", ""], ["Basu", "Subhadip", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1002.4046", "submitter": "William Jackson", "authors": "K. Perumal, R. Bhaskaran", "title": "Supervised Classification Performance of Multispectral Images", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays government and private agencies use remote sensing imagery for a\nwide range of applications from military applications to farm development. The\nimages may be a panchromatic, multispectral, hyperspectral or even\nultraspectral of terra bytes. Remote sensing image classification is one\namongst the most significant application worlds for remote sensing. A few\nnumber of image classification algorithms have proved good precision in\nclassifying remote sensing data. But, of late, due to the increasing\nspatiotemporal dimensions of the remote sensing data, traditional\nclassification algorithms have exposed weaknesses necessitating further\nresearch in the field of remote sensing image classification. So an efficient\nclassifier is needed to classify the remote sensing images to extract\ninformation. We are experimenting with both supervised and unsupervised\nclassification. Here we compare the different classification methods and their\nperformances. It is found that Mahalanobis classifier performed the best in our\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2010 03:12:14 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Perumal", "K.", ""], ["Bhaskaran", "R.", ""]]}, {"id": "1002.4058", "submitter": "Lev Reyzin", "authors": "Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E.\n  Schapire", "title": "Contextual Bandit Algorithms with Supervised Learning Guarantees", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning in an online, bandit setting where the\nlearner must repeatedly select among $K$ actions, but only receives partial\nfeedback based on its choices. We establish two new facts: First, using a new\nalgorithm called Exp4.P, we show that it is possible to compete with the best\nin a set of $N$ experts with probability $1-\\delta$ while incurring regret at\nmost $O(\\sqrt{KT\\ln(N/\\delta)})$ over $T$ time steps. The new algorithm is\ntested empirically in a large-scale, real-world dataset. Second, we give a new\nalgorithm called VE that competes with a possibly infinite set of policies of\nVC-dimension $d$ while incurring regret at most $O(\\sqrt{T(d\\ln(T) + \\ln\n(1/\\delta))})$ with probability $1-\\delta$. These guarantees improve on those\nof all previous algorithms, whether in a stochastic or adversarial environment,\nand bring us closer to providing supervised learning type guarantees for the\ncontextual bandit setting.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2010 07:11:39 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2010 21:25:22 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2011 19:28:49 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Langford", "John", ""], ["Li", "Lihong", ""], ["Reyzin", "Lev", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1002.4658", "submitter": "Huan Xu Dr.", "authors": "Huan Xu, Constantine Caramanis, Shie Mannor", "title": "Principal Component Analysis with Contaminated Data: The High\n  Dimensional Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dimensionality-reduction problem (finding a subspace\napproximation of observed data) for contaminated data in the high dimensional\nregime, where the number of observations is of the same magnitude as the number\nof variables of each observation, and the data set contains some (arbitrarily)\ncorrupted observations. We propose a High-dimensional Robust Principal\nComponent Analysis (HR-PCA) algorithm that is tractable, robust to contaminated\npoints, and easily kernelizable. The resulting subspace has a bounded deviation\nfrom the desired one, achieves maximal robustness -- a breakdown point of 50%\nwhile all existing algorithms have a breakdown point of zero, and unlike\nordinary PCA algorithms, achieves optimality in the limit case where the\nproportion of corrupted points goes to zero.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2010 23:24:17 GMT"}, {"version": "v2", "created": "Thu, 13 May 2010 03:06:22 GMT"}], "update_date": "2010-05-14", "authors_parsed": [["Xu", "Huan", ""], ["Caramanis", "Constantine", ""], ["Mannor", "Shie", ""]]}, {"id": "1002.4802", "submitter": "Ricardo Silva", "authors": "Ricardo Silva and Robert B. Gramacy", "title": "Gaussian Process Structural Equation Models with Latent Variables", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of disciplines such as social sciences, psychology, medicine and\neconomics, the recorded data are considered to be noisy measurements of latent\nvariables connected by some causal structure. This corresponds to a family of\ngraphical models known as the structural equation model with latent variables.\nWhile linear non-Gaussian variants have been well-studied, inference in\nnonparametric structural equation models is still underdeveloped. We introduce\na sparse Gaussian process parameterization that defines a non-linear structure\nconnecting latent variables, unlike common formulations of Gaussian process\nlatent variable models. The sparse parameterization is given a full Bayesian\ntreatment without compromising Markov chain Monte Carlo efficiency. We compare\nthe stability of the sampling procedure and the predictive ability of the model\nagainst the current practice.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2010 15:10:06 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2010 10:41:26 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Silva", "Ricardo", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "1002.4862", "submitter": "Matthew Streeter", "authors": "Matthew Streeter and H. Brendan McMahan", "title": "Less Regret via Online Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze and evaluate an online gradient descent algorithm with adaptive\nper-coordinate adjustment of learning rates. Our algorithm can be thought of as\nan online version of batch gradient descent with a diagonal preconditioner.\nThis approach leads to regret bounds that are stronger than those of standard\nonline gradient descent for general online convex optimization problems.\nExperimentally, we show that our algorithm is competitive with state-of-the-art\nalgorithms for large scale machine learning problems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2010 20:31:05 GMT"}], "update_date": "2010-02-26", "authors_parsed": [["Streeter", "Matthew", ""], ["McMahan", "H. Brendan", ""]]}, {"id": "1002.4908", "submitter": "Hugh Brendan McMahan", "authors": "H. Brendan McMahan, Matthew Streeter", "title": "Adaptive Bound Optimization for Online Convex Optimization", "comments": "Updates to match final COLT version", "journal-ref": "Proceedings of the 23rd Annual Conference on Learning Theory\n  (COLT) 2010", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new online convex optimization algorithm that adaptively\nchooses its regularization function based on the loss functions observed so\nfar. This is in contrast to previous algorithms that use a fixed regularization\nfunction such as L2-squared, and modify it only via a single time-dependent\nparameter. Our algorithm's regret bounds are worst-case optimal, and for\ncertain realistic classes of loss functions they are much better than existing\nbounds. These bounds are problem-dependent, which means they can exploit the\nstructure of the actual problem instance. Critically, however, our algorithm\ndoes not need to know this structure in advance. Rather, we prove competitive\nguarantees that show the algorithm provides a bound within a constant factor of\nthe best possible bound (of a certain functional form) in hindsight.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2010 01:36:34 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2010 19:07:16 GMT"}], "update_date": "2010-07-08", "authors_parsed": [["McMahan", "H. Brendan", ""], ["Streeter", "Matthew", ""]]}]