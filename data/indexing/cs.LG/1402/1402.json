[{"id": "1402.0030", "submitter": "Andriy Mnih", "authors": "Andriy Mnih, Karol Gregor", "title": "Neural Variational Inference and Learning in Belief Networks", "comments": null, "journal-ref": "Proceedings of the 31st International Conference on Machine\n  Learning (ICML), JMLR: W&CP volume 32, 2014 pgs 1791-1799", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly expressive directed latent variable models, such as sigmoid belief\nnetworks, are difficult to train on large datasets because exact inference in\nthem is intractable and none of the approximate inference methods that have\nbeen applied to them scale well. We propose a fast non-iterative approximate\ninference method that uses a feedforward network to implement efficient exact\nsampling from the variational posterior. The model and this inference network\nare trained jointly by maximizing a variational lower bound on the\nlog-likelihood. Although the naive estimator of the inference model gradient is\ntoo high-variance to be useful, we make it practical by applying several\nstraightforward model-independent variance reduction techniques. Applying our\napproach to training sigmoid belief networks and deep autoregressive networks,\nwe show that it outperforms the wake-sleep algorithm on MNIST and achieves\nstate-of-the-art results on the Reuters RCV1 document dataset.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 23:33:21 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 17:12:03 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Mnih", "Andriy", ""], ["Gregor", "Karol", ""]]}, {"id": "1402.0099", "submitter": "Louis Theran", "authors": "Franz J. Kir\\'aly, Martin Kreuzer, and Louis Theran", "title": "Dual-to-kernel learning with ideals", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AC math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a theory which unifies kernel learning and symbolic\nalgebraic methods. We show that both worlds are inherently dual to each other,\nand we use this duality to combine the structure-awareness of algebraic methods\nwith the efficiency and generality of kernels. The main idea lies in relating\npolynomial rings to feature space, and ideals to manifolds, then exploiting\nthis generative-discriminative duality on kernel matrices. We illustrate this\nby proposing two algorithms, IPCA and AVICA, for simultaneous manifold and\nfeature learning, and test their accuracy on synthetic and real world data.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 16:38:59 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Kreuzer", "Martin", ""], ["Theran", "Louis", ""]]}, {"id": "1402.0108", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Shyam Visweswaran", "title": "Markov Blanket Ranking using Kernel-based Conditional Dependence\n  Measures", "comments": "10 pages, 4 figures, 2 algorithms, NIPS 2013 Workshop on Causality,\n  code: github.com/ericstrobl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing feature selection algorithms that move beyond a pure correlational\nto a more causal analysis of observational data is an important problem in the\nsciences. Several algorithms attempt to do so by discovering the Markov blanket\nof a target, but they all contain a forward selection step which variables must\npass in order to be included in the conditioning set. As a result, these\nalgorithms may not consider all possible conditional multivariate combinations.\nWe improve on this limitation by proposing a backward elimination method that\nuses a kernel-based conditional dependence measure to identify the Markov\nblanket in a fully multivariate fashion. The algorithm is easy to implement and\ncompares favorably to other methods on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 17:51:54 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2014 22:16:00 GMT"}, {"version": "v3", "created": "Sat, 3 May 2014 01:07:49 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Strobl", "Eric V.", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1402.0119", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Suvrit Sra, Alex Smola, Zoubin Ghahramani, Bernhard\n  Sch\\\"olkopf", "title": "Randomized Nonlinear Component Analysis", "comments": "Appearing in ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical methods such as Principal Component Analysis (PCA) and Canonical\nCorrelation Analysis (CCA) are ubiquitous in statistics. However, these\ntechniques are only able to reveal linear relationships in data. Although\nnonlinear variants of PCA and CCA have been proposed, these are computationally\nprohibitive in the large scale.\n  In a separate strand of recent research, randomized methods have been\nproposed to construct features that help reveal nonlinear patterns in data. For\nbasic tasks such as regression or classification, random features exhibit\nlittle or no loss in performance, while achieving drastic savings in\ncomputational requirements.\n  In this paper we leverage randomness to design scalable new variants of\nnonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such\nas spectral clustering or LDA. We demonstrate our algorithms through\nexperiments on real-world data, on which we compare against the\nstate-of-the-art. A simple R implementation of the presented algorithms is\nprovided.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 19:54:06 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 16:41:11 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Lopez-Paz", "David", ""], ["Sra", "Suvrit", ""], ["Smola", "Alex", ""], ["Ghahramani", "Zoubin", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1402.0170", "submitter": "Shu Kong", "authors": "Shu Kong, Zhuolin Jiang, Qiang Yang", "title": "Collaborative Receptive Field Learning", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of object categorization in images is largely due to arbitrary\ntranslations and scales of the foreground objects. To attack this difficulty,\nwe propose a new approach called collaborative receptive field learning to\nextract specific receptive fields (RF's) or regions from multiple images, and\nthe selected RF's are supposed to focus on the foreground objects of a common\ncategory. To this end, we solve the problem by maximizing a submodular function\nover a similarity graph constructed by a pool of RF candidates. However,\nmeasuring pairwise distance of RF's for building the similarity graph is a\nnontrivial problem. Hence, we introduce a similarity metric called\npyramid-error distance (PED) to measure their pairwise distances through\nsumming up pyramid-like matching errors over a set of low-level features.\nBesides, in consistent with the proposed PED, we construct a simple\nnonparametric classifier for classification. Experimental results show that our\nmethod effectively discovers the foreground objects in images, and improves\nclassification performance.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2014 10:11:57 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Kong", "Shu", ""], ["Jiang", "Zhuolin", ""], ["Yang", "Qiang", ""]]}, {"id": "1402.0282", "submitter": "Benjamin Rubinstein", "authors": "Duo Zhang and Benjamin I. P. Rubinstein and Jim Gemmell", "title": "Principled Graph Matching Algorithms for Integrating Multiple Data\n  Sources", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores combinatorial optimization for problems of max-weight\ngraph matching on multi-partite graphs, which arise in integrating multiple\ndata sources. Entity resolution-the data integration problem of performing\nnoisy joins on structured data-typically proceeds by first hashing each record\ninto zero or more blocks, scoring pairs of records that are co-blocked for\nsimilarity, and then matching pairs of sufficient similarity. In the most\ncommon case of matching two sources, it is often desirable for the final\nmatching to be one-to-one (a record may be matched with at most one other);\nmembers of the database and statistical record linkage communities accomplish\nsuch matchings in the final stage by weighted bipartite graph matching on\nsimilarity scores. Such matchings are intuitively appealing: they leverage a\nnatural global property of many real-world entity stores-that of being nearly\ndeduped-and are known to provide significant improvements to precision and\nrecall. Unfortunately unlike the bipartite case, exact max-weight matching on\nmulti-partite graphs is known to be NP-hard. Our two-fold algorithmic\ncontributions approximate multi-partite max-weight matching: our first\nalgorithm borrows optimization techniques common to Bayesian probabilistic\ninference; our second is a greedy approximation algorithm. In addition to a\ntheoretical guarantee on the latter, we present comparisons on a real-world ER\nproblem from Bing significantly larger than typically found in the literature,\npublication data, and on a series of synthetic problems. Our results quantify\nsignificant improvements due to exploiting multiple sources, which are made\npossible by global one-to-one constraints linking otherwise independent\nmatching sub-problems. We also discover that our algorithms are complementary:\none being much more robust under noise, and the other being simple to implement\nand very fast to run.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 04:56:58 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Zhang", "Duo", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Gemmell", "Jim", ""]]}, {"id": "1402.0288", "submitter": "Gang Niu", "authors": "Gang Niu, Bo Dai, Marthinus Christoffel du Plessis, and Masashi\n  Sugiyama", "title": "Transductive Learning with Multi-class Volume Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a hypothesis space, the large volume principle by Vladimir Vapnik\nprioritizes equivalence classes according to their volume in the hypothesis\nspace. The volume approximation has hitherto been successfully applied to\nbinary learning problems. In this paper, we extend it naturally to a more\ngeneral definition which can be applied to several transductive problem\nsettings, such as multi-class, multi-label and serendipitous learning. Even\nthough the resultant learning method involves a non-convex optimization\nproblem, the globally optimal solution is almost surely unique and can be\nobtained in O(n^3) time. We theoretically provide stability and error analyses\nfor the proposed method, and then experimentally show that it is promising.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 06:09:52 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Niu", "Gang", ""], ["Dai", "Bo", ""], ["Plessis", "Marthinus Christoffel du", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1402.0422", "submitter": "Andrea Lancichinetti", "authors": "Andrea Lancichinetti, M. Irmak Sirer, Jane X. Wang, Daniel Acuna,\n  Konrad K\\\"ording, Lu\\'is A. Nunes Amaral", "title": "A high-reproducibility and high-accuracy method for automated topic\n  classification", "comments": "23 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of human knowledge sits in large databases of unstructured text.\nLeveraging this knowledge requires algorithms that extract and record metadata\non unstructured text documents. Assigning topics to documents will enable\nintelligent search, statistical characterization, and meaningful\nclassification. Latent Dirichlet allocation (LDA) is the state-of-the-art in\ntopic classification. Here, we perform a systematic theoretical and numerical\nanalysis that demonstrates that current optimization techniques for LDA often\nyield results which are not accurate in inferring the most suitable model\nparameters. Adapting approaches for community detection in networks, we propose\na new algorithm which displays high-reproducibility and high-accuracy, and also\nhas high computational efficiency. We apply it to a large set of documents in\nthe English Wikipedia and reveal its hierarchical structure. Our algorithm\npromises to make \"big data\" text analysis systems more reliable.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 16:45:13 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Lancichinetti", "Andrea", ""], ["Sirer", "M. Irmak", ""], ["Wang", "Jane X.", ""], ["Acuna", "Daniel", ""], ["K\u00f6rding", "Konrad", ""], ["Amaral", "Lu\u00eds A. Nunes", ""]]}, {"id": "1402.0452", "submitter": "Rangeet Mitra", "authors": "Rangeet Mitra, Amit Kumar Mishra and Tarun Choubisa", "title": "A Lower Bound for the Variance of Estimators for Nakagami m Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have proposed a maximum likelihood iterative algorithm for\nestimation of the parameters of the Nakagami-m distribution. This technique\nperforms better than state of art estimation techniques for this distribution.\nThis could be of particular use in low data or block based estimation problems.\nIn these scenarios, the estimator should be able to give accurate estimates in\nthe mean square sense with less amounts of data. Also, the estimates should\nimprove with the increase in number of blocks received. In this paper, we see\nthrough our simulations, that our proposal is well designed for such\nrequirements. Further, it is well known in the literature that an efficient\nestimator does not exist for Nakagami-m distribution. In this paper, we derive\na theoretical expression for the variance of our proposed estimator. We find\nthat this expression clearly fits the experimental curve for the variance of\nthe proposed estimator. This expression is pretty close to the cramer-rao lower\nbound(CRLB).\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 18:20:46 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Mitra", "Rangeet", ""], ["Mishra", "Amit Kumar", ""], ["Choubisa", "Tarun", ""]]}, {"id": "1402.0453", "submitter": "Qi Qian", "authors": "Qi Qian, Rong Jin, Shenghuo Zhu and Yuanqing Lin", "title": "Fine-Grained Visual Categorization via Multi-stage Metric Learning", "comments": "in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual categorization (FGVC) is to categorize objects into\nsubordinate classes instead of basic classes. One major challenge in FGVC is\nthe co-occurrence of two issues: 1) many subordinate classes are highly\ncorrelated and are difficult to distinguish, and 2) there exists the large\nintra-class variation (e.g., due to object pose). This paper proposes to\nexplicitly address the above two issues via distance metric learning (DML). DML\naddresses the first issue by learning an embedding so that data points from the\nsame class will be pulled together while those from different classes should be\npushed apart from each other; and it addresses the second issue by allowing the\nflexibility that only a portion of the neighbors (not all data points) from the\nsame class need to be pulled together. However, feature representation of an\nimage is often high dimensional, and DML is known to have difficulty in dealing\nwith high dimensional feature vectors since it would require $\\mathcal{O}(d^2)$\nfor storage and $\\mathcal{O}(d^3)$ for optimization. To this end, we proposed a\nmulti-stage metric learning framework that divides the large-scale high\ndimensional learning problem to a series of simple subproblems, achieving\n$\\mathcal{O}(d)$ computational complexity. The empirical study with FVGC\nbenchmark datasets verifies that our method is both effective and efficient\ncompared to the state-of-the-art FGVC approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 18:20:53 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 17:28:51 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Qian", "Qi", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1402.0459", "submitter": "Haoyang (Hubert) Duan", "authors": "Hubert Haoyang Duan", "title": "Applying Supervised Learning Algorithms and a New Feature Selection\n  Method to Predict Coronary Artery Disease", "comments": "This is a Master of Science in Mathematics thesis under the\n  supervision of Dr. Vladimir Pestov and Dr. George Wells submitted on January\n  31, 2014 at the University of Ottawa; 102 pages and 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a fresh data science perspective, this thesis discusses the prediction\nof coronary artery disease based on genetic variations at the DNA base pair\nlevel, called Single-Nucleotide Polymorphisms (SNPs), collected from the\nOntario Heart Genomics Study (OHGS).\n  First, the thesis explains two commonly used supervised learning algorithms,\nthe k-Nearest Neighbour (k-NN) and Random Forest classifiers, and includes a\ncomplete proof that the k-NN classifier is universally consistent in any finite\ndimensional normed vector space. Second, the thesis introduces two\ndimensionality reduction steps, Random Projections, a known feature extraction\ntechnique based on the Johnson-Lindenstrauss lemma, and a new method termed\nMass Transportation Distance (MTD) Feature Selection for discrete domains.\nThen, this thesis compares the performance of Random Projections with the k-NN\nclassifier against MTD Feature Selection and Random Forest, for predicting\nartery disease based on accuracy, the F-Measure, and area under the Receiver\nOperating Characteristic (ROC) curve.\n  The comparative results demonstrate that MTD Feature Selection with Random\nForest is vastly superior to Random Projections and k-NN. The Random Forest\nclassifier is able to obtain an accuracy of 0.6660 and an area under the ROC\ncurve of 0.8562 on the OHGS genetic dataset, when 3335 SNPs are selected by MTD\nFeature Selection for classification. This area is considerably better than the\nprevious high score of 0.608 obtained by Davies et al. in 2010 on the same\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 18:47:41 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Duan", "Hubert Haoyang", ""]]}, {"id": "1402.0480", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P. Kingma, Max Welling", "title": "Efficient Gradient-Based Inference through Transformations between Bayes\n  Nets and Neural Nets", "comments": null, "journal-ref": "Proceedings of The 31st International Conference on Machine\n  Learning, pp. 1782-1790, 2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Bayesian networks and neural networks with stochastic hidden\nunits are commonly perceived as two separate types of models. We show that\neither of these types of models can often be transformed into an instance of\nthe other, by switching between centered and differentiable non-centered\nparameterizations of the latent variables. The choice of parameterization\ngreatly influences the efficiency of gradient-based posterior inference; we\nshow that they are often complementary to eachother, we clarify when each\nparameterization is preferred and show how inference can be made robust. In the\nnon-centered form, a simple Monte Carlo estimator of the marginal likelihood\ncan be used for learning the parameters. Theoretical results are supported by\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 19:39:20 GMT"}, {"version": "v2", "created": "Mon, 3 Mar 2014 13:56:26 GMT"}, {"version": "v3", "created": "Tue, 13 May 2014 11:17:41 GMT"}, {"version": "v4", "created": "Mon, 16 Jun 2014 09:04:26 GMT"}, {"version": "v5", "created": "Thu, 22 Jan 2015 11:05:53 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Kingma", "Diederik P.", ""], ["Welling", "Max", ""]]}, {"id": "1402.0555", "submitter": "Daniel Hsu", "authors": "Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and\n  Robert E. Schapire", "title": "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for the contextual bandit learning problem, where\nthe learner repeatedly takes one of $K$ actions in response to the observed\ncontext, and observes the reward only for that chosen action. Our method\nassumes access to an oracle for solving fully supervised cost-sensitive\nclassification problems and achieves the statistically optimal regret guarantee\nwith only $\\tilde{O}(\\sqrt{KT/\\log N})$ oracle calls across all $T$ rounds,\nwhere $N$ is the number of policies in the policy class we compete against. By\ndoing so, we obtain the most practical contextual bandit learning algorithm\namongst approaches that work for general policy classes. We further conduct a\nproof-of-concept experiment which demonstrates the excellent computational and\nprediction performance of (an online variant of) our algorithm relative to\nseveral baselines.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 00:48:29 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 01:41:47 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Agarwal", "Alekh", ""], ["Hsu", "Daniel", ""], ["Kale", "Satyen", ""], ["Langford", "John", ""], ["Li", "Lihong", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1402.0558", "submitter": "Sebastian Ordyniak", "authors": "Sebastian Ordyniak, Stefan Szeider", "title": "Parameterized Complexity Results for Exact Bayesian Network Structure\n  Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  263-302, 2013", "doi": "10.1613/jair.3744", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structure learning is the notoriously difficult problem of\ndiscovering a Bayesian network that optimally represents a given set of\ntraining data. In this paper we study the computational worst-case complexity\nof exact Bayesian network structure learning under graph theoretic restrictions\non the (directed) super-structure. The super-structure is an undirected graph\nthat contains as subgraphs the skeletons of solution networks. We introduce the\ndirected super-structure as a natural generalization of its undirected\ncounterpart. Our results apply to several variants of score-based Bayesian\nnetwork structure learning where the score of a network decomposes into local\nscores of its nodes. Results: We show that exact Bayesian network structure\nlearning can be carried out in non-uniform polynomial time if the\nsuper-structure has bounded treewidth, and in linear time if in addition the\nsuper-structure has bounded maximum degree. Furthermore, we show that if the\ndirected super-structure is acyclic, then exact Bayesian network structure\nlearning can be carried out in quadratic time. We complement these positive\nresults with a number of hardness results. We show that both restrictions\n(treewidth and degree) are essential and cannot be dropped without loosing\nuniform polynomial time tractability (subject to a complexity-theoretic\nassumption). Similarly, exact Bayesian network structure learning remains\nNP-hard for \"almost acyclic\" directed super-structures. Furthermore, we show\nthat the restrictions remain essential if we do not search for a globally\noptimal network but aim to improve a given network by means of at most k arc\nadditions, arc deletions, or arc reversals (k-neighborhood local search).\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:33:50 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1402.0560", "submitter": "Javier Garcia", "authors": "Javier Garcia, Fernando Fernandez", "title": "Safe Exploration of State and Action Spaces in Reinforcement Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  515-564, 2012", "doi": "10.1613/jair.3761", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the important problem of safe exploration in\nreinforcement learning. While reinforcement learning is well-suited to domains\nwith complex transition dynamics and high-dimensional state-action spaces, an\nadditional challenge is posed by the need for safe and efficient exploration.\nTraditional exploration techniques are not particularly useful for solving\ndangerous tasks, where the trial and error process may lead to the selection of\nactions whose execution in some states may result in damage to the learning\nsystem (or any other system). Consequently, when an agent begins an interaction\nwith a dangerous and high-dimensional state-action space, an important question\narises; namely, that of how to avoid (or at least minimize) damage caused by\nthe exploration of the state-action space. We introduce the PI-SRL algorithm\nwhich safely improves suboptimal albeit robust behaviors for continuous state\nand action control tasks and which efficiently learns from the experience\ngained from the environment. We evaluate the proposed method in four complex\ntasks: automatic car parking, pole-balancing, helicopter hovering, and business\nmanagement.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:34:25 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Garcia", "Javier", ""], ["Fernandez", "Fernando", ""]]}, {"id": "1402.0562", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar, Alessandro Lazaric and Emma Brunskill", "title": "Online Stochastic Optimization under Correlated Bandit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of online stochastic optimization of a\nlocally smooth function under bandit feedback. We introduce the high-confidence\ntree (HCT) algorithm, a novel any-time $\\mathcal{X}$-armed bandit algorithm,\nand derive regret bounds matching the performance of existing state-of-the-art\nin terms of dependency on number of steps and smoothness factor. The main\nadvantage of HCT is that it handles the challenging case of correlated rewards,\nwhereas existing methods require that the reward-generating process of each arm\nis an identically and independent distributed (iid) random process. HCT also\nimproves on the state-of-the-art in terms of its memory requirement as well as\nrequiring a weaker smoothness assumption on the mean-reward function in compare\nto the previous anytime algorithms. Finally, we discuss how HCT can be applied\nto the problem of policy search in reinforcement learning and we report\npreliminary empirical results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:34:50 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2014 20:50:52 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 17:30:53 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Lazaric", "Alessandro", ""], ["Brunskill", "Emma", ""]]}, {"id": "1402.0570", "submitter": "Guangtao Wang", "authors": "Guangtao Wang, Qinbao Song, Heli Sun, Xueying Zhang, Baowen Xu, Yuming\n  Zhou", "title": "A Feature Subset Selection Algorithm Automatic Recommendation Method", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  1-34, 2013", "doi": "10.1613/jair.3831", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many feature subset selection (FSS) algorithms have been proposed, but not\nall of them are appropriate for a given feature selection problem. At the same\ntime, so far there is rarely a good way to choose appropriate FSS algorithms\nfor the problem at hand. Thus, FSS algorithm automatic recommendation is very\nimportant and practically useful. In this paper, a meta learning based FSS\nalgorithm automatic recommendation method is presented. The proposed method\nfirst identifies the data sets that are most similar to the one at hand by the\nk-nearest neighbor classification algorithm, and the distances among these data\nsets are calculated based on the commonly-used data set characteristics. Then,\nit ranks all the candidate FSS algorithms according to their performance on\nthese similar data sets, and chooses the algorithms with best performance as\nthe appropriate ones. The performance of the candidate FSS algorithms is\nevaluated by a multi-criteria metric that takes into account not only the\nclassification accuracy over the selected features, but also the runtime of\nfeature selection and the number of selected features. The proposed\nrecommendation method is extensively tested on 115 real world data sets with 22\nwell-known and frequently-used different FSS algorithms for five representative\nclassifiers. The results show the effectiveness of our proposed FSS algorithm\nrecommendation method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:37:24 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Wang", "Guangtao", ""], ["Song", "Qinbao", ""], ["Sun", "Heli", ""], ["Zhang", "Xueying", ""], ["Xu", "Baowen", ""], ["Zhou", "Yuming", ""]]}, {"id": "1402.0577", "submitter": "Rapha\\\"el Mourad", "authors": "Rapha\\\"el Mourad, Christine Sinoquet, Nevin L. Zhang, Tengfei Liu,\n  Philippe Leray", "title": "A Survey on Latent Tree Models and Applications", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  157-203, 2013", "doi": "10.1613/jair.3879", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data analysis, latent variables play a central role because they help\nprovide powerful insights into a wide variety of phenomena, ranging from\nbiological to human sciences. The latent tree model, a particular type of\nprobabilistic graphical models, deserves attention. Its simple structure - a\ntree - allows simple and efficient inference, while its latent variables\ncapture complex relationships. In the past decade, the latent tree model has\nbeen subject to significant theoretical and methodological developments. In\nthis review, we propose a comprehensive study of this model. First we summarize\nkey ideas underlying the model. Second we explain how it can be efficiently\nlearned from data. Third we illustrate its use within three types of\napplications: latent structure discovery, multidimensional clustering, and\nprobabilistic inference. Finally, we conclude and give promising directions for\nfuture researches in this field.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:40:28 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Mourad", "Rapha\u00ebl", ""], ["Sinoquet", "Christine", ""], ["Zhang", "Nevin L.", ""], ["Liu", "Tengfei", ""], ["Leray", "Philippe", ""]]}, {"id": "1402.0635", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy, Zheng Wen", "title": "Generalization and Exploration via Randomized Value Functions", "comments": "arXiv admin note: text overlap with arXiv:1307.4847", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose randomized least-squares value iteration (RLSVI) -- a new\nreinforcement learning algorithm designed to explore and generalize efficiently\nvia linearly parameterized value functions. We explain why versions of\nleast-squares value iteration that use Boltzmann or epsilon-greedy exploration\ncan be highly inefficient, and we present computational results that\ndemonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish\nan upper bound on the expected regret of RLSVI that demonstrates\nnear-optimality in a tabula rasa learning context. More broadly, our results\nsuggest that randomized value functions offer a promising approach to tackling\na critical challenge in reinforcement learning: synthesizing efficient\nexploration and effective generalization.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 06:41:59 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 23:11:02 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2016 10:20:11 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""], ["Wen", "Zheng", ""]]}, {"id": "1402.0645", "submitter": "Franziska Meier", "authors": "Franziska Meier and Philipp Hennig and Stefan Schaal", "title": "Local Gaussian Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally weighted regression was created as a nonparametric learning method\nthat is computationally efficient, can learn from very large amounts of data\nand add data incrementally. An interesting feature of locally weighted\nregression is that it can work with spatially varying length scales, a\nbeneficial property, for instance, in control problems. However, it does not\nprovide a generative model for function values and requires training and test\ndata to be generated identically, independently. Gaussian (process) regression,\non the other hand, provides a fully generative model without significant formal\nrequirements on the distribution of training data, but has much higher\ncomputational cost and usually works with one global scale per input dimension.\nUsing a localising function basis and approximate inference techniques, we take\nGaussian (process) regression to increasingly localised properties and toward\nthe same computational complexity class as locally weighted regression.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 07:35:48 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Meier", "Franziska", ""], ["Hennig", "Philipp", ""], ["Schaal", "Stefan", ""]]}, {"id": "1402.0779", "submitter": "Nathanael Perraudin N. P.", "authors": "Nathanael Perraudin, Vassilis Kalofolias, David Shuman, Pierre\n  Vandergheynst", "title": "UNLocBoX: A MATLAB convex optimization toolbox for proximal-splitting\n  methods", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization is an essential tool for machine learning, as many of its\nproblems can be formulated as minimization problems of specific objective\nfunctions. While there is a large variety of algorithms available to solve\nconvex problems, we can argue that it becomes more and more important to focus\non efficient, scalable methods that can deal with big data. When the objective\nfunction can be written as a sum of \"simple\" terms, proximal splitting methods\nare a good choice. UNLocBoX is a MATLAB library that implements many of these\nmethods, designed to solve convex optimization problems of the form $\\min_{x\n\\in \\mathbb{R}^N} \\sum_{n=1}^K f_n(x).$ It contains the most recent solvers\nsuch as FISTA, Douglas-Rachford, SDMM as well a primal dual techniques such as\nChambolle-Pock and forward-backward-forward. It also includes an extensive list\nof common proximal operators that can be combined, allowing for a quick\nimplementation of a large variety of convex problems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 15:58:44 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 11:09:57 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 07:38:32 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Perraudin", "Nathanael", ""], ["Kalofolias", "Vassilis", ""], ["Shuman", "David", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1402.0796", "submitter": "Alexandre Lacoste", "authors": "Alexandre Lacoste, Hugo Larochelle, Fran\\c{c}ois Laviolette, Mario\n  Marchand", "title": "Sequential Model-Based Ensemble Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most tedious tasks in the application of machine learning is model\nselection, i.e. hyperparameter selection. Fortunately, recent progress has been\nmade in the automation of this process, through the use of sequential\nmodel-based optimization (SMBO) methods. This can be used to optimize a\ncross-validation performance of a learning algorithm over the value of its\nhyperparameters. However, it is well known that ensembles of learned models\nalmost consistently outperform a single model, even if properly selected. In\nthis paper, we thus propose an extension of SMBO methods that automatically\nconstructs such ensembles. This method builds on a recently proposed ensemble\nconstruction paradigm known as agnostic Bayesian learning. In experiments on 22\nregression and 39 classification data sets, we confirm the success of this\nproposed approach, which is able to outperform model selection with SMBO.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 17:01:16 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Lacoste", "Alexandre", ""], ["Larochelle", "Hugo", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""]]}, {"id": "1402.0859", "submitter": "Varun Jampani", "authors": "Varun Jampani and Sebastian Nowozin and Matthew Loper and Peter V.\n  Gehler", "title": "The Informed Sampler: A Discriminative Approach to Bayesian Inference in\n  Generative Computer Vision Models", "comments": "Appearing in Computer Vision and Image Understanding Journal (Special\n  Issue on Generative Models in Computer Vision)", "journal-ref": null, "doi": "10.1016/j.cviu.2015.03.002", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is hard because of a large variability in lighting, shape,\nand texture; in addition the image signal is non-additive due to occlusion.\nGenerative models promised to account for this variability by accurately\nmodelling the image formation process as a function of latent variables with\nprior beliefs. Bayesian posterior inference could then, in principle, explain\nthe observation. While intuitively appealing, generative models for computer\nvision have largely failed to deliver on that promise due to the difficulty of\nposterior inference. As a result the community has favoured efficient\ndiscriminative approaches. We still believe in the usefulness of generative\nmodels in computer vision, but argue that we need to leverage existing\ndiscriminative or even heuristic computer vision methods. We implement this\nidea in a principled way with an \"informed sampler\" and in careful experiments\ndemonstrate it on challenging generative models which contain renderer programs\nas their components. We concentrate on the problem of inverting an existing\ngraphics rendering engine, an approach that can be understood as \"Inverse\nGraphics\". The informed sampler, using simple discriminative proposals based on\nexisting computer vision technology, achieves significant improvements of\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 20:52:26 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 11:28:13 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2015 19:50:59 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Jampani", "Varun", ""], ["Nowozin", "Sebastian", ""], ["Loper", "Matthew", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1402.0914", "submitter": "Scott Linderman", "authors": "Scott W. Linderman and Ryan P. Adams", "title": "Discovering Latent Network Structure in Point Process Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks play a central role in modern data analysis, enabling us to reason\nabout systems by studying the relationships between their parts. Most often in\nnetwork analysis, the edges are given. However, in many systems it is difficult\nor impossible to measure the network directly. Examples of latent networks\ninclude economic interactions linking financial instruments and patterns of\nreciprocity in gang violence. In these cases, we are limited to noisy\nobservations of events associated with each node. To enable analysis of these\nimplicit networks, we develop a probabilistic model that combines\nmutually-exciting point processes with random graph models. We show how the\nPoisson superposition principle enables an elegant auxiliary variable\nformulation and a fully-Bayesian, parallel inference algorithm. We evaluate\nthis new model empirically on several datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 23:48:23 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Linderman", "Scott W.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1402.0915", "submitter": "Oren Rippel", "authors": "Oren Rippel, Michael A. Gelbart, Ryan P. Adams", "title": "Learning Ordered Representations with Nested Dropout", "comments": "11 pages, 5 figures. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study ordered representations of data in which different\ndimensions have different degrees of importance. To learn these representations\nwe introduce nested dropout, a procedure for stochastically removing coherent\nnested sets of hidden units in a neural network. We first present a sequence of\ntheoretical results in the simple case of a semi-linear autoencoder. We\nrigorously show that the application of nested dropout enforces identifiability\nof the units, which leads to an exact equivalence with PCA. We then extend the\nalgorithm to deep models and demonstrate the relevance of ordered\nrepresentations to a number of applications. Specifically, we use the ordered\nproperty of the learned codes to construct hash-based data structures that\npermit very fast retrieval, achieving retrieval in time logarithmic in the\ndatabase size and independent of the dimensionality of the representation. This\nallows codes that are hundreds of times longer than currently feasible for\nretrieval. We therefore avoid the diminished quality associated with short\ncodes, while still performing retrieval that is competitive in speed with\nexisting methods. We also show that ordered representations are a promising way\nto learn adaptive compression for efficient online data reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 00:41:58 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Rippel", "Oren", ""], ["Gelbart", "Michael A.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1402.0929", "submitter": "Jasper Snoek", "authors": "Jasper Snoek, Kevin Swersky, Richard S. Zemel and Ryan P. Adams", "title": "Input Warping for Bayesian Optimization of Non-stationary Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has proven to be a highly effective methodology for the\nglobal optimization of unknown, expensive and multimodal functions. The ability\nto accurately model distributions over functions is critical to the\neffectiveness of Bayesian optimization. Although Gaussian processes provide a\nflexible prior over functions which can be queried efficiently, there are\nvarious classes of functions that remain difficult to model. One of the most\nfrequently occurring of these is the class of non-stationary functions. The\noptimization of the hyperparameters of machine learning algorithms is a problem\ndomain in which parameters are often manually transformed a priori, for example\nby optimizing in \"log-space,\" to mitigate the effects of spatially-varying\nlength scale. We develop a methodology for automatically learning a wide family\nof bijective transformations or warpings of the input space using the Beta\ncumulative distribution function. We further extend the warping framework to\nmulti-task Bayesian optimization so that multiple tasks can be warped into a\njointly stationary space. On a set of challenging benchmark optimization tasks,\nwe observe that the inclusion of warping greatly improves on the\nstate-of-the-art, producing better results faster and more reliably.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 03:55:39 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 22:00:38 GMT"}, {"version": "v3", "created": "Wed, 11 Jun 2014 20:32:11 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Snoek", "Jasper", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard S.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1402.1128", "submitter": "Hasim Sak", "authors": "Ha\\c{s}im Sak, Andrew Senior, Fran\\c{c}oise Beaufays", "title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for\n  Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)\narchitecture that has been designed to address the vanishing and exploding\ngradient problems of conventional RNNs. Unlike feedforward neural networks,\nRNNs have cyclic connections making them powerful for modeling sequences. They\nhave been successfully used for sequence labeling and sequence prediction\ntasks, such as handwriting recognition, language modeling, phonetic labeling of\nacoustic frames. However, in contrast to the deep neural networks, the use of\nRNNs in speech recognition has been limited to phone recognition in small scale\ntasks. In this paper, we present novel LSTM based RNN architectures which make\nmore effective use of model parameters to train acoustic models for large\nvocabulary speech recognition. We train and compare LSTM, RNN and DNN models at\nvarious numbers of parameters and configurations. We show that LSTM models\nconverge quickly and give state of the art speech recognition performance for\nrelatively small sized models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 19:01:51 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Sak", "Ha\u015fim", ""], ["Senior", "Andrew", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1402.1263", "submitter": "Eli A. Meirom", "authors": "Eli A. Meirom, Chris Milling, Constantine Caramanis, Shie Mannor,\n  Ariel Orda, Sanjay Shakkottai", "title": "Localized epidemic detection in networks with overwhelming noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting an epidemic in a population where\nindividual diagnoses are extremely noisy. The motivation for this problem is\nthe plethora of examples (influenza strains in humans, or computer viruses in\nsmartphones, etc.) where reliable diagnoses are scarce, but noisy data\nplentiful. In flu/phone-viruses, exceedingly few infected people/phones are\nprofessionally diagnosed (only a small fraction go to a doctor) but less\nreliable secondary signatures (e.g., people staying home, or\ngreater-than-typical upload activity) are more readily available. These\nsecondary data are often plagued by unreliability: many people with the flu do\nnot stay home, and many people that stay home do not have the flu. This paper\nidentifies the precise regime where knowledge of the contact network enables\nfinding the needle in the haystack: we provide a distributed, efficient and\nrobust algorithm that can correctly identify the existence of a spreading\nepidemic from highly unreliable local data. Our algorithm requires only\nlocal-neighbor knowledge of this graph, and in a broad array of settings that\nwe describe, succeeds even when false negatives and false positives make up an\noverwhelming fraction of the data available. Our results show it succeeds in\nthe presence of partial information about the contact network, and also when\nthere is not a single \"patient zero\", but rather many (hundreds, in our\nexamples) of initial patient-zeroes, spread across the graph.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 07:16:16 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Meirom", "Eli A.", ""], ["Milling", "Chris", ""], ["Caramanis", "Constantine", ""], ["Mannor", "Shie", ""], ["Orda", "Ariel", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1402.1298", "submitter": "Lenka Zdeborova", "authors": "Yoshiyuki Kabashima, Florent Krzakala, Marc M\\'ezard, Ayaka Sakata,\n  and Lenka Zdeborov\\'a", "title": "Phase transitions and sample complexity in Bayes-optimal matrix\n  factorization", "comments": "50 pages, 10 figures", "journal-ref": "IEEE Transactions on Information Theory (Volume:62 , Issue: 7,\n  Pages: 4228 - 4265) 2016", "doi": "10.1109/TIT.2016.2556702", "report-no": null, "categories": "cs.NA cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the matrix factorization problem. Given a noisy measurement of a\nproduct of two matrices, the problem is to estimate back the original matrices.\nIt arises in many applications such as dictionary learning, blind matrix\ncalibration, sparse principal component analysis, blind source separation, low\nrank matrix completion, robust principal component analysis or factor analysis.\nIt is also important in machine learning: unsupervised representation learning\ncan often be studied through matrix factorization. We use the tools of\nstatistical mechanics - the cavity and replica methods - to analyze the\nachievability and computational tractability of the inference problems in the\nsetting of Bayes-optimal inference, which amounts to assuming that the two\nmatrices have random independent elements generated from some known\ndistribution, and this information is available to the inference algorithm. In\nthis setting, we compute the minimal mean-squared-error achievable in principle\nin any computational time, and the error that can be achieved by an efficient\napproximate message passing algorithm. The computation is based on the\nasymptotic state-evolution analysis of the algorithm. The performance that our\nanalysis predicts, both in terms of the achieved mean-squared-error, and in\nterms of sample complexity, is extremely promising and motivating for a further\ndevelopment of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 09:56:50 GMT"}, {"version": "v2", "created": "Sat, 31 Jan 2015 20:56:04 GMT"}, {"version": "v3", "created": "Mon, 21 Mar 2016 18:07:08 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Kabashima", "Yoshiyuki", ""], ["Krzakala", "Florent", ""], ["M\u00e9zard", "Marc", ""], ["Sakata", "Ayaka", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1402.1349", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, David M. J. Tax, Marco Loog", "title": "Dissimilarity-based Ensembles for Multiple Instance Learning", "comments": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems, Special Issue on Learning in Non-(geo)metric Spaces", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, Volume\n  27, Issue 6, 2016, pages 1379 - 1391", "doi": "10.1109/TNNLS.2015.2424254", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiple instance learning, objects are sets (bags) of feature vectors\n(instances) rather than individual feature vectors. In this paper we address\nthe problem of how these bags can best be represented. Two standard approaches\nare to use (dis)similarities between bags and prototype bags, or between bags\nand prototype instances. The first approach results in a relatively\nlow-dimensional representation determined by the number of training bags, while\nthe second approach results in a relatively high-dimensional representation,\ndetermined by the total number of instances in the training set. In this paper\na third, intermediate approach is proposed, which links the two approaches and\ncombines their strengths. Our classifier is inspired by a random subspace\nensemble, and considers subspaces of the dissimilarity space, defined by\nsubsets of instances, as prototypes. We provide guidelines for using such an\nensemble, and show state-of-the-art performances on a range of multiple\ninstance learning problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 13:35:01 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Tax", "David M. J.", ""], ["Loog", "Marco", ""]]}, {"id": "1402.1389", "submitter": "Yarin Gal", "authors": "Yarin Gal, Mark van der Wilk, Carl E. Rasmussen", "title": "Distributed Variational Inference in Sparse Gaussian Process Regression\n  and Latent Variable Models", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are a powerful tool for probabilistic inference over\nfunctions. They have been applied to both regression and non-linear\ndimensionality reduction, and offer desirable properties such as uncertainty\nestimates, robustness to over-fitting, and principled ways for tuning\nhyper-parameters. However the scalability of these models to big datasets\nremains an active topic of research. We introduce a novel re-parametrisation of\nvariational inference for sparse GP regression and latent variable models that\nallows for an efficient distributed algorithm. This is done by exploiting the\ndecoupling of the data given the inducing points to re-formulate the evidence\nlower bound in a Map-Reduce setting. We show that the inference scales well\nwith data and computational resources, while preserving a balanced distribution\nof the load among the nodes. We further demonstrate the utility in scaling\nGaussian processes to big data. We show that GP performance improves with\nincreasing amounts of data in regression (on flight data with 2 million\nrecords) and latent variable modelling (on MNIST). The results show that GPs\nperform better than many common models often used for big data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 16:08:40 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 21:16:47 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Gal", "Yarin", ""], ["van der Wilk", "Mark", ""], ["Rasmussen", "Carl E.", ""]]}, {"id": "1402.1454", "submitter": "Sarath Chandar A P", "authors": "Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh M.\n  Khapra, Balaraman Ravindran, Vikas Raykar, Amrita Saha", "title": "An Autoencoder Approach to Learning Bilingual Word Representations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-language learning allows us to use training data from one language to\nbuild models for a different language. Many approaches to bilingual learning\nrequire that we have word-level alignment of sentences from parallel corpora.\nIn this work we explore the use of autoencoder-based methods for cross-language\nlearning of vectorial word representations that are aligned between two\nlanguages, while not relying on word-level alignments. We show that by simply\nlearning to reconstruct the bag-of-words representations of aligned sentences,\nwithin and between languages, we can in fact learn high-quality representations\nand do without word alignments. Since training autoencoders on word\nobservations presents certain computational issues, we propose and compare\ndifferent variations adapted to this setting. We also propose an explicit\ncorrelation maximizing regularizer that leads to significant improvement in the\nperformance. We empirically investigate the success of our approach on the\nproblem of cross-language test classification, where a classifier trained on a\ngiven language (e.g., English) must learn to generalize to a different language\n(e.g., German). These experiments demonstrate that our approaches are\ncompetitive with the state-of-the-art, achieving up to 10-14 percentage point\nimprovements over the best reported results on this task.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 18:53:30 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["P", "Sarath Chandar A", ""], ["Lauly", "Stanislas", ""], ["Larochelle", "Hugo", ""], ["Khapra", "Mitesh M.", ""], ["Ravindran", "Balaraman", ""], ["Raykar", "Vikas", ""], ["Saha", "Amrita", ""]]}, {"id": "1402.1473", "submitter": "Yuxin Chen", "authors": "Yuxin Chen and Leonidas J. Guibas and Qi-Xing Huang", "title": "Near-Optimal Joint Object Matching via Convex Relaxation", "comments": null, "journal-ref": "31st International Conference on Machine Learning, vol. 32, pp.\n  100 - 108, June 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint matching over a collection of objects aims at aggregating information\nfrom a large collection of similar instances (e.g. images, graphs, shapes) to\nimprove maps between pairs of them. Given multiple matches computed between a\nfew object pairs in isolation, the goal is to recover an entire collection of\nmaps that are (1) globally consistent, and (2) close to the provided maps ---\nand under certain conditions provably the ground-truth maps. Despite recent\nadvances on this problem, the best-known recovery guarantees are limited to a\nsmall constant barrier --- none of the existing methods find theoretical\nsupport when more than $50\\%$ of input correspondences are corrupted. Moreover,\nprior approaches focus mostly on fully similar objects, while it is practically\nmore demanding to match instances that are only partially similar to each\nother.\n  In this paper, we develop an algorithm to jointly match multiple objects that\nexhibit only partial similarities, given a few pairwise matches that are\ndensely corrupted. Specifically, we propose to recover the ground-truth maps\nvia a parameter-free convex program called MatchLift, following a spectral\nmethod that pre-estimates the total number of distinct elements to be matched.\nEncouragingly, MatchLift exhibits near-optimal error-correction ability, i.e.\nin the asymptotic regime it is guaranteed to work even when a dominant fraction\n$1-\\Theta\\left(\\frac{\\log^{2}n}{\\sqrt{n}}\\right)$ of the input maps behave like\nrandom outliers. Furthermore, MatchLift succeeds with minimal input complexity,\nnamely, perfect matching can be achieved as soon as the provided maps form a\nconnected map graph. We evaluate the proposed algorithm on various benchmark\ndata sets including synthetic examples and real-world examples, all of which\nconfirm the practical applicability of MatchLift.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 20:16:35 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Chen", "Yuxin", ""], ["Guibas", "Leonidas J.", ""], ["Huang", "Qi-Xing", ""]]}, {"id": "1402.1515", "submitter": "Jianshu Chen", "authors": "Jianshu Chen, Zaid J. Towfic, Ali H. Sayed", "title": "Dictionary Learning over Distributed Models", "comments": "16 pages, 8 figures. To appear in IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2014.2385045", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider learning dictionary models over a network of\nagents, where each agent is only in charge of a portion of the dictionary\nelements. This formulation is relevant in Big Data scenarios where large\ndictionary models may be spread over different spatial locations and it is not\nfeasible to aggregate all dictionaries in one location due to communication and\nprivacy considerations. We first show that the dual function of the inference\nproblem is an aggregation of individual cost functions associated with\ndifferent agents, which can then be minimized efficiently by means of diffusion\nstrategies. The collaborative inference step generates dual variables that are\nused by the agents to update their dictionaries without the need to share these\ndictionaries or even the coefficient models for the training data. This is a\npowerful property that leads to an effective distributed procedure for learning\ndictionaries over large networks (e.g., hundreds of agents in our experiments).\nFurthermore, the proposed learning strategy operates in an online manner and is\nable to respond to streaming data, where each data sample is presented to the\nnetwork once.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 22:19:19 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2014 05:40:44 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chen", "Jianshu", ""], ["Towfic", "Zaid J.", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1402.1526", "submitter": "Justin Hsu", "authors": "Marco Gaboardi, Emilio Jes\\'us Gallego Arias, Justin Hsu, Aaron Roth,\n  Zhiwei Steven Wu", "title": "Dual Query: Practical Private Query Release for High Dimensional Data", "comments": null, "journal-ref": "Journal of Privacy and Confidentiality 7(2) 53--77 (2017)", "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical, differentially private algorithm for answering a\nlarge number of queries on high dimensional datasets. Like all algorithms for\nthis task, ours necessarily has worst-case complexity exponential in the\ndimension of the data. However, our algorithm packages the computationally hard\nstep into a concisely defined integer program, which can be solved\nnon-privately using standard solvers. We prove accuracy and privacy theorems\nfor our algorithm, and then demonstrate experimentally that our algorithm\nperforms well in practice. For example, our algorithm can efficiently and\naccurately answer millions of queries on the Netflix dataset, which has over\n17,000 attributes; this is an improvement on the state of the art by multiple\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 23:20:43 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 04:36:00 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Gaboardi", "Marco", ""], ["Arias", "Emilio Jes\u00fas Gallego", ""], ["Hsu", "Justin", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1402.1754", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo, Arthur Gretton, Barnabas Poczos, Bharath Sriperumbudur", "title": "Two-stage Sampled Learning Theory on Distributions", "comments": "v6: accepted at AISTATS-2015 for oral presentation; final version;\n  code: https://bitbucket.org/szzoli/ite/; extension to the misspecified and\n  vector-valued case: http://arxiv.org/abs/1411.2066", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the distribution regression problem: regressing to a real-valued\nresponse from a probability distribution. Although there exist a large number\nof similarity measures between distributions, very little is known about their\ngeneralization performance in specific learning tasks. Learning problems\nformulated on distributions have an inherent two-stage sampled difficulty: in\npractice only samples from sampled distributions are observable, and one has to\nbuild an estimate on similarities computed between sets of points. To the best\nof our knowledge, the only existing method with consistency guarantees for\ndistribution regression requires kernel density estimation as an intermediate\nstep (which suffers from slow convergence issues in high dimensions), and the\ndomain of the distributions to be compact Euclidean. In this paper, we provide\ntheoretical guarantees for a remarkably simple algorithmic alternative to solve\nthe distribution regression problem: embed the distributions to a reproducing\nkernel Hilbert space, and learn a ridge regressor from the embeddings to the\noutputs. Our main contribution is to prove the consistency of this technique in\nthe two-stage sampled setting under mild conditions (on separable, topological\ndomains endowed with kernels). For a given total number of observations, we\nderive convergence rates as an explicit function of the problem difficulty. As\na special case, we answer a 15-year-old open question: we establish the\nconsistency of the classical set kernel [Haussler, 1999; Gartner et. al, 2002]\nin regression, and cover more recent kernels on distributions, including those\ndue to [Christmann and Steinwart, 2010].\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 20:37:59 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 11:35:58 GMT"}, {"version": "v3", "created": "Sun, 4 May 2014 19:29:36 GMT"}, {"version": "v4", "created": "Sat, 7 Jun 2014 17:42:06 GMT"}, {"version": "v5", "created": "Sat, 25 Oct 2014 21:03:01 GMT"}, {"version": "v6", "created": "Mon, 26 Jan 2015 22:20:59 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Szabo", "Zoltan", ""], ["Gretton", "Arthur", ""], ["Poczos", "Barnabas", ""], ["Sriperumbudur", "Bharath", ""]]}, {"id": "1402.1783", "submitter": "Jason J Corso", "authors": "Caiming Xiong, David Johnson, Jason J. Corso", "title": "Active Clustering with Model-Based Uncertainty Reduction", "comments": "14 pages, 8 figures, submitted to TPAMI (second version just fixes a\n  missing reference and format)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised clustering seeks to augment traditional clustering methods by\nincorporating side information provided via human expertise in order to\nincrease the semantic meaningfulness of the resulting clusters. However, most\ncurrent methods are \\emph{passive} in the sense that the side information is\nprovided beforehand and selected randomly. This may require a large number of\nconstraints, some of which could be redundant, unnecessary, or even detrimental\nto the clustering results. Thus in order to scale such semi-supervised\nalgorithms to larger problems it is desirable to pursue an \\emph{active}\nclustering method---i.e. an algorithm that maximizes the effectiveness of the\navailable human labor by only requesting human input where it will have the\ngreatest impact. Here, we propose a novel online framework for active\nsemi-supervised spectral clustering that selects pairwise constraints as\nclustering proceeds, based on the principle of uncertainty reduction. Using a\nfirst-order Taylor expansion, we decompose the expected uncertainty reduction\nproblem into a gradient and a step-scale, computed via an application of matrix\nperturbation theory and cluster-assignment entropy, respectively. The resulting\nmodel is used to estimate the uncertainty reduction potential of each sample in\nthe dataset. We then present the human user with pairwise queries with respect\nto only the best candidate sample. We evaluate our method using three different\nimage datasets (faces, leaves and dogs), a set of common UCI machine learning\ndatasets and a gene dataset. The results validate our decomposition formulation\nand show that our method is consistently superior to existing state-of-the-art\ntechniques, as well as being robust to noise and to unknown numbers of\nclusters.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 22:13:03 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 02:53:32 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Xiong", "Caiming", ""], ["Johnson", "David", ""], ["Corso", "Jason J.", ""]]}, {"id": "1402.1792", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi, Lijun Zhang, and Rong Jin", "title": "Binary Excess Risk for Smooth Convex Surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical learning theory, convex surrogates of the 0-1 loss are highly\npreferred because of the computational and theoretical virtues that convexity\nbrings in. This is of more importance if we consider smooth surrogates as\nwitnessed by the fact that the smoothness is further beneficial both\ncomputationally- by attaining an {\\it optimal} convergence rate for\noptimization, and in a statistical sense- by providing an improved {\\it\noptimistic} rate for generalization bound. In this paper we investigate the\nsmoothness property from the viewpoint of statistical consistency and show how\nit affects the binary excess risk. We show that in contrast to optimization and\ngeneralization errors that favor the choice of smooth surrogate loss, the\nsmoothness of loss function may degrade the binary excess risk. Motivated by\nthis negative result, we provide a unified analysis that integrates\noptimization error, generalization bound, and the error in translating convex\nexcess risk into a binary excess risk when examining the impact of smoothness\non the binary excess risk. We show that under favorable conditions appropriate\nchoice of smooth convex loss will result in a binary excess risk that is better\nthan $O(1/\\sqrt{n})$.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 23:02:50 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Mahdavi", "Mehrdad", ""], ["Zhang", "Lijun", ""], ["Jin", "Rong", ""]]}, {"id": "1402.1864", "submitter": "Massimiliano Pontil", "authors": "Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes", "title": "An Inequality with Applications to Structured Sparsity and Multitask\n  Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From concentration inequalities for the suprema of Gaussian or Rademacher\nprocesses an inequality is derived. It is applied to sharpen existing and to\nderive novel bounds on the empirical Rademacher complexities of unit balls in\nvarious norms appearing in the context of structured sparsity and multitask\ndictionary learning or matrix factorization. A key role is played by the\nlargest eigenvalue of the data covariance matrix.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 16:39:30 GMT"}, {"version": "v2", "created": "Sat, 7 Jun 2014 07:37:52 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""], ["Romera-Paredes", "Bernardino", ""]]}, {"id": "1402.1869", "submitter": "KyungHyun Cho", "authors": "Guido Mont\\'ufar, Razvan Pascanu, Kyunghyun Cho and Yoshua Bengio", "title": "On the Number of Linear Regions of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of functions computable by deep feedforward neural\nnetworks with piecewise linear activations in terms of the symmetries and the\nnumber of linear regions that they have. Deep networks are able to sequentially\nmap portions of each layer's input-space to the same output. In this way, deep\nmodels compute functions that react equally to complicated patterns of\ndifferent inputs. The compositional structure of these functions enables them\nto re-use pieces of computation exponentially often in terms of the network's\ndepth. This paper investigates the complexity of such compositional maps and\ncontributes new theoretical results regarding the advantage of depth for neural\nnetworks with piecewise linear activation functions. In particular, our\nanalysis is not specific to a single family of models, and as an example, we\nemploy it for rectifier and maxout networks. We improve complexity bounds from\npre-existing work and investigate the behavior of units in higher layers.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 17:16:27 GMT"}, {"version": "v2", "created": "Sat, 7 Jun 2014 19:56:14 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Mont\u00fafar", "Guido", ""], ["Pascanu", "Razvan", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1402.1892", "submitter": "Zachary Lipton", "authors": "Zachary Chase Lipton, Charles Elkan, Balakrishnan Narayanaswamy", "title": "Thresholding Classifiers to Maximize F1 Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides new insight into maximizing F1 scores in the context of\nbinary classification and also in the context of multilabel classification. The\nharmonic mean of precision and recall, F1 score is widely used to measure the\nsuccess of a binary classifier when one class is rare. Micro average, macro\naverage, and per instance average F1 scores are used in multilabel\nclassification. For any classifier that produces a real-valued output, we\nderive the relationship between the best achievable F1 score and the\ndecision-making threshold that achieves this optimum. As a special case, if the\nclassifier outputs are well-calibrated conditional probabilities, then the\noptimal threshold is half the optimal F1 score. As another special case, if the\nclassifier is completely uninformative, then the optimal behavior is to\nclassify all examples as positive. Since the actual prevalence of positive\nexamples typically is low, this behavior can be considered undesirable. As a\ncase study, we discuss the results, which can be surprising, of applying this\nprocedure when predicting 26,853 labels for Medline documents.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 21:14:29 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 01:29:47 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Lipton", "Zachary Chase", ""], ["Elkan", "Charles", ""], ["Narayanaswamy", "Balakrishnan", ""]]}, {"id": "1402.1921", "submitter": "Zhenhua Wang", "authors": "Qinfeng Shi, Mark Reid, Tiberio Caetano, Anton van den Hengel and\n  Zhenhua Wang", "title": "A Hybrid Loss for Multiclass and Structured Prediction", "comments": "12 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1009.3346", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hybrid loss for multiclass and structured prediction\nproblems that is a convex combination of a log loss for Conditional Random\nFields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs).\nWe provide a sufficient condition for when the hybrid loss is Fisher consistent\nfor classification. This condition depends on a measure of dominance between\nlabels--specifically, the gap between the probabilities of the best label and\nthe second best label. We also prove Fisher consistency is necessary for\nparametric consistency when learning models such as CRFs. We demonstrate\nempirically that the hybrid loss typically performs least as well as--and often\nbetter than--both of its constituent losses on a variety of tasks, such as\nhuman action recognition. In doing so we also provide an empirical comparison\nof the efficacy of probabilistic and margin based approaches to multiclass and\nstructured prediction.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 06:47:17 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Shi", "Qinfeng", ""], ["Reid", "Mark", ""], ["Caetano", "Tiberio", ""], ["Hengel", "Anton van den", ""], ["Wang", "Zhenhua", ""]]}, {"id": "1402.1947", "submitter": "Farrukh Arslan", "authors": "Farrukh Arslan", "title": "Classification Tree Diagrams in Health Informatics Applications", "comments": "In the Proceedings of 7th International Conference on the Theory and\n  Application of Diagrams 2012. 7th International Conference on the Theory and\n  Application of Diagrams 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health informatics deal with the methods used to optimize the acquisition,\nstorage and retrieval of medical data, and classify information in healthcare\napplications. Healthcare analysts are particularly interested in various\ncomputer informatics areas such as; knowledge representation from data, anomaly\ndetection, outbreak detection methods and syndromic surveillance applications.\nAlthough various parametric and non-parametric approaches are being proposed to\nclassify information from data, classification tree diagrams provide an\ninteractive visualization to analysts as compared to other methods. In this\nwork we discuss application of classification tree diagrams to classify\ninformation from medical data in healthcare applications.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 13:02:51 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Arslan", "Farrukh", ""]]}, {"id": "1402.1958", "submitter": "Arthur Guez", "authors": "Arthur Guez, David Silver, Peter Dayan", "title": "Better Optimism By Bayes: Adaptive Planning with Rich Models", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational costs of inference and planning have confined Bayesian\nmodel-based reinforcement learning to one of two dismal fates: powerful\nBayes-adaptive planning but only for simplistic models, or powerful, Bayesian\nnon-parametric models but using simple, myopic planning strategies such as\nThompson sampling. We ask whether it is feasible and truly beneficial to\ncombine rich probabilistic models with a closer approximation to fully Bayesian\nplanning. First, we use a collection of counterexamples to show formal problems\nwith the over-optimism inherent in Thompson sampling. Then we leverage\nstate-of-the-art techniques in efficient Bayes-adaptive planning and\nnon-parametric Bayesian methods to perform qualitatively better than both\nexisting conventional algorithms and Thompson sampling on two contextual\nbandit-like problems.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 15:38:57 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Guez", "Arthur", ""], ["Silver", "David", ""], ["Dayan", "Peter", ""]]}, {"id": "1402.1973", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Mike Davies, Pascal Frossard", "title": "Dictionary learning for fast classification based on soft-thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers based on sparse representations have recently been shown to\nprovide excellent results in many visual recognition and classification tasks.\nHowever, the high cost of computing sparse representations at test time is a\nmajor obstacle that limits the applicability of these methods in large-scale\nproblems, or in scenarios where computational power is restricted. We consider\nin this paper a simple yet efficient alternative to sparse coding for feature\nextraction. We study a classification scheme that applies the soft-thresholding\nnonlinear mapping in a dictionary, followed by a linear classifier. A novel\nsupervised dictionary learning algorithm tailored for this low complexity\nclassification architecture is proposed. The dictionary learning problem, which\njointly learns the dictionary and linear classifier, is cast as a difference of\nconvex (DC) program and solved efficiently with an iterative DC solver. We\nconduct experiments on several datasets, and show that our learning algorithm\nthat leverages the structure of the classification problem outperforms generic\nlearning procedures. Our simple classifier based on soft-thresholding also\ncompetes with the recent sparse coding classifiers, when the dictionary is\nlearned appropriately. The adopted classification scheme further requires less\ncomputational time at the testing stage, compared to other classifiers. The\nproposed scheme shows the potential of the adequately trained soft-thresholding\nmapping for classification and paves the way towards the development of very\nefficient classification methods for vision problems.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 18:18:33 GMT"}, {"version": "v2", "created": "Thu, 2 Oct 2014 16:45:19 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Davies", "Mike", ""], ["Frossard", "Pascal", ""]]}, {"id": "1402.2031", "submitter": "Hong Chang", "authors": "Wen Wang, Zhen Cui, Hong Chang, Shiguang Shan, Xilin Chen", "title": "Deeply Coupled Auto-encoder Networks for Cross-view Classification", "comments": "11 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The comparison of heterogeneous samples extensively exists in many\napplications, especially in the task of image classification. In this paper, we\npropose a simple but effective coupled neural network, called Deeply Coupled\nAutoencoder Networks (DCAN), which seeks to build two deep neural networks,\ncoupled with each other in every corresponding layers. In DCAN, each deep\nstructure is developed via stacking multiple discriminative coupled\nauto-encoders, a denoising auto-encoder trained with maximum margin criterion\nconsisting of intra-class compactness and inter-class penalty. This single\nlayer component makes our model simultaneously preserve the local consistency\nand enhance its discriminative capability. With increasing number of layers,\nthe coupled networks can gradually narrow the gap between the two views.\nExtensive experiments on cross-view image classification tasks demonstrate the\nsuperiority of our method over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 04:15:23 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Wang", "Wen", ""], ["Cui", "Zhen", ""], ["Chang", "Hong", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1402.2043", "submitter": "Gilles Stoltz", "authors": "Shie Mannor (EE-Technion), Vianney Perchet, Gilles Stoltz (GREGH)", "title": "Approachability in unknown games: Online learning meets multi-objective\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the standard setting of approachability there are two players and a target\nset. The players play repeatedly a known vector-valued game where the first\nplayer wants to have the average vector-valued payoff converge to the target\nset which the other player tries to exclude it from this set. We revisit this\nsetting in the spirit of online learning and do not assume that the first\nplayer knows the game structure: she receives an arbitrary vector-valued reward\nvector at every round. She wishes to approach the smallest (\"best\") possible\nset given the observed average payoffs in hindsight. This extension of the\nstandard setting has implications even when the original target set is not\napproachable and when it is not obvious which expansion of it should be\napproached instead. We show that it is impossible, in general, to approach the\nbest target set in hindsight and propose achievable though ambitious\nalternative goals. We further propose a concrete strategy to approach these\ngoals. Our method does not require projection onto a target set and amounts to\nswitching between scalar regret minimization algorithms that are performed in\nepisodes. Applications to global cost minimization and to approachability under\nsample path constraints are considered.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 05:44:40 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 06:52:49 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Mannor", "Shie", "", "EE-Technion"], ["Perchet", "Vianney", "", "GREGH"], ["Stoltz", "Gilles", "", "GREGH"]]}, {"id": "1402.2044", "submitter": "Gilles Stoltz", "authors": "Pierre Gaillard (GREGH), Gilles Stoltz (GREGH), Tim Van Erven (INRIA\n  Saclay - Ile de France)", "title": "A Second-order Bound with Excess Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online aggregation of the predictions of experts, and first show new\nsecond-order regret bounds in the standard setting, which are obtained via a\nversion of the Prod algorithm (and also a version of the polynomially weighted\naverage algorithm) with multiple learning rates. These bounds are in terms of\nexcess losses, the differences between the instantaneous losses suffered by the\nalgorithm and the ones of a given expert. We then demonstrate the interest of\nthese bounds in the context of experts that report their confidences as a\nnumber in the interval [0,1] using a generic reduction to the standard setting.\nWe conclude by two other applications in the standard setting, which improve\nthe known bounds in case of small excess losses and show a bounded regret\nagainst i.i.d. sequences of losses.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 05:45:29 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Gaillard", "Pierre", "", "GREGH"], ["Stoltz", "Gilles", "", "GREGH"], ["Van Erven", "Tim", "", "INRIA\n  Saclay - Ile de France"]]}, {"id": "1402.2058", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig", "title": "Probabilistic Interpretation of Linear Solvers", "comments": "final version, in press at SIAM J Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript proposes a probabilistic framework for algorithms that\niteratively solve unconstrained linear problems $Bx = b$ with positive definite\n$B$ for $x$. The goal is to replace the point estimates returned by existing\nmethods with a Gaussian posterior belief over the elements of the inverse of\n$B$, which can be used to estimate errors. Recent probabilistic interpretations\nof the secant family of quasi-Newton optimization algorithms are extended.\nCombined with properties of the conjugate gradient algorithm, this leads to\nuncertainty-calibrated methods with very limited cost overhead over conjugate\ngradients, a self-contained novel interpretation of the quasi-Newton and\nconjugate gradient algorithms, and a foundation for new nonlinear optimization\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 07:56:13 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 08:23:52 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Hennig", "Philipp", ""]]}, {"id": "1402.2092", "submitter": "Adish Singla", "authors": "Adish Singla, Ilija Bogunovic, G\\'abor Bart\\'ok, Amin Karbasi, and\n  Andreas Krause", "title": "Near-Optimally Teaching the Crowd to Classify", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should we present training examples to learners to teach them\nclassification rules? This is a natural problem when training workers for\ncrowdsourcing labeling tasks, and is also motivated by challenges in\ndata-driven online education. We propose a natural stochastic model of the\nlearners, modeling them as randomly switching among hypotheses based on\nobserved feedback. We then develop STRICT, an efficient algorithm for selecting\nexamples to teach to workers. Our solution greedily maximizes a submodular\nsurrogate objective function in order to select examples to show to the\nlearners. We prove that our strategy is competitive with the optimal teaching\npolicy. Moreover, for the special case of linear separators, we prove that an\nexponential reduction in error probability can be achieved. Our experiments on\nsimulated workers as well as three real image annotation tasks on Amazon\nMechanical Turk show the effectiveness of our teaching algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 10:36:49 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2014 11:24:42 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2014 12:25:01 GMT"}, {"version": "v4", "created": "Fri, 7 Mar 2014 19:38:37 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Singla", "Adish", ""], ["Bogunovic", "Ilija", ""], ["Bart\u00f3k", "G\u00e1bor", ""], ["Karbasi", "Amin", ""], ["Krause", "Andreas", ""]]}, {"id": "1402.2224", "submitter": "Uri Stemmer", "authors": "Amos Beimel, Kobbi Nissim, Uri Stemmer", "title": "Characterizing the Sample Complexity of Private Learners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2008, Kasiviswanathan et al. defined private learning as a combination of\nPAC learning and differential privacy. Informally, a private learner is applied\nto a collection of labeled individual information and outputs a hypothesis\nwhile preserving the privacy of each individual. Kasiviswanathan et al. gave a\ngeneric construction of private learners for (finite) concept classes, with\nsample complexity logarithmic in the size of the concept class. This sample\ncomplexity is higher than what is needed for non-private learners, hence\nleaving open the possibility that the sample complexity of private learning may\nbe sometimes significantly higher than that of non-private learning.\n  We give a combinatorial characterization of the sample size sufficient and\nnecessary to privately learn a class of concepts. This characterization is\nanalogous to the well known characterization of the sample complexity of\nnon-private learning in terms of the VC dimension of the concept class. We\nintroduce the notion of probabilistic representation of a concept class, and\nour new complexity measure RepDim corresponds to the size of the smallest\nprobabilistic representation of the concept class.\n  We show that any private learning algorithm for a concept class C with sample\ncomplexity m implies RepDim(C)=O(m), and that there exists a private learning\nalgorithm with sample complexity m=O(RepDim(C)). We further demonstrate that a\nsimilar characterization holds for the database size needed for privately\ncomputing a large class of optimization problems and also for the well studied\nproblem of private data release.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 17:45:00 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Beimel", "Amos", ""], ["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""]]}, {"id": "1402.2300", "submitter": "Aaron Karper", "authors": "Aaron Karper", "title": "Feature and Variable Selection in Classification", "comments": "Part of master seminar in document analysis held by Marcus\n  Eichenberger-Liwicki", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The amount of information in the form of features and variables avail- able\nto machine learning algorithms is ever increasing. This can lead to classifiers\nthat are prone to overfitting in high dimensions, high di- mensional models do\nnot lend themselves to interpretable results, and the CPU and memory resources\nnecessary to run on high-dimensional datasets severly limit the applications of\nthe approaches. Variable and feature selection aim to remedy this by finding a\nsubset of features that in some way captures the information provided best. In\nthis paper we present the general methodology and highlight some specific\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 21:05:58 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Karper", "Aaron", ""]]}, {"id": "1402.2324", "submitter": "Srinadh Bhojanapalli", "authors": "Srinadh Bhojanapalli, Prateek Jain", "title": "Universal Matrix Completion", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of low-rank matrix completion has recently generated a lot of\ninterest leading to several results that offer exact solutions to the problem.\nHowever, in order to do so, these methods make assumptions that can be quite\nrestrictive in practice. More specifically, the methods assume that: a) the\nobserved indices are sampled uniformly at random, and b) for every new matrix,\nthe observed indices are sampled afresh. In this work, we address these issues\nby providing a universal recovery guarantee for matrix completion that works\nfor a variety of sampling schemes. In particular, we show that if the set of\nsampled indices come from the edges of a bipartite graph with large spectral\ngap (i.e. gap between the first and the second singular value), then the\nnuclear norm minimization based method exactly recovers all low-rank matrices\nthat satisfy certain incoherence properties. Moreover, we also show that under\ncertain stricter incoherence conditions, $O(nr^2)$ uniformly sampled entries\nare enough to recover any rank-$r$ $n\\times n$ matrix, in contrast to the\n$O(nr\\log n)$ sample complexity required by other matrix completion algorithms\nas well as existing analyses of the nuclear norm method.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 22:53:15 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 15:21:56 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Jain", "Prateek", ""]]}, {"id": "1402.2331", "submitter": "Prasad Raghavendra", "authors": "Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz", "title": "Computational Limits for Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix Completion is the problem of recovering an unknown real-valued\nlow-rank matrix from a subsample of its entries. Important recent results show\nthat the problem can be solved efficiently under the assumption that the\nunknown matrix is incoherent and the subsample is drawn uniformly at random.\nAre these assumptions necessary?\n  It is well known that Matrix Completion in its full generality is NP-hard.\nHowever, little is known if make additional assumptions such as incoherence and\npermit the algorithm to output a matrix of slightly higher rank. In this paper\nwe prove that Matrix Completion remains computationally intractable even if the\nunknown matrix has rank $4$ but we are allowed to output any constant rank\nmatrix, and even if additionally we assume that the unknown matrix is\nincoherent and are shown $90%$ of the entries. This result relies on the\nconjectured hardness of the $4$-Coloring problem. We also consider the positive\nsemidefinite Matrix Completion problem. Here we show a similar hardness result\nunder the standard assumption that $\\mathrm{P}\\ne \\mathrm{NP}.$\n  Our results greatly narrow the gap between existing feasibility results and\ncomputational lower bounds. In particular, we believe that our results give the\nfirst complexity-theoretic justification for why distributional assumptions are\nneeded beyond the incoherence assumption in order to obtain positive results.\nOn the technical side, we contribute several new ideas on how to encode hard\ncombinatorial problems in low-rank optimization problems. We hope that these\ntechniques will be helpful in further understanding the computational limits of\nMatrix Completion and related problems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 23:43:11 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 07:30:37 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Hardt", "Moritz", ""], ["Meka", "Raghu", ""], ["Raghavendra", "Prasad", ""], ["Weitz", "Benjamin", ""]]}, {"id": "1402.2333", "submitter": "Vincent Michalski", "authors": "Vincent Michalski, Roland Memisevic, Kishore Konda", "title": "Modeling sequential data using higher-order relational features and\n  predictive training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bi-linear feature learning models, like the gated autoencoder, were proposed\nas a way to model relationships between frames in a video. By minimizing\nreconstruction error of one frame, given the previous frame, these models learn\n\"mapping units\" that encode the transformations inherent in a sequence, and\nthereby learn to encode motion. In this work we extend bi-linear models by\nintroducing \"higher-order mapping units\" that allow us to encode\ntransformations between frames and transformations between transformations.\n  We show that this makes it possible to encode temporal structure that is more\ncomplex and longer-range than the structure captured within standard bi-linear\nmodels. We also show that a natural way to train the model is by replacing the\ncommonly used reconstruction objective with a prediction objective which forces\nthe model to correctly predict the evolution of the input multiple steps into\nthe future. Learning can be achieved by back-propagating the multi-step\nprediction through time. We test the model on various temporal prediction\ntasks, and show that higher-order mappings and predictive training both yield a\nsignificant improvement over bi-linear models in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 23:53:29 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Michalski", "Vincent", ""], ["Memisevic", "Roland", ""], ["Konda", "Kishore", ""]]}, {"id": "1402.2359", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk, Josef Urban, Ji\\v{r}\\'i Vysko\\v{c}il", "title": "Machine Learner for Automated Reasoning 0.4 and 0.5", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learner for Automated Reasoning (MaLARea) is a learning and reasoning\nsystem for proving in large formal libraries where thousands of theorems are\navailable when attacking a new conjecture, and a large number of related\nproblems and proofs can be used to learn specific theorem-proving knowledge.\nThe last version of the system has by a large margin won the 2013 CASC LTB\ncompetition. This paper describes the motivation behind the methods used in\nMaLARea, discusses the general approach and the issues arising in evaluation of\nsuch system, and describes the Mizar@Turing100 and CASC'24 versions of MaLARea.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 03:42:00 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 13:51:17 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""], ["Vysko\u010dil", "Ji\u0159\u00ed", ""]]}, {"id": "1402.2447", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer, Albert Swart and David van Leeuwen", "title": "A comparison of linear and non-linear calibrations for speaker\n  recognition", "comments": "accepted for Odyssey 2014: The Speaker and Language Recognition\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work on both generative and discriminative score to\nlog-likelihood-ratio calibration, it was shown that linear transforms give good\naccuracy only for a limited range of operating points. Moreover, these methods\nrequired tailoring of the calibration training objective functions in order to\ntarget the desired region of best accuracy. Here, we generalize the linear\nrecipes to non-linear ones. We experiment with a non-linear, non-parametric,\ndiscriminative PAV solution, as well as parametric, generative,\nmaximum-likelihood solutions that use Gaussian, Student's T and\nnormal-inverse-Gaussian score distributions. Experiments on NIST SRE'12 scores\nsuggest that the non-linear methods provide wider ranges of optimal accuracy\nand can be trained without having to resort to objective function tailoring.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 11:13:51 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 10:49:48 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["Swart", "Albert", ""], ["van Leeuwen", "David", ""]]}, {"id": "1402.2594", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan", "title": "Online Nonparametric Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish optimal rates for online regression for arbitrary classes of\nregression functions in terms of the sequential entropy introduced in (Rakhlin,\nSridharan, Tewari, 2010). The optimal rates are shown to exhibit a phase\ntransition analogous to the i.i.d./statistical learning case, studied in\n(Rakhlin, Sridharan, Tsybakov 2013). In the frequently encountered situation\nwhen sequential entropy and i.i.d. empirical entropy match, our results point\nto the interesting phenomenon that the rates for statistical learning with\nsquared loss and online nonparametric regression are the same.\n  In addition to a non-algorithmic study of minimax regret, we exhibit a\ngeneric forecaster that enjoys the established optimal rates. We also provide a\nrecipe for designing online regression algorithms that can be computationally\nefficient. We illustrate the techniques by deriving existing and new\nforecasters for the case of finite experts and for online linear regression.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 18:36:11 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1402.2667", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang, Hariharan Narayanan and Alexander Rakhlin", "title": "On Zeroth-Order Stochastic Convex Optimization via Random Walks", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for zeroth order stochastic convex optimization that\nattains the suboptimality rate of $\\tilde{\\mathcal{O}}(n^{7}T^{-1/2})$ after\n$T$ queries for a convex bounded function $f:{\\mathbb R}^n\\to{\\mathbb R}$. The\nmethod is based on a random walk (the \\emph{Ball Walk}) on the epigraph of the\nfunction. The randomized approach circumvents the problem of gradient\nestimation, and appears to be less sensitive to noisy function evaluations\ncompared to noiseless zeroth order methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:18:11 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Liang", "Tengyuan", ""], ["Narayanan", "Hariharan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1402.2676", "submitter": "Parameswaran Raman", "authors": "Hyokun Yun, Parameswaran Raman, S.V.N. Vishwanathan", "title": "Ranking via Robust Binary Classification and Parallel Parameter\n  Estimation in Large-Scale Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RoBiRank, a ranking algorithm that is motivated by observing a\nclose connection between evaluation metrics for learning to rank and loss\nfunctions for robust classification. The algorithm shows a very competitive\nperformance on standard benchmark datasets against other representative\nalgorithms in the literature. On the other hand, in large scale problems where\nexplicit feature vectors and scores are not given, our algorithm can be\nefficiently parallelized across a large number of machines; for a task that\nrequires 386,133 x 49,824,519 pairwise interactions between items to be ranked,\nour algorithm finds solutions that are of dramatically higher quality than that\ncan be found by a state-of-the-art competitor algorithm, given the same amount\nof wall-clock time for computation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:39:54 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 21:08:34 GMT"}, {"version": "v3", "created": "Fri, 11 Apr 2014 06:19:04 GMT"}, {"version": "v4", "created": "Thu, 21 Aug 2014 06:00:32 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Yun", "Hyokun", ""], ["Raman", "Parameswaran", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1402.3032", "submitter": "Ziming Zhang", "authors": "Ziming Zhang", "title": "Regularization for Multiple Kernel Learning via Sum-Product Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in constructing general graph-based\nregularizers for multiple kernel learning (MKL) given a structure which is used\nto describe the way of combining basis kernels. Such structures are represented\nby sum-product networks (SPNs) in our method. Accordingly we propose a new\nconvex regularization method for MLK based on a path-dependent kernel weighting\nfunction which encodes the entire SPN structure in our method. Under certain\nconditions and from the view of probability, this function can be considered to\nfollow multinomial distributions over the weights associated with product nodes\nin SPNs. We also analyze the convexity of our regularizer and the complexity of\nour induced classifiers, and further propose an efficient wrapper algorithm to\noptimize our formulation. In our experiments, we apply our method to ......\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 05:06:53 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Zhang", "Ziming", ""]]}, {"id": "1402.3070", "submitter": "Parth Gupta", "authors": "Parth Gupta, Rafael E. Banchs and Paolo Rosso", "title": "Squeezing bottlenecks: exploring the limits of autoencoder semantic\n  representation capabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study on the use of autoencoders for modelling\ntext data, in which (differently from previous studies) we focus our attention\non the following issues: i) we explore the suitability of two different models\nbDA and rsDA for constructing deep autoencoders for text data at the sentence\nlevel; ii) we propose and evaluate two novel metrics for better assessing the\ntext-reconstruction capabilities of autoencoders; and iii) we propose an\nautomatic method to find the critical bottleneck dimensionality for text\nlanguage representations (below which structural information is lost).\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 09:54:01 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Gupta", "Parth", ""], ["Banchs", "Rafael E.", ""], ["Rosso", "Paolo", ""]]}, {"id": "1402.3144", "submitter": "Marc Claesen", "authors": "Marc Claesen, Frank De Smet, Johan A. K. Suykens, Bart De Moor", "title": "A Robust Ensemble Approach to Learn From Positive and Unlabeled Data\n  Using SVM Base Models", "comments": "34 pages, 6 figures, 6 tables. Accepted for publication in\n  Neurocomputing: Special Issue on Advances in Learning with Label Noise", "journal-ref": null, "doi": "10.1016/j.neucom.2014.10.081", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to learn binary classifiers when only positive\nand unlabeled instances are available (PU learning). This problem is routinely\ncast as a supervised task with label noise in the negative set. We use an\nensemble of SVM models trained on bootstrap resamples of the training data for\nincreased robustness against label noise. The approach can be considered in a\nbagging framework which provides an intuitive explanation for its mechanics in\na semi-supervised setting. We compared our method to state-of-the-art\napproaches in simulations using multiple public benchmark data sets. The\nincluded benchmark comprises three settings with increasing label noise: (i)\nfully supervised, (ii) PU learning and (iii) PU learning with false positives.\nOur approach shows a marginal improvement over existing methods in the second\nsetting and a significant improvement in the third.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 14:18:17 GMT"}, {"version": "v2", "created": "Tue, 21 Oct 2014 12:29:58 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Claesen", "Marc", ""], ["De Smet", "Frank", ""], ["Suykens", "Johan A. K.", ""], ["De Moor", "Bart", ""]]}, {"id": "1402.3337", "submitter": "Kishore Konda", "authors": "Kishore Konda, Roland Memisevic, David Krueger", "title": "Zero-bias autoencoders and the benefits of co-adapting features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized training of an autoencoder typically results in hidden unit\nbiases that take on large negative values. We show that negative biases are a\nnatural result of using a hidden layer whose responsibility is to both\nrepresent the input data and act as a selection mechanism that ensures sparsity\nof the representation. We then show that negative biases impede the learning of\ndata distributions whose intrinsic dimensionality is high. We also propose a\nnew activation function that decouples the two roles of the hidden layer and\nthat allows us to learn representations on data with very high intrinsic\ndimensionality, where standard autoencoders typically fail. Since the decoupled\nactivation function acts like an implicit regularizer, the model can be trained\nby minimizing the reconstruction error of training data, without requiring any\nadditional regularization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 23:37:39 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 21:39:48 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 02:07:47 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 01:15:33 GMT"}, {"version": "v5", "created": "Wed, 8 Apr 2015 14:51:11 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Konda", "Kishore", ""], ["Memisevic", "Roland", ""], ["Krueger", "David", ""]]}, {"id": "1402.3346", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Nihat Ay, Keyan Ghazi-Zahedi", "title": "Geometry and Expressive Power of Conditional Restricted Boltzmann\n  Machines", "comments": "30 pages, 5 figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional restricted Boltzmann machines are undirected stochastic neural\nnetworks with a layer of input and output units connected bipartitely to a\nlayer of hidden units. These networks define models of conditional probability\ndistributions on the states of the output units given the states of the input\nunits, parametrized by interaction weights and biases. We address the\nrepresentational power of these models, proving results their ability to\nrepresent conditional Markov random fields and conditional distributions with\nrestricted supports, the minimal size of universal approximators, the maximal\nmodel approximation errors, and on the dimension of the set of representable\nconditional distributions. We contribute new tools for investigating\nconditional probability models, which allow us to improve the results that can\nbe derived from existing work on restricted Boltzmann machine probability\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 02:15:09 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 17:00:35 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 15:20:04 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Montufar", "Guido", ""], ["Ay", "Nihat", ""], ["Ghazi-Zahedi", "Keyan", ""]]}, {"id": "1402.3427", "submitter": "Sotirios Chatzis", "authors": "Sotirios P. Chatzis", "title": "Indian Buffet Process Deep Generative Models for Semi-Supervised\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models (DGMs) have brought about a major breakthrough, as\nwell as renewed interest, in generative latent variable models. However, DGMs\ndo not allow for performing data-driven inference of the number of latent\nfeatures needed to represent the observed data. Traditional linear formulations\naddress this issue by resorting to tools from the field of nonparametric\nstatistics. Indeed, linear latent variable models imposed an Indian Buffet\nProcess (IBP) prior have been extensively studied by the machine learning\ncommunity; inference for such models can been performed either via exact\nsampling or via approximate variational techniques. Based on this inspiration,\nin this paper we examine whether similar ideas from the field of Bayesian\nnonparametrics can be utilized in the context of modern DGMs in order to\naddress the latent variable dimensionality inference problem. To this end, we\npropose a novel DGM formulation, based on the imposition of an IBP prior. We\ndevise an efficient Black-Box Variational inference algorithm for our model,\nand exhibit its efficacy in a number of semi-supervised classification\nexperiments. In all cases, we use popular benchmark datasets, and compare to\nstate-of-the-art DGMs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 10:44:48 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2015 18:41:41 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2016 19:37:54 GMT"}, {"version": "v4", "created": "Sat, 16 Jul 2016 14:11:47 GMT"}, {"version": "v5", "created": "Sun, 6 Aug 2017 21:27:48 GMT"}, {"version": "v6", "created": "Fri, 9 Feb 2018 21:25:41 GMT"}, {"version": "v7", "created": "Sat, 31 Mar 2018 13:38:19 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Chatzis", "Sotirios P.", ""]]}, {"id": "1402.3511", "submitter": "Jan Koutn\\'ik", "authors": "Jan Koutn\\'ik, Klaus Greff, Faustino Gomez, J\\\"urgen Schmidhuber", "title": "A Clockwork RNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence prediction and classification are ubiquitous and challenging\nproblems in machine learning that can require identifying complex dependencies\nbetween temporally distant inputs. Recurrent Neural Networks (RNNs) have the\nability, in theory, to cope with these temporal dependencies by virtue of the\nshort-term memory implemented by their recurrent (feedback) connections.\nHowever, in practice they are difficult to train successfully when the\nlong-term memory is required. This paper introduces a simple, yet powerful\nmodification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in\nwhich the hidden layer is partitioned into separate modules, each processing\ninputs at its own temporal granularity, making computations only at its\nprescribed clock rate. Rather than making the standard RNN models more complex,\nCW-RNN reduces the number of RNN parameters, improves the performance\nsignificantly in the tasks tested, and speeds up the network evaluation. The\nnetwork is demonstrated in preliminary experiments involving two tasks: audio\nsignal generation and TIMIT spoken word classification, where it outperforms\nboth RNN and LSTM networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 16:05:12 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Koutn\u00edk", "Jan", ""], ["Greff", "Klaus", ""], ["Gomez", "Faustino", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1402.3578", "submitter": "Cezary Kaliszyk", "authors": "Cezary Kaliszyk and Josef Urban", "title": "Learning-assisted Theorem Proving with Millions of Lemmas", "comments": "journal version of arXiv:1310.2797 (which was submitted to LPAR\n  conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large formal mathematical libraries consist of millions of atomic inference\nsteps that give rise to a corresponding number of proved statements (lemmas).\nAnalogously to the informal mathematical practice, only a tiny fraction of such\nstatements is named and re-used in later proofs by formal mathematicians. In\nthis work, we suggest and implement criteria defining the estimated usefulness\nof the HOL Light lemmas for proving further theorems. We use these criteria to\nmine the large inference graph of the lemmas in the HOL Light and Flyspeck\nlibraries, adding up to millions of the best lemmas to the pool of statements\nthat can be re-used in later proofs. We show that in combination with\nlearning-based relevance filtering, such methods significantly strengthen\nautomated theorem proving of new conjectures over large formal mathematical\nlibraries such as Flyspeck.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 03:08:02 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1402.3631", "submitter": "Justin Hsu", "authors": "Justin Hsu and Aaron Roth and Tim Roughgarden and Jonathan Ullman", "title": "Privately Solving Linear Programs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-43948-7_51", "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate the systematic study of solving linear programs\nunder differential privacy. The first step is simply to define the problem: to\nthis end, we introduce several natural classes of private linear programs that\ncapture different ways sensitive data can be incorporated into a linear\nprogram. For each class of linear programs we give an efficient, differentially\nprivate solver based on the multiplicative weights framework, or we give an\nimpossibility result.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2014 00:55:46 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 19:52:34 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hsu", "Justin", ""], ["Roth", "Aaron", ""], ["Roughgarden", "Tim", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1402.3722", "submitter": "Yoav Goldberg", "authors": "Yoav Goldberg and Omer Levy", "title": "word2vec Explained: deriving Mikolov et al.'s negative-sampling\n  word-embedding method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The word2vec software of Tomas Mikolov and colleagues\n(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and\nprovides state-of-the-art word embeddings. The learning models behind the\nsoftware are described in two research papers. We found the description of the\nmodels in these papers to be somewhat cryptic and hard to follow. While the\nmotivations and presentation may be obvious to the neural-networks\nlanguage-modeling crowd, we had to struggle quite a bit to figure out the\nrationale behind the equations.\n  This note is an attempt to explain equation (4) (negative sampling) in\n\"Distributed Representations of Words and Phrases and their Compositionality\"\nby Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2014 21:03:02 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Goldberg", "Yoav", ""], ["Levy", "Omer", ""]]}, {"id": "1402.3849", "submitter": "Radha Chitta", "authors": "Radha Chitta, Rong Jin, Timothy C. Havens, Anil K. Jain", "title": "Scalable Kernel Clustering: Approximate Kernel k-means", "comments": "15 pages, 6 figures,extension of the work \"Approximate Kernel\n  k-means: Solution to large scale kernel clustering\" published in KDD 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based clustering algorithms have the ability to capture the non-linear\nstructure in real world data. Among various kernel-based clustering algorithms,\nkernel k-means has gained popularity due to its simple iterative nature and\nease of implementation. However, its run-time complexity and memory footprint\nincrease quadratically in terms of the size of the data set, and hence, large\ndata sets cannot be clustered efficiently. In this paper, we propose an\napproximation scheme based on randomization, called the Approximate Kernel\nk-means. We approximate the cluster centers using the kernel similarity between\na few sampled points and all the points in the data set. We show that the\nproposed method achieves better clustering performance than the traditional low\nrank kernel approximation based clustering schemes. We also demonstrate that\nits running time and memory requirements are significantly lower than those of\nkernel k-means, with only a small reduction in the clustering quality on\nseveral public domain large data sets. We then employ ensemble clustering\ntechniques to further enhance the performance of our algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 22:19:40 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Chitta", "Radha", ""], ["Jin", "Rong", ""], ["Havens", "Timothy C.", ""], ["Jain", "Anil K.", ""]]}, {"id": "1402.3891", "submitter": "Vinodhini G", "authors": "Vinodhini G Chandrasekaran RM", "title": "Performance Evaluation of Machine Learning Classifiers in Sentiment\n  Mining", "comments": "4 pages 2 tables, International Journal of Computer Trends and\n  Technology, volume 4, Issue 6, june 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the use of machine learning classifiers is of great value in\nsolving a variety of problems in text classification. Sentiment mining is a\nkind of text classification in which, messages are classified according to\nsentiment orientation such as positive or negative. This paper extends the idea\nof evaluating the performance of various classifiers to show their\neffectiveness in sentiment mining of online product reviews. The product\nreviews are collected from Amazon reviews. To evaluate the performance of\nclassifiers various evaluation methods like random sampling, linear sampling\nand bootstrap sampling are used. Our results shows that support vector machine\nwith bootstrap sampling method outperforms others classifiers and sampling\nmethods in terms of misclassification rate.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 05:24:42 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["RM", "Vinodhini G Chandrasekaran", ""]]}, {"id": "1402.3902", "submitter": "Karthikeyan Shanmugam", "authors": "Murat Kocaoglu, Karthikeyan Shanmugam, Alexandros G. Dimakis and Adam\n  Klivans", "title": "Sparse Polynomial Learning and Graph Sketching", "comments": "14 pages; to appear in NIPS 2014l Updated proof of Theorem 5 and some\n  other minor changes during revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $f:\\{-1,1\\}^n$ be a polynomial with at most $s$ non-zero real\ncoefficients. We give an algorithm for exactly reconstructing f given random\nexamples from the uniform distribution on $\\{-1,1\\}^n$ that runs in time\npolynomial in $n$ and $2s$ and succeeds if the function satisfies the unique\nsign property: there is one output value which corresponds to a unique set of\nvalues of the participating parities. This sufficient condition is satisfied\nwhen every coefficient of f is perturbed by a small random noise, or satisfied\nwith high probability when s parity functions are chosen randomly or when all\nthe coefficients are positive. Learning sparse polynomials over the Boolean\ndomain in time polynomial in $n$ and $2s$ is considered notoriously hard in the\nworst-case. Our result shows that the problem is tractable for almost all\nsparse polynomials. Then, we show an application of this result to hypergraph\nsketching which is the problem of learning a sparse (both in the number of\nhyperedges and the size of the hyperedges) hypergraph from uniformly drawn\nrandom cuts. We also provide experimental results on a real world dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 06:00:16 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 06:56:27 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 22:35:40 GMT"}, {"version": "v4", "created": "Fri, 7 Nov 2014 03:00:28 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Kocaoglu", "Murat", ""], ["Shanmugam", "Karthikeyan", ""], ["Dimakis", "Alexandros G.", ""], ["Klivans", "Adam", ""]]}, {"id": "1402.4084", "submitter": "Edward Moroshko", "authors": "Edward Moroshko, Koby Crammer", "title": "Selective Sampling with Drift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been much work on selective sampling, an online active\nlearning setting, in which algorithms work in rounds. On each round an\nalgorithm receives an input and makes a prediction. Then, it can decide whether\nto query a label, and if so to update its model, otherwise the input is\ndiscarded. Most of this work is focused on the stationary case, where it is\nassumed that there is a fixed target model, and the performance of the\nalgorithm is compared to a fixed model. However, in many real-world\napplications, such as spam prediction, the best target function may drift over\ntime, or have shifts from time to time. We develop a novel selective sampling\nalgorithm for the drifting setting, analyze it under no assumptions on the\nmechanism generating the sequence of instances, and derive new mistake bounds\nthat depend on the amount of drift in the problem. Simulations on synthetic and\nreal-world datasets demonstrate the superiority of our algorithms as a\nselective sampling algorithm in the drifting setting.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 17:53:57 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Moroshko", "Edward", ""], ["Crammer", "Koby", ""]]}, {"id": "1402.4102", "submitter": "Tianqi Chen", "authors": "Tianqi Chen, Emily B. Fox, Carlos Guestrin", "title": "Stochastic Gradient Hamiltonian Monte Carlo", "comments": "ICML 2014 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for\ndefining distant proposals with high acceptance probabilities in a\nMetropolis-Hastings framework, enabling more efficient exploration of the state\nspace than standard random-walk proposals. The popularity of such methods has\ngrown significantly in recent years. However, a limitation of HMC methods is\nthe required gradient computation for simulation of the Hamiltonian dynamical\nsystem-such computation is infeasible in problems involving a large sample size\nor streaming data. Instead, we must rely on a noisy gradient estimate computed\nfrom a subset of the data. In this paper, we explore the properties of such a\nstochastic gradient HMC approach. Surprisingly, the natural implementation of\nthe stochastic approximation can be arbitrarily bad. To address this problem we\nintroduce a variant that uses second-order Langevin dynamics with a friction\nterm that counteracts the effects of the noisy gradient, maintaining the\ndesired target distribution as the invariant distribution. Results on simulated\ndata validate our theory. We also provide an application of our methods to a\nclassification task using neural networks and to online Bayesian matrix\nfactorization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 19:57:59 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 06:38:21 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Chen", "Tianqi", ""], ["Fox", "Emily B.", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1402.4279", "submitter": "Ingmar Schuster", "authors": "Ingmar Schuster", "title": "A Bayesian Model of node interaction in networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We are concerned with modeling the strength of links in networks by taking\ninto account how often those links are used. Link usage is a strong indicator\nof how closely two nodes are related, but existing network models in Bayesian\nStatistics and Machine Learning are able to predict only wether a link exists\nat all. As priors for latent attributes of network nodes we explore the Chinese\nRestaurant Process (CRP) and a multivariate Gaussian with fixed dimensionality.\nThe model is applied to a social network dataset and a word coocurrence\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 10:34:41 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 10:22:12 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Schuster", "Ingmar", ""]]}, {"id": "1402.4283", "submitter": "Dipti Rana Mrs.", "authors": "P. Chaudhari, D. P. Rana, R. G. Mehta, N. J. Mistry, M. M. Raghuwanshi", "title": "Discretization of Temporal Data: A Survey", "comments": "4 pages, 1 Table", "journal-ref": "International Journal of Computer Science and Information Security\n  (IJCSIS), ISSN:1947-5500, Vol. 11, No. 2", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world, the huge amount of temporal data is to be processed in many\napplication areas such as scientific, financial, network monitoring, sensor\ndata analysis. Data mining techniques are primarily oriented to handle discrete\nfeatures. In the case of temporal data the time plays an important role on the\ncharacteristics of data. To consider this effect, the data discretization\ntechniques have to consider the time while processing to resolve the issue by\nfinding the intervals of data which are more concise and precise with respect\nto time. Here, this research is reviewing different data discretization\ntechniques used in temporal data applications according to the inclusion or\nexclusion of: class label, temporal order of the data and handling of stream\ndata to open the research direction for temporal data discretization to improve\nthe performance of data mining technique.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 10:44:01 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Chaudhari", "P.", ""], ["Rana", "D. P.", ""], ["Mehta", "R. G.", ""], ["Mistry", "N. J.", ""], ["Raghuwanshi", "M. M.", ""]]}, {"id": "1402.4293", "submitter": "Alexander Davies", "authors": "Alex Davies, Zoubin Ghahramani", "title": "The Random Forest Kernel and other kernels for big data from random\n  partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Random Partition Kernels, a new class of kernels derived by\ndemonstrating a natural connection between random partitions of objects and\nkernels between those objects. We show how the construction can be used to\ncreate kernels from methods that would not normally be viewed as random\npartitions, such as Random Forest. To demonstrate the potential of this method,\nwe propose two new kernels, the Random Forest Kernel and the Fast Cluster\nKernel, and show that these kernels consistently outperform standard kernels on\nproblems involving real-world datasets. Finally, we show how the form of these\nkernels lend themselves to a natural approximation that is appropriate for\ncertain big data problems, allowing $O(N)$ inference in methods such as\nGaussian Processes, Support Vector Machines and Kernel PCA.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:13:45 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Davies", "Alex", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.4304", "submitter": "James Lloyd", "authors": "James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B. Tenenbaum,\n  Zoubin Ghahramani", "title": "Automatic Construction and Natural-Language Description of Nonparametric\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the beginnings of an automatic statistician, focusing on\nregression problems. Our system explores an open-ended space of statistical\nmodels to discover a good explanation of a data set, and then produces a\ndetailed report with figures and natural-language text. Our approach treats\nunknown regression functions nonparametrically using Gaussian processes, which\nhas two important consequences. First, Gaussian processes can model functions\nin terms of high-level properties (e.g. smoothness, trends, periodicity,\nchangepoints). Taken together with the compositional structure of our language\nof models this allows us to automatically describe functions in simple terms.\nSecond, the use of flexible nonparametric models and a rich language for\ncomposing them in an open-ended manner also results in state-of-the-art\nextrapolation performance evaluated over 13 real time series data sets from\nvarious domains.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:38:11 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2014 15:38:36 GMT"}, {"version": "v3", "created": "Thu, 24 Apr 2014 11:44:13 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Lloyd", "James Robert", ""], ["Duvenaud", "David", ""], ["Grosse", "Roger", ""], ["Tenenbaum", "Joshua B.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.4306", "submitter": "Amar Shah", "authors": "Amar Shah, Andrew Gordon Wilson and Zoubin Ghahramani", "title": "Student-t Processes as Alternatives to Gaussian Processes", "comments": "13 pages, 6 figures, 1 table. To appear in \"The Seventeenth\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2014.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the Student-t process as an alternative to the Gaussian\nprocess as a nonparametric prior over functions. We derive closed form\nexpressions for the marginal likelihood and predictive distribution of a\nStudent-t process, by integrating away an inverse Wishart process prior over\nthe covariance kernel of a Gaussian process model. We show surprising\nequivalences between different hierarchical Gaussian process models leading to\nStudent-t processes, and derive a new sampling scheme for the inverse Wishart\nprocess, which helps elucidate these equivalences. Overall, we show that a\nStudent-t process can retain the attractive properties of a Gaussian process --\na nonparametric representation, analytic marginal and predictive distributions,\nand easy model selection through covariance kernels -- but has enhanced\nflexibility, and predictive covariances that, unlike a Gaussian process,\nexplicitly depend on the values of training observations. We verify empirically\nthat a Student-t process is especially useful in situations where there are\nchanges in covariance structure, or in applications like Bayesian optimization,\nwhere accurate predictive covariances are critical for good performance. These\nadvantages come at no additional computational cost over Gaussian processes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:47:38 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 10:49:16 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Shah", "Amar", ""], ["Wilson", "Andrew Gordon", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.4322", "submitter": "Alvaro Martinez", "authors": "A. Mart\\'inez-P\\'erez", "title": "On the properties of $\\alpha$-unchaining single linkage hierarchical\n  clustering", "comments": "14 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1210.6292", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the election of a hierarchical clustering method, theoretic properties may\ngive some insight to determine which method is the most suitable to treat a\nclustering problem. Herein, we study some basic properties of two hierarchical\nclustering methods: $\\alpha$-unchaining single linkage or $SL(\\alpha)$ and a\nmodified version of this one, $SL^*(\\alpha)$. We compare the results with the\nproperties satisfied by the classical linkage-based hierarchical clustering\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 13:08:47 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Mart\u00ednez-P\u00e9rez", "A.", ""]]}, {"id": "1402.4354", "submitter": "Stefano Teso", "authors": "Stefano Teso and Roberto Sebastiani and Andrea Passerini", "title": "Hybrid SRL with Optimization Modulo Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally speaking, the goal of constructive learning could be seen as, given\nan example set of structured objects, to generate novel objects with similar\nproperties. From a statistical-relational learning (SRL) viewpoint, the task\ncan be interpreted as a constraint satisfaction problem, i.e. the generated\nobjects must obey a set of soft constraints, whose weights are estimated from\nthe data. Traditional SRL approaches rely on (finite) First-Order Logic (FOL)\nas a description language, and on MAX-SAT solvers to perform inference. Alas,\nFOL is unsuited for con- structive problems where the objects contain a mixture\nof Boolean and numerical variables. It is in fact difficult to implement, e.g.\nlinear arithmetic constraints within the language of FOL. In this paper we\npropose a novel class of hybrid SRL methods that rely on Satisfiability Modulo\nTheories, an alternative class of for- mal languages that allow to describe,\nand reason over, mixed Boolean-numerical objects and constraints. The resulting\nmethods, which we call Learning Mod- ulo Theories, are formulated within the\nstructured output SVM framework, and employ a weighted SMT solver as an\noptimization oracle to perform efficient in- ference and discriminative max\nmargin weight learning. We also present a few examples of constructive learning\napplications enabled by our method.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 14:35:30 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Teso", "Stefano", ""], ["Sebastiani", "Roberto", ""], ["Passerini", "Andrea", ""]]}, {"id": "1402.4371", "submitter": "Hung Nien", "authors": "Hung Nien and Jeffrey A. Fessler", "title": "A convergence proof of the split Bregman method for regularized\n  least-squares problems", "comments": "11 pages, 3 figures, submitted to SIAM J. Imaging Sci", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The split Bregman (SB) method [T. Goldstein and S. Osher, SIAM J. Imaging\nSci., 2 (2009), pp. 323-43] is a fast splitting-based algorithm that solves\nimage reconstruction problems with general l1, e.g., total-variation (TV) and\ncompressed sensing (CS), regularizations by introducing a single variable split\nto decouple the data-fitting term and the regularization term, yielding simple\nsubproblems that are separable (or partially separable) and easy to minimize.\nSeveral convergence proofs have been proposed, and these proofs either impose a\n\"full column rank\" assumption to the split or assume exact updates in all\nsubproblems. However, these assumptions are impractical in many applications\nsuch as the X-ray computed tomography (CT) image reconstructions, where the\ninner least-squares problem usually cannot be solved efficiently due to the\nhighly shift-variant Hessian. In this paper, we show that when the data-fitting\nterm is quadratic, the SB method is a convergent alternating direction method\nof multipliers (ADMM), and a straightforward convergence proof with inexact\nupdates is given using [J. Eckstein and D. P. Bertsekas, Mathematical\nProgramming, 55 (1992), pp. 293-318, Theorem 8]. Furthermore, since the SB\nmethod is just a special case of an ADMM algorithm, it seems likely that the\nADMM algorithm will be faster than the SB method if the augmented Largangian\n(AL) penalty parameters are selected appropriately. To have a concrete example,\nwe conduct a convergence rate analysis of the ADMM algorithm using two splits\nfor image restoration problems with quadratic data-fitting term and\nregularization term. According to our analysis, we can show that the two-split\nADMM algorithm can be faster than the SB method if the AL penalty parameter of\nthe SB method is suboptimal. Numerical experiments were conducted to verify our\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 15:16:45 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Nien", "Hung", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1402.4381", "submitter": "Hung Nien", "authors": "Hung Nien and Jeffrey A. Fessler", "title": "Fast X-ray CT image reconstruction using the linearized augmented\n  Lagrangian method with ordered subsets", "comments": "21 pages (including the supplementary material), 12 figures,\n  submitted to IEEE Trans. Med. Imag", "journal-ref": "IEEE Trans. Medical Imaging, 34(2):388-99, Feb. 2015", "doi": "10.1109/TMI.2014.2358499", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The augmented Lagrangian (AL) method that solves convex optimization problems\nwith linear constraints has drawn more attention recently in imaging\napplications due to its decomposable structure for composite cost functions and\nempirical fast convergence rate under weak conditions. However, for problems\nsuch as X-ray computed tomography (CT) image reconstruction and large-scale\nsparse regression with \"big data\", where there is no efficient way to solve the\ninner least-squares problem, the AL method can be slow due to the inevitable\niterative inner updates. In this paper, we focus on solving regularized\n(weighted) least-squares problems using a linearized variant of the AL method\nthat replaces the quadratic AL penalty term in the scaled augmented Lagrangian\nwith its separable quadratic surrogate (SQS) function, thus leading to a much\nsimpler ordered-subsets (OS) accelerable splitting-based algorithm, OS-LALM,\nfor X-ray CT image reconstruction. To further accelerate the proposed\nalgorithm, we use a second-order recursive system analysis to design a\ndeterministic downward continuation approach that avoids tedious parameter\ntuning and provides fast convergence. Experimental results show that the\nproposed algorithm significantly accelerates the \"convergence\" of X-ray CT\nimage reconstruction with negligible overhead and greatly reduces the OS\nartifacts in the reconstructed image when using many subsets for OS\nacceleration.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 16:02:36 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Nien", "Hung", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1402.4419", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann)", "title": "Incremental Majorization-Minimization Optimization with Application to\n  Large-Scale Machine Learning", "comments": "to appear in SIAM Journal on Optimization; final author's version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majorization-minimization algorithms consist of successively minimizing a\nsequence of upper bounds of the objective function. These upper bounds are\ntight at the current estimate, and each iteration monotonically drives the\nobjective function downhill. Such a simple principle is widely applicable and\nhas been very popular in various scientific fields, especially in signal\nprocessing and statistics. In this paper, we propose an incremental\nmajorization-minimization scheme for minimizing a large sum of continuous\nfunctions, a problem of utmost importance in machine learning. We present\nconvergence guarantees for non-convex and convex optimization when the upper\nbounds approximate the objective up to a smooth error; we call such upper\nbounds \"first-order surrogate functions\". More precisely, we study asymptotic\nstationary point guarantees for non-convex problems, and for convex ones, we\nprovide convergence rates for the expected objective function value. We apply\nour scheme to composite optimization and obtain a new incremental proximal\ngradient algorithm with linear convergence rate for strongly convex functions.\nIn our experiments, we show that our method is competitive with the state of\nthe art for solving machine learning problems such as logistic regression when\nthe number of training samples is large enough, and we demonstrate its\nusefulness for sparse estimation with non-convex penalties.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 17:50:30 GMT"}, {"version": "v2", "created": "Thu, 2 Oct 2014 19:39:32 GMT"}, {"version": "v3", "created": "Sun, 1 Feb 2015 07:20:36 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"]]}, {"id": "1402.4437", "submitter": "Taco Cohen", "authors": "Taco Cohen, Max Welling", "title": "Learning the Irreducible Representations of Commutative Lie Groups", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning,\n  2014", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new probabilistic model of compact commutative Lie groups that\nproduces invariant-equivariant and disentangled representations of data. To\ndefine the notion of disentangling, we borrow a fundamental principle from\nphysics that is used to derive the elementary particles of a system from its\nsymmetries. Our model employs a newfound Bayesian conjugacy relation that\nenables fully tractable probabilistic inference over compact commutative Lie\ngroups -- a class that includes the groups that describe the rotation and\ncyclic translation of images. We train the model on pairs of transformed image\npatches, and show that the learned invariant representation is highly effective\nfor classification.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 18:47:41 GMT"}, {"version": "v2", "created": "Sun, 25 May 2014 12:21:34 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Cohen", "Taco", ""], ["Welling", "Max", ""]]}, {"id": "1402.4512", "submitter": "Nikhil Rao", "authors": "Nikhil Rao, Robert Nowak, Christopher Cox and Timothy Rogers", "title": "Classification with Sparse Overlapping Groups", "comments": "Tighter result compared to the previous version. Some additional\n  details and justification on the problem being solved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification with a sparsity constraint on the solution plays a central\nrole in many high dimensional machine learning applications. In some cases, the\nfeatures can be grouped together so that entire subsets of features can be\nselected or not selected. In many applications, however, this can be too\nrestrictive. In this paper, we are interested in a less restrictive form of\nstructured sparse feature selection: we assume that while features can be\ngrouped according to some notion of similarity, not all features in a group\nneed be selected for the task at hand. When the groups are comprised of\ndisjoint sets of features, this is sometimes referred to as the \"sparse group\"\nlasso, and it allows for working with a richer class of models than traditional\ngroup lasso methods. Our framework generalizes conventional sparse group lasso\nfurther by allowing for overlapping groups, an additional flexiblity needed in\nmany applications and one that presents further challenges. The main\ncontribution of this paper is a new procedure called Sparse Overlapping Group\n(SOG) lasso, a convex optimization program that automatically selects similar\nfeatures for classification in high dimensions. We establish model selection\nerror bounds for SOGlasso classification problems under a fairly general\nsetting. In particular, the error bounds are the first such results for\nclassification using the sparse group lasso. Furthermore, the general SOGlasso\nbound specializes to results for the lasso and the group lasso, some known and\nsome new. The SOGlasso is motivated by multi-subject fMRI studies in which\nfunctional activity is classified using brain voxels as features, source\nlocalization problems in Magnetoencephalography (MEG), and analyzing gene\nactivation patterns in microarray data analysis. Experiments with real and\nsynthetic data demonstrate the advantages of SOGlasso compared to the lasso and\ngroup lasso.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 22:08:50 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2014 16:53:19 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Rao", "Nikhil", ""], ["Nowak", "Robert", ""], ["Cox", "Christopher", ""], ["Rogers", "Timothy", ""]]}, {"id": "1402.4542", "submitter": "Chunguo Li", "authors": "Chun-Guo Li, Xing Mei, Bao-Gang Hu", "title": "Unsupervised Ranking of Multi-Attribute Objects Based on Principal\n  Curves", "comments": "This paper has 14 pages and 9 figures. The paper has submitted to\n  IEEE Transactions on Knowledge and Data Engineering (TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised ranking faces one critical challenge in evaluation applications,\nthat is, no ground truth is available. When PageRank and its variants show a\ngood solution in related subjects, they are applicable only for ranking from\nlink-structure data. In this work, we focus on unsupervised ranking from\nmulti-attribute data which is also common in evaluation tasks. To overcome the\nchallenge, we propose five essential meta-rules for the design and assessment\nof unsupervised ranking approaches: scale and translation invariance, strict\nmonotonicity, linear/nonlinear capacities, smoothness, and explicitness of\nparameter size. These meta-rules are regarded as high level knowledge for\nunsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a\nranking principal curve (RPC) model, which learns a one-dimensional manifold\nfunction to perform unsupervised ranking tasks on multi-attribute observations.\nFurthermore, the RPC is modeled to be a cubic B\\'ezier curve with control\npoints restricted in the interior of a hypercube, thereby complying with all\nthe five meta-rules to infer a reasonable ranking list. With control points as\nthe model parameters, one is able to understand the learned manifold and to\ninterpret the ranking list semantically. Numerical experiments of the presented\nRPC model are conducted on two open datasets of different ranking applications.\nIn comparison with the state-of-the-art approaches, the new model is able to\nshow more reasonable ranking lists.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 01:29:14 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Li", "Chun-Guo", ""], ["Mei", "Xing", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1402.4566", "submitter": "Jaydeep De", "authors": "Jaydeep De and Xiaowei Zhang and Li Cheng", "title": "Transduction on Directed Graphs via Absorbing Random Walks", "comments": "The paper is withdrawn because of some violation in institute policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of graph-based transductive\nclassification, and we are particularly interested in the directed graph\nscenario which is a natural form for many real world applications. Different\nfrom existing research efforts that either only deal with undirected graphs or\ncircumvent directionality by means of symmetrization, we propose a novel random\nwalk approach on directed graphs using absorbing Markov chains, which can be\nregarded as maximizing the accumulated expected number of visits from the\nunlabeled transient states. Our algorithm is simple, easy to implement, and\nworks with large-scale graphs. In particular, it is capable of preserving the\ngraph structure even when the input graph is sparse and changes over time, as\nwell as retaining weak signals presented in the directed edges. We present its\nintimate connections to a number of existing methods, including graph kernels,\ngraph Laplacian based methods, and interestingly, spanning forest of graphs.\nIts computational complexity and the generalization error are also studied.\nEmpirically our algorithm is systematically evaluated on a wide range of\napplications, where it has shown to perform competitively comparing to a suite\nof state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 06:41:12 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 02:05:34 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["De", "Jaydeep", ""], ["Zhang", "Xiaowei", ""], ["Cheng", "Li", ""]]}, {"id": "1402.4645", "submitter": "Jothi Prakash V", "authors": "V. Jothi Prakash, Dr. L.M. Nithya", "title": "A Survey on Semi-Supervised Learning Techniques", "comments": "5 Pages, 3 figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  8(1):25-29, February 2014. ISSN:2231-2803.Published by Seventh Sense Research\n  Group", "doi": "10.14445/22312803/IJCTT-V8P105", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semisupervised learning is a learning standard which deals with the study of\nhow computers and natural systems such as human beings acquire knowledge in the\npresence of both labeled and unlabeled data. Semisupervised learning based\nmethods are preferred when compared to the supervised and unsupervised learning\nbecause of the improved performance shown by the semisupervised approaches in\nthe presence of large volumes of data. Labels are very hard to attain while\nunlabeled data are surplus, therefore semisupervised learning is a noble\nindication to shrink human labor and improve accuracy. There has been a large\nspectrum of ideas on semisupervised learning. In this paper we bring out some\nof the key approaches for semisupervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 12:40:31 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Prakash", "V. Jothi", ""], ["Nithya", "Dr. L. M.", ""]]}, {"id": "1402.4653", "submitter": "Sohan Seth", "authors": "Sohan Seth, John Shawe-Taylor, Samuel Kaski", "title": "Retrieval of Experiments by Efficient Estimation of Marginal Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of retrieving relevant experiments given a query\nexperiment. By experiment, we mean a collection of measurements from a set of\n`covariates' and the associated `outcomes'. While similar experiments can be\nretrieved by comparing available `annotations', this approach ignores the\nvaluable information available in the measurements themselves. To incorporate\nthis information in the retrieval task, we suggest employing a retrieval metric\nthat utilizes probabilistic models learned from the measurements. We argue that\nsuch a metric is a sensible measure of similarity between two experiments since\nit permits inclusion of experiment-specific prior knowledge. However, accurate\nmodels are often not analytical, and one must resort to storing posterior\nsamples which demands considerable resources. Therefore, we study strategies to\nselect informative posterior samples to reduce the computational load while\nmaintaining the retrieval performance. We demonstrate the efficacy of our\napproach on simulated data with simple linear regression as the models, and\nreal world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 13:21:40 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Seth", "Sohan", ""], ["Shawe-Taylor", "John", ""], ["Kaski", "Samuel", ""]]}, {"id": "1402.4732", "submitter": "Thomas Lasko", "authors": "Thomas A. Lasko", "title": "Efficient Inference of Gaussian Process Modulated Renewal Processes with\n  Application to Medical Event Data", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "VU-DBMI-2014-01-001", "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The episodic, irregular and asynchronous nature of medical data render them\ndifficult substrates for standard machine learning algorithms. We would like to\nabstract away this difficulty for the class of time-stamped categorical\nvariables (or events) by modeling them as a renewal process and inferring a\nprobability density over continuous, longitudinal, nonparametric intensity\nfunctions modulating that process. Several methods exist for inferring such a\ndensity over intensity functions, but either their constraints and assumptions\nprevent their use with our potentially bursty event streams, or their time\ncomplexity renders their use intractable on our long-duration observations of\nhigh-resolution events, or both. In this paper we present a new and efficient\nmethod for inferring a distribution over intensity functions that uses direct\nnumeric integration and smooth interpolation over Gaussian processes. We\ndemonstrate that our direct method is up to twice as accurate and two orders of\nmagnitude more efficient than the best existing method (thinning). Importantly,\nthe direct method can infer intensity functions over the full range of bursty\nto memoryless to regular events, which thinning and many other methods cannot.\nFinally, we apply the method to clinical event data and demonstrate the\nface-validity of the abstraction, which is now amenable to standard learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 17:09:14 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Lasko", "Thomas A.", ""]]}, {"id": "1402.4746", "submitter": "Ananda Theertha Suresh", "authors": "Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, Ananda Theertha\n  Suresh", "title": "Near-optimal-sample estimators for spherical Gaussian mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical and machine-learning algorithms are frequently applied to\nhigh-dimensional data. In many of these applications data is scarce, and often\nmuch more costly than computation time. We provide the first sample-efficient\npolynomial-time estimator for high-dimensional spherical Gaussian mixtures.\n  For mixtures of any $k$ $d$-dimensional spherical Gaussians, we derive an\nintuitive spectral-estimator that uses\n$\\mathcal{O}_k\\bigl(\\frac{d\\log^2d}{\\epsilon^4}\\bigr)$ samples and runs in time\n$\\mathcal{O}_{k,\\epsilon}(d^3\\log^5 d)$, both significantly lower than\npreviously known. The constant factor $\\mathcal{O}_k$ is polynomial for sample\ncomplexity and is exponential for the time complexity, again much smaller than\nwhat was previously known. We also show that\n$\\Omega_k\\bigl(\\frac{d}{\\epsilon^2}\\bigr)$ samples are needed for any\nalgorithm. Hence the sample complexity is near-optimal in the number of\ndimensions.\n  We also derive a simple estimator for one-dimensional mixtures that uses\n$\\mathcal{O}\\bigl(\\frac{k \\log \\frac{k}{\\epsilon} }{\\epsilon^2} \\bigr)$ samples\nand runs in time\n$\\widetilde{\\mathcal{O}}\\left(\\bigl(\\frac{k}{\\epsilon}\\bigr)^{3k+1}\\right)$.\nOur other technical contributions include a faster algorithm for choosing a\ndensity estimate from a set of distributions, that minimizes the $\\ell_1$\ndistance to an unknown underlying distribution.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 17:59:55 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Acharya", "Jayadev", ""], ["Jafarpour", "Ashkan", ""], ["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1402.4844", "submitter": "Alon Gonen", "authors": "Alon Gonen, Dan Rosenbaum, Yonina Eldar, Shai Shalev-Shwartz", "title": "Subspace Learning with Partial Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of subspace learning is to find a $k$-dimensional subspace of\n$\\mathbb{R}^d$, such that the expected squared distance between instance\nvectors and the subspace is as small as possible. In this paper we study\nsubspace learning in a partial information setting, in which the learner can\nonly observe $r \\le d$ attributes from each instance vector. We propose several\nefficient algorithms for this task, and analyze their sample complexity\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 22:57:03 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 14:06:50 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Gonen", "Alon", ""], ["Rosenbaum", "Dan", ""], ["Eldar", "Yonina", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1402.4845", "submitter": "Gionata Gelati", "authors": "Jonathan Gelati and Sithan Kanna", "title": "Diffusion Least Mean Square: Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report we analyse the performance of diffusion strategies\napplied to the Least-Mean-Square adaptive filter. We configure a network of\ncooperative agents running adaptive filters and discuss their behaviour when\ncompared with a non-cooperative agent which represents the average of the\nnetwork. The analysis provides conditions under which diversity in the filter\nparameters is beneficial in terms of convergence and stability. Simulations\ndrive and support the analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 22:59:14 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Gelati", "Jonathan", ""], ["Kanna", "Sithan", ""]]}, {"id": "1402.4861", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Alejandro Ribeiro", "title": "A Quasi-Newton Method for Large Scale Support Vector Machines", "comments": "5 pages, To appear in International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adapts a recently developed regularized stochastic version of the\nBroyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the\nsolution of support vector machine classification problems. The proposed method\nis shown to converge almost surely to the optimal classifier at a rate that is\nlinear in expectation. Numerical results show that the proposed method exhibits\na convergence rate that degrades smoothly with the dimensionality of the\nfeature vectors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 01:44:33 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1402.4862", "submitter": "Raja Hafiz Affandi", "authors": "Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams and Ben Taskar", "title": "Learning the Parameters of Determinantal Point Process Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are well-suited for modeling repulsion\nand have proven useful in many applications where diversity is desired. While\nDPPs have many appealing properties, such as efficient sampling, learning the\nparameters of a DPP is still considered a difficult problem due to the\nnon-convex nature of the likelihood function. In this paper, we propose using\nBayesian methods to learn the DPP kernel parameters. These methods are\napplicable in large-scale and continuous DPP settings even when the exact form\nof the eigendecomposition is unknown. We demonstrate the utility of our DPP\nlearning methods in studying the progression of diabetic neuropathy based on\nspatial distribution of nerve fibers, and in studying human perception of\ndiversity in images.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 01:54:37 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Affandi", "Raja Hafiz", ""], ["Fox", "Emily B.", ""], ["Adams", "Ryan P.", ""], ["Taskar", "Ben", ""]]}, {"id": "1402.4888", "submitter": "Johnvictor D", "authors": "D. Johnvictor, G. Selvavinayagam", "title": "Survey on Sparse Coded Features for Content Based Face Image Retrieval", "comments": "4 pages,3 figures,1 table, Published with International Journal of\n  Computer Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  8(1):30-33, February 2014. ISSN:2231-2803", "doi": "10.14445/22312803/IJCTT-V8P106", "report-no": null, "categories": "cs.IR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content based image retrieval, a technique which uses visual contents of\nimage to search images from large scale image databases according to users'\ninterests. This paper provides a comprehensive survey on recent technology used\nin the area of content based face image retrieval. Nowadays digital devices and\nphoto sharing sites are getting more popularity, large human face photos are\navailable in database. Multiple types of facial features are used to represent\ndiscriminality on large scale human facial image database. Searching and mining\nof facial images are challenging problems and important research issues. Sparse\nrepresentation on features provides significant improvement in indexing related\nimages to query image.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 04:32:40 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Johnvictor", "D.", ""], ["Selvavinayagam", "G.", ""]]}, {"id": "1402.5077", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Group-sparse Matrix Recovery", "comments": "ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the OSCAR (octagonal selection and clustering algorithms for\nregression) in recovering group-sparse matrices (two-dimensional---2D---arrays)\nfrom compressive measurements. We propose a 2D version of OSCAR (2OSCAR)\nconsisting of the $\\ell_1$ norm and the pair-wise $\\ell_{\\infty}$ norm, which\nis convex but non-differentiable. We show that the proximity operator of 2OSCAR\ncan be computed based on that of OSCAR. The 2OSCAR problem can thus be\nefficiently solved by state-of-the-art proximal splitting algorithms.\nExperiments on group-sparse 2D array recovery show that 2OSCAR regularization\nsolved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm\n(with debiasing) yields the most accurate results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 17:08:34 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1402.5131", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi and Anima Anandkumar and Edmond Jonckheere", "title": "Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse\n  Optimization and Noisy Matrix Decomposition", "comments": "appeared in Neural Information Processing Systems(NIPS) 2014. arXiv\n  admin note: text overlap with arXiv:1207.4421 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient ADMM method with guarantees for high-dimensional\nproblems. We provide explicit bounds for the sparse optimization problem and\nthe noisy matrix decomposition problem. For sparse optimization, we establish\nthat the modified ADMM method has an optimal convergence rate of\n$\\mathcal{O}(s\\log d/T)$, where $s$ is the sparsity level, $d$ is the data\ndimension and $T$ is the number of steps. This matches with the minimax lower\nbounds for sparse estimation. For matrix decomposition into sparse and low rank\ncomponents, we provide the first guarantees for any online method, and prove a\nconvergence rate of $\\tilde{\\mathcal{O}}((s+r)\\beta^2(p) /T) +\n\\mathcal{O}(1/p)$ for a $p\\times p$ matrix, where $s$ is the sparsity level,\n$r$ is the rank and $\\Theta(\\sqrt{p})\\leq \\beta(p)\\leq \\Theta(p)$. Our\nguarantees match the minimax lower bound with respect to $s,r$ and $T$. In\naddition, we match the minimax lower bound with respect to the matrix dimension\n$p$, i.e. $\\beta(p)=\\Theta(\\sqrt{p})$, for many important statistical models\nincluding the independent noise model, the linear Bayesian network and the\nlatent Gaussian graphical model under some conditions. Our ADMM method is based\non epoch-based annealing and consists of inexpensive steps which involve\nprojections on to simple norm balls. Experiments show that for both sparse\noptimization and matrix decomposition problems, our algorithm outperforms the\nstate-of-the-art methods. In particular, we reach higher accuracy with same\ntime complexity.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 20:48:10 GMT"}, {"version": "v2", "created": "Tue, 11 Mar 2014 00:15:42 GMT"}, {"version": "v3", "created": "Wed, 19 Mar 2014 09:42:26 GMT"}, {"version": "v4", "created": "Mon, 16 Jun 2014 03:53:05 GMT"}, {"version": "v5", "created": "Sun, 7 Dec 2014 03:36:03 GMT"}, {"version": "v6", "created": "Tue, 7 Jul 2015 00:13:55 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""], ["Jonckheere", "Edmond", ""]]}, {"id": "1402.5164", "submitter": "Justin Thaler", "authors": "Varun Kanade and Justin Thaler", "title": "Distribution-Independent Reliable Learning", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study several questions in the reliable agnostic learning framework of\nKalai et al. (2009), which captures learning tasks in which one type of error\nis costlier than others. A positive reliable classifier is one that makes no\nfalse positive errors. The goal in the positive reliable agnostic framework is\nto output a hypothesis with the following properties: (i) its false positive\nerror rate is at most $\\epsilon$, (ii) its false negative error rate is at most\n$\\epsilon$ more than that of the best positive reliable classifier from the\nclass. A closely related notion is fully reliable agnostic learning, which\nconsiders partial classifiers that are allowed to predict \"unknown\" on some\ninputs. The best fully reliable partial classifier is one that makes no errors\nand minimizes the probability of predicting \"unknown\", and the goal in fully\nreliable learning is to output a hypothesis that is almost as good as the best\nfully reliable partial classifier from a class.\n  For distribution-independent learning, the best known algorithms for PAC\nlearning typically utilize polynomial threshold representations, while the\nstate of the art agnostic learning algorithms use point-wise polynomial\napproximations. We show that one-sided polynomial approximations, an\nintermediate notion between polynomial threshold representations and point-wise\npolynomial approximations, suffice for learning in the reliable agnostic\nsettings. We then show that majorities can be fully reliably learned and\ndisjunctions of majorities can be positive reliably learned, through\nconstructions of appropriate one-sided polynomial approximations. Our fully\nreliable algorithm for majorities provides the first evidence that fully\nreliable learning may be strictly easier than agnostic learning. Our algorithms\nalso satisfy strong attribute-efficiency properties, and provide smooth\ntradeoffs between sample complexity and running time.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 22:41:39 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Kanade", "Varun", ""], ["Thaler", "Justin", ""]]}, {"id": "1402.5176", "submitter": "Ko-Jen Hsiao", "authors": "Ko-Jen Hsiao, Jeff Calder, Alfred O. Hero III", "title": "Pareto-depth for Multiple-query Image Retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2378057", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most content-based image retrieval systems consider either one single query,\nor multiple queries that include the same object or represent the same semantic\ninformation. In this paper we consider the content-based image retrieval\nproblem for multiple query images corresponding to different image semantics.\nWe propose a novel multiple-query information retrieval algorithm that combines\nthe Pareto front method (PFM) with efficient manifold ranking (EMR). We show\nthat our proposed algorithm outperforms state of the art multiple-query\nretrieval algorithms on real-world image databases. We attribute this\nperformance improvement to concavity properties of the Pareto fronts, and prove\na theoretical result that characterizes the asymptotic concavity of the fronts.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 00:42:48 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Hsiao", "Ko-Jen", ""], ["Calder", "Jeff", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1402.5180", "submitter": "Majid Janzamin", "authors": "Animashree Anandkumar and Rong Ge and Majid Janzamin", "title": "Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-$1$\n  Updates", "comments": "We have added an additional sub-algorithm to remove the (approximate)\n  residual error left after the tensor power iteration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide local and global convergence guarantees for\nrecovering CP (Candecomp/Parafac) tensor decomposition. The main step of the\nproposed algorithm is a simple alternating rank-$1$ update which is the\nalternating version of the tensor power iteration adapted for asymmetric\ntensors. Local convergence guarantees are established for third order tensors\nof rank $k$ in $d$ dimensions, when $k=o \\bigl( d^{1.5} \\bigr)$ and the tensor\ncomponents are incoherent. Thus, we can recover overcomplete tensor\ndecomposition. We also strengthen the results to global convergence guarantees\nunder stricter rank condition $k \\le \\beta d$ (for arbitrary constant $\\beta >\n1$) through a simple initialization procedure where the algorithm is\ninitialized by top singular vectors of random tensor slices. Furthermore, the\napproximate local convergence guarantees for $p$-th order tensors are also\nprovided under rank condition $k=o \\bigl( d^{p/2} \\bigr)$. The guarantees also\ninclude tight perturbation analysis given noisy tensor.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 01:37:02 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 16:29:13 GMT"}, {"version": "v3", "created": "Sun, 3 Aug 2014 22:00:58 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2015 20:40:42 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Ge", "Rong", ""], ["Janzamin", "Majid", ""]]}, {"id": "1402.5284", "submitter": "Andr\\'e Uschmajew", "authors": "Reinhold Schneider and Andr\\'e Uschmajew", "title": "Convergence results for projected line-search methods on varieties of\n  low-rank matrices via \\L{}ojasiewicz inequality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to derive convergence results for projected\nline-search methods on the real-algebraic variety $\\mathcal{M}_{\\le k}$ of real\n$m \\times n$ matrices of rank at most $k$. Such methods extend Riemannian\noptimization methods, which are successfully used on the smooth manifold\n$\\mathcal{M}_k$ of rank-$k$ matrices, to its closure by taking steps along\ngradient-related directions in the tangent cone, and afterwards projecting back\nto $\\mathcal{M}_{\\le k}$. Considering such a method circumvents the\ndifficulties which arise from the nonclosedness and the unbounded curvature of\n$\\mathcal{M}_k$. The pointwise convergence is obtained for real-analytic\nfunctions on the basis of a \\L{}ojasiewicz inequality for the projection of the\nantigradient to the tangent cone. If the derived limit point lies on the smooth\npart of $\\mathcal{M}_{\\le k}$, i.e. in $\\mathcal{M}_k$, this boils down to more\nor less known results, but with the benefit that asymptotic convergence rate\nestimates (for specific step-sizes) can be obtained without an a priori\ncurvature bound, simply from the fact that the limit lies on a smooth manifold.\nAt the same time, one can give a convincing justification for assuming critical\npoints to lie in $\\mathcal{M}_k$: if $X$ is a critical point of $f$ on\n$\\mathcal{M}_{\\le k}$, then either $X$ has rank $k$, or $\\nabla f(X) = 0$.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 12:49:51 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 14:46:26 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2015 17:15:02 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Schneider", "Reinhold", ""], ["Uschmajew", "Andr\u00e9", ""]]}, {"id": "1402.5360", "submitter": "Chanabasayya Vastrad M", "authors": "Doreswamy, Chanabasayya M. Vastrad", "title": "Important Molecular Descriptors Selection Using Self Tuned Reweighted\n  Sampling Method for Prediction of Antituberculosis Activity", "comments": "published 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, a new descriptor selection method for selecting an optimal\ncombination of important descriptors of sulfonamide derivatives data, named\nself tuned reweighted sampling (STRS), is developed. descriptors are defined as\nthe descriptors with large absolute coefficients in a multivariate linear\nregression model such as partial least squares(PLS). In this study, the\nabsolute values of regression coefficients of PLS model are used as an index\nfor evaluating the importance of each descriptor Then, based on the importance\nlevel of each descriptor, STRS sequentially selects N subsets of descriptors\nfrom N Monte Carlo (MC) sampling runs in an iterative and competitive manner.\nIn each sampling run, a fixed ratio (e.g. 80%) of samples is first randomly\nselected to establish a regresson model. Next, based on the regression\ncoefficients, a two-step procedure including rapidly decreasing function (RDF)\nbased enforced descriptor selection and self tuned sampling (STS) based\ncompetitive descriptor selection is adopted to select the important\ndescriptorss. After running the loops, a number of subsets of descriptors are\nobtained and root mean squared error of cross validation (RMSECV) of PLS models\nestablished with subsets of descriptors is computed. The subset of descriptors\nwith the lowest RMSECV is considered as the optimal descriptor subset. The\nperformance of the proposed algorithm is evaluated by sulfanomide derivative\ndataset. The results reveal an good characteristic of STRS that it can usually\nlocate an optimal combination of some important descriptors which are\ninterpretable to the biologically of interest. Additionally, our study shows\nthat better prediction is obtained by STRS when compared to full descriptor set\nPLS modeling, Monte Carlo uninformative variable elimination (MC-UVE).\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 17:24:53 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Doreswamy", "", ""], ["Vastrad", "Chanabasayya M.", ""]]}, {"id": "1402.5481", "submitter": "Nathan Kallus", "authors": "Dimitris Bertsimas, Nathan Kallus", "title": "From Predictive to Prescriptive Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we combine ideas from machine learning (ML) and operations\nresearch and management science (OR/MS) in developing a framework, along with\nspecific methods, for using data to prescribe optimal decisions in OR/MS\nproblems. In a departure from other work on data-driven optimization and\nreflecting our practical experience with the data available in applications of\nOR/MS, we consider data consisting, not only of observations of quantities with\ndirect effect on costs/revenues, such as demand or returns, but predominantly\nof observations of associated auxiliary quantities. The main problem of\ninterest is a conditional stochastic optimization problem, given imperfect\nobservations, where the joint probability distributions that specify the\nproblem are unknown. We demonstrate that our proposed solution methods, which\nare inspired by ML methods such as local regression, CART, and random forests,\nare generally applicable to a wide range of decision problems. We prove that\nthey are tractable and asymptotically optimal even when data is not iid and may\nbe censored. We extend this to the case where decision variables may directly\naffect uncertainty in unknown ways, such as pricing's effect on demand. As an\nanalogue to R^2, we develop a metric P termed the coefficient of\nprescriptiveness to measure the prescriptive content of data and the efficacy\nof a policy from an operations perspective. To demonstrate the power of our\napproach in a real-world setting we study an inventory management problem faced\nby the distribution arm of an international media conglomerate, which ships an\naverage of 1bil units per year. We leverage internal data and public online\ndata harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational\ndecisions that outperform baseline measures. Specifically, the data we collect,\nleveraged by our methods, accounts for an 88\\% improvement as measured by our\nP.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 05:10:56 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 17:25:20 GMT"}, {"version": "v3", "created": "Mon, 9 Feb 2015 20:07:10 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 15:36:29 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Kallus", "Nathan", ""]]}, {"id": "1402.5497", "submitter": "Chunhua Shen", "authors": "Yan Yan, Chunhua Shen, Hanzi Wang", "title": "Efficient Semidefinite Spectral Clustering via Lagrange Duality", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient approach to semidefinite spectral clustering (SSC),\nwhich addresses the Frobenius normalization with the positive semidefinite\n(p.s.d.) constraint for spectral clustering. Compared with the original\nFrobenius norm approximation based algorithm, the proposed algorithm can more\naccurately find the closest doubly stochastic approximation to the affinity\nmatrix by considering the p.s.d. constraint. In this paper, SSC is formulated\nas a semidefinite programming (SDP) problem. In order to solve the high\ncomputational complexity of SDP, we present a dual algorithm based on the\nLagrange dual formalization. Two versions of the proposed algorithm are\nproffered: one with less memory usage and the other with faster convergence\nrate. The proposed algorithm has much lower time complexity than that of the\nstandard interior-point based SDP solvers. Experimental results on both UCI\ndata sets and real-world image data sets demonstrate that 1) compared with the\nstate-of-the-art spectral clustering methods, the proposed algorithm achieves\nbetter clustering performance; and 2) our algorithm is much more efficient and\ncan solve larger-scale SSC problems than those standard interior-point SDP\nsolvers.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 09:39:52 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Yan", "Yan", ""], ["Shen", "Chunhua", ""], ["Wang", "Hanzi", ""]]}, {"id": "1402.5565", "submitter": "David M. Johnson", "authors": "David M. Johnson, Caiming Xiong and Jason J. Corso", "title": "Semi-Supervised Nonlinear Distance Metric Learning via Forests of\n  Max-Margin Cluster Hierarchies", "comments": "Manuscript submitted to SIGKDD on 21 Feb 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning is a key problem for many data mining and machine learning\napplications, and has long been dominated by Mahalanobis methods. Recent\nadvances in nonlinear metric learning have demonstrated the potential power of\nnon-Mahalanobis distance functions, particularly tree-based functions. We\npropose a novel nonlinear metric learning method that uses an iterative,\nhierarchical variant of semi-supervised max-margin clustering to construct a\nforest of cluster hierarchies, where each individual hierarchy can be\ninterpreted as a weak metric over the data. By introducing randomness during\nhierarchy training and combining the output of many of the resulting\nsemi-random weak hierarchy metrics, we can obtain a powerful and robust\nnonlinear metric model. This method has two primary contributions: first, it is\nsemi-supervised, incorporating information from both constrained and\nunconstrained points. Second, we take a relaxed approach to constraint\nsatisfaction, allowing the method to satisfy different subsets of the\nconstraints at different levels of the hierarchy rather than attempting to\nsimultaneously satisfy all of them. This leads to a more robust learning\nalgorithm. We compare our method to a number of state-of-the-art benchmarks on\n$k$-nearest neighbor classification, large-scale image retrieval and\nsemi-supervised clustering problems, and find that our algorithm yields results\ncomparable or superior to the state-of-the-art, and is significantly more\nrobust to noise.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 00:26:48 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Johnson", "David M.", ""], ["Xiong", "Caiming", ""], ["Corso", "Jason J.", ""]]}, {"id": "1402.5596", "submitter": "Jason Lee", "authors": "Jason D Lee and Jonathan E Taylor", "title": "Exact Post Model Selection Inference for Marginal Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for post model selection inference, via marginal\nscreening, in linear regression. At the core of this framework is a result that\ncharacterizes the exact distribution of linear functions of the response $y$,\nconditional on the model being selected (``condition on selection\" framework).\nThis allows us to construct valid confidence intervals and hypothesis tests for\nregression coefficients that account for the selection procedure. In contrast\nto recent work in high-dimensional statistics, our results are exact\n(non-asymptotic) and require no eigenvalue-like assumptions on the design\nmatrix $X$. Furthermore, the computational cost of marginal regression,\nconstructing confidence intervals and hypothesis testing is negligible compared\nto the cost of linear regression, thus making our methods particularly suitable\nfor extremely large datasets. Although we focus on marginal screening to\nillustrate the applicability of the condition on selection framework, this\nframework is much more broadly applicable. We show how to apply the proposed\nframework to several other selection procedures including orthogonal matching\npursuit, non-negative least squares, and marginal screening+Lasso.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 10:30:21 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 00:28:21 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Lee", "Jason D", ""], ["Taylor", "Jonathan E", ""]]}, {"id": "1402.5634", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Ambedkar Dukkipati", "title": "To go deep or wide in learning?", "comments": "9 pages, 1 figure, Accepted for publication in Seventeenth\n  International Conference on Artificial Intelligence and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve acceptable performance for AI tasks, one can either use\nsophisticated feature extraction methods as the first layer in a two-layered\nsupervised learning model, or learn the features directly using a deep\n(multi-layered) model. While the first approach is very problem-specific, the\nsecond approach has computational overheads in learning multiple layers and\nfine-tuning of the model. In this paper, we propose an approach called wide\nlearning based on arc-cosine kernels, that learns a single layer of infinite\nwidth. We propose exact and inexact learning strategies for wide learning and\nshow that wide learning with single layer outperforms single layer as well as\ndeep architectures of finite width for some benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 16:51:51 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Pandey", "Gaurav", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1402.5666", "submitter": "Richard Combes", "authors": "Richard Combes, Alexandre Proutiere", "title": "Dynamic Rate and Channel Selection in Cognitive Radio Systems", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate dynamic channel and rate selection in cognitive\nradio systems which exploit a large number of channels free from primary users.\nIn such systems, transmitters may rapidly change the selected (channel, rate)\npair to opportunistically learn and track the pair offering the highest\nthroughput. We formulate the problem of sequential channel and rate selection\nas an online optimization problem, and show its equivalence to a {\\it\nstructured} Multi-Armed Bandit problem. The structure stems from inherent\nproperties of the achieved throughput as a function of the selected channel and\nrate. We derive fundamental performance limits satisfied by {\\it any} channel\nand rate adaptation algorithm, and propose algorithms that achieve (or\napproach) these limits. In turn, the proposed algorithms optimally exploit the\ninherent structure of the throughput. We illustrate the efficiency of our\nalgorithms using both test-bed and simulation experiments, in both stationary\nand non-stationary radio environments. In stationary environments, the packet\nsuccessful transmission probabilities at the various channel and rate pairs do\nnot evolve over time, whereas in non-stationary environments, they may evolve.\nIn practical scenarios, the proposed algorithms are able to track the best\nchannel and rate quite accurately without the need of any explicit measurement\nand feedback of the quality of the various channels.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 20:16:41 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 17:06:23 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Combes", "Richard", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1402.5684", "submitter": "Orhan Firat", "authors": "Orhan Firat and Mete Ozay and Ilke Oztekin and Fatos T. Yarman Vural", "title": "Discriminative Functional Connectivity Measures for Brain Decoding", "comments": "This paper has been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical learning model for classifying cognitive processes\nbased on distributed patterns of neural activation in the brain, acquired via\nfunctional magnetic resonance imaging (fMRI). In the proposed learning method,\nlocal meshes are formed around each voxel. The distance between voxels in the\nmesh is determined by using a functional neighbourhood concept. In order to\ndefine the functional neighbourhood, the similarities between the time series\nrecorded for voxels are measured and functional connectivity matrices are\nconstructed. Then, the local mesh for each voxel is formed by including the\nfunctionally closest neighbouring voxels in the mesh. The relationship between\nthe voxels within a mesh is estimated by using a linear regression model. These\nrelationship vectors, called Functional Connectivity aware Local Relational\nFeatures (FC-LRF) are then used to train a statistical learning machine. The\nproposed method was tested on a recognition memory experiment, including data\npertaining to encoding and retrieval of words belonging to ten different\nsemantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn)\nand Support Vector Machine (SVM), are trained in order to predict the semantic\ncategory of the item being retrieved, based on activation patterns during\nencoding. The classification performance of the Functional Mesh Learning model,\nwhich range in 62%-71% is superior to the classical multi-voxel pattern\nanalysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 22:01:11 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 19:02:07 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Firat", "Orhan", ""], ["Ozay", "Mete", ""], ["Oztekin", "Ilke", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1402.5715", "submitter": "Tejas Kulkarni", "authors": "Ardavan Saeedi, Tejas D Kulkarni, Vikash Mansinghka, Samuel Gershman", "title": "Variational Particle Approximations", "comments": "First two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Approximate inference in high-dimensional, discrete probabilistic models is a\ncentral problem in computational statistics and machine learning. This paper\ndescribes discrete particle variational inference (DPVI), a new approach that\ncombines key strengths of Monte Carlo, variational and search-based techniques.\nDPVI is based on a novel family of particle-based variational approximations\nthat can be fit using simple, fast, deterministic search techniques. Like Monte\nCarlo, DPVI can handle multiple modes, and yields exact results in a\nwell-defined limit. Like unstructured mean-field, DPVI is based on optimizing a\nlower bound on the partition function; when this quantity is not of intrinsic\ninterest, it facilitates convergence assessment and debugging. Like both Monte\nCarlo and combinatorial search, DPVI can take advantage of factorization,\nsequential structure, and custom search operators. This paper defines DPVI\nparticle-based approximation family and partition function lower bounds, along\nwith the sequential DPVI and local DPVI algorithm templates for optimizing\nthem. DPVI is illustrated and evaluated via experiments on lattice Markov\nRandom Fields, nonparametric Bayesian mixtures and block-models, and parametric\nas well as non-parametric hidden Markov models. Results include applications to\nreal-world spike-sorting and relational modeling problems, and show that DPVI\ncan offer appealing time/accuracy trade-offs as compared to multiple\nalternatives.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 03:58:16 GMT"}, {"version": "v2", "created": "Sat, 1 Mar 2014 00:07:30 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2015 04:40:24 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Saeedi", "Ardavan", ""], ["Kulkarni", "Tejas D", ""], ["Mansinghka", "Vikash", ""], ["Gershman", "Samuel", ""]]}, {"id": "1402.5728", "submitter": "Mathukumalli Vidyasagar", "authors": "Mathukumalli Vidyasagar", "title": "Machine Learning Methods in the Computational Biology of Cancer", "comments": "35 pages, three figures", "journal-ref": null, "doi": "10.1098/rspa.2014.0081", "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objectives of this \"perspective\" paper are to review some recent advances\nin sparse feature selection for regression and classification, as well as\ncompressed sensing, and to discuss how these might be used to develop tools to\nadvance personalized cancer therapy. As an illustration of the possibilities, a\nnew algorithm for sparse regression is presented, and is applied to predict the\ntime to tumor recurrence in ovarian cancer. A new algorithm for sparse feature\nselection in classification problems is presented, and its validation in\nendometrial cancer is briefly discussed. Some open problems are also presented.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 06:07:56 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Vidyasagar", "Mathukumalli", ""]]}, {"id": "1402.5731", "submitter": "Cem Aksoylar", "authors": "Cem Aksoylar and Venkatesh Saligrama", "title": "Information-Theoretic Bounds for Adaptive Sparse Recovery", "comments": "Accepted to IEEE ISIT 2014. Better presentation and fixed errors\n  compared to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an information-theoretic lower bound for sample complexity in\nsparse recovery problems where inputs can be chosen sequentially and\nadaptively. This lower bound is in terms of a simple mutual information\nexpression and unifies many different linear and nonlinear observation models.\nUsing this formula we derive bounds for adaptive compressive sensing (CS),\ngroup testing and 1-bit CS problems. We show that adaptivity cannot decrease\nsample complexity in group testing, 1-bit CS and CS with linear sparsity. In\ncontrast, we show there might be mild performance gains for CS in the sublinear\nregime. Our unified analysis also allows characterization of gains due to\nadaptivity from a wider perspective on sparse problems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 06:20:34 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 19:18:08 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Aksoylar", "Cem", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1402.5758", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal and Nikhil R. Devanur", "title": "Bandits with concave rewards and convex knapsacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a very general model for exploration-exploitation\ntradeoff which allows arbitrary concave rewards and convex constraints on the\ndecisions across time, in addition to the customary limitation on the time\nhorizon. This model subsumes the classic multi-armed bandit (MAB) model, and\nthe Bandits with Knapsacks (BwK) model of Badanidiyuru et al.[2013]. We also\nconsider an extension of this model to allow linear contexts, similar to the\nlinear contextual extension of the MAB model. We demonstrate that a natural and\nsimple extension of the UCB family of algorithms for MAB provides a polynomial\ntime algorithm that has near-optimal regret guarantees for this substantially\nmore general model, and matches the bounds provided by Badanidiyuru et\nal.[2013] for the special case of BwK, which is quite surprising. We also\nprovide computationally more efficient algorithms by establishing interesting\nconnections between this problem and other well studied problems/algorithms\nsuch as the Blackwell approachability problem, online convex optimization, and\nthe Frank-Wolfe technique for convex optimization. We give examples of several\nconcrete applications, where this more general model of bandits allows for\nricher and/or more efficient formulations of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 09:27:18 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Agrawal", "Shipra", ""], ["Devanur", "Nikhil R.", ""]]}, {"id": "1402.5766", "submitter": "Adriana Romero", "authors": "Adriana Romero, Petia Radeva and Carlo Gatta", "title": "No more meta-parameter tuning in unsupervised sparse feature learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised\nfeature learning algorithm, which exploits a new way of optimizing for\nsparsity. Experiments on STL-10 show that the method presents state-of-the-art\nperformance and provides discriminative features that generalize well.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 09:49:04 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Romero", "Adriana", ""], ["Radeva", "Petia", ""], ["Gatta", "Carlo", ""]]}, {"id": "1402.5803", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (LORIA), Henrik Ohlsson", "title": "Sparse phase retrieval via group-sparse optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with sparse phase retrieval, i.e., the problem of estimating\na vector from quadratic measurements under the assumption that few components\nare nonzero. In particular, we consider the problem of finding the sparsest\nvector consistent with the measurements and reformulate it as a group-sparse\noptimization problem with linear constraints. Then, we analyze the convex\nrelaxation of the latter based on the minimization of a block l1-norm and show\nvarious exact recovery and stability results in the real and complex cases.\nInvariance to circular shifts and reflections are also discussed for real\nvectors measured via complex matrices.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 11:55:04 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Lauer", "Fabien", "", "LORIA"], ["Ohlsson", "Henrik", ""]]}, {"id": "1402.5836", "submitter": "David Duvenaud", "authors": "David Duvenaud, Oren Rippel, Ryan P. Adams, Zoubin Ghahramani", "title": "Avoiding pathologies in very deep networks", "comments": "Fixed a typo regarding number of layers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Choosing appropriate architectures and regularization strategies for deep\nnetworks is crucial to good predictive performance. To shed light on this\nproblem, we analyze the analogous problem of constructing useful priors on\ncompositions of functions. Specifically, we study the deep Gaussian process, a\ntype of infinitely-wide, deep neural network. We show that in standard\narchitectures, the representational capacity of the network tends to capture\nfewer degrees of freedom as the number of layers increases, retaining only a\nsingle degree of freedom in the limit. We propose an alternate network\narchitecture which does not suffer from this pathology. We also examine deep\ncovariance functions, obtained by composing infinitely many feature transforms.\nLastly, we characterize the class of models obtained by performing dropout on\nGaussian processes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 14:27:40 GMT"}, {"version": "v2", "created": "Sun, 14 Sep 2014 21:50:47 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2016 22:59:45 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Duvenaud", "David", ""], ["Rippel", "Oren", ""], ["Adams", "Ryan P.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.5874", "submitter": "Mohammad Ghasemi Hamed", "authors": "Mohammad Ghasemi Hamed, Mathieu Serrurier, Nicolas Durand", "title": "Predictive Interval Models for Non-parametric Regression", "comments": "This paper has been withdrawn by the authors due to multiple errors\n  in the formulations and equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having a regression model, we are interested in finding two-sided intervals\nthat are guaranteed to contain at least a desired proportion of the conditional\ndistribution of the response variable given a specific combination of\npredictors. We name such intervals predictive intervals. This work presents a\nnew method to find two-sided predictive intervals for non-parametric least\nsquares regression without the homoscedasticity assumption. Our predictive\nintervals are built by using tolerance intervals on prediction errors in the\nquery point's neighborhood. We proposed a predictive interval model test and we\nalso used it as a constraint in our hyper-parameter tuning algorithm. This\ngives an algorithm that finds the smallest reliable predictive intervals for a\ngiven dataset. We also introduce a measure for comparing different interval\nprediction methods yielding intervals having different size and coverage. These\nexperiments show that our methods are more reliable, effective and precise than\nother interval prediction methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 16:16:17 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 10:56:40 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Hamed", "Mohammad Ghasemi", ""], ["Serrurier", "Mathieu", ""], ["Durand", "Nicolas", ""]]}, {"id": "1402.5876", "submitter": "Roberto Calandra", "authors": "Roberto Calandra and Jan Peters and Carl Edward Rasmussen and Marc\n  Peter Deisenroth", "title": "Manifold Gaussian Processes for Regression", "comments": "8 pages, accepted to IJCNN 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness\nassumptions on the structure of the function to be modeled. To model complex\nand non-differentiable functions, these smoothness assumptions are often too\nrestrictive. One way to alleviate this limitation is to find a different\nrepresentation of the data by introducing a feature space. This feature space\nis often learned in an unsupervised way, which might lead to data\nrepresentations that are not useful for the overall regression task. In this\npaper, we propose Manifold Gaussian Processes, a novel supervised method that\njointly learns a transformation of the data into a feature space and a GP\nregression from the feature space to observed space. The Manifold GP is a full\nGP and allows to learn data representations, which are useful for the overall\nregression task. As a proof-of-concept, we evaluate our approach on complex\nnon-smooth functions where standard GPs perform poorly, such as step functions\nand robotics tasks with contacts.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 16:19:51 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 14:59:49 GMT"}, {"version": "v3", "created": "Thu, 8 May 2014 10:02:40 GMT"}, {"version": "v4", "created": "Mon, 11 Apr 2016 11:07:31 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Calandra", "Roberto", ""], ["Peters", "Jan", ""], ["Rasmussen", "Carl Edward", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1402.5886", "submitter": "Shervin Javdani", "authors": "Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, J. Andrew\n  Bagnell, Siddhartha Srinivasa", "title": "Near Optimal Bayesian Active Learning for Decision Making", "comments": "Extended version of work appearing in the International conference on\n  Artificial Intelligence and Statistics (AISTATS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should we gather information to make effective decisions? We address\nBayesian active learning and experimental design problems, where we\nsequentially select tests to reduce uncertainty about a set of hypotheses.\nInstead of minimizing uncertainty per se, we consider a set of overlapping\ndecision regions of these hypotheses. Our goal is to drive uncertainty into a\nsingle decision region as quickly as possible.\n  We identify necessary and sufficient conditions for correctly identifying a\ndecision region that contains all hypotheses consistent with observations. We\ndevelop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove\nthat is competitive with the intractable optimal policy. Our efficient\nimplementation of the algorithm relies on computing subsets of the complete\nhomogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on\ntwo practical applications: approximate comparison-based learning and active\nlocalization using a robot manipulator.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 16:59:21 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Javdani", "Shervin", ""], ["Chen", "Yuxin", ""], ["Karbasi", "Amin", ""], ["Krause", "Andreas", ""], ["Bagnell", "J. Andrew", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "1402.5902", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Krzysztof Choromanski, Sanjiv Kumar, Tony Jebara, Shih-Fu\n  Chang", "title": "On Learning from Label Proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from Label Proportions (LLP) is a learning setting, where the\ntraining data is provided in groups, or \"bags\", and only the proportion of each\nclass in each bag is known. The task is to learn a model to predict the class\nlabels of the individual instances. LLP has broad applications in political\nscience, marketing, healthcare, and computer vision. This work answers the\nfundamental question, when and why LLP is possible, by introducing a general\nframework, Empirical Proportion Risk Minimization (EPRM). EPRM learns an\ninstance label classifier to match the given label proportions on the training\ndata. Our result is based on a two-step analysis. First, we provide a VC bound\non the generalization error of the bag proportions. We show that the bag sample\ncomplexity is only mildly sensitive to the bag size. Second, we show that under\nsome mild assumptions, good bag proportion prediction guarantees good instance\nlabel prediction. The results together provide a formal guarantee that the\nindividual labels can indeed be learned in the LLP setting. We discuss\napplications of the analysis, including justification of LLP algorithms,\nlearning with population proportions, and a paradigm for learning algorithms\nwith privacy guarantees. We also demonstrate the feasibility of LLP based on a\ncase study in real-world setting: predicting income based on census data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 17:40:09 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 23:38:42 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Yu", "Felix X.", ""], ["Choromanski", "Krzysztof", ""], ["Kumar", "Sanjiv", ""], ["Jebara", "Tony", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1402.5988", "submitter": "Nikos Katzouris", "authors": "Nikos Katzouris, Alexander Artikis, George Paliouras", "title": "Incremental Learning of Event Definitions with Inductive Logic\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event recognition systems rely on properly engineered knowledge bases of\nevent definitions to infer occurrences of events in time. The manual\ndevelopment of such knowledge is a tedious and error-prone task, thus\nevent-based applications may benefit from automated knowledge construction\ntechniques, such as Inductive Logic Programming (ILP), which combines machine\nlearning with the declarative and formal semantics of First-Order Logic.\nHowever, learning temporal logical formalisms, which are typically utilized by\nlogic-based Event Recognition systems is a challenging task, which most ILP\nsystems cannot fully undertake. In addition, event-based data is usually\nmassive and collected at different times and under various circumstances.\nIdeally, systems that learn from temporal data should be able to operate in an\nincremental mode, that is, revise prior constructed knowledge in the face of\nnew evidence. Most ILP systems are batch learners, in the sense that in order\nto account for new evidence they have no alternative but to forget past\nknowledge and learn from scratch. Given the increased inherent complexity of\nILP and the volumes of real-life temporal data, this results to algorithms that\nscale poorly. In this work we present an incremental method for learning and\nrevising event-based knowledge, in the form of Event Calculus programs. The\nproposed algorithm relies on abductive-inductive learning and comprises a\nscalable clause refinement methodology, based on a compressive summarization of\nclause coverage in a stream of examples. We present an empirical evaluation of\nour approach on real and synthetic data from activity recognition and city\ntransport applications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 21:22:51 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 13:24:29 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Katzouris", "Nikos", ""], ["Artikis", "Alexander", ""], ["Paliouras", "George", ""]]}, {"id": "1402.6013", "submitter": "Joaquin Vanschoren", "authors": "Joaquin Vanschoren and Mikio L. Braun and Cheng Soon Ong", "title": "Open science in machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present OpenML and mldata, open science platforms that provides easy\naccess to machine learning data, software and results to encourage further\nstudy and application. They go beyond the more traditional repositories for\ndata sets and software packages in that they allow researchers to also easily\nshare the results they obtained in experiments and to compare their solutions\nwith those of others.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 23:12:42 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Vanschoren", "Joaquin", ""], ["Braun", "Mikio L.", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1402.6028", "submitter": "Volodymyr Kuleshov", "authors": "Volodymyr Kuleshov and Doina Precup", "title": "Algorithms for multi-armed bandit problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many algorithms for the multi-armed bandit problem are\nwell-understood theoretically, empirical confirmation of their effectiveness is\ngenerally scarce. This paper presents a thorough empirical study of the most\npopular multi-armed bandit algorithms. Three important observations can be made\nfrom our results. Firstly, simple heuristics such as epsilon-greedy and\nBoltzmann exploration outperform theoretically sound algorithms on most\nsettings by a significant margin. Secondly, the performance of most algorithms\nvaries dramatically with the parameters of the bandit problem. Our study\nidentifies for each algorithm the settings where it performs well, and the\nsettings where it performs poorly. Thirdly, the algorithms' performance\nrelative each to other is affected only by the number of bandit arms and the\nvariance of the rewards. This finding may guide the design of subsequent\nempirical evaluations. In the second part of the paper, we turn our attention\nto an important area of application of bandit algorithms: clinical trials.\nAlthough the design of clinical trials has been one of the principal practical\nproblems motivating research on multi-armed bandits, bandit algorithms have\nnever been evaluated as potential treatment allocation strategies. Using data\nfrom a real study, we simulate the outcome that a 2001-2002 clinical trial\nwould have had if bandit algorithms had been used to allocate patients to\ntreatments. We find that an adaptive trial would have successfully treated at\nleast 50% more patients, while significantly reducing the number of adverse\neffects and increasing patient retention. At the end of the trial, the best\ntreatment could have still been identified with a high level of statistical\nconfidence. Our findings demonstrate that bandit algorithms are attractive\nalternatives to current adaptive treatment allocation strategies.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 01:34:43 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Kuleshov", "Volodymyr", ""], ["Precup", "Doina", ""]]}, {"id": "1402.6076", "submitter": "Sergei Izrailev", "authors": "Sergei Izrailev and Jeremy M. Stanley", "title": "Machine Learning at Scale", "comments": "Submitted to KDD'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It takes skill to build a meaningful predictive model even with the abundance\nof implementations of modern machine learning algorithms and readily available\ncomputing resources. Building a model becomes challenging if hundreds of\nterabytes of data need to be processed to produce the training data set. In a\ndigital advertising technology setting, we are faced with the need to build\nthousands of such models that predict user behavior and power advertising\ncampaigns in a 24/7 chaotic real-time production environment. As data\nscientists, we also have to convince other internal departments critical to\nimplementation success, our management, and our customers that our machine\nlearning system works. In this paper, we present the details of the design and\nimplementation of an automated, robust machine learning platform that impacts\nbillions of advertising impressions monthly. This platform enables us to\ncontinuously optimize thousands of campaigns over hundreds of millions of\nusers, on multiple continents, against varying performance objectives.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 07:50:50 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Izrailev", "Sergei", ""], ["Stanley", "Jeremy M.", ""]]}, {"id": "1402.6077", "submitter": "Zhi-Hua Zhou", "authors": "Wang-Zhou Dai and Zhi-Hua Zhou", "title": "Inductive Logic Boosting", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a surge of interest in Probabilistic Logic Programming\n(PLP) and Statistical Relational Learning (SRL) models that combine logic with\nprobabilities. Structure learning of these systems is an intersection area of\nInductive Logic Programming (ILP) and statistical learning (SL). However, ILP\ncannot deal with probabilities, SL cannot model relational hypothesis. The\nbiggest challenge of integrating these two machine learning frameworks is how\nto estimate the probability of a logic clause only from the observation of\ngrounded logic atoms. Many current methods models a joint probability by\nrepresenting clause as graphical model and literals as vertices in it. This\nmodel is still too complicate and only can be approximate by pseudo-likelihood.\nWe propose Inductive Logic Boosting framework to transform the relational\ndataset into a feature-based dataset, induces logic rules by boosting Problog\nRule Trees and relaxes the independence constraint of pseudo-likelihood.\nExperimental evaluation on benchmark datasets demonstrates that the AUC-PR and\nAUC-ROC value of ILP learned rules are higher than current state-of-the-art SRL\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 07:53:49 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Dai", "Wang-Zhou", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1402.6133", "submitter": "Siddhant Sahu", "authors": "Siddhant Sahu, V. Sugumaran", "title": "Bayesian Sample Size Determination of Vibration Signals in Machine\n  Learning Approach to Fault Diagnosis of Roller Bearings", "comments": "14 pages, 1 table, 6 figures", "journal-ref": "Intentional Journal of Research in Mechanical Engineering, Volume\n  1, Issue 1, July-September, 2013, pp. 55-63, IASTER", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample size determination for a data set is an important statistical process\nfor analyzing the data to an optimum level of accuracy and using minimum\ncomputational work. The applications of this process are credible in every\ndomain which deals with large data sets and high computational work. This study\nuses Bayesian analysis for determination of minimum sample size of vibration\nsignals to be considered for fault diagnosis of a bearing using pre-defined\nparameters such as the inverse standard probability and the acceptable margin\nof error. Thus an analytical formula for sample size determination is\nintroduced. The fault diagnosis of the bearing is done using a machine learning\napproach using an entropy-based J48 algorithm. The following method will help\nresearchers involved in fault diagnosis to determine minimum sample size of\ndata for analysis for a good statistical stability and precision.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 11:11:28 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Sahu", "Siddhant", ""], ["Sugumaran", "V.", ""]]}, {"id": "1402.6238", "submitter": "Jobin Wilson", "authors": "Jobin Wilson, Santanu Chaudhury, Brejesh Lall, Prateek Kapadia", "title": "Improving Collaborative Filtering based Recommenders using Topic\n  Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Collaborative Filtering (CF) algorithms make use of interactions\nbetween users and items in the form of implicit or explicit ratings alone for\ngenerating recommendations. Similarity among users or items is calculated\npurely based on rating overlap in this case,without considering explicit\nproperties of users or items involved, limiting their applicability in domains\nwith very sparse rating spaces. In many domains such as movies, news or\nelectronic commerce recommenders, considerable contextual data in text form\ndescribing item properties is available along with the rating data, which could\nbe utilized to improve recommendation quality.In this paper, we propose a novel\napproach to improve standard CF based recommenders by utilizing latent\nDirichlet allocation (LDA) to learn latent properties of items, expressed in\nterms of topic proportions, derived from their textual description. We infer\nuser's topic preferences or persona in the same latent space,based on her\nhistorical ratings. While computing similarity between users, we make use of a\ncombined similarity measure involving rating overlap as well as similarity in\nthe latent topic space. This approach alleviates sparsity problem as it allows\ncalculation of similarity between users even if they have not rated any items\nin common. Our experiments on multiple public datasets indicate that the\nproposed hybrid approach significantly outperforms standard user Based and item\nBased CF recommenders in terms of classification accuracy metrics such as\nprecision, recall and f-measure.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 16:52:05 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Wilson", "Jobin", ""], ["Chaudhury", "Santanu", ""], ["Lall", "Brejesh", ""], ["Kapadia", "Prateek", ""]]}, {"id": "1402.6278", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and David Xiao", "title": "Sample Complexity Bounds on Differentially Private Learning via\n  Communication Complexity", "comments": "Extended abstract appears in Conference on Learning Theory (COLT)\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we analyze the sample complexity of classification by\ndifferentially private algorithms. Differential privacy is a strong and\nwell-studied notion of privacy introduced by Dwork et al. (2006) that ensures\nthat the output of an algorithm leaks little information about the data point\nprovided by any of the participating individuals. Sample complexity of private\nPAC and agnostic learning was studied in a number of prior works starting with\n(Kasiviswanathan et al., 2008) but a number of basic questions still remain\nopen, most notably whether learning with privacy requires more samples than\nlearning without privacy.\n  We show that the sample complexity of learning with (pure) differential\nprivacy can be arbitrarily higher than the sample complexity of learning\nwithout the privacy constraint or the sample complexity of learning with\napproximate differential privacy. Our second contribution and the main tool is\nan equivalence between the sample complexity of (pure) differentially private\nlearning of a concept class $C$ (or $SCDP(C)$) and the randomized one-way\ncommunication complexity of the evaluation problem for concepts from $C$. Using\nthis equivalence we prove the following bounds:\n  1. $SCDP(C) = \\Omega(LDim(C))$, where $LDim(C)$ is the Littlestone's (1987)\ndimension characterizing the number of mistakes in the online-mistake-bound\nlearning model. Known bounds on $LDim(C)$ then imply that $SCDP(C)$ can be much\nhigher than the VC-dimension of $C$.\n  2. For any $t$, there exists a class $C$ such that $LDim(C)=2$ but $SCDP(C)\n\\geq t$.\n  3. For any $t$, there exists a class $C$ such that the sample complexity of\n(pure) $\\alpha$-differentially private PAC learning is $\\Omega(t/\\alpha)$ but\nthe sample complexity of the relaxed $(\\alpha,\\beta)$-differentially private\nPAC learning is $O(\\log(1/\\beta)/\\alpha)$. This resolves an open problem of\nBeimel et al. (2013b).\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 19:00:15 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2014 04:14:50 GMT"}, {"version": "v3", "created": "Tue, 27 May 2014 02:06:43 GMT"}, {"version": "v4", "created": "Sun, 13 Sep 2015 04:53:25 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Feldman", "Vitaly", ""], ["Xiao", "David", ""]]}, {"id": "1402.6361", "submitter": "Tomer Koren", "authors": "Aharon Ben-Tal, Elad Hazan, Tomer Koren, Shie Mannor", "title": "Oracle-Based Robust Optimization via Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust optimization is a common framework in optimization under uncertainty\nwhen the problem parameters are not known, but it is rather known that the\nparameters belong to some given uncertainty set. In the robust optimization\nframework the problem solved is a min-max problem where a solution is judged\naccording to its performance on the worst possible realization of the\nparameters. In many cases, a straightforward solution of the robust\noptimization problem of a certain type requires solving an optimization problem\nof a more complicated type, and in some cases even NP-hard. For example,\nsolving a robust conic quadratic program, such as those arising in robust SVM,\nellipsoidal uncertainty leads in general to a semidefinite program. In this\npaper we develop a method for approximately solving a robust optimization\nproblem using tools from online convex optimization, where in every stage a\nstandard (non-robust) optimization program is solved. Our algorithms find an\napproximate robust solution using a number of calls to an oracle that solves\nthe original (non-robust) problem that is inversely proportional to the square\nof the target accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 22:06:58 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Ben-Tal", "Aharon", ""], ["Hazan", "Elad", ""], ["Koren", "Tomer", ""], ["Mannor", "Shie", ""]]}, {"id": "1402.6552", "submitter": "Ankur Sahai", "authors": "Ankur Sahai", "title": "Renewable Energy Prediction using Weather Forecasts for Optimal\n  Scheduling in HPC Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the GreenPAD project is to use green energy (wind, solar and\nbiomass) for powering data-centers that are used to run HPC jobs. As a part of\nthis it is important to predict the Renewable (Wind) energy for efficient\nscheduling (executing jobs that require higher energy when there is more green\nenergy available and vice-versa). For predicting the wind energy we first\nanalyze the historical data to find a statistical model that gives relation\nbetween wind energy and weather attributes. Then we use this model based on the\nweather forecast data to predict the green energy availability in the future.\nUsing the green energy prediction obtained from the statistical model we are\nable to precompute job schedules for maximizing the green energy utilization in\nthe future. We propose a model which uses live weather data in addition to\nmachine learning techniques (which can predict future deviations in weather\nconditions based on current deviations from the forecast) to make on-the-fly\nchanges to the precomputed schedule (based on green energy prediction).\n  For this we first analyze the data using histograms and simple statistical\ntools such as correlation. In addition we build (correlation) regression model\nfor finding the relation between wind energy availability and weather\nattributes (temperature, cloud cover, air pressure, wind speed / direction,\nprecipitation and sunshine). We also analyze different algorithms and machine\nlearning techniques for optimizing the job schedules for maximizing the green\nenergy utilization.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 14:29:33 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Sahai", "Ankur", ""]]}, {"id": "1402.6779", "submitter": "Aleksandrs Slivkins", "authors": "Ashwinkumar Badanidiyuru and John Langford and Aleksandrs Slivkins", "title": "Resourceful Contextual Bandits", "comments": "This is the full version of a paper in COLT 2014. Version history:\n  (v2) Added some details to one of the proofs, (v3) a big revision following\n  comments from COLT reviewers (but no new results), (v4) edits in related\n  work, minor edits elsewhere. (v6) A correction for Theorem 3, corollary for\n  contextual dynamic pricing with discretization; updated follow-up work & open\n  questions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study contextual bandits with ancillary constraints on resources, which\nare common in real-world applications such as choosing ads or dynamic pricing\nof items. We design the first algorithm for solving these problems that handles\nconstrained resources other than time, and improves over a trivial reduction to\nthe non-contextual case. We consider very general settings for both contextual\nbandits (arbitrary policy sets, e.g. Dudik et al. (UAI'11)) and bandits with\nresource constraints (bandits with knapsacks, Badanidiyuru et al. (FOCS'13)),\nand prove a regret guarantee with near-optimal statistical properties.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 03:17:19 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 22:00:13 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 23:01:03 GMT"}, {"version": "v4", "created": "Tue, 1 Jul 2014 14:55:01 GMT"}, {"version": "v5", "created": "Mon, 13 Jul 2015 00:12:19 GMT"}, {"version": "v6", "created": "Fri, 31 Jul 2015 18:31:27 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Badanidiyuru", "Ashwinkumar", ""], ["Langford", "John", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1402.6859", "submitter": "Ahmed Ibrahim Taloba", "authors": "M. H. Marghny, Ahmed I. Taloba", "title": "Outlier Detection using Improved Genetic K-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The outlier detection problem in some cases is similar to the classification\nproblem. For example, the main concern of clustering-based outlier detection\nalgorithms is to find clusters and outliers, which are often regarded as noise\nthat should be removed in order to make more reliable clustering. In this\narticle, we present an algorithm that provides outlier detection and data\nclustering simultaneously. The algorithmimprovesthe estimation of centroids of\nthe generative distribution during the process of clustering and outlier\ndiscovery. The proposed algorithm consists of two stages. The first stage\nconsists of improved genetic k-means algorithm (IGK) process, while the second\nstage iteratively removes the vectors which are far from their cluster\ncentroids.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 11:07:00 GMT"}], "update_date": "2014-05-25", "authors_parsed": [["Marghny", "M. H.", ""], ["Taloba", "Ahmed I.", ""]]}, {"id": "1402.6926", "submitter": "Peter Foster", "authors": "Peter Foster, Matthias Mauch and Simon Dixon", "title": "Sequential Complexity as a Descriptor for Musical Similarity", "comments": "13 pages, 9 figures, 8 tables. Accepted version", "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol. 22 no. 12, pp. 1965-1977, 2014", "doi": "10.1109/TASLP.2014.2357676", "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose string compressibility as a descriptor of temporal structure in\naudio, for the purpose of determining musical similarity. Our descriptors are\nbased on computing track-wise compression rates of quantised audio features,\nusing multiple temporal resolutions and quantisation granularities. To verify\nthat our descriptors capture musically relevant information, we incorporate our\ndescriptors into similarity rating prediction and song year prediction tasks.\nWe base our evaluation on a dataset of 15500 track excerpts of Western popular\nmusic, for which we obtain 7800 web-sourced pairwise similarity ratings. To\nassess the agreement among similarity ratings, we perform an evaluation under\ncontrolled conditions, obtaining a rank correlation of 0.33 between intersected\nsets of ratings. Combined with bag-of-features descriptors, we obtain\nperformance gains of 31.1% and 10.9% for similarity rating prediction and song\nyear prediction. For both tasks, analysis of selected descriptors reveals that\nrepresenting features at multiple time scales benefits prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 14:51:48 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 15:14:37 GMT"}, {"version": "v3", "created": "Sun, 28 Sep 2014 23:33:44 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Foster", "Peter", ""], ["Mauch", "Matthias", ""], ["Dixon", "Simon", ""]]}, {"id": "1402.6964", "submitter": "Austin Benson", "authors": "Austin R. Benson, Jason D. Lee, Bartek Rajwa, David F. Gleich", "title": "Scalable methods for nonnegative matrix factorizations of near-separable\n  tall-and-skinny matrices", "comments": null, "journal-ref": "Proceedings of Neural Information Processing Systems, 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous algorithms are used for nonnegative matrix factorization under the\nassumption that the matrix is nearly separable. In this paper, we show how to\nmake these algorithms efficient for data matrices that have many more rows than\ncolumns, so-called \"tall-and-skinny matrices\". One key component to these\nimproved methods is an orthogonal matrix transformation that preserves the\nseparability of the NMF problem. Our final methods need a single pass over the\ndata matrix and are suitable for streaming, multi-core, and MapReduce\narchitectures. We demonstrate the efficacy of these algorithms on\nterabyte-sized synthetic matrices and real-world matrices from scientific\ncomputing and bioinformatics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 16:41:26 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Benson", "Austin R.", ""], ["Lee", "Jason D.", ""], ["Rajwa", "Bartek", ""], ["Gleich", "David F.", ""]]}, {"id": "1402.7001", "submitter": "Laurens van der Maaten", "authors": "Laurens van der Maaten, Minmin Chen, Stephen Tyree and Kilian\n  Weinberger", "title": "Marginalizing Corrupted Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of machine learning is to develop predictors that generalize well to\ntest data. Ideally, this is achieved by training on an almost infinitely large\ntraining data set that captures all variations in the data distribution. In\npractical learning settings, however, we do not have infinite data and our\npredictors may overfit. Overfitting may be combatted, for example, by adding a\nregularizer to the training objective or by defining a prior over the model\nparameters and performing Bayesian inference. In this paper, we propose a\nthird, alternative approach to combat overfitting: we extend the training set\nwith infinitely many artificial training examples that are obtained by\ncorrupting the original training data. We show that this approach is practical\nand efficient for a range of predictors and corruption models. Our approach,\ncalled marginalized corrupted features (MCF), trains robust predictors by\nminimizing the expected value of the loss function under the corruption model.\nWe show empirically on a variety of data sets that MCF classifiers can be\ntrained efficiently, may generalize substantially better to test data, and are\nalso more robust to feature deletion at test time.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 18:31:33 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["van der Maaten", "Laurens", ""], ["Chen", "Minmin", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian", ""]]}, {"id": "1402.7005", "submitter": "Ziyu Wang", "authors": "Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas", "title": "Bayesian Multi-Scale Optimistic Optimization", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a powerful global optimization technique for\nexpensive black-box functions. One of its shortcomings is that it requires\nauxiliary optimization of an acquisition function at each iteration. This\nauxiliary optimization can be costly and very hard to carry out in practice.\nMoreover, it creates serious theoretical concerns, as most of the convergence\nresults assume that the exact optimum of the acquisition function can be found.\nIn this paper, we introduce a new technique for efficient global optimization\nthat combines Gaussian process confidence bounds and treed simultaneous\noptimistic optimization to eliminate the need for auxiliary optimization of\nacquisition functions. The experiments with global optimization benchmarks and\na novel application to automatic information extraction demonstrate that the\nresulting technique is more efficient than the two approaches from which it\ndraws inspiration. Unlike most theoretical analyses of Bayesian optimization\nwith Gaussian processes, our finite-time convergence rate proofs do not require\nexact optimization of an acquisition function. That is, our approach eliminates\nthe unsatisfactory assumption that a difficult, potentially NP-hard, problem\nhas to be solved in order to obtain vanishing regret rates.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 18:38:02 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Wang", "Ziyu", ""], ["Shakibi", "Babak", ""], ["Jin", "Lin", ""], ["de Freitas", "Nando", ""]]}, {"id": "1402.7015", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa (INRIA Saclay - Ile de France, INRIA Paris -\n  Rocquencourt), Michael Eickenberg (INRIA Saclay - Ile de France, LNAO),\n  Philippe Ciuciu (INRIA Saclay - Ile de France, NEUROSPIN), Bertrand Thirion\n  (INRIA Saclay - Ile de France, NEUROSPIN), Alexandre Gramfort (LTCI)", "title": "Data-driven HRF estimation for encoding and decoding models", "comments": "appears in NeuroImage (2015)", "journal-ref": null, "doi": "10.1016/j.neuroimage.2014.09.060", "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the common usage of a canonical, data-independent, hemodynamic\nresponse function (HRF), it is known that the shape of the HRF varies across\nbrain regions and subjects. This suggests that a data-driven estimation of this\nfunction could lead to more statistical power when modeling BOLD fMRI data.\nHowever, unconstrained estimation of the HRF can yield highly unstable results\nwhen the number of free parameters is large. We develop a method for the joint\nestimation of activation and HRF using a rank constraint causing the estimated\nHRF to be equal across events/conditions, yet permitting it to be different\nacross voxels. Model estimation leads to an optimization problem that we\npropose to solve with an efficient quasi-Newton method exploiting fast gradient\ncomputations. This model, called GLM with Rank-1 constraint (R1-GLM), can be\nextended to the setting of GLM with separate designs which has been shown to\nimprove decoding accuracy in brain activity decoding experiments. We compare 10\ndifferent HRF modeling methods in terms of encoding and decoding score in two\ndifferent datasets. Our results show that the R1-GLM model significantly\noutperforms competing methods in both encoding and decoding settings,\npositioning it as an attractive method both from the points of view of accuracy\nand computational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 18:50:58 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2014 06:11:17 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 11:14:00 GMT"}, {"version": "v4", "created": "Mon, 6 Oct 2014 16:39:55 GMT"}, {"version": "v5", "created": "Fri, 31 Oct 2014 13:47:01 GMT"}, {"version": "v6", "created": "Fri, 7 Nov 2014 11:27:19 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Pedregosa", "Fabian", "", "INRIA Saclay - Ile de France, INRIA Paris -\n  Rocquencourt"], ["Eickenberg", "Michael", "", "INRIA Saclay - Ile de France, LNAO"], ["Ciuciu", "Philippe", "", "INRIA Saclay - Ile de France, NEUROSPIN"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de France, NEUROSPIN"], ["Gramfort", "Alexandre", "", "LTCI"]]}, {"id": "1402.7025", "submitter": "Max Welling", "authors": "Max Welling", "title": "Exploiting the Statistics of Learning and Inference", "comments": "Proceedings of the NIPS workshop on \"Probabilistic Models for Big\n  Data\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with datasets containing a billion instances or with simulations\nthat require a supercomputer to execute, computational resources become part of\nthe equation. We can improve the efficiency of learning and inference by\nexploiting their inherent statistical nature. We propose algorithms that\nexploit the redundancy of data relative to a model by subsampling data-cases\nfor every update and reasoning about the uncertainty created in this process.\nIn the context of learning we propose to test for the probability that a\nstochastically estimated gradient points more than 180 degrees in the wrong\ndirection. In the context of MCMC sampling we use stochastic gradients to\nimprove the efficiency of MCMC updates, and hypothesis tests based on adaptive\nmini-batches to decide whether to accept or reject a proposed parameter update.\nFinally, we argue that in the context of likelihood free MCMC one needs to\nstore all the information revealed by all simulations, for instance in a\nGaussian process. We conclude that Bayesian methods will remain to play a\ncrucial role in the era of big data and big simulations, but only if we\novercome a number of computational challenges.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 10:47:09 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 21:12:43 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Welling", "Max", ""]]}, {"id": "1402.7344", "submitter": "Menghan Wang", "authors": "Meera Sitharam, Mohamad Tarifi, Menghan Wang", "title": "An Incidence Geometry approach to Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Dictionary Learning (aka Sparse Coding) problem of obtaining a\nsparse representation of data points, by learning \\emph{dictionary vectors}\nupon which the data points can be written as sparse linear combinations. We\nview this problem from a geometry perspective as the spanning set of a subspace\narrangement, and focus on understanding the case when the underlying hypergraph\nof the subspace arrangement is specified. For this Fitted Dictionary Learning\nproblem, we completely characterize the combinatorics of the associated\nsubspace arrangements (i.e.\\ their underlying hypergraphs). Specifically, a\ncombinatorial rigidity-type theorem is proven for a type of geometric incidence\nsystem. The theorem characterizes the hypergraphs of subspace arrangements that\ngenerically yield (a) at least one dictionary (b) a locally unique dictionary\n(i.e.\\ at most a finite number of isolated dictionaries) of the specified size.\nWe are unaware of prior application of combinatorial rigidity techniques in the\nsetting of Dictionary Learning, or even in machine learning. We also provide a\nsystematic classification of problems related to Dictionary Learning together\nwith various algorithms, their assumptions and performance.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 18:54:07 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 03:15:39 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Sitharam", "Meera", ""], ["Tarifi", "Mohamad", ""], ["Wang", "Menghan", ""]]}]