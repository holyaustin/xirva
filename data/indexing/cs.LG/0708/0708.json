[{"id": "0708.0171", "submitter": "Jean-Philippe Vert", "authors": "Pierre Mah\\'e (XRCE), Jean-Philippe Vert (CB)", "title": "Virtual screening with support vector machines and structure kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": null, "abstract": "  Support vector machines and kernel methods have recently gained considerable\nattention in chemoinformatics. They offer generally good performance for\nproblems of supervised classification or regression, and provide a flexible and\ncomputationally efficient framework to include relevant information and prior\nknowledge about the data and problems to be handled. In particular, with kernel\nmethods molecules do not need to be represented and stored explicitly as\nvectors or fingerprints, but only to be compared to each other through a\ncomparison function technically called a kernel. While classical kernels can be\nused to compare vector or fingerprint representations of molecules, completely\nnew kernels were developed in the recent years to directly compare the 2D or 3D\nstructures of molecules, without the need for an explicit vectorization step\nthrough the extraction of molecular descriptors. While still in their infancy,\nthese approaches have already demonstrated their relevance on several toxicity\nprediction and structure-activity relationship problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2007 19:13:52 GMT"}], "update_date": "2007-08-02", "authors_parsed": [["Mah\u00e9", "Pierre", "", "XRCE"], ["Vert", "Jean-Philippe", "", "CB"]]}, {"id": "0708.0654", "submitter": "James P. Crutchfield", "authors": "Susanne Still, James P. Crutchfield", "title": "Structure or Noise?", "comments": "6 pages, 2 figures;\n  http://cse.ucdavis.edu/~cmg/compmech/pubs/son.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cond-mat.stat-mech cs.IT cs.LG math-ph math.IT math.MP math.ST nlin.CD stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how rate-distortion theory provides a mechanism for automated theory\nbuilding by naturally distinguishing between regularity and randomness. We\nstart from the simple principle that model variables should, as much as\npossible, render the future and past conditionally independent. From this, we\nconstruct an objective function for model making whose extrema embody the\ntrade-off between a model's structural complexity and its predictive power. The\nsolutions correspond to a hierarchy of models that, at each level of\ncomplexity, achieve optimal predictive power at minimal cost. In the limit of\nmaximal prediction the resulting optimal model identifies a process's intrinsic\norganization by extracting the underlying causal states. In this limit, the\nmodel's complexity is given by the statistical complexity, which is known to be\nminimal for achieving maximum prediction. Examples show how theory building can\nprofit from analyzing a process's causal compressibility, which is reflected in\nthe optimal models' rate-distortion curve--the process's characteristic for\noptimally balancing structure and noise at different levels of representation.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2007 01:37:53 GMT"}, {"version": "v2", "created": "Sun, 29 Jun 2008 23:52:26 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Still", "Susanne", ""], ["Crutchfield", "James P.", ""]]}, {"id": "0708.1242", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis and Christian Savu-Krohn", "title": "Cost-minimising strategies for data labelling : optimal stopping and\n  active learning", "comments": "17 pages, 4 figures. Corrected some errors and changed the flow of\n  the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  Supervised learning deals with the inference of a distribution over an output\nor label space $\\CY$ conditioned on points in an observation space $\\CX$, given\na training dataset $D$ of pairs in $\\CX \\times \\CY$. However, in a lot of\napplications of interest, acquisition of large amounts of observations is easy,\nwhile the process of generating labels is time-consuming or costly. One way to\ndeal with this problem is {\\em active} learning, where points to be labelled\nare selected with the aim of creating a model with better performance than that\nof an model trained on an equal number of randomly sampled points. In this\npaper, we instead propose to deal with the labelling cost directly: The\nlearning goal is defined as the minimisation of a cost which is a function of\nthe expected model performance and the total cost of the labels used. This\nallows the development of general strategies and specific algorithms for (a)\noptimal stopping, where the expected cost dictates whether label acquisition\nshould continue (b) empirical evaluation, where the cost is used as a\nperformance metric for a given combination of inference, stopping and sampling\nmethods. Though the main focus of the paper is optimal stopping, we also aim to\nprovide the background for further developments and discussion in the related\nfield of active learning.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2007 10:21:34 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2007 22:05:57 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2007 16:37:51 GMT"}], "update_date": "2007-11-15", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Savu-Krohn", "Christian", ""]]}, {"id": "0708.1503", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Defensive forecasting for optimal prediction with expert advice", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  The method of defensive forecasting is applied to the problem of prediction\nwith expert advice for binary outcomes. It turns out that defensive forecasting\nis not only competitive with the Aggregating Algorithm but also handles the\ncase of \"second-guessing\" experts, whose advice depends on the learner's\nprediction; this paper assumes that the dependence on the learner's prediction\nis continuous.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2007 19:19:54 GMT"}], "update_date": "2007-08-13", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "0708.1580", "submitter": "James P. Crutchfield", "authors": "Susanne Still, James P. Crutchfield, Christopher J. Ellison", "title": "Optimal Causal Inference: Estimating Stored Information and\n  Approximating Causal Architecture", "comments": "14 pages, 13 figures;\n  http://cse.ucdavis.edu/~cmg/compmech/pubs/oci.htm; Updated figures and\n  citations; added corrections and clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cond-mat.stat-mech cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to inferring the causal architecture of stochastic\ndynamical systems that extends rate distortion theory to use causal\nshielding---a natural principle of learning. We study two distinct cases of\ncausal inference: optimal causal filtering and optimal causal estimation.\n  Filtering corresponds to the ideal case in which the probability distribution\nof measurement sequences is known, giving a principled method to approximate a\nsystem's causal structure at a desired level of representation. We show that,\nin the limit in which a model complexity constraint is relaxed, filtering finds\nthe exact causal architecture of a stochastic dynamical system, known as the\ncausal-state partition. From this, one can estimate the amount of historical\ninformation the process stores. More generally, causal filtering finds a graded\nmodel-complexity hierarchy of approximations to the causal architecture. Abrupt\nchanges in the hierarchy, as a function of approximation, capture distinct\nscales of structural organization.\n  For nonideal cases with finite data, we show how the correct number of\nunderlying causal states can be found by optimal causal estimation. A\npreviously derived model complexity control term allows us to correct for the\neffect of statistical fluctuations in probability estimates and thereby avoid\nover-fitting.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2007 19:13:29 GMT"}, {"version": "v2", "created": "Thu, 19 Aug 2010 23:46:24 GMT"}], "update_date": "2010-08-23", "authors_parsed": [["Still", "Susanne", ""], ["Crutchfield", "James P.", ""], ["Ellison", "Christopher J.", ""]]}, {"id": "0708.2319", "submitter": "Marcus Hutter", "authors": "Marcus Hutter and Andrej Muchnik", "title": "On Semimeasures Predicting Martin-Loef Random Sequences", "comments": "21 LaTeX pages", "journal-ref": "Theoretical Computer Science, 382 (2007) 247-261", "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.PR", "license": null, "abstract": "  Solomonoff's central result on induction is that the posterior of a universal\nsemimeasure M converges rapidly and with probability 1 to the true sequence\ngenerating posterior mu, if the latter is computable. Hence, M is eligible as a\nuniversal sequence predictor in case of unknown mu. Despite some nearby results\nand proofs in the literature, the stronger result of convergence for all\n(Martin-Loef) random sequences remained open. Such a convergence result would\nbe particularly interesting and natural, since randomness can be defined in\nterms of M itself. We show that there are universal semimeasures M which do not\nconverge for all random sequences, i.e. we give a partial negative answer to\nthe open problem. We also provide a positive answer for some non-universal\nsemimeasures. We define the incomputable measure D as a mixture over all\ncomputable measures and the enumerable semimeasure W as a mixture over all\nenumerable nearly-measures. We show that W converges to D and D to mu on all\nrandom sequences. The Hellinger distance measuring closeness of two\ndistributions plays a central role.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2007 06:39:11 GMT"}], "update_date": "2007-08-20", "authors_parsed": [["Hutter", "Marcus", ""], ["Muchnik", "Andrej", ""]]}, {"id": "0708.2353", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Continuous and randomized defensive forecasting: unified view", "comments": "10 pages. The new version: (1) relaxes the assumption that the\n  outcome space is finite, and now it is only assumed to be compact; (2) shows\n  that in the case where the outcome space is finite of cardinality C, the\n  randomized forecasts can be chosen concentrated on a finite set of\n  cardinality at most C", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  Defensive forecasting is a method of transforming laws of probability (stated\nin game-theoretic terms as strategies for Sceptic) into forecasting algorithms.\nThere are two known varieties of defensive forecasting: \"continuous\", in which\nSceptic's moves are assumed to depend on the forecasts in a (semi)continuous\nmanner and which produces deterministic forecasts, and \"randomized\", in which\nthe dependence of Sceptic's moves on the forecasts is arbitrary and\nForecaster's moves are allowed to be randomized. This note shows that the\nrandomized variety can be obtained from the continuous variety by smearing\nSceptic's moves to make them continuous.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2007 12:18:24 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2007 12:44:34 GMT"}], "update_date": "2007-08-23", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "0708.3226", "submitter": "Rustem Takhanov", "authors": "Rustem Takhanov", "title": "A Dichotomy Theorem for General Minimum Cost Homomorphism Problem", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the constraint satisfaction problem ($CSP$), the aim is to find an\nassignment of values to a set of variables subject to specified constraints. In\nthe minimum cost homomorphism problem ($MinHom$), one is additionally given\nweights $c_{va}$ for every variable $v$ and value $a$, and the aim is to find\nan assignment $f$ to the variables that minimizes $\\sum_{v} c_{vf(v)}$. Let\n$MinHom(\\Gamma)$ denote the $MinHom$ problem parameterized by the set of\npredicates allowed for constraints. $MinHom(\\Gamma)$ is related to many\nwell-studied combinatorial optimization problems, and concrete applications can\nbe found in, for instance, defence logistics and machine learning. We show that\n$MinHom(\\Gamma)$ can be studied by using algebraic methods similar to those\nused for CSPs. With the aid of algebraic techniques, we classify the\ncomputational complexity of $MinHom(\\Gamma)$ for all choices of $\\Gamma$. Our\nresult settles a general dichotomy conjecture previously resolved only for\ncertain classes of directed graphs, [Gutin, Hell, Rafiey, Yeo, European J. of\nCombinatorics, 2008].\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2007 18:26:21 GMT"}, {"version": "v2", "created": "Sun, 31 Aug 2008 21:54:49 GMT"}, {"version": "v3", "created": "Thu, 22 Jan 2009 13:53:56 GMT"}, {"version": "v4", "created": "Fri, 23 Jan 2009 16:13:44 GMT"}, {"version": "v5", "created": "Mon, 20 Apr 2009 15:18:35 GMT"}, {"version": "v6", "created": "Thu, 16 Jul 2009 16:43:08 GMT"}, {"version": "v7", "created": "Sun, 4 Apr 2010 20:39:03 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["Takhanov", "Rustem", ""]]}]