[{"id": "1612.00086", "submitter": "Ehsan Amid", "authors": "Ehsan Amid, Aristides Gionis, Antti Ukkonen", "title": "Semi-supervised Kernel Metric Learning Using Relative Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of metric learning subject to a set of constraints on\nrelative-distance comparisons between the data items. Such constraints are\nmeant to reflect side-information that is not expressed directly in the feature\nvectors of the data items. The relative-distance constraints used in this work\nare particularly effective in expressing structures at finer level of detail\nthan must-link (ML) and cannot-link (CL) constraints, which are most commonly\nused for semi-supervised clustering. Relative-distance constraints are thus\nuseful in settings where providing an ML or a CL constraint is difficult\nbecause the granularity of the true clustering is unknown.\n  Our main contribution is an efficient algorithm for learning a kernel matrix\nusing the log determinant divergence --- a variant of the Bregman divergence\n--- subject to a set of relative-distance constraints. The learned kernel\nmatrix can then be employed by many different kernel methods in a wide range of\napplications. In our experimental evaluations, we consider a semi-supervised\nclustering setting and show empirically that kernels found by our algorithm\nyield clusterings of higher quality than existing approaches that either use\nML/CL constraints or a different means to implement the supervision using\nrelative comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 00:16:53 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 09:09:05 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Amid", "Ehsan", ""], ["Gionis", "Aristides", ""], ["Ukkonen", "Antti", ""]]}, {"id": "1612.00100", "submitter": "Hongyang Zhang", "authors": "Maria-Florina Balcan and Hongyang Zhang", "title": "Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling", "comments": "24 pages, 5 figures in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering an incomplete $m\\times n$ matrix of rank\n$r$ with columns arriving online over time. This is known as the problem of\nlife-long matrix completion, and is widely applied to recommendation system,\ncomputer vision, system identification, etc. The challenge is to design\nprovable algorithms tolerant to a large amount of noises, with small sample\ncomplexity. In this work, we give algorithms achieving strong guarantee under\ntwo realistic noise models. In bounded deterministic noise, an adversary can\nadd any bounded yet unstructured noise to each column. For this problem, we\npresent an algorithm that returns a matrix of a small error, with sample\ncomplexity almost as small as the best prior results in the noiseless case. For\nsparse random noise, where the corrupted columns are sparse and drawn randomly,\nwe give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by\nprobability at least $1-\\delta$ with sample complexity as small as\n$O\\left(\\mu_0rn\\log (r/\\delta)\\right)$. This result advances the\nstate-of-the-art work and matches the lower bound in a worst case. We also\nstudy the scenario where the hidden matrix lies on a mixture of subspaces and\nshow that the sample complexity can be even smaller. Our proposed algorithms\nperform well experimentally in both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 01:10:07 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1612.00108", "submitter": "Zizhan Zheng", "authors": "Zizhan Zheng, Ness B. Shroff, Prasant Mohapatra", "title": "When to Reset Your Keys: Optimal Timing of Security Updates via Learning", "comments": "9 pages, 2 figures; accepted by the Thirty-First AAAI Conference on\n  Artificial Intelligence (AAAI-17), San Francisco, CA, USA, Feb. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity is increasingly threatened by advanced and persistent attacks.\nAs these attacks are often designed to disable a system (or a critical\nresource, e.g., a user account) repeatedly, it is crucial for the defender to\nkeep updating its security measures to strike a balance between the risk of\nbeing compromised and the cost of security updates. Moreover, these decisions\noften need to be made with limited and delayed feedback due to the stealthy\nnature of advanced attacks. In addition to targeted attacks, such an optimal\ntiming policy under incomplete information has broad applications in\ncybersecurity. Examples include key rotation, password change, application of\npatches, and virtual machine refreshing. However, rigorous studies of optimal\ntiming are rare. Further, existing solutions typically rely on a pre-defined\nattack model that is known to the defender, which is often not the case in\npractice. In this work, we make an initial effort towards achieving optimal\ntiming of security updates in the face of unknown stealthy attacks. We consider\na variant of the influential FlipIt game model with asymmetric feedback and\nunknown attack time distribution, which provides a general model to consecutive\nsecurity updates. The defender's problem is then modeled as a time associative\nbandit problem with dependent arms. We derive upper confidence bound based\nlearning policies that achieve low regret compared with optimal periodic\ndefense strategies that can only be derived when attack time distributions are\nknown.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 01:43:24 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 18:26:18 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Zheng", "Zizhan", ""], ["Shroff", "Ness B.", ""], ["Mohapatra", "Prasant", ""]]}, {"id": "1612.00151", "submitter": "Vijendra Singh", "authors": "Singh Vijendra, Hemjyotsana Parashar and Nisha Vasudeva", "title": "A New Method for Classification of Datasets for Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decision tree is an important method for both induction research and data\nmining, which is mainly used for model classification and prediction. ID3\nalgorithm is the most widely used algorithm in the decision tree so far. In\nthis paper, the shortcoming of ID3's inclining to choose attributes with many\nvalues is discussed, and then a new decision tree algorithm which is improved\nversion of ID3. In our proposed algorithm attributes are divided into groups\nand then we apply the selection measure 5 for these groups. If information gain\nis not good then again divide attributes values into groups. These steps are\ndone until we get good classification/misclassification ratio. The proposed\nalgorithms classify the data sets more accurately and efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:24:36 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Vijendra", "Singh", ""], ["Parashar", "Hemjyotsana", ""], ["Vasudeva", "Nisha", ""]]}, {"id": "1612.00155", "submitter": "Pedro Tabacof", "authors": "Pedro Tabacof, Julia Tavares, Eduardo Valle", "title": "Adversarial Images for Variational Autoencoders", "comments": "Workshop on Adversarial Training, NIPS 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate adversarial attacks for autoencoders. We propose a procedure\nthat distorts the input image to mislead the autoencoder in reconstructing a\ncompletely different target image. We attack the internal latent\nrepresentations, attempting to make the adversarial input produce an internal\nrepresentation as similar as possible as the target's. We find that\nautoencoders are much more robust to the attack than classifiers: while some\nexamples have tolerably small input distortion, and reasonable similarity to\nthe target image, there is a quasi-linear trade-off between those aims. We\nreport results on MNIST and SVHN datasets, and also test regular deterministic\nautoencoders, reaching similar conclusions in all cases. Finally, we show that\nthe usual adversarial attack for classifiers, while being much easier, also\npresents a direct proportion between distortion on the input, and misdirection\non the output. That proportionality however is hidden by the normalization of\nthe output, which maps a linear layer into non-linear probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:59:57 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Tabacof", "Pedro", ""], ["Tavares", "Julia", ""], ["Valle", "Eduardo", ""]]}, {"id": "1612.00188", "submitter": "Zakaria Mhammedi", "authors": "Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, James Bailey", "title": "Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using\n  Householder Reflections", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning long-term dependencies in sequences using Recurrent\nNeural Networks (RNNs) is still a major challenge. Recent methods have been\nsuggested to solve this problem by constraining the transition matrix to be\nunitary during training which ensures that its norm is equal to one and\nprevents exploding gradients. These methods either have limited expressiveness\nor scale poorly with the size of the network when compared with the simple RNN\ncase, especially when using stochastic gradient descent with a small mini-batch\nsize. Our contributions are as follows; we first show that constraining the\ntransition matrix to be unitary is a special case of an orthogonal constraint.\nThen we present a new parametrisation of the transition matrix which allows\nefficient training of an RNN while ensuring that the matrix is always\northogonal. Our results show that the orthogonal constraint on the transition\nmatrix applied through our parametrisation gives similar benefits to the\nunitary constraint, without the time complexity limitations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 09:55:10 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 12:08:34 GMT"}, {"version": "v3", "created": "Mon, 12 Dec 2016 14:09:19 GMT"}, {"version": "v4", "created": "Mon, 6 Mar 2017 12:01:53 GMT"}, {"version": "v5", "created": "Tue, 13 Jun 2017 07:07:33 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Mhammedi", "Zakaria", ""], ["Hellicar", "Andrew", ""], ["Rahman", "Ashfaqur", ""], ["Bailey", "James", ""]]}, {"id": "1612.00193", "submitter": "Gr\\'egoire Ferr\\'e", "authors": "G. Ferr\\'e, T. Haut and K. Barros", "title": "Learning molecular energies using localized graph kernels", "comments": null, "journal-ref": "The Journal of Chemical Physics, 146(11), 114107 (2017)", "doi": "10.1063/1.4978623", "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent machine learning methods make it possible to model potential energy of\natomic configurations with chemical-level accuracy (as calculated from\nab-initio calculations) and at speeds suitable for molecular dynam- ics\nsimulation. Best performance is achieved when the known physical constraints\nare encoded in the machine learning models. For example, the atomic energy is\ninvariant under global translations and rotations, it is also invariant to\npermutations of same-species atoms. Although simple to state, these symmetries\nare complicated to encode into machine learning algorithms. In this paper, we\npresent a machine learning approach based on graph theory that naturally\nincorporates translation, rotation, and permutation symmetries. Specifically,\nwe use a random walk graph kernel to measure the similarity of two adjacency\nmatrices, each of which represents a local atomic environment. This Graph\nApproximated Energy (GRAPE) approach is flexible and admits many possible\nextensions. We benchmark a simple version of GRAPE by predicting atomization\nenergies on a standard dataset of organic molecules.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 10:23:59 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 10:03:41 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Ferr\u00e9", "G.", ""], ["Haut", "T.", ""], ["Barros", "K.", ""]]}, {"id": "1612.00212", "submitter": "Shuchang Zhou", "authors": "He Wen, Shuchang Zhou, Zhe Liang, Yuxiang Zhang, Dieqiao Feng, Xinyu\n  Zhou, Cong Yao", "title": "Training Bit Fully Convolutional Network for Fast Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks give accurate, per-pixel prediction for\ninput images and have applications like semantic segmentation. However, a\ntypical FCN usually requires lots of floating point computation and large\nrun-time memory, which effectively limits its usability. We propose a method to\ntrain Bit Fully Convolution Network (BFCN), a fully convolutional neural\nnetwork that has low bit-width weights and activations. Because most of its\ncomputation-intensive convolutions are accomplished between low bit-width\nnumbers, a BFCN can be accelerated by an efficient bit-convolution\nimplementation. On CPU, the dot product operation between two bit vectors can\nbe reduced to bitwise operations and popcounts, which can offer much higher\nthroughput than 32-bit multiplications and additions.\n  To validate the effectiveness of BFCN, we conduct experiments on the PASCAL\nVOC 2012 semantic segmentation task and Cityscapes. Our BFCN with 1-bit weights\nand 2-bit activations, which runs 7.8x faster on CPU or requires less than 1\\%\nresources on FPGA, can achieve comparable performance as the 32-bit\ncounterpart.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 11:56:15 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Wen", "He", ""], ["Zhou", "Shuchang", ""], ["Liang", "Zhe", ""], ["Zhang", "Yuxiang", ""], ["Feng", "Dieqiao", ""], ["Zhou", "Xinyu", ""], ["Yao", "Cong", ""]]}, {"id": "1612.00221", "submitter": "Sven Banisch", "authors": "Sven Banisch and Eckehard Olbrich", "title": "The Coconut Model with Heterogeneous Strategies and Learning", "comments": "Accepted for publication in the Journal of Artificial Societies and\n  Social Simulation (JASSS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC cs.LG nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an agent-based version of the Diamond search\nequilibrium model - also called Coconut Model. In this model, agents are faced\nwith production decisions that have to be evaluated based on their expectations\nabout the future utility of the produced entity which in turn depends on the\nglobal production level via a trading mechanism. While the original dynamical\nsystems formulation assumes an infinite number of homogeneously adapting agents\nobeying strong rationality conditions, the agent-based setting allows to\ndiscuss the effects of heterogeneous and adaptive expectations and enables the\nanalysis of non-equilibrium trajectories. Starting from a baseline\nimplementation that matches the asymptotic behavior of the original model, we\nshow how agent heterogeneity can be accounted for in the aggregate dynamical\nequations. We then show that when agents adapt their strategies by a simple\ntemporal difference learning scheme, the system converges to one of the fixed\npoints of the original system. Systematic simulations reveal that this is the\nonly stable equilibrium solution.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 12:24:46 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Banisch", "Sven", ""], ["Olbrich", "Eckehard", ""]]}, {"id": "1612.00222", "submitter": "Peter Battaglia", "authors": "Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, Koray\n  Kavukcuoglu", "title": "Interaction Networks for Learning about Objects, Relations and Physics", "comments": "Published in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about objects, relations, and physics is central to human\nintelligence, and a key goal of artificial intelligence. Here we introduce the\ninteraction network, a model which can reason about how objects in complex\nsystems interact, supporting dynamical predictions, as well as inferences about\nthe abstract properties of the system. Our model takes graphs as input,\nperforms object- and relation-centric reasoning in a way that is analogous to a\nsimulation, and is implemented using deep neural networks. We evaluate its\nability to reason about several challenging physical domains: n-body problems,\nrigid-body collision, and non-rigid dynamics. Our results show it can be\ntrained to accurately simulate the physical trajectories of dozens of objects\nover thousands of time steps, estimate abstract quantities such as energy, and\ngeneralize automatically to systems with different numbers and configurations\nof objects and relations. Our interaction network implementation is the first\ngeneral-purpose, learnable physics engine, and a powerful general framework for\nreasoning about object and relations in a wide variety of complex real-world\ndomains.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 12:34:54 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Battaglia", "Peter W.", ""], ["Pascanu", "Razvan", ""], ["Lai", "Matthew", ""], ["Rezende", "Danilo", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1612.00334", "submitter": "Yanjun  Qi Dr.", "authors": "Beilun Wang, Ji Gao, Yanjun Qi", "title": "A Theoretical Framework for Robustness of (Deep) Classifiers against\n  Adversarial Examples", "comments": "38 pages , ICLR 2017 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine learning classifiers, including deep neural networks, are\nvulnerable to adversarial examples. Such inputs are typically generated by\nadding small but purposeful modifications that lead to incorrect outputs while\nimperceptible to human eyes. The goal of this paper is not to introduce a\nsingle method, but to make theoretical steps towards fully understanding\nadversarial examples. By using concepts from topology, our theoretical analysis\nbrings forth the key reasons why an adversarial example can fool a classifier\n($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By\ninvestigating the topological relationship between two (pseudo)metric spaces\ncorresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and\nsufficient conditions that can determine if $f_1$ is always robust\n(strong-robust) against adversarial examples according to $f_2$. Interestingly\nour theorems indicate that just one unnecessary feature can make $f_1$ not\nstrong-robust, and the right feature representation learning is the key to\ngetting a classifier that is both accurate and strong-robust.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 16:20:39 GMT"}, {"version": "v10", "created": "Thu, 9 Mar 2017 22:00:56 GMT"}, {"version": "v11", "created": "Thu, 27 Apr 2017 14:36:40 GMT"}, {"version": "v12", "created": "Wed, 27 Sep 2017 16:02:48 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 17:07:35 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 22:23:55 GMT"}, {"version": "v4", "created": "Sat, 21 Jan 2017 16:37:24 GMT"}, {"version": "v5", "created": "Thu, 26 Jan 2017 15:32:06 GMT"}, {"version": "v6", "created": "Wed, 1 Feb 2017 17:30:50 GMT"}, {"version": "v7", "created": "Thu, 2 Feb 2017 14:39:50 GMT"}, {"version": "v8", "created": "Fri, 3 Feb 2017 16:06:39 GMT"}, {"version": "v9", "created": "Mon, 27 Feb 2017 20:18:26 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Wang", "Beilun", ""], ["Gao", "Ji", ""], ["Qi", "Yanjun", ""]]}, {"id": "1612.00341", "submitter": "Michael Chang", "authors": "Michael B. Chang, Tomer Ullman, Antonio Torralba, Joshua B. Tenenbaum", "title": "A Compositional Object-Based Approach to Learning Physical Dynamics", "comments": "Published as a conference paper for ICLR 2017. 15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Neural Physics Engine (NPE), a framework for learning\nsimulators of intuitive physics that naturally generalize across variable\nobject count and different scene configurations. We propose a factorization of\na physical scene into composable object-based representations and a neural\nnetwork architecture whose compositional structure factorizes object dynamics\ninto pairwise interactions. Like a symbolic physics engine, the NPE is endowed\nwith generic notions of objects and their interactions; realized as a neural\nnetwork, it can be trained via stochastic gradient descent to adapt to specific\nobject properties and dynamics of different worlds. We evaluate the efficacy of\nour approach on simple rigid body dynamics in two-dimensional worlds. By\ncomparing to less structured architectures, we show that the NPE's\ncompositional representation of the structure in physical interactions improves\nits ability to predict movement, generalize across variable object count and\ndifferent scene configurations, and infer latent properties of objects such as\nmass.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 16:39:04 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 17:44:06 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Chang", "Michael B.", ""], ["Ullman", "Tomer", ""], ["Torralba", "Antonio", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1612.00367", "submitter": "Damien Lefortier", "authors": "Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims,\n  Maarten de Rijke", "title": "Large-scale Validation of Counterfactual Learning Methods: A Test-Bed", "comments": "10 pages, What If workshop NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform effective off-policy learning would revolutionize the\nprocess of building better interactive systems, such as search engines and\nrecommendation systems for e-commerce, computational advertising and news.\nRecent approaches for off-policy evaluation and learning in these settings\nappear promising. With this paper, we provide real-world data and a\nstandardized test-bed to systematically investigate these algorithms using data\nfrom display advertising. In particular, we consider the problem of filling a\nbanner ad with an aggregate of multiple products the user may want to purchase.\nThis paper presents our test-bed, the sanity checks we ran to ensure its\nvalidity, and shows results comparing state-of-the-art off-policy learning\nmethods like doubly robust optimization, POEM, and reductions to supervised\nlearning using regression baselines. Our results show experimental evidence\nthat recent off-policy learning methods can improve upon state-of-the-art\nsupervised learning techniques on a large-scale real-world data set.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 17:59:53 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 11:00:30 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Lefortier", "Damien", ""], ["Swaminathan", "Adith", ""], ["Gu", "Xiaotao", ""], ["Joachims", "Thorsten", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1612.00374", "submitter": "Ingrid Blaschzyk", "authors": "Philipp Thomann and Ingrid Blaschzyk and Mona Meister and Ingo\n  Steinwart", "title": "Spatial Decompositions for Large Scale SVMs", "comments": null, "journal-ref": "Proceedings of Machine Learning Research Volume 54: Proceedings of\n  the 20th International Conference on Artificial Intelligence and Statistics\n  2017 (A. Singh and J. Zhu, eds.), pp. 1329-1337, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although support vector machines (SVMs) are theoretically well understood,\ntheir underlying optimization problem becomes very expensive, if, for example,\nhundreds of thousands of samples and a non-linear kernel are considered.\nSeveral approaches have been proposed in the past to address this serious\nlimitation. In this work we investigate a decomposition strategy that learns on\nsmall, spatially defined data chunks. Our contributions are two fold: On the\ntheoretical side we establish an oracle inequality for the overall learning\nmethod using the hinge loss, and show that the resulting rates match those\nknown for SVMs solving the complete optimization problem with Gaussian kernels.\nOn the practical side we compare our approach to learning SVMs on small,\nrandomly chosen chunks. Here it turns out that for comparable training times\nour approach is significantly faster during testing and also reduces the test\nerror in most cases significantly. Furthermore, we show that our approach\neasily scales up to 10 million training samples: including hyper-parameter\nselection using cross validation, the entire training only takes a few hours on\na single machine. Finally, we report an experiment on 32 million training\nsamples. All experiments used liquidSVM (Steinwart and Thomann, 2017).\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 18:14:33 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 14:54:51 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Thomann", "Philipp", ""], ["Blaschzyk", "Ingrid", ""], ["Meister", "Mona", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1612.00377", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Alexander G. Ororbia II, Joelle Pineau, Aaron\n  Courville", "title": "Piecewise Latent Variables for Neural Variational Text Processing", "comments": "19 pages, 2 figures, 8 tables; EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in neural variational inference have facilitated the learning of\npowerful directed graphical models with continuous latent variables, such as\nvariational autoencoders. The hope is that such models will learn to represent\nrich, multi-modal latent factors in real-world data, such as natural language\ntext. However, current models often assume simplistic priors on the latent\nvariables - such as the uni-modal Gaussian distribution - which are incapable\nof representing complex latent factors efficiently. To overcome this\nrestriction, we propose the simple, but highly flexible, piecewise constant\ndistribution. This distribution has the capacity to represent an exponential\nnumber of modes of a latent target distribution, while remaining mathematically\ntractable. Our results demonstrate that incorporating this new latent\ndistribution into different models yields substantial improvements in natural\nlanguage processing tasks such as document modeling and natural language\ngeneration for dialogue.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 18:49:23 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 03:18:54 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 19:25:58 GMT"}, {"version": "v4", "created": "Sat, 23 Sep 2017 13:33:55 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Serban", "Iulian V.", ""], ["Ororbia", "Alexander G.", "II"], ["Pineau", "Joelle", ""], ["Courville", "Aaron", ""]]}, {"id": "1612.00383", "submitter": "Valentin Dalibard", "authors": "Valentin Dalibard, Michael Schaarschmidt, Eiko Yoneki", "title": "Tuning the Scheduling of Distributed Stochastic Gradient Descent with\n  Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimizer which uses Bayesian optimization to tune the system\nparameters of distributed stochastic gradient descent (SGD). Given a specific\ncontext, our goal is to quickly find efficient configurations which\nappropriately balance the load between the available machines to minimize the\naverage SGD iteration time. Our experiments consider setups with over thirty\nparameters. Traditional Bayesian optimization, which uses a Gaussian process as\nits model, is not well suited to such high dimensional domains. To reduce\nconvergence time, we exploit the available structure. We design a probabilistic\nmodel which simulates the behavior of distributed SGD and use it within\nBayesian optimization. Our model can exploit many runtime measurements for\ninference per evaluation of the objective function. Our experiments show that\nour resulting optimizer converges to efficient configurations within ten\niterations, the optimized configurations outperform those found by generic\noptimizer in thirty iterations by up to 2X.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 19:08:12 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Dalibard", "Valentin", ""], ["Schaarschmidt", "Michael", ""], ["Yoneki", "Eiko", ""]]}, {"id": "1612.00388", "submitter": "Wesley Tansey", "authors": "Wesley Tansey and Edward W. Lowe Jr. and James G. Scott", "title": "Diet2Vec: Multi-scale analysis of massive dietary data", "comments": "Accepted to the NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart phone apps that enable users to easily track their diets have become\nwidespread in the last decade. This has created an opportunity to discover new\ninsights into obesity and weight loss by analyzing the eating habits of the\nusers of such apps. In this paper, we present diet2vec: an approach to modeling\nlatent structure in a massive database of electronic diet journals. Through an\niterative contract-and-expand process, our model learns real-valued embeddings\nof users' diets, as well as embeddings for individual foods and meals. We\ndemonstrate the effectiveness of our approach on a real dataset of 55K users of\nthe popular diet-tracking app LoseIt\\footnote{http://www.loseit.com/}. To the\nbest of our knowledge, this is the largest fine-grained diet tracking study in\nthe history of nutrition and obesity research. Our results suggest that\ndiet2vec finds interpretable results at all levels, discovering intuitive\nrepresentations of foods, meals, and diets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 19:21:22 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Tansey", "Wesley", ""], ["Lowe", "Edward W.", "Jr."], ["Scott", "James G.", ""]]}, {"id": "1612.00393", "submitter": "Joachim van der Herten", "authors": "Joachim van der Herten and Ivo Couckuyt and Tom Dhaene", "title": "Hypervolume-based Multi-objective Bayesian Optimization with Student-t\n  Processes", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student-$t$ processes have recently been proposed as an appealing alternative\nnon-parameteric function prior. They feature enhanced flexibility and\npredictive variance. In this work the use of Student-$t$ processes are explored\nfor multi-objective Bayesian optimization. In particular, an analytical\nexpression for the hypervolume-based probability of improvement is developed\nfor independent Student-$t$ process priors of the objectives. Its effectiveness\nis shown on a multi-objective optimization problem which is known to be\ndifficult with traditional Gaussian processes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 19:41:50 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["van der Herten", "Joachim", ""], ["Couckuyt", "Ivo", ""], ["Dhaene", "Tom", ""]]}, {"id": "1612.00410", "submitter": "Alexander Alemi", "authors": "Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, Kevin Murphy", "title": "Deep Variational Information Bottleneck", "comments": "19 pages, 8 figures, Accepted to ICLR17", "journal-ref": "Proceedings of the International Conference on Learning\n  Representations (ICLR) 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a variational approximation to the information bottleneck of\nTishby et al. (1999). This variational approach allows us to parameterize the\ninformation bottleneck model using a neural network and leverage the\nreparameterization trick for efficient training. We call this method \"Deep\nVariational Information Bottleneck\", or Deep VIB. We show that models trained\nwith the VIB objective outperform those that are trained with other forms of\nregularization, in terms of generalization performance and robustness to\nadversarial attack.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 20:12:40 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 21:17:12 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 21:20:21 GMT"}, {"version": "v4", "created": "Thu, 16 Mar 2017 22:38:37 GMT"}, {"version": "v5", "created": "Mon, 17 Jul 2017 16:15:45 GMT"}, {"version": "v6", "created": "Tue, 28 May 2019 21:42:47 GMT"}, {"version": "v7", "created": "Wed, 23 Oct 2019 22:47:44 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Alemi", "Alexander A.", ""], ["Fischer", "Ian", ""], ["Dillon", "Joshua V.", ""], ["Murphy", "Kevin", ""]]}, {"id": "1612.00429", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, Sergey Levine", "title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "comments": "ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) can acquire complex behaviors from low-level\ninputs, such as images. However, real-world applications of such methods\nrequire generalizing to the vast variability of the real world. Deep networks\nare known to achieve remarkable generalization when provided with massive\namounts of labeled data, but can we provide this breadth of experience to an RL\nagent, such as a robot? The robot might continuously learn as it explores the\nworld around it, even while deployed. However, this learning requires access to\na reward function, which is often hard to measure in real-world domains, where\nthe reward could depend on, for example, unknown positions of objects or the\nemotional state of the user. Conversely, it is often quite practical to provide\nthe agent with reward functions in a limited set of situations, such as when a\nhuman supervisor is present or in a controlled setting. Can we make use of this\nlimited supervision, and still benefit from the breadth of experience an agent\nmight collect on its own? In this paper, we formalize this problem as\nsemisupervised reinforcement learning, where the reward function can only be\nevaluated in a set of \"labeled\" MDPs, and the agent must generalize its\nbehavior to the wide range of states it might encounter in a set of \"unlabeled\"\nMDPs, by using experience from both settings. Our proposed method infers the\ntask objective in the unlabeled MDPs through an algorithm that resembles\ninverse RL, using the agent's own prior experience in the labeled MDPs as a\nkind of demonstration of optimal behavior. We evaluate our method on\nchallenging tasks that require control directly from images, and show that our\napproach can improve the generalization of a learned deep neural network policy\nby using experience for which no reward function is available. We also show\nthat our method outperforms direct supervised learning of the reward.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 20:48:39 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 19:46:12 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Finn", "Chelsea", ""], ["Yu", "Tianhe", ""], ["Fu", "Justin", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1612.00475", "submitter": "Taylor Killian", "authors": "Taylor Killian, George Konidaris, Finale Doshi-Velez", "title": "Transfer Learning Across Patient Variations with Hidden Parameter Markov\n  Decision Processes", "comments": "Brief abstract for poster submission to Machine Learning for\n  Healthcare workshop at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to physiological variation, patients diagnosed with the same condition\nmay exhibit divergent, but related, responses to the same treatments. Hidden\nParameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learning\nproblem by embedding these tasks into a low-dimensional space. However, the\noriginal formulation of HiP-MDP had a critical flaw: the embedding uncertainty\nwas modeled independently of the agent's state uncertainty, requiring an\nunnatural training procedure in which all tasks visited every part of the state\nspace---possible for robots that can be moved to a particular location,\nimpossible for human patients. We update the HiP-MDP framework and extend it to\nmore robustly develop personalized medicine strategies for HIV treatment.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 21:26:52 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Killian", "Taylor", ""], ["Konidaris", "George", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1612.00516", "submitter": "Corinne Jones", "authors": "Corinne L. Jones, Sham M. Kakade, Lucas W. Thornblade, David R. Flum,\n  Abraham D. Flaxman", "title": "Canonical Correlation Analysis for Analyzing Sequences of Medical\n  Billing Codes", "comments": "Accepted at NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose using canonical correlation analysis (CCA) to generate features\nfrom sequences of medical billing codes. Applying this novel use of CCA to a\ndatabase of medical billing codes for patients with diverticulitis, we first\ndemonstrate that the CCA embeddings capture meaningful relationships among the\ncodes. We then generate features from these embeddings and establish their\nusefulness in predicting future elective surgery for diverticulitis, an\nimportant marker in efforts for reducing costs in healthcare.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 23:38:34 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 16:42:36 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Jones", "Corinne L.", ""], ["Kakade", "Sham M.", ""], ["Thornblade", "Lucas W.", ""], ["Flum", "David R.", ""], ["Flaxman", "Abraham D.", ""]]}, {"id": "1612.00525", "submitter": "Turki Turki", "authors": "Turki Turki and Zhi Wei", "title": "A Noise-Filtering Approach for Cancer Drug Sensitivity Prediction", "comments": "Accepted at NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurately predicting drug responses to cancer is an important problem\nhindering oncologists' efforts to find the most effective drugs to treat\ncancer, which is a core goal in precision medicine. The scientific community\nhas focused on improving this prediction based on genomic, epigenomic, and\nproteomic datasets measured in human cancer cell lines. Real-world cancer cell\nlines contain noise, which degrades the performance of machine learning\nalgorithms. This problem is rarely addressed in the existing approaches. In\nthis paper, we present a noise-filtering approach that integrates techniques\nfrom numerical linear algebra and information retrieval targeted at filtering\nout noisy cancer cell lines. By filtering out noisy cancer cell lines, we can\ntrain machine learning algorithms on better quality cancer cell lines. We\nevaluate the performance of our approach and compare it with an existing\napproach using the Area Under the ROC Curve (AUC) on clinical trial data. The\nexperimental results show that our proposed approach is stable and also yields\nthe highest AUC at a statistically significant level.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 00:41:11 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 05:15:51 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Turki", "Turki", ""], ["Wei", "Zhi", ""]]}, {"id": "1612.00542", "submitter": "Daniel L\\'evy", "authors": "Daniel L\\'evy, Arzav Jain", "title": "Breast Mass Classification from Mammograms using Deep Convolutional\n  Neural Networks", "comments": "NIPS 2016 ML4HC Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography is the most widely used method to screen breast cancer. Because\nof its mostly manual nature, variability in mass appearance, and low\nsignal-to-noise ratio, a significant number of breast masses are missed or\nmisdiagnosed. In this work, we present how Convolutional Neural Networks can be\nused to directly classify pre-segmented breast masses in mammograms as benign\nor malignant, using a combination of transfer learning, careful pre-processing\nand data augmentation to overcome limited training data. We achieve\nstate-of-the-art results on the DDSM dataset, surpassing human performance, and\nshow interpretability of our model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 02:06:15 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["L\u00e9vy", "Daniel", ""], ["Jain", "Arzav", ""]]}, {"id": "1612.00554", "submitter": "Chandrajit Bajaj", "authors": "Jilin Wu and Soumyajit Gupta and Chandrajit Bajaj", "title": "Higher Order Mutual Information Approximation for Feature Selection", "comments": "14 page, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a process of choosing a subset of relevant features so\nthat the quality of prediction models can be improved. An extensive body of\nwork exists on information-theoretic feature selection, based on maximizing\nMutual Information (MI) between subsets of features and class labels. The prior\nmethods use a lower order approximation, by treating the joint entropy as a\nsummation of several single variable entropies. This leads to locally optimal\nselections and misses multi-way feature combinations. We present a higher order\nMI based approximation technique called Higher Order Feature Selection (HOFS).\nInstead of producing a single list of features, our method produces a ranked\ncollection of feature subsets that maximizes MI, giving better comprehension\n(feature ranking) as to which features work best together when selected, due to\ntheir underlying interdependent structure. Our experiments demonstrate that the\nproposed method performs better than existing feature selection approaches\nwhile keeping similar running times and computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 03:34:44 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Wu", "Jilin", ""], ["Gupta", "Soumyajit", ""], ["Bajaj", "Chandrajit", ""]]}, {"id": "1612.00563", "submitter": "Steven Rennie", "authors": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross and\n  Vaibhava Goel", "title": "Self-critical Sequence Training for Image Captioning", "comments": "CVPR 2017 + additional analysis + fixed baseline results, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has been shown that policy-gradient methods for reinforcement\nlearning can be utilized to train deep end-to-end systems directly on\nnon-differentiable metrics for the task at hand. In this paper we consider the\nproblem of optimizing image captioning systems using reinforcement learning,\nand show that by carefully optimizing our systems using the test metrics of the\nMSCOCO task, significant gains in performance can be realized. Our systems are\nbuilt using a new optimization approach that we call self-critical sequence\ntraining (SCST). SCST is a form of the popular REINFORCE algorithm that, rather\nthan estimating a \"baseline\" to normalize the rewards and reduce variance,\nutilizes the output of its own test-time inference algorithm to normalize the\nrewards it experiences. Using this approach, estimating the reward signal (as\nactor-critic methods must do) and estimating normalization (as REINFORCE\nalgorithms typically do) is avoided, while at the same time harmonizing the\nmodel with respect to its test-time inference procedure. Empirically we find\nthat directly optimizing the CIDEr metric with SCST and greedy decoding at\ntest-time is highly effective. Our results on the MSCOCO evaluation sever\nestablish a new state-of-the-art on the task, improving the best result in\nterms of CIDEr from 104.9 to 114.7.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 04:37:22 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:38:37 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Rennie", "Steven J.", ""], ["Marcheret", "Etienne", ""], ["Mroueh", "Youssef", ""], ["Ross", "Jarret", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1612.00583", "submitter": "Yifei Ma", "authors": "Yifei Ma and Roman Garnett and Jeff Schneider", "title": "Active Search for Sparse Signals with Region Sensing", "comments": "aaai 2017 preprint; nips exhibition of rejections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems can be used to search for sparse signals in a large space;\ne.g., aerial robots can be deployed to localize threats, detect gas leaks, or\nrespond to distress calls. Intuitively, search algorithms may increase\nefficiency by collecting aggregate measurements summarizing large contiguous\nregions. However, most existing search methods either ignore the possibility of\nsuch region observations (e.g., Bayesian optimization and multi-armed bandits)\nor make strong assumptions about the sensing mechanism that allow each\nmeasurement to arbitrarily encode all signals in the entire environment (e.g.,\ncompressive sensing). We propose an algorithm that actively collects data to\nsearch for sparse signals using only noisy measurements of the average values\non rectangular regions (including single points), based on the greedy\nmaximization of information gain. We analyze our algorithm in 1d and show that\nit requires $\\tilde{O}(\\frac{n}{\\mu^2}+k^2)$ measurements to recover all of $k$\nsignal locations with small Bayes error, where $\\mu$ and $n$ are the signal\nstrength and the size of the search space, respectively. We also show that\nactive designs can be fundamentally more efficient than passive designs with\nregion sensing, contrasting with the results of Arias-Castro, Candes, and\nDavenport (2013). We demonstrate the empirical performance of our algorithm on\na search problem using satellite image data and in high dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:44:45 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Ma", "Yifei", ""], ["Garnett", "Roman", ""], ["Schneider", "Jeff", ""]]}, {"id": "1612.00585", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Aurobinda Routray, William K. Mohanty, Mamata Jenamani", "title": "Development of a hybrid learning system based on SVM, ANFIS and domain\n  knowledge: DKFIS", "comments": "6 pages, 5 figures, 3tables Presented at Indicon 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of a hybrid learning system based on\nSupport Vector Machines (SVM), Adaptive Neuro-Fuzzy Inference System (ANFIS)\nand domain knowledge to solve prediction problem. The proposed two-stage Domain\nKnowledge based Fuzzy Information System (DKFIS) improves the prediction\naccuracy attained by ANFIS alone. The proposed framework has been implemented\non a noisy and incomplete dataset acquired from a hydrocarbon field located at\nwestern part of India. Here, oil saturation has been predicted from four\ndifferent well logs i.e. gamma ray, resistivity, density, and clay volume. In\nthe first stage, depending on zero or near zero and non-zero oil saturation\nlevels the input vector is classified into two classes (Class 0 and Class 1)\nusing SVM. The classification results have been further fine-tuned applying\nexpert knowledge based on the relationship among predictor variables i.e. well\nlogs and target variable - oil saturation. Second, an ANFIS is designed to\npredict non-zero (Class 1) oil saturation values from predictor logs. The\npredicted output has been further refined based on expert knowledge. It is\napparent from the experimental results that the expert intervention with\nqualitative judgment at each stage has rendered the prediction into the\nfeasible and realistic ranges. The performance analysis of the prediction in\nterms of four performance metrics such as correlation coefficient (CC), root\nmean square error (RMSE), and absolute error mean (AEM), scatter index (SI) has\nestablished DKFIS as a useful tool for reservoir characterization.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:56:23 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Chaki", "Soumi", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.00599", "submitter": "Zihao Chen", "authors": "Zihao Chen, Luo Luo, Zhihua Zhang", "title": "Communication Lower Bounds for Distributed Convex Optimization:\n  Partition Data on Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been an increasing interest in designing distributed\nconvex optimization algorithms under the setting where the data matrix is\npartitioned on features. Algorithms under this setting sometimes have many\nadvantages over those under the setting where data is partitioned on samples,\nespecially when the number of features is huge. Therefore, it is important to\nunderstand the inherent limitations of these optimization problems. In this\npaper, with certain restrictions on the communication allowed in the\nprocedures, we develop tight lower bounds on communication rounds for a broad\nclass of non-incremental algorithms under this setting. We also provide a lower\nbound on communication rounds for a class of (randomized) incremental\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 09:01:57 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Chen", "Zihao", ""], ["Luo", "Luo", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1612.00611", "submitter": "Yinchong Yang", "authors": "Yinchong Yang, Peter A. Fasching, Markus Wallwiener, Tanja N. Fehm,\n  Sara Y. Brucker, Volker Tresp", "title": "Predictive Clinical Decision Support System with RNN Encoding and Tensor\n  Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of the Electric Health Records, large amounts of\ndigital data become available for analysis and decision support. When\nphysicians are prescribing treatments to a patient, they need to consider a\nlarge range of data variety and volume, making decisions increasingly complex.\nMachine learning based Clinical Decision Support systems can be a solution to\nthe data challenges. In this work we focus on a class of decision support in\nwhich the physicians' decision is directly predicted. Concretely, the model\nwould assign higher probabilities to decisions that it presumes the physician\nare more likely to make. Thus the CDS system can provide physicians with\nrational recommendations. We also address the problem of correlation in target\nfeatures: Often a physician is required to make multiple (sub-)decisions in a\nblock, and that these decisions are mutually dependent. We propose a solution\nto the target correlation problem using a tensor factorization model. In order\nto handle the patients' historical information as sequential data, we apply the\nso-called Encoder-Decoder-Framework which is based on Recurrent Neural Networks\n(RNN) as encoders and a tensor factorization model as a decoder, a combination\nwhich is novel in machine learning. With experiments with real-world datasets\nwe show that the proposed model does achieve better prediction performances.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 10:03:09 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Yang", "Yinchong", ""], ["Fasching", "Peter A.", ""], ["Wallwiener", "Markus", ""], ["Fehm", "Tanja N.", ""], ["Brucker", "Sara Y.", ""], ["Tresp", "Volker", ""]]}, {"id": "1612.00615", "submitter": "Samuele Fiorini", "authors": "Samuele Fiorini, Andrea Tacchino, Giampaolo Brichetto, Alessandro\n  Verri, Annalisa Barla", "title": "A temporal model for multiple sclerosis course evolution", "comments": "NIPS Machine Learning for health Workshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Sclerosis is a degenerative condition of the central nervous system\nthat affects nearly 2.5 million of individuals in terms of their physical,\ncognitive, psychological and social capabilities. Researchers are currently\ninvestigating on the use of patient reported outcome measures for the\nassessment of impact and evolution of the disease on the life of the patients.\nTo date, a clear understanding on the use of such measures to predict the\nevolution of the disease is still lacking. In this work we resort to\nregularized machine learning methods for binary classification and multiple\noutput regression. We propose a pipeline that can be used to predict the\ndisease progression from patient reported measures. The obtained model is\ntested on a data set collected from an ongoing clinical research project.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 10:13:16 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Fiorini", "Samuele", ""], ["Tacchino", "Andrea", ""], ["Brichetto", "Giampaolo", ""], ["Verri", "Alessandro", ""], ["Barla", "Annalisa", ""]]}, {"id": "1612.00637", "submitter": "Nurjahan Begum", "authors": "Nurjahan Begum, Liudmila Ulanova, Hoang Anh Dau, Jun Wang and Eamonn\n  Keogh", "title": "A General Framework for Density Based Time Series Clustering Exploiting\n  a Novel Admissible Pruning Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time Series Clustering is an important subroutine in many higher-level data\nmining analyses, including data editing for classifiers, summarization, and\noutlier detection. It is well known that for similarity search the superiority\nof Dynamic Time Warping (DTW) over Euclidean distance gradually diminishes as\nwe consider ever larger datasets. However, as we shall show, the same is not\ntrue for clustering. Clustering time series under DTW remains a computationally\nexpensive operation. In this work, we address this issue in two ways. We\npropose a novel pruning strategy that exploits both the upper and lower bounds\nto prune off a very large fraction of the expensive distance calculations. This\npruning strategy is admissible and gives us provably identical results to the\nbrute force algorithm, but is at least an order of magnitude faster. For\ndatasets where even this level of speedup is inadequate, we show that we can\nuse a simple heuristic to order the unavoidable calculations in a\nmost-useful-first ordering, thus casting the clustering into an anytime\nframework. We demonstrate the utility of our ideas with both single and\nmultidimensional case studies in the domains of astronomy, speech physiology,\nmedicine and entomology. In addition, we show the generality of our clustering\nframework to other domains by efficiently obtaining semantically significant\nclusters in protein sequences using the Edit Distance, the discrete data\nanalogue of DTW.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 11:27:44 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Begum", "Nurjahan", ""], ["Ulanova", "Liudmila", ""], ["Dau", "Hoang Anh", ""], ["Wang", "Jun", ""], ["Keogh", "Eamonn", ""]]}, {"id": "1612.00653", "submitter": "Antti Kangasr\\\"a\\\"asi\\\"o", "authors": "Antti Kangasr\\\"a\\\"asi\\\"o, Kumaripaba Athukorala, Andrew Howes, Jukka\n  Corander, Samuel Kaski, Antti Oulasvirta", "title": "Inferring Cognitive Models from Data using Approximate Bayesian\n  Computation", "comments": "To appear in CHI'2017", "journal-ref": null, "doi": "10.1145/3025453.3025576", "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem for HCI researchers is to estimate the parameter values\nof a cognitive model from behavioral data. This is a difficult problem, because\nof the substantial complexity and variety in human behavioral strategies. We\nreport an investigation into a new approach using approximate Bayesian\ncomputation (ABC) to condition model parameters to data and prior knowledge. As\nthe case study we examine menu interaction, where we have click time data only\nto infer a cognitive model that implements a search behaviour with parameters\nsuch as fixation duration and recall probability. Our results demonstrate that\nABC (i) improves estimates of model parameter values, (ii) enables meaningful\ncomparisons between model variants, and (iii) supports fitting models to\nindividual users. ABC provides ample opportunities for theoretical HCI research\nby allowing principled inference of model parameter values and their\nuncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:20:47 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 12:15:47 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Kangasr\u00e4\u00e4si\u00f6", "Antti", ""], ["Athukorala", "Kumaripaba", ""], ["Howes", "Andrew", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "1612.00662", "submitter": "Adam McCarthy", "authors": "Adam McCarthy and Christopher K.I. Williams", "title": "Predicting Patient State-of-Health using Sliding Window and Recurrent\n  Classifiers", "comments": "NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bedside monitors in Intensive Care Units (ICUs) frequently sound incorrectly,\nslowing response times and desensitising nurses to alarms (Chambrin, 2001),\ncausing true alarms to be missed (Hug et al., 2011). We compare sliding window\npredictors with recurrent predictors to classify patient state-of-health from\nICU multivariate time series; we report slightly improved performance for the\nRNN for three out of four targets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:44:31 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["McCarthy", "Adam", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "1612.00667", "submitter": "Santi Puch", "authors": "Santi Puch, Asier Aduriz, Adri\\`a Casamitjana, Veronica Vilaplana,\n  Paula Petrone, Gr\\'egory Operto, Raffaele Cacciaglia, Stavros Skouras, Carles\n  Falcon, Jos\\'e Luis Molinuevo, Juan Domingo Gispert", "title": "Voxelwise nonlinear regression toolbox for neuroimage analysis:\n  Application to aging and neurodegenerative disease modeling", "comments": "4 pages + 1 page for acknowledgements and references. NIPS 2016\n  Workshop on Machine Learning for Health (NIPS ML4HC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new neuroimaging analysis toolbox that allows for the\nmodeling of nonlinear effects at the voxel level, overcoming limitations of\nmethods based on linear models like the GLM. We illustrate its features using a\nrelevant example in which distinct nonlinear trajectories of Alzheimer's\ndisease related brain atrophy patterns were found across the full biological\nspectrum of the disease. The open-source toolbox presented in this paper is\navailable at https://github.com/imatge-upc/VNeAT.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:59:11 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 10:58:16 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 20:12:16 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Puch", "Santi", ""], ["Aduriz", "Asier", ""], ["Casamitjana", "Adri\u00e0", ""], ["Vilaplana", "Veronica", ""], ["Petrone", "Paula", ""], ["Operto", "Gr\u00e9gory", ""], ["Cacciaglia", "Raffaele", ""], ["Skouras", "Stavros", ""], ["Falcon", "Carles", ""], ["Molinuevo", "Jos\u00e9 Luis", ""], ["Gispert", "Juan Domingo", ""]]}, {"id": "1612.00671", "submitter": "Tirtharaj Dash", "authors": "Siddharth Dinesh, Tirtharaj Dash", "title": "Reliable Evaluation of Neural Network for Multiclass Classification of\n  Real-world Data", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-2016-STUDY-1", "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a systematic evaluation of Neural Network (NN) for\nclassification of real-world data. In the field of machine learning, it is\noften seen that a single parameter that is 'predictive accuracy' is being used\nfor evaluating the performance of a classifier model. However, this parameter\nmight not be considered reliable given a dataset with very high level of\nskewness. To demonstrate such behavior, seven different types of datasets have\nbeen used to evaluate a Multilayer Perceptron (MLP) using twelve(12) different\nparameters which include micro- and macro-level estimation. In the present\nstudy, the most common problem of prediction called 'multiclass' classification\nhas been considered. The results that are obtained for different parameters for\neach of the dataset could demonstrate interesting findings to support the\nusability of these set of performance evaluation parameters.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 19:58:44 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Dinesh", "Siddharth", ""], ["Dash", "Tirtharaj", ""]]}, {"id": "1612.00686", "submitter": "Philipp Seeb\\\"ock", "authors": "Philipp Seeb\\\"ock, Sebastian Waldstein, Sophie Klimscha, Bianca S.\n  Gerendas, Ren\\'e Donner, Thomas Schlegl, Ursula Schmidt-Erfurth and Georg\n  Langs", "title": "Identifying and Categorizing Anomalies in Retinal Imaging Data", "comments": "Extended Abstract, Accepted for NIPS 2016 Workshop \"Machine Learning\n  for Health\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification and quantification of markers in medical images is\ncritical for diagnosis, prognosis and management of patients in clinical\npractice. Supervised- or weakly supervised training enables the detection of\nfindings that are known a priori. It does not scale well, and a priori\ndefinition limits the vocabulary of markers to known entities reducing the\naccuracy of diagnosis and prognosis. Here, we propose the identification of\nanomalies in large-scale medical imaging data using healthy examples as a\nreference. We detect and categorize candidates for anomaly findings untypical\nfor the observed data. A deep convolutional autoencoder is trained on healthy\nretinal images. The learned model generates a new feature representation, and\nthe distribution of healthy retinal patches is estimated by a one-class support\nvector machine. Results demonstrate that we can identify pathologic regions in\nimages without using expert annotations. A subsequent clustering categorizes\nfindings into clinically meaningful classes. In addition the learned features\noutperform standard embedding approaches in a classification task.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 14:05:49 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Seeb\u00f6ck", "Philipp", ""], ["Waldstein", "Sebastian", ""], ["Klimscha", "Sophie", ""], ["Gerendas", "Bianca S.", ""], ["Donner", "Ren\u00e9", ""], ["Schlegl", "Thomas", ""], ["Schmidt-Erfurth", "Ursula", ""], ["Langs", "Georg", ""]]}, {"id": "1612.00712", "submitter": "Jayant Krishnamurthy", "authors": "Kenton W. Murray and Jayant Krishnamurthy", "title": "Probabilistic Neural Programs", "comments": "Appears in NAMPI workshop at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present probabilistic neural programs, a framework for program induction\nthat permits flexible specification of both a computational model and inference\nalgorithm while simultaneously enabling the use of deep neural networks.\nProbabilistic neural programs combine a computation graph for specifying a\nneural network with an operator for weighted nondeterministic choice. Thus, a\nprogram describes both a collection of decisions as well as the neural network\narchitecture used to make each one. We evaluate our approach on a challenging\ndiagram question answering task where probabilistic neural programs correctly\nexecute nearly twice as many programs as a baseline model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 15:46:09 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Murray", "Kenton W.", ""], ["Krishnamurthy", "Jayant", ""]]}, {"id": "1612.00745", "submitter": "Andras Lorincz", "authors": "Andr\\'as L\\H{o}rincz, M\\'at\\'e Cs\\'akv\\'ari, \\'Aron F\\'othi, Zolt\\'an\n  \\'Ad\\'am Milacski, Andr\\'as S\\'ark\\'any, Zolt\\'an T\\H{o}s\\'er", "title": "Cognitive Deep Machine Can Train Itself", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is making substantial progress in diverse applications. The\nsuccess is mostly due to advances in deep learning. However, deep learning can\nmake mistakes and its generalization abilities to new tasks are questionable.\nWe ask when and how one can combine network outputs, when (i) details of the\nobservations are evaluated by learned deep components and (ii) facts and\nconfirmation rules are available in knowledge based systems. We show that in\nlimited contexts the required number of training samples can be low and\nself-improvement of pre-trained networks in more general context is possible.\nWe argue that the combination of sparse outlier detection with deep components\nthat can support each other diminish the fragility of deep methods, an\nimportant requirement for engineering applications. We argue that supervised\nlearning of labels may be fully eliminated under certain conditions: a\ncomponent based architecture together with a knowledge based system can train\nitself and provide high quality answers. We demonstrate these concepts on the\nState Farm Distracted Driver Detection benchmark. We argue that the view of the\nStudy Panel (2016) may overestimate the requirements on `years of focused\nresearch' and `careful, unique construction' for `AI systems'.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 16:49:07 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["L\u0151rincz", "Andr\u00e1s", ""], ["Cs\u00e1kv\u00e1ri", "M\u00e1t\u00e9", ""], ["F\u00f3thi", "\u00c1ron", ""], ["Milacski", "Zolt\u00e1n \u00c1d\u00e1m", ""], ["S\u00e1rk\u00e1ny", "Andr\u00e1s", ""], ["T\u0151s\u00e9r", "Zolt\u00e1n", ""]]}, {"id": "1612.00767", "submitter": "Jost Tobias Springenberg", "authors": "Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, Frank Hutter", "title": "Asynchronous Stochastic Gradient MCMC with Elastic Coupling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parallel asynchronous Markov Chain Monte Carlo (MCMC) sampling\nfor problems where we can leverage (stochastic) gradients to define continuous\ndynamics which explore the target distribution. We outline a solution strategy\nfor this setting based on stochastic gradient Hamiltonian Monte Carlo sampling\n(SGHMC) which we alter to include an elastic coupling term that ties together\nmultiple MCMC instances. The proposed strategy turns inherently sequential HMC\nalgorithms into asynchronous parallel versions. First experiments empirically\nshow that the resulting parallel sampler significantly speeds up exploration of\nthe target distribution, when compared to standard SGHMC, and is less prone to\nthe harmful effects of stale gradients than a naive parallelization approach.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 17:43:33 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 09:19:30 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Springenberg", "Jost Tobias", ""], ["Klein", "Aaron", ""], ["Falkner", "Stefan", ""], ["Hutter", "Frank", ""]]}, {"id": "1612.00775", "submitter": "Christopher Beckham", "authors": "Christopher Beckham, Christopher Pal", "title": "A simple squared-error reformulation for ordinal classification", "comments": "v1: Camera-ready abstract for NIPS for Health Workshop (2016) v2:\n  Clean-up of some sections, added appendix section where we briefly explore\n  optimisation of quadratic weighted kappa (QWK)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore ordinal classification (in the context of deep\nneural networks) through a simple modification of the squared error loss which\nnot only allows it to not only be sensitive to class ordering, but also allows\nthe possibility of having a discrete probability distribution over the classes.\nOur formulation is based on the use of a softmax hidden layer, which has\nreceived relatively little attention in the literature. We empirically evaluate\nits performance on the Kaggle diabetic retinopathy dataset, an ordinal and\nhigh-resolution dataset and show that it outperforms all of the baselines\nemployed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 17:57:04 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 16:04:38 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Beckham", "Christopher", ""], ["Pal", "Christopher", ""]]}, {"id": "1612.00796", "submitter": "Raia Hadsell", "authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness,\n  Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho,\n  Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan\n  Kumaran, Raia Hadsell", "title": "Overcoming catastrophic forgetting in neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 19:18:37 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 13:01:51 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Kirkpatrick", "James", ""], ["Pascanu", "Razvan", ""], ["Rabinowitz", "Neil", ""], ["Veness", "Joel", ""], ["Desjardins", "Guillaume", ""], ["Rusu", "Andrei A.", ""], ["Milan", "Kieran", ""], ["Quan", "John", ""], ["Ramalho", "Tiago", ""], ["Grabska-Barwinska", "Agnieszka", ""], ["Hassabis", "Demis", ""], ["Clopath", "Claudia", ""], ["Kumaran", "Dharshan", ""], ["Hadsell", "Raia", ""]]}, {"id": "1612.00804", "submitter": "Ethan R. Elenberg", "authors": "Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, Sahand\n  Negahban", "title": "Restricted Strong Convexity Implies Weak Submodularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We connect high-dimensional subset selection and submodular maximization. Our\nresults extend the work of Das and Kempe (2011) from the setting of linear\nregression to arbitrary objective functions. For greedy feature selection, this\nconnection allows us to obtain strong multiplicative performance bounds on\nseveral methods without statistical modeling assumptions. We also derive\nrecovery guarantees of this form under standard assumptions. Our work shows\nthat greedy algorithms perform within a constant factor from the best possible\nsubset-selection solution for a broad class of general objective functions. Our\nmethods allow a direct control over the number of obtained features as opposed\nto regularization parameters that only implicitly control sparsity. Our proof\ntechnique uses the concept of weak submodularity initially defined by Das and\nKempe. We draw a connection between convex analysis and submodular set function\ntheory which may be of independent interest for other statistical learning\napplications that have combinatorial structure.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 19:32:55 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 05:25:40 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Elenberg", "Ethan R.", ""], ["Khanna", "Rajiv", ""], ["Dimakis", "Alexandros G.", ""], ["Negahban", "Sahand", ""]]}, {"id": "1612.00814", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, Honglak Lee", "title": "Perspective Transformer Nets: Learning Single-View 3D Object\n  Reconstruction without 3D Supervision", "comments": "published at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Understanding the 3D world is a fundamental problem in computer vision.\nHowever, learning a good representation of 3D objects is still an open problem\ndue to the high dimensionality of the data and many factors of variation\ninvolved. In this work, we investigate the task of single-view 3D object\nreconstruction from a learning agent's perspective. We formulate the learning\nprocess as an interaction between 3D and 2D representations and propose an\nencoder-decoder network with a novel projection loss defined by the perspective\ntransformation. More importantly, the projection loss enables the unsupervised\nlearning using 2D observation without explicit 3D supervision. We demonstrate\nthe ability of the model in generating 3D volume from a single 2D image with\nthree sets of experiments: (1) learning from single-class objects; (2) learning\nfrom multi-class objects and (3) testing on novel object classes. Results show\nsuperior performance and better generalization ability for 3D object\nreconstruction when the projection loss is involved.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:51:37 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 07:08:48 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 02:40:50 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Yan", "Xinchen", ""], ["Yang", "Jimei", ""], ["Yumer", "Ersin", ""], ["Guo", "Yijie", ""], ["Lee", "Honglak", ""]]}, {"id": "1612.00817", "submitter": "Alexander Gaunt", "authors": "Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman,\n  Pushmeet Kohli, Jonathan Taylor, Daniel Tarlow", "title": "Summary - TerpreT: A Probabilistic Programming Language for Program\n  Induction", "comments": "7 pages, 2 figures, 4 tables in 1st Workshop on Neural Abstract\n  Machines & Program Induction (NAMPI), @NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study machine learning formulations of inductive program synthesis; that\nis, given input-output examples, synthesize source code that maps inputs to\ncorresponding outputs. Our key contribution is TerpreT, a domain-specific\nlanguage for expressing program synthesis problems. A TerpreT model is composed\nof a specification of a program representation and an interpreter that\ndescribes how programs map inputs to outputs. The inference task is to observe\na set of input-output examples and infer the underlying program. From a TerpreT\nmodel we automatically perform inference using four different back-ends:\ngradient descent (thus each TerpreT model can be seen as defining a\ndifferentiable interpreter), linear program (LP) relaxations for graphical\nmodels, discrete satisfiability solving, and the Sketch program synthesis\nsystem. TerpreT has two main benefits. First, it enables rapid exploration of a\nrange of domains, program representations, and interpreter models. Second, it\nseparates the model specification from the inference algorithm, allowing proper\ncomparisons between different approaches to inference.\n  We illustrate the value of TerpreT by developing several interpreter models\nand performing an extensive empirical comparison between alternative inference\nalgorithms on a variety of program models. To our knowledge, this is the first\nwork to compare gradient-based search over program space to traditional\nsearch-based alternatives. Our key empirical finding is that constraint solvers\ndominate the gradient descent and LP-based formulations.\n  This is a workshop summary of a longer report at arXiv:1608.04428\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:08:22 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Gaunt", "Alexander L.", ""], ["Brockschmidt", "Marc", ""], ["Singh", "Rishabh", ""], ["Kushman", "Nate", ""], ["Kohli", "Pushmeet", ""], ["Taylor", "Jonathan", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1612.00824", "submitter": "Ingo Steinwart", "authors": "Ingo Steinwart and Philipp Thomann and Nico Schmid", "title": "Learning with Hierarchical Gaussian Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate iterated compositions of weighted sums of Gaussian kernels and\nprovide an interpretation of the construction that shows some similarities with\nthe architectures of deep neural networks. On the theoretical side, we show\nthat these kernels are universal and that SVMs using these kernels are\nuniversally consistent. We further describe a parameter optimization method for\nthe kernel parameters and empirically compare this method to SVMs, random\nforests, a multiple kernel learning approach, and to some deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:23:31 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Steinwart", "Ingo", ""], ["Thomann", "Philipp", ""], ["Schmid", "Nico", ""]]}, {"id": "1612.00827", "submitter": "Tristan Deleu", "authors": "Tristan Deleu, Joseph Dureau", "title": "Learning Operations on a Stack with Neural Turing Machines", "comments": "1st Workshop on Neural Abstract Machines & Program Induction (NAMPI),\n  NIPS 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple extensions of Recurrent Neural Networks (RNNs) have been proposed\nrecently to address the difficulty of storing information over long time\nperiods. In this paper, we experiment with the capacity of Neural Turing\nMachines (NTMs) to deal with these long-term dependencies on well-balanced\nstrings of parentheses. We show that not only does the NTM emulate a stack with\nits heads and learn an algorithm to recognize such words, but it is also\ncapable of strongly generalizing to much longer sequences.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:31:44 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Deleu", "Tristan", ""], ["Dureau", "Joseph", ""]]}, {"id": "1612.00835", "submitter": "Patsorn Sangkloy", "authors": "Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, James Hays", "title": "Scribbler: Controlling Deep Image Synthesis with Sketch and Color", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been several promising methods to generate realistic\nimagery from deep convolutional networks. These methods sidestep the\ntraditional computer graphics rendering pipeline and instead generate imagery\nat the pixel level by learning from large collections of photos (e.g. faces or\nbedrooms). However, these methods are of limited utility because it is\ndifficult for a user to control what the network produces. In this paper, we\npropose a deep adversarial image synthesis architecture that is conditioned on\nsketched boundaries and sparse color strokes to generate realistic cars,\nbedrooms, or faces. We demonstrate a sketch based image synthesis system which\nallows users to 'scribble' over the sketch to indicate preferred color for\nobjects. Our network can then generate convincing images that satisfy both the\ncolor and the sketch constraints of user. The network is feed-forward which\nallows users to see the effect of their edits in real time. We compare to\nrecent work on sketch to image synthesis and show that our approach can\ngenerate more realistic, more diverse, and more controllable outputs. The\narchitecture is also effective at user-guided colorization of grayscale images.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:53:01 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 20:06:57 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sangkloy", "Patsorn", ""], ["Lu", "Jingwan", ""], ["Fang", "Chen", ""], ["Yu", "Fisher", ""], ["Hays", "James", ""]]}, {"id": "1612.00837", "submitter": "Yash Goyal", "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems at the intersection of vision and language are of significant\nimportance both as challenging research questions and for the rich set of\napplications they enable. However, inherent structure in our world and bias in\nour language tend to be a simpler signal for learning than visual modalities,\nresulting in models that ignore visual information, leading to an inflated\nsense of their capability.\n  We propose to counter these language priors for the task of Visual Question\nAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balance\nthe popular VQA dataset by collecting complementary images such that every\nquestion in our balanced dataset is associated with not just a single image,\nbut rather a pair of similar images that result in two different answers to the\nquestion. Our dataset is by construction more balanced than the original VQA\ndataset and has approximately twice the number of image-question pairs. Our\ncomplete balanced dataset is publicly available at www.visualqa.org as part of\nthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA\nv2.0).\n  We further benchmark a number of state-of-art VQA models on our balanced\ndataset. All models perform significantly worse on our balanced dataset,\nsuggesting that these models have indeed learned to exploit language priors.\nThis finding provides the first concrete empirical evidence for what seems to\nbe a qualitative sense among practitioners.\n  Finally, our data collection protocol for identifying complementary images\nenables us to develop a novel interpretable model, which in addition to\nproviding an answer to the given (image, question) pair, also provides a\ncounter-example based explanation. Specifically, it identifies an image that is\nsimilar to the original image, but it believes has a different answer to the\nsame question. This can help in building trust for machines among their users.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:57:07 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 18:20:13 GMT"}, {"version": "v3", "created": "Mon, 15 May 2017 17:58:49 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Goyal", "Yash", ""], ["Khot", "Tejas", ""], ["Summers-Stay", "Douglas", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1612.00840", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Aurobinda Routray, William K. Mohanty, Mamata Jenamani", "title": "A novel multiclassSVM based framework to classify lithology from well\n  logs: a real-world application", "comments": "5 pages, 5 figures, 4 tables Presented at INDICON 2015 at New Delhi,\n  India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) have been recognized as a potential tool for\nsupervised classification analyses in different domains of research. In\nessence, SVM is a binary classifier. Therefore, in case of a multiclass\nproblem, the problem is divided into a series of binary problems which are\nsolved by binary classifiers, and finally the classification results are\ncombined following either the one-against-one or one-against-all strategies. In\nthis paper, an attempt has been made to classify lithology using a multiclass\nSVM based framework using well logs as predictor variables. Here, the lithology\nis classified into four classes such as sand, shaly sand, sandy shale and shale\nbased on the relative values of sand and shale fractions as suggested by an\nexpert geologist. The available dataset consisting well logs (gamma ray,\nneutron porosity, density, and P-sonic) and class information from four closely\nspaced wells from an onshore hydrocarbon field is divided into training and\ntesting sets. We have used one-against-all strategy to combine the results of\nmultiple binary classifiers. The reported results established the superiority\nof multiclass SVM compared to other classifiers in terms of classification\naccuracy. The selection of kernel function and associated parameters has also\nbeen investigated here. It can be envisaged from the results achieved in this\nstudy that the proposed framework based on multiclass SVM can further be used\nto solve classification problems. In future research endeavor, seismic\nattributes can be introduced in the framework to classify the lithology\nthroughout a study area from seismic inputs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:55:16 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chaki", "Soumi", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.00841", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Akhilesh Kumar Verma, Aurobinda Routray, William K.\n  Mohanty, Mamata Jenamani", "title": "A Novel Framework based on SVDD to Classify Water Saturation from\n  Seismic Attributes", "comments": "6 pages, 8 figures, 2table Presented at Fourth International\n  Conference on Emerging Applications of Information Technology (EAIT 2014),\n  ISI Kolkata, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water saturation is an important property in reservoir engineering domain.\nThus, satisfactory classification of water saturation from seismic attributes\nis beneficial for reservoir characterization. However, diverse and non-linear\nnature of subsurface attributes makes the classification task difficult. In\nthis context, this paper proposes a generalized Support Vector Data Description\n(SVDD) based novel classification framework to classify water saturation into\ntwo classes (Class high and Class low) from three seismic attributes seismic\nimpedance, amplitude envelop, and seismic sweetness. G-metric means and program\nexecution time are used to quantify the performance of the proposed framework\nalong with established supervised classifiers. The documented results imply\nthat the proposed framework is superior to existing classifiers. The present\nstudy is envisioned to contribute in further reservoir modeling.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:57:08 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chaki", "Soumi", ""], ["Verma", "Akhilesh Kumar", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.00882", "submitter": "Liangpeng Zhang", "authors": "Liangpeng Zhang, Ke Tang, Xin Yao", "title": "Success Probability of Exploration: a Concrete Analysis of Learning\n  Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration has been a crucial part of reinforcement learning, yet several\nimportant questions concerning exploration efficiency are still not answered\nsatisfactorily by existing analytical frameworks. These questions include\nexploration parameter setting, situation analysis, and hardness of MDPs, all of\nwhich are unavoidable for practitioners. To bridge the gap between the theory\nand practice, we propose a new analytical framework called the success\nprobability of exploration. We show that those important questions of\nexploration above can all be answered under our framework, and the answers\nprovided by our framework meet the needs of practitioners better than the\nexisting ones. More importantly, we introduce a concrete and practical approach\nto evaluating the success probabilities in certain MDPs without the need of\nactually running the learning algorithm. We then provide empirical results to\nverify our approach, and demonstrate how the success probability of exploration\ncan be used to analyse and predict the behaviours and possible outcomes of\nexploration, which are the keys to the answer of the important questions of\nexploration.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 22:38:37 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhang", "Liangpeng", ""], ["Tang", "Ke", ""], ["Yao", "Xin", ""]]}, {"id": "1612.00891", "submitter": "Jonathan Cox", "authors": "Jonathan A. Cox", "title": "Parameter Compression of Recurrent Neural Networks and Degradation of\n  Short-term Memory", "comments": "Accepted to IJCNN 2017. Final camera ready paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant computational costs of deploying neural networks in\nlarge-scale or resource constrained environments, such as data centers and\nmobile devices, has spurred interest in model compression, which can achieve a\nreduction in both arithmetic operations and storage memory. Several techniques\nhave been proposed for reducing or compressing the parameters for feed-forward\nand convolutional neural networks, but less is understood about the effect of\nparameter compression on recurrent neural networks (RNN). In particular, the\nextent to which the recurrent parameters can be compressed and the impact on\nshort-term memory performance, is not well understood. In this paper, we study\nthe effect of complexity reduction, through singular value decomposition rank\nreduction, on RNN and minimal gated recurrent unit (MGRU) networks for several\ntasks. We show that considerable rank reduction is possible when compressing\nrecurrent weights, even without fine tuning. Furthermore, we propose a\nperturbation model for the effect of general perturbations, such as a\ncompression, on the recurrent parameters of RNNs. The model is tested against a\nnoiseless memorization experiment that elucidates the short-term memory\nperformance. In this way, we demonstrate that the effect of compression of\nrecurrent parameters is dependent on the degree of temporal coherence present\nin the data and task. This work can guide on-the-fly RNN compression for novel\nenvironments or tasks, and provides insight for applying RNN compression in\nlow-power devices, such as hearing aids.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 23:11:10 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 18:22:30 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Cox", "Jonathan A.", ""]]}, {"id": "1612.00913", "submitter": "Xuesong Yang", "authors": "Xuesong Yang, Yun-Nung Chen, Dilek Hakkani-Tur, Paul Crook, Xiujun Li,\n  Jianfeng Gao, Li Deng", "title": "End-to-End Joint Learning of Natural Language Understanding and Dialogue\n  Manager", "comments": "Accepted in The 42nd IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language understanding and dialogue policy learning are both\nessential in conversational systems that predict the next system actions in\nresponse to a current user utterance. Conventional approaches aggregate\nseparate models of natural language understanding (NLU) and system action\nprediction (SAP) as a pipeline that is sensitive to noisy outputs of\nerror-prone NLU. To address the issues, we propose an end-to-end deep recurrent\nneural network with limited contextual dialogue memory by jointly training NLU\nand SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our\nproposed model significantly outperforms the state-of-the-art pipeline models\nfor both NLU and SAP, which indicates that our joint model is capable of\nmitigating the affects of noisy NLU outputs, and NLU model can be refined by\nerror flows backpropagating from the extra supervised signals of system\nactions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 02:13:18 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 08:39:09 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Yang", "Xuesong", ""], ["Chen", "Yun-Nung", ""], ["Hakkani-Tur", "Dilek", ""], ["Crook", "Paul", ""], ["Li", "Xiujun", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1612.00962", "submitter": "Leen De Baets", "authors": "Leen De Baets, Joeri Ruyssinck, Thomas Peiffer, Johan Decruyenaere,\n  Filip De Turck, Femke Ongenae, Tom Dhaene", "title": "Positive blood culture detection in time series data using a BiLSTM\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of bacteria or fungi in the bloodstream of patients is abnormal\nand can lead to life-threatening conditions. A computational model based on a\nbidirectional long short-term memory artificial neural network, is explored to\nassist doctors in the intensive care unit to predict whether examination of\nblood cultures of patients will return positive. As input it uses nine\nmonitored clinical parameters, presented as time series data, collected from\n2177 ICU admissions at the Ghent University Hospital. Our main goal is to\ndetermine if general machine learning methods and more specific, temporal\nmodels, can be used to create an early detection system. This preliminary\nresearch obtains an area of 71.95% under the precision recall curve, proving\nthe potential of temporal neural networks in this context.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 12:16:21 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["De Baets", "Leen", ""], ["Ruyssinck", "Joeri", ""], ["Peiffer", "Thomas", ""], ["Decruyenaere", "Johan", ""], ["De Turck", "Filip", ""], ["Ongenae", "Femke", ""], ["Dhaene", "Tom", ""]]}, {"id": "1612.00984", "submitter": "Corrado Monti", "authors": "Corrado Monti and Paolo Boldi", "title": "Estimating latent feature-feature interactions in large feature-rich\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world complex networks describe connections between objects; in reality,\nthose objects are often endowed with some kind of features. How does the\npresence or absence of such features interplay with the network link structure?\nAlthough the situation here described is truly ubiquitous, there is a limited\nbody of research dealing with large graphs of this kind. Many previous works\nconsidered homophily as the only possible transmission mechanism translating\nnode features into links. Other authors, instead, developed more sophisticated\nmodels, that are able to handle complex feature interactions, but are unfit to\nscale to very large networks. We expand on the MGJ model, where interactions\nbetween pairs of features can foster or discourage link formation. In this\nwork, we will investigate how to estimate the latent feature-feature\ninteractions in this model. We shall propose two solutions: the first one\nassumes feature independence and it is essentially based on Naive Bayes; the\nsecond one, which relaxes the independence assumption assumption, is based on\nperceptrons. In fact, we show it is possible to cast the model equation in\norder to see it as the prediction rule of a perceptron. We analyze how\nclassical results for the perceptrons can be interpreted in this context; then,\nwe define a fast and simple perceptron-like algorithm for this task, which can\nprocess $10^8$ links in minutes. We then compare these two techniques, first\nwith synthetic datasets that follows our model, gaining evidence that the Naive\nindependence assumptions are detrimental in practice. Secondly, we consider a\nreal, large-scale citation network where each node (i.e., paper) can be\ndescribed by different types of characteristics; there, our algorithm can\nassess how well each set of features can explain the links, and thus finding\nmeaningful latent feature-feature interactions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 16:42:59 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 18:18:55 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Monti", "Corrado", ""], ["Boldi", "Paolo", ""]]}, {"id": "1612.01020", "submitter": "Simon Du", "authors": "Simon Shaolei Du, Jayanth Koushik, Aarti Singh, and Barnabas Poczos", "title": "Hypothesis Transfer Learning via Transformation Functions", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Hypothesis Transfer Learning (HTL) problem where one\nincorporates a hypothesis trained on the source domain into the learning\nprocedure of the target domain. Existing theoretical analysis either only\nstudies specific algorithms or only presents upper bounds on the generalization\nerror but not on the excess risk. In this paper, we propose a unified\nalgorithm-dependent framework for HTL through a novel notion of transformation\nfunction, which characterizes the relation between the source and the target\ndomains. We conduct a general risk analysis of this framework and in\nparticular, we show for the first time, if two domains are related, HTL enjoys\nfaster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge\nRegression than those of the classical non-transfer learning settings.\nExperiments on real world data demonstrate the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 21:22:43 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 23:00:23 GMT"}, {"version": "v3", "created": "Sat, 27 May 2017 21:28:27 GMT"}, {"version": "v4", "created": "Sun, 5 Nov 2017 16:24:27 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Du", "Simon Shaolei", ""], ["Koushik", "Jayanth", ""], ["Singh", "Aarti", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1612.01030", "submitter": "Alexandre Drouin", "authors": "Alexandre Drouin, Fr\\'ed\\'eric Raymond, Ga\\\"el Letarte St-Pierre,\n  Mario Marchand, Jacques Corbeil, Fran\\c{c}ois Laviolette", "title": "Large scale modeling of antimicrobial resistance with interpretable\n  classifiers", "comments": "Peer-reviewed and accepted for presentation at the Machine Learning\n  for Health Workshop, NIPS 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antimicrobial resistance is an important public health concern that has\nimplications in the practice of medicine worldwide. Accurately predicting\nresistance phenotypes from genome sequences shows great promise in promoting\nbetter use of antimicrobial agents, by determining which antibiotics are likely\nto be effective in specific clinical cases. In healthcare, this would allow for\nthe design of treatment plans tailored for specific individuals, likely\nresulting in better clinical outcomes for patients with bacterial infections.\nIn this work, we present the recent work of Drouin et al. (2016) on using Set\nCovering Machines to learn highly interpretable models of antibiotic resistance\nand complement it by providing a large scale application of their method to the\nentire PATRIC database. We report prediction results for 36 new datasets and\npresent the Kover AMR platform, a new web-based tool allowing the visualization\nand interpretation of the generated models.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 22:52:44 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Drouin", "Alexandre", ""], ["Raymond", "Fr\u00e9d\u00e9ric", ""], ["St-Pierre", "Ga\u00ebl Letarte", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""], ["Laviolette", "Fran\u00e7ois", ""]]}, {"id": "1612.01055", "submitter": "Anna Goldenberg", "authors": "Lauren Erdman, Ekansh Sharma, Eva Unternahrer, Shantala Hari Dass,\n  Kieran ODonnell, Sara Mostafavi, Rachel Edgar, Michael Kobor, Helene\n  Gaudreau, Michael Meaney, Anna Goldenberg", "title": "Modeling trajectories of mental health: challenges and opportunities", "comments": "extended abstract for ML4HC at NIPS 2016, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than two thirds of mental health problems have their onset during\nchildhood or adolescence. Identifying children at risk for mental illness later\nin life and predicting the type of illness is not easy. We set out to develop a\nplatform to define subtypes of childhood social-emotional development using\nlongitudinal, multifactorial trait-based measures. Subtypes discovered through\nthis study could ultimately advance psychiatric knowledge of the early\nbehavioural signs of mental illness. To this extent we have examined two types\nof models: latent class mixture models and GP-based models. Our findings\nindicate that while GP models come close in accuracy of predicting future\ntrajectories, LCMMs predict the trajectories as well in a fraction of the time.\nUnfortunately, neither of the models are currently accurate enough to lead to\nimmediate clinical impact. The available data related to the development of\nchildhood mental health is often sparse with only a few time points measured\nand require novel methods with improved efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 03:20:54 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Erdman", "Lauren", ""], ["Sharma", "Ekansh", ""], ["Unternahrer", "Eva", ""], ["Dass", "Shantala Hari", ""], ["ODonnell", "Kieran", ""], ["Mostafavi", "Sara", ""], ["Edgar", "Rachel", ""], ["Kobor", "Michael", ""], ["Gaudreau", "Helene", ""], ["Meaney", "Michael", ""], ["Goldenberg", "Anna", ""]]}, {"id": "1612.01058", "submitter": "Margareta Ackerman", "authors": "Margareta Ackerman and David Loker", "title": "Algorithmic Songwriting with ALYSIA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces ALYSIA: Automated LYrical SongwrIting Application.\nALYSIA is based on a machine learning model using Random Forests, and we\ndiscuss its success at pitch and rhythm prediction. Next, we show how ALYSIA\nwas used to create original pop songs that were subsequently recorded and\nproduced. Finally, we discuss our vision for the future of Automated\nSongwriting for both co-creative and autonomous systems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 03:36:51 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ackerman", "Margareta", ""], ["Loker", "David", ""]]}, {"id": "1612.01064", "submitter": "Chenzhuo Zhu", "authors": "Chenzhuo Zhu, Song Han, Huizi Mao, William J. Dally", "title": "Trained Ternary Quantization", "comments": "Accepted for Poster Presentation on ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are widely used in machine learning applications.\nHowever, the deployment of large neural networks models can be difficult to\ndeploy on mobile devices with limited power budgets. To solve this problem, we\npropose Trained Ternary Quantization (TTQ), a method that can reduce the\nprecision of weights in neural networks to ternary values. This method has very\nlittle accuracy degradation and can even improve the accuracy of some models\n(32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet\nmodel is trained from scratch, which means it's as easy as to train normal full\nprecision model. We highlight our trained quantization method that can learn\nboth ternary values and ternary assignment. During inference, only ternary\nvalues (2-bit weights) and scaling factors are needed, therefore our models are\nnearly 16x smaller than full-precision models. Our ternary models can also be\nviewed as sparse binary weight networks, which can potentially be accelerated\nwith custom circuit. Experiments on CIFAR-10 show that the ternary models\nobtained by trained quantization method outperform full-precision models of\nResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model\noutperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and\noutperforms previous ternary models by 3%.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 05:00:22 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 03:27:20 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 06:52:28 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Zhu", "Chenzhuo", ""], ["Han", "Song", ""], ["Mao", "Huizi", ""], ["Dally", "William J.", ""]]}, {"id": "1612.01078", "submitter": "Ali Nassif", "authors": "Ali Bou Nassif, Luiz Fernando Capretz, Danny Ho", "title": "Enhancing Use Case Points Estimation Method Using Soft Computing\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software estimation is a crucial task in software engineering. Software\nestimation encompasses cost, effort, schedule, and size. The importance of\nsoftware estimation becomes critical in the early stages of the software life\ncycle when the details of software have not been revealed yet. Several\ncommercial and non-commercial tools exist to estimate software in the early\nstages. Most software effort estimation methods require software size as one of\nthe important metric inputs and consequently, software size estimation in the\nearly stages becomes essential. One of the approaches that has been used for\nabout two decades in the early size and effort estimation is called use case\npoints. Use case points method relies on the use case diagram to estimate the\nsize and effort of software projects. Although the use case points method has\nbeen widely used, it has some limitations that might adversely affect the\naccuracy of estimation. This paper presents some techniques using fuzzy logic\nand neural networks to improve the accuracy of the use case points method.\nResults showed that an improvement up to 22% can be obtained using the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 06:59:14 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Nassif", "Ali Bou", ""], ["Capretz", "Luiz Fernando", ""], ["Ho", "Danny", ""]]}, {"id": "1612.01086", "submitter": "Bar Hilleli", "authors": "Bar Hilleli and Ran El-Yaniv", "title": "Deep Learning of Robotic Tasks without a Simulator using Strong and Weak\n  Human Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scheme for training a computerized agent to perform complex\nhuman tasks such as highway steering. The scheme is designed to follow a\nnatural learning process whereby a human instructor teaches a computerized\ntrainee. The learning process consists of five elements: (i) unsupervised\nfeature learning; (ii) supervised imitation learning; (iii) supervised reward\ninduction; (iv) supervised safety module construction; and (v) reinforcement\nlearning. We implemented the last four elements of the scheme using deep\nconvolutional networks and applied it to successfully create a computerized\nagent capable of autonomous highway steering over the well-known racing game\nAssetto Corsa. We demonstrate that the use of the last four elements is\nessential to effectively carry out the steering task using vision alone,\nwithout access to a driving simulator internals, and operating in wall-clock\ntime. This is made possible also through the introduction of a safety network,\na novel way for preventing the agent from performing catastrophic mistakes\nduring the reinforcement learning stage.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 08:28:38 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 17:08:16 GMT"}, {"version": "v3", "created": "Sun, 26 Mar 2017 08:43:23 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Hilleli", "Bar", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1612.01094", "submitter": "Rudy Bunel", "authors": "Rudy Bunel, Alban Desmaison, M. Pawan Kumar, Philip H.S.Torr, Pushmeet\n  Kohli", "title": "Learning to superoptimize programs - Workshop Version", "comments": "Workshop version for the NIPS NAMPI Workshop. Extended version at\n  arXiv:1611.01787", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superoptimization requires the estimation of the best program for a given\ncomputational task. In order to deal with large programs, superoptimization\ntechniques perform a stochastic search. This involves proposing a modification\nof the current program, which is accepted or rejected based on the improvement\nachieved. The state of the art method uses uniform proposal distributions,\nwhich fails to exploit the problem structure to the fullest. To alleviate this\ndeficiency, we learn a proposal distribution over possible modifications using\nReinforcement Learning. We provide convincing results on the superoptimization\nof \"Hacker's Delight\" programs.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 10:39:49 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Bunel", "Rudy", ""], ["Desmaison", "Alban", ""], ["Kumar", "M. Pawan", ""], ["Torr", "Philip H. S.", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1612.01103", "submitter": "Michael Tschannen", "authors": "Michael Tschannen and Helmut B\\\"olcskei", "title": "Robust nonparametric nearest neighbor random process clustering", "comments": "15 pages, 7 figures", "journal-ref": "IEEE Transactions on Signal Processing, Vol. 65, No. 22, pp.\n  6009-6023, Nov. 2017", "doi": "10.1109/TSP.2017.2736513", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering noisy finite-length observations of\nstationary ergodic random processes according to their generative models\nwithout prior knowledge of the model statistics and the number of generative\nmodels. Two algorithms, both using the $L^1$-distance between estimated power\nspectral densities (PSDs) as a measure of dissimilarity, are analyzed. The\nfirst one, termed nearest neighbor process clustering (NNPC), relies on\npartitioning the nearest neighbor graph of the observations via spectral\nclustering. The second algorithm, simply referred to as $k$-means (KM),\nconsists of a single $k$-means iteration with farthest point initialization and\nwas considered before in the literature, albeit with a different dissimilarity\nmeasure. We prove that both algorithms succeed with high probability in the\npresence of noise and missing entries, and even when the generative process\nPSDs overlap significantly, all provided that the observation length is\nsufficiently large. Our results quantify the tradeoff between the overlap of\nthe generative process PSDs, the observation length, the fraction of missing\nentries, and the noise variance. Finally, we provide extensive numerical\nresults for synthetic and real data and find that NNPC outperforms\nstate-of-the-art algorithms in human motion sequence clustering.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 11:38:06 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 16:11:31 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 05:27:08 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Tschannen", "Michael", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1612.01158", "submitter": "Andee Kaplan", "authors": "Andee Kaplan, Daniel Nordman, and Stephen Vardeman", "title": "Properties and Bayesian fitting of restricted Boltzmann machines", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": "10.1002/sam.11396", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A restricted Boltzmann machine (RBM) is an undirected graphical model\nconstructed for discrete or continuous random variables, with two layers, one\nhidden and one visible, and no conditional dependency within a layer. In recent\nyears, RBMs have risen to prominence due to their connection to deep learning.\nBy treating a hidden layer of one RBM as the visible layer in a second RBM, a\ndeep architecture can be created. RBMs are thought to thereby have the ability\nto encode very complex and rich structures in data, making them attractive for\nsupervised learning. However, the generative behavior of RBMs is largely\nunexplored and typical fitting methodology does not easily allow for\nuncertainty quantification in addition to point estimates. In this paper, we\ndiscuss the relationship between RBM parameter specification in the binary case\nand model properties such as degeneracy, instability and uninterpretability. We\nalso describe the associated difficulties that can arise with likelihood-based\ninference and further discuss the potential Bayes fitting of such (highly\nflexible) models, especially as Gibbs sampling (quasi-Bayes) methods are often\nadvocated for the RBM model structure.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 18:12:56 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 01:34:45 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 02:45:10 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kaplan", "Andee", ""], ["Nordman", "Daniel", ""], ["Vardeman", "Stephen", ""]]}, {"id": "1612.01197", "submitter": "Chen Liang", "authors": "Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, Ni Lao", "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with\n  Weak Supervision (Short Version)", "comments": "Published in NAMPI workshop at NIPS 2016. Short version of\n  arXiv:1611.00020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending the success of deep neural networks to natural language\nunderstanding and symbolic reasoning requires complex operations and external\nmemory. Recent neural program induction approaches have attempted to address\nthis problem, but are typically limited to differentiable memory, and\nconsequently cannot scale beyond small synthetic tasks. In this work, we\npropose the Manager-Programmer-Computer framework, which integrates neural\nnetworks with non-differentiable memory to support abstract, scalable and\nprecise operations through a friendly neural computer interface. Specifically,\nwe introduce a Neural Symbolic Machine, which contains a sequence-to-sequence\nneural \"programmer\", and a non-differentiable \"computer\" that is a Lisp\ninterpreter with code assist. To successfully apply REINFORCE for training, we\naugment it with approximate gold programs found by an iterative maximum\nlikelihood training process. NSM is able to learn a semantic parser from weak\nsupervision over a large knowledge base. It achieves new state-of-the-art\nperformance on WebQuestionsSP, a challenging semantic parsing dataset, with\nweak supervision. Compared to previous approaches, NSM is end-to-end, therefore\ndoes not rely on feature engineering or domain specific knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 22:29:32 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Liang", "Chen", ""], ["Berant", "Jonathan", ""], ["Le", "Quoc", ""], ["Forbus", "Kenneth D.", ""], ["Lao", "Ni", ""]]}, {"id": "1612.01200", "submitter": "David Kale", "authors": "Tom Quisel, David C. Kale, Luca Foschini", "title": "Intra-day Activity Better Predicts Chronic Conditions", "comments": "Presented at the NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate intra-day patterns of activity on a population of\n7,261 users of mobile health wearable devices and apps. We show that: (1) using\nintra-day step and sleep data recorded from passive trackers significantly\nimproves classification performance on self-reported chronic conditions related\nto mental health and nervous system disorders, (2) Convolutional Neural\nNetworks achieve top classification performance vs. baseline models when\ntrained directly on multivariate time series of activity data, and (3) jointly\npredicting all condition classes via multi-task learning can be leveraged to\nextract features that generalize across data sets and achieve the highest\nclassification performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 23:00:45 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Quisel", "Tom", ""], ["Kale", "David C.", ""], ["Foschini", "Luca", ""]]}, {"id": "1612.01205", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang and Alekh Agarwal and Miroslav Dudik", "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits", "comments": null, "journal-ref": "International Conference on Machine Learning (pp. 3589-3597)\n  (2017)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the off-policy evaluation problem---estimating the value of a target\npolicy using data collected by another policy---under the contextual bandit\nmodel. We consider the general (agnostic) setting without access to a\nconsistent model of rewards and establish a minimax lower bound on the mean\nsquared error (MSE). The bound is matched up to constants by the inverse\npropensity scoring (IPS) and doubly robust (DR) estimators. This highlights the\ndifficulty of the agnostic contextual setting, in contrast with multi-armed\nbandits and contextual bandits with access to a consistent reward model, where\nIPS is suboptimal. We then propose the SWITCH estimator, which can use an\nexisting reward model (not necessarily consistent) to achieve a better\nbias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and\ndemonstrate its benefits empirically on a diverse collection of data sets,\noften outperforming prior work by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 23:24:17 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 05:57:11 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Agarwal", "Alekh", ""], ["Dudik", "Miroslav", ""]]}, {"id": "1612.01213", "submitter": "Hyun Oh Song", "authors": "Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, Kevin Murphy", "title": "Deep Metric Learning via Facility Location", "comments": "Submission accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the representation and the similarity metric in an end-to-end\nfashion with deep networks have demonstrated outstanding results for clustering\nand retrieval. However, these recent approaches still suffer from the\nperformance degradation stemming from the local metric training procedure which\nis unaware of the global structure of the embedding space.\n  We propose a global metric learning scheme for optimizing the deep metric\nembedding with the learnable clustering function and the clustering metric\n(NMI) in a novel structured prediction framework.\n  Our experiments on CUB200-2011, Cars196, and Stanford online products\ndatasets show state of the art performance both on the clustering and retrieval\ntasks measured in the NMI and Recall@K evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 01:09:35 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 17:24:42 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Song", "Hyun Oh", ""], ["Jegelka", "Stefanie", ""], ["Rathod", "Vivek", ""], ["Murphy", "Kevin", ""]]}, {"id": "1612.01251", "submitter": "Pedro Tabacof", "authors": "Ramon Oliveira, Pedro Tabacof, Eduardo Valle", "title": "Known Unknowns: Uncertainty Quality in Bayesian Neural Networks", "comments": "Workshop on Bayesian Deep Learning, NIPS 2016, Barcelona, Spain;\n  EDIT: Changed analysis from Logit-AUC space to AUC (with changes to Figs. 2\n  and 3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the uncertainty quality in neural networks using anomaly\ndetection. We extract uncertainty measures (e.g. entropy) from the predictions\nof candidate models, use those measures as features for an anomaly detector,\nand gauge how well the detector differentiates known from unknown classes. We\nassign higher uncertainty quality to candidate models that lead to better\ndetectors. We also propose a novel method for sampling a variational\napproximation of a Bayesian neural network, called One-Sample Bayesian\nApproximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We\ncompare the following candidate neural network models: Maximum Likelihood,\nBayesian Dropout, OSBA, and --- for MNIST --- the standard variational\napproximation. We show that Bayesian Dropout and OSBA provide better\nuncertainty information than Maximum Likelihood, and are essentially equivalent\nto the standard variational approximation, but much faster.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 05:21:42 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 00:24:27 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Oliveira", "Ramon", ""], ["Tabacof", "Pedro", ""], ["Valle", "Eduardo", ""]]}, {"id": "1612.01253", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Zhaoyang Lv, Zsolt Kira", "title": "Deep Image Category Discovery using a Transferred Similarity Function", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically discovering image categories in unlabeled natural images is one\nof the important goals of unsupervised learning. However, the task is\nchallenging and even human beings define visual categories based on a large\namount of prior knowledge. In this paper, we similarly utilize prior knowledge\nto facilitate the discovery of image categories. We present a novel end-to-end\nnetwork to map unlabeled images to categories as a clustering network. We\npropose that this network can be learned with contrastive loss which is only\nbased on weak binary pair-wise constraints. Such binary constraints can be\nlearned from datasets in other domains as transferred similarity functions,\nwhich mimic a simple knowledge transfer. We first evaluate our experiments on\nthe MNIST dataset as a proof of concept, based on predicted similarities\ntrained on Omniglot, showing a 99\\% accuracy which significantly outperforms\nclustering based approaches. Then we evaluate the discovery performance on\nCifar-10, STL-10, and ImageNet, which achieves both state-of-the-art accuracy\nand shows it can be scalable to various large natural images.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 05:41:26 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Lv", "Zhaoyang", ""], ["Kira", "Zsolt", ""]]}, {"id": "1612.01254", "submitter": "Soheil Bahrampour", "authors": "Shengdong Zhang and Soheil Bahrampour and Naveen Ramakrishnan and\n  Mohak Shah", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of event classification with\nmulti-variate time series data consisting of heterogeneous (continuous and\ncategorical) variables. The complex temporal dependencies between the variables\ncombined with sparsity of the data makes the event classification problem\nparticularly challenging. Most state-of-art approaches address this either by\ndesigning hand-engineered features or breaking up the problem over homogeneous\nvariates. In this work, we propose and compare three representation learning\nalgorithms over symbolized sequences which enables classification of\nheterogeneous time-series data using a deep architecture. The proposed\nrepresentations are trained jointly along with the rest of the network\narchitecture in an end-to-end fashion that makes the learned features\ndiscriminative for the given task. Experiments on three real-world datasets\ndemonstrate the effectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 05:53:47 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhang", "Shengdong", ""], ["Bahrampour", "Soheil", ""], ["Ramakrishnan", "Naveen", ""], ["Shah", "Mohak", ""]]}, {"id": "1612.01277", "submitter": "Zhengyao Jiang", "authors": "Zhengyao Jiang, Jinjun Liang", "title": "Cryptocurrency Portfolio Management with Deep Reinforcement Learning", "comments": "accepted by Intelligent Systems Conference (IntelliSys) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio management is the decision-making process of allocating an amount\nof fund into different financial investment products. Cryptocurrencies are\nelectronic and decentralized alternatives to government-issued money, with\nBitcoin as the best-known example of a cryptocurrency. This paper presents a\nmodel-less convolutional neural network with historic prices of a set of\nfinancial assets as its input, outputting portfolio weights of the set. The\nnetwork is trained with 0.7 years' price data from a cryptocurrency exchange.\nThe training is done in a reinforcement manner, maximizing the accumulative\nreturn, which is regarded as the reward function of the network. Backtest\ntrading experiments with trading period of 30 minutes is conducted in the same\nmarket, achieving 10-fold returns in 1.8 months' periods. Some recently\npublished portfolio selection strategies are also used to perform the same\nback-tests, whose results are compared with the neural network. The network is\nnot limited to cryptocurrency, but can be applied to any other financial\nmarkets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 08:47:01 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 02:21:08 GMT"}, {"version": "v3", "created": "Thu, 22 Dec 2016 08:54:08 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 01:41:59 GMT"}, {"version": "v5", "created": "Thu, 11 May 2017 05:14:26 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Jiang", "Zhengyao", ""], ["Liang", "Jinjun", ""]]}, {"id": "1612.01294", "submitter": "Arnab Ghosh", "authors": "Arnab Ghosh and Viveka Kulharia and Vinay Namboodiri", "title": "Message Passing Multi-Agent GANs", "comments": "The first 2 authors contributed equally for this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communicating and sharing intelligence among agents is an important facet of\nachieving Artificial General Intelligence. As a first step towards this\nchallenge, we introduce a novel framework for image generation: Message Passing\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\nrecently been shown to be very effective for image generation and other tasks,\nthese networks have been limited to mostly single generator-discriminator\nnetworks. We show that we can obtain multi-agent GANs that communicate through\nmessage passing to achieve better image generation. The objectives of the\nindividual agents in this framework are two fold: a co-operation objective and\na competing objective. The co-operation objective ensures that the message\nsharing mechanism guides the other generator to generate better than itself\nwhile the competing objective encourages each generator to generate better than\nits counterpart. We analyze and visualize the messages that these GANs share\namong themselves in various scenarios. We quantitatively show that the message\nsharing formulation serves as a regularizer for the adversarial training.\nQualitatively, we show that the different generators capture different traits\nof the underlying data distribution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 10:10:13 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ghosh", "Arnab", ""], ["Kulharia", "Viveka", ""], ["Namboodiri", "Vinay", ""]]}, {"id": "1612.01316", "submitter": "Konstantinos Sechidis", "authors": "Konstantinos Sechidis, Emily Turner, Paul D. Metcalfe, James\n  Weatherall and Gavin Brown", "title": "Ranking Biomarkers Through Mutual Information", "comments": "Accepted at NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study information theoretic methods for ranking biomarkers. In clinical\ntrials there are two, closely related, types of biomarkers: predictive and\nprognostic, and disentangling them is a key challenge. Our first step is to\nphrase biomarker ranking in terms of optimizing an information theoretic\nquantity. This formalization of the problem will enable us to derive rankings\nof predictive/prognostic biomarkers, by estimating different, high dimensional,\nconditional mutual information terms. To estimate these terms, we suggest\nefficient low dimensional approximations, and we derive an empirical Bayes\nestimator, which is suitable for small or sparse datasets. Finally, we\nintroduce a new visualisation tool that captures the prognostic and the\npredictive strength of a set of biomarkers. We believe this representation will\nprove to be a powerful tool in biomarker discovery.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 11:44:32 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sechidis", "Konstantinos", ""], ["Turner", "Emily", ""], ["Metcalfe", "Paul D.", ""], ["Weatherall", "James", ""], ["Brown", "Gavin", ""]]}, {"id": "1612.01349", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Akhilesh Kumar Verma, Aurobinda Routray, William K.\n  Mohanty, Mamata Jenamani", "title": "A One class Classifier based Framework using SVDD : Application to an\n  Imbalanced Geological Dataset", "comments": "presented at IEEE Students Technology Symposium (TechSym), 28\n  February to 2 March 2014, IIT Kharagpur, India. 6 pages, 7 figures, 2tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of hydrocarbon reservoir requires classification of petrophysical\nproperties from available dataset. However, characterization of reservoir\nattributes is difficult due to the nonlinear and heterogeneous nature of the\nsubsurface physical properties. In this context, present study proposes a\ngeneralized one class classification framework based on Support Vector Data\nDescription (SVDD) to classify a reservoir characteristic water saturation into\ntwo classes (Class high and Class low) from four logs namely gamma ray, neutron\nporosity, bulk density, and P sonic using an imbalanced dataset. A comparison\nis carried out among proposed framework and different supervised classification\nalgorithms in terms of g metric means and execution time. Experimental results\nshow that proposed framework has outperformed other classifiers in terms of\nthese performance evaluators. It is envisaged that the classification analysis\nperformed in this study will be useful in further reservoir modeling.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:54:23 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chaki", "Soumi", ""], ["Verma", "Akhilesh Kumar", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.01356", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Hedvig Kjellstrom, Bo C. Bertilson", "title": "Diagnostic Prediction Using Discomfort Drawings", "comments": "NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the possibility to apply machine learning to make\ndiagnostic predictions using discomfort drawings. A discomfort drawing is an\nintuitive way for patients to express discomfort and pain related symptoms.\nThese drawings have proven to be an effective method to collect patient data\nand make diagnostic decisions in real-life practice. A dataset from real-world\npatient cases is collected for which medical experts provide diagnostic labels.\nNext, we extend a factorized multimodal topic model, Inter-Battery Topic Model\n(IBTM), to train a system that can make diagnostic predictions given an unseen\ndiscomfort drawing. Experimental results show reasonable predictions of\ndiagnostic labels given an unseen discomfort drawing. The positive result\nindicates a significant potential of machine learning to be used for parts of\nthe pain diagnostic process and to be a decision support system for physicians\nand other health care personnel.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 14:11:20 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhang", "Cheng", ""], ["Kjellstrom", "Hedvig", ""], ["Bertilson", "Bo C.", ""]]}, {"id": "1612.01367", "submitter": "Mohammadreza Mohaghegh Neyshabouri", "authors": "Mohammadreza Mohaghegh Neyshabouri, Kaan Gokcesu, Huseyin Ozkan and\n  Suleyman S. Kozat", "title": "An Asymptotically Optimal Contextual Bandit Algorithm Using Hierarchical\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose online algorithms for sequential learning in the contextual\nmulti-armed bandit setting. Our approach is to partition the context space and\nthen optimally combine all of the possible mappings between the partition\nregions and the set of bandit arms in a data driven manner. We show that in our\napproach, the best mapping is able to approximate the best arm selection policy\nto any desired degree under mild Lipschitz conditions. Therefore, we design our\nalgorithms based on the optimal adaptive combination and asymptotically achieve\nthe performance of the best mapping as well as the best arm selection policy.\nThis optimality is also guaranteed to hold even in adversarial environments\nsince we do not rely on any statistical assumptions regarding the contexts or\nthe loss of the bandit arms. Moreover, we design efficient implementations for\nour algorithms in various hierarchical partitioning structures such as\nlexicographical or arbitrary position splitting and binary trees (and several\nother partitioning examples). For instance, in the case of binary tree\npartitioning, the computational complexity is only log-linear in the number of\nregions in the finest partition. In conclusion, we provide significant\nperformance improvements by introducing upper bounds (w.r.t. the best arm\nselection policy) that are mathematically proven to vanish in the average loss\nper round sense at a faster rate compared to the state-of-the-art. Our\nexperimental work extensively covers various scenarios ranging from bandit\nsettings to multi-class classification with real and synthetic data. In these\nexperiments, we show that our algorithms are highly superior over the\nstate-of-the-art techniques while maintaining the introduced mathematical\nguarantees and a computationally decent scalability.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 14:21:33 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 20:38:51 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Neyshabouri", "Mohammadreza Mohaghegh", ""], ["Gokcesu", "Kaan", ""], ["Ozkan", "Huseyin", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1612.01397", "submitter": "Dmitrij Schlesinger", "authors": "Dmitrij Schlesinger and Carsten Rother", "title": "Implicit Modeling -- A Generalization of Discriminative and Generative\n  Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new modeling approach that is a generalization of generative and\ndiscriminative models. The core idea is to use an implicit parameterization of\na joint probability distribution by specifying only the conditional\ndistributions. The proposed scheme combines the advantages of both worlds -- it\ncan use powerful complex discriminative models as its parts, having at the same\ntime better generalization capabilities. We thoroughly evaluate the proposed\nmethod for a simple classification task with artificial data and illustrate its\nadvantages for real-word scenarios on a semantic image segmentation problem.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 15:37:27 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Schlesinger", "Dmitrij", ""], ["Rother", "Carsten", ""]]}, {"id": "1612.01401", "submitter": "Qinglong Wang", "authors": "Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G. Ororbia II,\n  Xinyu Xing, Xue Liu, C. Lee Giles", "title": "Learning Adversary-Resistant Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have proven to be quite effective in a vast array\nof machine learning tasks, with recent examples in cyber security and\nautonomous vehicles. Despite the superior performance of DNNs in these\napplications, it has been recently shown that these models are susceptible to a\nparticular type of attack that exploits a fundamental flaw in their design.\nThis attack consists of generating particular synthetic examples referred to as\nadversarial samples. These samples are constructed by slightly manipulating\nreal data-points in order to \"fool\" the original DNN model, forcing it to\nmis-classify previously correctly classified samples with high confidence.\nAddressing this flaw in the model is essential if DNNs are to be used in\ncritical applications such as those in cyber security.\n  Previous work has provided various learning algorithms to enhance the\nrobustness of DNN models, and they all fall into the tactic of \"security\nthrough obscurity\". This means security can be guaranteed only if one can\nobscure the learning algorithms from adversaries. Once the learning technique\nis disclosed, DNNs protected by these defense mechanisms are still susceptible\nto adversarial samples. In this work, we investigate this issue shared across\nprevious research work and propose a generic approach to escalate a DNN's\nresistance to adversarial samples. More specifically, our approach integrates a\ndata transformation module with a DNN, making it robust even if we reveal the\nunderlying learning algorithm. To demonstrate the generality of our proposed\napproach and its potential for handling cyber security applications, we\nevaluate our method and several other existing solutions on datasets publicly\navailable. Our results indicate that our approach typically provides superior\nclassification performance and resistance in comparison with state-of-art\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 15:46:21 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 01:40:36 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Wang", "Qinglong", ""], ["Guo", "Wenbo", ""], ["Zhang", "Kaixuan", ""], ["Ororbia", "Alexander G.", "II"], ["Xing", "Xinyu", ""], ["Liu", "Xue", ""], ["Giles", "C. Lee", ""]]}, {"id": "1612.01414", "submitter": "Alexander Jung", "authors": "Alexander Jung, Alfred O. Hero III, Alexandru Mara, and Saeed Jahromi", "title": "Semi-Supervised Learning via Sparse Label Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel method for semi-supervised learning from partially\nlabeled massive network-structured datasets, i.e., big data over networks. We\nmodel the underlying hypothesis, which relates data points to labels, as a\ngraph signal, defined over some graph (network) structure intrinsic to the\ndataset. Following the key principle of supervised learning, i.e., similar\ninputs yield similar outputs, we require the graph signals induced by labels to\nhave small total variation. Accordingly, we formulate the problem of learning\nthe labels of data points as a non-smooth convex optimization problem which\namounts to balancing between the empirical loss, i.e., the discrepancy with\nsome partially available label information, and the smoothness quantified by\nthe total variation of the learned graph signal. We solve this optimization\nproblem by appealing to a recently proposed preconditioned variant of the\npopular primal-dual method by Pock and Chambolle, which results in a sparse\nlabel propagation algorithm. This learning algorithm allows for a highly\nscalable implementation as message passing over the underlying data graph. By\napplying concepts of compressed sensing to the learning problem, we are also\nable to provide a transparent sufficient condition on the underlying network\nstructure such that accurate learning of the labels is possible. We also\npresent an implementation of the message passing formulation allows for a\nhighly scalable implementation in big data frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 16:04:38 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 15:41:31 GMT"}, {"version": "v3", "created": "Wed, 10 May 2017 16:57:05 GMT"}, {"version": "v4", "created": "Mon, 15 May 2017 07:53:13 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Jung", "Alexander", ""], ["Hero", "Alfred O.", "III"], ["Mara", "Alexandru", ""], ["Jahromi", "Saeed", ""]]}, {"id": "1612.01425", "submitter": "Bin Gu", "authors": "Bin Gu and Zhouyuan Huo and Heng Huang", "title": "Zeroth-order Asynchronous Doubly Stochastic Algorithm with Variance\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zeroth-order (derivative-free) optimization attracts a lot of attention in\nmachine learning, because explicit gradient calculations may be computationally\nexpensive or infeasible. To handle large scale problems both in volume and\ndimension, recently asynchronous doubly stochastic zeroth-order algorithms were\nproposed. The convergence rate of existing asynchronous doubly stochastic\nzeroth order algorithms is $O(\\frac{1}{\\sqrt{T}})$ (also for the sequential\nstochastic zeroth-order optimization algorithms). In this paper, we focus on\nthe finite sums of smooth but not necessarily convex functions, and propose an\nasynchronous doubly stochastic zeroth-order optimization algorithm using the\naccelerated technology of variance reduction (AsyDSZOVR). Rigorous theoretical\nanalysis show that the convergence rate can be improved from\n$O(\\frac{1}{\\sqrt{T}})$ the best result of existing algorithms to\n$O(\\frac{1}{T})$. Also our theoretical results is an improvement to the ones of\nthe sequential stochastic zeroth-order optimization algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 16:40:00 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Gu", "Bin", ""], ["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "1612.01428", "submitter": "Seyed Mohammad Taheri", "authors": "Seyed Mohammad Taheri, Hamidreza Mahyar, Mohammad Firouzi, Elahe\n  Ghalebi K., Radu Grosu, Ali Movaghar", "title": "Extracting Implicit Social Relation for Social Recommendation Techniques\n  in User Rating Prediction", "comments": null, "journal-ref": null, "doi": "10.1145/3041021.3051153", "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation plays an increasingly important role in our daily lives.\nRecommender systems automatically suggest items to users that might be\ninteresting for them. Recent studies illustrate that incorporating social trust\nin Matrix Factorization methods demonstrably improves accuracy of rating\nprediction. Such approaches mainly use the trust scores explicitly expressed by\nusers. However, it is often challenging to have users provide explicit trust\nscores of each other. There exist quite a few works, which propose Trust\nMetrics to compute and predict trust scores between users based on their\ninteractions. In this paper, first we present how social relation can be\nextracted from users' ratings to items by describing Hellinger distance between\nusers in recommender systems. Then, we propose to incorporate the predicted\ntrust scores into social matrix factorization models. By analyzing social\nrelation extraction from three well-known real-world datasets, which both:\ntrust and recommendation data available, we conclude that using the implicit\nsocial relation in social recommendation techniques has almost the same\nperformance compared to the actual trust scores explicitly expressed by users.\nHence, we build our method, called Hell-TrustSVD, on top of the\nstate-of-the-art social recommendation technique to incorporate both the\nextracted implicit social relations and ratings given by users on the\nprediction of items for an active user. To the best of our knowledge, this is\nthe first work to extend TrustSVD with extracted social trust information. The\nexperimental results support the idea of employing implicit trust into matrix\nfactorization whenever explicit trust is not available, can perform much better\nthan the state-of-the-art approaches in user rating prediction.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 16:47:02 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 15:23:15 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Taheri", "Seyed Mohammad", ""], ["Mahyar", "Hamidreza", ""], ["Firouzi", "Mohammad", ""], ["K.", "Elahe Ghalebi", ""], ["Grosu", "Radu", ""], ["Movaghar", "Ali", ""]]}, {"id": "1612.01437", "submitter": "Celestine D\\\"unner", "authors": "Celestine D\\\"unner, Thomas Parnell, Kubilay Atasu, Manolis Sifalakis,\n  Haralampos Pozidis", "title": "Understanding and Optimizing the Performance of Distributed Machine\n  Learning Applications on Apache Spark", "comments": "To appear in the 2017 IEEE International Conference on Big Data (Big\n  Data 2017), December 11-14, 2017, Boston, MA, USA", "journal-ref": null, "doi": "10.1109/BigData.2017.8257942", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the performance limits of Apache Spark for machine\nlearning applications. We begin by analyzing the characteristics of a\nstate-of-the-art distributed machine learning algorithm implemented in Spark\nand compare it to an equivalent reference implementation using the high\nperformance computing framework MPI. We identify critical bottlenecks of the\nSpark framework and carefully study their implications on the performance of\nthe algorithm. In order to improve Spark performance we then propose a number\nof practical techniques to alleviate some of its overheads. However, optimizing\ncomputational efficiency and framework related overheads is not the only key to\nperformance -- we demonstrate that in order to get the best performance out of\nany implementation it is necessary to carefully tune the algorithm to the\nrespective trade-off between computation time and communication latency. The\noptimal trade-off depends on both the properties of the distributed algorithm\nas well as infrastructure and framework-related characteristics. Finally, we\napply these technical and algorithmic optimizations to three different\ndistributed linear machine learning algorithms that have been implemented in\nSpark. We present results using five large datasets and demonstrate that by\nusing the proposed optimizations, we can achieve a reduction in the performance\ndifference between Spark and MPI from 20x to 2x.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 17:16:05 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 03:45:52 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["D\u00fcnner", "Celestine", ""], ["Parnell", "Thomas", ""], ["Atasu", "Kubilay", ""], ["Sifalakis", "Manolis", ""], ["Pozidis", "Haralampos", ""]]}, {"id": "1612.01458", "submitter": "Alessandro Maria Rizzi", "authors": "Alessandro Maria Rizzi", "title": "Support vector regression model for BigData systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays Big Data are becoming more and more important. Many sectors of our\neconomy are now guided by data-driven decision processes. Big Data and business\nintelligence applications are facilitated by the MapReduce programming model\nwhile, at infrastructural layer, cloud computing provides flexible and cost\neffective solutions for allocating on demand large clusters. In such systems,\ncapacity allocation, which is the ability to optimally size minimal resources\nfor achieve a certain level of performance, is a key challenge to enhance\nperformance for MapReduce jobs and minimize cloud resource costs. In order to\ndo so, one of the biggest challenge is to build an accurate performance model\nto estimate job execution time of MapReduce systems. Previous works applied\nsimulation based models for modeling such systems. Although this approach can\naccurately describe the behavior of Big Data clusters, it is too\ncomputationally expensive and does not scale to large system. We try to\novercome these issues by applying machine learning techniques. More precisely\nwe focus on Support Vector Regression (SVR) which is intrinsically more robust\nw.r.t other techniques, like, e.g., neural networks, and less sensitive to\noutliers in the training set. To better investigate these benefits, we compare\nSVR to linear regression.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 18:14:12 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Rizzi", "Alessandro Maria", ""]]}, {"id": "1612.01474", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Alexander Pritzel and Charles Blundell", "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep\n  Ensembles", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (NNs) are powerful black box predictors that have\nrecently achieved impressive performance on a wide spectrum of tasks.\nQuantifying predictive uncertainty in NNs is a challenging and yet unsolved\nproblem. Bayesian NNs, which learn a distribution over weights, are currently\nthe state-of-the-art for estimating predictive uncertainty; however these\nrequire significant modifications to the training procedure and are\ncomputationally expensive compared to standard (non-Bayesian) NNs. We propose\nan alternative to Bayesian NNs that is simple to implement, readily\nparallelizable, requires very little hyperparameter tuning, and yields high\nquality predictive uncertainty estimates. Through a series of experiments on\nclassification and regression benchmarks, we demonstrate that our method\nproduces well-calibrated uncertainty estimates which are as good or better than\napproximate Bayesian NNs. To assess robustness to dataset shift, we evaluate\nthe predictive uncertainty on test examples from known and unknown\ndistributions, and show that our method is able to express higher uncertainty\non out-of-distribution examples. We demonstrate the scalability of our method\nby evaluating predictive uncertainty estimates on ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 18:54:43 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 06:03:39 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 01:33:43 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Pritzel", "Alexander", ""], ["Blundell", "Charles", ""]]}, {"id": "1612.01480", "submitter": "{\\L}ukasz Struski", "authors": "{\\L}ukasz Struski, Marek \\'Smieja, Jacek Tabor", "title": "Generalized RBF kernel for incomplete data", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct $\\bf genRBF$ kernel, which generalizes the classical Gaussian\nRBF kernel to the case of incomplete data. We model the uncertainty contained\nin missing attributes making use of data distribution and associate every point\nwith a conditional probability density function. This allows to embed\nincomplete data into the function space and to define a kernel between two\nmissing data points based on scalar product in $L_2$. Experiments show that\nintroduced kernel applied to SVM classifier gives better results than other\nstate-of-the-art methods, especially in the case when large number of features\nis missing. Moreover, it is easy to implement and can be used together with any\nkernel approaches with no additional modifications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 19:07:06 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 07:50:48 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Struski", "\u0141ukasz", ""], ["\u015amieja", "Marek", ""], ["Tabor", "Jacek", ""]]}, {"id": "1612.01481", "submitter": "Ehtsham Elahi", "authors": "Ehtsham Elahi", "title": "A Nonparametric Latent Factor Model For Location-Aware Video\n  Recommendations", "comments": "NIPS 2016 Workshop on Practical Bayesian Nonparametrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in learning customers' video preferences from their\nhistoric viewing patterns and geographical location. We consider a Bayesian\nlatent factor modeling approach for this task. In order to tune the complexity\nof the model to best represent the data, we make use of Bayesian nonparameteric\ntechniques. We describe an inference technique that can scale to large\nreal-world data sets. Finally we show results obtained by applying the model to\na large internal Netflix data set, that illustrates that the model was able to\ncapture interesting relationships between viewing patterns and geographical\nlocation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 19:12:00 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Elahi", "Ehtsham", ""]]}, {"id": "1612.01543", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee", "title": "Towards the Limit of Network Quantization", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network quantization is one of network compression techniques to reduce the\nredundancy of deep neural networks. It reduces the number of distinct network\nparameter values by quantization in order to save the storage for them. In this\npaper, we design network quantization schemes that minimize the performance\nloss due to quantization given a compression ratio constraint. We analyze the\nquantitative relation of quantization errors to the neural network loss\nfunction and identify that the Hessian-weighted distortion measure is locally\nthe right objective function for the optimization of network quantization. As a\nresult, Hessian-weighted k-means clustering is proposed for clustering network\nparameters to quantize. When optimal variable-length binary codes, e.g.,\nHuffman codes, are employed for further compression, we derive that the network\nquantization problem can be related to the entropy-constrained scalar\nquantization (ECSQ) problem in information theory and consequently propose two\nsolutions of ECSQ for network quantization, i.e., uniform quantization and an\niterative solution similar to Lloyd's algorithm. Finally, using the simple\nuniform quantization followed by Huffman coding, we show from our experiments\nthat the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet,\n32-layer ResNet and AlexNet, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 21:04:17 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 19:44:32 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1612.01589", "submitter": "Konrad \\.Zo{\\l}na", "authors": "Konrad Zolna", "title": "Improving the Performance of Neural Networks in Regression Tasks Using\n  Drawering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method presented extends a given regression neural network to make its\nperformance improve. The modification affects the learning procedure only,\nhence the extension may be easily omitted during evaluation without any change\nin prediction. It means that the modified model may be evaluated as quickly as\nthe original one but tends to perform better.\n  This improvement is possible because the modification gives better expressive\npower, provides better behaved gradients and works as a regularization. The\nknowledge gained by the temporarily extended neural network is contained in the\nparameters shared with the original neural network.\n  The only cost is an increase in learning time.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 23:28:54 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Zolna", "Konrad", ""]]}, {"id": "1612.01597", "submitter": "Morteza Ashraphijuo", "authors": "Morteza Ashraphijuo and Vaneet Aggarwal and Xiaodong Wang", "title": "Deterministic and Probabilistic Conditions for Finite Completability of\n  Low-Tucker-Rank Tensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the fundamental conditions on the sampling pattern, i.e.,\nlocations of the sampled entries, for finite completability of a low-rank\ntensor given some components of its Tucker rank. In order to find the\ndeterministic necessary and sufficient conditions, we propose an algebraic\ngeometric analysis on the Tucker manifold, which allows us to incorporate\nmultiple rank components in the proposed analysis in contrast with the\nconventional geometric approaches on the Grassmannian manifold. This analysis\ncharacterizes the algebraic independence of a set of polynomials defined based\non the sampling pattern, which is closely related to finite completion.\nProbabilistic conditions are then studied and a lower bound on the sampling\nprobability is given, which guarantees that the proposed deterministic\nconditions on the sampling patterns for finite completability hold with high\nprobability. Furthermore, using the proposed geometric approach for finite\ncompletability, we propose a sufficient condition on the sampling pattern that\nensures there exists exactly one completion for the sampled tensor.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 00:08:09 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 14:31:39 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 20:26:52 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Ashraphijuo", "Morteza", ""], ["Aggarwal", "Vaneet", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1612.01600", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c, Alex Olshevsky and C\\'esar A. Uribe", "title": "Distributed Gaussian Learning over Time-varying Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed (non-Bayesian) learning algorithm for the problem of\nparameter estimation with Gaussian noise. The algorithm is expressed as\nexplicit updates on the parameters of the Gaussian beliefs (i.e. means and\nprecision). We show a convergence rate of $O(1/k)$ with the constant term\ndepending on the number of agents and the topology of the network. Moreover, we\nshow almost sure convergence to the optimal solution of the estimation problem\nfor the general case of time-varying directed graphs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 00:15:33 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 03:39:59 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1612.01663", "submitter": "Yi Xu", "authors": "Yi Xu, Haiqin Yang, Lijun Zhang, Tianbao Yang", "title": "Efficient Non-oblivious Randomized Reduction for Risk Minimization with\n  Improved Excess Risk Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address learning problems for high dimensional data.\nPreviously, oblivious random projection based approaches that project high\ndimensional features onto a random subspace have been used in practice for\ntackling high-dimensionality challenge in machine learning. Recently, various\nnon-oblivious randomized reduction methods have been developed and deployed for\nsolving many numerical problems such as matrix product approximation, low-rank\nmatrix approximation, etc. However, they are less explored for the machine\nlearning tasks, e.g., classification. More seriously, the theoretical analysis\nof excess risk bounds for risk minimization, an important measure of\ngeneralization performance, has not been established for non-oblivious\nrandomized reduction methods. It therefore remains an open problem what is the\nbenefit of using them over previous oblivious random projection based\napproaches. To tackle these challenges, we propose an algorithmic framework for\nemploying non-oblivious randomized reduction method for general empirical risk\nminimizing in machine learning tasks, where the original high-dimensional\nfeatures are projected onto a random subspace that is derived from the data\nwith a small matrix approximation error. We then derive the first excess risk\nbound for the proposed non-oblivious randomized reduction approach without\nrequiring strong assumptions on the training data. The established excess risk\nbound exhibits that the proposed approach provides much better generalization\nperformance and it also sheds more insights about different randomized\nreduction approaches. Finally, we conduct extensive experiments on both\nsynthetic and real-world benchmark datasets, whose dimension scales to\n$O(10^7)$, to demonstrate the efficacy of our proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 04:58:45 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Xu", "Yi", ""], ["Yang", "Haiqin", ""], ["Zhang", "Lijun", ""], ["Yang", "Tianbao", ""]]}, {"id": "1612.01717", "submitter": "Haiping Huang", "authors": "Haiping Huang", "title": "Statistical mechanics of unsupervised feature learning in a restricted\n  Boltzmann machine with binary synapses", "comments": "24 pages, 9 figures, results added", "journal-ref": "J. Stat. Mech. (2017) 053302", "doi": "10.1088/1742-5468/aa6ddc", "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Revealing hidden features in unlabeled data is called unsupervised feature\nlearning, which plays an important role in pretraining a deep neural network.\nHere we provide a statistical mechanics analysis of the unsupervised learning\nin a restricted Boltzmann machine with binary synapses. A message passing\nequation to infer the hidden feature is derived, and furthermore, variants of\nthis equation are analyzed. A statistical analysis by replica theory describes\nthe thermodynamic properties of the model. Our analysis confirms an entropy\ncrisis preceding the non-convergence of the message passing equation,\nsuggesting a discontinuous phase transition as a key characteristic of the\nrestricted Boltzmann machine. Continuous phase transition is also confirmed\ndepending on the embedded feature strength in the data. The mean-field result\nunder the replica symmetric assumption agrees with that obtained by running\nmessage passing algorithms on single instances of finite sizes. Interestingly,\nin an approximate Hopfield model, the entropy crisis is absent, and a\ncontinuous phase transition is observed instead. We also develop an iterative\nequation to infer the hyper-parameter (temperature) hidden in the data, which\nin physics corresponds to iteratively imposing Nishimori condition. Our study\nprovides insights towards understanding the thermodynamic properties of the\nrestricted Boltzmann machine learning, and moreover important theoretical basis\nto build simplified deep networks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 09:17:14 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 04:47:07 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Huang", "Haiping", ""]]}, {"id": "1612.01746", "submitter": "Peter Karkus", "authors": "Peter Karkus, Andras Kupcsik, David Hsu, Wee Sun Lee", "title": "Factored Contextual Policy Search with Bayesian Optimization", "comments": "BayesOpt 2016, NeurIPS Workshop. A full paper extension is available\n  at arXiv:1904.11761", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarce data is a major challenge to scaling robot learning to truly complex\ntasks, as we need to generalize locally learned policies over different\n\"contexts\". Bayesian optimization approaches to contextual policy search (CPS)\noffer data-efficient policy learning that generalize over a context space. We\npropose to improve data-efficiency by factoring typically considered contexts\ninto two components: target-type contexts that correspond to a desired outcome\nof the learned behavior, e.g. target position for throwing a ball; and\nenvironment type contexts that correspond to some state of the environment,\ne.g. initial ball position or wind speed. Our key observation is that\nexperience can be directly generalized over target-type contexts. Based on that\nwe introduce Factored Contextual Policy Search with Bayesian Optimization for\nboth passive and active learning settings. Preliminary results show faster\npolicy generalization on a simulated toy problem. A full paper extension is\navailable at arXiv:1904.11761\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 10:51:51 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 04:08:29 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Karkus", "Peter", ""], ["Kupcsik", "Andras", ""], ["Hsu", "David", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1612.01756", "submitter": "Francesco Cricri", "authors": "Francesco Cricri, Xingyang Ni, Mikko Honkala, Emre Aksu, Moncef\n  Gabbouj", "title": "Video Ladder Networks", "comments": "This version extends the paper accepted at the NIPS 2016 workshop on\n  ML for Spatiotemporal Forecasting, with more details and more experimental\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Video Ladder Network (VLN) for efficiently generating future\nvideo frames. VLN is a neural encoder-decoder model augmented at all layers by\nboth recurrent and feedforward lateral connections. At each layer, these\nconnections form a lateral recurrent residual block, where the feedforward\nconnection represents a skip connection and the recurrent connection represents\nthe residual. Thanks to the recurrent connections, the decoder can exploit\ntemporal summaries generated from all layers of the encoder. This way, the top\nlayer is relieved from the pressure of modeling lower-level spatial and\ntemporal details. Furthermore, we extend the basic version of VLN to\nincorporate ResNet-style residual blocks in the encoder and decoder, which help\nimproving the prediction results. VLN is trained in self-supervised regime on\nthe Moving MNIST dataset, achieving competitive results while having very\nsimple structure and providing fast inference.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 11:15:28 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 11:35:22 GMT"}, {"version": "v3", "created": "Fri, 30 Dec 2016 09:01:02 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Cricri", "Francesco", ""], ["Ni", "Xingyang", ""], ["Honkala", "Mikko", ""], ["Aksu", "Emre", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1612.01812", "submitter": "Dang Nguyen", "authors": "Dang Nguyen, Wei Luo, Dinh Phung, Svetha Venkatesh", "title": "Control Matching via Discharge Code Sequences", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the patient similarity matching problem over a\ncancer cohort of more than 220,000 patients. Our approach first leverages on\nWord2Vec framework to embed ICD codes into vector-valued representation. We\nthen propose a sequential algorithm for case-control matching on this\nrepresentation space of diagnosis codes. The novel practice of applying the\nsequential matching on the vector representation lifted the matching accuracy\nmeasured through multiple clinical outcomes. We reported the results on a\nlarge-scale dataset to demonstrate the effectiveness of our method. For such a\nlarge dataset where most clinical information has been codified, the new method\nis particularly relevant.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 04:21:55 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Nguyen", "Dang", ""], ["Luo", "Wei", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1612.01859", "submitter": "Vianney Perchet", "authors": "R\\'emy Degenne, Vianney Perchet", "title": "Combinatorial semi-bandit with known covariance", "comments": "in NIPS 2016 (Conference on Neural Information Processing Systems),\n  Dec 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combinatorial stochastic semi-bandit problem is an extension of the\nclassical multi-armed bandit problem in which an algorithm pulls more than one\narm at each stage and the rewards of all pulled arms are revealed. One\ndifference with the single arm variant is that the dependency structure of the\narms is crucial. Previous works on this setting either used a worst-case\napproach or imposed independence of the arms. We introduce a way to quantify\nthe dependency structure of the problem and design an algorithm that adapts to\nit. The algorithm is based on linear regression and the analysis develops\ntechniques from the linear bandit literature. By comparing its performance to a\nnew lower bound, we prove that it is optimal, up to a poly-logarithmic factor\nin the number of pulled arms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 15:28:22 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Degenne", "R\u00e9my", ""], ["Perchet", "Vianney", ""]]}, {"id": "1612.01884", "submitter": "Entao Liu", "authors": "Entao Liu, Lijun Zhu, Anupama Govinda Raj, James H. McClellan,\n  Abdullatif Al-Shuhail, SanLinn I. Kaka, Naveed Iqbal", "title": "Microseismic events enhancement and detection in sensor arrays using\n  autocorrelation based filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passive microseismic data are commonly buried in noise, which presents a\nsignificant challenge for signal detection and recovery. For recordings from a\nsurface sensor array where each trace contains a time-delayed arrival from the\nevent, we propose an autocorrelation-based stacking method that designs a\ndenoising filter from all the traces, as well as a multi-channel detection\nscheme. This approach circumvents the issue of time aligning the traces prior\nto stacking because every trace's autocorrelation is centered at zero in the\nlag domain. The effect of white noise is concentrated near zero lag, so the\nfilter design requires a predictable adjustment of the zero-lag value.\nTruncation of the autocorrelation is employed to smooth the impulse response of\nthe denoising filter. In order to extend the applicability of the algorithm, we\nalso propose a noise prewhitening scheme that addresses cases with colored\nnoise. The simplicity and robustness of this method are validated with\nsynthetic and real seismic traces.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 15:59:25 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Liu", "Entao", ""], ["Zhu", "Lijun", ""], ["Raj", "Anupama Govinda", ""], ["McClellan", "James H.", ""], ["Al-Shuhail", "Abdullatif", ""], ["Kaka", "SanLinn I.", ""], ["Iqbal", "Naveed", ""]]}, {"id": "1612.01928", "submitter": "Dmitriy Serdyuk", "authors": "Dmitriy Serdyuk, Kartik Audhkhasi, Phil\\'emon Brakel, Bhuvana\n  Ramabhadran, Samuel Thomas, Yoshua Bengio", "title": "Invariant Representations for Noisy Speech Recognition", "comments": "5 pages, 1 figure, 1 table, NIPS workshop on end-to-end speech\n  recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern automatic speech recognition (ASR) systems need to be robust under\nacoustic variability arising from environmental, speaker, channel, and\nrecording conditions. Ensuring such robustness to variability is a challenge in\nmodern day neural network-based ASR systems, especially when all types of\nvariability are not seen during training. We attempt to address this problem by\nencouraging the neural network acoustic model to learn invariant feature\nrepresentations. We use ideas from recent research on image generation using\nGenerative Adversarial Networks and domain adaptation ideas extending\nadversarial gradient-based training. A recent work from Ganin et al. proposes\nto use adversarial training for image domain adaptation by using an\nintermediate representation from the main target classification network to\ndeteriorate the domain classifier performance through a separate neural\nnetwork. Our work focuses on investigating neural architectures which produce\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\nshow that our method generalizes better than the standard multi-condition\ntraining especially when only a few noise categories are seen during training.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 22:20:51 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Serdyuk", "Dmitriy", ""], ["Audhkhasi", "Kartik", ""], ["Brakel", "Phil\u00e9mon", ""], ["Ramabhadran", "Bhuvana", ""], ["Thomas", "Samuel", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1612.01936", "submitter": "Tan Nguyen", "authors": "Ankit B. Patel, Tan Nguyen, Richard G. Baraniuk", "title": "A Probabilistic Framework for Deep Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1504.00641", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a probabilistic framework for deep learning based on the Deep\nRendering Mixture Model (DRMM), a new generative probabilistic model that\nexplicitly capture variations in data due to latent task nuisance variables. We\ndemonstrate that max-sum inference in the DRMM yields an algorithm that exactly\nreproduces the operations in deep convolutional neural networks (DCNs),\nproviding a first principles derivation. Our framework provides new insights\ninto the successes and shortcomings of DCNs as well as a principled route to\ntheir improvement. DRMM training via the Expectation-Maximization (EM)\nalgorithm is a powerful alternative to DCN back-propagation, and initial\ntraining results are promising. Classification based on the DRMM and other\nvariants outperforms DCNs in supervised digit classification, training 2-3x\nfaster while achieving similar accuracy. Moreover, the DRMM is applicable to\nsemi-supervised and unsupervised learning tasks, achieving results that are\nstate-of-the-art in several categories on the MNIST benchmark and comparable to\nstate of the art on the CIFAR10 benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:15:40 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Patel", "Ankit B.", ""], ["Nguyen", "Tan", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1612.01942", "submitter": "Tan Nguyen", "authors": "Tan Nguyen, Wanjia Liu, Ethan Perez, Richard G. Baraniuk, Ankit B.\n  Patel", "title": "Semi-Supervised Learning with the Deep Rendering Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning algorithms reduce the high cost of acquiring labeled\ntraining data by using both labeled and unlabeled data during learning. Deep\nConvolutional Networks (DCNs) have achieved great success in supervised tasks\nand as such have been widely employed in the semi-supervised learning. In this\npaper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a\nprobabilistic generative model that models latent nuisance variation, and whose\ninference algorithm yields DCNs. We develop an EM algorithm for the DRMM to\nlearn from both labeled and unlabeled data. Guided by the theory of the DRMM,\nwe introduce a novel non-negativity constraint and a variational inference\nterm. We report state-of-the-art performance on MNIST and SVHN and competitive\nresults on CIFAR10. We also probe deeper into how a DRMM trained in a\nsemi-supervised setting represents latent nuisance variation using\nsynthetically rendered images. Taken together, our work provides a unified\nframework for supervised, unsupervised, and semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:32:53 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Nguyen", "Tan", ""], ["Liu", "Wanjia", ""], ["Perez", "Ethan", ""], ["Baraniuk", "Richard G.", ""], ["Patel", "Ankit B.", ""]]}, {"id": "1612.01943", "submitter": "Yuhao Zhang", "authors": "Yuhao Zhang, Sandeep Ayyar, Long-Huei Chen, Ethan J. Li", "title": "Segmental Convolutional Neural Networks for Detection of Cardiac\n  Abnormality With Noisy Heart Sound Recordings", "comments": "This work was finished in May 2016, and remains unpublished until\n  December 2016 due to a request from the data provider", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart diseases constitute a global health burden, and the problem is\nexacerbated by the error-prone nature of listening to and interpreting heart\nsounds. This motivates the development of automated classification to screen\nfor abnormal heart sounds. Existing machine learning-based systems achieve\naccurate classification of heart sound recordings but rely on expert features\nthat have not been thoroughly evaluated on noisy recordings. Here we propose a\nsegmental convolutional neural network architecture that achieves automatic\nfeature learning from noisy heart sound recordings. Our experiments show that\nour best model, trained on noisy recording segments acquired with an existing\nhidden semi-markov model-based approach, attains a classification accuracy of\n87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6%\naccuracy of the state-of-the-art statistical classifier trained and evaluated\non the same dataset. Our results indicate the potential of using neural\nnetwork-based methods to increase the accuracy of automated classification of\nheart sound recordings for improved screening of heart diseases.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:37:30 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Zhang", "Yuhao", ""], ["Ayyar", "Sandeep", ""], ["Chen", "Long-Huei", ""], ["Li", "Ethan J.", ""]]}, {"id": "1612.01981", "submitter": "Manohar Karki", "authors": "Manohar Karki, Robert DiBiano, Saikat Basu, Supratik Mukhopadhyay", "title": "Core Sampling Framework for Pixel Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intermediate map responses of a Convolutional Neural Network (CNN)\ncontain information about an image that can be used to extract contextual\nknowledge about it. In this paper, we present a core sampling framework that is\nable to use these activation maps from several layers as features to another\nneural network using transfer learning to provide an understanding of an input\nimage. Our framework creates a representation that combines features from the\ntest data and the contextual knowledge gained from the responses of a\npretrained network, processes it and feeds it to a separate Deep Belief\nNetwork. We use this representation to extract more information from an image\nat the pixel level, hence gaining understanding of the whole image. We\nexperimentally demonstrate the usefulness of our framework using a pretrained\nVGG-16 model to perform segmentation on the BAERI dataset of Synthetic Aperture\nRadar(SAR) imagery and the CAMVID dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 20:28:44 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Karki", "Manohar", ""], ["DiBiano", "Robert", ""], ["Basu", "Saikat", ""], ["Mukhopadhyay", "Supratik", ""]]}, {"id": "1612.01988", "submitter": "Abhishek Kumar", "authors": "Anant Raj, Abhishek Kumar, Youssef Mroueh, P. Thomas Fletcher,\n  Bernhard Sch\\\"olkopf", "title": "Local Group Invariant Representations via Orbit Embeddings", "comments": "AISTATS 2017 accepted version including appendix, 18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariance to nuisance transformations is one of the desirable properties of\neffective representations. We consider transformations that form a \\emph{group}\nand propose an approach based on kernel methods to derive local group invariant\nrepresentations. Locality is achieved by defining a suitable probability\ndistribution over the group which in turn induces distributions in the input\nfeature space. We learn a decision function over these distributions by\nappealing to the powerful framework of kernel methods and generate local\ninvariant random feature maps via kernel approximations. We show uniform\nconvergence bounds for kernel approximation and provide excess risk bounds for\nlearning with these features. We evaluate our method on three real datasets,\nincluding Rotated MNIST and CIFAR-10, and observe that it outperforms competing\nkernel based approaches. The proposed method also outperforms deep CNN on\nRotated-MNIST and performs comparably to the recently proposed\ngroup-equivariant CNN.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 20:46:39 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 16:50:08 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Raj", "Anant", ""], ["Kumar", "Abhishek", ""], ["Mroueh", "Youssef", ""], ["Fletcher", "P. Thomas", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1612.02099", "submitter": "Yu Lu", "authors": "Yu Lu and Harrison H. Zhou", "title": "Statistical and Computational Guarantees of Lloyd's Algorithm and its\n  Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental problem in statistics and machine learning.\nLloyd's algorithm, proposed in 1957, is still possibly the most widely used\nclustering algorithm in practice due to its simplicity and empirical\nperformance. However, there has been little theoretical investigation on the\nstatistical and computational guarantees of Lloyd's algorithm. This paper is an\nattempt to bridge this gap between practice and theory. We investigate the\nperformance of Lloyd's algorithm on clustering sub-Gaussian mixtures. Under an\nappropriate initialization for labels or centers, we show that Lloyd's\nalgorithm converges to an exponentially small clustering error after an order\nof $\\log n$ iterations, where $n$ is the sample size. The error rate is shown\nto be minimax optimal. For the two-mixture case, we only require the\ninitializer to be slightly better than random guess.\n  In addition, we extend the Lloyd's algorithm and its analysis to community\ndetection and crowdsourcing, two problems that have received a lot of attention\nrecently in statistics and machine learning. Two variants of Lloyd's algorithm\nare proposed respectively for community detection and crowdsourcing. On the\ntheoretical side, we provide statistical and computational guarantees of the\ntwo algorithms, and the results improve upon some previous signal-to-noise\nratio conditions in literature for both problems. Experimental results on\nsimulated and real data sets demonstrate competitive performance of our\nalgorithms to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 02:35:54 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Lu", "Yu", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1612.02130", "submitter": "Niek Tax", "authors": "Niek Tax, Ilya Verenich, Marcello La Rosa, Marlon Dumas", "title": "Predictive Business Process Monitoring with LSTM Neural Networks", "comments": "Accepted at the International Conference on Advanced Information\n  Systems Engineering (CAiSE) 2017", "journal-ref": "Lecture Notes in Computer Science, 10253 (2017) 477-492", "doi": "10.1007/978-3-319-59536-8_30", "report-no": null, "categories": "stat.AP cs.DB cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive business process monitoring methods exploit logs of completed\ncases of a process in order to make predictions about running cases thereof.\nExisting methods in this space are tailor-made for specific prediction tasks.\nMoreover, their relative accuracy is highly sensitive to the dataset at hand,\nthus requiring users to engage in trial-and-error and tuning when applying them\nin a specific setting. This paper investigates Long Short-Term Memory (LSTM)\nneural networks as an approach to build consistently accurate models for a wide\nrange of predictive process monitoring tasks. First, we show that LSTMs\noutperform existing techniques to predict the next event of a running case and\nits timestamp. Next, we show how to use models for predicting the next task in\norder to predict the full continuation of a running case. Finally, we apply the\nsame approach to predict the remaining time, and show that this approach\noutperforms existing tailor-made methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 07:04:17 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:51:41 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Tax", "Niek", ""], ["Verenich", "Ilya", ""], ["La Rosa", "Marcello", ""], ["Dumas", "Marlon", ""]]}, {"id": "1612.02136", "submitter": "Yanran Li", "authors": "Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, Wenjie Li", "title": "Mode Regularized Generative Adversarial Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Networks achieve state-of-the-art results on\na variety of generative tasks, they are regarded as highly unstable and prone\nto miss modes. We argue that these bad behaviors of GANs are due to the very\nparticular functional shape of the trained discriminators in high dimensional\nspaces, which can easily make training stuck or push probability mass in the\nwrong direction, towards that of higher concentration than that of the data\ngenerating distribution. We introduce several ways of regularizing the\nobjective, which can dramatically stabilize the training of GAN models. We also\nshow that our regularizers can help the fair distribution of probability mass\nacross the modes of the data generating distribution, during the early phases\nof training and thus providing a unified solution to the missing modes problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 07:45:38 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 06:08:37 GMT"}, {"version": "v3", "created": "Sun, 18 Dec 2016 05:55:22 GMT"}, {"version": "v4", "created": "Mon, 20 Feb 2017 05:01:27 GMT"}, {"version": "v5", "created": "Thu, 2 Mar 2017 06:28:13 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Che", "Tong", ""], ["Li", "Yanran", ""], ["Jacob", "Athul Paul", ""], ["Bengio", "Yoshua", ""], ["Li", "Wenjie", ""]]}, {"id": "1612.02161", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "Measuring the non-asymptotic convergence of sequential Monte Carlo\n  samplers using probabilistic programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key limitation of sampling algorithms for approximate inference is that it\nis difficult to quantify their approximation error. Widely used sampling\nschemes, such as sequential importance sampling with resampling and\nMetropolis-Hastings, produce output samples drawn from a distribution that may\nbe far from the target posterior distribution. This paper shows how to\nupper-bound the symmetric KL divergence between the output distribution of a\nbroad class of sequential Monte Carlo (SMC) samplers and their target posterior\ndistributions, subject to assumptions about the accuracy of a separate\ngold-standard sampler. The proposed method applies to samplers that combine\nmultiple particles, multinomial resampling, and rejuvenation kernels. The\nexperiments show the technique being used to estimate bounds on the divergence\nof SMC samplers for posterior inference in a Bayesian linear regression model\nand a Dirichlet process mixture model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 09:29:58 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 20:33:52 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1612.02179", "submitter": "Nir Ben-Zrihem", "authors": "Nir Baram, Oron Anschel, Shie Mannor", "title": "Model-based Adversarial Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial learning is a popular new approach to training\ngenerative models which has been proven successful for other related problems\nas well. The general idea is to maintain an oracle $D$ that discriminates\nbetween the expert's data distribution and that of the generative model $G$.\nThe generative model is trained to capture the expert's distribution by\nmaximizing the probability of $D$ misclassifying the data it generates.\nOverall, the system is \\emph{differentiable} end-to-end and is trained using\nbasic backpropagation. This type of learning was successfully applied to the\nproblem of policy imitation in a model-free setup. However, a model-free\napproach does not allow the system to be differentiable, which requires the use\nof high-variance gradient estimations. In this paper we introduce the Model\nbased Adversarial Imitation Learning (MAIL) algorithm. A model-based approach\nfor the problem of adversarial imitation learning. We show how to use a forward\nmodel to make the system fully differentiable, which enables us to train\npolicies using the (stochastic) gradient of $D$. Moreover, our approach\nrequires relatively few environment interactions, and fewer hyper-parameters to\ntune. We test our method on the MuJoCo physics simulator and report initial\nresults that surpass the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:10:31 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Baram", "Nir", ""], ["Anschel", "Oron", ""], ["Mannor", "Shie", ""]]}, {"id": "1612.02192", "submitter": "Sergey Bartunov", "authors": "Sergey Bartunov, Dmitry P. Vetrov", "title": "Fast Adaptation in Generative Models with Generative Matching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances, the remaining bottlenecks in deep generative models\nare necessity of extensive training and difficulties with generalization from\nsmall number of training examples. We develop a new generative model called\nGenerative Matching Network which is inspired by the recently proposed matching\nnetworks for one-shot learning in discriminative tasks. By conditioning on the\nadditional input dataset, our model can instantly learn new concepts that were\nnot available in the training data but conform to a similar generative process.\nThe proposed framework does not explicitly restrict diversity of the\nconditioning data and also does not require an extensive inference procedure\nfor training or adaptation. Our experiments on the Omniglot dataset demonstrate\nthat Generative Matching Networks significantly improve predictive performance\non the fly as more additional data is available and outperform existing state\nof the art conditional generative models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:50:37 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 07:41:15 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Bartunov", "Sergey", ""], ["Vetrov", "Dmitry P.", ""]]}, {"id": "1612.02222", "submitter": "Binghong Chen", "authors": "Binghong Chen, Jun Zhu", "title": "A Communication-Efficient Parallel Method for Group-Lasso", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-Lasso (gLasso) identifies important explanatory factors in predicting\nthe response variable by considering the grouping structure over input\nvariables. However, most existing algorithms for gLasso are not scalable to\ndeal with large-scale datasets, which are becoming a norm in many applications.\nIn this paper, we present a divide-and-conquer based parallel algorithm\n(DC-gLasso) to scale up gLasso in the tasks of regression with grouping\nstructures. DC-gLasso only needs two iterations to collect and aggregate the\nlocal estimates on subsets of the data, and is provably correct to recover the\ntrue model under certain conditions. We further extend it to deal with\noverlappings between groups. Empirical results on a wide range of synthetic and\nreal-world datasets show that DC-gLasso can significantly improve the time\nefficiency without sacrificing regression accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 12:32:44 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Chen", "Binghong", ""], ["Zhu", "Jun", ""]]}, {"id": "1612.02295", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Yandong Wen, Zhiding Yu, Meng Yang", "title": "Large-Margin Softmax Loss for Convolutional Neural Networks", "comments": "Published in ICML 2016 (with typo fixed)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-entropy loss together with softmax is arguably one of the most common\nused supervision components in convolutional neural networks (CNNs). Despite\nits simplicity, popularity and excellent performance, the component does not\nexplicitly encourage discriminative learning of features. In this paper, we\npropose a generalized large-margin softmax (L-Softmax) loss which explicitly\nencourages intra-class compactness and inter-class separability between learned\nfeatures. Moreover, L-Softmax not only can adjust the desired margin but also\ncan avoid overfitting. We also show that the L-Softmax loss can be optimized by\ntypical stochastic gradient descent. Extensive experiments on four benchmark\ndatasets demonstrate that the deeply-learned features with L-softmax loss\nbecome more discriminative, hence significantly boosting the performance on a\nvariety of visual classification and verification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 15:36:11 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 07:28:18 GMT"}, {"version": "v3", "created": "Mon, 23 Jan 2017 08:32:08 GMT"}, {"version": "v4", "created": "Fri, 17 Nov 2017 23:23:09 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Liu", "Weiyang", ""], ["Wen", "Yandong", ""], ["Yu", "Zhiding", ""], ["Yang", "Meng", ""]]}, {"id": "1612.02297", "submitter": "Michael Figurnov", "authors": "Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang, Jonathan\n  Huang, Dmitry Vetrov, Ruslan Salakhutdinov", "title": "Spatially Adaptive Computation Time for Residual Networks", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep learning architecture based on Residual Network\nthat dynamically adjusts the number of executed layers for the regions of the\nimage. This architecture is end-to-end trainable, deterministic and\nproblem-agnostic. It is therefore applicable without any modifications to a\nwide range of computer vision problems such as image classification, object\ndetection and image segmentation. We present experimental results showing that\nthis model improves the computational efficiency of Residual Networks on the\nchallenging ImageNet classification and COCO object detection datasets.\nAdditionally, we evaluate the computation time maps on the visual saliency\ndataset cat2000 and find that they correlate surprisingly well with human eye\nfixation positions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 15:37:57 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 17:49:27 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Figurnov", "Michael", ""], ["Collins", "Maxwell D.", ""], ["Zhu", "Yukun", ""], ["Zhang", "Li", ""], ["Huang", "Jonathan", ""], ["Vetrov", "Dmitry", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1612.02310", "submitter": "Ji Feng", "authors": "Ji Feng, Qingsheng Zhu, Jinlong Huang, Lijun Yang", "title": "Extend natural neighbor: a novel classification method with\n  self-adaptive neighborhood parameters in different stages", "comments": "10 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various kinds of k-nearest neighbor (KNN) based classification methods are\nthe bases of many well-established and high-performance pattern-recognition\ntechniques, but both of them are vulnerable to their parameter choice.\nEssentially, the challenge is to detect the neighborhood of various data sets,\nwhile utterly ignorant of the data characteristic. This article introduces a\nnew supervised classification method: the extend natural neighbor (ENaN)\nmethod, and shows that it provides a better classification result without\nchoosing the neighborhood parameter artificially. Unlike the original KNN based\nmethod which needs a prior k, the ENaNE method predicts different k in\ndifferent stages. Therefore, the ENaNE method is able to learn more from\nflexible neighbor information both in training stage and testing stage, and\nprovide a better classification result.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 16:13:52 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Feng", "Ji", ""], ["Zhu", "Qingsheng", ""], ["Huang", "Jinlong", ""], ["Yang", "Lijun", ""]]}, {"id": "1612.02334", "submitter": "Xingguo Li", "authors": "Xingguo Li and Jarvis Haupt", "title": "Robust Low-Complexity Randomized Methods for Locating Outliers in Large\n  Matrices", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the problem of locating outlier columns in a large,\notherwise low-rank matrix, in settings where {}{the data} are noisy, or where\nthe overall matrix has missing elements. We propose a randomized two-step\ninference framework, and establish sufficient conditions on the required sample\ncomplexities under which these methods succeed (with high probability) in\naccurately locating the outliers for each task. Comprehensive numerical\nexperimental results are provided to verify the theoretical bounds and\ndemonstrate the computational efficiency of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 17:18:10 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Li", "Xingguo", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1612.02350", "submitter": "Francisco Raposo", "authors": "Francisco Raposo, David Martins de Matos, Ricardo Ribeiro", "title": "An Information-theoretic Approach to Machine-oriented Music\n  Summarization", "comments": "7 pages, 1 algorithm, 7 figures, 1 table, submitted to Pattern\n  Recognition Letters (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music summarization allows for higher efficiency in processing, storage, and\nsharing of datasets. Machine-oriented approaches, being agnostic to human\nconsumption, optimize these aspects even further. Such summaries have already\nbeen successfully validated in some MIR tasks. We now generalize previous\nconclusions by evaluating the impact of generic summarization of music from a\nprobabilistic perspective. We estimate Gaussian distributions for original and\nsummarized songs and compute their relative entropy, in order to measure\ninformation loss incurred by summarization. Our results suggest that relative\nentropy is a good predictor of summarization performance in the context of\ntasks relying on a bag-of-features model. Based on this observation, we further\npropose a straightforward yet expressive summarizer, which minimizes relative\nentropy with respect to the original song, that objectively outperforms\nprevious methods and is better suited to avoid potential copyright issues.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 18:02:09 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 14:47:27 GMT"}, {"version": "v3", "created": "Wed, 26 Jul 2017 16:31:23 GMT"}, {"version": "v4", "created": "Sat, 12 Aug 2017 14:16:35 GMT"}, {"version": "v5", "created": "Mon, 25 Sep 2017 09:35:58 GMT"}, {"version": "v6", "created": "Fri, 21 Sep 2018 16:26:36 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Raposo", "Francisco", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1612.02482", "submitter": "Krupakar Hans", "authors": "Krupakar Hans, R S Milton", "title": "Improving the Performance of Neural Machine Translation Involving\n  Morphologically Rich Languages", "comments": "21 pages, 11 figures, 2 tables, Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of the attention mechanism in neural machine translation models\nhas improved the performance of machine translation systems by enabling\nselective lookup into the source sentence. In this paper, the efficiencies of\ntranslation using bidirectional encoder attention decoder models were studied\nwith respect to translation involving morphologically rich languages. The\nEnglish - Tamil language pair was selected for this analysis. First, the use of\nWord2Vec embedding for both the English and Tamil words improved the\ntranslation results by 0.73 BLEU points over the baseline RNNSearch model with\n4.84 BLEU score. The use of morphological segmentation before word\nvectorization to split the morphologically rich Tamil words into their\nrespective morphemes before the translation, caused a reduction in the target\nvocabulary size by a factor of 8. Also, this model (RNNMorph) improved the\nperformance of neural machine translation by 7.05 BLEU points over the\nRNNSearch model used over the same corpus. Since the BLEU evaluation of the\nRNNMorph model might be unreliable due to an increase in the number of matching\ntokens per sentence, the performances of the translations were also compared by\nmeans of human evaluation metrics of adequacy, fluency and relative ranking.\nFurther, the use of morphological segmentation also improved the efficacy of\nthe attention mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 23:20:53 GMT"}, {"version": "v2", "created": "Sun, 8 Jan 2017 06:04:50 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Hans", "Krupakar", ""], ["Milton", "R S", ""]]}, {"id": "1612.02487", "submitter": "Luana Micallef", "authors": "Luana Micallef, Iiris Sundin, Pekka Marttinen, Muhammad Ammad-ud-din,\n  Tomi Peltola, Marta Soare, Giulio Jacucci, Samuel Kaski", "title": "Interactive Elicitation of Knowledge on Feature Relevance Improves\n  Predictions in Small Data Sets", "comments": "in Proceedings of the 22nd International Conference on Intelligent\n  User Interfaces (IUI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Providing accurate predictions is challenging for machine learning algorithms\nwhen the number of features is larger than the number of samples in the data.\nPrior knowledge can improve machine learning models by indicating relevant\nvariables and parameter values. Yet, this prior knowledge is often tacit and\nonly available from domain experts. We present a novel approach that uses\ninteractive visualization to elicit the tacit prior knowledge and uses it to\nimprove the accuracy of prediction models. The main component of our approach\nis a user model that models the domain expert's knowledge of the relevance of\ndifferent features for a prediction task. In particular, based on the expert's\nearlier input, the user model guides the selection of the features on which to\nelicit user's knowledge next. The results of a controlled user study show that\nthe user model significantly improves prior knowledge elicitation and\nprediction accuracy, when predicting the relative citation counts of scientific\ndocuments in a specific domain.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 23:31:26 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 15:16:05 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Micallef", "Luana", ""], ["Sundin", "Iiris", ""], ["Marttinen", "Pekka", ""], ["Ammad-ud-din", "Muhammad", ""], ["Peltola", "Tomi", ""], ["Soare", "Marta", ""], ["Jacucci", "Giulio", ""], ["Kaski", "Samuel", ""]]}, {"id": "1612.02490", "submitter": "An Qu", "authors": "An Qu and Cheng Zhang and Paul Ackermann and Hedvig Kjellstr\\\"om", "title": "Bridging Medical Data Inference to Achilles Tendon Rupture\n  Rehabilitation", "comments": "Workshop on Machine Learning for Healthcare, NIPS 2016, Barcelona,\n  Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputing incomplete medical tests and predicting patient outcomes are crucial\nfor guiding the decision making for therapy, such as after an Achilles Tendon\nRupture (ATR). We formulate the problem of data imputation and prediction for\nATR relevant medical measurements into a recommender system framework. By\napplying MatchBox, which is a collaborative filtering approach, on a real\ndataset collected from 374 ATR patients, we aim at offering personalized\nmedical data imputation and prediction. In this work, we show the feasibility\nof this approach and discuss potential research directions by conducting\ninitial qualitative evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 23:58:36 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Qu", "An", ""], ["Zhang", "Cheng", ""], ["Ackermann", "Paul", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "1612.02526", "submitter": "Vatsal Sharan", "authors": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant", "title": "Prediction with a Short Memory", "comments": "Updates for STOC camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting the next observation given a sequence\nof past observations, and consider the extent to which accurate prediction\nrequires complex algorithms that explicitly leverage long-range dependencies.\nPerhaps surprisingly, our positive results show that for a broad class of\nsequences, there is an algorithm that predicts well on average, and bases its\npredictions only on the most recent few observation together with a set of\nsimple summary statistics of the past observations. Specifically, we show that\nfor any distribution over observations, if the mutual information between past\nobservations and future observations is upper bounded by $I$, then a simple\nMarkov model over the most recent $I/\\epsilon$ observations obtains expected KL\nerror $\\epsilon$---and hence $\\ell_1$ error $\\sqrt{\\epsilon}$---with respect to\nthe optimal predictor that has access to the entire past and knows the data\ngenerating distribution. For a Hidden Markov Model with $n$ hidden states, $I$\nis bounded by $\\log n$, a quantity that does not depend on the mixing time, and\nwe show that the trivial prediction algorithm based on the empirical\nfrequencies of length $O(\\log n/\\epsilon)$ windows of observations achieves\nthis error, provided the length of the sequence is $d^{\\Omega(\\log\nn/\\epsilon)}$, where $d$ is the size of the observation alphabet.\n  We also establish that this result cannot be improved upon, even for the\nclass of HMMs, in the following two senses: First, for HMMs with $n$ hidden\nstates, a window length of $\\log n/\\epsilon$ is information-theoretically\nnecessary to achieve expected $\\ell_1$ error $\\sqrt{\\epsilon}$. Second, the\n$d^{\\Theta(\\log n/\\epsilon)}$ samples required to estimate the Markov model for\nan observation alphabet of size $d$ is necessary for any computationally\ntractable learning algorithm, assuming the hardness of strongly refuting a\ncertain class of CSPs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 04:18:09 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 17:51:39 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 07:01:47 GMT"}, {"version": "v4", "created": "Sun, 27 May 2018 01:30:15 GMT"}, {"version": "v5", "created": "Thu, 28 Jun 2018 01:54:04 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Sharan", "Vatsal", ""], ["Kakade", "Sham", ""], ["Liang", "Percy", ""], ["Valiant", "Gregory", ""]]}, {"id": "1612.02572", "submitter": "Giovanni Montana", "authors": "James H Cole, Rudra PK Poudel, Dimosthenis Tsagkrasoulis, Matthan WA\n  Caan, Claire Steves, Tim D Spector, Giovanni Montana", "title": "Predicting brain age with deep learning from raw imaging data results in\n  a reliable and heritable biomarker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning analysis of neuroimaging data can accurately predict\nchronological age in healthy people and deviations from healthy brain ageing\nhave been associated with cognitive impairment and disease. Here we sought to\nfurther establish the credentials of \"brain-predicted age\" as a biomarker of\nindividual differences in the brain ageing process, using a predictive\nmodelling approach based on deep learning, and specifically convolutional\nneural networks (CNN), and applied to both pre-processed and raw T1-weighted\nMRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted\nage using a large dataset of healthy adults (N = 2001). Next, we sought to\nestablish the heritability of brain-predicted age using a sample of monozygotic\nand dizygotic female twins (N = 62). Thirdly, we examined the test-retest and\nmulti-centre reliability of brain-predicted age using two samples\n(within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were\ngenerated and compared to a Gaussian Process Regression (GPR) approach, on all\ndatasets. Input data were grey matter (GM) or white matter (WM) volumetric maps\ngenerated by Statistical Parametric Mapping (SPM) or raw data. Brain-predicted\nage represents an accurate, highly reliable and genetically-valid phenotype,\nthat has potential to be used as a biomarker of brain ageing. Moreover, age\npredictions can be accurately generated on raw T1-MRI data, substantially\nreducing computation time for novel data, bringing the process closer to giving\nreal-time information on brain health in clinical settings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 09:26:08 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Cole", "James H", ""], ["Poudel", "Rudra PK", ""], ["Tsagkrasoulis", "Dimosthenis", ""], ["Caan", "Matthan WA", ""], ["Steves", "Claire", ""], ["Spector", "Tim D", ""], ["Montana", "Giovanni", ""]]}, {"id": "1612.02605", "submitter": "Philip Bachman", "authors": "Philip Bachman and Alessandro Sordoni and Adam Trischler", "title": "Towards Information-Seeking Agents", "comments": "Under review for ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general problem setting for training and testing the ability of\nagents to gather information efficiently. Specifically, we present a collection\nof tasks in which success requires searching through a partially-observed\nenvironment, for fragments of information which can be pieced together to\naccomplish various goals. We combine deep architectures with techniques from\nreinforcement learning to develop agents that solve our tasks. We shape the\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\nempirically demonstrate that these agents learn to search actively and\nintelligently for new information to reduce their uncertainty, and to exploit\ninformation they have already acquired.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 11:47:01 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Bachman", "Philip", ""], ["Sordoni", "Alessandro", ""], ["Trischler", "Adam", ""]]}, {"id": "1612.02666", "submitter": "Barack Wanjawa Mr.", "authors": "Barack Wamkaya Wanjawa", "title": "Evaluating the Performance of ANN Prediction System at Shanghai Stock\n  Market in the Period 21-Sep-2016 to 11-Oct-2016", "comments": "13 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research evaluates the performance of an Artificial Neural Network based\nprediction system that was employed on the Shanghai Stock Exchange for the\nperiod 21-Sep-2016 to 11-Oct-2016. It is a follow-up to a previous paper in\nwhich the prices were predicted and published before September 21. Stock market\nprice prediction remains an important quest for investors and researchers. This\nresearch used an Artificial Intelligence system, being an Artificial Neural\nNetwork that is feedforward multi-layer perceptron with error backpropagation\nfor prediction, unlike other methods such as technical, fundamental or time\nseries analysis. While these alternative methods tend to guide on trends and\nnot the exact likely prices, neural networks on the other hand have the ability\nto predict the real value prices, as was done on this research. Nonetheless,\ndetermination of suitable network parameters remains a challenge in neural\nnetwork design, with this research settling on a configuration of 5:21:21:1\nwith 80% training data or 4-year of training data as a good enough model for\nstock prediction, as already determined in a previous research by the author.\nThe comparative results indicate that neural network can predict typical stock\nmarket prices with mean absolute percentage errors that are as low as 1.95%\nover the ten prediction instances that was studied in this research.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:25:29 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Wanjawa", "Barack Wamkaya", ""]]}, {"id": "1612.02695", "submitter": "Jan Chorowski", "authors": "Jan Chorowski and Navdeep Jaitly", "title": "Towards better decoding and language model integration in sequence to\n  sequence models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Sequence-to-Sequence (seq2seq) framework advocates\nreplacing complex data processing pipelines, such as an entire automatic speech\nrecognition system, with a single neural network trained in an end-to-end\nfashion. In this contribution, we analyse an attention-based seq2seq speech\nrecognition system that directly transcribes recordings into characters. We\nobserve two shortcomings: overconfidence in its predictions and a tendency to\nproduce incomplete transcriptions when language models are used. We propose\npractical solutions to both problems achieving competitive speaker independent\nword error rates on the Wall Street Journal dataset: without separate language\nmodels we reach 10.6% WER, while together with a trigram language model, we\nreach 6.7% WER.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 15:23:44 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Chorowski", "Jan", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1612.02696", "submitter": "Sven Kosub", "authors": "Sven Kosub", "title": "A note on the triangle inequality for the Jaccard distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two simple proofs of the triangle inequality for the Jaccard distance in\nterms of nonnegative, monotone, submodular functions are given and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 15:25:35 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Kosub", "Sven", ""]]}, {"id": "1612.02707", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara", "title": "CrowdMI: Multiple Imputation via Crowdsourcing", "comments": "Updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can humans impute missing data with similar proficiency as machines? This is\nthe question we aim to answer in this paper. We present a novel idea of\nconverting observations with missing data in to a survey questionnaire, which\nis presented to crowdworkers for completion. We replicate a multiple imputation\nframework by having multiple unique crowdworkers complete our questionnaire.\nExperimental results demonstrate that using our method, it is possible to\ngenerate valid imputations for qualitative and quantitative missing data, with\nresults comparable to imputations generated by complex statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 16:10:08 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 15:15:14 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 18:19:11 GMT"}, {"version": "v4", "created": "Fri, 23 Feb 2018 16:00:34 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Gondara", "Lovedeep", ""]]}, {"id": "1612.02712", "submitter": "Yingyu Liang", "authors": "Nan Du, Yingyu Liang, Maria-Florina Balcan, Manuel Gomez-Rodriguez,\n  Hongyuan Zha, Le Song", "title": "Scalable Influence Maximization for Multiple Products in Continuous-Time\n  Diffusion Networks", "comments": "45 pages, to appear in Journal of Machine Learning Research. arXiv\n  admin note: substantial text overlap with arXiv:1312.2164, arXiv:1311.3669", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical viral marketing model identifies influential users in a social\nnetwork to maximize a single product adoption assuming unlimited user\nattention, campaign budgets, and time. In reality, multiple products need\ncampaigns, users have limited attention, convincing users incurs costs, and\nadvertisers have limited budgets and expect the adoptions to be maximized soon.\nFacing these user, monetary, and timing constraints, we formulate the problem\nas a submodular maximization task in a continuous-time diffusion model under\nthe intersection of a matroid and multiple knapsack constraints. We propose a\nrandomized algorithm estimating the user influence in a network\n($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$\nwith $n=\\mathcal{O}(1/\\epsilon^2)$ randomizations and\n$\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By\nexploiting the influence estimation algorithm as a subroutine, we develop an\nadaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2\nk)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active.\nExtensive experiments on networks of millions of nodes demonstrate that the\nproposed algorithms achieve the state-of-the-art in terms of effectiveness and\nscalability.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 16:15:57 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 07:29:49 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Du", "Nan", ""], ["Liang", "Yingyu", ""], ["Balcan", "Maria-Florina", ""], ["Gomez-Rodriguez", "Manuel", ""], ["Zha", "Hongyuan", ""], ["Song", "Le", ""]]}, {"id": "1612.02734", "submitter": "Peter Sadowski", "authors": "Pierre Baldi, Peter Sadowski, Zhiqin Lu", "title": "Learning in the Machine: Random Backpropagation and the Deep Learning\n  Channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random backpropagation (RBP) is a variant of the backpropagation algorithm\nfor training neural networks, where the transpose of the forward matrices are\nreplaced by fixed random matrices in the calculation of the weight updates. It\nis remarkable both because of its effectiveness, in spite of using random\nmatrices to communicate error information, and because it completely removes\nthe taxing requirement of maintaining symmetric weights in a physical neural\nsystem. To better understand random backpropagation, we first connect it to the\nnotions of local learning and learning channels. Through this connection, we\nderive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP\n(ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their\ncomputational complexity. We then study their behavior through simulations\nusing the MNIST and CIFAR-10 bechnmark datasets. These simulations show that\nmost of these variants work robustly, almost as well as backpropagation, and\nthat multiplication by the derivatives of the activation functions is\nimportant. As a follow-up, we study also the low-end of the number of bits\nrequired to communicate error information over the learning channel. We then\nprovide partial intuitive explanations for some of the remarkable properties of\nRBP and its variations. Finally, we prove several mathematical results,\nincluding the convergence to fixed points of linear chains of arbitrary length,\nthe convergence to fixed points of linear autoencoders with decorrelated data,\nthe long-term existence of solutions for linear systems with a single hidden\nlayer and convergence in special cases, and the convergence to fixed points of\nnon-linear chains, when the derivative of the activation functions is included.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 17:15:45 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 17:29:20 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Baldi", "Pierre", ""], ["Sadowski", "Peter", ""], ["Lu", "Zhiqin", ""]]}, {"id": "1612.02739", "submitter": "Martin Pecka", "authors": "Martin Pecka, Karel Zimmermann, Michal Rein\\v{s}tein, Tom\\'a\\v{s}\n  Svoboda", "title": "Controlling Robot Morphology from Incomplete Measurements", "comments": "Accepted into IEEE Transactions to Industrial Electronics, Special\n  Section on Motion Control for Novel Emerging Robotic Devices and Systems", "journal-ref": null, "doi": "10.1109/TIE.2016.2580125", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots with complex morphology are essential for traversing rough\nterrains in Urban Search & Rescue missions (USAR). Since teleoperation of the\ncomplex morphology causes high cognitive load of the operator, the morphology\nis controlled autonomously. The autonomous control measures the robot state and\nsurrounding terrain which is usually only partially observable, and thus the\ndata are often incomplete. We marginalize the control over the missing\nmeasurements and evaluate an explicit safety condition. If the safety condition\nis violated, tactile terrain exploration by the body-mounted robotic arm\ngathers the missing data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 17:40:57 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Pecka", "Martin", ""], ["Zimmermann", "Karel", ""], ["Rein\u0161tein", "Michal", ""], ["Svoboda", "Tom\u00e1\u0161", ""]]}, {"id": "1612.02741", "submitter": "Lili Mou", "authors": "Lili Mou, Zhengdong Lu, Hang Li, Zhi Jin", "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries", "comments": "Accepted by ICML-17; also presented at ICLR-17 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building neural networks to query a knowledge base (a table) with natural\nlanguage is an emerging research topic in deep learning. An executor for table\nquerying typically requires multiple steps of execution because queries may\nhave complicated structures. In previous studies, researchers have developed\neither fully distributed executors or symbolic executors for table querying. A\ndistributed executor can be trained in an end-to-end fashion, but is weak in\nterms of execution efficiency and explicit interpretability. A symbolic\nexecutor is efficient in execution, but is very difficult to train especially\nat initial stages. In this paper, we propose to couple distributed and symbolic\nexecution for natural language queries, where the symbolic executor is\npretrained with the distributed executor's intermediate execution results in a\nstep-by-step fashion. Experiments show that our approach significantly\noutperforms both distributed and symbolic executors, exhibiting high accuracy,\nhigh learning efficiency, high execution efficiency, and high interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 17:45:16 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 11:37:44 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 20:39:57 GMT"}, {"version": "v4", "created": "Fri, 16 Jun 2017 14:33:31 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Mou", "Lili", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Jin", "Zhi", ""]]}, {"id": "1612.02751", "submitter": "David Koes", "authors": "Matthew Ragoza (1), Joshua Hochuli (1), Elisa Idrobo (2), Jocelyn\n  Sunseri (1) and David Ryan Koes (1) ((1) University of Pittsburgh, (2) The\n  College of New Jersey)", "title": "Protein-Ligand Scoring with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1021/acs.jcim.6b00740", "report-no": null, "categories": "stat.ML cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational approaches to drug discovery can reduce the time and cost\nassociated with experimental assays and enable the screening of novel\nchemotypes. Structure-based drug design methods rely on scoring functions to\nrank and predict binding affinities and poses. The ever-expanding amount of\nprotein-ligand binding and structural data enables the use of deep machine\nlearning techniques for protein-ligand scoring.\n  We describe convolutional neural network (CNN) scoring functions that take as\ninput a comprehensive 3D representation of a protein-ligand interaction. A CNN\nscoring function automatically learns the key features of protein-ligand\ninteractions that correlate with binding. We train and optimize our CNN scoring\nfunctions to discriminate between correct and incorrect binding poses and known\nbinders and non-binders. We find that our CNN scoring function outperforms the\nAutoDock Vina scoring function when ranking poses both for pose prediction and\nvirtual screening.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 18:18:29 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ragoza", "Matthew", ""], ["Hochuli", "Joshua", ""], ["Idrobo", "Elisa", ""], ["Sunseri", "Jocelyn", ""], ["Koes", "David Ryan", ""]]}, {"id": "1612.02780", "submitter": "Ben Poole", "authors": "Ben Poole, Alexander A. Alemi, Jascha Sohl-Dickstein, Anelia Angelova", "title": "Improved generator objectives for GANs", "comments": "NIPS 2016 Workshop on Adversarial Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to understand GAN training as alternating density\nratio estimation and approximate divergence minimization. This provides an\ninterpretation for the mismatched GAN generator and discriminator objectives\noften used in practice, and explains the problem of poor sample diversity. We\nalso derive a family of generator objectives that target arbitrary\n$f$-divergences without minimizing a lower bound, and use them to train\ngenerative image models that target either improved sample quality or greater\nsample diversity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 19:32:04 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Poole", "Ben", ""], ["Alemi", "Alexander A.", ""], ["Sohl-Dickstein", "Jascha", ""], ["Angelova", "Anelia", ""]]}, {"id": "1612.02802", "submitter": "Homayun Afrabandpey", "authors": "Homayun Afrabandpey, Tomi Peltola, Samuel Kaski", "title": "Interactive Prior Elicitation of Feature Similarities for Small Sample\n  Size Prediction", "comments": null, "journal-ref": null, "doi": "10.1145/3079628.3079698", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression under the \"small $n$, large $p$\" conditions, of small sample size\n$n$ and large number of features $p$ in the learning data set, is a recurring\nsetting in which learning from data is difficult. With prior knowledge about\nrelationships of the features, $p$ can effectively be reduced, but explicating\nsuch prior knowledge is difficult for experts. In this paper we introduce a new\nmethod for eliciting expert prior knowledge about the similarity of the roles\nof features in the prediction task. The key idea is to use an interactive\nmultidimensional-scaling (MDS) type scatterplot display of the features to\nelicit the similarity relationships, and then use the elicited relationships in\nthe prior distribution of prediction parameters. Specifically, for learning to\npredict a target variable with Bayesian linear regression, the feature\nrelationships are used to construct a Gaussian prior with a full covariance\nmatrix for the regression coefficients. Evaluation of our method in experiments\nwith simulated and real users on text data confirm that prior elicitation of\nfeature similarities improves prediction accuracy. Furthermore, elicitation\nwith an interactive scatterplot display outperforms straightforward elicitation\nwhere the users choose feature pairs from a feature list.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:35:46 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 15:00:29 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Afrabandpey", "Homayun", ""], ["Peltola", "Tomi", ""], ["Kaski", "Samuel", ""]]}, {"id": "1612.02803", "submitter": "Lin Yang", "authors": "Lin F. Yang, R. Arora, V. Braverman, Tuo Zhao", "title": "The Physical Systems Behind Optimization Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use differential equations based approaches to provide some {\\it\n\\textbf{physics}} insights into analyzing the dynamics of popular optimization\nalgorithms in machine learning. In particular, we study gradient descent,\nproximal gradient descent, coordinate gradient descent, proximal coordinate\ngradient, and Newton's methods as well as their Nesterov's accelerated variants\nin a unified framework motivated by a natural connection of optimization\nalgorithms to physical systems. Our analysis is applicable to more general\nalgorithms and optimization problems {\\it \\textbf{beyond}} convexity and strong\nconvexity, e.g. Polyak-\\L ojasiewicz and error bound conditions (possibly\nnonconvex).\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:36:30 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 17:44:34 GMT"}, {"version": "v3", "created": "Fri, 12 May 2017 19:01:38 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 18:24:39 GMT"}, {"version": "v5", "created": "Thu, 25 Oct 2018 04:04:13 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Yang", "Lin F.", ""], ["Arora", "R.", ""], ["Braverman", "V.", ""], ["Zhao", "Tuo", ""]]}, {"id": "1612.02814", "submitter": "Ting Chen Ting Chen", "authors": "Ting Chen and Yizhou Sun", "title": "Task-Guided and Path-Augmented Heterogeneous Network Embedding for\n  Author Identification", "comments": "Accepted by WSDM 2017. This is an extended version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of author identification under\ndouble-blind review setting, which is to identify potential authors given\ninformation of an anonymized paper. Different from existing approaches that\nrely heavily on feature engineering, we propose to use network embedding\napproach to address the problem, which can automatically represent nodes into\nlower dimensional feature vectors. However, there are two major limitations in\nrecent studies on network embedding: (1) they are usually general-purpose\nembedding methods, which are independent of the specific tasks; and (2) most of\nthese approaches can only deal with homogeneous networks, where the\nheterogeneity of the network is ignored. Hence, challenges faced here are two\nfolds: (1) how to embed the network under the guidance of the author\nidentification task, and (2) how to select the best type of information due to\nthe heterogeneity of the network.\n  To address the challenges, we propose a task-guided and path-augmented\nheterogeneous network embedding model. In our model, nodes are first embedded\nas vectors in latent feature space. Embeddings are then shared and jointly\ntrained according to task-specific and network-general objectives. We extend\nthe existing unsupervised network embedding to incorporate meta paths in\nheterogeneous networks, and select paths according to the specific task. The\nguidance from author identification task for network embedding is provided both\nexplicitly in joint training and implicitly during meta path selection. Our\nexperiments demonstrate that by using path-augmented network embedding with\ntask guidance, our model can obtain significantly better accuracy at\nidentifying the true authors comparing to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:56:48 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 04:20:03 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Chen", "Ting", ""], ["Sun", "Yizhou", ""]]}, {"id": "1612.02842", "submitter": "Andrew Stevens", "authors": "Andrew Stevens, Yunchen Pu, Yannan Sun, Greg Spell, Lawrence Carin", "title": "Tensor-Dictionary Learning with Deep Kruskal-Factor Analysis", "comments": "AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-way factor analysis model is introduced for tensor-variate data of\nany order. Each data item is represented as a (sparse) sum of Kruskal\ndecompositions, a Kruskal-factor analysis (KFA). KFA is nonparametric and can\ninfer both the tensor-rank of each dictionary atom and the number of dictionary\natoms. The model is adapted for online learning, which allows dictionary\nlearning on large data sets. After KFA is introduced, the model is extended to\na deep convolutional tensor-factor analysis, supervised by a Bayesian SVM. The\nexperiments section demonstrates the improvement of KFA over vectorized\napproaches (e.g., BPFA), tensor decompositions, and convolutional neural\nnetworks (CNN) in multi-way denoising, blind inpainting, and image\nclassification. The improvement in PSNR for the inpainting results over other\nmethods exceeds 1dB in several cases and we achieve state of the art results on\nCaltech101 image classification.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 21:21:50 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 19:38:37 GMT"}, {"version": "v3", "created": "Sun, 5 Mar 2017 07:41:03 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Stevens", "Andrew", ""], ["Pu", "Yunchen", ""], ["Sun", "Yannan", ""], ["Spell", "Greg", ""], ["Carin", "Lawrence", ""]]}, {"id": "1612.02879", "submitter": "Vivek Veeriah", "authors": "Vivek Veeriah, Shangtong Zhang, Richard S. Sutton", "title": "Learning Representations by Stochastic Meta-Gradient Descent in Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations are fundamental to artificial intelligence. The performance\nof a learning system depends on the type of representation used for\nrepresenting the data. Typically, these representations are hand-engineered\nusing domain knowledge. More recently, the trend is to learn these\nrepresentations through stochastic gradient descent in multi-layer neural\nnetworks, which is called backprop. Learning the representations directly from\nthe incoming data stream reduces the human labour involved in designing a\nlearning system. More importantly, this allows in scaling of a learning system\nfor difficult tasks. In this paper, we introduce a new incremental learning\nalgorithm called crossprop, which learns incoming weights of hidden units based\non the meta-gradient descent approach, that was previously introduced by Sutton\n(1992) and Schraudolph (1999) for learning step-sizes. The final update\nequation introduces an additional memory parameter for each of these weights\nand generalizes the backprop update equation. From our experiments, we show\nthat crossprop learns and reuses its feature representation while tackling new\nand unseen tasks whereas backprop relearns a new feature representation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 00:56:42 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 14:53:00 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Veeriah", "Vivek", ""], ["Zhang", "Shangtong", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1612.02893", "submitter": "Krupakar Hans", "authors": "Hans Krupakar, Akshay Jayakumar, Dhivya G", "title": "A Review of Intelligent Practices for Irrigation Prediction", "comments": "18 pages, 3 figures, 1 table, In National Conference on Computational\n  Intelligence and High-Performance Computing (NCCIHPC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population growth and increasing droughts are creating unprecedented strain\non the continued availability of water resources. Since irrigation is a major\nconsumer of fresh water, wastage of resources in this sector could have strong\nconsequences. To address this issue, irrigation water management and prediction\ntechniques need to be employed effectively and should be able to account for\nthe variabilities present in the environment. The different techniques surveyed\nin this paper can be classified into two categories: computational and\nstatistical. Computational methods deal with scientific correlations between\nphysical parameters whereas statistical methods involve specific prediction\nalgorithms that can be used to automate the process of irrigation water\nprediction. These algorithms interpret semantic relationships between the\nvarious parameters of temperature, pressure, evapotranspiration etc. and store\nthem as numerical precomputed entities specific to the conditions and the area\nused as the data for the training corpus used to train it. We focus on\nreviewing the computational methods used to determine Evapotranspiration and\nits implications. We compare the efficiencies of different data mining and\nmachine learning methods implemented in this area, such as Logistic Regression,\nDecision Tress Classifier, SysFor, Support Vector Machine(SVM), Fuzzy Logic\ntechniques, Artifical Neural Networks(ANNs) and various hybrids of Genetic\nAlgorithms (GA) applied to irrigation prediction. We also recommend a possible\ntechnique for the same based on its superior results in other such time series\nanalysis tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 22:52:05 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Krupakar", "Hans", ""], ["Jayakumar", "Akshay", ""], ["G", "Dhivya", ""]]}, {"id": "1612.02897", "submitter": "Kareem Abdelfatah", "authors": "Kareem Abdelfatah, Junshu Bao, Gabriel Terejanu", "title": "Environmental Modeling Framework using Stacked Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network of independently trained Gaussian processes (StackedGP) is\nintroduced to obtain predictions of quantities of interest with quantified\nuncertainties. The main applications of the StackedGP framework are to\nintegrate different datasets through model composition, enhance predictions of\nquantities of interest through a cascade of intermediate predictions, and to\npropagate uncertainties through emulated dynamical systems driven by uncertain\nforcing variables. By using analytical first and second-order moments of a\nGaussian process with uncertain inputs using squared exponential and polynomial\nkernels, approximated expectations of quantities of interests that require an\narbitrary composition of functions can be obtained. The StackedGP model is\nextended to any number of layers and nodes per layer, and it provides\nflexibility in kernel selection for the input nodes. The proposed nonparametric\nstacked model is validated using synthetic datasets, and its performance in\nmodel composition and cascading predictions is measured in two applications\nusing real data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 02:53:45 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 19:21:16 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Abdelfatah", "Kareem", ""], ["Bao", "Junshu", ""], ["Terejanu", "Gabriel", ""]]}, {"id": "1612.02954", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "A series of maximum entropy upper bounds of the differential entropy", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a series of closed-form maximum entropy upper bounds for the\ndifferential entropy of a continuous univariate random variable and study the\nproperties of that series. We then show how to use those generic bounds for\nupper bounding the differential entropy of Gaussian mixture models. This\nrequires to calculate the raw moments and raw absolute moments of Gaussian\nmixtures in closed-form that may also be handy in statistical machine learning\nand information theory. We report on our experiments and discuss on the\ntightness of those bounds.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 09:36:14 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "1612.02965", "submitter": "Nathan Lazar", "authors": "Nathan H Lazar, Mehmet G\\\"onen, Kemal S\\\"onmez", "title": "BaTFLED: Bayesian Tensor Factorization Linked to External Data", "comments": "4 main pages with 14 supplemental pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of current machine learning algorithms are designed to\npredict single responses or a vector of responses, yet many types of response\nare more naturally organized as matrices or higher-order tensor objects where\ncharacteristics are shared across modes. We present a new machine learning\nalgorithm BaTFLED (Bayesian Tensor Factorization Linked to External Data) that\npredicts values in a three-dimensional response tensor using input features for\neach of the dimensions. BaTFLED uses a probabilistic Bayesian framework to\nlearn projection matrices mapping input features for each mode into latent\nrepresentations that multiply to form the response tensor. By utilizing a\nTucker decomposition, the model can capture weights for interactions between\nlatent factors for each mode in a small core tensor. Priors that encourage\nsparsity in the projection matrices and core tensor allow for feature selection\nand model regularization. This method is shown to far outperform elastic net\nand neural net models on 'cold start' tasks from data simulated in a three-mode\nstructure. Additionally, we apply the model to predict dose-response curves in\na panel of breast cancer cell lines treated with drug compounds that was used\nas a Dialogue for Reverse Engineering Assessments and Methods (DREAM)\nchallenge.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 10:22:58 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 17:44:29 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Lazar", "Nathan H", ""], ["G\u00f6nen", "Mehmet", ""], ["S\u00f6nmez", "Kemal", ""]]}, {"id": "1612.03079", "submitter": "Daniel Crankshaw", "authors": "Daniel Crankshaw, Xin Wang, Giulio Zhou, Michael J. Franklin, Joseph\n  E. Gonzalez, Ion Stoica", "title": "Clipper: A Low-Latency Online Prediction Serving System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is being deployed in a growing number of applications which\ndemand real-time, accurate, and robust predictions under heavy query load.\nHowever, most machine learning frameworks and systems only address model\ntraining and not deployment.\n  In this paper, we introduce Clipper, a general-purpose low-latency prediction\nserving system. Interposing between end-user applications and a wide range of\nmachine learning frameworks, Clipper introduces a modular architecture to\nsimplify model deployment across frameworks and applications. Furthermore, by\nintroducing caching, batching, and adaptive model selection techniques, Clipper\nreduces prediction latency and improves prediction throughput, accuracy, and\nrobustness without modifying the underlying machine learning frameworks. We\nevaluate Clipper on four common machine learning benchmark datasets and\ndemonstrate its ability to meet the latency, accuracy, and throughput demands\nof online serving applications. Finally, we compare Clipper to the TensorFlow\nServing system and demonstrate that we are able to achieve comparable\nthroughput and latency while enabling model composition and online learning to\nimprove accuracy and render more robust predictions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 16:29:16 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 17:21:33 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Crankshaw", "Daniel", ""], ["Wang", "Xin", ""], ["Zhou", "Giulio", ""], ["Franklin", "Michael J.", ""], ["Gonzalez", "Joseph E.", ""], ["Stoica", "Ion", ""]]}, {"id": "1612.03117", "submitter": "Kim Peter Wabersich", "authors": "Kim Peter Wabersich and Marc Toussaint", "title": "Advancing Bayesian Optimization: The Mixed-Global-Local (MGL) Kernel and\n  Length-Scale Cool Down", "comments": "Long version of accepted NIPS BayesOpt 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Optimization (BO) has become a core method for solving expensive\nblack-box optimization problems. While much research focussed on the choice of\nthe acquisition function, we focus on online length-scale adaption and the\nchoice of kernel function. Instead of choosing hyperparameters in view of\nmaximum likelihood on past data, we propose to use the acquisition function to\ndecide on hyperparameter adaptation more robustly and in view of the future\noptimization progress. Further, we propose a particular kernel function that\nincludes non-stationarity and local anisotropy and thereby implicitly\nintegrates the efficiency of local convex optimization with global Bayesian\noptimization. Comparisons to state-of-the art BO methods underline the\nefficiency of these mechanisms on global optimization benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 18:20:00 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Wabersich", "Kim Peter", ""], ["Toussaint", "Marc", ""]]}, {"id": "1612.03132", "submitter": "Daniele Tantari", "authors": "Adriano Barra, Giuseppe Genovese, Peter Sollich, Daniele Tantari", "title": "Phase transitions in Restricted Boltzmann Machines with generic priors", "comments": "5 pages, 4 figures; extensive simulations and 2 new figures added;\n  corrected typos; added references", "journal-ref": "Phys. Rev. E 96, 042156 (2017)", "doi": "10.1103/PhysRevE.96.042156", "report-no": null, "categories": "cond-mat.dis-nn cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Generalised Restricted Boltzmann Machines with generic priors for\nunits and weights, interpolating between Boolean and Gaussian variables. We\npresent a complete analysis of the replica symmetric phase diagram of these\nsystems, which can be regarded as Generalised Hopfield models. We underline the\nrole of the retrieval phase for both inference and learning processes and we\nshow that retrieval is robust for a large class of weight and unit priors,\nbeyond the standard Hopfield scenario. Furthermore we show how the paramagnetic\nphase boundary is directly related to the optimal size of the training set\nnecessary for good generalisation in a teacher-student scenario of unsupervised\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 19:08:11 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 09:33:27 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Barra", "Adriano", ""], ["Genovese", "Giuseppe", ""], ["Sollich", "Peter", ""], ["Tantari", "Daniele", ""]]}, {"id": "1612.03147", "submitter": "Gautam Kamath", "authors": "Constantinos Daskalakis, Nishanth Dikkala, Gautam Kamath", "title": "Testing Ising Models", "comments": "Appeared SODA 2018. Final version to appear in IEEE Transactions on\n  Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples from an unknown multivariate distribution $p$, is it possible\nto distinguish whether $p$ is the product of its marginals versus $p$ being far\nfrom every product distribution? Similarly, is it possible to distinguish\nwhether $p$ equals a given distribution $q$ versus $p$ and $q$ being far from\neach other? These problems of testing independence and goodness-of-fit have\nreceived enormous attention in statistics, information theory, and theoretical\ncomputer science, with sample-optimal algorithms known in several interesting\nregimes of parameters. Unfortunately, it has also been understood that these\nproblems become intractable in large dimensions, necessitating exponential\nsample complexity.\n  Motivated by the exponential lower bounds for general distributions as well\nas the ubiquity of Markov Random Fields (MRFs) in the modeling of\nhigh-dimensional distributions, we initiate the study of distribution testing\non structured multivariate distributions, and in particular the prototypical\nexample of MRFs: the Ising Model. We demonstrate that, in this structured\nsetting, we can avoid the curse of dimensionality, obtaining sample and time\nefficient testers for independence and goodness-of-fit. One of the key\ntechnical challenges we face along the way is bounding the variance of\nfunctions of the Ising model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:04:56 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 05:34:14 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 15:06:28 GMT"}, {"version": "v4", "created": "Mon, 30 Oct 2017 20:34:46 GMT"}, {"version": "v5", "created": "Tue, 5 Feb 2019 16:33:20 GMT"}, {"version": "v6", "created": "Wed, 10 Jul 2019 22:02:20 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Dikkala", "Nishanth", ""], ["Kamath", "Gautam", ""]]}, {"id": "1612.03148", "submitter": "Anindya De", "authors": "Anindya De and Ryan O'Donnell and Rocco Servedio", "title": "Optimal mean-based algorithms for trace reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the (deletion-channel) trace reconstruction problem, there is an unknown\n$n$-bit source string $x$. An algorithm is given access to independent traces\nof $x$, where a trace is formed by deleting each bit of~$x$ independently with\nprobability~$\\delta$. The goal of the algorithm is to recover~$x$ exactly (with\nhigh probability), while minimizing samples (number of traces) and running\ntime.\n  Previously, the best known algorithm for the trace reconstruction problem was\ndue to Holenstein~et~al.; it uses $\\exp(\\tilde{O}(n^{1/2}))$ samples and\nrunning time for any fixed $0 < \\delta < 1$. It is also what we call a\n\"mean-based algorithm\", meaning that it only uses the empirical means of the\nindividual bits of the traces. Holenstein~et~al.~also gave a lower bound,\nshowing that any mean-based algorithm must use at least $n^{\\tilde{\\Omega}(\\log\nn)}$ samples.\n  In this paper we improve both of these results, obtaining matching upper and\nlower bounds for mean-based trace reconstruction. For any constant deletion\nrate $0 < \\delta < 1$, we give a mean-based algorithm that uses\n$\\exp(O(n^{1/3}))$ time and traces; we also prove that any mean-based algorithm\nmust use at least $\\exp(\\Omega(n^{1/3}))$ traces. In fact, we obtain matching\nupper and lower bounds even for $\\delta$ subconstant and $\\rho := 1-\\delta$\nsubconstant: when $(\\log^3 n)/n \\ll \\delta \\leq 1/2$ the bound is\n$\\exp(-\\Theta(\\delta n)^{1/3})$, and when $1/\\sqrt{n} \\ll \\rho \\leq 1/2$ the\nbound is $\\exp(-\\Theta(n/\\rho)^{1/3})$.\n  Our proofs involve estimates for the maxima of Littlewood polynomials on\ncomplex disks. We show that these techniques can also be used to perform trace\nreconstruction with random insertions and bit-flips in addition to deletions.\nWe also find a surprising result: for deletion probabilities $\\delta > 1/2$,\nthe presence of insertions can actually help with trace reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:05:19 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["De", "Anindya", ""], ["O'Donnell", "Ryan", ""], ["Servedio", "Rocco", ""]]}, {"id": "1612.03156", "submitter": "Cl\\'ement Canonne", "authors": "Clement Canonne, Ilias Diakonikolas, Daniel Kane, Alistair Stewart", "title": "Testing Bayesian Networks", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work initiates a systematic investigation of testing high-dimensional\nstructured distributions by focusing on testing Bayesian networks -- the\nprototypical family of directed graphical models. A Bayesian network is defined\nby a directed acyclic graph, where we associate a random variable with each\nnode. The value at any particular node is conditionally independent of all the\nother non-descendant nodes once its parents are fixed. Specifically, we study\nthe properties of identity testing and closeness testing of Bayesian networks.\nOur main contribution is the first non-trivial efficient testing algorithms for\nthese problems and corresponding information-theoretic lower bounds. For a wide\nrange of parameter settings, our testing algorithms have sample complexity\nsublinear in the dimension and are sample-optimal, up to constant factors.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:34:40 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 02:49:37 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Canonne", "Clement", ""], ["Diakonikolas", "Ilias", ""], ["Kane", "Daniel", ""], ["Stewart", "Alistair", ""]]}, {"id": "1612.03164", "submitter": "Constantinos Daskalakis", "authors": "Constantinos Daskalakis, Qinxuan Pan", "title": "Square Hellinger Subadditivity for Bayesian Networks and its\n  Applications to Identity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the square Hellinger distance between two Bayesian networks on\nthe same directed graph, $G$, is subadditive with respect to the neighborhoods\nof $G$. Namely, if $P$ and $Q$ are the probability distributions defined by two\nBayesian networks on the same DAG, our inequality states that the square\nHellinger distance, $H^2(P,Q)$, between $P$ and $Q$ is upper bounded by the\nsum, $\\sum_v H^2(P_{\\{v\\} \\cup \\Pi_v}, Q_{\\{v\\} \\cup \\Pi_v})$, of the square\nHellinger distances between the marginals of $P$ and $Q$ on every node $v$ and\nits parents $\\Pi_v$ in the DAG. Importantly, our bound does not involve the\nconditionals but the marginals of $P$ and $Q$. We derive a similar inequality\nfor more general Markov Random Fields.\n  As an application of our inequality, we show that distinguishing whether two\nBayesian networks $P$ and $Q$ on the same (but potentially unknown) DAG satisfy\n$P=Q$ vs $d_{\\rm TV}(P,Q)>\\epsilon$ can be performed from\n$\\tilde{O}(|\\Sigma|^{3/4(d+1)} \\cdot n/\\epsilon^2)$ samples, where $d$ is the\nmaximum in-degree of the DAG and $\\Sigma$ the domain of each variable of the\nBayesian networks. If $P$ and $Q$ are defined on potentially different and\npotentially unknown trees, the sample complexity becomes\n$\\tilde{O}(|\\Sigma|^{4.5} n/\\epsilon^2)$, whose dependence on $n, \\epsilon$ is\noptimal up to logarithmic factors. Lastly, if $P$ and $Q$ are product\ndistributions over $\\{0,1\\}^n$ and $Q$ is known, the sample complexity becomes\n$O(\\sqrt{n}/\\epsilon^2)$, which is optimal up to constant factors.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:58:12 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Pan", "Qinxuan", ""]]}, {"id": "1612.03186", "submitter": "Christian Grussler", "authors": "Christian Grussler and Pontus Giselsson", "title": "Low-Rank Inducing Norms with Optimality Interpretations", "comments": null, "journal-ref": "SIAM J. Optim., 28(4), 3057-3078, 2018", "doi": "10.1137/17M1115770", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems with rank constraints appear in many diverse fields\nsuch as control, machine learning and image analysis. Since the rank constraint\nis non-convex, these problems are often approximately solved via convex\nrelaxations. Nuclear norm regularization is the prevailing convexifying\ntechnique for dealing with these types of problem. This paper introduces a\nfamily of low-rank inducing norms and regularizers which includes the nuclear\nnorm as a special case. A posteriori guarantees on solving an underlying rank\nconstrained optimization problem with these convex relaxations are provided. We\nevaluate the performance of the low-rank inducing norms on three matrix\ncompletion problems. In all examples, the nuclear norm heuristic is\noutperformed by convex relaxations based on other low-rank inducing norms. For\ntwo of the problems there exist low-rank inducing norms that succeed in\nrecovering the partially unknown matrix, while the nuclear norm fails. These\nlow-rank inducing norms are shown to be representable as semi-definite\nprograms. Moreover, these norms have cheaply computable proximal mappings,\nwhich makes it possible to also solve problems of large size using first-order\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 21:40:40 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 11:14:57 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Grussler", "Christian", ""], ["Giselsson", "Pontus", ""]]}, {"id": "1612.03211", "submitter": "Xiaolin Andy Li", "authors": "Rajendra Rana Bhat, Vivek Viswanath, Xiaolin Li", "title": "DeepCancer: Detecting Cancer through Gene Expressions via Deep\n  Generative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcriptional profiling on microarrays to obtain gene expressions has been\nused to facilitate cancer diagnosis. We propose a deep generative machine\nlearning architecture (called DeepCancer) that learn features from unlabeled\nmicroarray data. These models have been used in conjunction with conventional\nclassifiers that perform classification of the tissue samples as either being\ncancerous or non-cancerous. The proposed model has been tested on two different\nclinical datasets. The evaluation demonstrates that DeepCancer model achieves a\nvery high precision score, while significantly controlling the false positive\nand false negative scores.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 23:01:12 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 16:27:34 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Bhat", "Rajendra Rana", ""], ["Viswanath", "Vivek", ""], ["Li", "Xiaolin", ""]]}, {"id": "1612.03214", "submitter": "Thomas Mesnard", "authors": "Thomas Mesnard, Wulfram Gerstner, Johanni Brea", "title": "Towards deep learning with spiking neurons in energy based models with\n  contrastive Hebbian plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, error back-propagation in multi-layer neural networks\n(deep learning) has been impressively successful in supervised and\nreinforcement learning tasks. As a model for learning in the brain, however,\ndeep learning has long been regarded as implausible, since it relies in its\nbasic form on a non-local plasticity rule. To overcome this problem,\nenergy-based models with local contrastive Hebbian learning were proposed and\ntested on a classification task with networks of rate neurons. We extended this\nwork by implementing and testing such a model with networks of leaky\nintegrate-and-fire neurons. Preliminary results indicate that it is possible to\nlearn a non-linear regression task with hidden layers, spiking neurons and a\nlocal synaptic plasticity rule.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 23:17:11 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Mesnard", "Thomas", ""], ["Gerstner", "Wulfram", ""], ["Brea", "Johanni", ""]]}, {"id": "1612.03225", "submitter": "Katya Scheinberg", "authors": "Oktay Gunluk, Jayant Kalagnanam, Minhan Li, Matt Menickelly, Katya\n  Scheinberg", "title": "Optimal Generalized Decision Trees via Integer Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees have been a very popular class of predictive models for\ndecades due to their interpretability and good performance on categorical\nfeatures. However, they are not always robust and tend to overfit the data.\nAdditionally, if allowed to grow large, they lose interpretability. In this\npaper, we present a mixed integer programming formulation to construct optimal\ndecision trees of a prespecified size. We take the special structure of\ncategorical features into account and allow combinatorial decisions (based on\nsubsets of values of features) at each node. Our approach can also handle\nnumerical features via thresholding. We show that very good accuracy can be\nachieved with small trees using moderately-sized training sets. The\noptimization problems we solve are tractable with modern solvers.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 00:05:37 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 20:56:14 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 17:19:17 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Gunluk", "Oktay", ""], ["Kalagnanam", "Jayant", ""], ["Li", "Minhan", ""], ["Menickelly", "Matt", ""], ["Scheinberg", "Katya", ""]]}, {"id": "1612.03226", "submitter": "Jiaji Huang Dr.", "authors": "Jiaji Huang, Rewon Child, Vinay Rao, Hairong Liu, Sanjeev Satheesh,\n  Adam Coates", "title": "Active Learning for Speech Recognition: the Power of Gradients", "comments": "published as a workshop paper at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In training speech recognition systems, labeling audio clips can be\nexpensive, and not all data is equally valuable. Active learning aims to label\nonly the most informative samples to reduce cost. For speech recognition,\nconfidence scores and other likelihood-based active learning methods have been\nshown to be effective. Gradient-based active learning methods, however, are\nstill not well-understood. This work investigates the Expected Gradient Length\n(EGL) approach in active learning for end-to-end speech recognition. We justify\nEGL from a variance reduction perspective, and observe that EGL's measure of\ninformativeness picks novel samples uncorrelated with confidence scores.\nExperimentally, we show that EGL can reduce word errors by 11\\%, or\nalternatively, reduce the number of samples to label by 50\\%, when compared to\nrandom sampling.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 00:09:45 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Huang", "Jiaji", ""], ["Child", "Rewon", ""], ["Rao", "Vinay", ""], ["Liu", "Hairong", ""], ["Satheesh", "Sanjeev", ""], ["Coates", "Adam", ""]]}, {"id": "1612.03268", "submitter": "Venkataraman Santhanam", "authors": "Venkataraman Santhanam, Vlad I. Morariu, Larry S. Davis", "title": "Generalized Deep Image to Image Regression", "comments": "Submitted to CVPR on November 15th, 2016. Code will be made available\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Deep Convolutional Neural Network architecture which serves as a\ngeneric image-to-image regressor that can be trained end-to-end without any\nfurther machinery. Our proposed architecture: the Recursively Branched\nDeconvolutional Network (RBDN) develops a cheap multi-context image\nrepresentation very early on using an efficient recursive branching scheme with\nextensive parameter sharing and learnable upsampling. This multi-context\nrepresentation is subjected to a highly non-linear locality preserving\ntransformation by the remainder of our network comprising of a series of\nconvolutions/deconvolutions without any spatial downsampling. The RBDN\narchitecture is fully convolutional and can handle variable sized images during\ninference. We provide qualitative/quantitative results on $3$ diverse tasks:\nrelighting, denoising and colorization and show that our proposed RBDN\narchitecture obtains comparable results to the state-of-the-art on each of\nthese tasks when used off-the-shelf without any post processing or\ntask-specific architectural modifications.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 08:22:27 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Santhanam", "Venkataraman", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1612.03301", "submitter": "Qi Lei", "authors": "Rashish Tandon, Qi Lei, Alexandros G. Dimakis and Nikos Karampatziakis", "title": "Gradient Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel coding theoretic framework for mitigating stragglers in\ndistributed learning. We show how carefully replicating data blocks and coding\nacross gradients can provide tolerance to failures and stragglers for\nSynchronous Gradient Descent. We implement our schemes in python (using MPI) to\nrun on Amazon EC2, and show how we compare against baseline approaches in\nrunning time and generalization error.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 14:25:00 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 01:00:33 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Tandon", "Rashish", ""], ["Lei", "Qi", ""], ["Dimakis", "Alexandros G.", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1612.03328", "submitter": "Pedram Daee", "authors": "Pedram Daee, Tomi Peltola, Marta Soare, Samuel Kaski", "title": "Knowledge Elicitation via Sequential Probabilistic Inference for\n  High-Dimensional Prediction", "comments": "22 pages, 9 figures. The paper is published in Machine Learning\n  journal (http://rdcu.be/t9KF). Codes and data available at\n  https://github.com/HIIT/knowledge-elicitation-for-linear-regression, Machine\n  Learning, (2017)", "journal-ref": null, "doi": "10.1007/s10994-017-5651-7", "report-no": null, "categories": "cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction in a small-sized sample with a large number of covariates, the\n\"small n, large p\" problem, is challenging. This setting is encountered in\nmultiple applications, such as precision medicine, where obtaining additional\nsamples can be extremely costly or even impossible, and extensive research\neffort has recently been dedicated to finding principled solutions for accurate\nprediction. However, a valuable source of additional information, domain\nexperts, has not yet been efficiently exploited. We formulate knowledge\nelicitation generally as a probabilistic inference process, where expert\nknowledge is sequentially queried to improve predictions. In the specific case\nof sparse linear regression, where we assume the expert has knowledge about the\nvalues of the regression coefficients or about the relevance of the features,\nwe propose an algorithm and computational approximation for fast and efficient\ninteraction, which sequentially identifies the most informative features on\nwhich to query expert knowledge. Evaluations of our method in experiments with\nsimulated and real users show improved prediction accuracy already with a small\neffort from the expert.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 18:11:32 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 13:08:56 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Daee", "Pedram", ""], ["Peltola", "Tomi", ""], ["Soare", "Marta", ""], ["Kaski", "Samuel", ""]]}, {"id": "1612.03349", "submitter": "Zheng Xu", "authors": "Zheng Xu, Soham De, Mario Figueiredo, Christoph Studer, Tom Goldstein", "title": "An Empirical Study of ADMM for Nonconvex Problems", "comments": "NIPS nonconvex workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) is a common\noptimization tool for solving constrained and non-differentiable problems. We\nprovide an empirical study of the practical performance of ADMM on several\nnonconvex applications, including l0 regularized linear regression, l0\nregularized image denoising, phase retrieval, and eigenvector computation. Our\nexperiments suggest that ADMM performs well on a broad class of non-convex\nproblems. Moreover, recently proposed adaptive ADMM methods, which\nautomatically tune penalty parameters as the method runs, can improve algorithm\nefficiency and solution quality compared to ADMM with a non-tuned penalty.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 22:18:18 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Xu", "Zheng", ""], ["De", "Soham", ""], ["Figueiredo", "Mario", ""], ["Studer", "Christoph", ""], ["Goldstein", "Tom", ""]]}, {"id": "1612.03350", "submitter": "Zheng Xu", "authors": "Zheng Xu, Furong Huang, Louiqa Raschid, Tom Goldstein", "title": "Non-negative Factorization of the Occurrence Tensor from Financial\n  Contracts", "comments": "NIPS tensor workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for the non-negative factorization of an occurrence\ntensor built from heterogeneous networks. We use l0 norm to model sparse errors\nover discrete values (occurrences), and use decomposed factors to model the\nembedded groups of nodes. An efficient splitting method is developed to\noptimize the nonconvex and nonsmooth objective. We study both synthetic\nproblems and a new dataset built from financial documents, resMBS.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 22:26:30 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Xu", "Zheng", ""], ["Huang", "Furong", ""], ["Raschid", "Louiqa", ""], ["Goldstein", "Tom", ""]]}, {"id": "1612.03364", "submitter": "Baojian Zhou", "authors": "Feng Chen, Baojian Zhou", "title": "Technical Report: A Generalized Matching Pursuit Approach for\n  Graph-Structured Sparsity", "comments": "International Joint Conference on Artificial Intelligence, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sparsity-constrained optimization is an important and challenging problem\nthat has wide applicability in data mining, machine learning, and statistics.\nIn this paper, we focus on sparsity-constrained optimization in cases where the\ncost function is a general nonlinear function and, in particular, the sparsity\nconstraint is defined by a graph-structured sparsity model. Existing methods\nexplore this problem in the context of sparse estimation in linear models. To\nthe best of our knowledge, this is the first work to present an efficient\napproximation algorithm, namely, Graph-structured Matching Pursuit (Graph-Mp),\nto optimize a general nonlinear function subject to graph-structured\nconstraints. We prove that our algorithm enjoys the strong guarantees analogous\nto those designed for linear models in terms of convergence rate and\napproximation accuracy. As a case study, we specialize Graph-Mp to optimize a\nnumber of well-known graph scan statistic models for the connected subgraph\ndetection task, and empirical evidence demonstrates that our general algorithm\nperforms superior over state-of-the-art methods that are designed specifically\nfor the task of connected subgraph detection.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 01:57:50 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Chen", "Feng", ""], ["Zhou", "Baojian", ""]]}, {"id": "1612.03412", "submitter": "Yochai Blau", "authors": "Yochai Blau and Tomer Michaeli", "title": "Non-Redundant Spectral Dimensionality Reduction", "comments": null, "journal-ref": "European Conference on Machine Learning and Knowledge Discovery in\n  Databases (ECML PKDD), Part I, LNAI 10534, pp. 256-271, 2017", "doi": "10.1007/978-3-319-71249-9_16", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral dimensionality reduction algorithms are widely used in numerous\ndomains, including for recognition, segmentation, tracking and visualization.\nHowever, despite their popularity, these algorithms suffer from a major\nlimitation known as the \"repeated Eigen-directions\" phenomenon. That is, many\nof the embedding coordinates they produce typically capture the same direction\nalong the data manifold. This leads to redundant and inefficient\nrepresentations that do not reveal the true intrinsic dimensionality of the\ndata. In this paper, we propose a general method for avoiding redundancy in\nspectral algorithms. Our approach relies on replacing the orthogonality\nconstraints underlying those methods by unpredictability constraints.\nSpecifically, we require that each embedding coordinate be unpredictable (in\nthe statistical sense) from all previous ones. We prove that these constraints\nnecessarily prevent redundancy, and provide a simple technique to incorporate\nthem into existing methods. As we illustrate on challenging high-dimensional\nscenarios, our approach produces significantly more informative and compact\nrepresentations, which improve visualization and classification tasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 14:04:33 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 12:58:06 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Blau", "Yochai", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1612.03441", "submitter": "Zhao Shen-Yi", "authors": "Shen-Yi Zhao, Gong-Duo Zhang, Wu-Jun Li", "title": "Lock-Free Optimization for Non-Convex Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent~(SGD) and its variants have attracted much\nattention in machine learning due to their efficiency and effectiveness for\noptimization. To handle large-scale problems, researchers have recently\nproposed several lock-free strategy based parallel SGD~(LF-PSGD) methods for\nmulti-core systems. However, existing works have only proved the convergence of\nthese LF-PSGD methods for convex problems. To the best of our knowledge, no\nwork has proved the convergence of the LF-PSGD methods for non-convex problems.\nIn this paper, we provide the theoretical proof about the convergence of two\nrepresentative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems.\nEmpirical results also show that both Hogwild! and AsySVRG are convergent on\nnon-convex problems, which successfully verifies our theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 17:26:43 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Zhao", "Shen-Yi", ""], ["Zhang", "Gong-Duo", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1612.03450", "submitter": "Michael Tschannen", "authors": "Michael Tschannen and Helmut B\\\"olcskei", "title": "Noisy subspace clustering via matching pursuits", "comments": "24 pages, 5 figures", "journal-ref": "IEEE Transactions on Information Theory, Vol. 64, No. 6, pp.\n  4081-4104, June 2018", "doi": "10.1109/TIT.2018.2812824", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-based subspace clustering algorithms have attracted significant\nattention thanks to their excellent performance in practical applications. A\nprominent example is the sparse subspace clustering (SSC) algorithm by\nElhamifar and Vidal, which performs spectral clustering based on an adjacency\nmatrix obtained by sparsely representing each data point in terms of all the\nother data points via the Lasso. When the number of data points is large or the\ndimension of the ambient space is high, the computational complexity of SSC\nquickly becomes prohibitive. Dyer et al. observed that SSC-OMP obtained by\nreplacing the Lasso by the greedy orthogonal matching pursuit (OMP) algorithm\nresults in significantly lower computational complexity, while often yielding\ncomparable performance. The central goal of this paper is an analytical\nperformance characterization of SSC-OMP for noisy data. Moreover, we introduce\nand analyze the SSC-MP algorithm, which employs matching pursuit (MP) in lieu\nof OMP. Both SSC-OMP and SSC-MP are proven to succeed even when the subspaces\nintersect and when the data points are contaminated by severe noise. The\nclustering conditions we obtain for SSC-OMP and SSC-MP are similar to those for\nSSC and for the thresholding-based subspace clustering (TSC) algorithm due to\nHeckel and B\\\"olcskei. Analytical results in combination with numerical results\nindicate that both SSC-OMP and SSC-MP with a data-dependent stopping criterion\nautomatically detect the dimensions of the subspaces underlying the data.\nMoreover, experiments on synthetic and on real data show that SSC-MP compares\nvery favorably to SSC, SSC-OMP, TSC, and the nearest subspace neighbor\nalgorithm, both in terms of clustering performance and running time. In\naddition, we find that, in contrast to SSC-OMP, the performance of SSC-MP is\nvery robust with respect to the choice of parameters in the stopping criteria.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 18:33:31 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 12:34:29 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Tschannen", "Michael", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1612.03480", "submitter": "Cengiz Pehlevan", "authors": "Yuansi Chen, Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "Self-calibrating Neural Networks for Dimensionality Reduction", "comments": "2016 Asilomar Conference on Signals, Systems and Computers", "journal-ref": null, "doi": "10.1109/ACSSC.2016.7869625", "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a novel family of biologically plausible online algorithms for\nreducing the dimensionality of streaming data has been derived from the\nsimilarity matching principle. In these algorithms, the number of output\ndimensions can be determined adaptively by thresholding the singular values of\nthe input data matrix. However, setting such threshold requires knowing the\nmagnitude of the desired singular values in advance. Here we propose online\nalgorithms where the threshold is self-calibrating based on the singular values\ncomputed from the existing observations. To derive these algorithms from the\nsimilarity matching cost function we propose novel regularizers. As before,\nthese online algorithms can be implemented by Hebbian/anti-Hebbian neural\nnetworks in which the learning rule depends on the chosen regularizer. We\ndemonstrate both mathematically and via simulation the effectiveness of these\nonline algorithms in various settings.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 21:15:05 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Chen", "Yuansi", ""], ["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1612.03615", "submitter": "Daniel Romero", "authors": "Daniel Romero, Vassilis N. Ioannidis, Georgios B. Giannakis", "title": "Kernel-based Reconstruction of Space-time Functions on Dynamic Graphs", "comments": "Submitted to IEEE Journal of Selected Topics in Signal processing,\n  Oct. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods pervade the inference toolkits of numerous disciplines\nincluding sociology, biology, neuroscience, physics, chemistry, and\nengineering. A challenging problem encountered in this context pertains to\ndetermining the attributes of a set of vertices given those of another subset\nat possibly different time instants. Leveraging spatiotemporal dynamics can\ndrastically reduce the number of observed vertices, and hence the cost of\nsampling. Alleviating the limited flexibility of existing approaches, the\npresent paper broadens the existing kernel-based graph function reconstruction\nframework to accommodate time-evolving functions over possibly time-evolving\ntopologies. This approach inherits the versatility and generality of\nkernel-based methods, for which no knowledge on distributions or second-order\nstatistics is required. Systematic guidelines are provided to construct two\nfamilies of space-time kernels with complementary strengths. The first\nfacilitates judicious control of regularization on a space-time frequency\nplane, whereas the second can afford time-varying topologies. Batch and online\nestimators are also put forth, and a novel kernel Kalman filter is developed to\nobtain these estimates at affordable computational cost. Numerical tests with\nreal data sets corroborate the merits of the proposed methods relative to\ncompeting alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 11:12:00 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 08:29:08 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Romero", "Daniel", ""], ["Ioannidis", "Vassilis N.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1612.03651", "submitter": "Armand Joulin", "authors": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze,\n  H\\'erve J\\'egou, Tomas Mikolov", "title": "FastText.zip: Compressing text classification models", "comments": "Submitted to ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of producing compact architectures for text\nclassification, such that the full model fits in a limited amount of memory.\nAfter considering different solutions inspired by the hashing literature, we\npropose a method built upon product quantization to store word embeddings.\nWhile the original technique leads to a loss in accuracy, we adapt this method\nto circumvent quantization artefacts. Our experiments carried out on several\nbenchmarks show that our approach typically requires two orders of magnitude\nless memory than fastText while being only slightly inferior with respect to\naccuracy. As a result, it outperforms the state of the art by a good margin in\nterms of the compromise between memory usage and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 12:51:03 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Joulin", "Armand", ""], ["Grave", "Edouard", ""], ["Bojanowski", "Piotr", ""], ["Douze", "Matthijs", ""], ["J\u00e9gou", "H\u00e9rve", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1612.03663", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein, and Bernt Schiele", "title": "Analysis and Optimization of Loss Functions for Multiclass, Top-k, and\n  Multilabel Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k error is currently a popular performance measure on large scale image\nclassification benchmarks such as ImageNet and Places. Despite its wide\nacceptance, our understanding of this metric is limited as most of the previous\nresearch is focused on its special case, the top-1 error. In this work, we\nexplore two directions that shed more light on the top-k error. First, we\nprovide an in-depth analysis of established and recently proposed single-label\nmulticlass methods along with a detailed account of efficient optimization\nalgorithms for them. Our results indicate that the softmax loss and the smooth\nmulticlass SVM are surprisingly competitive in top-k error uniformly across all\nk, which can be explained by our analysis of multiclass top-k calibration.\nFurther improvements for a specific k are possible with a number of proposed\ntop-k loss functions. Second, we use the top-k methods to explore the\ntransition from multiclass to multilabel learning. In particular, we find that\nit is possible to obtain effective multilabel classifiers on Pascal VOC using a\nsingle label per image for training, while the gap between multiclass and\nmultilabel methods on MS COCO is more significant. Finally, our contribution of\nefficient algorithms for training with the considered top-k and multilabel loss\nfunctions is of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 13:20:09 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1612.03770", "submitter": "James Aimone", "authors": "Timothy J. Draelos, Nadine E. Miner, Christopher C. Lamb, Jonathan A.\n  Cox, Craig M. Vineyard, Kristofor D. Carlson, William M. Severa, Conrad D.\n  James, and James B. Aimone", "title": "Neurogenesis Deep Learning", "comments": "8 pages, 8 figures, Accepted to 2017 International Joint Conference\n  on Neural Networks (IJCNN 2017)", "journal-ref": null, "doi": "10.1109/IJCNN.2017.7965898", "report-no": "SAND2017-2174 C", "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine learning methods, such as deep neural networks (DNN), have\nachieved remarkable success in a number of complex data processing tasks. These\nmethods have arguably had their strongest impact on tasks such as image and\naudio processing - data processing domains in which humans have long held clear\nadvantages over conventional algorithms. In contrast to biological neural\nsystems, which are capable of learning continuously, deep artificial networks\nhave a limited ability for incorporating new information in an already trained\nnetwork. As a result, methods for continuous learning are potentially highly\nimpactful in enabling the application of deep networks to dynamic data sets.\nHere, inspired by the process of adult neurogenesis in the hippocampus, we\nexplore the potential for adding new neurons to deep layers of artificial\nneural networks in order to facilitate their acquisition of novel information\nwhile preserving previously trained data representations. Our results on the\nMNIST handwritten digit dataset and the NIST SD 19 dataset, which includes\nlower and upper case letters and digits, demonstrate that neurogenesis is well\nsuited for addressing the stability-plasticity dilemma that has long challenged\nadaptive machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 16:25:23 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 16:45:11 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Draelos", "Timothy J.", ""], ["Miner", "Nadine E.", ""], ["Lamb", "Christopher C.", ""], ["Cox", "Jonathan A.", ""], ["Vineyard", "Craig M.", ""], ["Carlson", "Kristofor D.", ""], ["Severa", "William M.", ""], ["James", "Conrad D.", ""], ["Aimone", "James B.", ""]]}, {"id": "1612.03780", "submitter": "Ludovic Hofer", "authors": "Ludovic Hofer, Hugo Gimbert", "title": "Online Reinforcement Learning for Real-Time Exploration in Continuous\n  State and Action Markov Decision Processes", "comments": null, "journal-ref": "ICAPS 26th, PlanRob 4th (Workshop) (2016) 37-48", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method to learn online policies in continuous\nstate, continuous action, model-free Markov decision processes, with two\nproperties that are crucial for practical applications. First, the policies are\nimplementable with a very low computational cost: once the policy is computed,\nthe action corresponding to a given state is obtained in logarithmic time with\nrespect to the number of samples used. Second, our method is versatile: it does\nnot rely on any a priori knowledge of the structure of optimal policies. We\nbuild upon the Fitted Q-iteration algorithm which represents the $Q$-value as\nthe average of several regression trees. Our algorithm, the Fitted Policy\nForest algorithm (FPF), computes a regression forest representing the Q-value\nand transforms it into a single tree representing the policy, while keeping\ncontrol on the size of the policy using resampling and leaf merging. We\nintroduce an adaptation of Multi-Resolution Exploration (MRE) which is\nparticularly suited to FPF. We assess the performance of FPF on three classical\nbenchmarks for reinforcement learning: the \"Inverted Pendulum\", the \"Double\nIntegrator\" and \"Car on the Hill\" and show that FPF equals or outperforms other\nalgorithms, although these algorithms rely on the use of particular\nrepresentations of the policies, especially chosen in order to fit each of the\nthree problems. Finally, we exhibit that the combination of FPF and MRE allows\nto find nearly optimal solutions in problems where $\\epsilon$-greedy approaches\nwould fail.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 16:52:01 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Hofer", "Ludovic", ""], ["Gimbert", "Hugo", ""]]}, {"id": "1612.03789", "submitter": "Mason Bretan", "authors": "Mason Bretan, Gil Weinberg, and Larry Heck", "title": "A Unit Selection Methodology for Music Generation Using Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods exist for a computer to generate music based on data\nincluding Markov chains, recurrent neural networks, recombinancy, and grammars.\nWe explore the use of unit selection and concatenation as a means of generating\nmusic using a procedure based on ranking, where, we consider a unit to be a\nvariable length number of measures of music. We first examine whether a unit\nselection method, that is restricted to a finite size unit library, can be\nsufficient for encompassing a wide spectrum of music. We do this by developing\na deep autoencoder that encodes a musical input and reconstructs the input by\nselecting from the library. We then describe a generative model that combines a\ndeep structured semantic model (DSSM) with an LSTM to predict the next unit,\nwhere units consist of four, two, and one measures of music. We evaluate the\ngenerative model using objective metrics including mean rank and accuracy and\nwith a subjective listening test in which expert musicians are asked to\ncomplete a forced-choiced ranking task. We compare our model to a note-level\ngenerative baseline that consists of a stacked LSTM trained to predict forward\nby one note.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 17:06:19 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Bretan", "Mason", ""], ["Weinberg", "Gil", ""], ["Heck", "Larry", ""]]}, {"id": "1612.03809", "submitter": "Mehdi Mirza", "authors": "Mehdi Mirza, Aaron Courville, Yoshua Bengio", "title": "Generalizable Features From Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans learn a predictive model of the world and use this model to reason\nabout future events and the consequences of actions. In contrast to most\nmachine predictors, we exhibit an impressive ability to generalize to unseen\nscenarios and reason intelligently in these settings. One important aspect of\nthis ability is physical intuition(Lake et al., 2016). In this work, we explore\nthe potential of unsupervised learning to find features that promote better\ngeneralization to settings outside the supervised training distribution. Our\ntask is predicting the stability of towers of square blocks. We demonstrate\nthat an unsupervised model, trained to predict future frames of a video\nsequence of stable and unstable block configurations, can yield features that\nsupport extrapolating stability prediction to blocks configurations outside the\ntraining set distribution\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 17:45:48 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Mirza", "Mehdi", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1612.03839", "submitter": "Yun S. Song", "authors": "Miaoyan Wang and Yun S. Song", "title": "Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD)", "comments": "33 pages, 5 figures", "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics (AISTATS), PMLR, Vol. 54 (2017) 614-622", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decompositions have rich applications in statistics and machine\nlearning, and developing efficient, accurate algorithms for the problem has\nreceived much attention recently. Here, we present a new method built on\nKruskal's uniqueness theorem to decompose symmetric, nearly orthogonally\ndecomposable tensors. Unlike the classical higher-order singular value\ndecomposition which unfolds a tensor along a single mode, we consider\nunfoldings along two modes and use rank-1 constraints to characterize the\nunderlying components. This tensor decomposition method provably handles a\ngreater level of noise compared to previous methods and achieves a high\nestimation accuracy. Numerical results demonstrate that our algorithm is robust\nto various noise distributions and that it performs especially favorably as the\norder increases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 18:38:40 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 01:08:35 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Wang", "Miaoyan", ""], ["Song", "Yun S.", ""]]}, {"id": "1612.03871", "submitter": "Ashish Sabharwal", "authors": "Hanie Sedghi and Ashish Sabharwal", "title": "Knowledge Completion for Generics using Guided Tensor Factorization", "comments": "To appear in TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a knowledge base or KB containing (noisy) facts about common nouns or\ngenerics, such as \"all trees produce oxygen\" or \"some animals live in forests\",\nwe consider the problem of inferring additional such facts at a precision\nsimilar to that of the starting KB. Such KBs capture general knowledge about\nthe world, and are crucial for various applications such as question answering.\nDifferent from commonly studied named entity KBs such as Freebase, generics KBs\ninvolve quantification, have more complex underlying regularities, tend to be\nmore incomplete, and violate the commonly used locally closed world assumption\n(LCWA). We show that existing KB completion methods struggle with this new\ntask, and present the first approach that is successful. Our results\ndemonstrate that external information, such as relation schemas and entity\ntaxonomies, if used appropriately, can be a surprisingly powerful tool in this\nsetting. First, our simple yet effective knowledge guided tensor factorization\napproach achieves state-of-the-art results on two generics KBs (80% precise)\nfor science, doubling their size at 74%-86% precision. Second, our novel\ntaxonomy guided, submodular, active learning method for collecting annotations\nabout rare entities (e.g., oriole, a bird) is 6x more effective at inferring\nfurther new facts about them than multiple active learning baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 19:53:04 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 09:08:44 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 18:58:58 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Sedghi", "Hanie", ""], ["Sabharwal", "Ashish", ""]]}, {"id": "1612.03897", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Simon Lucey", "title": "Inverse Compositional Spatial Transformer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish a theoretical connection between the classical\nLucas & Kanade (LK) algorithm and the emerging topic of Spatial Transformer\nNetworks (STNs). STNs are of interest to the vision and learning communities\ndue to their natural ability to combine alignment and classification within the\nsame theoretical framework. Inspired by the Inverse Compositional (IC) variant\nof the LK algorithm, we present Inverse Compositional Spatial Transformer\nNetworks (IC-STNs). We demonstrate that IC-STNs can achieve better performance\nthan conventional STNs with less model capacity; in particular, we show\nsuperior performance in pure image alignment tasks as well as joint\nalignment/classification problems on real-world problems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 20:53:05 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Lucey", "Simon", ""]]}, {"id": "1612.03902", "submitter": "Denise M. Reeves", "authors": "Denise M. Reeves", "title": "Design of Data-Driven Mathematical Laws for Optimal Statistical\n  Classification Systems", "comments": "339 pages, 52 figures. arXiv admin note: text overlap with\n  arXiv:1511.05102", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article will devise data-driven, mathematical laws that generate\noptimal, statistical classification systems which achieve minimum error rates\nfor data distributions with unchanging statistics. Thereby, I will design\nlearning machines that minimize the expected risk or probability of\nmisclassification. I will devise a system of fundamental equations of binary\nclassification for a classification system in statistical equilibrium. I will\nuse this system of equations to formulate the problem of learning unknown,\nlinear and quadratic discriminant functions from data as a locus problem,\nthereby formulating geometric locus methods within a statistical framework.\nSolving locus problems involves finding equations of curves or surfaces defined\nby given properties and finding graphs or loci of given equations. I will\ndevise three systems of data-driven, locus equations that generate optimal,\nstatistical classification systems. Each class of learning machines satisfies\nfundamental statistical laws for a classification system in statistical\nequilibrium. Thereby, I will formulate three classes of learning machines that\nare scalable modules for optimal, statistical pattern recognition systems, all\nof which are capable of performing a wide variety of statistical pattern\nrecognition tasks, where any given M-class statistical pattern recognition\nsystem exhibits optimal generalization performance for an M-class feature\nspace.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 20:58:37 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 02:22:40 GMT"}, {"version": "v3", "created": "Fri, 23 Dec 2016 22:31:07 GMT"}, {"version": "v4", "created": "Fri, 30 Dec 2016 19:57:57 GMT"}, {"version": "v5", "created": "Thu, 23 Feb 2017 18:57:56 GMT"}, {"version": "v6", "created": "Wed, 22 Mar 2017 21:46:51 GMT"}, {"version": "v7", "created": "Sat, 29 Apr 2017 23:10:57 GMT"}, {"version": "v8", "created": "Fri, 22 Sep 2017 22:57:41 GMT"}, {"version": "v9", "created": "Sat, 19 May 2018 04:47:21 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Reeves", "Denise M.", ""]]}, {"id": "1612.03948", "submitter": "Boian Alexandrov S", "authors": "Valentin G. Stanev, Filip L. Iliev, Scott Hansen, Velimir V.\n  Vesselinov, Boian S. Alexandrov", "title": "Identification of release sources in advection-diffusion system by\n  machine learning combined with Green function inverse method", "comments": null, "journal-ref": null, "doi": "10.1016/j.apm.2018.03.006", "report-no": "LA-UR-16-27231", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of sources of advection-diffusion transport is based\nusually on solving complex ill-posed inverse models against the available\nstate- variable data records. However, if there are several sources with\ndifferent locations and strengths, the data records represent mixtures rather\nthan the separate influences of the original sources. Importantly, the number\nof these original release sources is typically unknown, which hinders\nreliability of the classical inverse-model analyses. To address this challenge,\nwe present here a novel hybrid method for identification of the unknown number\nof release sources. Our hybrid method, called HNMF, couples unsupervised\nlearning based on Nonnegative Matrix Factorization (NMF) and inverse-analysis\nGreen functions method. HNMF synergistically performs decomposition of the\nrecorded mixtures, finds the number of the unknown sources and uses the Green\nfunction of advection-diffusion equation to identify their characteristics. In\nthe paper, we introduce the method and demonstrate that it is capable of\nidentifying the advection velocity and dispersivity of the medium as well as\nthe unknown number, locations, and properties of various sets of synthetic\nrelease sources with different space and time dependencies, based only on the\nrecorded data. HNMF can be applied directly to any problem controlled by a\npartial-differential parabolic equation where mixtures of an unknown number of\nsources are measured at multiple locations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 22:05:10 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 05:02:43 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 22:13:59 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Stanev", "Valentin G.", ""], ["Iliev", "Filip L.", ""], ["Hansen", "Scott", ""], ["Vesselinov", "Velimir V.", ""], ["Alexandrov", "Boian S.", ""]]}, {"id": "1612.03950", "submitter": "Boian Alexandrov S", "authors": "Filip L. Iliev, Valentin G. Stanev, Velimir V. Vesselinov, Boian S.\n  Alexandrov", "title": "Nonnegative Matrix Factorization for identification of unknown number of\n  sources emitting delayed signals", "comments": null, "journal-ref": "PloS one. 2018 Mar 8;13(3):e0193974", "doi": null, "report-no": "LA-UR-16-27232", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis is broadly used as a powerful unsupervised machine learning\ntool for reconstruction of hidden features in recorded mixtures of signals. In\nthe case of a linear approximation, the mixtures can be decomposed by a variety\nof model-free Blind Source Separation (BSS) algorithms. Most of the available\nBSS algorithms consider an instantaneous mixing of signals, while the case when\nthe mixtures are linear combinations of signals with delays is less explored.\nEspecially difficult is the case when the number of sources of the signals with\ndelays is unknown and has to be determined from the data as well. To address\nthis problem, in this paper, we present a new method based on Nonnegative\nMatrix Factorization (NMF) that is capable of identifying: (a) the unknown\nnumber of the sources, (b) the delays and speed of propagation of the signals,\nand (c) the locations of the sources. Our method can be used to decompose\nrecords of mixtures of signals with delays emitted by an unknown number of\nsources in a nondispersive medium, based only on recorded data. This is the\ncase, for example, when electromagnetic signals from multiple antennas are\nreceived asynchronously; or mixtures of acoustic or seismic signals recorded by\nsensors located at different positions; or when a shift in frequency is induced\nby the Doppler effect. By applying our method to synthetic datasets, we\ndemonstrate its ability to identify the unknown number of sources as well as\nthe waveforms, the delays, and the strengths of the signals. Using Bayesian\nanalysis, we also evaluate estimation uncertainties and identify the region of\nlikelihood where the positions of the sources can be found.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 22:21:41 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 21:50:23 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Iliev", "Filip L.", ""], ["Stanev", "Valentin G.", ""], ["Vesselinov", "Velimir V.", ""], ["Alexandrov", "Boian S.", ""]]}, {"id": "1612.03981", "submitter": "Brett Israelsen", "authors": "Brett Israelsen and Nisar Ahmed", "title": "Hybrid Repeat/Multi-point Sampling for Highly Volatile Objective\n  Functions", "comments": null, "journal-ref": "BayesOpt Workshop, NIPS 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key drawback of the current generation of artificial decision-makers is\nthat they do not adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, will optimize behavior with respect to an\nobjective function that must be evaluated and learned through simulations. Once\nthis objective function has been modeled, the agent can then choose its desired\nbehavior in different situations. Bayesian optimization with a Gaussian Process\nsurrogate is used as the method for investigating the objective function. One\nkey benefit is that during optimization the Gaussian Process learns a global\nestimate of the true objective function, with predicted outcomes and a\nstatistical measure of confidence in areas that haven't been investigated yet.\nHowever, standard Bayesian optimization does not perform consistently or\nprovide an accurate Gaussian Process surrogate function for highly volatile\nobjective functions. We treat these problems by introducing a novel sampling\ntechnique called Hybrid Repeat/Multi-point Sampling. This technique gives the\nAI ability to learn optimum behaviors in a highly uncertain environment. More\nimportantly, it not only improves the reliability of the optimization, but also\ncreates a better model of the entire objective surface. With this improved\nmodel the agent is equipped to better adapt behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 00:21:45 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Israelsen", "Brett", ""], ["Ahmed", "Nisar", ""]]}, {"id": "1612.04010", "submitter": "Daniel Jiwoong Im", "authors": "Daniel Jiwoong Im, Michael Tao, Kristin Branson", "title": "An empirical analysis of the optimization of deep network loss surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks hinges on our ability to accurately and\nefficiently optimize high-dimensional, non-convex functions. In this paper, we\nempirically investigate the loss functions of state-of-the-art networks, and\nhow commonly-used stochastic gradient descent variants optimize these loss\nfunctions. To do this, we visualize the loss function by projecting them down\nto low-dimensional spaces chosen based on the convergence points of different\noptimization algorithms. Our observations suggest that optimization algorithms\nencounter and choose different descent directions at many saddle points to find\ndifferent final weights. Based on consistency we observe across re-runs of the\nsame stochastic optimization algorithm, we hypothesize that each optimization\nalgorithm makes characteristic choices at these saddle points.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 03:27:11 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 22:30:14 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 01:42:38 GMT"}, {"version": "v4", "created": "Thu, 7 Dec 2017 22:39:41 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Tao", "Michael", ""], ["Branson", "Kristin", ""]]}, {"id": "1612.04021", "submitter": "Daniel Jiwoong Im", "authors": "Daniel Jiwoong Im, He Ma, Chris Dongjoo Kim, Graham Taylor", "title": "Generative Adversarial Parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks have become one of the most studied\nframeworks for unsupervised learning due to their intuitive formulation. They\nhave also been shown to be capable of generating convincing examples in limited\ndomains, such as low-resolution images. However, they still prove difficult to\ntrain in practice and tend to ignore modes of the data generating distribution.\nQuantitatively capturing effects such as mode coverage and more generally the\nquality of the generative model still remain elusive. We propose Generative\nAdversarial Parallelization, a framework in which many GANs or their variants\nare trained simultaneously, exchanging their discriminators. This eliminates\nthe tight coupling between a generator and discriminator, leading to improved\nconvergence and improved coverage of modes. We also propose an improved variant\nof the recently proposed Generative Adversarial Metric and show how it can\nscore individual GANs or their collections under the GAP model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 04:19:04 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Ma", "He", ""], ["Kim", "Chris Dongjoo", ""], ["Taylor", "Graham", ""]]}, {"id": "1612.04022", "submitter": "Sulin Liu", "authors": "Sulin Liu, Sinno Jialin Pan, Qirong Ho", "title": "Distributed Multi-Task Relationship Learning", "comments": "To appear in KDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning aims to learn multiple tasks jointly by exploiting their\nrelatedness to improve the generalization performance for each task.\nTraditionally, to perform multi-task learning, one needs to centralize data\nfrom all the tasks to a single machine. However, in many real-world\napplications, data of different tasks may be geo-distributed over different\nlocal machines. Due to heavy communication caused by transmitting the data and\nthe issue of data privacy and security, it is impossible to send data of\ndifferent task to a master machine to perform multi-task learning. Therefore,\nin this paper, we propose a distributed multi-task learning framework that\nsimultaneously learns predictive models for each task as well as task\nrelationships between tasks alternatingly in the parameter server paradigm. In\nour framework, we first offer a general dual form for a family of regularized\nmulti-task relationship learning methods. Subsequently, we propose a\ncommunication-efficient primal-dual distributed optimization algorithm to solve\nthe dual problem by carefully designing local subproblems to make the dual\nproblem decomposable. Moreover, we provide a theoretical convergence analysis\nfor the proposed algorithm, which is specific for distributed multi-task\nrelationship learning. We conduct extensive experiments on both synthetic and\nreal-world datasets to evaluate our proposed framework in terms of\neffectiveness and convergence.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 04:22:10 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 14:09:19 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 12:00:03 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Liu", "Sulin", ""], ["Pan", "Sinno Jialin", ""], ["Ho", "Qirong", ""]]}, {"id": "1612.04035", "submitter": "Victor Dorobantu", "authors": "Victor Dorobantu, Per Andre Stromhaug, Jess Renteria", "title": "DizzyRNN: Reparameterizing Recurrent Neural Networks for Norm-Preserving\n  Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vanishing and exploding gradient problems are well-studied obstacles that\nmake it difficult for recurrent neural networks to learn long-term time\ndependencies. We propose a reparameterization of standard recurrent neural\nnetworks to update linear transformations in a provably norm-preserving way\nthrough Givens rotations. Additionally, we use the absolute value function as\nan element-wise non-linearity to preserve the norm of backpropagated signals\nover the entire network. We show that this reparameterization reduces the\nnumber of parameters and maintains the same algorithmic complexity as a\nstandard recurrent neural network, while outperforming standard recurrent\nneural networks with orthogonal initializations and Long Short-Term Memory\nnetworks on the copy problem.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 05:40:20 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Dorobantu", "Victor", ""], ["Stromhaug", "Per Andre", ""], ["Renteria", "Jess", ""]]}, {"id": "1612.04052", "submitter": "Bodo Rueckauer", "authors": "Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer", "title": "Theory and Tools for the Conversion of Analog to Spiking Convolutional\n  Neural Networks", "comments": "9 pages, 2 figures, presented at the workshop \"Computing with Spikes\"\n  at NIPS 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have shown great potential for\nnumerous real-world machine learning applications, but performing inference in\nlarge CNNs in real-time remains a challenge. We have previously demonstrated\nthat traditional CNNs can be converted into deep spiking neural networks\n(SNNs), which exhibit similar accuracy while reducing both latency and\ncomputational load as a consequence of their data-driven, event-based style of\ncomputing. Here we provide a novel theory that explains why this conversion is\nsuccessful, and derive from it several new tools to convert a larger and more\npowerful class of deep networks into SNNs. We identify the main sources of\napproximation errors in previous conversion methods, and propose simple\nmechanisms to fix these issues. Furthermore, we develop spiking implementations\nof common CNN operations such as max-pooling, softmax, and batch-normalization,\nwhich allow almost loss-less conversion of arbitrary CNN architectures into the\nspiking domain. Empirical evaluation of different network architectures on the\nMNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 07:58:34 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Rueckauer", "Bodo", ""], ["Lungu", "Iulia-Alexandra", ""], ["Hu", "Yuhuang", ""], ["Pfeiffer", "Michael", ""]]}, {"id": "1612.04056", "submitter": "Zhijian Ou", "authors": "Yiyan Wang, Haotian Xu, Zhijian Ou", "title": "Joint Bayesian Gaussian discriminant analysis for speaker verification", "comments": "accepted by ICASSP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art i-vector based speaker verification relies on variants of\nProbabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We\nare mainly motivated by the recent work of the joint Bayesian (JB) method,\nwhich is originally proposed for discriminant analysis in face verification. We\napply JB to speaker verification and make three contributions beyond the\noriginal JB. 1) In contrast to the EM iterations with approximated statistics\nin the original JB, the EM iterations with exact statistics are employed and\ngive better performance. 2) We propose to do simultaneous diagonalization (SD)\nof the within-class and between-class covariance matrices to achieve efficient\ntesting, which has broader application scope than the SVD-based efficient\ntesting method in the original JB. 3) We scrutinize similarities and\ndifferences between various Gaussian PLDAs and JB, complementing the previous\nanalysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are\nconducted on NIST SRE10 core condition 5, empirically validating the\nsuperiority of JB with faster convergence rate and 9-13% EER reduction compared\nwith state-of-the-art PLDA.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 08:13:03 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 15:33:53 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Wang", "Yiyan", ""], ["Xu", "Haotian", ""], ["Ou", "Zhijian", ""]]}, {"id": "1612.04108", "submitter": "Sam Work", "authors": "Sam Work", "title": "Corporate Disruption in the Science of Machine Learning", "comments": "MSc dissertation, qualitative analysis, machine learning researchers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This MSc dissertation considers the effects of the current corporate interest\non researchers in the field of machine learning. Situated within the field's\ncyclical history of academic, public and corporate interest, this dissertation\ninvestigates how current researchers view recent developments and negotiate\ntheir own research practices within an environment of increased commercial\ninterest and funding. The original research consists of in-depth interviews\nwith 12 machine learning researchers working in both academia and industry.\nBuilding on theory from science, technology and society studies, this\ndissertation problematizes the traditional narratives of the neoliberalization\nof academic research by allowing the researchers themselves to discuss how\ntheir career choices, working environments and interactions with others in the\nfield have been affected by the reinvigorated corporate interest of recent\nyears.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 11:54:22 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Work", "Sam", ""]]}, {"id": "1612.04111", "submitter": "Alec Koppel", "authors": "Alec Koppel, Garrett Warnell, Ethan Stump, Alejandro Ribeiro", "title": "Parsimonious Online Learning with Kernels via Sparse Projections in\n  Function Space", "comments": "Submitted to JMLR on 11/24/2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their attractiveness, popular perception is that techniques for\nnonparametric function approximation do not scale to streaming data due to an\nintractable growth in the amount of storage they require. To solve this problem\nin a memory-affordable way, we propose an online technique based on functional\nstochastic gradient descent in tandem with supervised sparsification based on\ngreedy function subspace projections. The method, called parsimonious online\nlearning with kernels (POLK), provides a controllable tradeoff? between its\nsolution accuracy and the amount of memory it requires. We derive conditions\nunder which the generated function sequence converges almost surely to the\noptimal function, and we establish that the memory requirement remains finite.\nWe evaluate POLK for kernel multi-class logistic regression and kernel\nhinge-loss classification on three canonical data sets: a synthetic Gaussian\nmixture model, the MNIST hand-written digits, and the Brodatz texture database.\nOn all three tasks, we observe a favorable tradeoff of objective function\nevaluation, classification performance, and complexity of the nonparametric\nregressor extracted the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 12:01:28 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Koppel", "Alec", ""], ["Warnell", "Garrett", ""], ["Stump", "Ethan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1612.04112", "submitter": "Naoki Hayashi", "authors": "Naoki Hayashi, Sumio Watanabe", "title": "Upper Bound of Bayesian Generalization Error in Non-negative Matrix\n  Factorization", "comments": "21 pages, 1 table. / Neurocomputing Vol. 266. / ERRATA: Proof of\n  Lemma 3.3 and Discussion is corrected", "journal-ref": "Neurocomputing, Volume 266C, 29 November 2017, pp.21-28", "doi": "10.1016/j.neucom.2017.04.068", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is a new knowledge discovery method\nthat is used for text mining, signal processing, bioinformatics, and consumer\nanalysis. However, its basic property as a learning machine is not yet\nclarified, as it is not a regular statistical model, resulting that theoretical\noptimization method of NMF has not yet established. In this paper, we study the\nreal log canonical threshold of NMF and give an upper bound of the\ngeneralization error in Bayesian learning. The results show that the\ngeneralization error of the matrix factorization can be made smaller than\nregular statistical models if Bayesian learning is applied.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 12:02:24 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 07:24:03 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 10:30:36 GMT"}, {"version": "v4", "created": "Fri, 16 Jun 2017 03:54:56 GMT"}, {"version": "v5", "created": "Sun, 1 Oct 2017 03:41:30 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Hayashi", "Naoki", ""], ["Watanabe", "Sumio", ""]]}, {"id": "1612.04118", "submitter": "pmeerkamp", "authors": "Philipp Meerkamp (Bloomberg LP) and Zhengyi Zhou (AT&T Labs Research)", "title": "Information Extraction with Character-level Neural Networks and Free\n  Noisy Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an architecture for information extraction from text that augments\nan existing parser with a character-level neural network. The network is\ntrained using a measure of consistency of extracted data with existing\ndatabases as a form of noisy supervision. Our architecture combines the ability\nof constraint-based information extraction systems to easily incorporate domain\nknowledge and constraints with the ability of deep neural networks to leverage\nlarge amounts of data to learn complex features. Boosting the existing parser's\nprecision, the system led to large improvements over a mature and highly tuned\nconstraint-based production information extraction system used at Bloomberg for\nfinancial language text.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 12:12:20 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 01:01:28 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Meerkamp", "Philipp", "", "Bloomberg LP"], ["Zhou", "Zhengyi", "", "AT&T Labs Research"]]}, {"id": "1612.04251", "submitter": "Yuan Tang", "authors": "Yuan Tang", "title": "TF.Learn: TensorFlow's High-level Module for Distributed Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 16:00:51 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Tang", "Yuan", ""]]}, {"id": "1612.04256", "submitter": "Robert Hoehndorf", "authors": "Mona Alshahrani, Mohammed Asif Khan, Omar Maddouri, Akira R Kinjo,\n  N\\'uria Queralt-Rosinach, Robert Hoehndorf", "title": "Neuro-symbolic representation learning on biological knowledge graphs", "comments": null, "journal-ref": null, "doi": "10.1093/bioinformatics/btx275", "report-no": null, "categories": "q-bio.QM cs.LG q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Biological data and knowledge bases increasingly rely on Semantic\nWeb technologies and the use of knowledge graphs for data integration,\nretrieval and federated queries. In the past years, feature learning methods\nthat are applicable to graph-structured data are becoming available, but have\nnot yet widely been applied and evaluated on structured biological knowledge.\nResults: We develop a novel method for feature learning on biological knowledge\ngraphs. Our method combines symbolic methods, in particular knowledge\nrepresentation using symbolic logic and automated reasoning, with neural\nnetworks to generate embeddings of nodes that encode for related information\nwithin knowledge graphs. Through the use of symbolic logic, these embeddings\ncontain both explicit and implicit information. We apply these embeddings to\nthe prediction of edges in the knowledge graph representing problems of\nfunction prediction, finding candidate genes of diseases, protein-protein\ninteractions, or drug target relations, and demonstrate performance that\nmatches and sometimes outperforms traditional approaches based on manually\ncrafted features. Our method can be applied to any biological knowledge graph,\nand will thereby open up the increasing amount of Semantic Web based knowledge\nbases in biology to use in machine learning and data analytics. Availability\nand Implementation:\nhttps://github.com/bio-ontology-research-group/walking-rdf-and-owl Contact:\nrobert.hoehndorf@kaust.edu.sa\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 16:06:39 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Alshahrani", "Mona", ""], ["Khan", "Mohammed Asif", ""], ["Maddouri", "Omar", ""], ["Kinjo", "Akira R", ""], ["Queralt-Rosinach", "N\u00faria", ""], ["Hoehndorf", "Robert", ""]]}, {"id": "1612.04262", "submitter": "Long-Gang Pang", "authors": "Long-Gang Pang, Kai Zhou, Nan Su, Hannah Petersen, Horst St\\\"ocker and\n  Xin-Nian Wang", "title": "An equation-of-state-meter of QCD transition from deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.LG hep-th nucl-th stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning with a deep convolutional neural network is used to\nidentify the QCD equation of state (EoS) employed in relativistic hydrodynamic\nsimulations of heavy-ion collisions from the simulated final-state particle\nspectra $\\rho(p_T,\\Phi)$. High-level correlations of $\\rho(p_T,\\Phi)$ learned\nby the neural network act as an effective \"EoS-meter\" in detecting the nature\nof the QCD transition. The EoS-meter is model independent and insensitive to\nother simulation inputs, especially the initial conditions. Thus it provides a\npowerful direct-connection of heavy-ion collision observables with the bulk\nproperties of QCD.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 16:19:00 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 23:53:50 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 01:50:06 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Pang", "Long-Gang", ""], ["Zhou", "Kai", ""], ["Su", "Nan", ""], ["Petersen", "Hannah", ""], ["St\u00f6cker", "Horst", ""], ["Wang", "Xin-Nian", ""]]}, {"id": "1612.04315", "submitter": "Brett Israelsen", "authors": "Brett W. Israelsen, Nisar Ahmed, Kenneth Center, Roderick Green,\n  Winston Bennett Jr", "title": "Towards Adaptive Training of Agent-based Sparring Partners for Fighter\n  Pilots", "comments": "submitted copy", "journal-ref": "SciTech 2017, paper 2545524", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for the current generation of artificial decision-makers is\nthat they should adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, must optimize behavior with respect to an\nobjective function that is evaluated and learned through simulations. Bayesian\noptimization with a Gaussian Process surrogate is used as the method for\ninvestigating the objective function. One key benefit is that during\noptimization, the Gaussian Process learns a global estimate of the true\nobjective function, with predicted outcomes and a statistical measure of\nconfidence in areas that haven't been investigated yet. Having a model of the\nobjective function is important for being able to understand possible outcomes\nin the decision space; for example this is crucial for training and providing\nfeedback to human pilots. However, standard Bayesian optimization does not\nperform consistently or provide an accurate Gaussian Process surrogate function\nfor highly volatile objective functions. We treat these problems by introducing\na novel sampling technique called Hybrid Repeat/Multi-point Sampling. This\ntechnique gives the AI ability to learn optimum behaviors in a highly uncertain\nenvironment. More importantly, it not only improves the reliability of the\noptimization, but also creates a better model of the entire objective surface.\nWith this improved model the agent is equipped to more accurately/efficiently\npredict performance in unexplored scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 18:43:59 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Israelsen", "Brett W.", ""], ["Ahmed", "Nisar", ""], ["Center", "Kenneth", ""], ["Green", "Roderick", ""], ["Bennett", "Winston", "Jr"]]}, {"id": "1612.04318", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier, Dushyant Rao, Ingmar Posner", "title": "Incorporating Human Domain Knowledge into Large Scale Cost Function\n  Learning", "comments": "Neural Information Processing Systems 2016, Deep Reinforcement\n  Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances have shown the capability of Fully Convolutional Neural\nNetworks (FCN) to model cost functions for motion planning in the context of\nlearning driving preferences purely based on demonstration data from human\ndrivers. While pure learning from demonstrations in the framework of Inverse\nReinforcement Learning (IRL) is a promising approach, we can benefit from well\ninformed human priors and incorporate them into the learning process. Our work\nachieves this by pretraining a model to regress to a manual cost function and\nrefining it based on Maximum Entropy Deep Inverse Reinforcement Learning. When\ninjecting prior knowledge as pretraining for the network, we achieve higher\nrobustness, more visually distinct obstacle boundaries, and the ability to\ncapture instances of obstacles that elude models that purely learn from\ndemonstration data. Furthermore, by exploiting these human priors, the\nresulting model can more accurately handle corner cases that are scarcely seen\nin the demonstration data, such as stairs, slopes, and underpasses.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 18:56:03 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Wulfmeier", "Markus", ""], ["Rao", "Dushyant", ""], ["Posner", "Ingmar", ""]]}, {"id": "1612.04337", "submitter": "Tian Qi Chen", "authors": "Tian Qi Chen and Mark Schmidt", "title": "Fast Patch-based Style Transfer of Arbitrary Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistic style transfer is an image synthesis problem where the content of an\nimage is reproduced with the style of another. Recent works show that a\nvisually appealing style transfer can be achieved by using the hidden\nactivations of a pretrained convolutional neural network. However, existing\nmethods either apply (i) an optimization procedure that works for any style\nimage but is very expensive, or (ii) an efficient feedforward network that only\nallows a limited number of trained styles. In this work we propose a simpler\noptimization objective based on local matching that combines the content\nstructure and style textures in a single layer of the pretrained network. We\nshow that our objective has desirable properties such as a simpler optimization\nlandscape, intuitive parameter tuning, and consistent frame-by-frame\nperformance on video. Furthermore, we use 80,000 natural images and 80,000\npaintings to train an inverse network that approximates the result of the\noptimization. This results in a procedure for artistic style transfer that is\nefficient but also allows arbitrary content and style images.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:05:37 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Chen", "Tian Qi", ""], ["Schmidt", "Mark", ""]]}, {"id": "1612.04340", "submitter": "Senthil Yogamani", "authors": "Ahmad El Sallab, Mohammed Abdou, Etienne Perot and Senthil Yogamani", "title": "End-to-End Deep Reinforcement Learning for Lane Keeping Assist", "comments": "Presented at the Machine Learning for Intelligent Transportation\n  Systems Workshop, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is considered to be a strong AI paradigm which can be\nused to teach machines through interaction with the environment and learning\nfrom their mistakes, but it has not yet been successfully used for automotive\napplications. There has recently been a revival of interest in the topic,\nhowever, driven by the ability of deep learning algorithms to learn good\nrepresentations of the environment. Motivated by Google DeepMind's successful\ndemonstrations of learning for games from Breakout to Go, we will propose\ndifferent methods for autonomous driving using deep reinforcement learning.\nThis is of particular interest as it is difficult to pose autonomous driving as\na supervised learning problem as it has a strong interaction with the\nenvironment including other vehicles, pedestrians and roadworks. As this is a\nrelatively new area of research for autonomous driving, we will formulate two\nmain categories of algorithms: 1) Discrete actions category, and 2) Continuous\nactions category. For the discrete actions category, we will deal with Deep\nQ-Network Algorithm (DQN) while for the continuous actions category, we will\ndeal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition to\nthat, We will also discover the performance of these two categories on an open\nsource car simulator for Racing called (TORCS) which stands for The Open Racing\ncar Simulator. Our simulation results demonstrate learning of autonomous\nmaneuvering in a scenario of complex road curvatures and simple interaction\nwith other vehicles. Finally, we explain the effect of some restricted\nconditions, put on the car during the learning phase, on the convergence time\nfor finishing its learning phase.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:19:42 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Sallab", "Ahmad El", ""], ["Abdou", "Mohammed", ""], ["Perot", "Etienne", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1612.04357", "submitter": "Xun Huang", "authors": "Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie", "title": "Stacked Generative Adversarial Networks", "comments": "CVPR 2017, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel generative model named Stacked Generative\nAdversarial Networks (SGAN), which is trained to invert the hierarchical\nrepresentations of a bottom-up discriminative network. Our model consists of a\ntop-down stack of GANs, each learned to generate lower-level representations\nconditioned on higher-level representations. A representation discriminator is\nintroduced at each feature hierarchy to encourage the representation manifold\nof the generator to align with that of the bottom-up discriminative network,\nleveraging the powerful discriminative representations to guide the generative\nmodel. In addition, we introduce a conditional loss that encourages the use of\nconditional information from the layer above, and a novel entropy loss that\nmaximizes a variational lower bound on the conditional entropy of generator\noutputs. We first train each stack independently, and then train the whole\nmodel end-to-end. Unlike the original GAN that uses a single noise vector to\nrepresent all the variations, our SGAN decomposes variations into multiple\nlevels and gradually resolves uncertainties in the top-down generative process.\nBased on visual inspection, Inception scores and visual Turing test, we\ndemonstrate that SGAN is able to generate images of much higher quality than\nGANs without stacking.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:48:58 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 01:21:55 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 07:50:27 GMT"}, {"version": "v4", "created": "Wed, 12 Apr 2017 15:04:01 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Huang", "Xun", ""], ["Li", "Yixuan", ""], ["Poursaeed", "Omid", ""], ["Hopcroft", "John", ""], ["Belongie", "Serge", ""]]}, {"id": "1612.04413", "submitter": "Rahul Gupta", "authors": "Rahul Gupta, Shrikanth Narayanan", "title": "Inferring object rankings based on noisy pairwise comparisons from\n  multiple annotators", "comments": "4 figures, includes appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking a set of objects involves establishing an order allowing for\ncomparisons between any pair of objects in the set. Oftentimes, due to the\nunavailability of a ground truth of ranked orders, researchers resort to\nobtaining judgments from multiple annotators followed by inferring the ground\ntruth based on the collective knowledge of the crowd. However, the aggregation\nis often ad-hoc and involves imposing stringent assumptions in inferring the\nground truth (e.g. majority vote). In this work, we propose\nExpectation-Maximization (EM) based algorithms that rely on the judgments from\nmultiple annotators and the object attributes for inferring the latent ground\ntruth. The algorithm learns the relation between the latent ground truth and\nobject attributes as well as annotator specific probabilities of flipping, a\nmetric to assess annotator quality. We further extend the EM algorithm to allow\nfor a variable probability of flipping based on the pair of objects at hand. We\ntest our algorithms on two data sets with synthetic annotations and investigate\nthe impact of annotator quality and quantity on the inferred ground truth. We\nalso obtain the results on two other data sets with annotations from\nmachine/human annotators and interpret the output trends based on the data\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 21:52:12 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Gupta", "Rahul", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "1612.04418", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa (Yandex, Moscow, Russia), Andrey Shutovich (Yandex,\n  Moscow, Russia), Philipp Pushnyakov (Yandex, Moscow, Russia), Evgeniy\n  Krokhalyov (Yandex, Moscow, Russia), Gleb Gusev (Yandex, Moscow, Russia),\n  Pavel Serdyukov (Yandex, Moscow, Russia)", "title": "User Model-Based Intent-Aware Metrics for Multilingual Search Evaluation", "comments": "7 pages, 1 figure, 3 tables", "journal-ref": "NIPS 2016 Workshop \"What If? Inference and Learning of\n  Hypothetical and Counterfactual Interventions in Complex Systems\" (What If\n  2016) pre-print", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing importance of multilingual aspect of web search, no\nappropriate offline metrics to evaluate its quality are proposed so far. At the\nsame time, personal language preferences can be regarded as intents of a query.\nThis approach translates the multilingual search problem into a particular task\nof search diversification. Furthermore, the standard intent-aware approach\ncould be adopted to build a diversified metric for multilingual search on the\nbasis of a classical IR metric such as ERR. The intent-aware approach estimates\nuser satisfaction under a user behavior model. We show however that the\nunderlying user behavior models is not realistic in the multilingual case, and\nthe produced intent-aware metric do not appropriately estimate the user\nsatisfaction. We develop a novel approach to build intent-aware user behavior\nmodels, which overcome these limitations and convert to quality metrics that\nbetter correlate with standard online metrics of user satisfaction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 22:09:24 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Drutsa", "Alexey", "", "Yandex, Moscow, Russia"], ["Shutovich", "Andrey", "", "Yandex,\n  Moscow, Russia"], ["Pushnyakov", "Philipp", "", "Yandex, Moscow, Russia"], ["Krokhalyov", "Evgeniy", "", "Yandex, Moscow, Russia"], ["Gusev", "Gleb", "", "Yandex, Moscow, Russia"], ["Serdyukov", "Pavel", "", "Yandex, Moscow, Russia"]]}, {"id": "1612.04426", "submitter": "Edouard Grave", "authors": "Edouard Grave, Armand Joulin, Nicolas Usunier", "title": "Improving Neural Language Models with a Continuous Cache", "comments": "Submitted to ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 23:09:49 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Grave", "Edouard", ""], ["Joulin", "Armand", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1612.04431", "submitter": "Ali Burak \\\"Unal", "authors": "Ali Burak \\\"Unal, \\\"Oznur Ta\\c{s}tan", "title": "Identification of Cancer Patient Subgroups via Smoothed Shortest Path\n  Graph Kernel", "comments": "NIPS Workshop on Machine Learning in Computational Biology,\n  Barcelona, Spain, December 10, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing patient somatic mutations through next-generation sequencing\ntechnologies opens up possibilities for refining cancer subtypes. However,\ncatalogues of mutations reveal that only a small fraction of the genes are\naltered frequently in patients. On the other hand different genomic alterations\nmay perturb the same pathways. We propose a novel clustering procedure that\nquantifies the similarities of patients from their mutational profile on\npathways via a novel graph kernel. We represent each KEGG pathway as an\nundirected graph. For each patient the vertex labels are assigned based on her\naltered genes. Smoothed shortest path graph kernel (smSPK) evaluates each pair\nof patients by comparing their vertex labeled pathway graphs. Our clustering\nprocedure involves two steps: the smSPK kernel matrix derived for each pathway\nare input to kernel k-means algorithm and each pathway is evaluated\nindividually. In the next step, only those pathways that are successful are\ncombined in to a single kernel input to kernel k-means to stratify patients.\nEvaluating the procedure on simulated data showed that smSPK clusters patients\nup to 88\\% accuracy. Finally to identify ovarian cancer patient subgroups, we\napply our methodology to the cancer genome atlas ovarian data that involves 481\npatients. The identified subgroups are evaluated through survival analysis.\nGrouping patients into four clusters results with patients groups that are\nsignificantly different in their survival times ($p$-value $\\le 0.005$).\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 23:47:41 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 10:27:58 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["\u00dcnal", "Ali Burak", ""], ["Ta\u015ftan", "\u00d6znur", ""]]}, {"id": "1612.04440", "submitter": "Will Grathwohl", "authors": "Will Grathwohl, Aaron Wilson", "title": "Disentangling Space and Time in Video with Hierarchical Variational\n  Auto-encoders", "comments": "fixed typo in equation 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many forms of feature information present in video data. Principle\namong them are object identity information which is largely static across\nmultiple video frames, and object pose and style information which continuously\ntransforms from frame to frame. Most existing models confound these two types\nof representation by mapping them to a shared feature space. In this paper we\npropose a probabilistic approach for learning separable representations of\nobject identity and pose information using unsupervised video data. Our\napproach leverages a deep generative model with a factored prior distribution\nthat encodes properties of temporal invariances in the hidden feature set.\nLearning is achieved via variational inference. We present results of learning\nidentity and pose information on a dataset of moving characters as well as a\ndataset of rotating 3D objects. Our experimental results demonstrate our\nmodel's success in factoring its representation, and demonstrate that the model\nachieves improved performance in transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 00:20:46 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 17:17:26 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Grathwohl", "Will", ""], ["Wilson", "Aaron", ""]]}, {"id": "1612.04599", "submitter": "Ivan Mari\\'c", "authors": "Ivan Maric", "title": "Retrieving sinusoids from nonuniformly sampled data using recursive\n  formulation", "comments": "16 pages, 17 figures, Expert Systems with Applications - published", "journal-ref": "I.Maric Expert Systems With Applications 72 (2017) 245-257", "doi": "10.1016/j.eswa.2016.10.057", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A heuristic procedure based on novel recursive formulation of sinusoid (RFS)\nand on regression with predictive least-squares (LS) enables to decompose both\nuniformly and nonuniformly sampled 1-d signals into a sparse set of sinusoids\n(SSS). An optimal SSS is found by Levenberg-Marquardt (LM) optimization of RFS\nparameters of near-optimal sinusoids combined with common criteria for the\nestimation of the number of sinusoids embedded in noise. The procedure\nestimates both the cardinality and the parameters of SSS. The proposed\nalgorithm enables to identify the RFS parameters of a sinusoid from a data\nsequence containing only a fraction of its cycle. In extreme cases when the\nfrequency of a sinusoid approaches zero the algorithm is able to detect a\nlinear trend in data. Also, an irregular sampling pattern enables the algorithm\nto correctly reconstruct the under-sampled sinusoid. Parsimonious nature of the\nobtaining models opens the possibilities of using the proposed method in\nmachine learning and in expert and intelligent systems needing analysis and\nsimple representation of 1-d signals. The properties of the proposed algorithm\nare evaluated on examples of irregularly sampled artificial signals in noise\nand are compared with high accuracy frequency estimation algorithms based on\nlinear prediction (LP) approach, particularly with respect to Cramer-Rao Bound\n(CRB).\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 12:27:25 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 13:03:34 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Maric", "Ivan", ""]]}, {"id": "1612.04600", "submitter": "Joerg Evermann", "authors": "Joerg Evermann, Jana-Rebecca Rehse, Peter Fettke", "title": "Predicting Process Behaviour using Deep Learning", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.dss.2017.04.003", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting business process behaviour is an important aspect of business\nprocess management. Motivated by research in natural language processing, this\npaper describes an application of deep learning with recurrent neural networks\nto the problem of predicting the next event in a business process. This is both\na novel method in process prediction, which has largely relied on explicit\nprocess models, and also a novel application of deep learning methods. The\napproach is evaluated on two real datasets and our results surpass the\nstate-of-the-art in prediction precision.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 12:33:28 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 17:22:08 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Evermann", "Joerg", ""], ["Rehse", "Jana-Rebecca", ""], ["Fettke", "Peter", ""]]}, {"id": "1612.04642", "submitter": "Daniel Worrall", "authors": "Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov and\n  Gabriel J. Brostow", "title": "Harmonic Networks: Deep Translation and Rotation Equivariance", "comments": "Submitted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating or rotating an input image should not affect the results of many\ncomputer vision tasks. Convolutional neural networks (CNNs) are already\ntranslation equivariant: input image translations produce proportionate feature\nmap translations. This is not the case for rotations. Global rotation\nequivariance is typically sought through data augmentation, but patch-wise\nequivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN\nexhibiting equivariance to patch-wise translation and 360-rotation. We achieve\nthis by replacing regular CNN filters with circular harmonics, returning a\nmaximal response and orientation for every receptive field patch.\n  H-Nets use a rich, parameter-efficient and low computational complexity\nrepresentation, and we show that deep feature maps within the network encode\ncomplicated rotational invariants. We demonstrate that our layers are general\nenough to be used in conjunction with the latest architectures and techniques,\nsuch as deep supervision and batch normalization. We also achieve\nstate-of-the-art classification on rotated-MNIST, and competitive results on\nother benchmark challenges.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 14:01:11 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 13:34:17 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Worrall", "Daniel E.", ""], ["Garbin", "Stephan J.", ""], ["Turmukhambetov", "Daniyar", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1612.04659", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Zhi Wang, Liwei Wang", "title": "Stable Memory Allocation in the Hippocampus: Fundamental Limits and\n  Neural Realization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is believed that hippocampus functions as a memory allocator in brain, the\nmechanism of which remains unrevealed. In Valiant's neuroidal model, the\nhippocampus was described as a randomly connected graph, the computation on\nwhich maps input to a set of activated neuroids with stable size. Valiant\nproposed three requirements for the hippocampal circuit to become a stable\nmemory allocator (SMA): stability, continuity and orthogonality. The\nfunctionality of SMA in hippocampus is essential in further computation within\ncortex, according to Valiant's model.\n  In this paper, we put these requirements for memorization functions into\nrigorous mathematical formulation and introduce the concept of capacity, based\non the probability of erroneous allocation. We prove fundamental limits for the\ncapacity and error probability of SMA, in both data-independent and\ndata-dependent settings. We also establish an example of stable memory\nallocator that can be implemented via neuroidal circuits. Both theoretical\nbounds and simulation results show that the neural SMA functions well.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 14:26:05 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Mou", "Wenlong", ""], ["Wang", "Zhi", ""], ["Wang", "Liwei", ""]]}, {"id": "1612.04739", "submitter": "Philip Bachman", "authors": "Philip Bachman", "title": "An Architecture for Deep, Hierarchical Generative Models", "comments": "Published in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an architecture which lets us train deep, directed generative\nmodels with many layers of latent variables. We include deterministic paths\nbetween all latent variables and the generated output, and provide a richer set\nof connections between computations for inference and generation, which enables\nmore effective communication of information throughout the model during\ntraining. To improve performance on natural images, we incorporate a\nlightweight autoregressive model in the reconstruction distribution. These\ntechniques permit end-to-end training of models with 10+ layers of latent\nvariables. Experiments show that our approach achieves state-of-the-art\nperformance on standard image modelling benchmarks, can expose latent class\nstructure in the absence of label information, and can provide convincing\nimputations of occluded regions in natural images.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 11:17:16 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Bachman", "Philip", ""]]}, {"id": "1612.04744", "submitter": "Peidong Wang", "authors": "Peidong Wang, Deliang Wang", "title": "Incorporating Language Level Information into Acoustic Models", "comments": "The project was discontinued", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a class of novel Deep Recurrent Neural Networks which can\nincorporate language-level information into acoustic models. For simplicity, we\nnamed these networks Recurrent Deep Language Networks (RDLNs). Multiple\nvariants of RDLNs were considered, including two kinds of context information,\ntwo methods to process the context, and two methods to incorporate the\nlanguage-level information. RDLNs provided possible methods to fine-tune the\nwhole Automatic Speech Recognition (ASR) system in the acoustic modeling\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 17:40:02 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 02:20:33 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Wang", "Peidong", ""], ["Wang", "Deliang", ""]]}, {"id": "1612.04759", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "Encapsulating models and approximate inference programs in probabilistic\n  modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the probabilistic module interface, which allows\nencapsulation of complex probabilistic models with latent variables alongside\ncustom stochastic approximate inference machinery, and provides a\nplatform-agnostic abstraction barrier separating the model internals from the\nhost probabilistic inference system. The interface can be seen as a stochastic\ngeneralization of a standard simulation and density interface for probabilistic\nprimitives. We show that sound approximate inference algorithms can be\nconstructed for networks of probabilistic modules, and we demonstrate that the\ninterface can be implemented using learned stochastic inference networks and\nMCMC and SMC approximate inference programs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 18:14:59 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 21:13:42 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1612.04770", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Nikos Komodakis", "title": "Detect, Replace, Refine: Deep Structured Prediction For Pixel Wise\n  Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel wise image labeling is an interesting and challenging problem with\ngreat significance in the computer vision community. In order for a dense\nlabeling algorithm to be able to achieve accurate and precise results, it has\nto consider the dependencies that exist in the joint space of both the input\nand the output variables. An implicit approach for modeling those dependencies\nis by training a deep neural network that, given as input an initial estimate\nof the output labels and the input image, it will be able to predict a new\nrefined estimate for the labels. In this context, our work is concerned with\nwhat is the optimal architecture for performing the label improvement task. We\nargue that the prior approaches of either directly predicting new label\nestimates or predicting residual corrections w.r.t. the initial labels with\nfeed-forward deep network architectures are sub-optimal. Instead, we propose a\ngeneric architecture that decomposes the label improvement task to three steps:\n1) detecting the initial label estimates that are incorrect, 2) replacing the\nincorrect labels with new ones, and finally 3) refining the renewed labels by\npredicting residual corrections w.r.t. them. Furthermore, we explore and\ncompare various other alternative architectures that consist of the\naforementioned Detection, Replace, and Refine components. We extensively\nevaluate the examined architectures in the challenging task of dense disparity\nestimation (stereo matching) and we report both quantitative and qualitative\nresults on three different datasets. Finally, our dense disparity estimation\nnetwork that implements the proposed generic architecture, achieves\nstate-of-the-art results in the KITTI 2015 test surpassing prior approaches by\na significant margin.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 18:54:33 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1612.04799", "submitter": "William Guss", "authors": "William H. Guss", "title": "Deep Function Machines: Generalized Neural Networks for Topological\n  Layer Expression", "comments": "23 pages, 9 figures, with experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a generalization of deep neural networks called deep\nfunction machines (DFMs). DFMs act on vector spaces of arbitrary (possibly\ninfinite) dimension and we show that a family of DFMs are invariant to the\ndimension of input data; that is, the parameterization of the model does not\ndirectly hinge on the quality of the input (eg. high resolution images). Using\nthis generalization we provide a new theory of universal approximation of\nbounded non-linear operators between function spaces. We then suggest that DFMs\nprovide an expressive framework for designing new neural network layer types\nwith topological considerations in mind. Finally, we introduce a novel\narchitecture, RippLeNet, for resolution invariant computer vision, which\nempirically achieves state of the art invariance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:39:23 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 20:51:33 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Guss", "William H.", ""]]}, {"id": "1612.04804", "submitter": "Asaf Shabtai", "authors": "Asaf Shabtai", "title": "Anomaly Detection Using the Knowledge-based Temporal Abstraction Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth in stored time-oriented data necessitates the development of\nnew methods for handling, processing, and interpreting large amounts of\ntemporal data. One important example of such processing is detecting anomalies\nin time-oriented data. The Knowledge-Based Temporal Abstraction method was\npreviously proposed for intelligent interpretation of temporal data based on\npredefined domain knowledge. In this study we propose a framework that\nintegrates the KBTA method with a temporal pattern mining process for anomaly\ndetection. According to the proposed method a temporal pattern mining process\nis applied on a dataset of basic temporal abstraction database in order to\nextract patterns representing normal behavior. These patterns are then analyzed\nin order to identify abnormal time periods characterized by a significantly\nsmall number of normal patterns. The proposed approach was demonstrated using a\ndataset collected from a real server.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:50:48 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Shabtai", "Asaf", ""]]}, {"id": "1612.04831", "submitter": "Utkarsh Upadhyay", "authors": "Utkarsh Upadhyay, Isabel Valera, Manuel Gomez-Rodriguez", "title": "Uncovering the Dynamics of Crowdlearning and the Value of Knowledge", "comments": "To appear in Tenth ACM International conference on Web Search and\n  Data Mining (WSDM) in 2017", "journal-ref": null, "doi": "10.1145/3018661.3018685", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from the crowd has become increasingly popular in the Web and social\nmedia. There is a wide variety of crowdlearning sites in which, on the one\nhand, users learn from the knowledge that other users contribute to the site,\nand, on the other hand, knowledge is reviewed and curated by the same users\nusing assessment measures such as upvotes or likes.\n  In this paper, we present a probabilistic modeling framework of\ncrowdlearning, which uncovers the evolution of a user's expertise over time by\nleveraging other users' assessments of her contributions. The model allows for\nboth off-site and on-site learning and captures forgetting of knowledge. We\nthen develop a scalable estimation method to fit the model parameters from\nmillions of recorded learning and contributing events. We show the\neffectiveness of our model by tracing activity of ~25 thousand users in Stack\nOverflow over a 4.5 year period. We find that answers with high knowledge value\nare rare. Newbies and experts tend to acquire less knowledge than users in the\nmiddle range. Prolific learners tend to be also proficient contributors that\npost answers with high knowledge value.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 21:00:11 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Upadhyay", "Utkarsh", ""], ["Valera", "Isabel", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1612.04853", "submitter": "Hoel Le Capitaine", "authors": "Hoel Le Capitaine", "title": "Constraint Selection in Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of machine learning algorithms are using a metric, or a distance, in\norder to compare individuals. The Euclidean distance is usually employed, but\nit may be more efficient to learn a parametric distance such as Mahalanobis\nmetric. Learning such a metric is a hot topic since more than ten years now,\nand a number of methods have been proposed to efficiently learn it. However,\nthe nature of the problem makes it quite difficult for large scale data, as\nwell as data for which classes overlap. This paper presents a simple way of\nimproving accuracy and scalability of any iterative metric learning algorithm,\nwhere constraints are obtained prior to the algorithm. The proposed approach\nrelies on a loss-dependent weighted selection of constraints that are used for\nlearning the metric. Using the corresponding dedicated loss function, the\nmethod clearly allows to obtain better results than state-of-the-art methods,\nboth in terms of accuracy and time complexity. Some experimental results on\nreal world, and potentially large, datasets are demonstrating the effectiveness\nof our proposition.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 21:45:14 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Capitaine", "Hoel Le", ""]]}, {"id": "1612.04858", "submitter": "Ian Dewancker", "authors": "Ian Dewancker, Michael McCourt, Scott Clark", "title": "Bayesian Optimization for Machine Learning : A Practical Guidebook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 22:04:33 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dewancker", "Ian", ""], ["McCourt", "Michael", ""], ["Clark", "Scott", ""]]}, {"id": "1612.04868", "submitter": "I\\~nigo Lopez-Gazpio", "authors": "I. Lopez-Gazpio and M. Maritxalar and A. Gonzalez-Agirre and G. Rigau\n  and L. Uria and E. Agirre", "title": "Interpretable Semantic Textual Similarity: Finding and explaining\n  differences between sentences", "comments": "Preprint version, Knowledge-Based Systems (ISSN: 0950-7051). (2016)", "journal-ref": null, "doi": "10.1016/j.knosys.2016.12.013", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User acceptance of artificial intelligence agents might depend on their\nability to explain their reasoning, which requires adding an interpretability\nlayer that fa- cilitates users to understand their behavior. This paper focuses\non adding an in- terpretable layer on top of Semantic Textual Similarity (STS),\nwhich measures the degree of semantic equivalence between two sentences. The\ninterpretability layer is formalized as the alignment between pairs of segments\nacross the two sentences, where the relation between the segments is labeled\nwith a relation type and a similarity score. We present a publicly available\ndataset of sentence pairs annotated following the formalization. We then\ndevelop a system trained on this dataset which, given a sentence pair, explains\nwhat is similar and different, in the form of graded and typed segment\nalignments. When evaluated on the dataset, the system performs better than an\ninformed baseline, showing that the dataset and task are well-defined and\nfeasible. Most importantly, two user studies show how the system output can be\nused to automatically produce explanations in natural language. Users performed\nbetter when having access to the explanations, pro- viding preliminary evidence\nthat our dataset and method to automatically produce explanations is useful in\nreal applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 22:22:33 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Lopez-Gazpio", "I.", ""], ["Maritxalar", "M.", ""], ["Gonzalez-Agirre", "A.", ""], ["Rigau", "G.", ""], ["Uria", "L.", ""], ["Agirre", "E.", ""]]}, {"id": "1612.04887", "submitter": "Kai Xu", "authors": "Kai Xu, Yixing Li, Fengbo Ren", "title": "A Data-Driven Compressive Sensing Framework Tailored For\n  Energy-Efficient Wearable Sensing", "comments": "Accepted as an oral presentation in 42th IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) is a promising technology for realizing\nenergy-efficient wireless sensors for long-term health monitoring. However,\nconventional model-driven CS frameworks suffer from limited compression ratio\nand reconstruction quality when dealing with physiological signals due to\ninaccurate models and the overlook of individual variability. In this paper, we\npropose a data-driven CS framework that can learn signal characteristics and\npersonalized features from any individual recording of physiologic signals to\nenhance CS performance with a minimized number of measurements. Such\nimprovements are accomplished by a co-training approach that optimizes the\nsensing matrix and the dictionary towards improved restricted isometry property\nand signal sparsity, respectively. Experimental results upon ECG signals show\nthat the proposed method, at a compression ratio of 10x, successfully reduces\nthe isometry constant of the trained sensing matrices by 86% against random\nmatrices and improves the overall reconstructed signal-to-noise ratio by 15dB\nover conventional model-driven approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 00:10:19 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 20:20:45 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Xu", "Kai", ""], ["Li", "Yixing", ""], ["Ren", "Fengbo", ""]]}, {"id": "1612.04891", "submitter": "Aaron Lee", "authors": "Cecilia S. Lee, Doug M. Baughman, Aaron Y. Lee", "title": "Deep learning is effective for the classification of OCT images of\n  normal versus Age-related Macular Degeneration", "comments": "4 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: The advent of Electronic Medical Records (EMR) with large\nelectronic imaging databases along with advances in deep neural networks with\nmachine learning has provided a unique opportunity to achieve milestones in\nautomated image analysis. Optical coherence tomography (OCT) is the most\ncommonly obtained imaging modality in ophthalmology and represents a dense and\nrich dataset when combined with labels derived from the EMR. We sought to\ndetermine if deep learning could be utilized to distinguish normal OCT images\nfrom images from patients with Age-related Macular Degeneration (AMD). Methods:\nAutomated extraction of an OCT imaging database was performed and linked to\nclinical endpoints from the EMR. OCT macula scans were obtained by Heidelberg\nSpectralis, and each OCT scan was linked to EMR clinical endpoints extracted\nfrom EPIC. The central 11 images were selected from each OCT scan of two\ncohorts of patients: normal and AMD. Cross-validation was performed using a\nrandom subset of patients. Area under receiver operator curves (auROC) were\nconstructed at an independent image level, macular OCT level, and patient\nlevel. Results: Of an extraction of 2.6 million OCT images linked to clinical\ndatapoints from the EMR, 52,690 normal and 48,312 AMD macular OCT images were\nselected. A deep neural network was trained to categorize images as either\nnormal or AMD. At the image level, we achieved an auROC of 92.78% with an\naccuracy of 87.63%. At the macula level, we achieved an auROC of 93.83% with an\naccuracy of 88.98%. At a patient level, we achieved an auROC of 97.45% with an\naccuracy of 93.45%. Peak sensitivity and specificity with optimal cutoffs were\n92.64% and 93.69% respectively. Conclusions: Deep learning techniques are\neffective for classifying OCT images. These findings have important\nimplications in utilizing OCT in automated screening and computer aided\ndiagnosis tools.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 00:23:43 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Lee", "Cecilia S.", ""], ["Baughman", "Doug M.", ""], ["Lee", "Aaron Y.", ""]]}, {"id": "1612.04898", "submitter": "Sunil Thulasidasan", "authors": "Sunil Thulasidasan, Jeffrey Bilmes, Garrett Kenyon", "title": "Efficient Distributed Semi-Supervised Learning using Stochastic\n  Regularization over Affinity Graphs", "comments": "NIPS 2016 Workshop on Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": "LA-UR-16-28681", "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a computationally efficient, stochastic graph-regularization\ntechnique that can be utilized for the semi-supervised training of deep neural\nnetworks in a parallel or distributed setting. We utilize a technique, first\ndescribed in [13] for the construction of mini-batches for stochastic gradient\ndescent (SGD) based on synthesized partitions of an affinity graph that are\nconsistent with the graph structure, but also preserve enough stochasticity for\nconvergence of SGD to good local minima. We show how our technique allows a\ngraph-based semi-supervised loss function to be decomposed into a sum over\nobjectives, facilitating data parallelism for scalable training of machine\nlearning models. Empirical results indicate that our method significantly\nimproves classification accuracy compared to the fully-supervised case when the\nfraction of labeled data is low, and in the parallel case, achieves significant\nspeed-up in terms of wall-clock time to convergence. We show the results for\nboth sequential and distributed-memory semi-supervised DNN training on a speech\ncorpus.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 01:00:23 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 17:23:25 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Thulasidasan", "Sunil", ""], ["Bilmes", "Jeffrey", ""], ["Kenyon", "Garrett", ""]]}, {"id": "1612.04899", "submitter": "Sunil Thulasidasan", "authors": "Sunil Thulasidasan, Jeffrey Bilmes", "title": "Semi-Supervised Phone Classification using Deep Neural Networks and\n  Stochastic Graph-Based Entropic Regularization", "comments": "InterSpeech Workshop on Machine Learning in Speech and Language\n  Processing, 2016. Based on and extends work in arXiv:1612.04898", "journal-ref": null, "doi": null, "report-no": "LA-UR-16-24599", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a graph-based semi-supervised learning framework in the context\nof deep neural networks that uses a graph-based entropic regularizer to favor\nsmooth solutions over a graph induced by the data. The main contribution of\nthis work is a computationally efficient, stochastic graph-regularization\ntechnique that uses mini-batches that are consistent with the graph structure,\nbut also provides enough stochasticity (in terms of mini-batch data diversity)\nfor convergence of stochastic gradient descent methods to good solutions. For\nthis work, we focus on results of frame-level phone classification accuracy on\nthe TIMIT speech corpus but our method is general and scalable to much larger\ndata sets. Results indicate that our method significantly improves\nclassification accuracy compared to the fully-supervised case when the fraction\nof labeled data is low, and it is competitive with other methods in the fully\nlabeled case.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 01:00:45 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 16:04:46 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Thulasidasan", "Sunil", ""], ["Bilmes", "Jeffrey", ""]]}, {"id": "1612.04933", "submitter": "Benjamin Jantzen", "authors": "Benjamin C. Jantzen", "title": "Dynamical Kinds and their Discovery", "comments": "Accepted for the proceedings of the Causation: Foundation to\n  Application Workshop, UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the possibility of classifying causal systems into kinds that\nshare a common structure without first constructing an explicit dynamical model\nor using prior knowledge of the system dynamics. The algorithmic ability to\ndetermine whether arbitrary systems are governed by causal relations of the\nsame form offers significant practical applications in the development and\nvalidation of dynamical models. It is also of theoretical interest as an\nessential stage in the scientific inference of laws from empirical data. The\nalgorithm presented is based on the dynamical symmetry approach to dynamical\nkinds. A dynamical symmetry with respect to time is an intervention on one or\nmore variables of a system that commutes with the time evolution of the system.\nA dynamical kind is a class of systems sharing a set of dynamical symmetries.\nThe algorithm presented classifies deterministic, time-dependent causal systems\nby directly comparing their exhibited symmetries. Using simulated, noisy data\nfrom a variety of nonlinear systems, we show that this algorithm correctly\nsorts systems into dynamical kinds. It is robust under significant sampling\nerror, is immune to violations of normality in sampling error, and fails\ngracefully with increasing dynamical similarity. The algorithm we demonstrate\nis the first to address this aspect of automated scientific discovery.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 05:25:41 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Jantzen", "Benjamin C.", ""]]}, {"id": "1612.04970", "submitter": "Tomas Maul", "authors": "Kien Tuong Phan, Tomas Henrique Maul, Tuong Thuy Vu, Lai Weng Kin", "title": "Improving Neural Network Generalization by Combining Parallel Circuits\n  with Dropout", "comments": "Pre-print. The final publication is available at Springer via\n  http://dx.doi.org/10.1007/978-3-319-46675-0_63", "journal-ref": null, "doi": "10.1007/978-3-319-46675-0_63", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an attempt to solve the lengthy training times of neural networks, we\nproposed Parallel Circuits (PCs), a biologically inspired architecture.\nPrevious work has shown that this approach fails to maintain generalization\nperformance in spite of achieving sharp speed gains. To address this issue, and\nmotivated by the way Dropout prevents node co-adaption, in this paper, we\nsuggest an improvement by extending Dropout to the PC architecture. The paper\nprovides multiple insights into this combination, including a variety of fusion\napproaches. Experiments show promising results in which improved error rates\nare achieved in most cases, whilst maintaining the speed advantage of the PC\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 08:38:58 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Phan", "Kien Tuong", ""], ["Maul", "Tomas Henrique", ""], ["Vu", "Tuong Thuy", ""], ["Kin", "Lai Weng", ""]]}, {"id": "1612.05001", "submitter": "Leto Peel", "authors": "Leto Peel", "title": "Graph-based semi-supervised learning for relational networks", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of semi-supervised learning in relational networks,\nnetworks in which nodes are entities and links are the relationships or\ninteractions between them. Typically this problem is confounded with the\nproblem of graph-based semi-supervised learning (GSSL), because both problems\nrepresent the data as a graph and predict the missing class labels of nodes.\nHowever, not all graphs are created equally. In GSSL a graph is constructed,\noften from independent data, based on similarity. As such, edges tend to\nconnect instances with the same class label. Relational networks, however, can\nbe more heterogeneous and edges do not always indicate similarity. For\ninstance, instead of links being more likely to connect nodes with the same\nclass label, they may occur more frequently between nodes with different class\nlabels (link-heterogeneity). Or nodes with the same class label do not\nnecessarily have the same type of connectivity across the whole network\n(class-heterogeneity), e.g. in a network of sexual interactions we may observe\nlinks between opposite genders in some parts of the graph and links between the\nsame genders in others. Performing classification in networks with different\ntypes of heterogeneity is a hard problem that is made harder still when we do\nnot know a-priori the type or level of heterogeneity. Here we present two\nscalable approaches for graph-based semi-supervised learning for the more\ngeneral case of relational networks. We demonstrate these approaches on\nsynthetic and real-world networks that display different link patterns within\nand between classes. Compared to state-of-the-art approaches, ours give better\nclassification performance without prior knowledge of how classes interact. In\nparticular, our two-step label propagation algorithm gives consistently good\naccuracy and runs on networks of over 1.6 million nodes and 30 million edges in\naround 12 seconds.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 10:15:57 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Peel", "Leto", ""]]}, {"id": "1612.05024", "submitter": "Andrey Y. Lokhov", "authors": "Andrey Y. Lokhov, Marc Vuffray, Sidhant Misra, Michael Chertkov", "title": "Optimal structure and parameter learning of Ising models", "comments": "11 pages, 10 pages of supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of structure and parameters of an Ising model from binary\nsamples is a problem of practical importance in a variety of disciplines,\nranging from statistical physics and computational biology to image processing\nand machine learning. The focus of the research community shifted towards\ndeveloping universal reconstruction algorithms which are both computationally\nefficient and require the minimal amount of expensive data. We introduce a new\nmethod, Interaction Screening, which accurately estimates the model parameters\nusing local optimization problems. The algorithm provably achieves perfect\ngraph structure recovery with an information-theoretically optimal number of\nsamples, notably in the low-temperature regime which is known to be the hardest\nfor learning. The efficacy of Interaction Screening is assessed through\nextensive numerical tests on synthetic Ising models of various topologies with\ndifferent types of interactions, as well as on a real data produced by a D-Wave\nquantum computer. This study shows that the Interaction Screening method is an\nexact, tractable and optimal technique universally solving the inverse Ising\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 11:30:40 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 11:02:50 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Lokhov", "Andrey Y.", ""], ["Vuffray", "Marc", ""], ["Misra", "Sidhant", ""], ["Chertkov", "Michael", ""]]}, {"id": "1612.05050", "submitter": "Matthias Dorfer", "authors": "Matthias Dorfer, Andreas Arzt, Gerhard Widmer", "title": "Towards Score Following in Sheet Music Images", "comments": "Published In Proceedings of the 17th International Society for Music\n  Information Retrieval Conference (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the matching of short music audio snippets to the\ncorresponding pixel location in images of sheet music. A system is presented\nthat simultaneously learns to read notes, listens to music and matches the\ncurrently played music to its corresponding notes in the sheet. It consists of\nan end-to-end multi-modal convolutional neural network that takes as input\nimages of sheet music and spectrograms of the respective audio snippets. It\nlearns to predict, for a given unseen audio snippet (covering approximately one\nbar of music), the corresponding position in the respective score line. Our\nresults suggest that with the use of (deep) neural networks -- which have\nproven to be powerful image processing models -- working with sheet music\nbecomes feasible and a promising future research direction.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 13:10:13 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dorfer", "Matthias", ""], ["Arzt", "Andreas", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1612.05054", "submitter": "Ashish Bora", "authors": "Ashish Bora, Sugato Basu, Joydeep Ghosh", "title": "Graphical RNN Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many time series are generated by a set of entities that interact with one\nanother over time. This paper introduces a broad, flexible framework to learn\nfrom multiple inter-dependent time series generated by such entities. Our\nframework explicitly models the entities and their interactions through time.\nIt achieves this by building on the capabilities of Recurrent Neural Networks,\nwhile also offering several ways to incorporate domain knowledge/constraints\ninto the model architecture. The capabilities of our approach are showcased\nthrough an application to weather prediction, which shows gains over strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 13:24:41 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Bora", "Ashish", ""], ["Basu", "Sugato", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1612.05065", "submitter": "Filip Korzeniowski", "authors": "Filip Korzeniowski and Gerhard Widmer", "title": "Feature Learning for Chord Recognition: The Deep Chroma Extractor", "comments": "In Proceedings of the 17th International Society for Music\n  Information Retrieval Conference (ISMIR), New York, USA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore frame-level audio feature learning for chord recognition using\nartificial neural networks. We present the argument that chroma vectors\npotentially hold enough information to model harmonic content of audio for\nchord recognition, but that standard chroma extractors compute too noisy\nfeatures. This leads us to propose a learned chroma feature extractor based on\nartificial neural networks. It is trained to compute chroma features that\nencode harmonic information important for chord recognition, while being robust\nto irrelevant interferences. We achieve this by feeding the network an audio\nspectrum with context instead of a single frame as input. This way, the network\ncan learn to selectively compensate noise and resolve harmonic ambiguities.\n  We compare the resulting features to hand-crafted ones by using a simple\nlinear frame-wise classifier for chord recognition on various data sets. The\nresults show that the learned feature extractor produces superior chroma\nvectors for chord recognition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 14:01:50 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Korzeniowski", "Filip", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1612.05070", "submitter": "Matthias Dorfer", "authors": "Matthias Dorfer, Andreas Arzt, Gerhard Widmer", "title": "Towards End-to-End Audio-Sheet-Music Retrieval", "comments": "In NIPS 2016 End-to-end Learning for Speech and Audio Processing\n  Workshop, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the feasibility of learning to retrieve short\nsnippets of sheet music (images) when given a short query excerpt of music\n(audio) -- and vice versa --, without any symbolic representation of music or\nscores. This would be highly useful in many content-based musical retrieval\nscenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA)\nand learns correlated latent spaces allowing for cross-modality retrieval in\nboth directions. Initial experiments with relatively simple monophonic music\nshow promising results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 14:07:51 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dorfer", "Matthias", ""], ["Arzt", "Andreas", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1612.05082", "submitter": "Filip Korzeniowski", "authors": "Filip Korzeniowski and Gerhard Widmer", "title": "A Fully Convolutional Deep Auditory Model for Musical Chord Recognition", "comments": "In Proceedings of the 2016 IEEE 26th International Workshop on\n  Machine Learning for Signal Processing (MLSP), Vietro sul Mare, Italy", "journal-ref": null, "doi": "10.1109/MLSP.2016.7738895", "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chord recognition systems depend on robust feature extraction pipelines.\nWhile these pipelines are traditionally hand-crafted, recent advances in\nend-to-end machine learning have begun to inspire researchers to explore\ndata-driven methods for such tasks. In this paper, we present a chord\nrecognition system that uses a fully convolutional deep auditory model for\nfeature extraction. The extracted features are processed by a Conditional\nRandom Field that decodes the final chord sequence. Both processing stages are\ntrained automatically and do not require expert knowledge for optimising\nparameters. We show that the learned auditory system extracts musically\ninterpretable features, and that the proposed chord recognition system achieves\nresults on par or better than state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 14:32:20 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Korzeniowski", "Filip", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1612.05086", "submitter": "Lukas Balles", "authors": "Lukas Balles and Javier Romero and Philipp Hennig", "title": "Coupling Adaptive Batch Sizes with Learning Rates", "comments": "Thirty-Third Conference on Uncertainty in Artificial Intelligence\n  (UAI), 2017, (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mini-batch stochastic gradient descent and variants thereof have become\nstandard for large-scale empirical risk minimization like the training of\nneural networks. These methods are usually used with a constant batch size\nchosen by simple empirical inspection. The batch size significantly influences\nthe behavior of the stochastic optimization algorithm, though, since it\ndetermines the variance of the gradient estimates. This variance also changes\nover the optimization process; when using a constant batch size, stability and\nconvergence is thus often enforced by means of a (manually tuned) decreasing\nlearning rate schedule.\n  We propose a practical method for dynamic batch size adaptation. It estimates\nthe variance of the stochastic gradients and adapts the batch size to decrease\nthe variance proportionally to the value of the objective function, removing\nthe need for the aforementioned learning rate decrease. In contrast to recent\nrelated work, our algorithm couples the batch size to the learning rate,\ndirectly reflecting the known relationship between the two. On popular image\nclassification benchmarks, our batch size adaptation yields faster optimization\nconvergence, while simultaneously simplifying learning rate tuning. A\nTensorFlow implementation is available.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 14:42:45 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 12:07:02 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Balles", "Lukas", ""], ["Romero", "Javier", ""], ["Hennig", "Philipp", ""]]}, {"id": "1612.05153", "submitter": "Rainer Kelz", "authors": "Rainer Kelz, Matthias Dorfer, Filip Korzeniowski, Sebastian B\\\"ock,\n  Andreas Arzt, Gerhard Widmer", "title": "On the Potential of Simple Framewise Approaches to Piano Transcription", "comments": "Proceedings of the 17th International Society for Music Information\n  Retrieval Conference (ISMIR 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In an attempt at exploring the limitations of simple approaches to the task\nof piano transcription (as usually defined in MIR), we conduct an in-depth\nanalysis of neural network-based framewise transcription. We systematically\ncompare different popular input representations for transcription systems to\ndetermine the ones most suitable for use with neural networks. Exploiting\nrecent advances in training techniques and new regularizers, and taking into\naccount hyper-parameter tuning, we show that it is possible, by simple\nbottom-up frame-wise processing, to obtain a piano transcriber that outperforms\nthe current published state of the art on the publicly available MAPS dataset\n-- without any complex post-processing steps. Thus, we propose this simple\napproach as a new baseline for this dataset, for future transcription research\nto build on and improve.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 17:32:11 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Kelz", "Rainer", ""], ["Dorfer", "Matthias", ""], ["Korzeniowski", "Filip", ""], ["B\u00f6ck", "Sebastian", ""], ["Arzt", "Andreas", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1612.05159", "submitter": "Harm van Seijen", "authors": "Harm van Seijen and Mehdi Fatemi and Joshua Romoff and Romain Laroche", "title": "Separation of Concerns in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework for solving a single-agent task by\nusing multiple agents, each focusing on different aspects of the task. This\napproach has two main advantages: 1) it allows for training specialized agents\non different parts of the task, and 2) it provides a new way to transfer\nknowledge, by transferring trained agents. Our framework generalizes the\ntraditional hierarchical decomposition, in which, at any moment in time, a\nsingle agent has control until it has solved its particular subtask. We\nillustrate our framework with empirical experiments on two domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 17:41:41 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 19:43:48 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["van Seijen", "Harm", ""], ["Fatemi", "Mehdi", ""], ["Romoff", "Joshua", ""], ["Laroche", "Romain", ""]]}, {"id": "1612.05203", "submitter": "Kai Xu", "authors": "Kai Xu, Fengbo Ren", "title": "CSVideoNet: A Real-time End-to-end Learning Framework for\n  High-frame-rate Video Compressive Sensing", "comments": "9 pages, 6 figures, 7 tables. Accepted by WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the real-time encoding-decoding problem for\nhigh-frame-rate video compressive sensing (CS). Unlike prior works that perform\nreconstruction using iterative optimization-based approaches, we propose a\nnon-iterative model, named \"CSVideoNet\". CSVideoNet directly learns the inverse\nmapping of CS and reconstructs the original input in a single forward\npropagation. To overcome the limitations of existing CS cameras, we propose a\nmulti-rate CNN and a synthesizing RNN to improve the trade-off between\ncompression ratio (CR) and spatial-temporal resolution of the reconstructed\nvideos. The experiment results demonstrate that CSVideoNet significantly\noutperforms the state-of-the-art approaches. With no pre/post-processing, we\nachieve 25dB PSNR recovery quality at 100x CR, with a frame rate of 125 fps on\na Titan X GPU. Due to the feedforward and high-data-concurrency natures of\nCSVideoNet, it can take advantage of GPU acceleration to achieve three orders\nof magnitude speed-up over conventional iterative-based approaches. We share\nthe source code at https://github.com/PSCLab-ASU/CSVideoNet.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 19:28:38 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 20:19:46 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 19:40:08 GMT"}, {"version": "v4", "created": "Wed, 5 Apr 2017 05:13:49 GMT"}, {"version": "v5", "created": "Sun, 28 Jan 2018 04:32:43 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Xu", "Kai", ""], ["Ren", "Fengbo", ""]]}, {"id": "1612.05231", "submitter": "Li Jing", "authors": "Li Jing, Yichen Shen, Tena Dub\\v{c}ek, John Peurifoy, Scott Skirlo,\n  Yann LeCun, Max Tegmark, Marin Solja\\v{c}i\\'c", "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application\n  to RNNs", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using unitary (instead of general) matrices in artificial neural networks\n(ANNs) is a promising way to solve the gradient explosion/vanishing problem, as\nwell as to enable ANNs to learn long-term correlations in the data. This\napproach appears particularly promising for Recurrent Neural Networks (RNNs).\nIn this work, we present a new architecture for implementing an Efficient\nUnitary Neural Network (EUNNs); its main advantages can be summarized as\nfollows. Firstly, the representation capacity of the unitary space in an EUNN\nis fully tunable, ranging from a subspace of SU(N) to the entire unitary space.\nSecondly, the computational complexity for training an EUNN is merely\n$\\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on\nthe standard copying task, the pixel-permuted MNIST digit recognition benchmark\nas well as the Speech Prediction Test (TIMIT). We find that our architecture\nsignificantly outperforms both other state-of-the-art unitary RNNs and the LSTM\narchitecture, in terms of the final performance and/or the wall-clock training\nspeed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide\nvariety of applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:39:15 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 19:00:50 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 17:13:38 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Jing", "Li", ""], ["Shen", "Yichen", ""], ["Dub\u010dek", "Tena", ""], ["Peurifoy", "John", ""], ["Skirlo", "Scott", ""], ["LeCun", "Yann", ""], ["Tegmark", "Max", ""], ["Solja\u010di\u0107", "Marin", ""]]}, {"id": "1612.05236", "submitter": "Shripad Gade", "authors": "Shripad Gade and Nitin H. Vaidya", "title": "Private Learning on Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual data collection and widespread deployment of machine learning\nalgorithms, particularly the distributed variants, have raised new privacy\nchallenges. In a distributed machine learning scenario, the dataset is stored\namong several machines and they solve a distributed optimization problem to\ncollectively learn the underlying model. We present a secure multi-party\ncomputation inspired privacy preserving distributed algorithm for optimizing a\nconvex function consisting of several possibly non-convex functions. Each\nindividual objective function is privately stored with an agent while the\nagents communicate model parameters with neighbor machines connected in a\nnetwork. We show that our algorithm can correctly optimize the overall\nobjective function and learn the underlying model accurately. We further prove\nthat under a vertex connectivity condition on the topology, our algorithm\npreserves privacy of individual objective functions. We establish limits on the\nwhat a coalition of adversaries can learn by observing the messages and states\nshared over a network.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:44:50 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Gade", "Shripad", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1612.05270", "submitter": "Daniela Moctezuma", "authors": "Eric S. Tellez, Sabino Miranda Jim\\'enez, Mario Graff, Daniela\n  Moctezuma, Ranyart R. Su\\'arez, Oscar S. Siordia", "title": "A Simple Approach to Multilingual Polarity Classification in Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, sentiment analysis has received a lot of attention due to the\ninterest in mining opinions of social media users. Sentiment analysis consists\nin determining the polarity of a given text, i.e., its degree of positiveness\nor negativeness. Traditionally, Sentiment Analysis algorithms have been\ntailored to a specific language given the complexity of having a number of\nlexical variations and errors introduced by the people generating content. In\nthis contribution, our aim is to provide a simple to implement and easy to use\nmultilingual framework, that can serve as a baseline for sentiment analysis\ncontests, and as starting point to build new sentiment analysis systems. We\ncompare our approach in eight different languages, three of them have important\ninternational contests, namely, SemEval (English), TASS (Spanish), and\nSENTIPOLC (Italian). Within the competitions our approach reaches from medium\nto high positions in the rankings; whereas in the remaining languages our\napproach outperforms the reported results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 21:07:12 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Tellez", "Eric S.", ""], ["Jim\u00e9nez", "Sabino Miranda", ""], ["Graff", "Mario", ""], ["Moctezuma", "Daniela", ""], ["Su\u00e1rez", "Ranyart R.", ""], ["Siordia", "Oscar S.", ""]]}, {"id": "1612.05296", "submitter": "Ben Fulcher", "authors": "Ben D Fulcher and Nick S Jones", "title": "Automatic time-series phenotyping using massive feature extraction", "comments": null, "journal-ref": "Cell Systems 5 (2017) 527", "doi": "10.1016/j.cels.2017.10.001", "report-no": null, "categories": "cs.LG physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across a far-reaching diversity of scientific and industrial applications, a\ngeneral key problem involves relating the structure of time-series data to a\nmeaningful outcome, such as detecting anomalous events from sensor recordings,\nor diagnosing patients from physiological time-series measurements like heart\nrate or brain activity. Currently, researchers must devote considerable effort\nmanually devising, or searching for, properties of their time series that are\nsuitable for the particular analysis problem at hand. Addressing this\nnon-systematic and time-consuming procedure, here we introduce a new tool,\nhctsa, that selects interpretable and useful properties of time series\nautomatically, by comparing implementations over 7700 time-series features\ndrawn from diverse scientific literatures. Using two exemplar biological\napplications, we show how hctsa allows researchers to leverage decades of\ntime-series research to quantify and understand informative structure in their\ntime-series data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 22:40:57 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Fulcher", "Ben D", ""], ["Jones", "Nick S", ""]]}, {"id": "1612.05299", "submitter": "Karl Ridgeway", "authors": "Karl Ridgeway", "title": "A Survey of Inductive Biases for Factorial Representation-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the resurgence of interest in neural networks, representation learning\nhas re-emerged as a central focus in artificial intelligence. Representation\nlearning refers to the discovery of useful encodings of data that make\ndomain-relevant information explicit. Factorial representations identify\nunderlying independent causal factors of variation in data. A factorial\nrepresentation is compact and faithful, makes the causal factors explicit, and\nfacilitates human interpretation of data. Factorial representations support a\nvariety of applications, including the generation of novel examples, indexing\nand search, novelty detection, and transfer learning.\n  This article surveys various constraints that encourage a learning algorithm\nto discover factorial representations. I dichotomize the constraints in terms\nof unsupervised and supervised inductive bias. Unsupervised inductive biases\nexploit assumptions about the environment, such as the statistical distribution\nof factor coefficients, assumptions about the perturbations a factor should be\ninvariant to (e.g. a representation of an object can be invariant to rotation,\ntranslation or scaling), and assumptions about how factors are combined to\nsynthesize an observation. Supervised inductive biases are constraints on the\nrepresentations based on additional information connected to observations.\nSupervisory labels come in variety of types, which vary in how strongly they\nconstrain the representation, how many factors are labeled, how many\nobservations are labeled, and whether or not we know the associations between\nthe constraints and the factors they are related to.\n  This survey brings together a wide variety of models that all touch on the\nproblem of learning factorial representations and lays out a framework for\ncomparing these models based on the strengths of the underlying supervised and\nunsupervised inductive biases.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 22:55:41 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Ridgeway", "Karl", ""]]}, {"id": "1612.05356", "submitter": "Jie Liu", "authors": "Jie Liu, Martin Takac", "title": "Projected Semi-Stochastic Gradient Descent Method with Mini-Batch Scheme\n  under Weak Strong Convexity Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a projected semi-stochastic gradient descent method with\nmini-batch for improving both the theoretical complexity and practical\nperformance of the general stochastic gradient descent method (SGD). We are\nable to prove linear convergence under weak strong convexity assumption. This\nrequires no strong convexity assumption for minimizing the sum of smooth convex\nfunctions subject to a compact polyhedral set, which remains popular across\nmachine learning community. Our PS2GD preserves the low-cost per iteration and\nhigh optimization accuracy via stochastic gradient variance-reduced technique,\nand admits a simple parallel implementation with mini-batches. Moreover, PS2GD\nis also applicable to dual problem of SVM with hinge loss.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 03:54:30 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 02:40:16 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 00:44:45 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Liu", "Jie", ""], ["Takac", "Martin", ""]]}, {"id": "1612.05369", "submitter": "Pengfei Sun", "authors": "Pengfei Sun and Jun Qin", "title": "Neural networks based EEG-Speech Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end neural network (NN) based EEG-speech\n(NES) modeling framework, in which three network structures are developed to\nmap imagined EEG signals to phonemes. The proposed NES models incorporate a\nlanguage model based EEG feature extraction layer, an acoustic feature mapping\nlayer, and a restricted Boltzmann machine (RBM) based the feature learning\nlayer. The NES models can jointly realize the representation of multichannel\nEEG signals and the projection of acoustic speech signals. Among three proposed\nNES models, two augmented networks utilize spoken EEG signals as either bias or\ngate information to strengthen the feature learning and translation of imagined\nEEG signals. Experimental results show that all three proposed NES models\noutperform the baseline support vector machine (SVM) method on EEG-speech\nclassification. With respect to binary classification, our approach achieves\ncomparable results relative to deep believe network approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 05:09:14 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 20:17:12 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Sun", "Pengfei", ""], ["Qin", "Jun", ""]]}, {"id": "1612.05502", "submitter": "Neil Seward", "authors": "Neil Seward", "title": "Defensive Player Classification in the National Basketball Association", "comments": "Methods provided in paper do not go into enough detail into how\n  defensive formations are beneficial to teams. Needs more explanation behind\n  formations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The National Basketball Association(NBA) has expanded their data gathering\nand have heavily invested in new technologies to gather advanced performance\nmetrics on players. This expanded data set allows analysts to use unique\nperformance metrics in models to estimate and classify player performance.\nInstead of grouping players together based on physical attributes and positions\nplayed, analysts can group together players that play similar to each other\nbased on these tracked metrics. Existing methods for player classification have\ntypically used offensive metrics for clustering [1]. There have been attempts\nto classify players using past defensive metrics, but the lack of quality\nmetrics has not produced promising results. The classifications presented in\nthe paper use newly introduced defensive metrics to find different defensive\npositions for each player. Without knowing the number of categories that\nplayers can be cast into, Gaussian Mixture Models (GMM) can be applied to find\nthe optimal number of clusters. In the model presented, five different\ndefensive player types can be identified.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:22:00 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 18:53:29 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Seward", "Neil", ""]]}, {"id": "1612.05533", "submitter": "Jingwei Zhang", "authors": "Jingwei Zhang, Jost Tobias Springenberg, Joschka Boedecker, Wolfram\n  Burgard", "title": "Deep Reinforcement Learning with Successor Features for Navigation\n  across Similar Environments", "comments": "Camera ready version for IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of robot navigation in simple maze-like\nenvironments where the robot has to rely on its onboard sensors to perform the\nnavigation task. In particular, we are interested in solutions to this problem\nthat do not require localization, mapping or planning. Additionally, we require\nthat our solution can quickly adapt to new situations (e.g., changing\nnavigation goals and environments). To meet these criteria we frame this\nproblem as a sequence of related reinforcement learning tasks. We propose a\nsuccessor feature based deep reinforcement learning algorithm that can learn to\ntransfer knowledge from previously mastered navigation tasks to new problem\ninstances. Our algorithm substantially decreases the required learning time\nafter the first task instance has been solved, which makes it easily adaptable\nto changing environments. We validate our method in both simulated and real\nrobot experiments with a Robotino and compare it to a set of baseline methods\nincluding classical planning-based navigation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 16:15:26 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 21:11:04 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 16:36:33 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zhang", "Jingwei", ""], ["Springenberg", "Jost Tobias", ""], ["Boedecker", "Joschka", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1612.05627", "submitter": "Giulio Ruffini", "authors": "Giulio Ruffini", "title": "Models, networks and algorithmic complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": "STARLAB TECHNICAL NOTE, TN00339 (V0.9)", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I aim to show that models, classification or generating functions,\ninvariances and datasets are algorithmically equivalent concepts once properly\ndefined, and provide some concrete examples of them. I then show that a) neural\nnetworks (NNs) of different kinds can be seen to implement models, b) that\nperturbations of inputs and nodes in NNs trained to optimally implement simple\nmodels propagate strongly, c) that there is a framework in which recurrent,\ndeep and shallow networks can be seen to fall into a descriptive power\nhierarchy in agreement with notions from the theory of recursive functions. The\nmotivation for these definitions and following analysis lies in the context of\ncognitive neuroscience, and in particular in Ruffini (2016), where the concept\nof model is used extensively, as is the concept of algorithmic complexity.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 00:54:03 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Ruffini", "Giulio", ""]]}, {"id": "1612.05628", "submitter": "Kavosh Asadi", "authors": "Kavosh Asadi, Michael L. Littman", "title": "An Alternative Softmax Operator for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A softmax operator applied to a set of values acts somewhat like the\nmaximization function and somewhat like an average. In sequential decision\nmaking, softmax is often used in settings where it is necessary to maximize\nutility but also to hedge against problems that arise from putting all of one's\nweight behind a single maximum utility decision. The Boltzmann softmax operator\nis the most commonly used softmax operator in this setting, but we show that\nthis operator is prone to misbehavior. In this work, we study a differentiable\nsoftmax operator that, among other properties, is a non-expansion ensuring a\nconvergent behavior in learning and planning. We introduce a variant of SARSA\nalgorithm that, by utilizing the new operator, computes a Boltzmann policy with\na state-dependent temperature parameter. We show that the algorithm is\nconvergent and that it performs favorably in practice.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 20:49:35 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 19:02:06 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 02:05:31 GMT"}, {"version": "v4", "created": "Tue, 13 Jun 2017 05:28:04 GMT"}, {"version": "v5", "created": "Wed, 14 Jun 2017 14:29:04 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Asadi", "Kavosh", ""], ["Littman", "Michael L.", ""]]}, {"id": "1612.05688", "submitter": "Xiujun Li", "authors": "Xiujun Li, Zachary C. Lipton, Bhuwan Dhingra, Lihong Li, Jianfeng Gao,\n  Yun-Nung Chen", "title": "A User Simulator for Task-Completion Dialogues", "comments": "14 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite widespread interests in reinforcement-learning for task-oriented\ndialogue systems, several obstacles can frustrate research and development\nprogress. First, reinforcement learners typically require interaction with the\nenvironment, so conventional dialogue corpora cannot be used directly. Second,\neach task presents specific challenges, requiring separate corpus of\ntask-specific annotated data. Third, collecting and annotating human-machine or\nhuman-human conversations for task-oriented dialogues requires extensive domain\nknowledge. Because building an appropriate dataset can be both financially\ncostly and time-consuming, one popular approach is to build a user simulator\nbased upon a corpus of example dialogues. Then, one can train reinforcement\nlearning agents in an online fashion as they interact with the simulator.\nDialogue agents trained on these simulators can serve as an effective starting\npoint. Once agents master the simulator, they may be deployed in a real\nenvironment to interact with humans, and continue to be trained online. To ease\nempirical algorithmic comparisons in dialogues, this paper introduces a new,\npublicly available simulation framework, where our simulator, designed for the\nmovie-booking domain, leverages both rules and collected data. The simulator\nsupports two tasks: movie ticket booking and movie seeking. Finally, we\ndemonstrate several agents and detail the procedure to add and test your own\nagent in the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 01:03:55 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 20:04:30 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 05:52:42 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Li", "Xiujun", ""], ["Lipton", "Zachary C.", ""], ["Dhingra", "Bhuwan", ""], ["Li", "Lihong", ""], ["Gao", "Jianfeng", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1612.05695", "submitter": "Pooya Ronagh", "authors": "Daniel Crawford, Anna Levit, Navid Ghadermarzy, Jaspreet S. Oberoi,\n  Pooya Ronagh", "title": "Reinforcement Learning Using Quantum Boltzmann Machines", "comments": null, "journal-ref": "Quantum Information & Computation, Volume 18 (1-2), pp. 0051-0074\n  (2018)", "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether quantum annealers with select chip layouts can\noutperform classical computers in reinforcement learning tasks. We associate a\ntransverse field Ising spin Hamiltonian with a layout of qubits similar to that\nof a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to\nnumerically simulate quantum sampling from this system. We design a\nreinforcement learning algorithm in which the set of visible nodes representing\nthe states and actions of an optimal policy are the first and last layers of\nthe deep network. In absence of a transverse field, our simulations show that\nDBMs are trained more effectively than restricted Boltzmann machines (RBM) with\nthe same number of nodes. We then develop a framework for training the network\nas a quantum Boltzmann machine (QBM) in the presence of a significant\ntransverse field for reinforcement learning. This method also outperforms the\nreinforcement learning method that uses RBMs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 02:33:41 GMT"}, {"version": "v2", "created": "Sun, 25 Dec 2016 08:18:19 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2019 20:49:47 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Crawford", "Daniel", ""], ["Levit", "Anna", ""], ["Ghadermarzy", "Navid", ""], ["Oberoi", "Jaspreet S.", ""], ["Ronagh", "Pooya", ""]]}, {"id": "1612.05708", "submitter": "Jacob Hunter", "authors": "Jacob S. Hunter (1) and Nathan O. Hodas (1) ((1) Pacific Northwest\n  National Laboratory)", "title": "Mutual information for fitting deep nonlinear models", "comments": null, "journal-ref": null, "doi": null, "report-no": "PNNL-SA-121434", "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep nonlinear models pose a challenge for fitting parameters due to lack of\nknowledge of the hidden layer and the potentially non-affine relation of the\ninitial and observed layers. In the present work we investigate the use of\ninformation theoretic measures such as mutual information and Kullback-Leibler\n(KL) divergence as objective functions for fitting such models without\nknowledge of the hidden layer. We investigate one model as a proof of concept\nand one application of cogntive performance. We further investigate the use of\noptimizers with these methods. Mutual information is largely successful as an\nobjective, depending on the parameters. KL divergence is found to be similarly\nsuccesful, given some knowledge of the statistics of the hidden layer.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 05:26:46 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Hunter", "Jacob S.", ""], ["Hodas", "Nathan O.", ""]]}, {"id": "1612.05729", "submitter": "Mirko Polato", "authors": "Mirko Polato and Fabio Aiolli", "title": "Exploiting sparsity to build efficient kernel based collaborative\n  filtering for top-N item recommendation", "comments": "Under revision for Neurocomputing (Elsevier Journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of implicit feedback datasets has raised the\ninterest in developing effective collaborative filtering techniques able to\ndeal asymmetrically with unambiguous positive feedback and ambiguous negative\nfeedback. In this paper, we propose a principled kernel-based collaborative\nfiltering method for top-N item recommendation with implicit feedback. We\npresent an efficient implementation using the linear kernel, and we show how to\ngeneralize it to kernels of the dot product family preserving the efficiency.\nWe also investigate on the elements which influence the sparsity of a standard\ncosine kernel. This analysis shows that the sparsity of the kernel strongly\ndepends on the properties of the dataset, in particular on the long tail\ndistribution. We compare our method with state-of-the-art algorithms achieving\ngood results both in terms of efficiency and effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 10:50:41 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Polato", "Mirko", ""], ["Aiolli", "Fabio", ""]]}, {"id": "1612.05730", "submitter": "Snehasis Banerjee", "authors": "Snehasis Banerjee, Tanushyam Chattopadhyay, Swagata Biswas, Rohan\n  Banerjee, Anirban Dutta Choudhury, Arpan Pal and Utpal Garain", "title": "Towards Wide Learning: Experiments in Healthcare", "comments": "4 pages, Machine Learning for Health Workshop, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Wide Learning architecture is proposed that attempts to\nautomate the feature engineering portion of the machine learning (ML) pipeline.\nFeature engineering is widely considered as the most time consuming and expert\nknowledge demanding portion of any ML task. The proposed feature recommendation\napproach is tested on 3 healthcare datasets: a) PhysioNet Challenge 2016\ndataset of phonocardiogram (PCG) signals, b) MIMIC II blood pressure\nclassification dataset of photoplethysmogram (PPG) signals and c) an emotion\nclassification dataset of PPG signals. While the proposed method beats the\nstate of the art techniques for 2nd and 3rd dataset, it reaches 94.38% of the\naccuracy level of the winner of PhysioNet Challenge 2016. In all cases, the\neffort to reach a satisfactory performance was drastically less (a few days)\nthan manual feature engineering.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 11:00:49 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 13:53:15 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Banerjee", "Snehasis", ""], ["Chattopadhyay", "Tanushyam", ""], ["Biswas", "Swagata", ""], ["Banerjee", "Rohan", ""], ["Choudhury", "Anirban Dutta", ""], ["Pal", "Arpan", ""], ["Garain", "Utpal", ""]]}, {"id": "1612.05740", "submitter": "Bohdan Pavlyshenko", "authors": "B. Pavlyshenko", "title": "Machine Learning, Linear and Bayesian Models for Logistic Regression in\n  Failure Detection Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the use of logistic regression in manufacturing\nfailures detection. As a data set for the analysis, we used the data from\nKaggle competition Bosch Production Line Performance. We considered the use of\nmachine learning, linear and Bayesian models. For machine learning approach, we\nanalyzed XGBoost tree based classifier to obtain high scored classification.\nUsing the generalized linear model for logistic regression makes it possible to\nanalyze the influence of the factors under study. The Bayesian approach for\nlogistic regression gives the statistical distribution for the parameters of\nthe model. It can be useful in the probabilistic analysis, e.g. risk\nassessment.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 11:57:45 GMT"}], "update_date": "2016-12-31", "authors_parsed": [["Pavlyshenko", "B.", ""]]}, {"id": "1612.05753", "submitter": "Seyed Sajad Mousavi", "authors": "Sajad Mousavi, Michael Schukat, Enda Howley, Ali Borji and Nasser\n  Mozayani", "title": "Learning to predict where to look in interactive environments using deep\n  recurrent q-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottom-Up (BU) saliency models do not perform well in complex interactive\nenvironments where humans are actively engaged in tasks (e.g., sandwich making\nand playing the video games). In this paper, we leverage Reinforcement Learning\n(RL) to highlight task-relevant locations of input frames. We propose a soft\nattention mechanism combined with the Deep Q-Network (DQN) model to teach an RL\nagent how to play a game and where to look by focusing on the most pertinent\nparts of its visual input. Our evaluations on several Atari 2600 games show\nthat the soft attention based model could predict fixation locations\nsignificantly better than bottom-up models such as Itti-Kochs saliency and\nGraph-Based Visual Saliency (GBVS) models.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 13:29:59 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 21:23:14 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Mousavi", "Sajad", ""], ["Schukat", "Michael", ""], ["Howley", "Enda", ""], ["Borji", "Ali", ""], ["Mozayani", "Nasser", ""]]}, {"id": "1612.05794", "submitter": "Zeeshan Malik Khawar", "authors": "Zeeshan Khawar Malik, Zain U. Hussain, Ziad Kobti, Charlie W. Lees,\n  Newton Howard and Amir Hussain", "title": "A new recurrent neural network based predictive model for Faecal\n  Calprotectin analysis: A retrospective study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faecal Calprotectin (FC) is a surrogate marker for intestinal inflammation,\ntermed Inflammatory Bowel Disease (IBD), but not for cancer. In this\nretrospective study of 804 patients, an enhanced benchmark predictive model for\nanalyzing FC is developed, based on a novel state-of-the-art Echo State Network\n(ESN), an advanced dynamic recurrent neural network which implements a\nbiologically plausible architecture, and a supervised learning mechanism. The\nproposed machine learning driven predictive model is benchmarked against a\nconventional logistic regression model, demonstrating statistically significant\nperformance improvements.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 17:01:08 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Malik", "Zeeshan Khawar", ""], ["Hussain", "Zain U.", ""], ["Kobti", "Ziad", ""], ["Lees", "Charlie W.", ""], ["Howard", "Newton", ""], ["Hussain", "Amir", ""]]}, {"id": "1612.05836", "submitter": "Shervin Ardeshir", "authors": "Shervin Ardeshir, Krishna Regmi, and Ali Borji", "title": "EgoTransfer: Transferring Motion Across Egocentric and Exocentric\n  Domains using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mirror neurons have been observed in the primary motor cortex of primate\nspecies, in particular in humans and monkeys. A mirror neuron fires when a\nperson performs a certain action, and also when he observes the same action\nbeing performed by another person. A crucial step towards building fully\nautonomous intelligent systems with human-like learning abilities is the\ncapability in modeling the mirror neuron. On one hand, the abundance of\negocentric cameras in the past few years has offered the opportunity to study a\nlot of vision problems from the first-person perspective. A great deal of\ninteresting research has been done during the past few years, trying to explore\nvarious computer vision tasks from the perspective of the self. On the other\nhand, videos recorded by traditional static cameras, capture humans performing\ndifferent actions from an exocentric third-person perspective. In this work, we\ntake the first step towards relating motion information across these two\nperspectives. We train models that predict motion in an egocentric view, by\nobserving it from an exocentric view, and vice versa. This allows models to\npredict how an egocentric motion would look like from outside. To do so, we\ntrain linear and nonlinear models and evaluate their performance in terms of\nretrieving the egocentric (exocentric) motion features, while having access to\nan exocentric (egocentric) motion feature. Our experimental results demonstrate\nthat motion information can be successfully transferred across the two views.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 23:33:37 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ardeshir", "Shervin", ""], ["Regmi", "Krishna", ""], ["Borji", "Ali", ""]]}, {"id": "1612.05888", "submitter": "Jiuyong Li", "authors": "Jiuyong Li, Lin Liu, Jixue Liu and Ryan Green", "title": "Building Diversified Multiple Trees for Classification in High\n  Dimensional Noisy Biomedical Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common that a trained classification model is applied to the operating\ndata that is deviated from the training data because of noise. This paper\ndemonstrates that an ensemble classifier, Diversified Multiple Tree (DMT), is\nmore robust in classifying noisy data than other widely used ensemble methods.\nDMT is tested on three real world biomedical data sets from different\nlaboratories in comparison with four benchmark ensemble classifiers.\nExperimental results show that DMT is significantly more accurate than other\nbenchmark ensemble classifiers on noisy test data. We also discuss a limitation\nof DMT and its possible variations.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 10:21:20 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 10:48:23 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Liu", "Jixue", ""], ["Green", "Ryan", ""]]}, {"id": "1612.05968", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie", "title": "Deep Multi-instance Networks with Sparse Label Assignment for Whole\n  Mammogram Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammogram classification is directly related to computer-aided diagnosis of\nbreast cancer. Traditional methods requires great effort to annotate the\ntraining data by costly manual labeling and specialized computational models to\ndetect these annotations during test. Inspired by the success of using deep\nconvolutional features for natural image analysis and multi-instance learning\nfor labeling a set of instances/patches, we propose end-to-end trained deep\nmulti-instance networks for mass classification based on whole mammogram\nwithout the aforementioned costly need to annotate the training data. We\nexplore three different schemes to construct deep multi-instance networks for\nwhole mammogram classification. Experimental results on the INbreast dataset\ndemonstrate the robustness of proposed deep networks compared to previous work\nusing segmentation and detection annotations in the training.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 18:31:11 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Zhu", "Wentao", ""], ["Lou", "Qi", ""], ["Vang", "Yeeleng Scott", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1612.05970", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Xiang Xiang, Trac D. Tran, Xiaohui Xie", "title": "Adversarial Deep Structural Networks for Mammographic Mass Segmentation", "comments": "First version on arXiv 2016, MICCAI 2017 Deep Learning in Medical\n  Image Analysis (DLMIA) workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass segmentation is an important task in mammogram analysis, providing\neffective morphological features and regions of interest (ROI) for mass\ndetection and classification. Inspired by the success of using deep\nconvolutional features for natural image analysis and conditional random fields\n(CRF) for structural learning, we propose an end-to-end network for\nmammographic mass segmentation. The network employs a fully convolutional\nnetwork (FCN) to model potential function, followed by a CRF to perform\nstructural learning. Because the mass distribution varies greatly with pixel\nposition, the FCN is combined with position priori for the task. Due to the\nsmall size of mammogram datasets, we use adversarial training to control\nover-fitting. Four models with different convolutional kernels are further\nfused to improve the segmentation results. Experimental results on two public\ndatasets, INbreast and DDSM-BCRP, show that our end-to-end network combined\nwith adversarial training achieves the-state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 18:40:21 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 21:32:38 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zhu", "Wentao", ""], ["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1612.05974", "submitter": "Francesco Conti", "authors": "Francesco Conti, Robert Schilling, Pasquale Davide Schiavone, Antonio\n  Pullini, Davide Rossi, Frank Kagan G\\\"urkaynak, Michael Muehlberghuber,\n  Michael Gautschi, Igor Loi, Germain Haugou, Stefan Mangard, Luca Benini", "title": "An IoT Endpoint System-on-Chip for Secure and Energy-Efficient\n  Near-Sensor Analytics", "comments": "15 pages, 12 figures, accepted for publication to the IEEE\n  Transactions on Circuits and Systems - I: Regular Papers", "journal-ref": null, "doi": "10.1109/TCSI.2017.2698019", "report-no": null, "categories": "cs.AR cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-sensor data analytics is a promising direction for IoT endpoints, as it\nminimizes energy spent on communication and reduces network load - but it also\nposes security concerns, as valuable data is stored or sent over the network at\nvarious stages of the analytics pipeline. Using encryption to protect sensitive\ndata at the boundary of the on-chip analytics engine is a way to address data\nsecurity issues. To cope with the combined workload of analytics and encryption\nin a tight power envelope, we propose Fulmine, a System-on-Chip based on a\ntightly-coupled multi-core cluster augmented with specialized blocks for\ncompute-intensive data processing and encryption functions, supporting software\nprogrammability for regular computing tasks. The Fulmine SoC, fabricated in\n65nm technology, consumes less than 20mW on average at 0.8V achieving an\nefficiency of up to 70pJ/B in encryption, 50pJ/px in convolution, or up to\n25MIPS/mW in software. As a strong argument for real-life flexible application\nof our platform, we show experimental results for three secure analytics use\ncases: secure autonomous aerial surveillance with a state-of-the-art deep CNN\nconsuming 3.16pJ per equivalent RISC op; local CNN-based face detection with\nsecured remote recognition in 5.74pJ/op; and seizure detection with encrypted\ndata collection from EEG within 12.7pJ/op.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 19:20:42 GMT"}, {"version": "v2", "created": "Sun, 2 Apr 2017 22:55:15 GMT"}, {"version": "v3", "created": "Sun, 23 Apr 2017 17:39:09 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Conti", "Francesco", ""], ["Schilling", "Robert", ""], ["Schiavone", "Pasquale Davide", ""], ["Pullini", "Antonio", ""], ["Rossi", "Davide", ""], ["G\u00fcrkaynak", "Frank Kagan", ""], ["Muehlberghuber", "Michael", ""], ["Gautschi", "Michael", ""], ["Loi", "Igor", ""], ["Haugou", "Germain", ""], ["Mangard", "Stefan", ""], ["Benini", "Luca", ""]]}, {"id": "1612.06000", "submitter": "Kavosh Asadi", "authors": "Kavosh Asadi, Jason D. Williams", "title": "Sample-efficient Deep Reinforcement Learning for Dialog Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a dialog policy as a recurrent neural network (RNN) is\nattractive because it handles partial observability, infers a latent\nrepresentation of state, and can be optimized with supervised learning (SL) or\nreinforcement learning (RL). For RL, a policy gradient approach is natural, but\nis sample inefficient. In this paper, we present 3 methods for reducing the\nnumber of dialogs required to optimize an RNN-based dialog policy with RL. The\nkey idea is to maintain a second RNN which predicts the value of the current\npolicy, and to apply experience replay to both networks. On two tasks, these\nmethods reduce the number of dialogs/episodes required by about a third, vs.\nstandard policy gradient methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 21:51:10 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Asadi", "Kavosh", ""], ["Williams", "Jason D.", ""]]}, {"id": "1612.06003", "submitter": "Bin Gu", "authors": "Bin Gu and De Wang and Zhouyuan Huo and Heng Huang", "title": "Inexact Proximal Gradient Methods for Non-convex and Non-smooth\n  Optimization", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning research, the proximal gradient methods are popular for\nsolving various optimization problems with non-smooth regularization. Inexact\nproximal gradient methods are extremely important when exactly solving the\nproximal operator is time-consuming, or the proximal operator does not have an\nanalytic solution. However, existing inexact proximal gradient methods only\nconsider convex problems. The knowledge of inexact proximal gradient methods in\nthe non-convex setting is very limited. % Moreover, for some machine learning\nmodels, there is still no proposed solver for exactly solving the proximal\noperator. To address this challenge, in this paper, we first propose three\ninexact proximal gradient algorithms, including the basic version and\nNesterov's accelerated version. After that, we provide the theoretical analysis\nto the basic and Nesterov's accelerated versions. The theoretical results show\nthat our inexact proximal gradient algorithms can have the same convergence\nrates as the ones of exact proximal gradient algorithms in the non-convex\nsetting.\n  Finally, we show the applications of our inexact proximal gradient algorithms\non three representative non-convex learning problems. All experimental results\nconfirm the superiority of our new inexact proximal gradient algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 22:14:36 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 12:38:31 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Gu", "Bin", ""], ["Wang", "De", ""], ["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "1612.06018", "submitter": "Erik Talvitie", "authors": "Erik Talvitie", "title": "Self-Correcting Models for Model-Based Reinforcement Learning", "comments": "Original paper appeared in Proceedings of the 31st AAAI Conference on\n  Artificial Intelligence, 2017. This version incorporates the appendix into\n  document (rather than as supplementary material), corrects a minor error in\n  Lemma 1, and fixes some type-os", "journal-ref": "Proceedings of the Thirty-First AAAI Conference on Artificial\n  Intelligence, 2597-2603 (2017)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an agent cannot represent a perfectly accurate model of its\nenvironment's dynamics, model-based reinforcement learning (MBRL) can fail\ncatastrophically. Planning involves composing the predictions of the model;\nwhen flawed predictions are composed, even minor errors can compound and render\nthe model useless for planning. Hallucinated Replay (Talvitie 2014) trains the\nmodel to \"correct\" itself when it produces errors, substantially improving MBRL\nwith flawed models. This paper theoretically analyzes this approach,\nilluminates settings in which it is likely to be effective or ineffective, and\npresents a novel error bound, showing that a model's ability to self-correct is\nmore tightly related to MBRL performance than one-step prediction error. These\nresults inspire an MBRL algorithm for deterministic MDPs with performance\nguarantees that are robust to model class limitations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 01:09:23 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 18:53:51 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Talvitie", "Erik", ""]]}, {"id": "1612.06052", "submitter": "Penghang Yin", "authors": "Penghang Yin, Shuai Zhang, Yingyong Qi, Jack Xin", "title": "Quantization and Training of Low Bit-Width Convolutional Neural Networks\n  for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LBW-Net, an efficient optimization based method for quantization\nand training of the low bit-width convolutional neural networks (CNNs).\nSpecifically, we quantize the weights to zero or powers of two by minimizing\nthe Euclidean distance between full-precision weights and quantized weights\nduring backpropagation. We characterize the combinatorial nature of the low\nbit-width quantization problem. For 2-bit (ternary) CNNs, the quantization of\n$N$ weights can be done by an exact formula in $O(N\\log N)$ complexity. When\nthe bit-width is three and above, we further propose a semi-analytical\nthresholding scheme with a single free parameter for quantization that is\ncomputationally inexpensive. The free parameter is further determined by\nnetwork retraining and object detection tests. LBW-Net has several desirable\nadvantages over full-precision CNNs, including considerable memory savings,\nenergy efficiency, and faster deployment. Our experiments on PASCAL VOC dataset\nshow that compared with its 32-bit floating-point counterpart, the performance\nof the 6-bit LBW-Net is nearly lossless in the object detection tasks, and can\neven do better in some real world visual scenes, while empirically enjoying\nmore than 4$\\times$ faster deployment.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 05:54:18 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 06:56:17 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Yin", "Penghang", ""], ["Zhang", "Shuai", ""], ["Qi", "Yingyong", ""], ["Xin", "Jack", ""]]}, {"id": "1612.06070", "submitter": "Mihir Mongia", "authors": "Mihir Mongia and Kundan Kumar and Akram Erraqabi and Yoshua Bengio", "title": "On Random Weights for Texture Generation in One Layer Neural Networks", "comments": "ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in the literature has shown experimentally that one can use the\nlower layers of a trained convolutional neural network (CNN) to model natural\ntextures. More interestingly, it has also been experimentally shown that only\none layer with random filters can also model textures although with less\nvariability. In this paper we ask the question as to why one layer CNNs with\nrandom filters are so effective in generating textures? We theoretically show\nthat one layer convolutional architectures (without a non-linearity) paired\nwith the an energy function used in previous literature, can in fact preserve\nand modulate frequency coefficients in a manner so that random weights and\npretrained weights will generate the same type of images. Based on the results\nof this analysis we question whether similar properties hold in the case where\none uses one convolution layer with a non-linearity. We show that in the case\nof ReLu non-linearity there are situations where only one input will give the\nminimum possible energy whereas in the case of no nonlinearity, there are\nalways infinite solutions that will give the minimum possible energy. Thus we\ncan show that in certain situations adding a ReLu non-linearity generates less\nvariable images.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 08:21:04 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Mongia", "Mihir", ""], ["Kumar", "Kundan", ""], ["Erraqabi", "Akram", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1612.06083", "submitter": "Yannis Papanikolaou", "authors": "Yannis Papanikolaou, Ioannis Katakis, Grigorios Tsoumakas", "title": "Hierarchical Partitioning of the Output Space in Multi-label Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchy Of Multi-label classifiers (HOMER) is a multi-label learning\nalgorithm that breaks the initial learning task to several, easier sub-tasks by\nfirst constructing a hierarchy of labels from a given label set and secondly\nemploying a given base multi-label classifier (MLC) to the resulting\nsub-problems. The primary goal is to effectively address class imbalance and\nscalability issues that often arise in real-world multi-label classification\nproblems. In this work, we present the general setup for a HOMER model and a\nsimple extension of the algorithm that is suited for MLCs that output rankings.\nFurthermore, we provide a detailed analysis of the properties of the algorithm,\nboth from an aspect of effectiveness and computational complexity. A secondary\ncontribution involves the presentation of a balanced variant of the k means\nalgorithm, which serves in the first step of the label hierarchy construction.\nWe conduct extensive experiments on six real-world datasets, studying\nempirically HOMER's parameters and providing examples of instantiations of the\nalgorithm with different clustering approaches and MLCs, The empirical results\ndemonstrate a significant improvement over the given base MLC.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 09:08:59 GMT"}], "update_date": "2016-12-31", "authors_parsed": [["Papanikolaou", "Yannis", ""], ["Katakis", "Ioannis", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "1612.06212", "submitter": "Thomas Laurent", "authors": "Thomas Laurent and James von Brecht", "title": "A recurrent neural network without chaos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an exceptionally simple gated recurrent neural network (RNN)\nthat achieves performance comparable to well-known gated architectures, such as\nLSTMs and GRUs, on the word-level language modeling task. We prove that our\nmodel has simple, predicable and non-chaotic dynamics. This stands in stark\ncontrast to more standard gated architectures, whose underlying dynamical\nsystems exhibit chaotic behavior.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 14:59:14 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Laurent", "Thomas", ""], ["von Brecht", "James", ""]]}, {"id": "1612.06246", "submitter": "Haipeng Luo", "authors": "Alekh Agarwal, Haipeng Luo, Behnam Neyshabur and Robert E. Schapire", "title": "Corralling a Band of Bandit Algorithms", "comments": "Accepted to COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of combining multiple bandit algorithms (that is, online\nlearning algorithms with partial feedback) with the goal of creating a master\nalgorithm that performs almost as well as the best base algorithm if it were to\nbe run on its own. The main challenge is that when run with a master, base\nalgorithms unavoidably receive much less feedback and it is thus critical that\nthe master not starve a base algorithm that might perform uncompetitively\ninitially but would eventually outperform others if given enough feedback. We\naddress this difficulty by devising a version of Online Mirror Descent with a\nspecial mirror map together with a sophisticated learning rate scheme. We show\nthat this approach manages to achieve a more delicate balance between\nexploiting and exploring base algorithms than previous works yielding superior\nregret bounds.\n  Our results are applicable to many settings, such as multi-armed bandits,\ncontextual bandits, and convex bandits. As examples, we present two main\napplications. The first is to create an algorithm that enjoys worst-case\nrobustness while at the same time performing much better when the environment\nis relatively easy. The second is to create an algorithm that works\nsimultaneously under different assumptions of the environment, such as\ndifferent priors or different loss structures.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 16:17:56 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 15:41:31 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 03:21:09 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Agarwal", "Alekh", ""], ["Luo", "Haipeng", ""], ["Neyshabur", "Behnam", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1612.06287", "submitter": "Antoine Deleforge", "authors": "Cl\\'ement Gaultier (PANAMA), Saurabh Kataria (PANAMA, IIT Kanpur),\n  Antoine Deleforge (PANAMA)", "title": "VAST : The Virtual Acoustic Space Traveler Dataset", "comments": "International Conference on Latent Variable Analysis and Signal\n  Separation (LVA/ICA), Feb 2017, Grenoble, France. International Conference on\n  Latent Variable Analysis and Signal Separation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new paradigm for sound source lo-calization referred\nto as virtual acoustic space traveling (VAST) and presents a first dataset\ndesigned for this purpose. Existing sound source localization methods are\neither based on an approximate physical model (physics-driven) or on a\nspecific-purpose calibration set (data-driven). With VAST, the idea is to learn\na mapping from audio features to desired audio properties using a massive\ndataset of simulated room impulse responses. This virtual dataset is designed\nto be maximally representative of the potential audio scenes that the\nconsidered system may be evolving in, while remaining reasonably compact. We\nshow that virtually-learned mappings on this dataset generalize to real data,\novercoming some intrinsic limitations of traditional binaural sound\nlocalization methods based on time differences of arrival.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 15:40:44 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Gaultier", "Cl\u00e9ment", "", "PANAMA"], ["Kataria", "Saurabh", "", "PANAMA, IIT Kanpur"], ["Deleforge", "Antoine", "", "PANAMA"]]}, {"id": "1612.06299", "submitter": "Shiva Kasiviswanathan", "authors": "Nina Narodytska, Shiva Prasad Kasiviswanathan", "title": "Simple Black-Box Adversarial Perturbations for Deep Networks", "comments": "19 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are powerful and popular learning models that achieve\nstate-of-the-art pattern recognition performance on many computer vision,\nspeech, and language processing tasks. However, these networks have also been\nshown susceptible to carefully crafted adversarial perturbations which force\nmisclassification of the inputs. Adversarial examples enable adversaries to\nsubvert the expected system behavior leading to undesired consequences and\ncould pose a security risk when these systems are deployed in the real world.\n  In this work, we focus on deep convolutional neural networks and demonstrate\nthat adversaries can easily craft adversarial examples even without any\ninternal knowledge of the target network. Our attacks treat the network as an\noracle (black-box) and only assume that the output of the network can be\nobserved on the probed inputs. Our first attack is based on a simple idea of\nadding perturbation to a randomly selected single pixel or a small set of them.\nWe then improve the effectiveness of this attack by carefully constructing a\nsmall set of pixels to perturb by using the idea of greedy local-search. Our\nproposed attacks also naturally extend to a stronger notion of\nmisclassification. Our extensive experimental results illustrate that even\nthese elementary attacks can reveal a deep neural network's vulnerabilities.\nThe simplicity and effectiveness of our proposed schemes mean that they could\nserve as a litmus test for designing robust networks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 18:12:20 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Narodytska", "Nina", ""], ["Kasiviswanathan", "Shiva Prasad", ""]]}, {"id": "1612.06340", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried and Farzana Yusuf", "title": "Computing Human-Understandable Strategies", "comments": "Earlier version appeared in Proceedings of the Workshop on Computer\n  Poker and Imperfect Information Games at AAAI Conference on Artificial\n  Intelligence, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for equilibrium computation generally make no attempt to ensure\nthat the computed strategies are understandable by humans. For instance the\nstrategies for the strongest poker agents are represented as massive binary\nfiles. In many situations, we would like to compute strategies that can\nactually be implemented by humans, who may have computational limitations and\nmay only be able to remember a small number of features or components of the\nstrategies that have been computed. We study poker games where private\ninformation distributions can be arbitrary. We create a large training set of\ngame instances and solutions, by randomly selecting the information\nprobabilities, and present algorithms that learn from the training instances in\norder to perform well in games with unseen information distributions. We are\nable to conclude several new fundamental rules about poker strategy that can be\neasily implemented by humans.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:40:19 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 17:54:11 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ganzfried", "Sam", ""], ["Yusuf", "Farzana", ""]]}, {"id": "1612.06370", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Ross Girshick, Piotr Doll\\'ar, Trevor Darrell, Bharath\n  Hariharan", "title": "Learning Features by Watching Objects Move", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel yet intuitive approach to unsupervised feature\nlearning. Inspired by the human visual system, we explore whether low-level\nmotion-based grouping cues can be used to learn an effective visual\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\nconvolutional network to segment objects from a single frame. Given the\nextensive evidence that motion plays a key role in the development of the human\nvisual system, we hope that this straightforward approach to unsupervised\nlearning will be more effective than cleverly designed 'pretext' tasks studied\nin the literature. Indeed, our extensive experiments show that this is the\ncase. When used for transfer learning on object detection, our representation\nsignificantly outperforms previous unsupervised approaches across multiple\nsettings, especially when training data for the target task is scarce.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:56:04 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 04:28:47 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Pathak", "Deepak", ""], ["Girshick", "Ross", ""], ["Doll\u00e1r", "Piotr", ""], ["Darrell", "Trevor", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1612.06470", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki, Stephen Becker", "title": "Randomized Clustered Nystrom for Large-Scale Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nystrom method has been popular for generating the low-rank approximation\nof kernel matrices that arise in many machine learning problems. The\napproximation quality of the Nystrom method depends crucially on the number of\nselected landmark points and the selection procedure. In this paper, we present\na novel algorithm to compute the optimal Nystrom low-approximation when the\nnumber of landmark points exceed the target rank. Moreover, we introduce a\nrandomized algorithm for generating landmark points that is scalable to\nlarge-scale data sets. The proposed method performs K-means clustering on\nlow-dimensional random projections of a data set and, thus, leads to\nsignificant savings for high-dimensional data sets. Our theoretical results\ncharacterize the tradeoffs between the accuracy and efficiency of our proposed\nmethod. Extensive experiments demonstrate the competitive performance as well\nas the efficiency of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 01:07:04 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Becker", "Stephen", ""]]}, {"id": "1612.06505", "submitter": "Kim Batselier", "authors": "Zhongming Chen, Kim Batselier, Johan A.K. Suykens, Ngai Wong", "title": "Parallelized Tensor Train Learning of Polynomial Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pattern classification, polynomial classifiers are well-studied methods as\nthey are capable of generating complex decision surfaces. Unfortunately, the\nuse of multivariate polynomials is limited to kernels as in support vector\nmachines, because polynomials quickly become impractical for high-dimensional\nproblems. In this paper, we effectively overcome the curse of dimensionality by\nemploying the tensor train format to represent a polynomial classifier. Based\non the structure of tensor trains, two learning algorithms are proposed which\ninvolve solving different optimization problems of low computational\ncomplexity. Furthermore, we show how both regularization to prevent overfitting\nand parallelization, which enables the use of large training sets, are\nincorporated into these methods. Both the efficiency and efficacy of our\ntensor-based polynomial classifier are then demonstrated on the two popular\ndatasets USPS and MNIST.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 04:54:49 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 08:37:39 GMT"}, {"version": "v3", "created": "Fri, 23 Dec 2016 12:48:53 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 01:03:44 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Chen", "Zhongming", ""], ["Batselier", "Kim", ""], ["Suykens", "Johan A. K.", ""], ["Wong", "Ngai", ""]]}, {"id": "1612.06519", "submitter": "Forrest Iandola", "authors": "Forrest Iandola", "title": "Exploring the Design Space of Deep Convolutional Neural Networks at\n  Large Scale", "comments": "thesis, UC Berkeley (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the research community has discovered that deep neural\nnetworks (DNNs) and convolutional neural networks (CNNs) can yield higher\naccuracy than all previous solutions to a broad array of machine learning\nproblems. To our knowledge, there is no single CNN/DNN architecture that solves\nall problems optimally. Instead, the \"right\" CNN/DNN architecture varies\ndepending on the application at hand. CNN/DNNs comprise an enormous design\nspace. Quantitatively, we find that a small region of the CNN design space\ncontains 30 billion different CNN architectures.\n  In this dissertation, we develop a methodology that enables systematic\nexploration of the design space of CNNs. Our methodology is comprised of the\nfollowing four themes.\n  1. Judiciously choosing benchmarks and metrics.\n  2. Rapidly training CNN models.\n  3. Defining and describing the CNN design space.\n  4. Exploring the design space of CNN architectures.\n  Taken together, these four themes comprise an effective methodology for\ndiscovering the \"right\" CNN architectures to meet the needs of practical\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 06:20:43 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Iandola", "Forrest", ""]]}, {"id": "1612.06598", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Sheng-Jun Huang, Daoqiang Zhang", "title": "WoCE: a framework for clustering ensemble by exploiting the wisdom of\n  Crowds theory", "comments": "Accepted in IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wisdom of Crowds (WOC), as a theory in the social science, gets a new\nparadigm in computer science. The WOC theory explains that the aggregate\ndecision made by a group is often better than those of its individual members\nif specific conditions are satisfied. This paper presents a novel framework for\nunsupervised and semi-supervised cluster ensemble by exploiting the WOC theory.\nWe employ four conditions in the WOC theory, i.e., diversity, independency,\ndecentralization and aggregation, to guide both the constructing of individual\nclustering results and the final combination for clustering ensemble. Firstly,\nindependency criterion, as a novel mapping system on the raw data set, removes\nthe correlation between features on our proposed method. Then, decentralization\nas a novel mechanism generates high-quality individual clustering results.\nNext, uniformity as a new diversity metric evaluates the generated clustering\nresults. Further, weighted evidence accumulation clustering method is proposed\nfor the final aggregation without using thresholding procedure. Experimental\nstudy on varied data sets demonstrates that the proposed approach achieves\nsuperior performance to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 10:39:45 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Huang", "Sheng-Jun", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1612.06623", "submitter": "Gal Dalal", "authors": "Raphael Canyasse, Gal Dalal, Shie Mannor", "title": "Supervised Learning for Optimal Power Flow as a Real-Time Proxy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we design and compare different supervised learning algorithms\nto compute the cost of Alternating Current Optimal Power Flow (ACOPF). The\nmotivation for quick calculation of OPF cost outcomes stems from the growing\nneed of algorithmic-based long-term and medium-term planning methodologies in\npower networks. Integrated in a multiple time-horizon coordination framework,\nwe refer to this approximation module as a proxy for predicting short-term\ndecision outcomes without the need of actual simulation and optimization of\nthem. Our method enables fast approximate calculation of OPF cost with less\nthan 1% error on average, achieved in run-times that are several orders of\nmagnitude lower than of exact computation. Several test-cases such as\nIEEE-RTS96 are used to demonstrate the efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 12:15:17 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Canyasse", "Raphael", ""], ["Dalal", "Gal", ""], ["Mannor", "Shie", ""]]}, {"id": "1612.06669", "submitter": "Siddharth Bhela", "authors": "Siddharth Bhela, Vassilis Kekatos, Sriharsha Veeramachaneni", "title": "Enhancing Observability in Distribution Grids using Smart Meter Data", "comments": "8 pages, 8 figures, submitted to IEEE Transactions on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to limited metering infrastructure, distribution grids are currently\nchallenged by observability issues. On the other hand, smart meter data,\nincluding local voltage magnitudes and power injections, are communicated to\nthe utility operator from grid buses with renewable generation and\ndemand-response programs. This work employs grid data from metered buses\ntowards inferring the underlying grid state. To this end, a coupled formulation\nof the power flow problem (CPF) is put forth. Exploiting the high variability\nof injections at metered buses, the controllability of solar inverters, and the\nrelative time-invariance of conventional loads, the idea is to solve the\nnon-linear power flow equations jointly over consecutive time instants. An\nintuitive and easily verifiable rule pertaining to the locations of metered and\nnon-metered buses on the physical grid is shown to be a necessary and\nsufficient criterion for local observability in radial networks. To account for\nnoisy smart meter readings, a coupled power system state estimation (CPSSE)\nproblem is further developed. Both CPF and CPSSE tasks are tackled via\naugmented semi-definite program relaxations. The observability criterion along\nwith the CPF and CPSSE solvers are numerically corroborated using synthetic and\nactual solar generation and load data on the IEEE 34-bus benchmark feeder.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 14:12:58 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Bhela", "Siddharth", ""], ["Kekatos", "Vassilis", ""], ["Veeramachaneni", "Sriharsha", ""]]}, {"id": "1612.06676", "submitter": "Andrey Lavrentyev Andrey Lavrentyev", "authors": "Pavel Filonov, Andrey Lavrentyev, Artem Vorontsov", "title": "Multivariate Industrial Time Series with Cyber-Attack Simulation: Fault\n  Detection Using an LSTM-based Predictive Data Model", "comments": "Accepted at NIPS Time Series Workshop 2016, Barcelona, Spain, 2016.\n  Reference update in this version,\n  https://sites.google.com/site/nipsts2016/NIPS_2016_TSW_paper_10.pdf?attredirects=0&d=1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adopted an approach based on an LSTM neural network to monitor and detect\nfaults in industrial multivariate time series data. To validate the approach we\ncreated a Modelica model of part of a real gasoil plant. By introducing hacks\ninto the logic of the Modelica model, we were able to generate both the roots\nand causes of fault behavior in the plant. Having a self-consistent data set\nwith labeled faults, we used an LSTM architecture with a forecasting error\nthreshold to obtain precision and recall quality metrics. The dependency of the\nquality metric on the threshold level is considered. An appropriate mechanism\nsuch as \"one handle\" was introduced for filtering faults that are outside of\nthe plant operator field of interest.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 14:24:49 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 11:26:03 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Filonov", "Pavel", ""], ["Lavrentyev", "Andrey", ""], ["Vorontsov", "Artem", ""]]}, {"id": "1612.06704", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Sunggyun Park, Kyunghyun Paeng, Joon-Young Lee, In So\n  Kweon", "title": "Action-Driven Object Detection with Top-Down Visual Attentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dominant paradigm for deep learning based object detection relies on a\n\"bottom-up\" approach using \"passive\" scoring of class agnostic proposals. These\napproaches are efficient but lack of holistic analysis of scene-level context.\nIn this paper, we present an \"action-driven\" detection mechanism using our\n\"top-down\" visual attention model. We localize an object by taking sequential\nactions that the attention model provides. The attention model conditioned with\nan image region provides required actions to get closer toward a target object.\nAn action at each time step is weak itself but an ensemble of the sequential\nactions makes a bounding-box accurately converge to a target object boundary.\nThis attention model we call AttentionNet is composed of a convolutional neural\nnetwork. During our whole detection procedure, we only utilize the actions from\na single AttentionNet without any modules for object proposals nor post\nbounding-box regression. We evaluate our top-down detection mechanism over the\nPASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art\nperformances compared to the major bottom-up detection methods. In particular,\nour detection mechanism shows a strong advantage in elaborate localization by\noutperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we\nincrease the IoU threshold for positive detection to 0.7.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 15:24:46 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Yoo", "Donggeun", ""], ["Park", "Sunggyun", ""], ["Paeng", "Kyunghyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1612.06851", "submitter": "Abhinav Shrivastava", "authors": "Abhinav Shrivastava, Rahul Sukthankar, Jitendra Malik, Abhinav Gupta", "title": "Beyond Skip Connections: Top-Down Modulation for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have seen tremendous progress in the field of object\ndetection. Most of the recent improvements have been achieved by targeting\ndeeper feedforward networks. However, many hard object categories such as\nbottle, remote, etc. require representation of fine details and not just\ncoarse, semantic representations. But most of these fine details are lost in\nthe early convolutional layers. What we need is a way to incorporate finer\ndetails from lower layers into the detection architecture. Skip connections\nhave been proposed to combine high-level and low-level features, but we argue\nthat selecting the right features from low-level requires top-down contextual\ninformation. Inspired by the human visual pathway, in this paper we propose\ntop-down modulations as a way to incorporate fine details into the detection\nframework. Our approach supplements the standard bottom-up, feedforward ConvNet\nwith a top-down modulation (TDM) network, connected using lateral connections.\nThese connections are responsible for the modulation of lower layer filters,\nand the top-down network handles the selection and integration of contextual\ninformation and low-level features. The proposed TDM architecture provides a\nsignificant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,\n35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any\nbells and whistles (e.g., multi-scale, iterative box refinement, etc.).\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 20:57:59 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 22:37:40 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Shrivastava", "Abhinav", ""], ["Sukthankar", "Rahul", ""], ["Malik", "Jitendra", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1612.06856", "submitter": "Haishuai Wang", "authors": "Haishuai Wang, Jia Wu, Peng Zhang, Chengqi Zhang", "title": "Temporal Feature Selection on Networked Time Series", "comments": "submitted to a blind review journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formulates the problem of learning discriminative features\n(\\textit{i.e.,} segments) from networked time series data considering the\nlinked information among time series. For example, social network users are\nconsidered to be social sensors that continuously generate social signals\n(tweets) represented as a time series. The discriminative segments are often\nreferred to as \\emph{shapelets} in a time series. Extracting shapelets for time\nseries classification has been widely studied. However, existing works on\nshapelet selection assume that the time series are independent and identically\ndistributed (i.i.d.). This assumption restricts their applications to social\nnetworked time series analysis, since a user's actions can be correlated to\nhis/her social affiliations. In this paper we propose a new Network Regularized\nLeast Squares (NetRLS) feature selection model that combines typical time\nseries data and user network data for analysis. Experiments on real-world\nnetworked time series Twitter and DBLP data demonstrate the performance of the\nproposed method. NetRLS performs better than LTS, the state-of-the-art time\nseries feature selection approach, on real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 19:33:35 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 20:58:04 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Wang", "Haishuai", ""], ["Wu", "Jia", ""], ["Zhang", "Peng", ""], ["Zhang", "Chengqi", ""]]}, {"id": "1612.06879", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Robust mixture of experts modeling using the skew $t$ distribution", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.06707", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) is a popular framework in the fields of statistics\nand machine learning for modeling heterogeneity in data for regression,\nclassification and clustering. MoE for continuous data are usually based on the\nnormal distribution. However, it is known that for data with asymmetric\nbehavior, heavy tails and atypical observations, the use of the normal\ndistribution is unsuitable. We introduce a new robust non-normal mixture of\nexperts modeling using the skew $t$ distribution. The proposed skew $t$ mixture\nof experts, named STMoE, handles these issues of the normal mixtures experts\nregarding possibly skewed, heavy-tailed and noisy data. We develop a dedicated\nexpectation conditional maximization (ECM) algorithm to estimate the model\nparameters by monotonically maximizing the observed data log-likelihood. We\ndescribe how the presented model can be used in prediction and in model-based\nclustering of regression data. Numerical experiments carried out on simulated\ndata show the effectiveness and the robustness of the proposed model in fitting\nnon-linear regression functions as well as in model-based clustering. Then, the\nproposed model is applied to the real-world data of tone perception for musical\ndata analysis, and the one of temperature anomalies for the analysis of climate\nchange data. The obtained results confirm the usefulness of the model for\npractical data analysis applications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 19:25:27 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1612.06890", "submitter": "Justin Johnson", "authors": "Justin Johnson and Bharath Hariharan and Laurens van der Maaten and Li\n  Fei-Fei and C. Lawrence Zitnick and Ross Girshick", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary\n  Visual Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 21:40:40 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Johnson", "Justin", ""], ["Hariharan", "Bharath", ""], ["van der Maaten", "Laurens", ""], ["Fei-Fei", "Li", ""], ["Zitnick", "C. Lawrence", ""], ["Girshick", "Ross", ""]]}, {"id": "1612.06935", "submitter": "Xingzhong Du", "authors": "Xingzhong Du, Hongzhi Yin, Ling Chen, Yang Wang, Yi Yang, Xiaofang\n  Zhou", "title": "Personalized Video Recommendation Using Rich Contents from Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video recommendation has become an essential way of helping people explore\nthe massive videos and discover the ones that may be of interest to them. In\nthe existing video recommender systems, the models make the recommendations\nbased on the user-video interactions and single specific content features. When\nthe specific content features are unavailable, the performance of the existing\nmodels will seriously deteriorate. Inspired by the fact that rich contents\n(e.g., text, audio, motion, and so on) exist in videos, in this paper, we\nexplore how to use these rich contents to overcome the limitations caused by\nthe unavailability of the specific ones. Specifically, we propose a novel\ngeneral framework that incorporates arbitrary single content feature with\nuser-video interactions, named as collaborative embedding regression (CER)\nmodel, to make effective video recommendation in both in-matrix and\nout-of-matrix scenarios. Our extensive experiments on two real-world\nlarge-scale datasets show that CER beats the existing recommender models with\nany single content feature and is more time efficient. In addition, we propose\na priority-based late fusion (PRI) method to gain the benefit brought by the\nintegrating the multiple content features. The corresponding experiment shows\nthat PRI brings real performance improvement to the baseline and outperforms\nthe existing fusion methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 01:01:49 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 00:55:16 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 12:05:27 GMT"}, {"version": "v4", "created": "Tue, 27 Jun 2017 01:42:29 GMT"}, {"version": "v5", "created": "Mon, 24 Jul 2017 08:12:58 GMT"}, {"version": "v6", "created": "Wed, 5 Dec 2018 03:56:00 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Du", "Xingzhong", ""], ["Yin", "Hongzhi", ""], ["Chen", "Ling", ""], ["Wang", "Yang", ""], ["Yang", "Yi", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1612.07019", "submitter": "Badong Chen", "authors": "Badong Chen, Lei Xing, Xin Wang, Jing Qin, Nanning Zheng", "title": "Robust Learning with Kernel Mean p-Power Error Loss", "comments": "11 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correntropy is a second order statistical measure in kernel space, which has\nbeen successfully applied in robust learning and signal processing. In this\npaper, we define a nonsecond order statistical measure in kernel space, called\nthe kernel mean-p power error (KMPE), including the correntropic loss (CLoss)\nas a special case. Some basic properties of KMPE are presented. In particular,\nwe apply the KMPE to extreme learning machine (ELM) and principal component\nanalysis (PCA), and develop two robust learning algorithms, namely ELM-KMPE and\nPCA-KMPE. Experimental results on synthetic and benchmark data show that the\ndeveloped algorithms can achieve consistently better performance when compared\nwith some existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 09:10:48 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chen", "Badong", ""], ["Xing", "Lei", ""], ["Wang", "Xin", ""], ["Qin", "Jing", ""], ["Zheng", "Nanning", ""]]}, {"id": "1612.07086", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu, Gang Wang, Jianfei Cai, Tsuhan Chen", "title": "An Empirical Study of Language CNN for Image Captioning", "comments": "Comments: 10 pages, In proceedings of ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language Models based on recurrent neural networks have dominated recent\nimage caption generation tasks. In this paper, we introduce a Language CNN\nmodel which is suitable for statistical language modeling tasks and shows\ncompetitive performance in image captioning. In contrast to previous models\nwhich predict next word based on one previous word and hidden state, our\nlanguage CNN is fed with all the previous words and can model the long-range\ndependencies of history words, which are critical for image captioning. The\neffectiveness of our approach is validated on two datasets MS COCO and\nFlickr30K. Our extensive experimental results show that our method outperforms\nthe vanilla recurrent neural network based language models and is competitive\nwith the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 13:04:18 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 07:02:31 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 12:33:50 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Wang", "Gang", ""], ["Cai", "Jianfei", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1612.07117", "submitter": "Nam Khanh Tran", "authors": "Nam Khanh Tran", "title": "Classification and Learning-to-rank Approaches for Cross-Device Matching\n  at CIKM Cup 2016", "comments": "CIKM Cup 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two methods for tackling the problem of\ncross-device matching for online advertising at CIKM Cup 2016. The first method\nconsiders the matching problem as a binary classification task and solve it by\nutilizing ensemble learning techniques. The second method defines the matching\nproblem as a ranking task and effectively solve it with using learning-to-rank\nalgorithms. The results show that the proposed methods obtain promising\nresults, in which the ranking-based method outperforms the classification-based\nmethod for the task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 15:02:41 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Tran", "Nam Khanh", ""]]}, {"id": "1612.07119", "submitter": "Yaman Umuroglu", "authors": "Yaman Umuroglu, Nicholas J. Fraser, Giulio Gambardella, Michaela\n  Blott, Philip Leong, Magnus Jahre, Kees Vissers", "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference", "comments": "To appear in the 25th International Symposium on Field-Programmable\n  Gate Arrays, February 2017", "journal-ref": null, "doi": "10.1145/3020078.3021744", "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown that convolutional neural networks contain significant\nredundancy, and high classification accuracy can be obtained even when weights\nand activations are reduced from floating point to binary values. In this\npaper, we present FINN, a framework for building fast and flexible FPGA\naccelerators using a flexible heterogeneous streaming architecture. By\nutilizing a novel set of optimizations that enable efficient mapping of\nbinarized neural networks to hardware, we implement fully connected,\nconvolutional and pooling layers, with per-layer compute resources being\ntailored to user-provided throughput requirements. On a ZC706 embedded FPGA\nplatform drawing less than 25 W total system power, we demonstrate up to 12.3\nmillion image classifications per second with 0.31 {\\mu}s latency on the MNIST\ndataset with 95.8% accuracy, and 21906 image classifications per second with\n283 {\\mu}s latency on the CIFAR-10 and SVHN datasets with respectively 80.1%\nand 94.9% accuracy. To the best of our knowledge, ours are the fastest\nclassification rates reported to date on these benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 22:19:47 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Umuroglu", "Yaman", ""], ["Fraser", "Nicholas J.", ""], ["Gambardella", "Giulio", ""], ["Blott", "Michaela", ""], ["Leong", "Philip", ""], ["Jahre", "Magnus", ""], ["Vissers", "Kees", ""]]}, {"id": "1612.07139", "submitter": "Lei Tai", "authors": "Lei Tai and Jingwei Zhang and Ming Liu and Joschka Boedecker and\n  Wolfram Burgard", "title": "A Survey of Deep Network Solutions for Learning Control in Robotics:\n  From Reinforcement to Imitation", "comments": "19 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been widely applied, achieving state-of-the-art\nresults in various fields of study. This survey focuses on deep learning\nsolutions that target learning control policies for robotics applications. We\ncarry out our discussions on the two main paradigms for learning control with\ndeep networks: deep reinforcement learning and imitation learning. For deep\nreinforcement learning (DRL), we begin from traditional reinforcement learning\nalgorithms, showing how they are extended to the deep context and effective\nmechanisms that could be added on top of the DRL algorithms. We then introduce\nrepresentative works that utilize DRL to solve navigation and manipulation\ntasks in robotics. We continue our discussion on methods addressing the\nchallenge of the reality gap for transferring DRL policies trained in\nsimulation to real-world scenarios, and summarize robotics simulation platforms\nfor conducting DRL research. For imitation leaning, we go through its three\nmain categories, behavior cloning, inverse reinforcement learning and\ngenerative adversarial imitation learning, by introducing their formulations\nand their corresponding robotics applications. Finally, we discuss the open\nchallenges and research frontiers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 14:31:47 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 09:20:29 GMT"}, {"version": "v3", "created": "Sun, 1 Jan 2017 09:47:49 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 03:46:53 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Tai", "Lei", ""], ["Zhang", "Jingwei", ""], ["Liu", "Ming", ""], ["Boedecker", "Joschka", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1612.07141", "submitter": "Carlos M. Ala\\'iz", "authors": "Carlos M. Ala\\'iz, Micha\\\"el Fanuel, Johan A. K. Suykens", "title": "Robust Classification of Graph-Based Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph-based classification method is proposed for semi-supervised learning\nin the case of Euclidean data and for classification in the case of graph data.\nOur manifold learning technique is based on a convex optimization problem\ninvolving a convex quadratic regularization term and a concave quadratic loss\nfunction with a trade-off parameter carefully chosen so that the objective\nfunction remains convex. As shown empirically, the advantage of considering a\nconcave loss function is that the learning problem becomes more robust in the\npresence of noisy labels. Furthermore, the loss function considered here is\nthen more similar to a classification loss while several other methods treat\ngraph-based classification problems as regression problems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 14:33:32 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 10:50:31 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 13:20:31 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Ala\u00edz", "Carlos M.", ""], ["Fanuel", "Micha\u00ebl", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1612.07146", "submitter": "Chao Du", "authors": "Chao Du, Chongxuan Li, Yin Zheng, Jun Zhu, Bo Zhang", "title": "Collaborative Filtering with User-Item Co-Autoregressive Models", "comments": "Published in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown promise in collaborative filtering (CF).\nHowever, existing neural approaches are either user-based or item-based, which\ncannot leverage all the underlying information explicitly. We propose CF-UIcA,\na neural co-autoregressive model for CF tasks, which exploits the structural\ncorrelation in the domains of both users and items. The co-autoregression\nallows extra desired properties to be incorporated for different tasks.\nFurthermore, we develop an efficient stochastic learning algorithm to handle\nlarge scale datasets. We evaluate CF-UIcA on two popular benchmarks: MovieLens\n1M and Netflix, and achieve state-of-the-art performance in both rating\nprediction and top-N recommendation tasks, which demonstrates the effectiveness\nof CF-UIcA.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 14:35:26 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 13:29:55 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 08:28:41 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Du", "Chao", ""], ["Li", "Chongxuan", ""], ["Zheng", "Yin", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1612.07182", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Alexander Peysakhovich, Marco Baroni", "title": "Multi-Agent Cooperation and the Emergence of (Natural) Language", "comments": "Accepted at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.GT cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current mainstream approach to train natural language systems is to\nexpose them to large amounts of text. This passive learning is problematic if\nwe are interested in developing interactive machines, such as conversational\nagents. We propose a framework for language learning that relies on multi-agent\ncommunication. We study this learning in the context of referential games. In\nthese games, a sender and a receiver see a pair of images. The sender is told\none of them is the target and is allowed to send a message from a fixed,\narbitrary vocabulary to the receiver. The receiver must rely on this message to\nidentify the target. Thus, the agents develop their own language interactively\nout of the need to communicate. We show that two networks with simple\nconfigurations are able to learn to coordinate in the referential game. We\nfurther explore how to make changes to the game environment to cause the \"word\nmeanings\" induced in the game to better reflect intuitive semantic properties\nof the images. In addition, we present a simple strategy for grounding the\nagents' code into natural language. Both of these are necessary steps towards\ndeveloping machines that are able to communicate with humans productively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 15:27:06 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 21:40:51 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Peysakhovich", "Alexander", ""], ["Baroni", "Marco", ""]]}, {"id": "1612.07222", "submitter": "Kevin Jiao", "authors": "Xi Chen, Kevin Jiao, Qihang Lin", "title": "Bayesian Decision Process for Cost-Efficient Dynamic Ranking via\n  Crowdsourcing", "comments": null, "journal-ref": "Journal of Machine Learning Research 17 (2016) 1-40", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank aggregation based on pairwise comparisons over a set of items has a wide\nrange of applications. Although considerable research has been devoted to the\ndevelopment of rank aggregation algorithms, one basic question is how to\nefficiently collect a large amount of high-quality pairwise comparisons for the\nranking purpose. Because of the advent of many crowdsourcing services, a crowd\nof workers are often hired to conduct pairwise comparisons with a small\nmonetary reward for each pair they compare. Since different workers have\ndifferent levels of reliability and different pairs have different levels of\nambiguity, it is desirable to wisely allocate the limited budget for\ncomparisons among the pairs of items and workers so that the global ranking can\nbe accurately inferred from the comparison results. To this end, we model the\nactive sampling problem in crowdsourced ranking as a Bayesian Markov decision\nprocess, which dynamically selects item pairs and workers to improve the\nranking accuracy under a budget constraint. We further develop a\ncomputationally efficient sampling policy based on knowledge gradient as well\nas a moment matching technique for posterior approximation. Experimental\nevaluations on both synthetic and real data show that the proposed policy\nachieves high ranking accuracy with a lower labeling cost.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 16:24:27 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chen", "Xi", ""], ["Jiao", "Kevin", ""], ["Lin", "Qihang", ""]]}, {"id": "1612.07307", "submitter": "Evan Shelhamer", "authors": "Evan Shelhamer, Parsa Mahmoudieh, Max Argus, Trevor Darrell", "title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning optimizes policies for expected cumulative reward.\nNeed the supervision be so narrow? Reward is delayed and sparse for many tasks,\nmaking it a difficult and impoverished signal for end-to-end optimization. To\naugment reward, we consider a range of self-supervised tasks that incorporate\nstates, actions, and successors to provide auxiliary losses. These losses offer\nubiquitous and instantaneous supervision for representation learning even in\nthe absence of reward. While current results show that learning from reward\nalone is feasible, pure reinforcement learning methods are constrained by\ncomputational and data efficiency issues that can be remedied by auxiliary\nlosses. Self-supervised pre-training and joint optimization improve the data\nefficiency and policy returns of end-to-end reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 20:29:26 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 18:29:09 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Shelhamer", "Evan", ""], ["Mahmoudieh", "Parsa", ""], ["Argus", "Max", ""], ["Darrell", "Trevor", ""]]}, {"id": "1612.07335", "submitter": "Amir Daneshmand", "authors": "Amir Daneshmand, Gesualdo Scutari, Francisco Facchinei", "title": "Distributed Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper studies distributed Dictionary Learning (DL) problems where the\nlearning task is distributed over a multi-agent network with time-varying\n(nonsymmetric) connectivity. This formulation is relevant, for instance, in\nbig-data scenarios where massive amounts of data are collected/stored in\ndifferent spatial locations and it is unfeasible to aggregate and/or process\nall the data in a fusion center, due to resource limitations, communication\noverhead or privacy considerations. We develop a general distributed\nalgorithmic framework for the (nonconvex) DL problem and establish its\nasymptotic convergence. The new method hinges on Successive Convex\nApproximation (SCA) techniques coupled with i) a gradient tracking mechanism\ninstrumental to locally estimate the missing global information; and ii) a\nconsensus step, as a mechanism to distribute the computations among the agents.\nTo the best of our knowledge, this is the first distributed algorithm with\nprovable convergence for the DL problem and, more in general, bi-convex\noptimization problems over (time-varying) directed graphs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 21:12:27 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Daneshmand", "Amir", ""], ["Scutari", "Gesualdo", ""], ["Facchinei", "Francisco", ""]]}, {"id": "1612.07374", "submitter": "Charmgil Hong", "authors": "Charmgil Hong, Milos Hauskrecht", "title": "Detecting Unusual Input-Output Associations in Multivariate Conditional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite tremendous progress in outlier detection research in recent years,\nthe majority of existing methods are designed only to detect unconditional\noutliers that correspond to unusual data patterns expressed in the joint space\nof all data attributes. Such methods are not applicable when we seek to detect\nconditional outliers that reflect unusual responses associated with a given\ncontext or condition. This work focuses on multivariate conditional outlier\ndetection, a special type of the conditional outlier detection problem, where\ndata instances consist of multi-dimensional input (context) and output\n(responses) pairs. We present a novel outlier detection framework that\nidentifies abnormal input-output associations in data with the help of a\ndecomposable conditional probabilistic model that is learned from all data\ninstances. Since components of this model can vary in their quality, we combine\nthem with the help of weights reflecting their reliability in assessment of\noutliers. We study two ways of calculating the component weights: global that\nrelies on all data, and local that relies only on instances similar to the\ntarget instance. Experimental results on data from various domains demonstrate\nthe ability of our framework to successfully identify multivariate conditional\noutliers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 22:43:08 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Hong", "Charmgil", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1612.07401", "submitter": "Ruijin Cang", "authors": "Ruijin Cang, Yaopengxiao Xu, Shaohua Chen, Yongming Liu, Yang Jiao,\n  Max Yi Ren", "title": "Microstructure Representation and Reconstruction of Heterogeneous\n  Materials via Deep Belief Network for Computational Material Design", "comments": "27 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Computational Materials Engineering (ICME) aims to accelerate\noptimal design of complex material systems by integrating material science and\ndesign automation. For tractable ICME, it is required that (1) a structural\nfeature space be identified to allow reconstruction of new designs, and (2) the\nreconstruction process be property-preserving. The majority of existing\nstructural presentation schemes rely on the designer's understanding of\nspecific material systems to identify geometric and statistical features, which\ncould be biased and insufficient for reconstructing physically meaningful\nmicrostructures of complex material systems. In this paper, we develop a\nfeature learning mechanism based on convolutional deep belief network to\nautomate a two-way conversion between microstructures and their\nlower-dimensional feature representations, and to achieves a 1000-fold\ndimension reduction from the microstructure space. The proposed model is\napplied to a wide spectrum of heterogeneous material systems with distinct\nmicrostructural features including Ti-6Al-4V alloy, Pb63-Sn37 alloy,\nFontainebleau sandstone, and Spherical colloids, to produce material\nreconstructions that are close to the original samples with respect to 2-point\ncorrelation functions and mean critical fracture strength. This capability is\nnot achieved by existing synthesis methods that rely on the Markovian\nassumption of material microstructures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 00:29:25 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 19:36:09 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 00:11:29 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Cang", "Ruijin", ""], ["Xu", "Yaopengxiao", ""], ["Chen", "Shaohua", ""], ["Liu", "Yongming", ""], ["Jiao", "Yang", ""], ["Ren", "Max Yi", ""]]}, {"id": "1612.07411", "submitter": "Huayu Li", "authors": "Huayu Li, Martin Renqiang Min, Yong Ge, Asim Kadav", "title": "A Context-aware Attention Network for Interactive Question Answering", "comments": "9 pages", "journal-ref": null, "doi": "10.1145/3097983.3098115", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based sequence-to-sequence models in an encoder-decoder\nframework have been successfully applied to solve Question Answering (QA)\nproblems, predicting answers from statements and questions. However, almost all\nprevious models have failed to consider detailed context information and\nunknown states under which systems do not have enough information to answer\ngiven questions. These scenarios with incomplete or ambiguous information are\nvery common in the setting of Interactive Question Answering (IQA). To address\nthis challenge, we develop a novel model, employing context-dependent\nword-level attention for more accurate statement representations and\nquestion-guided sentence-level attention for better context modeling. We also\ngenerate unique IQA datasets to test our model, which will be made publicly\navailable. Employing these attention mechanisms, our model accurately\nunderstands when it can output an answer or when it requires generating a\nsupplementary question for additional input depending on different contexts.\nWhen available, user's feedback is encoded and directly applied to update\nsentence-level attention to infer an answer. Extensive experiments on QA and\nIQA datasets quantitatively demonstrate the effectiveness of our model with\nsignificant improvement over state-of-the-art conventional QA models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 01:25:20 GMT"}, {"version": "v2", "created": "Sun, 3 Sep 2017 21:41:07 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Li", "Huayu", ""], ["Min", "Martin Renqiang", ""], ["Ge", "Yong", ""], ["Kadav", "Asim", ""]]}, {"id": "1612.07454", "submitter": "Angshul Majumdar Dr.", "authors": "Vanika Singhal, Shikha Singh and Angshul Majumdar", "title": "How to Train Your Deep Neural Network with Dictionary Learning", "comments": "DCC 2017 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently there are two predominant ways to train deep neural networks. The\nfirst one uses restricted Boltzmann machine (RBM) and the second one\nautoencoders. RBMs are stacked in layers to form deep belief network (DBN); the\nfinal representation layer is attached to the target to complete the deep\nneural network. Autoencoders are nested one inside the other to form stacked\nautoencoders; once the stcaked autoencoder is learnt the decoder portion is\ndetached and the target attached to the deepest layer of the encoder to form\nthe deep neural network. This work proposes a new approach to train deep neural\nnetworks using dictionary learning as the basic building block; the idea is to\nuse the features from the shallower layer as inputs for training the next\ndeeper layer. One can use any type of dictionary learning (unsupervised,\nsupervised, discriminative etc.) as basic units till the pre-final layer. In\nthe final layer one needs to use the label consistent dictionary learning\nformulation for classification. We compare our proposed framework with existing\nstate-of-the-art deep learning techniques on benchmark problems; we are always\nwithin the top 10 results. In actual problems of age and gender classification,\nwe are better than the best known techniques.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 06:17:01 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Singhal", "Vanika", ""], ["Singh", "Shikha", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1612.07516", "submitter": "Sascha Brauer", "authors": "Johannes Bl\\\"omer, Sascha Brauer, Kathrin Bujna", "title": "On Coreset Constructions for the Fuzzy $K$-Means Problem", "comments": "Coreset Construction unchanged, improved applications section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fuzzy $K$-means problem is a popular generalization of the well-known\n$K$-means problem to soft clusterings. We present the first coresets for fuzzy\n$K$-means with size linear in the dimension, polynomial in the number of\nclusters, and poly-logarithmic in the number of points. We show that these\ncoresets can be employed in the computation of a $(1+\\epsilon)$-approximation\nfor fuzzy $K$-means, improving previously presented results. We further show\nthat our coresets can be maintained in an insertion-only streaming setting,\nwhere data points arrive one-by-one.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:10:11 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 10:02:03 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 06:50:30 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Brauer", "Sascha", ""], ["Bujna", "Kathrin", ""]]}, {"id": "1612.07523", "submitter": "Monisankha Pal", "authors": "Monisankha Pal, Dipjyoti Paul, Md Sahidullah, Goutam Saha", "title": "Robustness of Voice Conversion Techniques Under Mismatched Conditions", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing studies on voice conversion (VC) are conducted in\nacoustically matched conditions between source and target signal. However, the\nrobustness of VC methods in presence of mismatch remains unknown. In this\npaper, we report a comparative analysis of different VC techniques under\nmismatched conditions. The extensive experiments with five different VC\ntechniques on CMU ARCTIC corpus suggest that performance of VC methods\nsubstantially degrades in noisy conditions. We have found that bilinear\nfrequency warping with amplitude scaling (BLFWAS) outperforms other methods in\nmost of the noisy conditions. We further explore the suitability of different\nspeech enhancement techniques for robust conversion. The objective evaluation\nresults indicate that spectral subtraction and log minimum mean square error\n(logMMSE) based speech enhancement techniques can be used to improve the\nperformance in specific noisy conditions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:14:59 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Pal", "Monisankha", ""], ["Paul", "Dipjyoti", ""], ["Sahidullah", "Md", ""], ["Saha", "Goutam", ""]]}, {"id": "1612.07548", "submitter": "Wendelin B\\\"ohmer", "authors": "Wendelin B\\\"ohmer and Rong Guo and Klaus Obermayer", "title": "Non-Deterministic Policy Improvement Stabilizes Approximated\n  Reinforcement Learning", "comments": "This paper has been presented at the 13th European Workshop on\n  Reinforcement Learning (EWRL 2016) on the 3rd and 4th of December 2016 in\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a type of instability that is linked to the greedy\npolicy improvement in approximated reinforcement learning. We show empirically\nthat non-deterministic policy improvement can stabilize methods like LSPI by\ncontrolling the improvements' stochasticity. Additionally we show that a\nsuitable representation of the value function also stabilizes the solution to\nsome degree. The presented approach is simple and should also be easily\ntransferable to more sophisticated algorithms like deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 11:30:35 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["B\u00f6hmer", "Wendelin", ""], ["Guo", "Rong", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1612.07562", "submitter": "Prasenjit Karmakar", "authors": "Prasenjit Karmakar, Shalabh Bhatnagar", "title": "On the function approximation error for risk-sensitive reinforcement\n  learning", "comments": "Improved the bound in Theorem V.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we obtain several informative error bounds on function\napproximation for the policy evaluation algorithm proposed by Basu et al. when\nthe aim is to find the risk-sensitive cost represented using exponential\nutility. The main idea is to use classical Bapat's inequality and to use\nPerron-Frobenius eigenvectors (exists if we assume irreducible Markov chain) to\nget the new bounds. The novelty of our approach is that we use the\nirreduciblity of Markov chain to get the new bounds whereas the earlier work by\nBasu et al. used spectral variation bound which is true for any matrix. We also\ngive examples where all our bounds achieve the \"actual error\" whereas the\nearlier bound given by Basu et al. is much weaker in comparison. We show that\nthis happens due to the absence of difference term in the earlier bound which\nis always present in all our bounds when the state space is large.\nAdditionally, we discuss how all our bounds compare with each other. As a\ncorollary of our main result we provide a bound between largest eigenvalues of\ntwo irreducibile matrices in terms of the matrix entries.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 12:05:29 GMT"}, {"version": "v10", "created": "Sun, 14 Oct 2018 06:01:20 GMT"}, {"version": "v11", "created": "Tue, 16 Oct 2018 16:48:38 GMT"}, {"version": "v12", "created": "Tue, 15 Jan 2019 19:43:06 GMT"}, {"version": "v13", "created": "Fri, 15 Feb 2019 12:59:43 GMT"}, {"version": "v14", "created": "Thu, 28 Mar 2019 15:10:52 GMT"}, {"version": "v15", "created": "Tue, 22 Oct 2019 14:48:35 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 03:22:28 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 15:35:03 GMT"}, {"version": "v4", "created": "Thu, 2 Mar 2017 11:44:54 GMT"}, {"version": "v5", "created": "Thu, 29 Jun 2017 11:19:13 GMT"}, {"version": "v6", "created": "Sun, 2 Jul 2017 15:47:10 GMT"}, {"version": "v7", "created": "Wed, 5 Jul 2017 14:01:42 GMT"}, {"version": "v8", "created": "Tue, 11 Jul 2017 12:14:48 GMT"}, {"version": "v9", "created": "Sun, 7 Oct 2018 10:13:02 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Karmakar", "Prasenjit", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1612.07597", "submitter": "Andreas Henelius", "authors": "Andreas Henelius, Antti Ukkonen, Kai Puolam\\\"aki", "title": "Finding Statistically Significant Attribute Interactions", "comments": "9 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many data exploration tasks it is meaningful to identify groups of\nattribute interactions that are specific to a variable of interest. For\ninstance, in a dataset where the attributes are medical markers and the\nvariable of interest (class variable) is binary indicating presence/absence of\ndisease, we would like to know which medical markers interact with respect to\nthe binary class label. These interactions are useful in several practical\napplications, for example, to gain insight into the structure of the data, in\nfeature selection, and in data anonymisation. We present a novel method, based\non statistical significance testing, that can be used to verify if the data set\nhas been created by a given factorised class-conditional joint distribution,\nwhere the distribution is parametrised by a partition of its attributes.\nFurthermore, we provide a method, named ASTRID, for automatically finding a\npartition of attributes describing the distribution that has generated the\ndata. State-of-the-art classifiers are utilised to capture the interactions\npresent in the data by systematically breaking attribute interactions and\nobserving the effect of this breaking on classifier performance. We empirically\ndemonstrate the utility of the proposed method with examples using real and\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 13:53:42 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 12:21:36 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Henelius", "Andreas", ""], ["Ukkonen", "Antti", ""], ["Puolam\u00e4ki", "Kai", ""]]}, {"id": "1612.07640", "submitter": "Rui Zhao", "authors": "Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang and Robert\n  X. Gao", "title": "Deep Learning and Its Applications to Machine Health Monitoring: A\n  Survey", "comments": "14 pages, 9 figures, submitted to IEEE Transactions on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2006, deep learning (DL) has become a rapidly growing research\ndirection, redefining state-of-the-art performances in a wide range of areas\nsuch as object recognition, image segmentation, speech recognition and machine\ntranslation. In modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread deployment of\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\nprovides useful tools for processing and analyzing these big machinery data.\nThe main purpose of this paper is to review and summarize the emerging research\nwork of deep learning on machine health monitoring. After the brief\nintroduction of deep learning techniques, the applications of deep learning in\nmachine health monitoring systems are reviewed mainly from the following\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\nFinally, some new trends of DL-based machine health monitoring methods are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 04:56:30 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Zhao", "Rui", ""], ["Yan", "Ruqiang", ""], ["Chen", "Zhenghua", ""], ["Mao", "Kezhi", ""], ["Wang", "Peng", ""], ["Gao", "Robert X.", ""]]}, {"id": "1612.07659", "submitter": "Youngjoo Seo", "authors": "Youngjoo Seo, Micha\\\"el Defferrard, Pierre Vandergheynst, Xavier\n  Bresson", "title": "Structured Sequence Modeling with Graph Convolutional Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep\nlearning model able to predict structured sequences of data. Precisely, GCRN is\na generalization of classical recurrent neural networks (RNN) to data\nstructured by an arbitrary graph. Such structured sequences can represent\nseries of frames in videos, spatio-temporal measurements on a network of\nsensors, or random walks on a vocabulary graph for natural language modeling.\nThe proposed model combines convolutional neural networks (CNN) on graphs to\nidentify spatial structures and RNN to find dynamic patterns. We study two\npossible architectures of GCRN, and apply the models to two practical problems:\npredicting moving MNIST data, and modeling natural language with the Penn\nTreebank dataset. Experiments show that exploiting simultaneously graph spatial\nand dynamic information about data can improve both precision and learning\nspeed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 15:53:57 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Seo", "Youngjoo", ""], ["Defferrard", "Micha\u00ebl", ""], ["Vandergheynst", "Pierre", ""], ["Bresson", "Xavier", ""]]}, {"id": "1612.07725", "submitter": "Alexandre Alves", "authors": "Alexandre Alves", "title": "Stacking machine learning classifiers to identify Higgs bosons at the\n  LHC", "comments": "20 pages, 4 figures, 3 tables. Version published in the Journal of\n  Instrumentation", "journal-ref": null, "doi": "10.1088/1748-0221/12/05/T05005", "report-no": null, "categories": "hep-ph cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) algorithms have been employed in the problem of\nclassifying signal and background events with high accuracy in particle\nphysics. In this paper, we compare the performance of a widespread ML\ntechnique, namely, \\emph{stacked generalization}, against the results of two\nstate-of-art algorithms: (1) a deep neural network (DNN) in the task of\ndiscovering a new neutral Higgs boson and (2) a scalable machine learning\nsystem for tree boosting, in the Standard Model Higgs to tau leptons channel,\nboth at the 8 TeV LHC. In a cut-and-count analysis, \\emph{stacking} three\nalgorithms performed around 16\\% worse than DNN but demanding far less\ncomputation efforts, however, the same \\emph{stacking} outperforms boosted\ndecision trees. Using the stacked classifiers in a multivariate statistical\nanalysis (MVA), on the other hand, significantly enhances the statistical\nsignificance compared to cut-and-count in both Higgs processes, suggesting that\ncombining an ensemble of simpler and faster ML algorithms with MVA tools is a\nbetter approach than building a complex state-of-art algorithm for\ncut-and-count.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 20:01:37 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 01:07:42 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 19:03:03 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Alves", "Alexandre", ""]]}, {"id": "1612.07771", "submitter": "Klaus Greff", "authors": "Klaus Greff and Rupesh K. Srivastava and J\\\"urgen Schmidhuber", "title": "Highway and Residual Networks learn Unrolled Iterative Estimation", "comments": "10 + 4 pages, accepted for ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The past year saw the introduction of new architectures such as Highway\nnetworks and Residual networks which, for the first time, enabled the training\nof feedforward networks with dozens to hundreds of layers using simple gradient\ndescent. While depth of representation has been posited as a primary reason for\ntheir success, there are indications that these architectures defy a popular\nview of deep learning as a hierarchical computation of increasingly abstract\nfeatures at each layer.\n  In this report, we argue that this view is incomplete and does not adequately\nexplain several recent findings. We propose an alternative viewpoint based on\nunrolled iterative estimation -- a group of successive layers iteratively\nrefine their estimates of the same features instead of computing an entirely\nnew representation. We demonstrate that this viewpoint directly leads to the\nconstruction of Highway and Residual networks. Finally we provide preliminary\nexperiments to discuss the similarities and differences between the two\narchitectures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 19:57:35 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 19:52:47 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 21:27:03 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Greff", "Klaus", ""], ["Srivastava", "Rupesh K.", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1612.07823", "submitter": "Marcell Vazquez-Chanlatte", "authors": "Marcell Vazquez-Chanlatte, Jyotirmoy V. Deshmukh, Xiaoqing Jin, Sanjit\n  A. Seshia", "title": "Logic-based Clustering and Learning for Time-Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To effectively analyze and design cyberphysical systems (CPS), designers\ntoday have to combat the data deluge problem, i.e., the burden of processing\nintractably large amounts of data produced by complex models and experiments.\nIn this work, we utilize monotonic Parametric Signal Temporal Logic (PSTL) to\ndesign features for unsupervised classification of time series data. This\nenables using off-the-shelf machine learning tools to automatically cluster\nsimilar traces with respect to a given PSTL formula. We demonstrate how this\ntechnique produces interpretable formulas that are amenable to analysis and\nunderstanding using a few representative examples. We illustrate this with case\nstudies related to automotive engine testing, highway traffic analysis, and\nauto-grading massively open online courses.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 21:58:32 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 19:58:58 GMT"}, {"version": "v3", "created": "Mon, 15 May 2017 20:07:13 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Vazquez-Chanlatte", "Marcell", ""], ["Deshmukh", "Jyotirmoy V.", ""], ["Jin", "Xiaoqing", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1612.07828", "submitter": "Ashish Shrivastava", "authors": "Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda\n  Wang, Russ Webb", "title": "Learning from Simulated and Unsupervised Images through Adversarial\n  Training", "comments": "Accepted at CVPR 2017 for oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent progress in graphics, it has become more tractable to train\nmodels on synthetic images, potentially avoiding the need for expensive\nannotations. However, learning from synthetic images may not achieve the\ndesired performance due to a gap between synthetic and real image\ndistributions. To reduce this gap, we propose Simulated+Unsupervised (S+U)\nlearning, where the task is to learn a model to improve the realism of a\nsimulator's output using unlabeled real data, while preserving the annotation\ninformation from the simulator. We develop a method for S+U learning that uses\nan adversarial network similar to Generative Adversarial Networks (GANs), but\nwith synthetic images as inputs instead of random vectors. We make several key\nmodifications to the standard GAN algorithm to preserve annotations, avoid\nartifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a\nlocal adversarial loss, and (iii) updating the discriminator using a history of\nrefined images. We show that this enables generation of highly realistic\nimages, which we demonstrate both qualitatively and with a user study. We\nquantitatively evaluate the generated images by training models for gaze\nestimation and hand pose estimation. We show a significant improvement over\nusing synthetic images, and achieve state-of-the-art results on the MPIIGaze\ndataset without any labeled real data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 22:10:51 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 21:24:52 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Shrivastava", "Ashish", ""], ["Pfister", "Tomas", ""], ["Tuzel", "Oncel", ""], ["Susskind", "Josh", ""], ["Wang", "Wenda", ""], ["Webb", "Russ", ""]]}, {"id": "1612.07843", "submitter": "Wojciech Samek", "authors": "Leila Arras, Franziska Horn, Gr\\'egoire Montavon, Klaus-Robert\n  M\\\"uller, Wojciech Samek", "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine\n  Learning Approach", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0181142", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text documents can be described by a number of abstract concepts such as\nsemantic category, writing style, or sentiment. Machine learning (ML) models\nhave been trained to automatically map documents to these abstract concepts,\nallowing to annotate very large text collections, more than could be processed\nby a human in a lifetime. Besides predicting the text's category very\naccurately, it is also highly desirable to understand how and why the\ncategorization process takes place. In this paper, we demonstrate that such\nunderstanding can be achieved by tracing the classification decision back to\nindividual words using layer-wise relevance propagation (LRP), a recently\ndeveloped technique for explaining predictions of complex non-linear\nclassifiers. We train two word-based ML models, a convolutional neural network\n(CNN) and a bag-of-words SVM classifier, on a topic categorization task and\nadapt the LRP method to decompose the predictions of these models onto words.\nResulting scores indicate how much individual words contribute to the overall\nclassification decision. This enables one to distill relevant information from\ntext documents without an explicit semantic information extraction step. We\nfurther use the word-wise relevance scores for generating novel vector-based\ndocument representations which capture semantic information. Based on these\ndocument vectors, we introduce a measure of model explanatory power and show\nthat, although the SVM and CNN models perform similarly in terms of\nclassification accuracy, the latter exhibits a higher level of explainability\nwhich makes it more comprehensible for humans and potentially more useful for\nother applications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 00:31:30 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Arras", "Leila", ""], ["Horn", "Franziska", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1612.07857", "submitter": "Waheed Bajwa", "authors": "Tong Wu, Prudhvi Gurram, Raghuveer M. Rao, and Waheed U. Bajwa", "title": "Human Action Attribute Learning From Video Data Using Low-Rank\n  Representations", "comments": "26 pages; 8 figures; 2 tables; Rutgers University Technical Report\n  #2020-07-001", "journal-ref": null, "doi": "10.7282/t3-t7fe-4a02", "report-no": "Rutgers University Technical Report #2020-07-001", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation of human actions as a sequence of human body movements or\naction attributes enables the development of models for human activity\nrecognition and summarization. We present an extension of the low-rank\nrepresentation (LRR) model, termed the clustering-aware structure-constrained\nlow-rank representation (CS-LRR) model, for unsupervised learning of human\naction attributes from video data. Our model is based on the union-of-subspaces\n(UoS) framework, and integrates spectral clustering into the LRR optimization\nproblem for better subspace clustering results. We lay out an efficient linear\nalternating direction method to solve the CS-LRR optimization problem. We also\nintroduce a hierarchical subspace clustering approach, termed hierarchical\nCS-LRR, to learn the attributes without the need for a priori specification of\ntheir number. By visualizing and labeling these action attributes, the\nhierarchical model can be used to semantically summarize long video sequences\nof human actions at multiple resolutions. A human action or activity can also\nbe uniquely represented as a sequence of transitions from one action attribute\nto another, which can then be used for human action recognition. We demonstrate\nthe effectiveness of the proposed model for semantic summarization and action\nrecognition through comprehensive experiments on five real-world human action\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 02:28:04 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 01:19:57 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wu", "Tong", ""], ["Gurram", "Prudhvi", ""], ["Rao", "Raghuveer M.", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1612.07896", "submitter": "Christopher Burges", "authors": "C.J.C. Burges, T. Hart, Z. Yang, S. Cucerzan, R.W. White, A.\n  Pastusiak, J. Lewis", "title": "A Base Camp for Scaling AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical machine learning (SML) methods share a major limitation\nwith the early approaches to AI: there is no scalable way to adapt them to new\ndomains. Human learning solves this in part by leveraging a rich, shared,\nupdateable world model. Such scalability requires modularity: updating part of\nthe world model should not impact unrelated parts. We have argued that such\nmodularity will require both \"correctability\" (so that errors can be corrected\nwithout introducing new errors) and \"interpretability\" (so that we can\nunderstand what components need correcting).\n  To achieve this, one could attempt to adapt state of the art SML systems to\nbe interpretable and correctable; or one could see how far the simplest\npossible interpretable, correctable learning methods can take us, and try to\ncontrol the limitations of SML methods by applying them only where needed. Here\nwe focus on the latter approach and we investigate two main ideas: \"Teacher\nAssisted Learning\", which leverages crowd sourcing to learn language; and\n\"Factored Dialog Learning\", which factors the process of application\ndevelopment into roles where the language competencies needed are isolated,\nenabling non-experts to quickly create new applications.\n  We test these ideas in an \"Automated Personal Assistant\" (APA) setting, with\ntwo scenarios: that of detecting user intent from a user-APA dialog; and that\nof creating a class of event reminder applications, where a non-expert\n\"teacher\" can then create specific apps. For the intent detection task, we use\na dataset of a thousand labeled utterances from user dialogs with Cortana, and\nwe show that our approach matches state of the art SML methods, but in addition\nprovides full transparency: the whole (editable) model can be summarized on one\nhuman-readable page. For the reminder app task, we ran small user studies to\nverify the efficacy of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 08:03:20 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Burges", "C. J. C.", ""], ["Hart", "T.", ""], ["Yang", "Z.", ""], ["Cucerzan", "S.", ""], ["White", "R. W.", ""], ["Pastusiak", "A.", ""], ["Lewis", "J.", ""]]}, {"id": "1612.07940", "submitter": "Lei Shu", "authors": "Lei Shu, Bing Liu, Hu Xu, Annice Kim", "title": "Supervised Opinion Aspect Extraction by Exploiting Past Extraction\n  Results", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key tasks of sentiment analysis of product reviews is to extract\nproduct aspects or features that users have expressed opinions on. In this\nwork, we focus on using supervised sequence labeling as the base approach to\nperforming the task. Although several extraction methods using sequence\nlabeling methods such as Conditional Random Fields (CRF) and Hidden Markov\nModels (HMM) have been proposed, we show that this supervised approach can be\nsignificantly improved by exploiting the idea of concept sharing across\nmultiple domains. For example, \"screen\" is an aspect in iPhone, but not only\niPhone has a screen, many electronic devices have screens too. When \"screen\"\nappears in a review of a new domain (or product), it is likely to be an aspect\ntoo. Knowing this information enables us to do much better extraction in the\nnew domain. This paper proposes a novel extraction method exploiting this idea\nin the context of supervised sequence labeling. Experimental results show that\nit produces markedly better results than without using the past information.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 11:32:37 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Shu", "Lei", ""], ["Liu", "Bing", ""], ["Xu", "Hu", ""], ["Kim", "Annice", ""]]}, {"id": "1612.07976", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Yusuke Mukuta, Yoshitaka Ushiku, Tatsuya Harada", "title": "DeMIAN: Deep Modality Invariant Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining common representations from different modalities is important in\nthat they are interchangeable with each other in a classification problem. For\nexample, we can train a classifier on image features in the common\nrepresentations and apply it to the testing of the text features in the\nrepresentations. Existing multi-modal representation learning methods mainly\naim to extract rich information from paired samples and train a classifier by\nthe corresponding labels; however, collecting paired samples and their labels\nsimultaneously involves high labor costs. Addressing paired modal samples\nwithout their labels and single modal data with their labels independently is\nmuch easier than addressing labeled multi-modal data. To obtain the common\nrepresentations under such a situation, we propose to make the distributions\nover different modalities similar in the learned representations, namely\nmodality-invariant representations. In particular, we propose a novel algorithm\nfor modality-invariant representation learning, named Deep Modality Invariant\nAdversarial Network (DeMIAN), which utilizes the idea of Domain Adaptation\n(DA). Using the modality-invariant representations learned by DeMIAN, we\nachieved better classification accuracy than with the state-of-the-art methods,\nespecially for some benchmark datasets of zero-shot learning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 14:07:01 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 02:29:15 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Saito", "Kuniaki", ""], ["Mukuta", "Yusuke", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1612.07993", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe", "title": "RSSL: Semi-supervised Learning in R", "comments": "Presented at RRPR 2016: 1st Workshop on Reproducible Research in\n  Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a package for semi-supervised learning research\nin the R programming language called RSSL. We cover the purpose of the package,\nthe methods it includes and comment on their use and implementation. We then\nshow, using several code examples, how the package can be used to replicate\nwell-known results from the semi-supervised learning literature.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 15:02:54 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Krijthe", "Jesse H.", ""]]}, {"id": "1612.08082", "submitter": "Onur Atan", "authors": "Onur Atan, William R. Zame, Qiaojun Feng, Mihaela van der Schaar", "title": "Constructing Effective Personalized Policies Using Counterfactual\n  Inference from Biased Data Sets with Many Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for constructing effective personalized\npolicies when the observed data lacks counter-factual information, is biased\nand possesses many features. The approach is applicable in a wide variety of\nsettings from healthcare to advertising to education to finance. These settings\nhave in common that the decision maker can observe, for each previous instance,\nan array of features of the instance, the action taken in that instance, and\nthe reward realized -- but not the rewards of actions that were not taken: the\ncounterfactual information. Learning in such settings is made even more\ndifficult because the observed data is typically biased by the existing policy\n(that generated the data) and because the array of features that might affect\nthe reward in a particular instance -- and hence should be taken into account\nin deciding on an action in each particular instance -- is often vast. The\napproach presented here estimates propensity scores for the observed data,\ninfers counterfactuals, identifies a (relatively small) number of features that\nare (most) relevant for each possible action and instance, and prescribes a\npolicy to be followed. Comparison of the proposed algorithm against the\nstate-of-art algorithm on actual datasets demonstrates that the proposed\nalgorithm achieves a significant improvement in performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 20:29:52 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 05:14:03 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 08:16:55 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Atan", "Onur", ""], ["Zame", "William R.", ""], ["Feng", "Qiaojun", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1612.08102", "submitter": "Xintao Wu", "authors": "Yuemeng Li, Xintao Wu, Aidong Lu", "title": "On Spectral Analysis of Directed Signed Graphs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that the adjacency eigenspace of a network contains key\ninformation of its underlying structure. However, there has been no study on\nspectral analysis of the adjacency matrices of directed signed graphs. In this\npaper, we derive theoretical approximations of spectral projections from such\ndirected signed networks using matrix perturbation theory. We use the derived\ntheoretical results to study the influences of negative intra cluster and inter\ncluster directed edges on node spectral projections. We then develop a spectral\nclustering based graph partition algorithm, SC-DSG, and conduct evaluations on\nboth synthetic and real datasets. Both theoretical analysis and empirical\nevaluation demonstrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 21:20:55 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Li", "Yuemeng", ""], ["Wu", "Xintao", ""], ["Lu", "Aidong", ""]]}, {"id": "1612.08354", "submitter": "Gwang Been Park", "authors": "Gwangbeen Park, Woobin Im", "title": "Image-Text Multi-Modal Representation Learning by Adversarial\n  Backpropagation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel method for image-text multi-modal representation learning.\nIn our knowledge, this work is the first approach of applying adversarial\nlearning concept to multi-modal learning and not exploiting image-text pair\ninformation to learn multi-modal feature. We only use category information in\ncontrast with most previous methods using image-text pair information for\nmulti-modal embedding. In this paper, we show that multi-modal feature can be\nachieved without image-text pair information and our method makes more similar\ndistribution with image and text in multi-modal feature space than other\nmethods which use image-text pair information. And we show our multi-modal\nfeature has universal semantic information, even though it was trained for\ncategory prediction. Our model is end-to-end backpropagation, intuitive and\neasily extended to other multi-modal learning work.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 09:51:18 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Park", "Gwangbeen", ""], ["Im", "Woobin", ""]]}, {"id": "1612.08388", "submitter": "Cesar Comin PhD", "authors": "Mayra Z. Rodriguez, Cesar H. Comin, Dalcimar Casanova, Odemir M.\n  Bruno, Diego R. Amancio, Francisco A. Rodrigues, Luciano da F. Costa", "title": "Clustering Algorithms: A Comparative Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real-world systems can be studied in terms of pattern recognition tasks,\nso that proper use (and understanding) of machine learning methods in practical\napplications becomes essential. While a myriad of classification methods have\nbeen proposed, there is no consensus on which methods are more suitable for a\ngiven dataset. As a consequence, it is important to comprehensively compare\nmethods in many possible scenarios. In this context, we performed a systematic\ncomparison of 7 well-known clustering methods available in the R language. In\norder to account for the many possible variations of data, we considered\nartificial datasets with several tunable properties (number of classes,\nseparation between classes, etc). In addition, we also evaluated the\nsensitivity of the clustering methods with regard to their parameters\nconfiguration. The results revealed that, when considering the default\nconfigurations of the adopted methods, the spectral approach usually\noutperformed the other clustering algorithms. We also found that the default\nconfiguration of the adopted implementations was not accurate. In these cases,\na simple approach based on random selection of parameters values proved to be a\ngood alternative to improve the performance. All in all, the reported approach\nprovides subsidies guiding the choice of clustering algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 14:25:32 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Rodriguez", "Mayra Z.", ""], ["Comin", "Cesar H.", ""], ["Casanova", "Dalcimar", ""], ["Bruno", "Odemir M.", ""], ["Amancio", "Diego R.", ""], ["Rodrigues", "Francisco A.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1612.08392", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Daoqiang Zhang", "title": "Multi-Region Neural Representation: A novel model for decoding visual\n  stimuli in human brains", "comments": "Accepted in SIAM International Conference on Data Mininig (SDM),\n  Houston, Texas, USA, April/27-29, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate Pattern (MVP) classification holds enormous potential for\ndecoding visual stimuli in the human brain by employing task-based fMRI data\nsets. There is a wide range of challenges in the MVP techniques, i.e.\ndecreasing noise and sparsity, defining effective regions of interest (ROIs),\nvisualizing results, and the cost of brain studies. In overcoming these\nchallenges, this paper proposes a novel model of neural representation, which\ncan automatically detect the active regions for each visual stimulus and then\nutilize these anatomical regions for visualizing and analyzing the functional\nactivities. Therefore, this model provides an opportunity for neuroscientists\nto ask this question: what is the effect of a stimulus on each of the detected\nregions instead of just study the fluctuation of voxels in the manually\nselected ROIs. Moreover, our method introduces analyzing snapshots of brain\nimage for decreasing sparsity rather than using the whole of fMRI time series.\nFurther, a new Gaussian smoothing method is proposed for removing noise of\nvoxels in the level of ROIs. The proposed method enables us to combine\ndifferent fMRI data sets for reducing the cost of brain studies. Experimental\nstudies on 4 visual categories (words, consonants, objects and nonsense photos)\nconfirm that the proposed method achieves superior performance to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 14:37:57 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1612.08406", "submitter": "Torsten Ensslin", "authors": "Torsten A. En{\\ss}lin, Jakob Knollm\\\"uller", "title": "Correlated signal inference by free energy exploration", "comments": "19 pages, 5 figures, submitted, updated acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML astro-ph.IM cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inference of correlated signal fields with unknown correlation structures\nis of high scientific and technological relevance, but poses significant\nconceptual and numerical challenges. To address these, we develop the\ncorrelated signal inference (CSI) algorithm within information field theory\n(IFT) and discuss its numerical implementation. To this end, we introduce the\nfree energy exploration (FrEE) strategy for numerical information field theory\n(NIFTy) applications. The FrEE strategy is to let the mathematical structure of\nthe inference problem determine the dynamics of the numerical solver. FrEE uses\nthe Gibbs free energy formalism for all involved unknown fields and correlation\nstructures without marginalization of nuisance quantities. It thereby avoids\nthe complexity marginalization often impose to IFT equations. FrEE\nsimultaneously solves for the mean and the uncertainties of signal, nuisance,\nand auxiliary fields, while exploiting any analytically calculable quantity.\nFinally, FrEE uses a problem specific and self-tuning exploration strategy to\nswiftly identify the optimal field estimates as well as their uncertainty maps.\nFor all estimated fields, properly weighted posterior samples drawn from their\nexact, fully non-Gaussian distributions can be generated. Here, we develop the\nFrEE strategies for the CSI of a normal, a log-normal, and a Poisson log-normal\nIFT signal inference problem and demonstrate their performances via their NIFTy\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 15:42:22 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 22:21:17 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["En\u00dflin", "Torsten A.", ""], ["Knollm\u00fcller", "Jakob", ""]]}, {"id": "1612.08425", "submitter": "Chris Hodapp", "authors": "Chris Hodapp", "title": "Unsupervised Learning for Computational Phenotyping", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With large volumes of health care data comes the research area of\ncomputational phenotyping, making use of techniques such as machine learning to\ndescribe illnesses and other clinical concepts from the data itself. The\n\"traditional\" approach of using supervised learning relies on a domain expert,\nand has two main limitations: requiring skilled humans to supply correct labels\nlimits its scalability and accuracy, and relying on existing clinical\ndescriptions limits the sorts of patterns that can be found. For instance, it\nmay fail to acknowledge that a disease treated as a single condition may really\nhave several subtypes with different phenotypes, as seems to be the case with\nasthma and heart disease. Some recent papers cite successes instead using\nunsupervised learning. This shows great potential for finding patterns in\nElectronic Health Records that would otherwise be hidden and that can lead to\ngreater understanding of conditions and treatments. This work implements a\nmethod derived strongly from Lasko et al., but implements it in Apache Spark\nand Python and generalizes it to laboratory time-series data in MIMIC-III. It\nis released as an open-source tool for exploration, analysis, and\nvisualization, available at https://github.com/Hodapp87/mimic3_phenotyping\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 18:47:11 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 16:25:34 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Hodapp", "Chris", ""]]}, {"id": "1612.08461", "submitter": "Liang Zhang", "authors": "Liang Zhang, Gang Wang, Daniel Romero, Georgios B. Giannakis", "title": "Randomized Block Frank-Wolfe for Convergent Large-Scale Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2755597", "report-no": null, "categories": "math.OC cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to their low-complexity iterations, Frank-Wolfe (FW) solvers are well\nsuited for various large-scale learning tasks. When block-separable constraints\nare present, randomized block FW (RB-FW) has been shown to further reduce\ncomplexity by updating only a fraction of coordinate blocks per iteration. To\ncircumvent the limitations of existing methods, the present work develops step\nsizes for RB-FW that enable a flexible selection of the number of blocks to\nupdate per iteration while ensuring convergence and feasibility of the\niterates. To this end, convergence rates of RB-FW are established through\ncomputational bounds on a primal sub-optimality measure and on the duality gap.\nThe novel bounds extend the existing convergence analysis, which only applies\nto a step-size sequence that does not generally lead to feasible iterates.\nFurthermore, two classes of step-size sequences that guarantee feasibility of\nthe iterates are also proposed to enhance flexibility in choosing decay rates.\nThe novel convergence results are markedly broadened to encompass also\nnonconvex objectives, and further assert that RB-FW with exact line-search\nreaches a stationary point at rate $\\mathcal{O}(1/\\sqrt{t})$. Performance of\nRB-FW with different step sizes and number of blocks is demonstrated in two\napplications, namely charging of electrical vehicles and structural support\nvector machines. Extensive simulated tests demonstrate the performance\nimprovement of RB-FW relative to existing randomized single-block FW methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 00:01:13 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 21:59:46 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Zhang", "Liang", ""], ["Wang", "Gang", ""], ["Romero", "Daniel", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1612.08498", "submitter": "Taco Cohen", "authors": "Taco S. Cohen, Max Welling", "title": "Steerable CNNs", "comments": null, "journal-ref": "Proceedings of the International Conference on Learning\n  Representations, 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been recognized that the invariance and equivariance properties\nof a representation are critically important for success in many vision tasks.\nIn this paper we present Steerable Convolutional Neural Networks, an efficient\nand flexible class of equivariant convolutional networks. We show that\nsteerable CNNs achieve state of the art results on the CIFAR image\nclassification benchmark. The mathematical theory of steerable representations\nreveals a type system in which any steerable representation is a composition of\nelementary feature types, each one associated with a particular kind of\nsymmetry. We show how the parameter cost of a steerable filter bank depends on\nthe types of the input and output features, and show how to use this knowledge\nto construct CNNs that utilize parameters effectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 04:38:28 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1612.08544", "submitter": "Anuj Karpatne", "authors": "Anuj Karpatne, Gowtham Atluri, James Faghmous, Michael Steinbach,\n  Arindam Banerjee, Auroop Ganguly, Shashi Shekhar, Nagiza Samatova, and Vipin\n  Kumar", "title": "Theory-guided Data Science: A New Paradigm for Scientific Discovery from\n  Data", "comments": null, "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 29(10),\n  pp.2318-2331. 2017", "doi": "10.1109/TKDE.2017.2720168", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science models, although successful in a number of commercial domains,\nhave had limited applicability in scientific problems involving complex\nphysical phenomena. Theory-guided data science (TGDS) is an emerging paradigm\nthat aims to leverage the wealth of scientific knowledge for improving the\neffectiveness of data science models in enabling scientific discovery. The\noverarching vision of TGDS is to introduce scientific consistency as an\nessential component for learning generalizable models. Further, by producing\nscientifically interpretable models, TGDS aims to advance our scientific\nunderstanding by discovering novel domain insights. Indeed, the paradigm of\nTGDS has started to gain prominence in a number of scientific disciplines such\nas turbulence modeling, material discovery, quantum chemistry, bio-medical\nscience, bio-marker discovery, climate science, and hydrology. In this paper,\nwe formally conceptualize the paradigm of TGDS and present a taxonomy of\nresearch themes in TGDS. We describe several approaches for integrating domain\nknowledge in different research themes using illustrative examples from\ndifferent disciplines. We also highlight some of the promising avenues of novel\nresearch for realizing the full potential of theory-guided data science.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:14:16 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 17:42:12 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Karpatne", "Anuj", ""], ["Atluri", "Gowtham", ""], ["Faghmous", "James", ""], ["Steinbach", "Michael", ""], ["Banerjee", "Arindam", ""], ["Ganguly", "Auroop", ""], ["Shekhar", "Shashi", ""], ["Samatova", "Nagiza", ""], ["Kumar", "Vipin", ""]]}, {"id": "1612.08608", "submitter": "Asim Kadav", "authors": "Asim Kadav, Erik Kruus", "title": "ASAP: Asynchronous Approximate Data-Parallel Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging workloads, such as graph processing and machine learning are\napproximate because of the scale of data involved and the stochastic nature of\nthe underlying algorithms. These algorithms are often distributed over multiple\nmachines using bulk-synchronous processing (BSP) or other synchronous\nprocessing paradigms such as map-reduce. However, data parallel processing\nprimitives such as repeated barrier and reduce operations introduce high\nsynchronization overheads. Hence, many existing data-processing platforms use\nasynchrony and staleness to improve data-parallel job performance. Often, these\nsystems simply change the synchronous communication to asynchronous between the\nworker nodes in the cluster. This improves the throughput of data processing\nbut results in poor accuracy of the final output since different workers may\nprogress at different speeds and process inconsistent intermediate outputs.\n  In this paper, we present ASAP, a model that provides asynchronous and\napproximate processing semantics for data-parallel computation. ASAP provides\nfine-grained worker synchronization using NOTIFY-ACK semantics that allows\nindependent workers to run asynchronously. ASAP also provides stochastic reduce\nthat provides approximate but guaranteed convergence to the same result as an\naggregated all-reduce. In our results, we show that ASAP can reduce\nsynchronization costs and provides 2-10X speedups in convergence and up to 10X\nsavings in network costs for distributed machine learning applications and\nprovides strong convergence guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 12:40:39 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Kadav", "Asim", ""], ["Kruus", "Erik", ""]]}, {"id": "1612.08633", "submitter": "Vishal Kakkar", "authors": "Vishal Kakkar, Shirish K. Shevade, S Sundararajan, Dinesh Garg", "title": "A Sparse Nonlinear Classifier Design Using AUC Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AUC (Area under the ROC curve) is an important performance measure for\napplications where the data is highly imbalanced. Learning to maximize AUC\nperformance is thus an important research problem. Using a max-margin based\nsurrogate loss function, AUC optimization problem can be approximated as a\npairwise rankSVM learning problem. Batch learning methods for solving the\nkernelized version of this problem suffer from scalability and may not result\nin sparse classifiers. Recent years have witnessed an increased interest in the\ndevelopment of online or single-pass online learning algorithms that design a\nclassifier by maximizing the AUC performance. The AUC performance of nonlinear\nclassifiers, designed using online methods, is not comparable with that of\nnonlinear classifiers designed using batch learning algorithms on many\nreal-world datasets. Motivated by these observations, we design a scalable\nalgorithm for maximizing AUC performance by greedily adding the required number\nof basis functions into the classifier model. The resulting sparse classifiers\nperform faster inference. Our experimental results show that the level of\nsparsity achievable can be order of magnitude smaller than the Kernel RankSVM\nmodel without affecting the AUC performance much.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 13:52:56 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Kakkar", "Vishal", ""], ["Shevade", "Shirish K.", ""], ["Sundararajan", "S", ""], ["Garg", "Dinesh", ""]]}, {"id": "1612.08650", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Reproducible Pattern Recognition Research: The Case of Optimistic SSL", "comments": "Presented at RRPR 2016: 1st Workshop on Reproducible Research in\n  Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the approaches we took and trade-offs involved in\nmaking a paper on a conceptual topic in pattern recognition research fully\nreproducible. We discuss our definition of reproducibility, the tools used, how\nthe analysis was set up, show some examples of alternative analyses the code\nenables and discuss our views on reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 14:57:22 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1612.08669", "submitter": "Chao-Hsuan Ke", "authors": "Li-Yeh Chuang, Chao-Hsuan Ke, and Cheng-Hong Yang", "title": "A Hybrid Both Filter and Wrapper Feature Selection Method for Microarray\n  Classification", "comments": "5 pages, 2 figures, 4tables", "journal-ref": "IMECS2008_pp146-150", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gene expression data is widely used in disease analysis and cancer diagnosis.\nHowever, since gene expression data could contain thousands of genes\nsimultaneously, successful microarray classification is rather difficult.\nFeature selection is an important pre-treatment for any classification process.\nSelecting a useful gene subset as a classifier not only decreases the\ncomputational time and cost, but also increases classification accuracy. In\nthis study, we applied the information gain method as a filter approach, and an\nimproved binary particle swarm optimization as a wrapper approach to implement\nfeature selection; selected gene subsets were used to evaluate the performance\nof classification. Experimental results show that by employing the proposed\nmethod fewer gene subsets needed to be selected and better classification\naccuracy could be obtained.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 16:25:28 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Chuang", "Li-Yeh", ""], ["Ke", "Chao-Hsuan", ""], ["Yang", "Cheng-Hong", ""]]}, {"id": "1612.08714", "submitter": "Andreas Henelius", "authors": "Andreas Henelius, Kai Puolam\\\"aki, Henrik Bostr\\\"om, Panagiotis\n  Papapetrou", "title": "Clustering with Confidence: Finding Clusters with Statistical Guarantees", "comments": "30 pages, 5 figures, 5 tables. Added URL to the source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a widely used unsupervised learning method for finding\nstructure in the data. However, the resulting clusters are typically presented\nwithout any guarantees on their robustness; slightly changing the used data\nsample or re-running a clustering algorithm involving some stochastic component\nmay lead to completely different clusters. There is, hence, a need for\ntechniques that can quantify the instability of the generated clusters. In this\nstudy, we propose a technique for quantifying the instability of a clustering\nsolution and for finding robust clusters, termed core clusters, which\ncorrespond to clusters where the co-occurrence probability of each data item\nwithin a cluster is at least $1 - \\alpha$. We demonstrate how solving the core\nclustering problem is linked to finding the largest maximal cliques in a graph.\nWe show that the method can be used with both clustering and classification\nalgorithms. The proposed method is tested on both simulated and real datasets.\nThe results show that the obtained clusters indeed meet the guarantees on\nrobustness.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 19:39:23 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 17:56:48 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Henelius", "Andreas", ""], ["Puolam\u00e4ki", "Kai", ""], ["Bostr\u00f6m", "Henrik", ""], ["Papapetrou", "Panagiotis", ""]]}, {"id": "1612.08789", "submitter": "Manuel Martin Salvador", "authors": "Manuel Martin Salvador, Marcin Budka, Bogdan Gabrys", "title": "Automatic Composition and Optimization of Multicomponent Predictive\n  Systems With an Extended Auto-WEKA", "comments": null, "journal-ref": "in IEEE Transactions on Automation Science and Engineering. (2018)\n  1-14", "doi": "10.1109/TASE.2018.2876430", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composition and parameterization of multicomponent predictive systems (MCPSs)\nconsisting of chains of data transformation steps are a challenging task.\nAuto-WEKA is a tool to automate the combined algorithm selection and\nhyperparameter (CASH) optimization problem. In this paper, we extend the CASH\nproblem and Auto-WEKA to support the MCPS, including preprocessing steps for\nboth classification and regression tasks. We define the optimization problem in\nwhich the search space consists of suitably parameterized Petri nets forming\nthe sought MCPS solutions. In the experimental analysis, we focus on examining\nthe impact of considerably extending the search space (from approximately\n22,000 to 812 billion possible combinations of methods and categorical\nhyperparameters). In a range of extensive experiments, three different\noptimization strategies are used to automatically compose MCPSs for 21 publicly\navailable data sets. The diversity of the composed MCPSs found is an indication\nthat fully and automatically exploiting different combinations of data cleaning\nand preprocessing techniques is possible and highly beneficial for different\npredictive models. We also present the results on seven data sets from real\nchemical production processes. Our findings can have a major impact on the\ndevelopment of high-quality predictive models as well as their maintenance and\nscalability aspects needed in modern applications and deployment scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 02:31:22 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 18:18:21 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Salvador", "Manuel Martin", ""], ["Budka", "Marcin", ""], ["Gabrys", "Bogdan", ""]]}, {"id": "1612.08795", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Rong Ge, Tengyu Ma, Andrej Risteski", "title": "Provable learning of Noisy-or Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications use latent variable models to explain\nstructure in data, whereby visible variables (= coordinates of the given\ndatapoint) are explained as a probabilistic function of some hidden variables.\nFinding parameters with the maximum likelihood is NP-hard even in very simple\nsettings. In recent years, provably efficient algorithms were nevertheless\ndeveloped for models with linear structures: topic models, mixture models,\nhidden markov models, etc. These algorithms use matrix or tensor decomposition,\nand make some reasonable assumptions about the parameters of the underlying\nmodel.\n  But matrix or tensor decomposition seems of little use when the latent\nvariable model has nonlinearities. The current paper shows how to make\nprogress: tensor decomposition is applied for learning the single-layer {\\em\nnoisy or} network, which is a textbook example of a Bayes net, and used for\nexample in the classic QMR-DT software for diagnosing which disease(s) a\npatient may have by observing the symptoms he/she exhibits.\n  The technical novelty here, which should be useful in other settings in\nfuture, is analysis of tensor decomposition in presence of systematic error\n(i.e., where the noise/error is correlated with the signal, and doesn't\ndecrease as number of samples goes to infinity). This requires rethinking all\nsteps of tensor decomposition methods from the ground up.\n  For simplicity our analysis is stated assuming that the network parameters\nwere chosen from a probability distribution but the method seems more generally\napplicable.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 03:35:59 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1612.08810", "submitter": "Hado van Hasselt", "authors": "David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur\n  Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz,\n  Andre Barreto, Thomas Degris", "title": "The Predictron: End-To-End Learning and Planning", "comments": "Camera-ready version, ICML 2017, with supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges of artificial intelligence is to learn models that\nare effective in the context of planning. In this document we introduce the\npredictron architecture. The predictron consists of a fully abstract model,\nrepresented by a Markov reward process, that can be rolled forward multiple\n\"imagined\" planning steps. Each forward pass of the predictron accumulates\ninternal rewards and values over multiple planning depths. The predictron is\ntrained end-to-end so as to make these accumulated values accurately\napproximate the true value function. We applied the predictron to procedurally\ngenerated random mazes and a simulator for the game of pool. The predictron\nyielded significantly more accurate predictions than conventional deep neural\nnetwork architectures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 06:47:15 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 14:57:31 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 09:21:54 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Silver", "David", ""], ["van Hasselt", "Hado", ""], ["Hessel", "Matteo", ""], ["Schaul", "Tom", ""], ["Guez", "Arthur", ""], ["Harley", "Tim", ""], ["Dulac-Arnold", "Gabriel", ""], ["Reichert", "David", ""], ["Rabinowitz", "Neil", ""], ["Barreto", "Andre", ""], ["Degris", "Thomas", ""]]}, {"id": "1612.08875", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "The Pessimistic Limits and Possibilities of Margin-based Losses in\n  Semi-supervised Learning", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a classification problem where we have both labeled and unlabeled\ndata available. We show that for linear classifiers defined by convex\nmargin-based surrogate losses that are decreasing, it is impossible to\nconstruct any semi-supervised approach that is able to guarantee an improvement\nover the supervised classifier measured by this surrogate loss on the labeled\nand unlabeled data. For convex margin-based loss functions that also increase,\nwe demonstrate safe improvements are possible.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 13:17:07 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 15:20:33 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 11:01:35 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1612.08967", "submitter": "Nicolas Le Roux", "authors": "Nicolas Le Roux", "title": "Efficient iterative policy optimization", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the issue of finding a good policy when the number of policy\nupdates is limited. This is done by approximating the expected policy reward as\na sequence of concave lower bounds which can be efficiently maximized,\ndrastically reducing the number of policy updates required to achieve good\nperformance. We also extend existing methods to negative rewards, enabling the\nuse of control variates.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 19:53:08 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Roux", "Nicolas Le", ""]]}, {"id": "1612.09007", "submitter": "Huan Song", "authors": "Huan Song, Jayaraman J. Thiagarajan, Prasanna Sattigeri, Karthikeyan\n  Natesan Ramamurthy, Andreas Spanias", "title": "A Deep Learning Approach To Multiple Kernel Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel fusion is a popular and effective approach for combining multiple\nfeatures that characterize different aspects of data. Traditional approaches\nfor Multiple Kernel Learning (MKL) attempt to learn the parameters for\ncombining the kernels through sophisticated optimization procedures. In this\npaper, we propose an alternative approach that creates dense embeddings for\ndata using the kernel similarities and adopts a deep neural network\narchitecture for fusing the embeddings. In order to improve the effectiveness\nof this network, we introduce the kernel dropout regularization strategy\ncoupled with the use of an expanded set of composition kernels. Experiment\nresults on a real-world activity recognition dataset show that the proposed\narchitecture is effective in fusing kernels and achieves state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 23:43:27 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Song", "Huan", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Sattigeri", "Prasanna", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Spanias", "Andreas", ""]]}, {"id": "1612.09030", "submitter": "Vikas Garg", "authors": "Vikas K. Garg and Adam Tauman Kalai", "title": "Meta-Unsupervised-Learning: A supervised approach to unsupervised\n  learning", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new paradigm to investigate unsupervised learning, reducing\nunsupervised learning to supervised learning. Specifically, we mitigate the\nsubjectivity in unsupervised decision-making by leveraging knowledge acquired\nfrom prior, possibly heterogeneous, supervised learning tasks. We demonstrate\nthe versatility of our framework via comprehensive expositions and detailed\nexperiments on several unsupervised problems such as (a) clustering, (b)\noutlier detection, and (c) similarity prediction under a common umbrella of\nmeta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to\nestablish the theoretical foundations of our framework, and show that our\nframing of meta-clustering circumvents Kleinberg's impossibility theorem for\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 03:20:33 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 17:34:39 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Garg", "Vikas K.", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1612.09034", "submitter": "Shiqian Ma", "authors": "Shixiang Chen, Shiqian Ma, Wei Liu", "title": "Geometric descent method for convex composite minimization", "comments": "Updated numerical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the geometric descent method recently proposed by\nBubeck, Lee and Singh to tackle nonsmooth and strongly convex composite\nproblems. We prove that our proposed algorithm, dubbed geometric proximal\ngradient method (GeoPG), converges with a linear rate $(1-1/\\sqrt{\\kappa})$ and\nthus achieves the optimal rate among first-order methods, where $\\kappa$ is the\ncondition number of the problem. Numerical results on linear regression and\nlogistic regression with elastic net regularization show that GeoPG compares\nfavorably with Nesterov's accelerated proximal gradient method, especially when\nthe problem is ill-conditioned.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 04:25:28 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 02:50:34 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2017 02:53:48 GMT"}, {"version": "v4", "created": "Tue, 30 May 2017 06:20:50 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Chen", "Shixiang", ""], ["Ma", "Shiqian", ""], ["Liu", "Wei", ""]]}, {"id": "1612.09057", "submitter": "Elchanan Mossel", "authors": "Elchanan Mossel", "title": "Deep Learning and Hierarchal Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is argued that deep learning is efficient for data that is generated from\nhierarchal generative models. Examples of such generative models include\nwavelet scattering networks, functions of compositional structure, and deep\nrendering models. Unfortunately so far, for all such models, it is either not\nrigorously known that they can be learned efficiently, or it is not known that\n\"deep algorithms\" are required in order to learn them.\n  We propose a simple family of \"generative hierarchal models\" which can be\nefficiently learned and where \"deep\" algorithm are necessary for learning. Our\ndefinition of \"deep\" algorithms is based on the empirical observation that deep\nnets necessarily use correlations between features. More formally, we show that\nin a semi-supervised setting, given access to low-order moments of the labeled\ndata and all of the unlabeled data, it is information theoretically impossible\nto perform classification while at the same time there is an efficient\nalgorithm, that given all labelled and unlabeled data, perfectly labels all\nunlabelled data with high probability.\n  For the proof, we use and strengthen the fact that Belief Propagation does\nnot admit a good approximation in terms of linear functions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 07:26:26 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 20:54:32 GMT"}, {"version": "v3", "created": "Sun, 29 Oct 2017 16:27:27 GMT"}, {"version": "v4", "created": "Tue, 4 Sep 2018 23:18:35 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Mossel", "Elchanan", ""]]}, {"id": "1612.09076", "submitter": "Yunlong Liu", "authors": "Yunlong Liu and Hexing Zhu", "title": "Selecting Bases in Spectral learning of Predictive State Representations\n  via Model Entropy", "comments": "9 papges, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive State Representations (PSRs) are powerful techniques for modelling\ndynamical systems, which represent a state as a vector of predictions about\nfuture observable events (tests). In PSRs, one of the fundamental problems is\nthe learning of the PSR model of the underlying system. Recently, spectral\nmethods have been successfully used to address this issue by treating the\nlearning problem as the task of computing an singular value decomposition (SVD)\nover a submatrix of a special type of matrix called the Hankel matrix. Under\nthe assumptions that the rows and columns of the submatrix of the Hankel Matrix\nare sufficient~(which usually means a very large number of rows and columns,\nand almost fails in practice) and the entries of the matrix can be estimated\naccurately, it has been proven that the spectral approach for learning PSRs is\nstatistically consistent and the learned parameters can converge to the true\nparameters. However, in practice, due to the limit of the computation ability,\nonly a finite set of rows or columns can be chosen to be used for the spectral\nlearning. While different sets of columns usually lead to variant accuracy of\nthe learned model, in this paper, we propose an approach for selecting the set\nof columns, namely basis selection, by adopting a concept of model entropy to\nmeasure the accuracy of the learned model. Experimental results are shown to\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 08:53:20 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Liu", "Yunlong", ""], ["Zhu", "Hexing", ""]]}, {"id": "1612.09106", "submitter": "Mingjun Zhong", "authors": "Chaoyun Zhang, Mingjun Zhong, Zongzuo Wang, Nigel Goddard, Charles\n  Sutton", "title": "Sequence-to-point learning with neural networks for nonintrusive load\n  monitoring", "comments": "8 pages, 3 figures", "journal-ref": "The Thirty-Second AAAI Conference on Artificial Intelligence\n  (AAAI-18), 2018", "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy disaggregation (a.k.a nonintrusive load monitoring, NILM), a\nsingle-channel blind source separation problem, aims to decompose the mains\nwhich records the whole house electricity consumption into appliance-wise\nreadings. This problem is difficult because it is inherently unidentifiable.\nRecent approaches have shown that the identifiability problem could be reduced\nby introducing domain knowledge into the model. Deep neural networks have been\nshown to be a promising approach for these problems, but sliding windows are\nnecessary to handle the long sequences which arise in signal processing\nproblems, which raises issues about how to combine predictions from different\nsliding windows. In this paper, we propose sequence-to-point learning, where\nthe input is a window of the mains and the output is a single point of the\ntarget appliance. We use convolutional neural networks to train the model.\nInterestingly, we systematically show that the convolutional neural networks\ncan inherently learn the signatures of the target appliances, which are\nautomatically added into the model to reduce the identifiability problem. We\napplied the proposed neural network approaches to real-world household energy\ndata, and show that the methods achieve state-of-the-art performance, improving\ntwo standard error measures by 84% and 92%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 11:47:23 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 12:42:06 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 08:37:11 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Zhang", "Chaoyun", ""], ["Zhong", "Mingjun", ""], ["Wang", "Zongzuo", ""], ["Goddard", "Nigel", ""], ["Sutton", "Charles", ""]]}, {"id": "1612.09122", "submitter": "John Glover", "authors": "John Glover", "title": "Modeling documents with Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for using Generative Adversarial Networks to\nlearn distributed representations of natural language documents. We propose a\nmodel that is based on the recently proposed Energy-Based GAN, but instead uses\na Denoising Autoencoder as the discriminator network. Document representations\nare extracted from the hidden layer of the discriminator and evaluated both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 12:29:20 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Glover", "John", ""]]}, {"id": "1612.09147", "submitter": "Ofer Dekel", "authors": "Ofer Dekel", "title": "Linear Learning with Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear predictors are especially useful when the data is high-dimensional and\nsparse. One of the standard techniques used to train a linear predictor is the\nAveraged Stochastic Gradient Descent (ASGD) algorithm. We present an efficient\nimplementation of ASGD that avoids dense vector operations. We also describe a\ntranslation invariant extension called Centered Averaged Stochastic Gradient\nDescent (CASGD).\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 14:02:31 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 16:44:56 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Dekel", "Ofer", ""]]}, {"id": "1612.09158", "submitter": "Gianluigi Pillonetto Dr.", "authors": "Gianluigi Pillonetto", "title": "The interplay between system identification and machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from examples is one of the key problems in science and engineering.\nIt deals with function reconstruction from a finite set of direct and noisy\nsamples. Regularization in reproducing kernel Hilbert spaces (RKHSs) is widely\nused to solve this task and includes powerful estimators such as regularization\nnetworks. Recent achievements include the proof of the statistical consistency\nof these kernel- based approaches. Parallel to this, many different system\nidentification techniques have been developed but the interaction with machine\nlearning does not appear so strong yet. One reason is that the RKHSs usually\nemployed in machine learning do not embed the information available on dynamic\nsystems, e.g. BIBO stability. In addition, in system identification the\nindependent data assumptions routinely adopted in machine learning are never\nsatisfied in practice. This paper provides new results which strengthen the\nconnection between system identification and machine learning. Our starting\npoint is the introduction of RKHSs of dynamic systems. They contain functionals\nover spaces defined by system inputs and allow to interpret system\nidentification as learning from examples. In both linear and nonlinear\nsettings, it is shown that this perspective permits to derive in a relatively\nsimple way conditions on RKHS stability (i.e. the property of containing only\nBIBO stable systems or predictors), also facilitating the design of new kernels\nfor system identification. Furthermore, we prove the convergence of the\nregularized estimator to the optimal predictor under conditions typical of\ndynamic systems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 14:32:51 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Pillonetto", "Gianluigi", ""]]}, {"id": "1612.09205", "submitter": "Tamas Madl", "authors": "Tamas Madl", "title": "Deep neural heart rate variability analysis", "comments": "6 pages in NIPS 2016 Workshop on Machine Learning for Health (ML4HC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite of the pain and limited accuracy of blood tests for early recognition\nof cardiovascular disease, they dominate risk screening and triage. On the\nother hand, heart rate variability is non-invasive and cheap, but not\nconsidered accurate enough for clinical practice. Here, we tackle heart beat\ninterval based classification with deep learning. We introduce an end to end\ndifferentiable hybrid architecture, consisting of a layer of biological neuron\nmodels of cardiac dynamics (modified FitzHugh Nagumo neurons) and several\nlayers of a standard feed-forward neural network. The proposed model is\nevaluated on ECGs from 474 stable at-risk (coronary artery disease) patients,\nand 1172 chest pain patients of an emergency department. We show that it can\nsignificantly outperform models based on traditional heart rate variability\npredictors, as well as approaching or in some cases outperforming clinical\nblood tests, based only on 60 seconds of inter-beat intervals.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 17:14:05 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Madl", "Tamas", ""]]}, {"id": "1612.09283", "submitter": "Ping Li", "authors": "Ping Li", "title": "Generalized Intersection Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the very recent line of work on the ``generalized min-max'' (GMM)\nkernel, this study proposes the ``generalized intersection'' (GInt) kernel and\nthe related ``normalized generalized min-max'' (NGMM) kernel. In computer\nvision, the (histogram) intersection kernel has been popular, and the GInt\nkernel generalizes it to data which can have both negative and positive\nentries. Through an extensive empirical classification study on 40 datasets\nfrom the UCI repository, we are able to show that this (tuning-free) GInt\nkernel performs fairly well.\n  The empirical results also demonstrate that the NGMM kernel typically\noutperforms the GInt kernel. Interestingly, the NGMM kernel has another\ninterpretation --- it is the ``asymmetrically transformed'' version of the GInt\nkernel, based on the idea of ``asymmetric hashing''. Just like the GMM kernel,\nthe NGMM kernel can be efficiently linearized through (e.g.,) generalized\nconsistent weighted sampling (GCWS), as empirically validated in our study.\nOwing to the discrete nature of hashed values, it also provides a scheme for\napproximate near neighbor search.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 20:40:52 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1612.09296", "submitter": "Tuo Zhao", "authors": "Xingguo Li, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, Zhaoran\n  Wang, Tuo Zhao", "title": "Symmetry, Saddle Points, and Global Optimization Landscape of Nonconvex\n  Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general theory for studying the \\xl{landscape} of nonconvex\n\\xl{optimization} with underlying symmetric structures \\tz{for a class of\nmachine learning problems (e.g., low-rank matrix factorization, phase\nretrieval, and deep linear neural networks)}. In specific, we characterize the\nlocations of stationary points and the null space of Hessian matrices \\xl{of\nthe objective function} via the lens of invariant groups\\removed{for associated\noptimization problems, including low-rank matrix factorization, phase\nretrieval, and deep linear neural networks}. As a major motivating example, we\napply the proposed general theory to characterize the global \\xl{landscape} of\nthe \\xl{nonconvex optimization in} low-rank matrix factorization problem. In\nparticular, we illustrate how the rotational symmetry group gives rise to\ninfinitely many nonisolated strict saddle points and equivalent global minima\nof the objective function. By explicitly identifying all stationary points, we\ndivide the entire parameter space into three regions: ($\\cR_1$) the region\ncontaining the neighborhoods of all strict saddle points, where the objective\nhas negative curvatures; ($\\cR_2$) the region containing neighborhoods of all\nglobal minima, where the objective enjoys strong convexity along certain\ndirections; and ($\\cR_3$) the complement of the above regions, where the\ngradient has sufficiently large magnitudes. We further extend our result to the\nmatrix sensing problem. Such global landscape implies strong global convergence\nguarantees for popular iterative algorithms with arbitrary initial solutions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 20:57:19 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2017 18:14:33 GMT"}, {"version": "v3", "created": "Sat, 20 Jan 2018 02:45:55 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Li", "Xingguo", ""], ["Lu", "Junwei", ""], ["Arora", "Raman", ""], ["Haupt", "Jarvis", ""], ["Liu", "Han", ""], ["Wang", "Zhaoran", ""], ["Zhao", "Tuo", ""]]}, {"id": "1612.09328", "submitter": "Hongyuan Mei", "authors": "Hongyuan Mei and Jason Eisner", "title": "The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point\n  Process", "comments": "NIPS 2017 camera-ready. New experiments including intensity\n  prediction evaluation, sensitivity to # of parameters, training speed\n  analysis. Results updated to use final test data instead of devtest. Improved\n  exposition, especially of continuous-time LSTM and thinning algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many events occur in the world. Some event types are stochastically excited\nor inhibited---in the sense of having their probabilities elevated or\ndecreased---by patterns in the sequence of previous events. Discovering such\npatterns can help us predict which type of event will happen next and when. We\nmodel streams of discrete events in continuous time, by constructing a neurally\nself-modulating multivariate point process in which the intensities of multiple\nevent types evolve according to a novel continuous-time LSTM. This generative\nmodel allows past events to influence the future in complex and realistic ways,\nby conditioning future event intensities on the hidden state of a recurrent\nneural network that has consumed the stream of past events. Our model has\ndesirable qualitative properties. It achieves competitive likelihood and\npredictive accuracy on real and synthetic datasets, including under\nmissing-data conditions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 22:02:53 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 04:31:56 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 16:04:21 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Mei", "Hongyuan", ""], ["Eisner", "Jason", ""]]}, {"id": "1612.09434", "submitter": "Frederic Chazal", "authors": "Fr\\'ed\\'eric Chazal (DATASHAPE), Ilaria Giulini (DATASHAPE), Bertrand\n  Michel (LSTA)", "title": "Data driven estimation of Laplace-Beltrami operator", "comments": null, "journal-ref": "30th Conference on Neural Information Processing Systems (NIPS\n  2016), Dec 2016, Barcelona, Spain. 30th Conference on Neural Information\n  Processing Systems (NIPS 2016), Barcelona, Spain., 2016", "doi": null, "report-no": null, "categories": "cs.CG cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximations of Laplace-Beltrami operators on manifolds through graph\nLapla-cians have become popular tools in data analysis and machine learning.\nThese discretized operators usually depend on bandwidth parameters whose tuning\nremains a theoretical and practical problem. In this paper, we address this\nproblem for the unnormalized graph Laplacian by establishing an oracle\ninequality that opens the door to a well-founded data-driven procedure for the\nbandwidth selection. Our approach relies on recent results by Lacour and\nMassart [LM15] on the so-called Lepski's method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 09:33:07 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Chazal", "Fr\u00e9d\u00e9ric", "", "DATASHAPE"], ["Giulini", "Ilaria", "", "DATASHAPE"], ["Michel", "Bertrand", "", "LSTA"]]}, {"id": "1612.09438", "submitter": "Shuai Li", "authors": "Shuai Li, Kui Jia, Xiaogang Wang", "title": "Automatic Discoveries of Physical and Semantic Concepts via Association\n  Priors of Neuron Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent successful deep neural networks are largely trained in a\nsupervised manner. It {\\it associates} complex patterns of input samples with\nneurons in the last layer, which form representations of {\\it concepts}. In\nspite of their successes, the properties of complex patterns associated a\nlearned concept remain elusive. In this work, by analyzing how neurons are\nassociated with concepts in supervised networks, we hypothesize that with\nproper priors to regulate learning, neural networks can automatically associate\nneurons in the intermediate layers with concepts that are aligned with real\nworld concepts, when trained only with labels that associate concepts with top\nlevel neurons, which is a plausible way for unsupervised learning. We develop a\nprior to verify the hypothesis and experimentally find the proposed prior help\nneural networks automatically learn both basic physical concepts at the lower\nlayers, e.g., rotation of filters, and highly semantic concepts at the higher\nlayers, e.g., fine-grained categories of an entry-level category.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 09:57:27 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 17:26:42 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Li", "Shuai", ""], ["Jia", "Kui", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1612.09465", "submitter": "Hugo Penedones", "authors": "Timothy A. Mann and Hugo Penedones and Shie Mannor and Todd Hester", "title": "Adaptive Lambda Least-Squares Temporal Difference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Difference learning or TD($\\lambda$) is a fundamental algorithm in\nthe field of reinforcement learning. However, setting TD's $\\lambda$ parameter,\nwhich controls the timescale of TD updates, is generally left up to the\npractitioner. We formalize the $\\lambda$ selection problem as a bias-variance\ntrade-off where the solution is the value of $\\lambda$ that leads to the\nsmallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest\napplying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the\nspace of $\\lambda$ values. Unfortunately, this approach is too computationally\nexpensive for most practical applications. For Least Squares TD (LSTD) we show\nthat LOTO-CV can be implemented efficiently to automatically tune $\\lambda$ and\napply function optimization methods to efficiently search the space of\n$\\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our\nexperiments demonstrate that ALLSTD is significantly computationally faster\nthan the na\\\"{i}ve LOTO-CV implementation while achieving similar performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 11:51:14 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Mann", "Timothy A.", ""], ["Penedones", "Hugo", ""], ["Mannor", "Shie", ""], ["Hester", "Todd", ""]]}, {"id": "1612.09529", "submitter": "Jurae Kim", "authors": "Juno Nam and Jurae Kim", "title": "Linking the Neural Machine Translation and the Prediction of Organic\n  Chemistry Reactions", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the main product of a chemical reaction is one of the important\nproblems of organic chemistry. This paper describes a method of applying a\nneural machine translation model to the prediction of organic chemical\nreactions. In order to translate 'reactants and reagents' to 'products', a\ngated recurrent unit based sequence-to-sequence model and a parser to generate\ninput tokens for model from reaction SMILES strings were built. Training sets\nare composed of reactions from the patent databases, and reactions manually\ngenerated applying the elementary reactions in an organic chemistry textbook of\nWade. The trained models were tested by examples and problems in the textbook.\nThe prediction process does not need manual encoding of rules (e.g., SMARTS\ntransformations) to predict products, hence it only needs sufficient training\nreaction sets to learn new types of reactions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 05:55:34 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Nam", "Juno", ""], ["Kim", "Jurae", ""]]}, {"id": "1612.09596", "submitter": "Matt Taddy", "authors": "Jason Hartford, Greg Lewis, Kevin Leyton-Brown, Matt Taddy", "title": "Counterfactual Prediction with Deep Instrumental Variables Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are in the middle of a remarkable rise in the use and capability of\nartificial intelligence. Much of this growth has been fueled by the success of\ndeep learning architectures: models that map from observables to outputs via\nmultiple layers of latent representations. These deep learning algorithms are\neffective tools for unstructured prediction, and they can be combined in AI\nsystems to solve complex automated reasoning problems. This paper provides a\nrecipe for combining ML algorithms to solve for causal effects in the presence\nof instrumental variables -- sources of treatment randomization that are\nconditionally independent from the response. We show that a flexible IV\nspecification resolves into two prediction tasks that can be solved with deep\nneural nets: a first-stage network for treatment prediction and a second-stage\nnetwork whose loss function involves integration over the conditional treatment\ndistribution. This Deep IV framework imposes some specific structure on the\nstochastic gradient descent routine used for training, but it is general enough\nthat we can take advantage of off-the-shelf ML capabilities and avoid extensive\nalgorithm customization. We outline how to obtain out-of-sample causal\nvalidation in order to avoid over-fit. We also introduce schemes for both\nBayesian and frequentist inference: the former via a novel adaptation of\ndropout training, and the latter via a data splitting routine.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 20:56:41 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Hartford", "Jason", ""], ["Lewis", "Greg", ""], ["Leyton-Brown", "Kevin", ""], ["Taddy", "Matt", ""]]}]