[{"id": "1504.00028", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem\n  Agarwala, Jonathan Brandt, Thomas S. Huang", "title": "Real-World Font Recognition Using Deep Network and Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a challenging fine-grain classification problem: recognizing a\nfont style from an image of text. In this task, it is very easy to generate\nlots of rendered font examples but very hard to obtain real-world labeled\nimages. This real-to-synthetic domain gap caused poor generalization to new\nreal data in previous methods (Chen et al. (2014)). In this paper, we refer to\nConvolutional Neural Networks, and use an adaptation technique based on a\nStacked Convolutional Auto-Encoder that exploits unlabeled real-world images\ncombined with synthetic data. The proposed method achieves an accuracy of\nhigher than 80% (top-5) on a real-world dataset.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 20:30:00 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Jianchao", ""], ["Jin", "Hailin", ""], ["Shechtman", "Eli", ""], ["Agarwala", "Aseem", ""], ["Brandt", "Jonathan", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1504.00052", "submitter": "Eric Bax", "authors": "Eric Bax", "title": "Improved Error Bounds Based on Worst Likely Assignments", "comments": "IJCNN 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error bounds based on worst likely assignments use permutation tests to\nvalidate classifiers. Worst likely assignments can produce effective bounds\neven for data sets with 100 or fewer training examples. This paper introduces a\nstatistic for use in the permutation tests of worst likely assignments that\nimproves error bounds, especially for accurate classifiers, which are typically\nthe classifiers of interest.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 21:48:56 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Bax", "Eric", ""]]}, {"id": "1504.00064", "submitter": "James Zou", "authors": "James Y. Zou, Kamalika Chaudhuri, Adam Tauman Kalai", "title": "Crowdsourcing Feature Discovery via Adaptively Chosen Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an unsupervised approach to efficiently discover the underlying\nfeatures in a data set via crowdsourcing. Our queries ask crowd members to\narticulate a feature common to two out of three displayed examples. In addition\nwe also ask the crowd to provide binary labels to the remaining examples based\non the discovered features. The triples are chosen adaptively based on the\nlabels of the previously discovered features on the data set. In two natural\nmodels of features, hierarchical and independent, we show that a simple\nadaptive algorithm, using \"two-out-of-three\" similarity queries, recovers all\nfeatures with less labor than any nonadaptive algorithm. Experimental results\nvalidate the theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 23:27:03 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Zou", "James Y.", ""], ["Chaudhuri", "Kamalika", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1504.00083", "submitter": "Brendan van Rooyen", "authors": "Brendan van Rooyen, Robert C. Williamson", "title": "A Theory of Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature Learning aims to extract relevant information contained in data sets\nin an automated fashion. It is driving force behind the current deep learning\ntrend, a set of methods that have had widespread empirical success. What is\nlacking is a theoretical understanding of different feature learning schemes.\nThis work provides a theoretical framework for feature learning and then\ncharacterizes when features can be learnt in an unsupervised fashion. We also\nprovide means to judge the quality of features via rate-distortion theory and\nits generalizations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 02:31:55 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["van Rooyen", "Brendan", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1504.00091", "submitter": "Brendan van Rooyen", "authors": "Brendan van Rooyen, Robert C. Williamson", "title": "Learning in the Presence of Corruption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised learning one wishes to identify a pattern present in a joint\ndistribution $P$, of instances, label pairs, by providing a function $f$ from\ninstances to labels that has low risk $\\mathbb{E}_{P}\\ell(y,f(x))$. To do so,\nthe learner is given access to $n$ iid samples drawn from $P$. In many real\nworld problems clean samples are not available. Rather, the learner is given\naccess to samples from a corrupted distribution $\\tilde{P}$ from which to\nlearn, while the goal of predicting the clean pattern remains. There are many\ndifferent types of corruption one can consider, and as of yet there is no\ngeneral means to compare the relative ease of learning under these different\ncorruption processes. In this paper we develop a general framework for tackling\nsuch problems as well as introducing upper and lower bounds on the risk for\nlearning in the presence of corruption. Our ultimate goal is to be able to make\ninformed economic decisions in regards to the acquisition of data sets. For a\ncertain subclass of corruption processes (those that are\n\\emph{reconstructible}) we achieve this goal in a particular sense. Our lower\nbounds are in terms of the coefficient of ergodicity, a simple to calculate\nproperty of stochastic matrices. Our upper bounds proceed via a generalization\nof the method of unbiased estimators appearing in recent work of Natarajan et\nal and implicit in the earlier work of Kearns.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 02:54:38 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2015 14:09:14 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["van Rooyen", "Brendan", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1504.00110", "submitter": "Daniel Lowd", "authors": "Daniel Lowd, Amirmohammad Rooshenas", "title": "The Libra Toolkit for Probabilistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Libra Toolkit is a collection of algorithms for learning and inference\nwith discrete probabilistic models, including Bayesian networks, Markov\nnetworks, dependency networks, and sum-product networks. Compared to other\ntoolkits, Libra places a greater emphasis on learning the structure of\ntractable models in which exact inference is efficient. It also includes a\nvariety of algorithms for learning graphical models in which inference is\npotentially intractable, and for performing exact and approximate inference.\nLibra is released under a 2-clause BSD license to encourage broad use in\nacademia and industry.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 06:05:40 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Lowd", "Daniel", ""], ["Rooshenas", "Amirmohammad", ""]]}, {"id": "1504.00284", "submitter": "Adrian  Calma", "authors": "Adrian Calma, Tobias Reitmaier, Bernhard Sick, Paul Lukowicz, Mark\n  Embrechts", "title": "A New Vision of Collaborative Active Learning", "comments": "16 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning (AL) is a learning paradigm where an active learner has to\ntrain a model (e.g., a classifier) which is in principal trained in a\nsupervised way, but in AL it has to be done by means of a data set with\ninitially unlabeled samples. To get labels for these samples, the active\nlearner has to ask an oracle (e.g., a human expert) for labels. The goal is to\nmaximize the performance of the model and to minimize the number of queries at\nthe same time. In this article, we first briefly discuss the state of the art\nand own, preliminary work in the field of AL. Then, we propose the concept of\ncollaborative active learning (CAL). With CAL, we will overcome some of the\nharsh limitations of current AL. In particular, we envision scenarios where an\nexpert may be wrong for various reasons, there might be several or even many\nexperts with different expertise, the experts may label not only samples but\nalso knowledge at a higher level such as rules, and we consider that the\nlabeling costs depend on many conditions. Moreover, in a CAL process human\nexperts will profit by improving their own knowledge, too.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 16:39:26 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 15:17:33 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 16:11:15 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Calma", "Adrian", ""], ["Reitmaier", "Tobias", ""], ["Sick", "Bernhard", ""], ["Lukowicz", "Paul", ""], ["Embrechts", "Mark", ""]]}, {"id": "1504.00377", "submitter": "Zhengwu Zhang", "authors": "Zhengwu Zhang, Debdeep Pati, Anuj Srivastava", "title": "Bayesian Clustering of Shapes of Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised clustering of curves according to their shapes is an important\nproblem with broad scientific applications. The existing model-based clustering\ntechniques either rely on simple probability models (e.g., Gaussian) that are\nnot generally valid for shape analysis or assume the number of clusters. We\ndevelop an efficient Bayesian method to cluster curve data using an elastic\nshape metric that is based on joint registration and comparison of shapes of\ncurves. The elastic-inner product matrix obtained from the data is modeled\nusing a Wishart distribution whose parameters are assigned carefully chosen\nprior distributions to allow for automatic inference on the number of clusters.\nPosterior is sampled through an efficient Markov chain Monte Carlo procedure\nbased on the Chinese restaurant process to infer (1) the posterior distribution\non the number of clusters, and (2) clustering configuration of shapes. This\nmethod is demonstrated on a variety of synthetic data and real data examples on\nprotein structure analysis, cell shape analysis in microscopy images, and\nclustering of shaped from MPEG7 database.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 20:35:33 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Zhang", "Zhengwu", ""], ["Pati", "Debdeep", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1504.00386", "submitter": "James P. Crutchfield", "authors": "James P. Crutchfield and Sarah Marzen", "title": "Signatures of Infinity: Nonergodicity and Resource Scaling in\n  Prediction, Complexity, and Learning", "comments": "8 pages, 1 figure; http://csc.ucdavis.edu/~cmg/compmech/pubs/soi.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple analysis of the structural complexity of\ninfinite-memory processes built from random samples of stationary, ergodic\nfinite-memory component processes. Such processes are familiar from the well\nknown multi-arm Bandit problem. We contrast our analysis with\ncomputation-theoretic and statistical inference approaches to understanding\ntheir complexity. The result is an alternative view of the relationship between\npredictability, complexity, and learning that highlights the distinct ways in\nwhich informational and correlational divergences arise in complex ergodic and\nnonergodic processes. We draw out consequences for the resource divergences\nthat delineate the structural hierarchy of ergodic processes and for processes\nthat are themselves hierarchical.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 20:55:10 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Crutchfield", "James P.", ""], ["Marzen", "Sarah", ""]]}, {"id": "1504.00430", "submitter": "Hanyang Peng", "authors": "Hanyang Peng, Yong Fan", "title": "Direct l_(2,p)-Norm Learning for Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel sparse learning based feature selection\nmethod that directly optimizes a large margin linear classification model\nsparsity with l_(2,p)-norm (0 < p < 1)subject to data-fitting constraints,\nrather than using the sparsity as a regularization term. To solve the direct\nsparsity optimization problem that is non-smooth and non-convex when 0<p<1, we\nprovide an efficient iterative algorithm with proved convergence by converting\nit to a convex and smooth optimization problem at every iteration step. The\nproposed algorithm has been evaluated based on publicly available datasets, and\nextensive comparison experiments have demonstrated that our algorithm could\nachieve feature selection performance competitive to state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 02:16:39 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Peng", "Hanyang", ""], ["Fan", "Yong", ""]]}, {"id": "1504.00580", "submitter": "Piotr Gawron jr.", "authors": "Mateusz Ostaszewski and Przemys{\\l}aw Sadowski and Piotr Gawron", "title": "Quantum image classification using principal component analysis", "comments": "9 pages", "journal-ref": "Theoretical and Applied Informatics, Vol. 27, No. 1, pp. 1-12\n  (2015)", "doi": "10.20904/271001", "report-no": null, "categories": "quant-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel quantum algorithm for classification of images. The\nalgorithm is constructed using principal component analysis and von Neuman\nquantum measurements. In order to apply the algorithm we present a new quantum\nrepresentation of grayscale images.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 14:53:51 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Ostaszewski", "Mateusz", ""], ["Sadowski", "Przemys\u0142aw", ""], ["Gawron", "Piotr", ""]]}, {"id": "1504.00641", "submitter": "Ankit Patel", "authors": "Ankit B. Patel, Tan Nguyen and Richard G. Baraniuk", "title": "A Probabilistic Theory of Deep Learning", "comments": "56 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "Rice University Electrical and Computer Engineering Dept. Technical\n  Report No 2015-1", "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A grand challenge in machine learning is the development of computational\nalgorithms that match or outperform humans in perceptual inference tasks that\nare complicated by nuisance variation. For instance, visual object recognition\ninvolves the unknown object position, orientation, and scale in object\nrecognition while speech recognition involves the unknown voice pronunciation,\npitch, and speed. Recently, a new breed of deep learning algorithms have\nemerged for high-nuisance inference tasks that routinely yield pattern\nrecognition systems with near- or super-human capabilities. But a fundamental\nquestion remains: Why do they work? Intuitions abound, but a coherent framework\nfor understanding, analyzing, and synthesizing deep learning architectures has\nremained elusive. We answer this question by developing a new probabilistic\nframework for deep learning based on the Deep Rendering Model: a generative\nprobabilistic model that explicitly captures latent nuisance variation. By\nrelaxing the generative model to a discriminative one, we can recover two of\nthe current leading deep learning systems, deep convolutional neural networks\nand random decision forests, providing insights into their successes and\nshortcomings, as well as a principled route to their improvement.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 18:38:38 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Patel", "Ankit B.", ""], ["Nguyen", "Tan", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1504.00702", "submitter": "Sergey Levine", "authors": "Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel", "title": "End-to-End Training of Deep Visuomotor Policies", "comments": "updating with revisions for JMLR final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy search methods can allow robots to learn control policies for a wide\nrange of tasks, but practical applications of policy search often require\nhand-engineered components for perception, state estimation, and low-level\ncontrol. In this paper, we aim to answer the following question: does training\nthe perception and control systems jointly end-to-end provide better\nperformance than training each component separately? To this end, we develop a\nmethod that can be used to learn policies that map raw image observations\ndirectly to torques at the robot's motors. The policies are represented by deep\nconvolutional neural networks (CNNs) with 92,000 parameters, and are trained\nusing a partially observed guided policy search method, which transforms policy\nsearch into supervised learning, with supervision provided by a simple\ntrajectory-centric reinforcement learning method. We evaluate our method on a\nrange of real-world manipulation tasks that require close coordination between\nvision and control, such as screwing a cap onto a bottle, and present simulated\ncomparisons to a range of prior policy search methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 22:23:51 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 22:46:23 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 04:33:01 GMT"}, {"version": "v4", "created": "Mon, 7 Dec 2015 16:39:49 GMT"}, {"version": "v5", "created": "Tue, 19 Apr 2016 01:33:13 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Levine", "Sergey", ""], ["Finn", "Chelsea", ""], ["Darrell", "Trevor", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1504.00736", "submitter": "Liang Du", "authors": "Liang Du, Yi-Dong Shen", "title": "Unsupervised Feature Selection with Adaptive Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of feature selection has raised considerable interests in the\npast decade. Traditional unsupervised methods select the features which can\nfaithfully preserve the intrinsic structures of data, where the intrinsic\nstructures are estimated using all the input features of data. However, the\nestimated intrinsic structures are unreliable/inaccurate when the redundant and\nnoisy features are not removed. Therefore, we face a dilemma here: one need the\ntrue structures of data to identify the informative features, and one need the\ninformative features to accurately estimate the true structures of data. To\naddress this, we propose a unified learning framework which performs structure\nlearning and feature selection simultaneously. The structures are adaptively\nlearned from the results of feature selection, and the informative features are\nreselected to preserve the refined structures of data. By leveraging the\ninteractions between these two essential tasks, we are able to capture accurate\nstructures and select more informative features. Experimental results on many\nbenchmark data sets demonstrate that the proposed method outperforms many state\nof the art unsupervised feature selection methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 03:21:15 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Du", "Liang", ""], ["Shen", "Yi-Dong", ""]]}, {"id": "1504.00757", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Venkatesh Saligrama", "title": "Learning Mixed Membership Mallows Models from Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel parameterized family of Mixed Membership Mallows Models\n(M4) to account for variability in pairwise comparisons generated by a\nheterogeneous population of noisy and inconsistent users. M4 models individual\npreferences as a user-specific probabilistic mixture of shared latent Mallows\ncomponents. Our key algorithmic insight for estimation is to establish a\nstatistical connection between M4 and topic models by viewing pairwise\ncomparisons as words, and users as documents. This key insight leads us to\nexplore Mallows components with a separable structure and leverage recent\nadvances in separable topic discovery. While separability appears to be overly\nrestrictive, we nevertheless show that it is an inevitable outcome of a\nrelatively small number of latent Mallows components in a world of large number\nof items. We then develop an algorithm based on robust extreme-point\nidentification of convex polygons to learn the reference rankings, and is\nprovably consistent with polynomial sample complexity guarantees. We\ndemonstrate that our new model is empirically competitive with the current\nstate-of-the-art approaches in predicting real-world preferences.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 07:02:49 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1504.00781", "submitter": "Bhaveshkumar Dharmani", "authors": "Dharmani Bhaveshkumar C", "title": "The Gram-Charlier A Series based Extended Rule-of-Thumb for Bandwidth\n  Selection in Univariate and Multivariate Kernel Density Estimations", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article derives a novel Gram-Charlier A (GCA) Series based Extended\nRule-of-Thumb (ExROT) for bandwidth selection in Kernel Density Estimation\n(KDE). There are existing various bandwidth selection rules achieving\nminimization of the Asymptotic Mean Integrated Square Error (AMISE) between the\nestimated probability density function (PDF) and the actual PDF. The rules\ndiffer in a way to estimate the integration of the squared second order\nderivative of an unknown PDF $(f(\\cdot))$, identified as the roughness\n$R(f''(\\cdot))$. The simplest Rule-of-Thumb (ROT) estimates $R(f''(\\cdot))$\nwith an assumption that the density being estimated is Gaussian. Intuitively,\nbetter estimation of $R(f''(\\cdot))$ and consequently better bandwidth\nselection rules can be derived, if the unknown PDF is approximated through an\ninfinite series expansion based on a more generalized density assumption. As a\ndemonstration and verification to this concept, the ExROT derived in the\narticle uses an extended assumption that the density being estimated is near\nGaussian. This helps use of the GCA expansion as an approximation to the\nunknown near Gaussian PDF. The ExROT for univariate KDE is extended to that for\nmultivariate KDE. The required multivariate AMISE criteria is re-derived using\nelementary calculus of several variables, instead of Tensor calculus. The\nderivation uses the Kronecker product and the vector differential operator to\nachieve the AMISE expression in vector notations. There is also derived ExROT\nfor kernel based density derivative estimator.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 08:42:44 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["C", "Dharmani Bhaveshkumar", ""]]}, {"id": "1504.00905", "submitter": "Jose Lopez", "authors": "Jose A. Lopez, Octavia Camps, Mario Sznaier", "title": "Robust Anomaly Detection Using Semidefinite Programming", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach, based on polynomial optimization and the\nmethod of moments, to the problem of anomaly detection. The proposed technique\nonly requires information about the statistical moments of the normal-state\ndistribution of the features of interest and compares favorably with existing\napproaches (such as Parzen windows and 1-class SVM). In addition, it provides a\nsuccinct description of the normal state. Thus, it leads to a substantial\nsimplification of the the anomaly detection problem when working with higher\ndimensional datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 18:20:36 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 15:58:36 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Lopez", "Jose A.", ""], ["Camps", "Octavia", ""], ["Sznaier", "Mario", ""]]}, {"id": "1504.00923", "submitter": "Fred Richardson", "authors": "Fred Richardson, Douglas Reynolds, Najim Dehak", "title": "A Unified Deep Neural Network for Speaker and Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned feature representations and sub-phoneme posteriors from Deep Neural\nNetworks (DNNs) have been used separately to produce significant performance\ngains for speaker and language recognition tasks. In this work we show how\nthese gains are possible using a single DNN for both speaker and language\nrecognition. The unified DNN approach is shown to yield substantial performance\nimprovements on the the 2013 Domain Adaptation Challenge speaker recognition\ntask (55% reduction in EER for the out-of-domain condition) and on the NIST\n2011 Language Recognition Evaluation (48% reduction in EER for the 30s test\ncondition).\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 19:57:06 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Richardson", "Fred", ""], ["Reynolds", "Douglas", ""], ["Dehak", "Najim", ""]]}, {"id": "1504.00941", "submitter": "Quoc Le", "authors": "Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton", "title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning long term dependencies in recurrent networks is difficult due to\nvanishing and exploding gradients. To overcome this difficulty, researchers\nhave developed sophisticated optimization techniques and network architectures.\nIn this paper, we propose a simpler solution that use recurrent neural networks\ncomposed of rectified linear units. Key to our solution is the use of the\nidentity matrix or its scaled version to initialize the recurrent weight\nmatrix. We find that our solution is comparable to LSTM on our four benchmarks:\ntwo toy problems involving long-range temporal structures, a large language\nmodeling problem and a benchmark speech recognition problem.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 21:22:52 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 22:39:18 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Le", "Quoc V.", ""], ["Jaitly", "Navdeep", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1504.00948", "submitter": "Liangyue Li", "authors": "Liangyue Li, Hanghang Tong", "title": "The Child is Father of the Man: Foresee the Success at the Early Stage", "comments": "Correct some typos in our KDD paper", "journal-ref": null, "doi": "10.1145/2783258.2783340", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the dynamic mechanisms that drive the high-impact scientific\nwork (e.g., research papers, patents) is a long-debated research topic and has\nmany important implications, ranging from personal career development and\nrecruitment search, to the jurisdiction of research resources. Recent advances\nin characterizing and modeling scientific success have made it possible to\nforecast the long-term impact of scientific work, where data mining techniques,\nsupervised learning in particular, play an essential role. Despite much\nprogress, several key algorithmic challenges in relation to predicting\nlong-term scientific impact have largely remained open. In this paper, we\npropose a joint predictive model to forecast the long-term scientific impact at\nthe early stage, which simultaneously addresses a number of these open\nchallenges, including the scholarly feature design, the non-linearity, the\ndomain-heterogeneity and dynamics. In particular, we formulate it as a\nregularized optimization problem and propose effective and scalable algorithms\nto solve it. We perform extensive empirical evaluations on large, real\nscholarly data sets to validate the effectiveness and the efficiency of our\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 22:04:05 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 21:34:24 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2015 02:41:40 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Li", "Liangyue", ""], ["Tong", "Hanghang", ""]]}, {"id": "1504.00981", "submitter": "Wu Ai", "authors": "Wu Ai and Weisheng Chen", "title": "ELM-Based Distributed Cooperative Learning Over Networks", "comments": "This paper has been withdrawn by the authors due to the incorrect\n  proof of Theorem 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates distributed cooperative learning algorithms for data\nprocessing in a network setting. Specifically, the extreme learning machine\n(ELM) is introduced to train a set of data distributed across several\ncomponents, and each component runs a program on a subset of the entire data.\nIn this scheme, there is no requirement for a fusion center in the network due\nto e.g., practical limitations, security, or privacy reasons. We first\nreformulate the centralized ELM training problem into a separable form among\nnodes with consensus constraints. Then, we solve the equivalent problem using\ndistributed optimization tools. A new distributed cooperative learning\nalgorithm based on ELM, called DC-ELM, is proposed. The architecture of this\nalgorithm differs from that of some existing parallel/distributed ELMs based on\nMapReduce or cloud computing. We also present an online version of the proposed\nalgorithm that can learn data sequentially in a one-by-one or chunk-by-chunk\nmode. The novel algorithm is well suited for potential applications such as\nartificial intelligence, computational biology, finance, wireless sensor\nnetworks, and so on, involving datasets that are often extremely large,\nhigh-dimensional and located on distributed data sources. We show simulation\nresults on both synthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 04:40:48 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 05:48:56 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Ai", "Wu", ""], ["Chen", "Weisheng", ""]]}, {"id": "1504.01033", "submitter": "Zhiwei Steven Wu", "authors": "Aaron Roth, Jonathan Ullman, Zhiwei Steven Wu", "title": "Watch and Learn: Optimizing from Revealed Preferences Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Stackelberg game is played between a leader and a follower. The leader\nfirst chooses an action, then the follower plays his best response. The goal of\nthe leader is to pick the action that will maximize his payoff given the\nfollower's best response. In this paper we present an approach to solving for\nthe leader's optimal strategy in certain Stackelberg games where the follower's\nutility function (and thus the subsequent best response of the follower) is\nunknown.\n  Stackelberg games capture, for example, the following interaction between a\nproducer and a consumer. The producer chooses the prices of the goods he\nproduces, and then a consumer chooses to buy a utility maximizing bundle of\ngoods. The goal of the seller here is to set prices to maximize his\nprofit---his revenue, minus the production cost of the purchased bundle. It is\nquite natural that the seller in this example should not know the buyer's\nutility function. However, he does have access to revealed preference\nfeedback---he can set prices, and then observe the purchased bundle and his own\nprofit. We give algorithms for efficiently solving, in terms of both\ncomputational and query complexity, a broad class of Stackelberg games in which\nthe follower's utility function is unknown, using only \"revealed preference\"\naccess to it. This class includes in particular the profit maximization\nproblem, as well as the optimal tolling problem in nonatomic congestion games,\nwhen the latency functions are unknown. Surprisingly, we are able to solve\nthese problems even though the optimization problems are non-convex in the\nleader's actions.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 18:13:15 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 03:34:42 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Roth", "Aaron", ""], ["Ullman", "Jonathan", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1504.01044", "submitter": "Heng Wang", "authors": "Heng Wang and Zubin Abraham", "title": "Concept Drift Detection for Streaming Data", "comments": "9 pages, accepted in the International Joint Conference of Neural\n  Networks 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common statistical prediction models often require and assume stationarity in\nthe data. However, in many practical applications, changes in the relationship\nof the response and predictor variables are regularly observed over time,\nresulting in the deterioration of the predictive performance of these models.\nThis paper presents Linear Four Rates (LFR), a framework for detecting these\nconcept drifts and subsequently identifying the data points that belong to the\nnew concept (for relearning the model). Unlike conventional concept drift\ndetection approaches, LFR can be applied to both batch and stream data; is not\nlimited by the distribution properties of the response variable (e.g., datasets\nwith imbalanced labels); is independent of the underlying statistical-model;\nand uses user-specified parameters that are intuitively comprehensible. The\nperformance of LFR is compared to benchmark approaches using both simulated and\ncommonly used public datasets that span the gamut of concept drift types. The\nresults show LFR significantly outperforms benchmark approaches in terms of\nrecall, accuracy and delay in detection of concept drifts across datasets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 19:55:35 GMT"}, {"version": "v2", "created": "Sun, 3 May 2015 22:11:21 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Wang", "Heng", ""], ["Abraham", "Zubin", ""]]}, {"id": "1504.01046", "submitter": "Yining Wang", "authors": "Yining Wang, Yu-Xiang Wang and Aarti Singh", "title": "Graph Connectivity in Noisy Sparse Subspace Clustering", "comments": "14 pages. To appear in The 19th International Conference on\n  Artificial Intelligence and Statistics, held at Cadiz, Spain in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is the problem of clustering data points into a union of\nlow-dimensional linear/affine subspaces. It is the mathematical abstraction of\nmany important problems in computer vision, image processing and machine\nlearning. A line of recent work (4, 19, 24, 20) provided strong theoretical\nguarantee for sparse subspace clustering (4), the state-of-the-art algorithm\nfor subspace clustering, on both noiseless and noisy data sets. It was shown\nthat under mild conditions, with high probability no two points from different\nsubspaces are clustered together. Such guarantee, however, is not sufficient\nfor the clustering to be correct, due to the notorious \"graph connectivity\nproblem\" (15). In this paper, we investigate the graph connectivity problem for\nnoisy sparse subspace clustering and show that a simple post-processing\nprocedure is capable of delivering consistent clustering under certain \"general\nposition\" or \"restricted eigenvalue\" assumptions. We also show that our\ncondition is almost tight with adversarial noise perturbation by constructing a\ncounter-example. These results provide the first exact clustering guarantee of\nnoisy SSC for subspaces of dimension greater then 3.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 20:05:17 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 15:30:48 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Yu-Xiang", ""], ["Singh", "Aarti", ""]]}, {"id": "1504.01050", "submitter": "Yang Liu", "authors": "Yang Liu and Mingyan Liu", "title": "An Online Approach to Dynamic Channel Access and Transmission Scheduling", "comments": "10 pages, to appear in MobiHoc 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making judicious channel access and transmission scheduling decisions is\nessential for improving performance as well as energy and spectral efficiency\nin multichannel wireless systems. This problem has been a subject of extensive\nstudy in the past decade, and the resulting dynamic and opportunistic channel\naccess schemes can bring potentially significant improvement over traditional\nschemes. However, a common and severe limitation of these dynamic schemes is\nthat they almost always require some form of a priori knowledge of the channel\nstatistics. A natural remedy is a learning framework, which has also been\nextensively studied in the same context, but a typical learning algorithm in\nthis literature seeks only the best static policy, with performance measured by\nweak regret, rather than learning a good dynamic channel access policy. There\nis thus a clear disconnect between what an optimal channel access policy can\nachieve with known channel statistics that actively exploits temporal, spatial\nand spectral diversity, and what a typical existing learning algorithm aims\nfor, which is the static use of a single channel devoid of diversity gain. In\nthis paper we bridge this gap by designing learning algorithms that track known\noptimal or sub-optimal dynamic channel access and transmission scheduling\npolicies, thereby yielding performance measured by a form of strong regret, the\naccumulated difference between the reward returned by an optimal solution when\na priori information is available and that by our online algorithm. We do so in\nthe context of two specific algorithms that appeared in [1] and [2],\nrespectively, the former for a multiuser single-channel setting and the latter\nfor a single-user multichannel setting. In both cases we show that our\nalgorithms achieve sub-linear regret uniform in time and outperforms the\nstandard weak-regret learning algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 20:37:52 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Liu", "Yang", ""], ["Liu", "Mingyan", ""]]}, {"id": "1504.01070", "submitter": "Mihai Cucuringu", "authors": "Mihai Cucuringu", "title": "Sync-Rank: Robust Ranking, Constrained Ranking and Rank Aggregation via\n  Eigenvector and Semidefinite Programming Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic problem of establishing a statistical ranking of a\nset of n items given a set of inconsistent and incomplete pairwise comparisons\nbetween such items. Instantiations of this problem occur in numerous\napplications in data analysis (e.g., ranking teams in sports data), computer\nvision, and machine learning. We formulate the above problem of ranking with\nincomplete noisy information as an instance of the group synchronization\nproblem over the group SO(2) of planar rotations, whose usefulness has been\ndemonstrated in numerous applications in recent years. Its least squares\nsolution can be approximated by either a spectral or a semidefinite programming\n(SDP) relaxation, followed by a rounding procedure. We perform extensive\nnumerical simulations on both synthetic and real-world data sets, showing that\nour proposed method compares favorably to other algorithms from the recent\nliterature. Existing theoretical guarantees on the group synchronization\nproblem imply lower bounds on the largest amount of noise permissible in the\nranking data while still achieving exact recovery. We propose a similar\nsynchronization-based algorithm for the rank-aggregation problem, which\nintegrates in a globally consistent ranking pairwise comparisons given by\ndifferent rating systems on the same set of items. We also discuss the problem\nof semi-supervised ranking when there is available information on the ground\ntruth rank of a subset of players, and propose an algorithm based on SDP which\nrecovers the ranks of the remaining players. Finally, synchronization-based\nranking, combined with a spectral technique for the densest subgraph problem,\nallows one to extract locally-consistent partial rankings, in other words, to\nidentify the rank of a small subset of players whose pairwise comparisons are\nless noisy than the rest of the data, which other methods are not able to\nidentify.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 01:40:35 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Cucuringu", "Mihai", ""]]}, {"id": "1504.01072", "submitter": "Silvija Kokalj-Filipovic", "authors": "Silvija Kokalj-Filipovic and Larry Greenstein", "title": "EM-Based Channel Estimation from Crowd-Sourced RSSI Samples Corrupted by\n  Noise and Interference", "comments": "CISS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for estimating channel parameters from RSSI measurements\nand the lost packet count, which can work in the presence of losses due to both\ninterference and signal attenuation below the noise floor. This is especially\nimportant in the wireless networks, such as vehicular, where propagation model\nchanges with the density of nodes. The method is based on Stochastic\nExpectation Maximization, where the received data is modeled as a mixture of\ndistributions (no/low interference and strong interference), incomplete\n(censored) due to packet losses. The PDFs in the mixture are Gamma, according\nto the commonly accepted model for wireless signal and interference power. This\napproach leverages the loss count as additional information, hence\noutperforming maximum likelihood estimation, which does not use this\ninformation (ML-), for a small number of received RSSI samples. Hence, it\nallows inexpensive on-line channel estimation from ad-hoc collected data. The\nmethod also outperforms ML- on uncensored data mixtures, as ML- assumes that\nsamples are from a single-mode PDF.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 02:20:55 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Kokalj-Filipovic", "Silvija", ""], ["Greenstein", "Larry", ""]]}, {"id": "1504.01106", "submitter": "Lili Mou", "authors": "Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, Zhi Jin", "title": "Discriminative Neural Sentence Modeling by Tree-Based Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a tree-based convolutional neural network (TBCNN) for\ndiscriminative sentence modeling. Our models leverage either constituency trees\nor dependency trees of sentences. The tree-based convolution process extracts\nsentences' structural features, and these features are aggregated by max\npooling. Such architecture allows short propagation paths between the output\nlayer and underlying feature detectors, which enables effective structural\nfeature learning and extraction. We evaluate our models on two tasks: sentiment\nanalysis and question classification. In both experiments, TBCNN outperforms\nprevious state-of-the-art results, including existing neural networks and\ndedicated feature/rule engineering. We also make efforts to visualize the\ntree-based convolution process, shedding light on how our models work.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 10:18:32 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 07:30:08 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 17:16:32 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 12:23:16 GMT"}, {"version": "v5", "created": "Tue, 2 Jun 2015 05:56:06 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Mou", "Lili", ""], ["Peng", "Hao", ""], ["Li", "Ge", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1504.01142", "submitter": "Nam-phuong Nguyen", "authors": "Nam-phuong Nguyen, Siavash Mirarab, Keerthana Kumar, Tandy Warnow", "title": "Ultra-large alignments using Phylogeny-aware Profiles", "comments": "Online supplemental materials and data are available at\n  http://www.cs.utexas.edu/users/phylo/software/upp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological questions, including the estimation of deep evolutionary\nhistories and the detection of remote homology between protein sequences, rely\nupon multiple sequence alignments (MSAs) and phylogenetic trees of large\ndatasets. However, accurate large-scale multiple sequence alignment is very\ndifficult, especially when the dataset contains fragmentary sequences. We\npresent UPP, an MSA method that uses a new machine learning technique - the\nEnsemble of Hidden Markov Models - that we propose here. UPP produces highly\naccurate alignments for both nucleotide and amino acid sequences, even on\nultra-large datasets or datasets containing fragmentary sequences. UPP is\navailable at https://github.com/smirarab/sepp.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 17:15:38 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Nguyen", "Nam-phuong", ""], ["Mirarab", "Siavash", ""], ["Kumar", "Keerthana", ""], ["Warnow", "Tandy", ""]]}, {"id": "1504.01169", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki, Stephen Becker, Shannon M. Hughes", "title": "Efficient Dictionary Learning via Very Sparse Random Projections", "comments": "5 pages, 2 figures, accepted in Sampling Theory and Applications\n  (SampTA) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing signal processing tasks on compressive measurements of data has\nreceived great attention in recent years. In this paper, we extend previous\nwork on compressive dictionary learning by showing that more general random\nprojections may be used, including sparse ones. More precisely, we examine\ncompressive K-means clustering as a special case of compressive dictionary\nlearning and give theoretical guarantees for its performance for a very general\nclass of random projections. We then propose a memory and computation efficient\ndictionary learning algorithm, specifically designed for analyzing large\nvolumes of high-dimensional data, which learns the dictionary from very sparse\nrandom projections. Experimental results demonstrate that our approach allows\nfor reduction of computational complexity and memory/data access, with\ncontrollable loss in accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 23:20:47 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Becker", "Stephen", ""], ["Hughes", "Shannon M.", ""]]}, {"id": "1504.01255", "submitter": "Rie Johnson", "authors": "Rie Johnson and Tong Zhang", "title": "Semi-supervised Convolutional Neural Networks for Text Categorization\n  via Region Embedding", "comments": "v1 has a different title, and the results there are obsolete. The\n  current version is to appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new semi-supervised framework with convolutional neural\nnetworks (CNNs) for text categorization. Unlike the previous approaches that\nrely on word embeddings, our method learns embeddings of small text regions\nfrom unlabeled data for integration into a supervised CNN. The proposed scheme\nfor embedding learning is based on the idea of two-view semi-supervised\nlearning, which is intended to be useful for the task of interest even though\nthe training is done on unlabeled data. Our models achieve better results than\nprevious approaches on sentiment classification and topic classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 10:42:07 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 11:32:44 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2015 15:26:16 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1504.01294", "submitter": "Tsvetan Asamov", "authors": "Tsvetan Asamov and Adi Ben-Israel", "title": "A Probabilistic $\\ell_1$ Method for Clustering High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, the clustering problem is NP-hard, and global optimality cannot\nbe established for non-trivial instances. For high-dimensional data,\ndistance-based methods for clustering or classification face an additional\ndifficulty, the unreliability of distances in very high-dimensional spaces. We\npropose a distance-based iterative method for clustering data in very\nhigh-dimensional space, using the $\\ell_1$-metric that is less sensitive to\nhigh dimensionality than the Euclidean distance. For $K$ clusters in\n$\\mathbb{R}^n$, the problem decomposes to $K$ problems coupled by\nprobabilities, and an iteration reduces to finding $Kn$ weighted medians of\npoints on a line. The complexity of the algorithm is linear in the dimension of\nthe data space, and its performance was observed to improve significantly as\nthe dimension increases.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 14:49:13 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 21:58:42 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Asamov", "Tsvetan", ""], ["Ben-Israel", "Adi", ""]]}, {"id": "1504.01344", "submitter": "David Duvenaud", "authors": "Dougal Maclaurin, David Duvenaud, Ryan P. Adams", "title": "Early Stopping is Nonparametric Variational Inference", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that unconverged stochastic gradient descent can be interpreted as a\nprocedure that samples from a nonparametric variational approximate posterior\ndistribution. This distribution is implicitly defined as the transformation of\nan initial distribution by a sequence of optimization updates. By tracking the\nchange in entropy over this sequence of transformations during optimization, we\nform a scalable, unbiased estimate of the variational lower bound on the log\nmarginal likelihood. We can use this bound to optimize hyperparameters instead\nof using cross-validation. This Bayesian interpretation of SGD suggests\nimproved, overfitting-resistant optimization procedures, and gives a\ntheoretical foundation for popular tricks such as early stopping and\nensembling. We investigate the properties of this marginal likelihood estimator\non neural network models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 18:19:45 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Maclaurin", "Dougal", ""], ["Duvenaud", "David", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1504.01365", "submitter": "Hsiang-Fu Yu", "authors": "Cho-Jui Hsieh and Hsiang-Fu Yu and Inderjit S. Dhillon", "title": "PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Dual Coordinate Descent (SDCD) has become one of the most\nefficient ways to solve the family of $\\ell_2$-regularized empirical risk\nminimization problems, including linear SVM, logistic regression, and many\nothers. The vanilla implementation of DCD is quite slow; however, by\nmaintaining primal variables while updating dual variables, the time complexity\nof SDCD can be significantly reduced. Such a strategy forms the core algorithm\nin the widely-used LIBLINEAR package. In this paper, we parallelize the SDCD\nalgorithms in LIBLINEAR. In recent research, several synchronized parallel SDCD\nalgorithms have been proposed, however, they fail to achieve good speedup in\nthe shared memory multi-core setting. In this paper, we propose a family of\nasynchronous stochastic dual coordinate descent algorithms (ASDCD). Each thread\nrepeatedly selects a random dual variable and conducts coordinate updates using\nthe primal variables that are stored in the shared memory. We analyze the\nconvergence properties when different locking/atomic mechanisms are applied.\nFor implementation with atomic operations, we show linear convergence under\nmild conditions. For implementation without any atomic operations or locking,\nwe present the first {\\it backward error analysis} for ASDCD under the\nmulti-core environment, showing that the converged solution is the exact\nsolution for a primal problem with perturbed regularizer. Experimental results\nshow that our methods are much faster than previous parallel coordinate descent\nsolvers.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 19:25:47 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Hsieh", "Cho-Jui", ""], ["Yu", "Hsiang-Fu", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1504.01369", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Changho Suh, Andrea J. Goldsmith", "title": "Information Recovery from Pairwise Measurements", "comments": "This work has been presented in part in ISIT 2014\n  (http://arxiv.org/abs/1404.7105) and ISIT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with jointly recovering $n$ node-variables $\\left\\{\nx_{i}\\right\\}_{1\\leq i\\leq n}$ from a collection of pairwise difference\nmeasurements. Imagine we acquire a few observations taking the form of\n$x_{i}-x_{j}$; the observation pattern is represented by a measurement graph\n$\\mathcal{G}$ with an edge set $\\mathcal{E}$ such that $x_{i}-x_{j}$ is\nobserved if and only if $(i,j)\\in\\mathcal{E}$. To account for noisy\nmeasurements in a general manner, we model the data acquisition process by a\nset of channels with given input/output transition measures. Employing\ninformation-theoretic tools applied to channel decoding problems, we develop a\n\\emph{unified} framework to characterize the fundamental recovery criterion,\nwhich accommodates general graph structures, alphabet sizes, and channel\ntransition measures. In particular, our results isolate a family of\n\\emph{minimum} \\emph{channel divergence measures} to characterize the degree of\nmeasurement corruption, which together with the size of the minimum cut of\n$\\mathcal{G}$ dictates the feasibility of exact information recovery. For\nvarious homogeneous graphs, the recovery condition depends almost only on the\nedge sparsity of the measurement graph irrespective of other graphical metrics;\nalternatively, the minimum sample complexity required for these graphs scales\nlike \\[ \\text{minimum sample complexity }\\asymp\\frac{n\\log\nn}{\\mathsf{Hel}_{1/2}^{\\min}} \\] for certain information metric\n$\\mathsf{Hel}_{1/2}^{\\min}$ defined in the main text, as long as the alphabet\nsize is not super-polynomial in $n$. We apply our general theory to three\nconcrete applications, including the stochastic block model, the outlier model,\nand the haplotype assembly problem. Our theory leads to order-wise tight\nrecovery conditions for all these scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 19:47:01 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 14:07:04 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2015 05:37:56 GMT"}, {"version": "v4", "created": "Fri, 6 May 2016 03:18:52 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Chen", "Yuxin", ""], ["Suh", "Changho", ""], ["Goldsmith", "Andrea J.", ""]]}, {"id": "1504.01446", "submitter": "Vasil S. Denchev", "authors": "Vasil S. Denchev, Nan Ding, Shin Matsushima, S.V.N. Vishwanathan,\n  Hartmut Neven", "title": "Totally Corrective Boosting with Cardinality Penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a totally corrective boosting algorithm with explicit cardinality\nregularization. The resulting combinatorial optimization problems are not known\nto be efficiently solvable with existing classical methods, but emerging\nquantum optimization technology gives hope for achieving sparser models in\npractice. In order to demonstrate the utility of our algorithm, we use a\ndistributed classical heuristic optimizer as a stand-in for quantum hardware.\nEven though this evaluation methodology incurs large time and resource costs on\nclassical computing machinery, it allows us to gauge the potential gains in\ngeneralization performance and sparsity of the resulting boosted ensembles. Our\nexperimental results on public data sets commonly used for benchmarking of\nboosting algorithms decidedly demonstrate the existence of such advantages. If\nactual quantum optimization were to be used with this algorithm in the future,\nwe would expect equivalent or superior results at much smaller time and energy\ncosts during training. Moreover, studying cardinality-penalized boosting also\nsheds light on why unregularized boosting algorithms with early stopping often\nyield better results than their counterparts with explicit convex\nregularization: Early stopping performs suboptimal cardinality regularization.\nThe results that we present here indicate it is beneficial to explicitly solve\nthe combinatorial problem still left open at early termination.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 00:50:30 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Denchev", "Vasil S.", ""], ["Ding", "Nan", ""], ["Matsushima", "Shin", ""], ["Vishwanathan", "S. V. N.", ""], ["Neven", "Hartmut", ""]]}, {"id": "1504.01482", "submitter": "William Chan", "authors": "William Chan, Ian Lane", "title": "Deep Recurrent Neural Networks for Acoustic Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep Recurrent Neural Network (RNN) model for acoustic\nmodelling in Automatic Speech Recognition (ASR). We term our contribution as a\nTC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with\nTime Convolution (TC), followed by a Bidirectional Long Short-Term Memory\n(BLSTM), and a final DNN. The first DNN acts as a feature processor to our\nmodel, the BLSTM then generates a context from the sequence acoustic signal,\nand the final DNN takes the context and models the posterior probabilities of\nthe acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)\neval92 task or more than 8% relative improvement over the baseline DNN models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:12:14 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Chan", "William", ""], ["Lane", "Ian", ""]]}, {"id": "1504.01483", "submitter": "William Chan", "authors": "William Chan and Nan Rosemary Ke and Ian Lane", "title": "Transferring Knowledge from a RNN to a DNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art\nresults in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent\nNeural Network (RNN) models have been shown to outperform DNNs counterparts.\nHowever, state-of-the-art DNN and RNN models tend to be impractical to deploy\non embedded systems with limited computational capacity. Traditionally, the\napproach for embedded platforms is to either train a small DNN directly, or to\ntrain a small DNN that learns the output distribution of a large DNN. In this\npaper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We\nuse the RNN model to generate soft alignments and minimize the Kullback-Leibler\ndivergence against the small DNN. The small DNN trained on the soft RNN\nalignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task\ncompared to a baseline 4.54 WER or more than 13% relative improvement.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:15:44 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Chan", "William", ""], ["Ke", "Nan Rosemary", ""], ["Lane", "Ian", ""]]}, {"id": "1504.01492", "submitter": "Chunhua Shen", "authors": "Peng Wang, Chunhua Shen, Anton van den Hengel", "title": "Efficient SDP Inference for Fully-connected CRFs Based on Low-rank\n  Decomposition", "comments": "15 pages. A conference version of this work appears in Proc. IEEE\n  Conference on Computer Vision and Pattern Recognition, 2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298942", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Random Fields (CRF) have been widely used in a variety of\ncomputer vision tasks. Conventional CRFs typically define edges on neighboring\nimage pixels, resulting in a sparse graph such that efficient inference can be\nperformed. However, these CRFs fail to model long-range contextual\nrelationships. Fully-connected CRFs have thus been proposed. While there are\nefficient approximate inference methods for such CRFs, usually they are\nsensitive to initialization and make strong assumptions. In this work, we\ndevelop an efficient, yet general algorithm for inference on fully-connected\nCRFs. The algorithm is based on a scalable SDP algorithm and the low- rank\napproximation of the similarity/kernel matrix. The core of the proposed\nalgorithm is a tailored quasi-Newton method that takes advantage of the\nlow-rank matrix approximation when solving the specialized SDP dual problem.\nExperiments demonstrate that our method can be applied on fully-connected CRFs\nthat cannot be solved previously, such as pixel-level image co-segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:43:50 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1504.01575", "submitter": "Mathias Berglund", "authors": "Mathias Berglund, Tapani Raiko, Mikko Honkala, Leo K\\\"arkk\\\"ainen,\n  Akos Vetek, Juha Karhunen", "title": "Bidirectional Recurrent Neural Networks as Generative Models -\n  Reconstructing Gaps in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional recurrent neural networks (RNN) are trained to predict both in\nthe positive and negative time directions simultaneously. They have not been\nused commonly in unsupervised tasks, because a probabilistic interpretation of\nthe model has been difficult. Recently, two different frameworks, GSN and NADE,\nprovide a connection between reconstruction and probabilistic modeling, which\nmakes the interpretation possible. As far as we know, neither GSN or NADE have\nbeen studied in the context of time series before. As an example of an\nunsupervised task, we study the problem of filling in gaps in high-dimensional\ntime series with complex dynamics. Although unidirectional RNNs have recently\nbeen trained successfully to model such time series, inference in the negative\ntime direction is non-trivial. We propose two probabilistic interpretations of\nbidirectional RNNs that can be used to reconstruct missing gaps efficiently.\nOur experiments on text data show that both proposed methods are much more\naccurate than unidirectional reconstructions, although a bit less accurate than\na computationally complex bidirectional Bayesian inference on the\nunidirectional RNN. We also provide results on music data for which the\nBayesian inference is computationally infeasible, demonstrating the scalability\nof the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 12:21:03 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 13:29:05 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 07:46:24 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Berglund", "Mathias", ""], ["Raiko", "Tapani", ""], ["Honkala", "Mikko", ""], ["K\u00e4rkk\u00e4inen", "Leo", ""], ["Vetek", "Akos", ""], ["Karhunen", "Juha", ""]]}, {"id": "1504.01697", "submitter": "Alex Gittens", "authors": "Jiyan Yang and Alex Gittens", "title": "Tensor machines for learning target-specific polynomial features", "comments": "19 pages, 4 color figures, 2 tables. Submitted to ECML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have demonstrated that using random feature maps can\nsignificantly decrease the training and testing times of kernel-based\nalgorithms without significantly lowering their accuracy. Regrettably, because\nrandom features are target-agnostic, typically thousands of such features are\nnecessary to achieve acceptable accuracies. In this work, we consider the\nproblem of learning a small number of explicit polynomial features. Our\napproach, named Tensor Machines, finds a parsimonious set of features by\noptimizing over the hypothesis class introduced by Kar and Karnick for random\nfeature maps in a target-specific manner. Exploiting a natural connection\nbetween polynomials and tensors, we provide bounds on the generalization error\nof Tensor Machines. Empirically, Tensor Machines behave favorably on several\nreal-world datasets compared to other state-of-the-art techniques for learning\npolynomial features, and deliver significantly more parsimonious models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 18:21:37 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Yang", "Jiyan", ""], ["Gittens", "Alex", ""]]}, {"id": "1504.01840", "submitter": "Yegor Tkachenko", "authors": "Yegor Tkachenko", "title": "Autonomous CRM Control via CLV Approximation with Deep Reinforcement\n  Learning in Discrete and Continuous Action Space", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper outlines a framework for autonomous control of a CRM (customer\nrelationship management) system. First, it explores how a modified version of\nthe widely accepted Recency-Frequency-Monetary Value system of metrics can be\nused to define the state space of clients or donors. Second, it describes a\nprocedure to determine the optimal direct marketing action in discrete and\ncontinuous action space for the given individual, based on his position in the\nstate space. The procedure involves the use of model-free Q-learning to train a\ndeep neural network that relates a client's position in the state space to\nrewards associated with possible marketing actions. The estimated value\nfunction over the client state space can be interpreted as customer lifetime\nvalue, and thus allows for a quick plug-in estimation of CLV for a given\nclient. Experimental results are presented, based on KDD Cup 1998 mailing\ndataset of donation solicitations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 06:22:44 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Tkachenko", "Yegor", ""]]}, {"id": "1504.01934", "submitter": "Gaurav Thakur", "authors": "Gaurav Singh Thakur, Anubhav Gupta and Sangita Gupta", "title": "Data Mining for Prediction of Human Performance Capability in the\n  Software-Industry", "comments": "Data Mining for Prediction of Human Performance Capability in the\n  Software-Industry, International Journal of Data-Mining and Knowledge\n  Management Process (IJDKP) - March 2015 Issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recruitment of new personnel is one of the most essential business\nprocesses which affect the quality of human capital within any company. It is\nhighly essential for the companies to ensure the recruitment of right talent to\nmaintain a competitive edge over the others in the market. However IT companies\noften face a problem while recruiting new people for their ongoing projects due\nto lack of a proper framework that defines a criteria for the selection\nprocess. In this paper we aim to develop a framework that would allow any\nproject manager to take the right decision for selecting new talent by\ncorrelating performance parameters with the other domain-specific attributes of\nthe candidates. Also, another important motivation behind this project is to\ncheck the validity of the selection procedure often followed by various big\ncompanies in both public and private sectors which focus only on academic\nscores, GPA/grades of students from colleges and other academic backgrounds. We\ntest if such a decision will produce optimal results in the industry or is\nthere a need for change that offers a more holistic approach to recruitment of\nnew talent in the software companies. The scope of this work extends beyond the\nIT domain and a similar procedure can be adopted to develop a recruitment\nframework in other fields as well. Data-mining techniques provide useful\ninformation from the historical projects depending on which the hiring-manager\ncan make decisions for recruiting high-quality workforce. This study aims to\nbridge this hiatus by developing a data-mining framework based on an\nensemble-learning technique to refocus on the criteria for personnel selection.\nThe results from this research clearly demonstrated that there is a need to\nrefocus on the selection-criteria for quality objectives.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 12:26:09 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Thakur", "Gaurav Singh", ""], ["Gupta", "Anubhav", ""], ["Gupta", "Sangita", ""]]}, {"id": "1504.01982", "submitter": "Jesus Fernandez-Bes", "authors": "Jesus Fernandez-Bes, Jer\\'onimo Arenas-Garc\\'ia, Magno T. M. Silva,\n  Luis A. Azpicueta-Ruiz", "title": "Adaptive Diffusion Schemes for Heterogeneous Networks", "comments": "To appear in in IEEE Transactions on Signal Processing. URL:\n  http://ieeexplore.ieee.org/document/8010454/", "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 65, Issue: 21,\n  Nov.1, 1 2017 )", "doi": "10.1109/TSP.2017.2740199", "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with distributed estimation problems in diffusion\nnetworks with heterogeneous nodes, i.e., nodes that either implement different\nadaptive rules or differ in some other aspect such as the filter structure or\nlength, or step size. Although such heterogeneous networks have been considered\nfrom the first works on diffusion networks, obtaining practical and robust\nschemes to adaptively adjust the combiners in different scenarios is still an\nopen problem. In this paper, we study a diffusion strategy specially designed\nand suited to heterogeneous networks. Our approach is based on two key\ningredients: 1) the adaptation and combination phases are completely decoupled,\nso that network nodes keep purely local estimations at all times; and 2)\ncombiners are adapted to minimize estimates of the network mean-square-error.\nOur scheme is compared with the standard Adapt-then-Combine scheme and\ntheoretically analyzed using energy conservation arguments. Several experiments\ninvolving networks with heterogeneous nodes show that the proposed decoupled\nAdapt-then-Combine approach with adaptive combiners outperforms other\nstate-of-the-art techniques, becoming a competitive approach in these\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 14:27:39 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 08:24:06 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Fernandez-Bes", "Jesus", ""], ["Arenas-Garc\u00eda", "Jer\u00f3nimo", ""], ["Silva", "Magno T. M.", ""], ["Azpicueta-Ruiz", "Luis A.", ""]]}, {"id": "1504.01989", "submitter": "Tyng-Luh Liu", "authors": "Jyh-Jing Hwang and Tyng-Luh Liu", "title": "Pixel-wise Deep Learning for Contour Detection", "comments": "2 pages. arXiv admin note: substantial text overlap with\n  arXiv:1412.6857", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of contour detection via per-pixel classifications of\nedge point. To facilitate the process, the proposed approach leverages with\nDenseNet, an efficient implementation of multiscale convolutional neural\nnetworks (CNNs), to extract an informative feature vector for each pixel and\nuses an SVM classifier to accomplish contour detection. In the experiment of\ncontour detection, we look into the effectiveness of combining per-pixel\nfeatures from different CNN layers and verify their performance on BSDS500.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 14:44:20 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Hwang", "Jyh-Jing", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1504.02010", "submitter": "Tuhin Sahai", "authors": "Tuhin Sahai, George Mathew and Amit Surana", "title": "A Chaotic Dynamical System that Paints", "comments": "Four movies have been uploaded in the ancillary folder. They display\n  the evolution of the dynamical system based reconstruction of paintings and\n  pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can a dynamical system paint masterpieces such as Da Vinci's Mona Lisa or\nMonet's Water Lilies? Moreover, can this dynamical system be chaotic in the\nsense that although the trajectories are sensitive to initial conditions, the\nsame painting is created every time? Setting aside the creative aspect of\npainting a picture, in this work, we develop a novel algorithm to reproduce\npaintings and photographs. Combining ideas from ergodic theory and control\ntheory, we construct a chaotic dynamical system with predetermined statistical\nproperties. If one makes the spatial distribution of colors in the picture the\ntarget distribution, akin to a human, the algorithm first captures large scale\nfeatures and then goes on to refine small scale features. Beyond reproducing\npaintings, this approach is expected to have a wide variety of applications\nsuch as uncertainty quantification, sampling for efficient inference in\nscalable machine learning for big data, and developing effective strategies for\nsearch and rescue. In particular, our preliminary studies demonstrate that this\nalgorithm provides significant acceleration and higher accuracy than competing\nmethods for Markov Chain Monte Carlo (MCMC).\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 15:41:30 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Sahai", "Tuhin", ""], ["Mathew", "George", ""], ["Surana", "Amit", ""]]}, {"id": "1504.02089", "submitter": "Tomer Koren", "authors": "Elad Hazan, Tomer Koren", "title": "The Computational Power of Optimization in Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of prediction with expert advice where\nthe experts are \"optimizable\": there is a black-box optimization oracle that\ncan be used to compute, in constant time, the leading expert in retrospect at\nany point in time. In this setting, we give a novel online algorithm that\nattains vanishing regret with respect to $N$ experts in total\n$\\widetilde{O}(\\sqrt{N})$ computation time. We also give a lower bound showing\nthat this running time cannot be improved (up to log factors) in the oracle\nmodel, thereby exhibiting a quadratic speedup as compared to the standard,\noracle-free setting where the required time for vanishing regret is\n$\\widetilde{\\Theta}(N)$. These results demonstrate an exponential gap between\nthe power of optimization in online learning and its power in statistical\nlearning: in the latter, an optimization oracle---i.e., an efficient empirical\nrisk minimizer---allows to learn a finite hypothesis class of size $N$ in time\n$O(\\log{N})$. We also study the implications of our results to learning in\nrepeated zero-sum games, in a setting where the players have access to oracles\nthat compute, in constant time, their best-response to any mixed strategy of\ntheir opponent. We show that the runtime required for approximating the minimax\nvalue of the game in this setting is $\\widetilde{\\Theta}(\\sqrt{N})$, yielding\nagain a quadratic improvement upon the oracle-free setting, where\n$\\widetilde{\\Theta}(N)$ is known to be tight.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 19:54:27 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 12:15:37 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 20:30:50 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2016 09:07:59 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Hazan", "Elad", ""], ["Koren", "Tomer", ""]]}, {"id": "1504.02125", "submitter": "Frederik Ruelens", "authors": "Frederik Ruelens, Bert Claessens, Stijn Vandael, Bart De Schutter,\n  Robert Babuska and Ronnie Belmans", "title": "Residential Demand Response Applications Using Batch Reinforcement\n  Learning", "comments": "Submitted to Trans. on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by recent advances in batch Reinforcement Learning (RL), this paper\ncontributes to the application of batch RL to demand response. In contrast to\nconventional model-based approaches, batch RL techniques do not require a\nsystem identification step, which makes them more suitable for a large-scale\nimplementation. This paper extends fitted Q-iteration, a standard batch RL\ntechnique, to the situation where a forecast of the exogenous data is provided.\nIn general, batch RL techniques do not rely on expert knowledge on the system\ndynamics or the solution. However, if some expert knowledge is provided, it can\nbe incorporated by using our novel policy adjustment method. Finally, we tackle\nthe challenge of finding an open-loop schedule required to participate in the\nday-ahead market. We propose a model-free Monte-Carlo estimator method that\nuses a metric to construct artificial trajectories and we illustrate this\nmethod by finding the day-ahead schedule of a heat-pump thermostat. Our\nexperiments show that batch RL techniques provide a valuable alternative to\nmodel-based controllers and that they can be used to construct both closed-loop\nand open-loop policies.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 20:53:33 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Ruelens", "Frederik", ""], ["Claessens", "Bert", ""], ["Vandael", "Stijn", ""], ["De Schutter", "Bart", ""], ["Babuska", "Robert", ""], ["Belmans", "Ronnie", ""]]}, {"id": "1504.02141", "submitter": "Shehroz Khan", "authors": "Shehroz S. Khan, Michelle E. Karg, Dana Kulic, Jesse Hoey", "title": "Detecting Falls with X-Factor Hidden Markov Models", "comments": "27 pages, 4 figures, 3 tables, Applied Soft Computing, 2017", "journal-ref": "Applied Soft Computing Volume 55, June 2017, Pages 168-177", "doi": "10.1016/j.asoc.2017.01.034", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of falls while performing normal activities of daily living\n(ADL) is important to ensure personal safety and well-being. However, falling\nis a short term activity that occurs infrequently. This poses a challenge to\ntraditional classification algorithms, because there may be very little\ntraining data for falls (or none at all). This paper proposes an approach for\nthe identification of falls using a wearable device in the absence of training\ndata for falls but with plentiful data for normal ADL. We propose three\n`X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs model unseen falls\nusing \"inflated\" output covariances (observation models). To estimate the\ninflated covariances, we propose a novel cross validation method to remove\n\"outliers\" from the normal ADL that serve as proxies for the unseen falls and\nallow learning the XHMMs using only normal activities. We tested the proposed\nXHMM approaches on two activity recognition datasets and show high detection\nrates for falls in the absence of fall-specific training data. We show that the\ntraditional method of choosing a threshold based on maximum of negative of\nlog-likelihood to identify unseen falls is ill-posed for this problem. We also\nshow that supervised classification methods perform poorly when very limited\nfall data are available during the training phase.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:02:27 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 00:57:15 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 23:20:48 GMT"}, {"version": "v4", "created": "Thu, 30 Jun 2016 20:48:13 GMT"}, {"version": "v5", "created": "Fri, 20 Jan 2017 20:18:15 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Karg", "Michelle E.", ""], ["Kulic", "Dana", ""], ["Hoey", "Jesse", ""]]}, {"id": "1504.02147", "submitter": "Thomas Goldstein", "authors": "Tom Goldstein, Gavin Taylor, Kawika Barabin, Kent Sayre", "title": "Unwrapping ADMM: Efficient Distributed Computing via Transpose Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches to distributed model fitting rely heavily on consensus\nADMM, where each node solves small sub-problems using only local data. We\npropose iterative methods that solve {\\em global} sub-problems over an entire\ndistributed dataset. This is possible using transpose reduction strategies that\nallow a single node to solve least-squares over massive datasets without\nputting all the data in one place. This results in simple iterative methods\nthat avoid the expensive inner loops required for consensus methods. To\ndemonstrate the efficiency of this approach, we fit linear classifiers and\nsparse linear models to datasets over 5 Tb in size using a distributed\nimplementation with over 7000 cores in far less time than previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:35:18 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Goldstein", "Tom", ""], ["Taylor", "Gavin", ""], ["Barabin", "Kawika", ""], ["Sayre", "Kent", ""]]}, {"id": "1504.02247", "submitter": "Alexey Melnikov", "authors": "Alexey A. Melnikov, Adi Makmal, Vedran Dunjko and Hans J. Briegel", "title": "Projective simulation with generalization", "comments": "14 pages, 9 figures", "journal-ref": "Sci. Rep. 7, 14430 (2017)", "doi": "10.1038/s41598-017-14740-y", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generalize is an important feature of any intelligent agent.\nNot only because it may allow the agent to cope with large amounts of data, but\nalso because in some environments, an agent with no generalization capabilities\ncannot learn. In this work we outline several criteria for generalization, and\npresent a dynamic and autonomous machinery that enables projective simulation\nagents to meaningfully generalize. Projective simulation, a novel, physical\napproach to artificial intelligence, was recently shown to perform well in\nstandard reinforcement learning problems, with applications in advanced\nrobotics as well as quantum experiments. Both the basic projective simulation\nmodel and the presented generalization machinery are based on very simple\nprinciples. This allows us to provide a full analytical analysis of the agent's\nperformance and to illustrate the benefit the agent gains by generalizing.\nSpecifically, we show that already in basic (but extreme) environments,\nlearning without generalization may be impossible, and demonstrate how the\npresented generalization machinery enables the projective simulation agent to\nlearn.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 10:37:11 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 19:18:40 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Melnikov", "Alexey A.", ""], ["Makmal", "Adi", ""], ["Dunjko", "Vedran", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1504.02338", "submitter": "Devis Tuia", "authors": "Devis Tuia and Gustau Camps-Valls", "title": "Kernel Manifold Alignment", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0148655", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a kernel method for manifold alignment (KEMA) and domain\nadaptation that can match an arbitrary number of data sources without needing\ncorresponding pairs, just few labeled examples in all domains. KEMA has\ninteresting properties: 1) it generalizes other manifold alignment methods, 2)\nit can align manifolds of very different complexities, performing a sort of\nmanifold unfolding plus alignment, 3) it can define a domain-specific metric to\ncope with multimodal specificities, 4) it can align data spaces of different\ndimensionality, 5) it is robust to strong nonlinear feature deformations, and\n6) it is closed-form invertible which allows transfer across-domains and data\nsynthesis. We also present a reduced-rank version for computational efficiency\nand discuss the generalization performance of KEMA under Rademacher principles\nof stability. KEMA exhibits very good performance over competing methods in\nsynthetic examples, visual object recognition and recognition of facial\nexpressions tasks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 14:51:49 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 08:04:51 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 20:58:53 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Tuia", "Devis", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1504.02351", "submitter": "Yongxin Yang", "authors": "Guosheng Hu, Yongxin Yang, Dong Yi, Josef Kittler, William Christmas,\n  Stan Z. Li and Timothy Hospedales", "title": "When Face Recognition Meets with Deep Learning: an Evaluation of\n  Convolutional Neural Networks for Face Recognition", "comments": "7 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, in particular Convolutional Neural Network (CNN), has achieved\npromising results in face recognition recently. However, it remains an open\nquestion: why CNNs work well and how to design a 'good' architecture. The\nexisting works tend to focus on reporting CNN architectures that work well for\nface recognition rather than investigate the reason. In this work, we conduct\nan extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a\ncommon ground to make our work easily reproducible. Specifically, we use public\ndatabase LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing\nCNNs trained on private databases. We propose three CNN architectures which are\nthe first reported architectures trained using LFW data. This paper\nquantitatively compares the architectures of CNNs and evaluate the effect of\ndifferent implementation choices. We identify several useful properties of\nCNN-FRS. For instance, the dimensionality of the learned features can be\nsignificantly reduced without adverse effect on face recognition accuracy. In\naddition, traditional metric learning method exploiting CNN-learned features is\nevaluated. Experiments show two crucial factors to good CNN-FRS performance are\nthe fusion of multiple CNNs and metric learning. To make our work reproducible,\nsource code and models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 15:27:49 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Hu", "Guosheng", ""], ["Yang", "Yongxin", ""], ["Yi", "Dong", ""], ["Kittler", "Josef", ""], ["Christmas", "William", ""], ["Li", "Stan Z.", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1504.02406", "submitter": "Maja", "authors": "Maja Temerinac-Ott and Armaghan W. Naik and Robert F. Murphy", "title": "Deciding when to stop: Efficient stopping of active learning guided\n  drug-target prediction", "comments": "This paper was selected for oral presentation at RECOMB 2015 and an\n  abstract is published in the conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning has shown to reduce the number of experiments needed to\nobtain high-confidence drug-target predictions. However, in order to actually\nsave experiments using active learning, it is crucial to have a method to\nevaluate the quality of the current prediction and decide when to stop the\nexperimentation process. Only by applying reliable stoping criteria to active\nlearning, time and costs in the experimental process can be actually saved. We\ncompute active learning traces on simulated drug-target matrices in order to\nlearn a regression model for the accuracy of the active learner. By analyzing\nthe performance of the regression model on simulated data, we design stopping\ncriteria for previously unseen experimental matrices. We demonstrate on four\npreviously characterized drug effect data sets that applying the stopping\ncriteria can result in upto 40% savings of the total experiments for highly\naccurate predictions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 18:10:38 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Temerinac-Ott", "Maja", ""], ["Naik", "Armaghan W.", ""], ["Murphy", "Robert F.", ""]]}, {"id": "1504.02462", "submitter": "Suresh Venkatasubramanian", "authors": "Arnab Paul, Suresh Venkatasubramanian", "title": "A Group Theoretic Perspective on Unsupervised Deep Learning", "comments": "2-page version of arXiv:1412.6621 prepared for presentation at ICLR\n  2015 workshop as required by ICLR PC). This version has some minor formatting\n  changes as required by the conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called {\\em pretraining}: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:39:05 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 22:03:36 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 06:05:52 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Paul", "Arnab", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1504.02518", "submitter": "Rostislav Goroshin", "authors": "Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun", "title": "Unsupervised Feature Learning from Temporal Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1412.6056", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art classification and detection algorithms rely on\nsupervised training. In this work we study unsupervised feature learning in the\ncontext of temporally coherent video data. We focus on feature learning from\nunlabeled video data, using the assumption that adjacent video frames contain\nsemantically similar information. This assumption is exploited to train a\nconvolutional pooling auto-encoder regularized by slowness and sparsity. We\nestablish a connection between slow feature learning to metric learning and\nshow that the trained encoder can be used to define a more temporally and\nsemantically coherent metric.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 23:26:26 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 23:08:30 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Goroshin", "Ross", ""], ["Bruna", "Joan", ""], ["Tompson", "Jonathan", ""], ["Eigen", "David", ""], ["LeCun", "Yann", ""]]}, {"id": "1504.02526", "submitter": "Jian Li", "authors": "Jian Li, Yuval Rabani, Leonard J. Schulman, Chaitanya Swamy", "title": "Learning Arbitrary Statistical Mixtures of Discrete Distributions", "comments": "23 pages. Preliminary version in the Proceeding of the 47th ACM\n  Symposium on the Theory of Computing (STOC15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning from unlabeled samples very general\nstatistical mixture models on large finite sets. Specifically, the model to be\nlearned, $\\vartheta$, is a probability distribution over probability\ndistributions $p$, where each such $p$ is a probability distribution over $[n]\n= \\{1,2,\\dots,n\\}$. When we sample from $\\vartheta$, we do not observe $p$\ndirectly, but only indirectly and in very noisy fashion, by sampling from $[n]$\nrepeatedly, independently $K$ times from the distribution $p$. The problem is\nto infer $\\vartheta$ to high accuracy in transportation (earthmover) distance.\n  We give the first efficient algorithms for learning this mixture model\nwithout making any restricting assumptions on the structure of the distribution\n$\\vartheta$. We bound the quality of the solution as a function of the size of\nthe samples $K$ and the number of samples used. Our model and results have\napplications to a variety of unsupervised learning scenarios, including\nlearning topic models and collaborative filtering.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 01:17:28 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Li", "Jian", ""], ["Rabani", "Yuval", ""], ["Schulman", "Leonard J.", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1504.02622", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki, Rafa{\\l} J\\'ozefowicz, Jacek Tabor", "title": "Maximum Entropy Linear Manifold for Learning Discriminative\n  Low-dimensional Representation", "comments": "submitted to ECMLPKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is currently a very hot topic in modern machine\nlearning, mostly due to the great success of the deep learning methods. In\nparticular low-dimensional representation which discriminates classes can not\nonly enhance the classification procedure, but also make it faster, while\ncontrary to the high-dimensional embeddings can be efficiently used for visual\nbased exploratory data analysis.\n  In this paper we propose Maximum Entropy Linear Manifold (MELM), a\nmultidimensional generalization of Multithreshold Entropy Linear Classifier\nmodel which is able to find a low-dimensional linear data projection maximizing\ndiscriminativeness of projected classes. As a result we obtain a linear\nembedding which can be used for classification, class aware dimensionality\nreduction and data visualization. MELM provides highly discriminative 2D\nprojections of the data which can be used as a method for constructing robust\nclassifiers.\n  We provide both empirical evaluation as well as some interesting theoretical\nproperties of our objective function such us scale and affine transformation\ninvariance, connections with PCA and bounding of the expected balanced accuracy\nerror.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 09:56:51 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""], ["J\u00f3zefowicz", "Rafa\u0142", ""], ["Tabor", "Jacek", ""]]}, {"id": "1504.02712", "submitter": "Bhaveshkumar Dharmani", "authors": "Dharmani Bhaveshkumar C", "title": "Gradient of Probability Density Functions based Contrasts for Blind\n  Source Separation (BSS)", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article derives some novel independence measures and contrast functions\nfor Blind Source Separation (BSS) application. For the $k^{th}$ order\ndifferentiable multivariate functions with equal hyper-volumes (region bounded\nby hyper-surfaces) and with a constraint of bounded support for $k>1$, it\nproves that equality of any $k^{th}$ order derivatives implies equality of the\nfunctions. The difference between product of marginal Probability Density\nFunctions (PDFs) and joint PDF of a random vector is defined as Function\nDifference (FD) of a random vector. Assuming the PDFs are $k^{th}$ order\ndifferentiable, the results on generalized functions are applied to the\nindependence condition. This brings new sets of independence measures and BSS\ncontrasts based on the $L^p$-Norm, $ p \\geq 1$ of - FD, gradient of FD (GFD)\nand Hessian of FD (HFD). Instead of a conventional two stage indirect\nestimation method for joint PDF based BSS contrast estimation, a single stage\ndirect estimation of the contrasts is desired. The article targets both the\nefficient estimation of the proposed contrasts and extension of the potential\ntheory for an information field. The potential theory has a concept of\nreference potential and it is used to derive closed form expression for the\nrelative analysis of potential field. Analogous to it, there are introduced\nconcepts of Reference Information Potential (RIP) and Cross Reference\nInformation Potential (CRIP) based on the potential due to kernel functions\nplaced at selected sample points as basis in kernel methods. The quantities are\nused to derive closed form expressions for information field analysis using\nleast squares. The expressions are used to estimate $L^2$-Norm of FD and\n$L^2$-Norm of GFD based contrasts.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 15:28:37 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["C", "Dharmani Bhaveshkumar", ""]]}, {"id": "1504.02719", "submitter": "Jian Peng", "authors": "Hyunghoon Cho, Bonnie Berger and Jian Peng", "title": "Diffusion Component Analysis: Unraveling Functional Topology in\n  Biological Networks", "comments": "RECOMB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex biological systems have been successfully modeled by biochemical and\ngenetic interaction networks, typically gathered from high-throughput (HTP)\ndata. These networks can be used to infer functional relationships between\ngenes or proteins. Using the intuition that the topological role of a gene in a\nnetwork relates to its biological function, local or diffusion based\n\"guilt-by-association\" and graph-theoretic methods have had success in\ninferring gene functions. Here we seek to improve function prediction by\nintegrating diffusion-based methods with a novel dimensionality reduction\ntechnique to overcome the incomplete and noisy nature of network data. In this\npaper, we introduce diffusion component analysis (DCA), a framework that plugs\nin a diffusion model and learns a low-dimensional vector representation of each\nnode to encode the topological properties of a network. As a proof of concept,\nwe demonstrate DCA's substantial improvement over state-of-the-art\ndiffusion-based approaches in predicting protein function from molecular\ninteraction networks. Moreover, our DCA framework can integrate multiple\nnetworks from heterogeneous sources, consisting of genomic information,\nbiochemical experiments and other resources, to even further improve function\nprediction. Yet another layer of performance gain is achieved by integrating\nthe DCA framework with support vector machines that take our node vector\nrepresentations as features. Overall, our DCA framework provides a novel\nrepresentation of nodes in a network that can be used as a plug-in architecture\nto other machine learning algorithms to decipher topological properties of and\nobtain novel insights into interactomes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 15:42:11 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Cho", "Hyunghoon", ""], ["Berger", "Bonnie", ""], ["Peng", "Jian", ""]]}, {"id": "1504.02763", "submitter": "Filipe Condessa", "authors": "Filipe Condessa, Jelena Kovacevic, Jose Bioucas-Dias", "title": "Performance measures for classification systems with rejection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers with rejection are essential in real-world applications where\nmisclassifications and their effects are critical. However, if no problem\nspecific cost function is defined, there are no established measures to assess\nthe performance of such classifiers. We introduce a set of desired properties\nfor performance measures for classifiers with rejection, based on which we\npropose a set of three performance measures for the evaluation of the\nperformance of classifiers with rejection that satisfy the desired properties.\nThe nonrejected accuracy measures the ability of the classifier to accurately\nclassify nonrejected samples; the classification quality measures the correct\ndecision making of the classifier with rejector; and the rejection quality\nmeasures the ability to concentrate all misclassified samples onto the set of\nrejected samples. From the measures, we derive the concept of relative\noptimality that allows us to connect the measures to a family of cost functions\nthat take into account the trade-off between rejection and misclassification.\nWe illustrate the use of the proposed performance measures on classifiers with\nrejection applied to synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 19:15:39 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 11:29:13 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Condessa", "Filipe", ""], ["Kovacevic", "Jelena", ""], ["Bioucas-Dias", "Jose", ""]]}, {"id": "1504.02824", "submitter": "Yelong Shen", "authors": "Yelong Shen, Ruoming Jin, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li\n  Deng", "title": "A Deep Embedding Model for Co-occurrence Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-occurrence Data is a common and important information source in many\nareas, such as the word co-occurrence in the sentences, friends co-occurrence\nin social networks and products co-occurrence in commercial transaction data,\netc, which contains rich correlation and clustering information about the\nitems. In this paper, we study co-occurrence data using a general energy-based\nprobabilistic model, and we analyze three different categories of energy-based\nmodel, namely, the $L_1$, $L_2$ and $L_k$ models, which are able to capture\ndifferent levels of dependency in the co-occurrence data. We also discuss how\nseveral typical existing models are related to these three types of energy\nmodels, including the Fully Visible Boltzmann Machine (FVBM) ($L_2$), Matrix\nFactorization ($L_2$), Log-BiLinear (LBL) models ($L_2$), and the Restricted\nBoltzmann Machine (RBM) model ($L_k$). Then, we propose a Deep Embedding Model\n(DEM) (an $L_k$ model) from the energy model in a \\emph{principled} manner.\nFurthermore, motivated by the observation that the partition function in the\nenergy model is intractable and the fact that the major objective of modeling\nthe co-occurrence data is to predict using the conditional probability, we\napply the \\emph{maximum pseudo-likelihood} method to learn DEM. In consequence,\nthe developed model and its learning method naturally avoid the above\ndifficulties and can be easily used to compute the conditional probability in\nprediction. Interestingly, our method is equivalent to learning a special\nstructured deep neural network using back-propagation and a special sampling\nstrategy, which makes it scalable on large-scale datasets. Finally, in the\nexperiments, we show that the DEM can achieve comparable or better results than\nstate-of-the-art methods on datasets across several application domains.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 02:56:01 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 09:07:13 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Shen", "Yelong", ""], ["Jin", "Ruoming", ""], ["Chen", "Jianshu", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1504.02870", "submitter": "Ichiro Takeuchi Prof.", "authors": "Shota Okumura and Yoshiki Suzuki and Ichiro Takeuchi", "title": "Quick sensitivity analysis for incremental data modification and its\n  application to leave-one-out CV in linear classification problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel sensitivity analysis framework for large scale\nclassification problems that can be used when a small number of instances are\nincrementally added or removed. For quickly updating the classifier in such a\nsituation, incremental learning algorithms have been intensively studied in the\nliterature. Although they are much more efficient than solving the optimization\nproblem from scratch, their computational complexity yet depends on the entire\ntraining set size. It means that, if the original training set is large,\ncompletely solving an incremental learning problem might be still rather\nexpensive. To circumvent this computational issue, we propose a novel framework\nthat allows us to make an inference about the updated classifier without\nactually re-optimizing it. Specifically, the proposed framework can quickly\nprovide a lower and an upper bounds of a quantity on the unknown updated\nclassifier. The main advantage of the proposed framework is that the\ncomputational cost of computing these bounds depends only on the number of\nupdated instances. This property is quite advantageous in a typical sensitivity\nanalysis task where only a small number of instances are updated. In this paper\nwe demonstrate that the proposed framework is applicable to various practical\nsensitivity analysis tasks, and the bounds provided by the framework are often\nsufficiently tight for making desired inferences.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 13:25:37 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Okumura", "Shota", ""], ["Suzuki", "Yoshiki", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1504.02902", "submitter": "Alexander Kalmanovich", "authors": "Alexander Kalmanovich and Gal Chechik", "title": "Gradual Training Method for Denoising Auto Encoders", "comments": "arXiv admin note: substantial text overlap with arXiv:1412.6257", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacked denoising auto encoders (DAEs) are well known to learn useful deep\nrepresentations, which can be used to improve supervised training by\ninitializing a deep network. We investigate a training scheme of a deep DAE,\nwhere DAE layers are gradually added and keep adapting as additional layers are\nadded. We show that in the regime of mid-sized datasets, this gradual training\nprovides a small but consistent improvement over stacked training in both\nreconstruction quality and classification error over stacked training on MNIST\nand CIFAR datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 17:51:41 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Kalmanovich", "Alexander", ""], ["Chechik", "Gal", ""]]}, {"id": "1504.02945", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Deep Transform: Cocktail Party Source Separation via Complex Convolution\n  in a Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional deep neural networks (DNN) are state of the art in many\nengineering problems but have not yet addressed the issue of how to deal with\ncomplex spectrograms. Here, we use circular statistics to provide a convenient\nprobabilistic estimate of spectrogram phase in a complex convolutional DNN. In\na typical cocktail party source separation scenario, we trained a convolutional\nDNN to re-synthesize the complex spectrograms of two source speech signals\ngiven a complex spectrogram of the monaural mixture - a discriminative deep\ntransform (DT). We then used this complex convolutional DT to obtain\nprobabilistic estimates of the magnitude and phase components of the source\nspectrograms. Our separation results are on a par with equivalent binary-mask\nbased non-complex separation approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 08:44:56 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1504.02975", "submitter": "F. Ozgur Catak", "authors": "Ferhat \\\"Ozg\\\"ur \\c{C}atak", "title": "Classification with Extreme Learning Machine and Ensemble Algorithms\n  Over Randomly Partitioned Data", "comments": "In Turkish, SIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this age of Big Data, machine learning based data mining methods are\nextensively used to inspect large scale data sets. Deriving applicable\npredictive modeling from these type of data sets is a challenging obstacle\nbecause of their high complexity. Opportunity with high data availability\nlevels, automated classification of data sets has become a critical and\ncomplicated function. In this paper, the power of applying MapReduce based\nDistributed AdaBoosting of Extreme Learning Machine (ELM) are explored to build\nreliable predictive bag of classification models. Thus, (i) dataset ensembles\nare build; (ii) ELM algorithm is used to build weak classification models; and\n(iii) build a strong classification model from a set of weak classification\nmodels. This training model is applied to the publicly available knowledge\ndiscovery and data mining datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 14:03:25 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["\u00c7atak", "Ferhat \u00d6zg\u00fcr", ""]]}, {"id": "1504.03071", "submitter": "Jaeyong Sung", "authors": "Jaeyong Sung, Seok Hyun Jin, Ashutosh Saxena", "title": "Robobarista: Object Part based Transfer of Manipulation Trajectories\n  from Crowd-sourcing in 3D Pointclouds", "comments": "In International Symposium on Robotics Research (ISRR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large variety of objects and appliances in human environments,\nsuch as stoves, coffee dispensers, juice extractors, and so on. It is\nchallenging for a roboticist to program a robot for each of these object types\nand for each of their instantiations. In this work, we present a novel approach\nto manipulation planning based on the idea that many household objects share\nsimilarly-operated object parts. We formulate the manipulation planning as a\nstructured prediction problem and design a deep learning model that can handle\nlarge noise in the manipulation demonstrations and learns features from three\ndifferent modalities: point-clouds, language and trajectory. In order to\ncollect a large number of manipulation demonstrations for different objects, we\ndeveloped a new crowd-sourcing platform called Robobarista. We test our model\non our dataset consisting of 116 objects with 249 parts along with 250 language\ninstructions, for which there are 1225 crowd-sourced manipulation\ndemonstrations. We further show that our robot can even manipulate objects it\nhas never seen before.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 06:25:42 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 23:43:19 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Sung", "Jaeyong", ""], ["Jin", "Seok Hyun", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1504.03101", "submitter": "Carlo Ciliberto", "authors": "Carlo Ciliberto, Youssef Mroueh, Tomaso Poggio and Lorenzo Rosasco", "title": "Convex Learning of Multiple Tasks and their Structure", "comments": "26 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the amount of human supervision is a key problem in machine learning\nand a natural approach is that of exploiting the relations (structure) among\ndifferent tasks. This is the idea at the core of multi-task learning. In this\ncontext a fundamental question is how to incorporate the tasks structure in the\nlearning problem.We tackle this question by studying a general computational\nframework that allows to encode a-priori knowledge of the tasks structure in\nthe form of a convex penalty; in this setting a variety of previously proposed\nmethods can be recovered as special cases, including linear and non-linear\napproaches. Within this framework, we show that tasks and their structure can\nbe efficiently learned considering a convex optimization problem that can be\napproached by means of block coordinate methods such as alternating\nminimization and for which we prove convergence to the global minimum.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 09:13:23 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 20:03:21 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Mroueh", "Youssef", ""], ["Poggio", "Tomaso", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1504.03106", "submitter": "Carlo Ciliberto", "authors": "Carlo Ciliberto, Lorenzo Rosasco and Silvia Villa", "title": "Learning Multiple Visual Tasks while Discovering their Structure", "comments": "19 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is a natural approach for computer vision applications\nthat require the simultaneous solution of several distinct but related\nproblems, e.g. object detection, classification, tracking of multiple agents,\nor denoising, to name a few. The key idea is that exploring task relatedness\n(structure) can lead to improved performances.\n  In this paper, we propose and study a novel sparse, non-parametric approach\nexploiting the theory of Reproducing Kernel Hilbert Spaces for vector-valued\nfunctions. We develop a suitable regularization framework which can be\nformulated as a convex optimization problem, and is provably solvable using an\nalternating minimization approach. Empirical tests show that the proposed\nmethod compares favorably to state of the art techniques and further allows to\nrecover interpretable structures, a problem of interest in its own right.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 09:27:23 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Rosasco", "Lorenzo", ""], ["Villa", "Silvia", ""]]}, {"id": "1504.03154", "submitter": "Carlo Ciliberto", "authors": "Giulia Pasquale, Carlo Ciliberto, Francesca Odone, Lorenzo Rosasco and\n  Lorenzo Natale", "title": "Real-world Object Recognition with Off-the-shelf Deep Conv Nets: How\n  Many Objects can iCub Learn?", "comments": "18 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to visually recognize objects is a fundamental skill for robotics\nsystems. Indeed, a large variety of tasks involving manipulation, navigation or\ninteraction with other agents, deeply depends on the accurate understanding of\nthe visual scene. Yet, at the time being, robots are lacking good visual\nperceptual systems, which often become the main bottleneck preventing the use\nof autonomous agents for real-world applications.\n  Lately in computer vision, systems that learn suitable visual representations\nand based on multi-layer deep convolutional networks are showing remarkable\nperformance in tasks such as large-scale visual recognition and image\nretrieval. To this regard, it is natural to ask whether such remarkable\nperformance would generalize also to the robotic setting.\n  In this paper we investigate such possibility, while taking further steps in\ndeveloping a computational vision system to be embedded on a robotic platform,\nthe iCub humanoid robot. In particular, we release a new dataset ({\\sc\niCubWorld28}) that we use as a benchmark to address the question: {\\it how many\nobjects can iCub recognize?} Our study is developed in a learning framework\nwhich reflects the typical visual experience of a humanoid robot like the iCub.\nExperiments shed interesting insights on the strength and weaknesses of current\ncomputer vision approaches applied in real robotic settings.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 12:45:09 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 05:56:01 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Pasquale", "Giulia", ""], ["Ciliberto", "Carlo", ""], ["Odone", "Francesca", ""], ["Rosasco", "Lorenzo", ""], ["Natale", "Lorenzo", ""]]}, {"id": "1504.03391", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Jan Vondrak", "title": "Tight Bounds on Low-degree Spectral Concentration of Submodular and XOS\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular and fractionally subadditive (or equivalently XOS) functions play\na fundamental role in combinatorial optimization, algorithmic game theory and\nmachine learning. Motivated by learnability of these classes of functions from\nrandom examples, we consider the question of how well such functions can be\napproximated by low-degree polynomials in $\\ell_2$ norm over the uniform\ndistribution. This question is equivalent to understanding of the concentration\nof Fourier weight on low-degree coefficients, a central concept in Fourier\nanalysis. We show that\n  1. For any submodular function $f:\\{0,1\\}^n \\rightarrow [0,1]$, there is a\npolynomial of degree $O(\\log (1/\\epsilon) / \\epsilon^{4/5})$ approximating $f$\nwithin $\\epsilon$ in $\\ell_2$, and there is a submodular function that requires\ndegree $\\Omega(1/\\epsilon^{4/5})$.\n  2. For any XOS function $f:\\{0,1\\}^n \\rightarrow [0,1]$, there is a\npolynomial of degree $O(1/\\epsilon)$ and there exists an XOS function that\nrequires degree $\\Omega(1/\\epsilon)$.\n  This improves on previous approaches that all showed an upper bound of\n$O(1/\\epsilon^2)$ for submodular and XOS functions. The best previous lower\nbound was $\\Omega(1/\\epsilon^{2/3})$ for monotone submodular functions. Our\ntechniques reveal new structural properties of submodular and XOS functions and\nthe upper bounds lead to nearly optimal PAC learning algorithms for these\nclasses of functions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 23:51:45 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2015 22:26:16 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Feldman", "Vitaly", ""], ["Vondrak", "Jan", ""]]}, {"id": "1504.03415", "submitter": "Darshana Wickramarachchi Mr", "authors": "D. C. Wickramarachchi, B. L. Robertson, M. Reale, C. J. Price and J.\n  Brown", "title": "HHCART: An Oblique Decision Tree", "comments": "13 Pages, 1 Figure, 4 Tables, 1 Algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees are a popular technique in statistical data classification.\nThey recursively partition the feature space into disjoint sub-regions until\neach sub-region becomes homogeneous with respect to a particular class. The\nbasic Classification and Regression Tree (CART) algorithm partitions the\nfeature space using axis parallel splits. When the true decision boundaries are\nnot aligned with the feature axes, this approach can produce a complicated\nboundary structure. Oblique decision trees use oblique decision boundaries to\npotentially simplify the boundary structure. The major limitation of this\napproach is that the tree induction algorithm is computationally expensive. In\nthis article we present a new decision tree algorithm, called HHCART. The\nmethod utilizes a series of Householder matrices to reflect the training data\nat each node during the tree construction. Each reflection is based on the\ndirections of the eigenvectors from each classes' covariance matrix.\nConsidering axis parallel splits in the reflected training data provides an\nefficient way of finding oblique splits in the unreflected training data.\nExperimental results show that the accuracy and size of the HHCART trees are\ncomparable with some benchmark methods in the literature. The appealing feature\nof HHCART is that it can handle both qualitative and quantitative features in\nthe same oblique split.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 04:04:00 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Wickramarachchi", "D. C.", ""], ["Robertson", "B. L.", ""], ["Reale", "M.", ""], ["Price", "C. J.", ""], ["Brown", "J.", ""]]}, {"id": "1504.03509", "submitter": "Shuang Liu", "authors": "Shuang Liu, Cheng Chen and Zhihua Zhang", "title": "Regret vs. Communication: Distributed Stochastic Multi-Armed Bandits and\n  Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the distributed stochastic multi-armed bandit\nproblem, where a global arm set can be accessed by multiple players\nindependently. The players are allowed to exchange their history of\nobservations with each other at specific points in time. We study the\nrelationship between regret and communication. When the time horizon is known,\nwe propose the Over-Exploration strategy, which only requires one-round\ncommunication and whose regret does not scale with the number of players. When\nthe time horizon is unknown, we measure the frequency of communication through\na new notion called the density of the communication set, and give an exact\ncharacterization of the interplay between regret and communication.\nSpecifically, a lower bound is established and stable strategies that match the\nlower bound are developed. The results and analyses in this paper are specific\nbut can be translated into more general settings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 12:14:46 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2015 20:26:00 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Liu", "Shuang", ""], ["Chen", "Cheng", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1504.03641", "submitter": "Sergey Zagoruyko", "authors": "Sergey Zagoruyko and Nikos Komodakis", "title": "Learning to Compare Image Patches via Convolutional Neural Networks", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to learn directly from image data (i.e., without\nresorting to manually-designed features) a general similarity function for\ncomparing image patches, which is a task of fundamental importance for many\ncomputer vision problems. To encode such a function, we opt for a CNN-based\nmodel that is trained to account for a wide variety of changes in image\nappearance. To that end, we explore and study multiple neural network\narchitectures, which are specifically adapted to this task. We show that such\nan approach can significantly outperform the state-of-the-art on several\nproblems and benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 17:53:51 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Zagoruyko", "Sergey", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1504.03655", "submitter": "Yingyu Liang", "authors": "Bo Xie, Yingyu Liang, Le Song", "title": "Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear component analysis such as kernel Principle Component Analysis\n(KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in\nmachine learning, statistics and data analysis, but they can not scale up to\nbig datasets. Recent attempts have employed random feature approximations to\nconvert the problem to the primal form for linear computational complexity.\nHowever, to obtain high quality solutions, the number of random features should\nbe the same order of magnitude as the number of data points, making such\napproach not directly applicable to the regime with millions of data points.\n  We propose a simple, computationally efficient, and memory friendly algorithm\nbased on the \"doubly stochastic gradients\" to scale up a range of kernel\nnonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the\n\\emph{non-convex} nature of these problems, our method enjoys theoretical\nguarantees that it converges at the rate $\\tilde{O}(1/t)$ to the global\noptimum, even for the top $k$ eigen subspace. Unlike many alternatives, our\nalgorithm does not require explicit orthogonalization, which is infeasible on\nbig datasets. We demonstrate the effectiveness and scalability of our algorithm\non large scale synthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 18:34:03 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 02:47:45 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2015 23:09:21 GMT"}, {"version": "v4", "created": "Sun, 10 Jan 2016 22:54:59 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Xie", "Bo", ""], ["Liang", "Yingyu", ""], ["Song", "Le", ""]]}, {"id": "1504.03701", "submitter": "Julia Vogt", "authors": "Julia E. Vogt, Marius Kloft, Stefan Stark, Sudhir S. Raman, Sandhya\n  Prabhakaran, Volker Roth and Gunnar R\\\"atsch", "title": "Probabilistic Clustering of Time-Evolving Distance Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel probabilistic clustering model for objects that are\nrepresented via pairwise distances and observed at different time points. The\nproposed method utilizes the information given by adjacent time points to find\nthe underlying cluster structure and obtain a smooth cluster evolution. This\napproach allows the number of objects and clusters to differ at every time\npoint, and no identification on the identities of the objects is needed.\nFurther, the model does not require the number of clusters being specified in\nadvance -- they are instead determined automatically using a Dirichlet process\nprior. We validate our model on synthetic data showing that the proposed method\nis more accurate than state-of-the-art clustering methods. Finally, we use our\ndynamic clustering model to analyze and illustrate the evolution of brain\ncancer patients over time.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 20:05:45 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Vogt", "Julia E.", ""], ["Kloft", "Marius", ""], ["Stark", "Stefan", ""], ["Raman", "Sudhir S.", ""], ["Prabhakaran", "Sandhya", ""], ["Roth", "Volker", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1504.03874", "submitter": "Thomas Burger", "authors": "Thomas Burger", "title": "Bridging belief function theory to modern machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a quickly evolving field which now looks really different\nfrom what it was 15 years ago, when classification and clustering were major\nissues. This document proposes several trends to explore the new questions of\nmodern machine learning, with the strong afterthought that the belief function\nframework has a major role to play.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 12:04:58 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Burger", "Thomas", ""]]}, {"id": "1504.03892", "submitter": "Christos Tzelepis", "authors": "Christos Tzelepis, Vasileios Mezaris and Ioannis Patras", "title": "Linear Maximum Margin Classifier for Learning from Uncertain Data", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence. (c)\n  2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235 Author's accepted version. The\n  final publication is available at\n  http://ieeexplore.ieee.org/document/8103808/", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2772235", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a maximum margin classifier that deals with\nuncertainty in data input. More specifically, we reformulate the SVM framework\nsuch that each training example can be modeled by a multi-dimensional Gaussian\ndistribution described by its mean vector and its covariance matrix -- the\nlatter modeling the uncertainty. We address the classification problem and\ndefine a cost function that is the expected value of the classical SVM cost\nwhen data samples are drawn from the multi-dimensional Gaussian distributions\nthat form the set of the training examples. Our formulation approximates the\nclassical SVM formulation when the training examples are isotropic Gaussians\nwith variance tending to zero. We arrive at a convex optimization problem,\nwhich we solve efficiently in the primal form using a stochastic gradient\ndescent approach. The resulting classifier, which we name SVM with Gaussian\nSample Uncertainty (SVM-GSU), is tested on synthetic data and five publicly\navailable and popular datasets; namely, the MNIST, WDBC, DEAP, TV News Channel\nCommercial Detection, and TRECVID MED datasets. Experimental results verify the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 12:47:50 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 17:10:11 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Tzelepis", "Christos", ""], ["Mezaris", "Vasileios", ""], ["Patras", "Ioannis", ""]]}, {"id": "1504.03991", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu", "title": "Theory of Dual-sparse Regularized Randomized Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study randomized reduction methods, which reduce\nhigh-dimensional features into low-dimensional space by randomized methods\n(e.g., random projection, random hashing), for large-scale high-dimensional\nclassification. Previous theoretical results on randomized reduction methods\nhinge on strong assumptions about the data, e.g., low rank of the data matrix\nor a large separable margin of classification, which hinder their applications\nin broad domains. To address these limitations, we propose dual-sparse\nregularized randomized reduction methods that introduce a sparse regularizer\ninto the reduced dual problem. Under a mild condition that the original dual\nsolution is a (nearly) sparse vector, we show that the resulting dual solution\nis close to the original dual solution and concentrates on its support set. In\nnumerical experiments, we present an empirical study to support the analysis\nand we also present a novel application of the dual-sparse regularized\nrandomized reduction methods to reducing the communication cost of distributed\nlearning from large-scale high-dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 19:16:54 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 13:03:39 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 21:44:02 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2015 21:16:09 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yang", "Tianbao", ""], ["Zhang", "Lijun", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1504.04054", "submitter": "Xin Yuan", "authors": "Yunchen Pu, Xin Yuan, Lawrence Carin", "title": "A Generative Model for Deep Convolutional Learning", "comments": "3 pages, 1 figure, ICLR workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generative model is developed for deep (multi-layered) convolutional\ndictionary learning. A novel probabilistic pooling operation is integrated into\nthe deep model, yielding efficient bottom-up (pretraining) and top-down\n(refinement) probabilistic learning. Experimental results demonstrate powerful\ncapabilities of the model to learn multi-layer features from images, and\nexcellent classification results are obtained on the MNIST and Caltech 101\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 21:31:58 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Pu", "Yunchen", ""], ["Yuan", "Xin", ""], ["Carin", "Lawrence", ""]]}, {"id": "1504.04103", "submitter": "Ananda Theertha Suresh", "authors": "Moein Falahatgar and Ashkan Jafarpour and Alon Orlitsky and\n  Venkatadheeraj Pichapathi and Ananda Theertha Suresh", "title": "Faster Algorithms for Testing under Conditional Sampling", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable recent interest in distribution-tests whose\nrun-time and sample requirements are sublinear in the domain-size $k$. We study\ntwo of the most important tests under the conditional-sampling model where each\nquery specifies a subset $S$ of the domain, and the response is a sample drawn\nfrom $S$ according to the underlying distribution.\n  For identity testing, which asks whether the underlying distribution equals a\nspecific given distribution or $\\epsilon$-differs from it, we reduce the known\ntime and sample complexities from $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ to\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$, thereby matching the information\ntheoretic lower bound. For closeness testing, which asks whether two\ndistributions underlying observed data sets are equal or different, we reduce\nexisting complexity from $\\tilde{\\mathcal{O}}(\\epsilon^{-4} \\log^5 k)$ to an\neven sub-logarithmic $\\tilde{\\mathcal{O}}(\\epsilon^{-5} \\log \\log k)$ thus\nproviding a better bound to an open problem in Bertinoro Workshop on Sublinear\nAlgorithms [Fisher, 2004].\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 05:56:34 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Falahatgar", "Moein", ""], ["Jafarpour", "Ashkan", ""], ["Orlitsky", "Alon", ""], ["Pichapathi", "Venkatadheeraj", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1504.04114", "submitter": "Nir Levin", "authors": "Nir Levine, Timothy A. Mann, Shie Mannor", "title": "Actively Learning to Attract Followers on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter, a popular social network, presents great opportunities for on-line\nmachine learning research. However, previous research has focused almost\nentirely on learning from passively collected data. We study the problem of\nlearning to acquire followers through normative user behavior, as opposed to\nthe mass following policies applied by many bots. We formalize the problem as a\ncontextual bandit problem, in which we consider retweeting content to be the\naction chosen and each tweet (content) is accompanied by context. We design\nreward signals based on the change in followers. The result of our month long\nexperiment with 60 agents suggests that (1) aggregating experience across\nagents can adversely impact prediction accuracy and (2) the Twitter community's\nresponse to different actions is non-stationary. Our findings suggest that\nactively learning on-line can provide deeper insights about how to attract\nfollowers than machine learning over passively collected data alone.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 07:26:11 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Levine", "Nir", ""], ["Mann", "Timothy A.", ""], ["Mannor", "Shie", ""]]}, {"id": "1504.04343", "submitter": "Ce Zhang", "authors": "Stefan Hadjis, Firas Abuzaid, Ce Zhang, Christopher R\\'e", "title": "Caffe con Troll: Shallow Ideas to Speed Up Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Caffe con Troll (CcT), a fully compatible end-to-end version of\nthe popular framework Caffe with rebuilt internals. We built CcT to examine the\nperformance characteristics of training and deploying general-purpose\nconvolutional neural networks across different hardware architectures. We find\nthat, by employing standard batching optimizations for CPU training, we achieve\na 4.5x throughput improvement over Caffe on popular networks like CaffeNet.\nMoreover, with these improvements, the end-to-end training time for CNNs is\ndirectly proportional to the FLOPS delivered by the CPU, which enables us to\nefficiently train hybrid CPU-GPU systems for CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 19:11:08 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 20:12:33 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Hadjis", "Stefan", ""], ["Abuzaid", "Firas", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1504.04406", "submitter": "Mark Schmidt", "authors": "Mark Schmidt, Reza Babanezhad, Mohamed Osama Ahmed, Aaron Defazio, Ann\n  Clifton, Anoop Sarkar", "title": "Non-Uniform Stochastic Average Gradient Method for Training Conditional\n  Random Fields", "comments": "AI/Stats 2015, 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply stochastic average gradient (SAG) algorithms for training\nconditional random fields (CRFs). We describe a practical implementation that\nuses structure in the CRF gradient to reduce the memory requirement of this\nlinearly-convergent stochastic gradient method, propose a non-uniform sampling\nscheme that substantially improves practical performance, and analyze the rate\nof convergence of the SAGA variant under non-uniform sampling. Our experimental\nresults reveal that our method often significantly outperforms existing methods\nin terms of the training objective, and performs as well or better than\noptimally-tuned stochastic gradient methods in terms of test error.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 23:26:35 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Schmidt", "Mark", ""], ["Babanezhad", "Reza", ""], ["Ahmed", "Mohamed Osama", ""], ["Defazio", "Aaron", ""], ["Clifton", "Ann", ""], ["Sarkar", "Anoop", ""]]}, {"id": "1504.04407", "submitter": "Jakub Kone\\v{c}n\\'y", "authors": "Jakub Kone\\v{c}n\\'y, Jie Liu, Peter Richt\\'arik, Martin Tak\\'a\\v{c}", "title": "Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2015.2505682", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose mS2GD: a method incorporating a mini-batching scheme for improving\nthe theoretical complexity and practical performance of semi-stochastic\ngradient descent (S2GD). We consider the problem of minimizing a strongly\nconvex function represented as the sum of an average of a large number of\nsmooth convex functions, and a simple nonsmooth convex regularizer. Our method\nfirst performs a deterministic step (computation of the gradient of the\nobjective function at the starting point), followed by a large number of\nstochastic steps. The process is repeated a few times with the last iterate\nbecoming the new starting point. The novelty of our method is in introduction\nof mini-batching into the computation of stochastic steps. In each step,\ninstead of choosing a single function, we sample $b$ functions, compute their\ngradients, and compute the direction based on this. We analyze the complexity\nof the method and show that it benefits from two speedup effects. First, we\nprove that as long as $b$ is below a certain threshold, we can reach any\npredefined accuracy with less overall work than without mini-batching. Second,\nour mini-batching scheme admits a simple parallel implementation, and hence is\nsuitable for further acceleration by parallelization.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 23:31:38 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 13:26:17 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Liu", "Jie", ""], ["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1504.04588", "submitter": "James-A. Goulet", "authors": "James-A. Goulet", "title": "The Nataf-Beta Random Field Classifier: An Extension of the Beta\n  Conjugate Prior to Classification Problems", "comments": "17 pages, 4 figures, Submitted for publication in the Journal of\n  Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Nataf-Beta Random Field Classifier, a discriminative\napproach that extends the applicability of the Beta conjugate prior to\nclassification problems. The approach's key feature is to model the probability\nof a class conditional on attribute values as a random field whose marginals\nare Beta distributed, and where the parameters of marginals are themselves\ndescribed by random fields. Although the classification accuracy of the\napproach proposed does not statistically outperform the best accuracies\nreported in the literature, it ranks among the top tier for the six benchmark\ndatasets tested. The Nataf-Beta Random Field Classifier is suited as a general\npurpose classification approach for real-continuous and real-integer attribute\nvalue problems.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 17:32:00 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Goulet", "James-A.", ""]]}, {"id": "1504.04599", "submitter": "Bhaswar Bhattacharya", "authors": "Bhaswar B. Bhattacharya and Gregory Valiant", "title": "Testing Closeness With Unequal Sized Samples", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of closeness testing for two discrete distributions\nin the practically relevant setting of \\emph{unequal} sized samples drawn from\neach of them. Specifically, given a target error parameter $\\varepsilon > 0$,\n$m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws from\nan unknown distribution $q$, we describe a test for distinguishing the case\nthat $p=q$ from the case that $||p-q||_1 \\geq \\varepsilon$. If $p$ and $q$ are\nsupported on at most $n$ elements, then our test is successful with high\nprobability provided $m_1\\geq n^{2/3}/\\varepsilon^{4/3}$ and $m_2 =\n\\Omega(\\max\\{\\frac{n}{\\sqrt m_1\\varepsilon^2}, \\frac{\\sqrt\nn}{\\varepsilon^2}\\});$ we show that this tradeoff is optimal throughout this\nrange, to constant factors. These results extend the recent work of Chan et al.\nwho established the sample complexity when the two samples have equal sizes,\nand tightens the results of Acharya et al. by polynomials factors in both $n$\nand $\\varepsilon$. As a consequence, we obtain an algorithm for estimating the\nmixing time of a Markov chain on $n$ states up to a $\\log n$ factor that uses\n$\\tilde{O}(n^{3/2} \\tau_{mix})$ queries to a \"next node\" oracle, improving upon\nthe $\\tilde{O}(n^{5/3}\\tau_{mix})$ query algorithm of Batu et al. Finally, we\nnote that the core of our testing algorithm is a relatively simple statistic\nthat seems to perform well in practice, both on synthetic data and on natural\nlanguage data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 18:35:35 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Bhattacharya", "Bhaswar B.", ""], ["Valiant", "Gregory", ""]]}, {"id": "1504.04646", "submitter": "Kwetishe Danjuma", "authors": "Kwetishe Joro Danjuma", "title": "Performance Evaluation of Machine Learning Algorithms in Post-operative\n  Life Expectancy in the Lung Cancer Patients", "comments": "11 pages, 3 figures, 2 tables, ISSN (Print): 1694-0814 | ISSN\n  (Online): 1694-0784", "journal-ref": "IJCSI International Journal of Computer Science Issues, Volume 12,\n  Issue 2, March 2015", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nature of clinical data makes it difficult to quickly select, tune and\napply machine learning algorithms to clinical prognosis. As a result, a lot of\ntime is spent searching for the most appropriate machine learning algorithms\napplicable in clinical prognosis that contains either binary-valued or\nmulti-valued attributes. The study set out to identify and evaluate the\nperformance of machine learning classification schemes applied in clinical\nprognosis of post-operative life expectancy in the lung cancer patients.\nMultilayer Perceptron, J48, and the Naive Bayes algorithms were used to train\nand test models on Thoracic Surgery datasets obtained from the University of\nCalifornia Irvine machine learning repository. Stratified 10-fold\ncross-validation was used to evaluate baseline performance accuracy of the\nclassifiers. The comparative analysis shows that multilayer perceptron\nperformed best with classification accuracy of 82.3%, J48 came out second with\nclassification accuracy of 81.8%, and Naive Bayes came out the worst with\nclassification accuracy of 74.4%. The quality and outcome of the chosen machine\nlearning algorithms depends on the ingenuity of the clinical miner.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 22:05:34 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Danjuma", "Kwetishe Joro", ""]]}, {"id": "1504.04658", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson, Gerard Roma, Mark D. Plumbley", "title": "Deep Karaoke: Extracting Vocals from Musical Mixtures Using a\n  Convolutional Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification and extraction of singing voice from within musical mixtures\nis a key challenge in source separation and machine audition. Recently, deep\nneural networks (DNN) have been used to estimate 'ideal' binary masks for\ncarefully controlled cocktail party speech separation problems. However, it is\nnot yet known whether these methods are capable of generalizing to the\ndiscrimination of voice and non-voice in the context of musical mixtures. Here,\nwe trained a convolutional DNN (of around a billion parameters) to provide\nprobabilistic estimates of the ideal binary mask for separation of vocal sounds\nfrom real-world musical mixtures. We contrast our DNN results with more\ntraditional linear methods. Our approach may be useful for automatic removal of\nvocal sounds from musical mixtures for 'karaoke' type applications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 23:07:17 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Simpson", "Andrew J. R.", ""], ["Roma", "Gerard", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1504.04666", "submitter": "Phong Le", "authors": "Phong Le and Willem Zuidema", "title": "Unsupervised Dependency Parsing: Let's Use Supervised Parsers", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-training approach to unsupervised dependency parsing that\nreuses existing supervised and unsupervised parsing algorithms. Our approach,\ncalled `iterated reranking' (IR), starts with dependency trees generated by an\nunsupervised parser, and iteratively improves these trees using the richer\nprobability models used in supervised parsing that are in turn trained on these\ntrees. Our system achieves 1.8% accuracy higher than the state-of-the-part\nparser of Spitkovsky et al. (2013) on the WSJ corpus.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 00:23:16 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Le", "Phong", ""], ["Zuidema", "Willem", ""]]}, {"id": "1504.04686", "submitter": "Raef Bassily", "authors": "Raef Bassily and Adam Smith", "title": "Local, Private, Efficient Protocols for Succinct Histograms", "comments": null, "journal-ref": null, "doi": "10.1145/2746539.2746632", "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give efficient protocols and matching accuracy lower bounds for frequency\nestimation in the local model for differential privacy. In this model,\nindividual users randomize their data themselves, sending differentially\nprivate reports to an untrusted server that aggregates them.\n  We study protocols that produce a succinct histogram representation of the\ndata. A succinct histogram is a list of the most frequent items in the data\n(often called \"heavy hitters\") along with estimates of their frequencies; the\nfrequency of all other items is implicitly estimated as 0.\n  If there are $n$ users whose items come from a universe of size $d$, our\nprotocols run in time polynomial in $n$ and $\\log(d)$. With high probability,\nthey estimate the accuracy of every item up to error\n$O\\left(\\sqrt{\\log(d)/(\\epsilon^2n)}\\right)$ where $\\epsilon$ is the privacy\nparameter. Moreover, we show that this much error is necessary, regardless of\ncomputational efficiency, and even for the simple setting where only one item\nappears with significant frequency in the data set.\n  Previous protocols (Mishra and Sandler, 2006; Hsu, Khanna and Roth, 2012) for\nthis task either ran in time $\\Omega(d)$ or had much worse error (about\n$\\sqrt[6]{\\log(d)/(\\epsilon^2n)}$), and the only known lower bound on error was\n$\\Omega(1/\\sqrt{n})$.\n  We also adapt a result of McGregor et al (2010) to the local setting. In a\nmodel with public coins, we show that each user need only send 1 bit to the\nserver. For all known local protocols (including ours), the transformation\npreserves computational efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 06:23:34 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Bassily", "Raef", ""], ["Smith", "Adam", ""]]}, {"id": "1504.04739", "submitter": "Wojciech Czarnecki", "authors": "Rafal Jozefowicz, Wojciech Marian Czarnecki", "title": "Fast optimization of Multithreshold Entropy Linear Classifier", "comments": "Presented at Theoretical Foundations of Machine Learning 2015\n  (http://tfml.gmum.net), final version published in Schedae Informaticae\n  Journal", "journal-ref": null, "doi": "10.4467/20838476SI.14.005.3022", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multithreshold Entropy Linear Classifier (MELC) is a density based model\nwhich searches for a linear projection maximizing the Cauchy-Schwarz Divergence\nof dataset kernel density estimation. Despite its good empirical results, one\nof its drawbacks is the optimization speed. In this paper we analyze how one\ncan speed it up through solving an approximate problem. We analyze two methods,\nboth similar to the approximate solutions of the Kernel Density Estimation\nquerying and provide adaptive schemes for selecting a crucial parameters based\non user-specified acceptable error. Furthermore we show how one can exploit\nwell known conjugate gradients and L-BFGS optimizers despite the fact that the\noriginal optimization problem should be solved on the sphere. All above methods\nand modifications are tested on 10 real life datasets from UCI repository to\nconfirm their practical usability.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 16:19:22 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Jozefowicz", "Rafal", ""], ["Czarnecki", "Wojciech Marian", ""]]}, {"id": "1504.04740", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki", "title": "On the consistency of Multithreshold Entropy Linear Classifier", "comments": "Presented at Theoretical Foundations of Machine Learning 2015\n  (http://tfml.gmum.net), final version published in Schedae Informaticae\n  Journal", "journal-ref": null, "doi": "10.4467/20838476SI.15.012.3034", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multithreshold Entropy Linear Classifier (MELC) is a recent classifier idea\nwhich employs information theoretic concept in order to create a multithreshold\nmaximum margin model. In this paper we analyze its consistency over\nmultithreshold linear models and show that its objective function upper bounds\nthe amount of misclassified points in a similar manner like hinge loss does in\nsupport vector machines. For further confirmation we also conduct some\nnumerical experiments on five datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 16:29:26 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""]]}, {"id": "1504.04770", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, C\\'edric Archambeau", "title": "Online Inference for Relation Extraction with a Reduced Feature Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to web-scale corpora is gradually bringing robust automatic knowledge\nbase creation and extension within reach. To exploit these large\nunannotated---and extremely difficult to annotate---corpora, unsupervised\nmachine learning methods are required. Probabilistic models of text have\nrecently found some success as such a tool, but scalability remains an obstacle\nin their application, with standard approaches relying on sampling schemes that\nare known to be difficult to scale. In this report, we therefore present an\nempirical assessment of the sublinear time sparse stochastic variational\ninference (SSVI) scheme applied to RelLDA. We demonstrate that online inference\nleads to relatively strong qualitative results but also identify some of its\npathologies---and those of the model---which will need to be overcome if SSVI\nis to be used for large-scale relation extraction.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 22:08:50 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Archambeau", "C\u00e9dric", ""]]}, {"id": "1504.04788", "submitter": "Wenlin Chen", "authors": "Wenlin Chen and James T. Wilson and Stephen Tyree and Kilian Q.\n  Weinberger and Yixin Chen", "title": "Compressing Neural Networks with the Hashing Trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep nets are increasingly used in applications suited for mobile devices,\na fundamental dilemma becomes apparent: the trend in deep learning is to grow\nmodels to absorb ever-increasing data set sizes; however mobile devices are\ndesigned with very little memory and cannot store such large models. We present\na novel network architecture, HashedNets, that exploits inherent redundancy in\nneural networks to achieve drastic reductions in model sizes. HashedNets uses a\nlow-cost hash function to randomly group connection weights into hash buckets,\nand all connections within the same hash bucket share a single parameter value.\nThese parameters are tuned to adjust to the HashedNets weight sharing\narchitecture with standard backprop during training. Our hashing procedure\nintroduces no additional memory overhead, and we demonstrate on several\nbenchmark data sets that HashedNets shrink the storage requirements of neural\nnetworks substantially while mostly preserving generalization performance.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 04:24:15 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Chen", "Wenlin", ""], ["Wilson", "James T.", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian Q.", ""], ["Chen", "Yixin", ""]]}, {"id": "1504.04850", "submitter": "Adway Mitra", "authors": "Adway Mitra", "title": "Exploring Bayesian Models for Multi-level Clustering of Hierarchically\n  Grouped Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of Bayesian models have been proposed for data that is divided\nhierarchically into groups. These models aim to cluster the data at different\nlevels of grouping, by assigning a mixture component to each datapoint, and a\nmixture distribution to each group. Multi-level clustering is facilitated by\nthe sharing of these components and distributions by the groups. In this paper,\nwe introduce the concept of Degree of Sharing (DoS) for the mixture components\nand distributions, with an aim to analyze and classify various existing models.\nNext we introduce a generalized hierarchical Bayesian model, of which the\nexisting models can be shown to be special cases. Unlike most of these models,\nour model takes into account the sequential nature of the data, and various\nother temporal structures at different levels while assigning mixture\ncomponents and distributions. We show one specialization of this model aimed at\nhierarchical segmentation of news transcripts, and present a Gibbs Sampling\nbased inference algorithm for it. We also show experimentally that the proposed\nmodel outperforms existing models for the same task.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 16:37:46 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Mitra", "Adway", ""]]}, {"id": "1504.05035", "submitter": "Wangmeng Zuo", "authors": "Xiaohe Wu, Wangmeng Zuo, Yuanyuan Zhu, Liang Lin", "title": "F-SVM: Combination of Feature Transformation and SVM Learning via Convex\n  Relaxation", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalization error bound of support vector machine (SVM) depends on the\nratio of radius and margin, while standard SVM only considers the maximization\nof the margin but ignores the minimization of the radius. Several approaches\nhave been proposed to integrate radius and margin for joint learning of feature\ntransformation and SVM classifier. However, most of them either require the\nform of the transformation matrix to be diagonal, or are non-convex and\ncomputationally expensive. In this paper, we suggest a novel approximation for\nthe radius of minimum enclosing ball (MEB) in feature space, and then propose a\nconvex radius-margin based SVM model for joint learning of feature\ntransformation and SVM classifier, i.e., F-SVM. An alternating minimization\nmethod is adopted to solve the F-SVM model, where the feature transformation is\nupdatedvia gradient descent and the classifier is updated by employing the\nexisting SVM solver. By incorporating with kernel principal component analysis,\nF-SVM is further extended for joint learning of nonlinear transformation and\nclassifier. Experimental results on the UCI machine learning datasets and the\nLFW face datasets show that F-SVM outperforms the standard SVM and the existing\nradius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\\mu}.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 12:36:50 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Wu", "Xiaohe", ""], ["Zuo", "Wangmeng", ""], ["Zhu", "Yuanyuan", ""], ["Lin", "Liang", ""]]}, {"id": "1504.05059", "submitter": "Michael Tschannen", "authors": "Michael Tschannen, Helmut B\\\"olcskei", "title": "Nonparametric Nearest Neighbor Random Process Clustering", "comments": "IEEE International Symposium on Information Theory (ISIT), June 2015,\n  to appear", "journal-ref": null, "doi": "10.1109/ISIT.2015.7282647", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering noisy finite-length observations of\nstationary ergodic random processes according to their nonparametric generative\nmodels without prior knowledge of the model statistics and the number of\ngenerative models. Two algorithms, both using the L1-distance between estimated\npower spectral densities (PSDs) as a measure of dissimilarity, are analyzed.\nThe first algorithm, termed nearest neighbor process clustering (NNPC), to the\nbest of our knowledge, is new and relies on partitioning the nearest neighbor\ngraph of the observations via spectral clustering. The second algorithm, simply\nreferred to as k-means (KM), consists of a single k-means iteration with\nfarthest point initialization and was considered before in the literature,\nalbeit with a different measure of dissimilarity and with asymptotic\nperformance results only. We show that both NNPC and KM succeed with high\nprobability under noise and even when the generative process PSDs overlap\nsignificantly, all provided that the observation length is sufficiently large.\nOur results quantify the tradeoff between the overlap of the generative process\nPSDs, the noise variance, and the observation length. Finally, we present\nnumerical performance results for synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 13:48:45 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Tschannen", "Michael", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1504.05070", "submitter": "Han Zhao", "authors": "Han Zhao, Zhengdong Lu, Pascal Poupart", "title": "Self-Adaptive Hierarchical Sentence Model", "comments": "8 pages, 7 figures, accepted as a full paper at IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately model a sentence at varying stages (e.g.,\nword-phrase-sentence) plays a central role in natural language processing. As\nan effort towards this goal we propose a self-adaptive hierarchical sentence\nmodel (AdaSent). AdaSent effectively forms a hierarchy of representations from\nwords to phrases and then to sentences through recursive gated local\ncomposition of adjacent segments. We design a competitive mechanism (through\ngating networks) to allow the representations of the same sentence to be\nengaged in a particular learning task (e.g., classification), therefore\neffectively mitigating the gradient vanishing problem persistent in other\nrecursive models. Both qualitative and quantitative analysis shows that AdaSent\ncan automatically form and select the representations suitable for the task at\nhand during training, yielding superior classification performance over\ncompetitor models on 5 benchmark data sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 14:26:41 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 17:12:56 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Zhao", "Han", ""], ["Lu", "Zhengdong", ""], ["Poupart", "Pascal", ""]]}, {"id": "1504.05122", "submitter": "Reinaldo Augusto Uribe Muriel", "authors": "Reinaldo Uribe Muriel, Fernando Lozando and Charles Anderson", "title": "Optimal Nudging: Solving Average-Reward Semi-Markov Decision Processes\n  as a Minimal Sequence of Cumulative Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel method to solve average-reward semi-Markov\ndecision processes, by reducing them to a minimal sequence of cumulative reward\nproblems. The usual solution methods for this type of problems update the gain\n(optimal average reward) immediately after observing the result of taking an\naction. The alternative introduced, optimal nudging, relies instead on setting\nthe gain to some fixed value, which transitorily makes the problem a\ncumulative-reward task, solving it by any standard reinforcement learning\nmethod, and only then updating the gain in a way that minimizes uncertainty in\na minmax sense. The rule for optimal gain update is derived by exploiting the\ngeometric features of the w-l space, a simple mapping of the space of policies.\nThe total number of cumulative reward tasks that need to be solved is shown to\nbe small. Some experiments are presented to explore the features of the\nalgorithm and to compare its performance with other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 16:58:26 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Muriel", "Reinaldo Uribe", ""], ["Lozando", "Fernando", ""], ["Anderson", "Charles", ""]]}, {"id": "1504.05229", "submitter": "Yao Xie", "authors": "Yang Cao and Yao Xie", "title": "Poisson Matrix Recovery and Completion", "comments": "Submitted to IEEE Journal. Parts of the paper have appeared in\n  GlobalSIP 2013, GlobalSIP 2014, and ISIT 2015. arXiv admin note: substantial\n  text overlap with arXiv:1501.06243", "journal-ref": null, "doi": "10.1109/TSP.2015.2500192", "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the theory of low-rank matrix recovery and completion to the case\nwhen Poisson observations for a linear combination or a subset of the entries\nof a matrix are available, which arises in various applications with count\ndata. We consider the usual matrix recovery formulation through maximum\nlikelihood with proper constraints on the matrix $M$ of size $d_1$-by-$d_2$,\nand establish theoretical upper and lower bounds on the recovery error. Our\nbounds for matrix completion are nearly optimal up to a factor on the order of\n$\\mathcal{O}(\\log(d_1 d_2))$. These bounds are obtained by combing techniques\nfor compressed sensing for sparse vectors with Poisson noise and for analyzing\nlow-rank matrices, as well as adapting the arguments used for one-bit matrix\ncompletion \\cite{davenport20121} (although these two problems are different in\nnature) and the adaptation requires new techniques exploiting properties of the\nPoisson likelihood function and tackling the difficulties posed by the locally\nsub-Gaussian characteristic of the Poisson distribution. Our results highlight\na few important distinctions of the Poisson case compared to the prior work\nincluding having to impose a minimum signal-to-noise requirement on each\nobserved entry and a gap in the upper and lower bounds. We also develop a set\nof efficient iterative algorithms and demonstrate their good performance on\nsynthetic examples and real data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 21:09:47 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 02:13:26 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1504.05287", "submitter": "Tengyu Ma", "authors": "Rong Ge, Tengyu Ma", "title": "Decomposing Overcomplete 3rd Order Tensors using Sum-of-Squares\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor rank and low-rank tensor decompositions have many applications in\nlearning and complexity theory. Most known algorithms use unfoldings of tensors\nand can only handle rank up to $n^{\\lfloor p/2 \\rfloor}$ for a $p$-th order\ntensor in $\\mathbb{R}^{n^p}$. Previously no efficient algorithm can decompose\n3rd order tensors when the rank is super-linear in the dimension. Using ideas\nfrom sum-of-squares hierarchy, we give the first quasi-polynomial time\nalgorithm that can decompose a random 3rd order tensor decomposition when the\nrank is as large as $n^{3/2}/\\textrm{polylog} n$.\n  We also give a polynomial time algorithm for certifying the injective norm of\nrandom low rank tensors. Our tensor decomposition algorithm exploits the\nrelationship between injective norm and the tensor components. The proof relies\non interesting tools for decoupling random variables to prove better matrix\nconcentration bounds, which can be useful in other settings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 03:21:53 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1504.05289", "submitter": "Sebastian Roch", "authors": "Elchanan Mossel, Sebastien Roch", "title": "Distance-based species tree estimation: information-theoretic trade-off\n  between number of loci and sequence length under the coalescent", "comments": "To appear in The Annals of Applied Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the reconstruction of a phylogeny from multiple genes under the\nmultispecies coalescent. We establish a connection with the sparse signal\ndetection problem, where one seeks to distinguish between a distribution and a\nmixture of the distribution and a sparse signal. Using this connection, we\nderive an information-theoretic trade-off between the number of genes, $m$,\nneeded for an accurate reconstruction and the sequence length, $k$, of the\ngenes. Specifically, we show that to detect a branch of length $f$, one needs\n$m = \\Theta(1/[f^{2} \\sqrt{k}])$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 03:39:05 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 18:36:35 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Mossel", "Elchanan", ""], ["Roch", "Sebastien", ""]]}, {"id": "1504.05321", "submitter": "Gregory Valiant", "authors": "Gregory Valiant, Paul Valiant", "title": "Instance Optimal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following basic learning task: given independent draws from\nan unknown distribution over a discrete support, output an approximation of the\ndistribution that is as accurate as possible in $\\ell_1$ distance (i.e. total\nvariation or statistical distance). Perhaps surprisingly, it is often possible\nto \"de-noise\" the empirical distribution of the samples to return an\napproximation of the true distribution that is significantly more accurate than\nthe empirical distribution, without relying on any prior assumptions on the\ndistribution. We present an instance optimal learning algorithm which optimally\nperforms this de-noising for every distribution for which such a de-noising is\npossible. More formally, given $n$ independent draws from a distribution $p$,\nour algorithm returns a labelled vector whose expected distance from $p$ is\nequal to the minimum possible expected error that could be obtained by any\nalgorithm that knows the true unlabeled vector of probabilities of distribution\n$p$ and simply needs to assign labels, up to an additive subconstant term that\nis independent of $p$ and goes to zero as $n$ gets large. One conceptual\nimplication of this result is that for large samples, Bayesian assumptions on\nthe \"shape\" or bounds on the tail probabilities of a distribution over discrete\nsupport are not helpful for the task of learning the distribution.\n  As a consequence of our techniques, we also show that given a set of $n$\nsamples from an arbitrary distribution, one can accurately estimate the\nexpected number of distinct elements that will be observed in a sample of any\nsize up to $n \\log n$. This sort of extrapolation is practically relevant,\nparticularly to domains such as genomics where it is important to understand\nhow much more might be discovered given larger sample sizes, and we are\noptimistic that our approach is practically viable.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 07:05:24 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 05:09:21 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Valiant", "Gregory", ""], ["Valiant", "Paul", ""]]}, {"id": "1504.05408", "submitter": "Hong Tao", "authors": "Hong Tao, Chenping Hou, Feiping Nie, Yuanyuan Jiao, Dongyun Yi", "title": "Effective Discriminative Feature Selection with Non-trivial Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection and feature transformation, the two main ways to reduce\ndimensionality, are often presented separately. In this paper, a feature\nselection method is proposed by combining the popular transformation based\ndimensionality reduction method Linear Discriminant Analysis (LDA) and sparsity\nregularization. We impose row sparsity on the transformation matrix of LDA\nthrough ${\\ell}_{2,1}$-norm regularization to achieve feature selection, and\nthe resultant formulation optimizes for selecting the most discriminative\nfeatures and removing the redundant ones simultaneously. The formulation is\nextended to the ${\\ell}_{2,p}$-norm regularized case: which is more likely to\noffer better sparsity when $0<p<1$. Thus the formulation is a better\napproximation to the feature selection problem. An efficient algorithm is\ndeveloped to solve the ${\\ell}_{2,p}$-norm based optimization problem and it is\nproved that the algorithm converges when $0<p\\le 2$. Systematical experiments\nare conducted to understand the work of the proposed method. Promising\nexperimental results on various types of real-world data sets demonstrate the\neffectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 12:40:32 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Tao", "Hong", ""], ["Hou", "Chenping", ""], ["Nie", "Feiping", ""], ["Jiao", "Yuanyuan", ""], ["Yi", "Dongyun", ""]]}, {"id": "1504.05473", "submitter": "Yury Kashnitsky", "authors": "Yury Kashnitsky, Dmitry I. Ignatov", "title": "Can FCA-based Recommender System Suggest a Proper Classifier?", "comments": "10 pages, 1 figure, 4 tables, ECAI 2014, workshop \"What FCA can do\n  for \"Artifficial Intelligence\"", "journal-ref": "CEUR Workshop Proceedings, 1257, pp. 17-26 (2014)", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper briefly introduces multiple classifier systems and describes a new\nalgorithm, which improves classification accuracy by means of recommendation of\na proper algorithm to an object classification. This recommendation is done\nassuming that a classifier is likely to predict the label of the object\ncorrectly if it has correctly classified its neighbors. The process of\nassigning a classifier to each object is based on Formal Concept Analysis. We\nexplain the idea of the algorithm with a toy example and describe our first\nexperiments with real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:38:23 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Kashnitsky", "Yury", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1504.05477", "submitter": "Christopher Musco", "authors": "Cameron Musco and Christopher Musco", "title": "Randomized Block Krylov Methods for Stronger and Faster Approximate\n  Singular Value Decomposition", "comments": "Neural Information Processing Systems 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since being analyzed by Rokhlin, Szlam, and Tygert and popularized by Halko,\nMartinsson, and Tropp, randomized Simultaneous Power Iteration has become the\nmethod of choice for approximate singular value decomposition. It is more\naccurate than simpler sketching algorithms, yet still converges quickly for any\nmatrix, independently of singular value gaps. After $\\tilde{O}(1/\\epsilon)$\niterations, it gives a low-rank approximation within $(1+\\epsilon)$ of optimal\nfor spectral norm error.\n  We give the first provable runtime improvement on Simultaneous Iteration: a\nsimple randomized block Krylov method, closely related to the classic Block\nLanczos algorithm, gives the same guarantees in just\n$\\tilde{O}(1/\\sqrt{\\epsilon})$ iterations and performs substantially better\nexperimentally. Despite their long history, our analysis is the first of a\nKrylov subspace method that does not depend on singular value gaps, which are\nunreliable in practice.\n  Furthermore, while it is a simple accuracy benchmark, even $(1+\\epsilon)$\nerror for spectral norm low-rank approximation does not imply that an algorithm\nreturns high quality principal components, a major issue for data applications.\nWe address this problem for the first time by showing that both Block Krylov\nIteration and a minor modification of Simultaneous Iteration give nearly\noptimal PCA for any matrix. This result further justifies their strength over\nnon-iterative sketching methods.\n  Finally, we give insight beyond the worst case, justifying why both\nalgorithms can run much faster in practice than predicted. We clarify how\nsimple techniques can take advantage of common matrix properties to\nsignificantly improve runtime.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:48:44 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 23:43:50 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2015 03:55:11 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2015 19:35:08 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1504.05487", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Helmut B\\\"olcskei", "title": "Deep Convolutional Neural Networks Based on Semi-Discrete Frames", "comments": "Proc. of IEEE International Symposium on Information Theory (ISIT),\n  Hong Kong, China, June 2015, to appear", "journal-ref": "Proc. of IEEE International Symposium on Information Theory\n  (ISIT), Hong Kong, China, pp. 1212-1216, June 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have led to breakthrough results in\npractical feature extraction applications. The mathematical analysis of these\nnetworks was pioneered by Mallat, 2012. Specifically, Mallat considered\nso-called scattering networks based on identical semi-discrete wavelet frames\nin each network layer, and proved translation-invariance as well as deformation\nstability of the resulting feature extractor. The purpose of this paper is to\ndevelop Mallat's theory further by allowing for different and, most\nimportantly, general semi-discrete frames (such as, e.g., Gabor frames,\nwavelets, curvelets, shearlets, ridgelets) in distinct network layers. This\nallows to extract wider classes of features than point singularities resolved\nby the wavelet transform. Our generalized feature extractor is proven to be\ntranslation-invariant, and we develop deformation stability results for a\nlarger class of deformations than those considered by Mallat. For Mallat's\nwavelet-based feature extractor, we get rid of a number of technical\nconditions. The mathematical engine behind our results is continuous frame\ntheory, which allows us to completely detach the invariance and deformation\nstability proofs from the particular algebraic structure of the underlying\nframes.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 16:01:00 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Wiatowski", "Thomas", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1504.05517", "submitter": "Francisco Zamora-Martinez", "authors": "Juan Pardo and Francisco Zamora-Martinez and Paloma Botella-Rocamora", "title": "Online Learning Algorithm for Time Series Forecasting Suitable for Low\n  Cost Wireless Sensor Networks Nodes", "comments": "28 pages, Published 21 April 2015 at MDPI's journal \"Sensors\"", "journal-ref": "Sensors 2015, 15(4), 9277-9304", "doi": "10.3390/s150409277", "report-no": null, "categories": "cs.NI cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Time series forecasting is an important predictive methodology which can be\napplied to a wide range of problems. Particularly, forecasting the indoor\ntemperature permits an improved utilization of the HVAC (Heating, Ventilating\nand Air Conditioning) systems in a home and thus a better energy efficiency.\nWith such purpose the paper describes how to implement an Artificial Neural\nNetwork (ANN) algorithm in a low cost system-on-chip to develop an autonomous\nintelligent wireless sensor network. The present paper uses a Wireless Sensor\nNetworks (WSN) to monitor and forecast the indoor temperature in a smart home,\nbased on low resources and cost microcontroller technology as the 8051MCU. An\non-line learning approach, based on Back-Propagation (BP) algorithm for ANNs,\nhas been developed for real-time time series learning. It performs the model\ntraining with every new data that arrive to the system, without saving enormous\nquantities of data to create a historical database as usual, i.e., without\nprevious knowledge. Consequently to validate the approach a simulation study\nthrough a Bayesian baseline model have been tested in order to compare with a\ndatabase of a real application aiming to see the performance and accuracy. The\ncore of the paper is a new algorithm, based on the BP one, which has been\ndescribed in detail, and the challenge was how to implement a computational\ndemanding algorithm in a simple architecture with very few hardware resources.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 17:31:41 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Pardo", "Juan", ""], ["Zamora-Martinez", "Francisco", ""], ["Botella-Rocamora", "Paloma", ""]]}, {"id": "1504.05539", "submitter": "Richard Sutton", "authors": "Richard S. Sutton, Brian Tanner", "title": "Temporal-Difference Networks", "comments": "8 pages, 3 figures, presented at the 2004 conference on Neural\n  Information Processing Systems. in Advances in Neural Information Processing\n  Systems 17 (proceedings of the 2004 conference), Saul, L. K., Weiss, Y., and\n  Bottou, L. (Eds)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generalization of temporal-difference (TD) learning to\nnetworks of interrelated predictions. Rather than relating a single prediction\nto itself at a later time, as in conventional TD methods, a TD network relates\neach prediction in a set of predictions to other predictions in the set at a\nlater time. TD networks can represent and apply TD learning to a much wider\nclass of predictions than has previously been possible. Using a random-walk\nexample, we show that these networks can be used to learn to predict by a fixed\ninterval, which is not possible with conventional TD methods. Secondly, we show\nthat if the inter-predictive relationships are made conditional on action, then\nthe usual learning-efficiency advantage of TD methods over Monte Carlo\n(supervised learning) methods becomes particularly pronounced. Thirdly, we\ndemonstrate that TD networks can learn predictive state representations that\nenable exact solution of a non-Markov problem. A very broad range of\ninter-predictive temporal relationships can be expressed in these networks.\nOverall we argue that TD networks represent a substantial extension of the\nabilities of TD methods and bring us closer to the goal of representing world\nknowledge in entirely predictive, grounded terms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 18:33:39 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Sutton", "Richard S.", ""], ["Tanner", "Brian", ""]]}, {"id": "1504.05619", "submitter": "Hamid Tizhoosh", "authors": "Hamid R. Tizhoosh and Shahryar Rahnamayan", "title": "Learning Opposites with Evolving Rules", "comments": "Accepted for publication in The 2015 IEEE International Conference on\n  Fuzzy Systems (FUZZ-IEEE 2015), August 2-5, 2015, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of opposition-based learning was introduced 10 years ago. Since then\na noteworthy group of researchers has used some notions of oppositeness to\nimprove existing optimization and learning algorithms. Among others,\nevolutionary algorithms, reinforcement agents, and neural networks have been\nreportedly extended into their opposition-based version to become faster and/or\nmore accurate. However, most works still use a simple notion of opposites,\nnamely linear (or type- I) opposition, that for each $x\\in[a,b]$ assigns its\nopposite as $\\breve{x}_I=a+b-x$. This, of course, is a very naive estimate of\nthe actual or true (non-linear) opposite $\\breve{x}_{II}$, which has been\ncalled type-II opposite in literature. In absence of any knowledge about a\nfunction $y=f(\\mathbf{x})$ that we need to approximate, there seems to be no\nalternative to the naivety of type-I opposition if one intents to utilize\noppositional concepts. But the question is if we can receive some level of\naccuracy increase and time savings by using the naive opposite estimate\n$\\breve{x}_I$ according to all reports in literature, what would we be able to\ngain, in terms of even higher accuracies and more reduction in computational\ncomplexity, if we would generate and employ true opposites? This work\nintroduces an approach to approximate type-II opposites using evolving fuzzy\nrules when we first perform opposition mining. We show with multiple examples\nthat learning true opposites is possible when we mine the opposites from the\ntraining data to subsequently approximate $\\breve{x}_{II}=f(\\mathbf{x},y)$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 22:16:17 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Tizhoosh", "Hamid R.", ""], ["Rahnamayan", "Shahryar", ""]]}, {"id": "1504.05632", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Wei Han,\n  Jianchao Yang, and Thomas S. Huang", "title": "Self-Tuned Deep Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successfully applied to image super resolution (SR).\nIn this paper, we propose a deep joint super resolution (DJSR) model to exploit\nboth external and self similarities for SR. A Stacked Denoising Convolutional\nAuto Encoder (SDCAE) is first pre-trained on external examples with proper data\naugmentations. It is then fine-tuned with multi-scale self examples from each\ninput, where the reliability of self examples is explicitly taken into account.\nWe also enhance the model performance by sub-model training and selection. The\nDJSR model is extensively evaluated and compared with state-of-the-arts, and\nshow noticeable performance improvements both quantitatively and perceptually\non a wide range of images.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 02:01:36 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Yingzhen", ""], ["Wang", "Zhaowen", ""], ["Chang", "Shiyu", ""], ["Han", "Wei", ""], ["Yang", "Jianchao", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1504.05665", "submitter": "Kohei Hayashi", "authors": "Kohei Hayashi, Shin-ichi Maeda, Ryohei Fujimaki", "title": "Rebuilding Factorized Information Criterion: Asymptotically Accurate\n  Marginal Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorized information criterion (FIC) is a recently developed approximation\ntechnique for the marginal log-likelihood, which provides an automatic model\nselection framework for a few latent variable models (LVMs) with tractable\ninference algorithms. This paper reconsiders FIC and fills theoretical gaps of\nprevious FIC studies. First, we reveal the core idea of FIC that allows\ngeneralization for a broader class of LVMs, including continuous LVMs, in\ncontrast to previous FICs, which are applicable only to binary LVMs. Second, we\ninvestigate the model selection mechanism of the generalized FIC. Our analysis\nprovides a formal justification of FIC as a model selection criterion for LVMs\nand also a systematic procedure for pruning redundant latent variables that\nhave been removed heuristically in previous studies. Third, we provide an\ninterpretation of FIC as a variational free energy and uncover a few\npreviously-unknown their relationships. A demonstrative study on Bayesian\nprincipal component analysis is provided and numerical experiments support our\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 06:27:19 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Hayashi", "Kohei", ""], ["Maeda", "Shin-ichi", ""], ["Fujimaki", "Ryohei", ""]]}, {"id": "1504.05800", "submitter": "Uri Stemmer", "authors": "Kobbi Nissim, Uri Stemmer", "title": "On the Generalization Properties of Differential Privacy", "comments": "This paper was merged with another manuscript and is now subsumed by\n  arXiv:1511.02513", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new line of work, started with Dwork et al., studies the task of answering\nstatistical queries using a sample and relates the problem to the concept of\ndifferential privacy. By the Hoeffding bound, a sample of size $O(\\log\nk/\\alpha^2)$ suffices to answer $k$ non-adaptive queries within error $\\alpha$,\nwhere the answers are computed by evaluating the statistical queries on the\nsample. This argument fails when the queries are chosen adaptively (and can\nhence depend on the sample). Dwork et al. showed that if the answers are\ncomputed with $(\\epsilon,\\delta)$-differential privacy then $O(\\epsilon)$\naccuracy is guaranteed with probability $1-O(\\delta^\\epsilon)$. Using the\nPrivate Multiplicative Weights mechanism, they concluded that the sample size\ncan still grow polylogarithmically with the $k$.\n  Very recently, Bassily et al. presented an improved bound and showed that (a\nvariant of) the private multiplicative weights algorithm can answer $k$\nadaptively chosen statistical queries using sample complexity that grows\nlogarithmically in $k$. However, their results no longer hold for every\ndifferentially private algorithm, and require modifying the private\nmultiplicative weights algorithm in order to obtain their high probability\nbounds.\n  We greatly simplify the results of Dwork et al. and improve on the bound by\nshowing that differential privacy guarantees $O(\\epsilon)$ accuracy with\nprobability $1-O(\\delta\\log(1/\\epsilon)/\\epsilon)$. It would be tempting to\nguess that an $(\\epsilon,\\delta)$-differentially private computation should\nguarantee $O(\\epsilon)$ accuracy with probability $1-O(\\delta)$. However, we\nshow that this is not the case, and that our bound is tight (up to logarithmic\nfactors).\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 13:40:04 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 03:08:34 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""]]}, {"id": "1504.05811", "submitter": "Michele Colledanchise", "authors": "Michele Colledanchise, Ramviyas Parasuraman, and Petter \\\"Ogren", "title": "Learning of Behavior Trees for Autonomous Agents", "comments": null, "journal-ref": "IEEE Transactions on Games ( Volume: 11 , Issue: 2 , June 2019 )", "doi": "10.1109/TG.2018.2816806", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Definition of an accurate system model for Automated Planner (AP) is often\nimpractical, especially for real-world problems. Conversely, off-the-shelf\nplanners fail to scale up and are domain dependent. These drawbacks are\ninherited from conventional transition systems such as Finite State Machines\n(FSMs) that describes the action-plan execution generated by the AP. On the\nother hand, Behavior Trees (BTs) represent a valid alternative to FSMs\npresenting many advantages in terms of modularity, reactiveness, scalability\nand domain-independence. In this paper, we propose a model-free AP framework\nusing Genetic Programming (GP) to derive an optimal BT for an autonomous agent\nto achieve a given goal in unknown (but fully observable) environments. We\nillustrate the proposed framework using experiments conducted with an open\nsource benchmark Mario AI for automated generation of BTs that can play the\ngame character Mario to complete a certain level at various levels of\ndifficulty to include enemies and obstacles.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 14:06:06 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Colledanchise", "Michele", ""], ["Parasuraman", "Ramviyas", ""], ["\u00d6gren", "Petter", ""]]}, {"id": "1504.05823", "submitter": "Michael Katehakis", "authors": "Wesley Cowan and Junya Honda and Michael N. Katehakis", "title": "Normal Bandits of Unknown Means and Variances: Asymptotic Optimality,\n  Finite Horizon Regret Bounds, and a Solution to an Open Problem", "comments": "15 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of sampling sequentially from a finite number of $N \\geq\n2$ populations, specified by random variables $X^i_k$, $ i = 1,\\ldots , N,$ and\n$k = 1, 2, \\ldots$; where $X^i_k$ denotes the outcome from population $i$ the\n$k^{th}$ time it is sampled. It is assumed that for each fixed $i$,\n  $\\{ X^i_k \\}_{k \\geq 1}$ is a sequence of i.i.d. normal random variables,\nwith unknown mean $\\mu_i$ and unknown variance $\\sigma_i^2$.\n  The objective is to have a policy $\\pi$ for deciding from which of the $N$\npopulations to sample form at any time $n=1,2,\\ldots$ so as to maximize the\nexpected sum of outcomes of $n$ samples or equivalently to minimize the regret\ndue to lack on information of the parameters $\\mu_i$ and $\\sigma_i^2$. In this\npaper, we present a simple inflated sample mean (ISM) index policy that is\nasymptotically optimal in the sense of Theorem 4 below. This resolves a\nstanding open problem from Burnetas and Katehakis (1996). Additionally, finite\nhorizon regret bounds are given.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 14:30:13 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 03:15:23 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Cowan", "Wesley", ""], ["Honda", "Junya", ""], ["Katehakis", "Michael N.", ""]]}, {"id": "1504.05843", "submitter": "Hao Yang Mr", "authors": "Hao Yang, Joey Tianyi Zhou, Yu Zhang, Bin-Bin Gao, Jianxin Wu, Jianfei\n  Cai", "title": "Exploit Bounding Box Annotations for Multi-label Object Recognition", "comments": "Accepted in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown great performance as general\nfeature representations for object recognition applications. However, for\nmulti-label images that contain multiple objects from different categories,\nscales and locations, global CNN features are not optimal. In this paper, we\nincorporate local information to enhance the feature discriminative power. In\nparticular, we first extract object proposals from each image. With each image\ntreated as a bag and object proposals extracted from it treated as instances,\nwe transform the multi-label recognition problem into a multi-class\nmulti-instance learning problem. Then, in addition to extracting the typical\nCNN feature representation from each proposal, we propose to make use of\nground-truth bounding box annotations (strong labels) to add another level of\nlocal information by using nearest-neighbor relationships of local regions to\nform a multi-view pipeline. The proposed multi-view multi-instance framework\nutilizes both weak and strong labels effectively, and more importantly it has\nthe generalization ability to even boost the performance of unseen categories\nby partial strong labels from other categories. Our framework is extensively\ncompared with state-of-the-art hand-crafted feature based methods and CNN based\nmethods on two multi-label benchmark datasets. The experimental results\nvalidate the discriminative power and the generalization ability of the\nproposed framework. With strong labels, our framework is able to achieve\nstate-of-the-art results in both datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 15:01:29 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 09:44:35 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Yang", "Hao", ""], ["Zhou", "Joey Tianyi", ""], ["Zhang", "Yu", ""], ["Gao", "Bin-Bin", ""], ["Wu", "Jianxin", ""], ["Cai", "Jianfei", ""]]}, {"id": "1504.05854", "submitter": "Jordan Frecon", "authors": "Jordan Frecon, Nelly Pustelnik, Patrice Abry and Laurent Condat", "title": "On-the-fly Approximation of Multivariate Total Variation Minimization", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2516962", "report-no": null, "categories": "cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of change-point detection, addressed by Total Variation\nminimization strategies, an efficient on-the-fly algorithm has been designed\nleading to exact solutions for univariate data. In this contribution, an\nextension of such an on-the-fly strategy to multivariate data is investigated.\nThe proposed algorithm relies on the local validation of the Karush-Kuhn-Tucker\nconditions on the dual problem. Showing that the non-local nature of the\nmultivariate setting precludes to obtain an exact on-the-fly solution, we\ndevise an on-the-fly algorithm delivering an approximate solution, whose\nquality is controlled by a practitioner-tunable parameter, acting as a\ntrade-off between quality and computational cost. Performance assessment shows\nthat high quality solutions are obtained on-the-fly while benefiting of\ncomputational costs several orders of magnitude lower than standard iterative\nprocedures. The proposed algorithm thus provides practitioners with an\nefficient multivariate change-point detection on-the-fly procedure.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 16:01:55 GMT"}, {"version": "v2", "created": "Sun, 28 Aug 2016 17:48:03 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Frecon", "Jordan", ""], ["Pustelnik", "Nelly", ""], ["Abry", "Patrice", ""], ["Condat", "Laurent", ""]]}, {"id": "1504.05880", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan and Mark Rudelson", "title": "Spectral Norm of Random Kernel Matrices with Applications to Privacy", "comments": "16 pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are an extremely popular set of techniques used for many\nimportant machine learning and data analysis applications. In addition to\nhaving good practical performances, these methods are supported by a\nwell-developed theory. Kernel methods use an implicit mapping of the input data\ninto a high dimensional feature space defined by a kernel function, i.e., a\nfunction returning the inner product between the images of two data points in\nthe feature space. Central to any kernel method is the kernel matrix, which is\nbuilt by evaluating the kernel function on a given sample dataset.\n  In this paper, we initiate the study of non-asymptotic spectral theory of\nrandom kernel matrices. These are n x n random matrices whose (i,j)th entry is\nobtained by evaluating the kernel function on $x_i$ and $x_j$, where\n$x_1,...,x_n$ are a set of n independent random high-dimensional vectors. Our\nmain contribution is to obtain tight upper bounds on the spectral norm (largest\neigenvalue) of random kernel matrices constructed by commonly used kernel\nfunctions based on polynomials and Gaussian radial basis.\n  As an application of these results, we provide lower bounds on the distortion\nneeded for releasing the coefficients of kernel ridge regression under\nattribute privacy, a general privacy notion which captures a large class of\nprivacy definitions. Kernel ridge regression is standard method for performing\nnon-parametric regression that regularly outperforms traditional regression\napproaches in various domains. Our privacy distortion lower bounds are the\nfirst for any kernel technique, and our analysis assumes realistic scenarios\nfor the input, unlike all previous lower bounds for other release problems\nwhich only hold under very restrictive input settings.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 16:54:48 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Rudelson", "Mark", ""]]}, {"id": "1504.06080", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne", "title": "svcR: An R Package for Support Vector Clustering improved with Geometric\n  Hashing applied to Lexical Pattern Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new R package which takes a numerical matrix format as data\ninput, and computes clusters using a support vector clustering method (SVC). We\nhave implemented an original 2D-grid labeling approach to speed up cluster\nextraction. In this sense, SVC can be seen as an efficient cluster extraction\nif clusters are separable in a 2-D map. Secondly we showed that this SVC\napproach using a Jaccard-Radial base kernel can help to classify well enough a\nset of terms into ontological classes and help to define regular expression\nrules for information extraction in documents; our case study concerns a set of\nterms and documents about developmental and molecular biology.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 08:29:11 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Turenne", "Nicolas", ""]]}, {"id": "1504.06165", "submitter": "Nitish Gupta", "authors": "Nitish Gupta, Sameer Singh", "title": "Collectively Embedding Multi-Relational Data for Predicting User\n  Preferences", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization has found incredible success and widespread application\nas a collaborative filtering based approach to recommendations. Unfortunately,\nincorporating additional sources of evidence, especially ones that are\nincomplete and noisy, is quite difficult to achieve in such models, however, is\noften crucial for obtaining further gains in accuracy. For example, additional\ninformation about businesses from reviews, categories, and attributes should be\nleveraged for predicting user preferences, even though this information is\noften inaccurate and partially-observed. Instead of creating customized methods\nthat are specific to each type of evidences, in this paper we present a generic\napproach to factorization of relational data that collectively models all the\nrelations in the database. By learning a set of embeddings that are shared\nacross all the relations, the model is able to incorporate observed information\nfrom all the relations, while also predicting all the relations of interest.\nOur evaluation on multiple Amazon and Yelp datasets demonstrates effective\nutilization of additional information for held-out preference prediction, but\nfurther, we present accurate models even for the cold-starting businesses and\nproducts for which we do not observe any ratings or reviews. We also illustrate\nthe capability of the model in imputing missing information and jointly\nvisualizing words, categories, and attribute factors.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 13:07:24 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Gupta", "Nitish", ""], ["Singh", "Sameer", ""]]}, {"id": "1504.06274", "submitter": "Qiang Wu", "authors": "Dong Mao, Yang Wang and Qiang Wu", "title": "A new approach for physiological time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a new approach for the analysis of physiological time series. An\niterative convolution filter is used to decompose the time series into various\ncomponents. Statistics of these components are extracted as features to\ncharacterize the mechanisms underlying the time series. Motivated by the\nstudies that show many normal physiological systems involve irregularity while\nthe decrease of irregularity usually implies the abnormality, the statistics\nfor \"outliers\" in the components are used as features measuring irregularity.\nSupport vector machines are used to select the most relevant features that are\nable to differentiate the time series from normal and abnormal systems. This\nnew approach is successfully used in the study of congestive heart failure by\nheart beat interval time series.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 17:56:33 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Mao", "Dong", ""], ["Wang", "Yang", ""], ["Wu", "Qiang", ""]]}, {"id": "1504.06305", "submitter": "Ping Li", "authors": "Martin Slawski, Ping Li, Matthias Hein", "title": "Regularization-free estimation in trace regression with symmetric\n  positive semidefinite matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, trace regression models have received considerable\nattention in the context of matrix completion, quantum state tomography, and\ncompressed sensing. Estimation of the underlying matrix from\nregularization-based approaches promoting low-rankedness, notably nuclear norm\nregularization, have enjoyed great popularity. In the present paper, we argue\nthat such regularization may no longer be necessary if the underlying matrix is\nsymmetric positive semidefinite (\\textsf{spd}) and the design satisfies certain\nconditions. In this situation, simple least squares estimation subject to an\n\\textsf{spd} constraint may perform as well as regularization-based approaches\nwith a proper choice of the regularization parameter, which entails knowledge\nof the noise level and/or tuning. By contrast, constrained least squares\nestimation comes without any tuning parameter and may hence be preferred due to\nits simplicity.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 19:30:38 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Slawski", "Martin", ""], ["Li", "Ping", ""], ["Hein", "Matthias", ""]]}, {"id": "1504.06329", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and John Grothendieck", "title": "Analysis of Stopping Active Learning based on Stabilizing Predictions", "comments": "10 pages, 8 tables; appeared in Proceedings of the Seventeenth\n  Conference on Computational Natural Language Learning, August 2013", "journal-ref": "In Proceedings of the Seventeenth Conference on Computational\n  Natural Language Learning, pages 10-19, Sofia, Bulgaria, August 2013.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the natural language processing (NLP) community, active learning has\nbeen widely investigated and applied in order to alleviate the annotation\nbottleneck faced by developers of new NLP systems and technologies. This paper\npresents the first theoretical analysis of stopping active learning based on\nstabilizing predictions (SP). The analysis has revealed three elements that are\ncentral to the success of the SP method: (1) bounds on Cohen's Kappa agreement\nbetween successively trained models impose bounds on differences in F-measure\nperformance of the models; (2) since the stop set does not have to be labeled,\nit can be made large in practice, helping to guarantee that the results\ntransfer to previously unseen streams of examples at test/application time; and\n(3) good (low variance) sample estimates of Kappa between successive models can\nbe obtained. Proofs of relationships between the level of Kappa agreement and\nthe difference in performance between consecutive models are presented.\nSpecifically, if the Kappa agreement between two models exceeds a threshold T\n(where $T>0$), then the difference in F-measure performance between those\nmodels is bounded above by $\\frac{4(1-T)}{T}$ in all cases. If precision of the\npositive conjunction of the models is assumed to be $p$, then the bound can be\ntightened to $\\frac{4(1-T)}{(p+1)T}$.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 20:07:01 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Bloodgood", "Michael", ""], ["Grothendieck", "John", ""]]}, {"id": "1504.06341", "submitter": "Burkhard C. Schipper", "authors": "Burkhard C. Schipper", "title": "Strategic Teaching and Learning in Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that there are uncoupled learning heuristics leading to Nash\nequilibrium in all finite games. Why should players use such learning\nheuristics and where could they come from? We show that there is no uncoupled\nlearning heuristic leading to Nash equilibrium in all finite games that a\nplayer has an incentive to adopt, that would be evolutionary stable or that\ncould \"learn itself\". Rather, a player has an incentive to strategically teach\nsuch a learning opponent in order secure at least the Stackelberg leader\npayoff. The impossibility result remains intact when restricted to the classes\nof generic games, two-player games, potential games, games with strategic\ncomplements or 2x2 games, in which learning is known to be \"nice\". More\ngenerally, it also applies to uncoupled learning heuristics leading to\ncorrelated equilibria, rationalizable outcomes, iterated admissible outcomes,\nor minimal curb sets. A possibility result restricted to \"strategically\ntrivial\" games fails if some generic games outside this class are considered as\nwell.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 20:49:16 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Schipper", "Burkhard C.", ""]]}, {"id": "1504.06366", "submitter": "Sakthithasan Sripirakas", "authors": "Sripirakas Sakthithasan, Russel Pears, Albert Bifet and Bernhard\n  Pfahringer", "title": "Use of Ensembles of Fourier Spectra in Capturing Recurrent Concepts in\n  Data Streams", "comments": "This paper has been accepted for IJCNN 2015 conference, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we apply ensembles of Fourier encoded spectra to capture\nand mine recurring concepts in a data stream environment. Previous research\nshowed that compact versions of Decision Trees can be obtained by applying the\nDiscrete Fourier Transform to accurately capture recurrent concepts in a data\nstream. However, in highly volatile environments where new concepts emerge\noften, the approach of encoding each concept in a separate spectrum is no\nlonger viable due to memory overload and thus in this research we present an\nensemble approach that addresses this problem. Our empirical results on real\nworld data and synthetic data exhibiting varying degrees of recurrence reveal\nthat the ensemble approach outperforms the single spectrum approach in terms of\nclassification accuracy, memory and execution time.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 23:34:39 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Sakthithasan", "Sripirakas", ""], ["Pears", "Russel", ""], ["Bifet", "Albert", ""], ["Pfahringer", "Bernhard", ""]]}, {"id": "1504.06394", "submitter": "Jing Wang", "authors": "Jing Wang and Jie Shen and Huan Xu", "title": "Social Trust Prediction via Max-norm Constrained 1-bit Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social trust prediction addresses the significant problem of exploring\ninteractions among users in social networks. Naturally, this problem can be\nformulated in the matrix completion framework, with each entry indicating the\ntrustness or distrustness. However, there are two challenges for the social\ntrust problem: 1) the observed data are with sign (1-bit) measurements; 2) they\nare typically sampled non-uniformly. Most of the previous matrix completion\nmethods do not well handle the two issues. Motivated by the recent progress of\nmax-norm, we propose to solve the problem with a 1-bit max-norm constrained\nformulation. Since max-norm is not easy to optimize, we utilize a reformulation\nof max-norm which facilitates an efficient projected gradient decent algorithm.\nWe demonstrate the superiority of our formulation on two benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 05:01:12 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Wang", "Jing", ""], ["Shen", "Jie", ""], ["Xu", "Huan", ""]]}, {"id": "1504.06494", "submitter": "Konstantinos Georgatzis", "authors": "Konstantinos Georgatzis, Christopher K. I. Williams", "title": "Discriminative Switching Linear Dynamical Systems applied to\n  Physiological Condition Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Discriminative Switching Linear Dynamical System (DSLDS) applied\nto patient monitoring in Intensive Care Units (ICUs). Our approach is based on\nidentifying the state-of-health of a patient given their observed vital signs\nusing a discriminative classifier, and then inferring their underlying\nphysiological values conditioned on this status. The work builds on the\nFactorial Switching Linear Dynamical System (FSLDS) (Quinn et al., 2009) which\nhas been previously used in a similar setting. The FSLDS is a generative model,\nwhereas the DSLDS is a discriminative model. We demonstrate on two real-world\ndatasets that the DSLDS is able to outperform the FSLDS in most cases of\ninterest, and that an $\\alpha$-mixture of the two models achieves higher\nperformance than either of the two models separately.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 13:23:40 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Georgatzis", "Konstantinos", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "1504.06544", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement Canonne, Themis Gouleakis and Ronitt Rubinfeld", "title": "Sampling Correctors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many situations, sample data is obtained from a noisy or imperfect source.\nIn order to address such corruptions, this paper introduces the concept of a\nsampling corrector. Such algorithms use structure that the distribution is\npurported to have, in order to allow one to make \"on-the-fly\" corrections to\nsamples drawn from probability distributions. These algorithms then act as\nfilters between the noisy data and the end user.\n  We show connections between sampling correctors, distribution learning\nalgorithms, and distribution property testing algorithms. We show that these\nconnections can be utilized to expand the applicability of known distribution\nlearning and property testing algorithms as well as to achieve improved\nalgorithms for those tasks.\n  As a first step, we show how to design sampling correctors using proper\nlearning algorithms. We then focus on the question of whether algorithms for\nsampling correctors can be more efficient in terms of sample complexity than\nlearning algorithms for the analogous families of distributions. When\ncorrecting monotonicity, we show that this is indeed the case when also granted\nquery access to the cumulative distribution function. We also obtain sampling\ncorrectors for monotonicity without this stronger type of access, provided that\nthe distribution be originally very close to monotone (namely, at a distance\n$O(1/\\log^2 n)$). In addition to that, we consider a restricted error model\nthat aims at capturing \"missing data\" corruptions. In this model, we show that\ndistributions that are close to monotone have sampling correctors that are\nsignificantly more efficient than achievable by the learning approach.\n  We also consider the question of whether an additional source of independent\nrandom bits is required by sampling correctors to implement the correction\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 15:39:52 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 03:56:00 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Canonne", "Cl\u00e9ment", ""], ["Gouleakis", "Themis", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1504.06580", "submitter": "Cicero dos Santos", "authors": "Cicero Nogueira dos Santos, Bing Xiang, Bowen Zhou", "title": "Classifying Relations by Ranking with Convolutional Neural Networks", "comments": "Accepted as a long paper in the 53rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation classification is an important semantic processing task for which\nstate-ofthe-art systems still rely on costly handcrafted features. In this work\nwe tackle the relation classification task using a convolutional neural network\nthat performs classification by ranking (CR-CNN). We propose a new pairwise\nranking loss function that makes it easy to reduce the impact of artificial\nclasses. We perform experiments using the the SemEval-2010 Task 8 dataset,\nwhich is designed for the task of classifying the relationship between two\nnominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art\nfor this dataset and achieve a F1 of 84.1 without using any costly handcrafted\nfeatures. Additionally, our experimental results show that: (1) our approach is\nmore effective than CNN followed by a softmax classifier; (2) omitting the\nrepresentation of the artificial class Other improves both precision and\nrecall; and (3) using only word embeddings as input features is enough to\nachieve state-of-the-art results if we consider only the text between the two\ntarget nominals.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 17:50:33 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 13:58:05 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Santos", "Cicero Nogueira dos", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1504.06667", "submitter": "Benjamin Fish", "authors": "Benjamin Fish, Rajmonda S. Caceres", "title": "Handling oversampling in dynamic networks using link prediction", "comments": "ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oversampling is a common characteristic of data representing dynamic\nnetworks. It introduces noise into representations of dynamic networks, but\nthere has been little work so far to compensate for it. Oversampling can affect\nthe quality of many important algorithmic problems on dynamic networks,\nincluding link prediction. Link prediction seeks to predict edges that will be\nadded to the network given previous snapshots. We show that not only does\noversampling affect the quality of link prediction, but that we can use link\nprediction to recover from the effects of oversampling. We also introduce a\nnovel generative model of noise in dynamic networks that represents\noversampling. We demonstrate the results of our approach on both synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 23:38:30 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 15:30:54 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Fish", "Benjamin", ""], ["Caceres", "Rajmonda S.", ""]]}, {"id": "1504.06681", "submitter": "Niangjun Chen", "authors": "Niangjun Chen, Anish Agarwal, Adam Wierman, Siddharth Barman and\n  Lachlan L. H. Andrew", "title": "Online Convex Optimization Using Predictions", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making use of predictions is a crucial, but under-explored, area of online\nalgorithms. This paper studies a class of online optimization problems where we\nhave external noisy predictions available. We propose a stochastic prediction\nerror model that generalizes prior models in the learning and stochastic\ncontrol communities, incorporates correlation among prediction errors, and\ncaptures the fact that predictions improve as time passes. We prove that\nachieving sublinear regret and constant competitive ratio for online algorithms\nrequires the use of an unbounded prediction window in adversarial settings, but\nthat under more realistic stochastic prediction error models it is possible to\nuse Averaging Fixed Horizon Control (AFHC) to simultaneously achieve sublinear\nregret and constant competitive ratio in expectation using only a\nconstant-sized prediction window. Furthermore, we show that the performance of\nAFHC is tightly concentrated around its mean.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 04:41:30 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Chen", "Niangjun", ""], ["Agarwal", "Anish", ""], ["Wierman", "Adam", ""], ["Barman", "Siddharth", ""], ["Andrew", "Lachlan L. H.", ""]]}, {"id": "1504.06692", "submitter": "Junhua Mao", "authors": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille", "title": "Learning like a Child: Fast Novel Visual Concept Learning from Sentence\n  Descriptions of Images", "comments": "ICCV 2015 camera ready version. We add much more novel visual\n  concepts in the NVC dataset and have released it, see\n  http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of learning novel visual concepts, and\ntheir interactions with other concepts, from a few images with sentence\ndescriptions. Using linguistic context and visual features, our method is able\nto efficiently hypothesize the semantic meaning of new words and add them to\nits word dictionary so that they can be used to describe images which contain\nthese novel concepts. Our method has an image captioning module based on m-RNN\nwith several improvements. In particular, we propose a transposed weight\nsharing scheme, which not only improves performance on image captioning, but\nalso makes the model more suitable for the novel concept learning task. We\npropose methods to prevent overfitting the new concepts. In addition, three\nnovel concept datasets are constructed for this new task. In the experiments,\nwe show that our method effectively learns novel visual concepts from a few\nexamples without disturbing the previously learned concepts. The project page\nis http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 06:45:35 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 02:36:05 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Wei", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Huang", "Zhiheng", ""], ["Yuille", "Alan", ""]]}, {"id": "1504.06785", "submitter": "Ju Sun", "authors": "Ju Sun, Qing Qu, John Wright", "title": "Complete Dictionary Recovery over the Sphere", "comments": "104 pages, 5 figures. Due to length constraint of publication, this\n  long paper are subsequently divided into two papers (arXiv:1511.03607 and\n  arXiv:1511.04777). Further updates will be made only to the two papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb R^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to the theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals, and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$. In contrast, prior\nresults based on efficient algorithms provide recovery guarantees when $\\mathbf\nX_0$ has only $O(n^{1-\\delta})$ nonzeros per column for any constant $\\delta\n\\in (0, 1)$.\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint, and hence is naturally\nphrased in the language of manifold optimization. To show this apparently hard\nproblem is tractable, we first provide a geometric characterization of the\nhigh-dimensional objective landscape, which shows that with high probability\nthere are no \"spurious\" local minima. This particular geometric structure\nallows us to design a Riemannian trust region algorithm over the sphere that\nprovably converges to one local minimizer with an arbitrary initialization,\ndespite the presence of saddle points. The geometric approach we develop here\nmay also shed light on other problems arising from nonconvex recovery of\nstructured signals.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 04:57:19 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 06:06:04 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 21:56:30 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Sun", "Ju", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "1504.06787", "submitter": "Chongxuan Li", "authors": "Chongxuan Li and Jun Zhu and Tianlin Shi and Bo Zhang", "title": "Max-margin Deep Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, little work has been done on\nexamining or empowering the discriminative ability of DGMs on making accurate\npredictions. This paper presents max-margin deep generative models (mmDGMs),\nwhich explore the strongly discriminative principle of max-margin learning to\nimprove the discriminative power of DGMs, while retaining the generative\ncapability. We develop an efficient doubly stochastic subgradient algorithm for\nthe piecewise linear objective. Empirical results on MNIST and SVHN datasets\ndemonstrate that (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; and\n(2) mmDGMs are competitive to the state-of-the-art fully discriminative\nnetworks by employing deep convolutional neural networks (CNNs) as both\nrecognition and generative models.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 06:01:19 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 01:58:31 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2015 08:40:09 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2015 03:01:06 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Li", "Chongxuan", ""], ["Zhu", "Jun", ""], ["Shi", "Tianlin", ""], ["Zhang", "Bo", ""]]}, {"id": "1504.06796", "submitter": "Mark Kozdoba", "authors": "Mark Kozdoba and Shie Mannor", "title": "Overlapping Communities Detection via Measure Space Embedding", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 28, 2890--2898,\n  2015", "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for community detection. The algorithm uses random\nwalks to embed the graph in a space of measures, after which a modification of\n$k$-means in that space is applied. The algorithm is therefore fast and easily\nparallelizable. We evaluate the algorithm on standard random graph benchmarks,\nincluding some overlapping community benchmarks, and find its performance to be\nbetter or at least as good as previously known algorithms. We also prove a\nlinear time (in number of edges) guarantee for the algorithm on a\n$p,q$-stochastic block model with $p \\geq c\\cdot N^{-\\frac{1}{2} + \\epsilon}$\nand $p-q \\geq c' \\sqrt{p N^{-\\frac{1}{2} + \\epsilon} \\log N}$.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 10:00:29 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 08:00:34 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Kozdoba", "Mark", ""], ["Mannor", "Shie", ""]]}, {"id": "1504.06798", "submitter": "Mark Kozdoba", "authors": "Mark Kozdoba and Shie Mannor", "title": "Overlapping Community Detection by Online Cluster Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new online algorithm for detecting overlapping communities. The\nmain ingredients are a modification of an online k-means algorithm and a new\napproach to modelling overlap in communities. An evaluation on large benchmark\ngraphs shows that the quality of discovered communities compares favorably to\nseveral methods in the recent literature, while the running time is\nsignificantly improved.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 10:18:09 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Kozdoba", "Mark", ""], ["Mannor", "Shie", ""]]}, {"id": "1504.06817", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou", "title": "Analysis of Nuclear Norm Regularization for Full-rank Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a theoretical analysis of the nuclear-norm\nregularized least squares for full-rank matrix completion. Although similar\nformulations have been examined by previous studies, their results are\nunsatisfactory because only additive upper bounds are provided. Under the\nassumption that the top eigenspaces of the target matrix are incoherent, we\nderive a relative upper bound for recovering the best low-rank approximation of\nthe unknown matrix. Our relative upper bound is tighter than previous additive\nbounds of other methods if the mass of the target matrix is concentrated on its\ntop eigenspaces, and also implies perfect recovery if it is low-rank. The\nanalysis is built upon the optimality condition of the regularized formulation\nand existing guarantees for low-rank matrix completion. To the best of our\nknowledge, this is first time such a relative bound is proved for the\nregularized formulation of matrix completion.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 13:12:16 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Zhang", "Lijun", ""], ["Yang", "Tianbao", ""], ["Jin", "Rong", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1504.06825", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner", "title": "Comparison of Training Methods for Deep Neural Networks", "comments": "50 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the difficulties of training neural networks and in\nparticular deep neural networks. It then provides a literature review of\ntraining methods for deep neural networks, with a focus on pre-training. It\nfocuses on Deep Belief Networks composed of Restricted Boltzmann Machines and\nStacked Autoencoders and provides an outreach on further and alternative\napproaches. It also includes related practical recommendations from the\nliterature on training them. In the second part, initial experiments using some\nof the covered methods are performed on two databases. In particular,\nexperiments are performed on the MNIST hand-written digit dataset and on facial\nemotion data from a Kaggle competition. The results are discussed in the\ncontext of results reported in other research papers. An error rate lower than\nthe best contribution to the Kaggle competition is achieved using an optimized\nStacked Autoencoder.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 14:09:17 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Glauner", "Patrick O.", ""]]}, {"id": "1504.06837", "submitter": "Marc Claesen", "authors": "Marc Claesen, Jesse Davis, Frank De Smet, Bart De Moor", "title": "Assessing binary classifiers using only positive and unlabeled data", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the performance of a learned model is a crucial part of machine\nlearning. However, in some domains only positive and unlabeled examples are\navailable, which prohibits the use of most standard evaluation metrics. We\npropose an approach to estimate any metric based on contingency tables,\nincluding ROC and PR curves, using only positive and unlabeled data. Estimating\nthese performance metrics is essentially reduced to estimating the fraction of\n(latent) positives in the unlabeled set, assuming known positives are a random\nsample of all positives. We provide theoretical bounds on the quality of our\nestimates, illustrate the importance of estimating the fraction of positives in\nthe unlabeled set and demonstrate empirically that we are able to reliably\nestimate ROC and PR curves on real data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 14:59:12 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 13:18:43 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Claesen", "Marc", ""], ["Davis", "Jesse", ""], ["De Smet", "Frank", ""], ["De Moor", "Bart", ""]]}, {"id": "1504.06852", "submitter": "Philipp Fischer", "authors": "Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip H\\\"ausser, Caner\n  Haz{\\i}rba\\c{s}, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers,\n  Thomas Brox", "title": "FlowNet: Learning Optical Flow with Convolutional Networks", "comments": "Added supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have recently been very successful in a\nvariety of computer vision tasks, especially on those linked to recognition.\nOptical flow estimation has not been among the tasks where CNNs were\nsuccessful. In this paper we construct appropriate CNNs which are capable of\nsolving the optical flow estimation problem as a supervised learning task. We\npropose and compare two architectures: a generic architecture and another one\nincluding a layer that correlates feature vectors at different image locations.\n  Since existing ground truth data sets are not sufficiently large to train a\nCNN, we generate a synthetic Flying Chairs dataset. We show that networks\ntrained on this unrealistic data still generalize very well to existing\ndatasets such as Sintel and KITTI, achieving competitive accuracy at frame\nrates of 5 to 10 fps.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 17:30:32 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 08:50:57 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Fischer", "Philipp", ""], ["Dosovitskiy", "Alexey", ""], ["Ilg", "Eddy", ""], ["H\u00e4usser", "Philip", ""], ["Haz\u0131rba\u015f", "Caner", ""], ["Golkov", "Vladimir", ""], ["van der Smagt", "Patrick", ""], ["Cremers", "Daniel", ""], ["Brox", "Thomas", ""]]}, {"id": "1504.06868", "submitter": "Gordon Cormack", "authors": "Gordon V. Cormack and Maura R. Grossman", "title": "Autonomy and Reliability of Continuous Active Learning for\n  Technology-Assisted Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We enhance the autonomy of the continuous active learning method shown by\nCormack and Grossman (SIGIR 2014) to be effective for technology-assisted\nreview, in which documents from a collection are retrieved and reviewed, using\nrelevance feedback, until substantially all of the relevant documents have been\nreviewed. Autonomy is enhanced through the elimination of topic-specific and\ndataset-specific tuning parameters, so that the sole input required by the user\nis, at the outset, a short query, topic description, or single relevant\ndocument; and, throughout the review, ongoing relevance assessments of the\nretrieved documents. We show that our enhancements consistently yield superior\nresults to Cormack and Grossman's version of continuous active learning, and\nother methods, not only on average, but on the vast majority of topics from\nfour separate sets of tasks: the legal datasets examined by Cormack and\nGrossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and\nthe construction of the TREC 2002 filtering test collection.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 19:19:01 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Cormack", "Gordon V.", ""], ["Grossman", "Maura R.", ""]]}, {"id": "1504.06897", "submitter": "Yilun Wang", "authors": "Chengqiang Bao and Liangtian He and Yilun Wang", "title": "Linear Spatial Pyramid Matching Using Non-convex and non-negative Sparse\n  Coding for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently sparse coding have been highly successful in image classification\nmainly due to its capability of incorporating the sparsity of image\nrepresentation. In this paper, we propose an improved sparse coding model based\non linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform\n(SIFT ) descriptors. The novelty is the simultaneous non-convex and\nnon-negative characters added to the sparse coding model. Our numerical\nexperiments show that the improved approach using non-convex and non-negative\nsparse coding is superior than the original ScSPM[1] on several typical\ndatabases.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 00:46:54 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Bao", "Chengqiang", ""], ["He", "Liangtian", ""], ["Wang", "Yilun", ""]]}, {"id": "1504.06937", "submitter": "Huasen Wu", "authors": "Huasen Wu, R. Srikant, Xin Liu, and Chong Jiang", "title": "Algorithms with Logarithmic or Sublinear Regret for Constrained\n  Contextual Bandits", "comments": "36 pages, 4 figures; accepted by the 29th Annual Conference on Neural\n  Information Processing Systems (NIPS), Montr\\'eal, Canada, Dec. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study contextual bandits with budget and time constraints, referred to as\nconstrained contextual bandits.The time and budget constraints significantly\ncomplicate the exploration and exploitation tradeoff because they introduce\ncomplex coupling among contexts over time.Such coupling effects make it\ndifficult to obtain oracle solutions that assume known statistics of bandits.\nTo gain insight, we first study unit-cost systems with known context\ndistribution. When the expected rewards are known, we develop an approximation\nof the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves\nnear-optimality and only requires the ordering of expected rewards. With these\nhighly desirable features, we then combine ALP with the upper-confidence-bound\n(UCB) method in the general case where the expected rewards are unknown {\\it a\npriori}. We show that the proposed UCB-ALP algorithm achieves logarithmic\nregret except for certain boundary cases. Further, we design algorithms and\nobtain similar regret analysis results for more general systems with unknown\ncontext distribution and heterogeneous costs. To the best of our knowledge,\nthis is the first work that shows how to achieve logarithmic regret in\nconstrained contextual bandits. Moreover, this work also sheds light on the\nstudy of computationally efficient algorithms for general constrained\ncontextual bandits.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 06:03:50 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 17:55:35 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 16:47:20 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Wu", "Huasen", ""], ["Srikant", "R.", ""], ["Liu", "Xin", ""], ["Jiang", "Chong", ""]]}, {"id": "1504.06952", "submitter": "Raphael F\\'eraud", "authors": "Rapha\\\"el F\\'eraud and Robin Allesiardo and Tanguy Urvoy and Fabrice\n  Cl\\'erot", "title": "Random Forest for the Contextual Bandit Problem - extended version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the contextual bandit problem, we propose an online random forest\nalgorithm. The analysis of the proposed algorithm is based on the sample\ncomplexity needed to find the optimal decision stump. Then, the decision stumps\nare assembled in a random collection of decision trees, Bandit Forest. We show\nthat the proposed algorithm is optimal up to logarithmic factors. The\ndependence of the sample complexity upon the number of contextual variables is\nlogarithmic. The computational cost of the proposed algorithm with respect to\nthe time horizon is linear. These analytical results allow the proposed\nalgorithm to be efficient in real applications, where the number of events to\nprocess is huge, and where we expect that some contextual variables, chosen\nfrom a large set, have potentially non- linear dependencies with the rewards.\nIn the experiments done to illustrate the theoretical analysis, Bandit Forest\nobtain promising results in comparison with state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 07:27:10 GMT"}, {"version": "v10", "created": "Thu, 25 Jun 2015 09:46:07 GMT"}, {"version": "v11", "created": "Wed, 2 Sep 2015 15:54:35 GMT"}, {"version": "v12", "created": "Thu, 3 Sep 2015 08:23:29 GMT"}, {"version": "v13", "created": "Tue, 22 Sep 2015 14:42:51 GMT"}, {"version": "v14", "created": "Wed, 23 Sep 2015 12:37:44 GMT"}, {"version": "v15", "created": "Mon, 28 Sep 2015 17:21:16 GMT"}, {"version": "v16", "created": "Tue, 29 Sep 2015 13:59:09 GMT"}, {"version": "v17", "created": "Thu, 17 Dec 2015 08:53:23 GMT"}, {"version": "v18", "created": "Mon, 11 Jan 2016 10:58:35 GMT"}, {"version": "v19", "created": "Sun, 31 Jan 2016 19:36:18 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 09:24:33 GMT"}, {"version": "v20", "created": "Mon, 20 Jun 2016 08:09:34 GMT"}, {"version": "v21", "created": "Thu, 15 Sep 2016 08:41:21 GMT"}, {"version": "v3", "created": "Sun, 3 May 2015 17:54:09 GMT"}, {"version": "v4", "created": "Tue, 5 May 2015 18:44:41 GMT"}, {"version": "v5", "created": "Fri, 8 May 2015 09:23:38 GMT"}, {"version": "v6", "created": "Mon, 11 May 2015 10:02:10 GMT"}, {"version": "v7", "created": "Tue, 19 May 2015 09:18:23 GMT"}, {"version": "v8", "created": "Thu, 21 May 2015 12:55:35 GMT"}, {"version": "v9", "created": "Tue, 23 Jun 2015 09:39:06 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["F\u00e9raud", "Rapha\u00ebl", ""], ["Allesiardo", "Robin", ""], ["Urvoy", "Tanguy", ""], ["Cl\u00e9rot", "Fabrice", ""]]}, {"id": "1504.07000", "submitter": "Nikolaos Gkalelis", "authors": "Nikolaos Gkalelis and Vasileios Mezaris", "title": "Accelerated kernel discriminant analysis", "comments": "14 pages, journal, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, using a novel matrix factorization and simultaneous reduction\nto diagonal form approach (or in short simultaneous reduction approach),\nAccelerated Kernel Discriminant Analysis (AKDA) and Accelerated Kernel Subclass\nDiscriminant Analysis (AKSDA) are proposed. Specifically, instead of performing\nthe simultaneous reduction of the between- and within-class or subclass scatter\nmatrices, the nonzero eigenpairs (NZEP) of the so-called core matrix, which is\nof relatively small dimensionality, and the Cholesky factorization of the\nkernel matrix are computed, achieving more than one order of magnitude speed up\nover kernel discriminant analysis (KDA). Moreover, consisting of a few\nelementary matrix operations and very stable numerical algorithms, AKDA and\nAKSDA offer improved classification accuracy. The experimental evaluation on\nvarious datasets confirms that the proposed approaches provide state-of-the-art\nperformance in terms of both training time and classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 09:41:36 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 07:38:46 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gkalelis", "Nikolaos", ""], ["Mezaris", "Vasileios", ""]]}, {"id": "1504.07004", "submitter": "Moitreya Chatterjee", "authors": "Moitreya Chatterjee and Anton Leuski", "title": "An Active Learning Based Approach For Effective Video Annotation And\n  Retrieval", "comments": "5 pages, 3 figures, Compressed version published at ACM ICMR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional multimedia annotation/retrieval systems such as Normalized\nContinuous Relevance Model (NormCRM) [16] require a fully labeled training data\nfor a good performance. Active Learning, by determining an order for labeling\nthe training data, allows for a good performance even before the training data\nis fully annotated. In this work we propose an active learning algorithm, which\ncombines a novel measure of sample uncertainty with a novel clustering-based\napproach for determining sample density and diversity and integrate it with\nNormCRM. The clusters are also iteratively refined to ensure both feature and\nlabel-level agreement among samples. We show that our approach outperforms\nmultiple baselines both on a recent, open character animation dataset and on\nthe popular TRECVID corpus at both the tasks of annotation and text-based\nretrieval of videos.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 09:44:30 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Chatterjee", "Moitreya", ""], ["Leuski", "Anton", ""]]}, {"id": "1504.07107", "submitter": "Wenbo Hu", "authors": "Wenbo Hu, Jun Zhu, Bo Zhang", "title": "Fast Sampling for Bayesian Max-Margin Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian max-margin models have shown superiority in various practical\napplications, such as text categorization, collaborative prediction, social\nnetwork link prediction and crowdsourcing, and they conjoin the flexibility of\nBayesian modeling and predictive strengths of max-margin learning. However,\nMonte Carlo sampling for these models still remains challenging, especially for\napplications that involve large-scale datasets. In this paper, we present the\nstochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to\nimplement and computationally efficient. We show the approximate detailed\nbalance property of subgradient HMC which reveals a natural and validated\ngeneralization of the ordinary HMC. Furthermore, we investigate the variants\nthat use stochastic subsampling and thermostats for better scalability and\nmixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we\nefficiently solve the posterior inference task of various Bayesian max-margin\nmodels and extensive experimental results demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 14:29:40 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 12:28:41 GMT"}, {"version": "v3", "created": "Sat, 9 May 2015 07:26:02 GMT"}, {"version": "v4", "created": "Sat, 20 Jun 2015 12:53:35 GMT"}, {"version": "v5", "created": "Tue, 18 Oct 2016 13:44:30 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Hu", "Wenbo", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1504.07116", "submitter": "Kevin Moon", "authors": "Kevin R. Moon, Veronique Delouille, Alfred O. Hero III", "title": "Meta learning of bounds on the Bayes classifier error", "comments": "6 pages, 3 figures, to appear in proceedings of 2015 IEEE Signal\n  Processing and SP Education Workshop", "journal-ref": "IEEE Signal Processing and SP Education Workshop, pp. 13-18, Aug.\n  2015", "doi": "10.1109/DSP-SPE.2015.7369520", "report-no": null, "categories": "cs.LG astro-ph.SR cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta learning uses information from base learners (e.g. classifiers or\nestimators) as well as information about the learning problem to improve upon\nthe performance of a single base learner. For example, the Bayes error rate of\na given feature space, if known, can be used to aid in choosing a classifier,\nas well as in feature selection and model selection for the base classifiers\nand the meta classifier. Recent work in the field of f-divergence functional\nestimation has led to the development of simple and rapidly converging\nestimators that can be used to estimate various bounds on the Bayes error. We\nestimate multiple bounds on the Bayes error using an estimator that applies\nmeta learning to slowly converging plug-in estimators to obtain the parametric\nconvergence rate. We compare the estimated bounds empirically on simulated data\nand then estimate the tighter bounds on features extracted from an image patch\nanalysis of sunspot continuum and magnetogram images.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 14:49:24 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 17:34:13 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Moon", "Kevin R.", ""], ["Delouille", "Veronique", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1504.07218", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Changho Suh", "title": "Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons", "comments": "accepted to International Conference on Machine Learning (ICML), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the preference-based top-$K$ rank aggregation problem.\nSuppose that a collection of items is repeatedly compared in pairs, and one\nwishes to recover a consistent ordering that emphasizes the top-$K$ ranked\nitems, based on partially revealed preferences. We focus on the\nBradley-Terry-Luce (BTL) model that postulates a set of latent preference\nscores underlying all items, where the odds of paired comparisons depend only\non the relative scores of the items involved.\n  We characterize the minimax limits on identifiability of top-$K$ ranked\nitems, in the presence of random and non-adaptive sampling. Our results\nhighlight a separation measure that quantifies the gap of preference scores\nbetween the $K^{\\text{th}}$ and $(K+1)^{\\text{th}}$ ranked items. The minimum\nsample complexity required for reliable top-$K$ ranking scales inversely with\nthe separation measure irrespective of other preference distribution metrics.\nTo approach this minimax limit, we propose a nearly linear-time ranking scheme,\ncalled \\emph{Spectral MLE}, that returns the indices of the top-$K$ items in\naccordance to a careful score estimate. In a nutshell, Spectral MLE starts with\nan initial score estimate with minimal squared loss (obtained via a spectral\nmethod), and then successively refines each component with the assistance of\ncoordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ item\nidentification under minimal sample complexity. The practical applicability of\nSpectral MLE is further corroborated by numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:30:01 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 06:04:15 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Chen", "Yuxin", ""], ["Suh", "Changho", ""]]}, {"id": "1504.07225", "submitter": "Sarath Chandar", "authors": "Sarath Chandar, Mitesh M. Khapra, Hugo Larochelle, Balaraman Ravindran", "title": "Correlational Neural Networks", "comments": "27 pages. To Appear in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common Representation Learning (CRL), wherein different descriptions (or\nviews) of the data are embedded in a common subspace, is receiving a lot of\nattention recently. Two popular paradigms here are Canonical Correlation\nAnalysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA\nbased approaches learn a joint representation by maximizing correlation of the\nviews when projected to the common subspace. AE based methods learn a common\nrepresentation by minimizing the error of reconstructing the two views. Each of\nthese approaches has its own advantages and disadvantages. For example, while\nCCA based approaches outperform AE based approaches for the task of transfer\nlearning, they are not as scalable as the latter. In this work we propose an AE\nbased approach called Correlational Neural Network (CorrNet), that explicitly\nmaximizes correlation among the views when projected to the common subspace.\nThrough a series of experiments, we demonstrate that the proposed CorrNet is\nbetter than the above mentioned approaches with respect to its ability to learn\ncorrelated common representations. Further, we employ CorrNet for several cross\nlanguage tasks and show that the representations learned using CorrNet perform\nbetter than the ones learned using other state of the art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:51:34 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 20:34:28 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2015 19:14:05 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Chandar", "Sarath", ""], ["Khapra", "Mitesh M.", ""], ["Larochelle", "Hugo", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1504.07235", "submitter": "Ping Li", "authors": "Ping Li", "title": "Sign Stable Random Projections for Large-Scale Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of \"sign $\\alpha$-stable random projections\" (where\n$0<\\alpha\\leq 2$) for building basic data processing tools in the context of\nlarge-scale machine learning applications (e.g., classification, regression,\nclustering, and near-neighbor search). After the processing by sign stable\nrandom projections, the inner products of the processed data approximate\nvarious types of nonlinear kernels depending on the value of $\\alpha$. Thus,\nthis approach provides an effective strategy for approximating nonlinear\nlearning algorithms essentially at the cost of linear learning. When $\\alpha\n=2$, it is known that the corresponding nonlinear kernel is the arc-cosine\nkernel. When $\\alpha=1$, the procedure approximates the arc-cos-$\\chi^2$ kernel\n(under certain condition). When $\\alpha\\rightarrow0+$, it corresponds to the\nresemblance kernel.\n  From practitioners' perspective, the method of sign $\\alpha$-stable random\nprojections is ready to be tested for large-scale learning applications, where\n$\\alpha$ can be simply viewed as a tuning parameter. What is missing in the\nliterature is an extensive empirical study to show the effectiveness of sign\nstable random projections, especially for $\\alpha\\neq 2$ or 1. The paper\nsupplies such a study on a wide variety of classification datasets. In\nparticular, we compare shoulder-by-shoulder sign stable random projections with\nthe recently proposed \"0-bit consistent weighted sampling (CWS)\" (Li 2015).\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:50:40 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1504.07272", "submitter": "Wojciech Kot{\\l}owski", "authors": "Wojciech Kot{\\l}owski, Krzysztof Dembczy\\'nski", "title": "Surrogate regret bounds for generalized classification performance\n  metrics", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider optimization of generalized performance metrics for binary\nclassification by means of surrogate losses. We focus on a class of metrics,\nwhich are linear-fractional functions of the false positive and false negative\nrates (examples of which include $F_{\\beta}$-measure, Jaccard similarity\ncoefficient, AM measure, and many others). Our analysis concerns the following\ntwo-step procedure. First, a real-valued function $f$ is learned by minimizing\na surrogate loss for binary classification on the training sample. It is\nassumed that the surrogate loss is a strongly proper composite loss function\n(examples of which include logistic loss, squared-error loss, exponential loss,\netc.). Then, given $f$, a threshold $\\widehat{\\theta}$ is tuned on a separate\nvalidation sample, by direct optimization of the target performance metric. We\nshow that the regret of the resulting classifier (obtained from thresholding\n$f$ on $\\widehat{\\theta}$) measured with respect to the target metric is\nupperbounded by the regret of $f$ measured with respect to the surrogate loss.\nWe also extend our results to cover multilabel classification and provide\nregret bounds for micro- and macro-averaging measures. Our findings are further\nanalyzed in a computational study on both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 20:45:47 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 15:31:44 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Kot\u0142owski", "Wojciech", ""], ["Dembczy\u0144ski", "Krzysztof", ""]]}, {"id": "1504.07313", "submitter": "Daniel Aranki", "authors": "Daniel Aranki and Ruzena Bajcsy", "title": "Private Disclosure of Information in Health Tele-monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework, called Private Disclosure of Information (PDI),\nwhich is aimed to prevent an adversary from inferring certain sensitive\ninformation about subjects using the data that they disclosed during\ncommunication with an intended recipient. We show cases where it is possible to\nachieve perfect privacy regardless of the adversary's auxiliary knowledge while\npreserving full utility of the information to the intended recipient and\nprovide sufficient conditions for such cases. We also demonstrate the\napplicability of PDI on a real-world data set that simulates a health\ntele-monitoring scenario.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 00:20:50 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Aranki", "Daniel", ""], ["Bajcsy", "Ruzena", ""]]}, {"id": "1504.07395", "submitter": "Thanh-Le Ha", "authors": "Thanh-Le Ha, Jan Niehues, Alex Waibel", "title": "Lexical Translation Model Using a Deep Neural Network Architecture", "comments": null, "journal-ref": "Proceedings of the 11th International Workshop on Spoken Language\n  Translation (IWSLT 2014), page 223-229, Lake Tahoe - US, December 4th and\n  5th, 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we combine the advantages of a model using global source\nsentence contexts, the Discriminative Word Lexicon, and neural networks. By\nusing deep neural networks instead of the linear maximum entropy model in the\nDiscriminative Word Lexicon models, we are able to leverage dependencies\nbetween different source words due to the non-linearity. Furthermore, the\nmodels for different target words can share parameters and therefore data\nsparsity problems are effectively reduced.\n  By using this approach in a state-of-the-art translation system, we can\nimprove the performance by up to 0.5 BLEU points for three different language\npairs on the TED translation task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 09:43:40 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Ha", "Thanh-Le", ""], ["Niehues", "Jan", ""], ["Waibel", "Alex", ""]]}, {"id": "1504.07550", "submitter": "Soufiane Belharbi", "authors": "Soufiane Belharbi and Romain H\\'erault and Cl\\'ement Chatelain and\n  S\\'ebastien Adam", "title": "Deep Neural Networks Regularization for Structured Output Prediction", "comments": "Submitted to Neurocomputing, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep neural network model is a powerful framework for learning\nrepresentations. Usually, it is used to learn the relation $x \\to y$ by\nexploiting the regularities in the input $x$. In structured output prediction\nproblems, $y$ is multi-dimensional and structural relations often exist between\nthe dimensions. The motivation of this work is to learn the output dependencies\nthat may lie in the output data in order to improve the prediction accuracy.\nUnfortunately, feedforward networks are unable to exploit the relations between\nthe outputs. In order to overcome this issue, we propose in this paper a\nregularization scheme for training neural networks for these particular tasks\nusing a multi-task framework. Our scheme aims at incorporating the learning of\nthe output representation $y$ in the training process in an unsupervised\nfashion while learning the supervised mapping function $x \\to y$.\n  We evaluate our framework on a facial landmark detection problem which is a\ntypical structured output task. We show over two public challenging datasets\n(LFPW and HELEN) that our regularization scheme improves the generalization of\ndeep neural networks and accelerates their training. The use of unlabeled data\nand label-only data is also explored, showing an additional improvement of the\nresults. We provide an opensource implementation\n(https://github.com/sbelharbi/structured-output-ae) of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 16:11:15 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 13:10:53 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 12:30:27 GMT"}, {"version": "v4", "created": "Fri, 18 Nov 2016 15:30:04 GMT"}, {"version": "v5", "created": "Mon, 3 Apr 2017 11:05:23 GMT"}, {"version": "v6", "created": "Mon, 30 Oct 2017 17:00:29 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Belharbi", "Soufiane", ""], ["H\u00e9rault", "Romain", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Adam", "S\u00e9bastien", ""]]}, {"id": "1504.07553", "submitter": "Mark Bun", "authors": "Mark Bun and Kobbi Nissim and Uri Stemmer and Salil Vadhan", "title": "Differentially Private Release and Learning of Threshold Functions", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove new upper and lower bounds on the sample complexity of $(\\epsilon,\n\\delta)$ differentially private algorithms for releasing approximate answers to\nthreshold functions. A threshold function $c_x$ over a totally ordered domain\n$X$ evaluates to $c_x(y) = 1$ if $y \\le x$, and evaluates to $0$ otherwise. We\ngive the first nontrivial lower bound for releasing thresholds with\n$(\\epsilon,\\delta)$ differential privacy, showing that the task is impossible\nover an infinite domain $X$, and moreover requires sample complexity $n \\ge\n\\Omega(\\log^*|X|)$, which grows with the size of the domain. Inspired by the\ntechniques used to prove this lower bound, we give an algorithm for releasing\nthresholds with $n \\le 2^{(1+ o(1))\\log^*|X|}$ samples. This improves the\nprevious best upper bound of $8^{(1 + o(1))\\log^*|X|}$ (Beimel et al., RANDOM\n'13).\n  Our sample complexity upper and lower bounds also apply to the tasks of\nlearning distributions with respect to Kolmogorov distance and of properly PAC\nlearning thresholds with differential privacy. The lower bound gives the first\nseparation between the sample complexity of properly learning a concept class\nwith $(\\epsilon,\\delta)$ differential privacy and learning without privacy. For\nproperly learning thresholds in $\\ell$ dimensions, this lower bound extends to\n$n \\ge \\Omega(\\ell \\cdot \\log^*|X|)$.\n  To obtain our results, we give reductions in both directions from releasing\nand properly learning thresholds and the simpler interior point problem. Given\na database $D$ of elements from $X$, the interior point problem asks for an\nelement between the smallest and largest elements in $D$. We introduce new\nrecursive constructions for bounding the sample complexity of the interior\npoint problem, as well as further reductions and techniques for proving\nimpossibility results for other basic problems in differential privacy.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 16:15:01 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Bun", "Mark", ""], ["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""], ["Vadhan", "Salil", ""]]}, {"id": "1504.07575", "submitter": "Oisin Mac Aodha", "authors": "Edward Johns and Oisin Mac Aodha and Gabriel J. Brostow", "title": "Becoming the Expert - Interactive Multi-Class Machine Teaching", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to machines, humans are extremely good at classifying images into\ncategories, especially when they possess prior knowledge of the categories at\nhand. If this prior information is not available, supervision in the form of\nteaching images is required. To learn categories more quickly, people should\nsee important and representative images first, followed by less important\nimages later - or not at all. However, image-importance is individual-specific,\ni.e. a teaching image is important to a student if it changes their overall\nability to discriminate between classes. Further, students keep learning, so\nwhile image-importance depends on their current knowledge, it also varies with\ntime.\n  In this work we propose an Interactive Machine Teaching algorithm that\nenables a computer to teach challenging visual concepts to a human. Our\nadaptive algorithm chooses, online, which labeled images from a teaching set\nshould be shown to the student as they learn. We show that a teaching strategy\nthat probabilistically models the student's ability and progress, based on\ntheir correct and incorrect answers, produces better 'experts'. We present\nresults using real human participants across several varied and challenging\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 17:22:29 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Johns", "Edward", ""], ["Mac Aodha", "Oisin", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1504.07614", "submitter": "Tong Wang", "authors": "Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica\n  Klampfl, Perry MacNeille", "title": "Or's of And's for Interpretable Classification, with Application to\n  Context-Aware Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning algorithm for building classifiers that are\ncomprised of a small number of disjunctions of conjunctions (or's of and's). An\nexample of a classifier of this form is as follows: If X satisfies (x1 = 'blue'\nAND x3 = 'middle') OR (x1 = 'blue' AND x2 = '<15') OR (x1 = 'yellow'), then we\npredict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literal\nand a conjunction of literals is called a pattern. Models of this form have the\nadvantage of being interpretable to human experts, since they produce a set of\nconditions that concisely describe a specific class. We present two\nprobabilistic models for forming a pattern set, one with a Beta-Binomial prior,\nand the other with Poisson priors. In both cases, there are prior parameters\nthat the user can set to encourage the model to have a desired size and shape,\nto conform with a domain-specific definition of interpretability. We provide\ntwo scalable MAP inference approaches: a pattern level search, which involves\nassociation rule mining, and a literal level search. We show stronger priors\nreduce computation. We apply the Bayesian Or's of And's (BOA) model to predict\nuser behavior with respect to in-vehicle context-aware personalized recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 19:53:59 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Wang", "Tong", ""], ["Rudin", "Cynthia", ""], ["Doshi-Velez", "Finale", ""], ["Liu", "Yimin", ""], ["Klampfl", "Erica", ""], ["MacNeille", "Perry", ""]]}, {"id": "1504.07648", "submitter": "Mahdi Cheraghchi", "authors": "Mahdi Cheraghchi, Piotr Indyk", "title": "Nearly Optimal Deterministic Algorithm for Sparse Walsh-Hadamard\n  Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.LG math.FA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every fixed constant $\\alpha > 0$, we design an algorithm for computing\nthe $k$-sparse Walsh-Hadamard transform of an $N$-dimensional vector $x \\in\n\\mathbb{R}^N$ in time $k^{1+\\alpha} (\\log N)^{O(1)}$. Specifically, the\nalgorithm is given query access to $x$ and computes a $k$-sparse $\\tilde{x} \\in\n\\mathbb{R}^N$ satisfying $\\|\\tilde{x} - \\hat{x}\\|_1 \\leq c \\|\\hat{x} -\nH_k(\\hat{x})\\|_1$, for an absolute constant $c > 0$, where $\\hat{x}$ is the\ntransform of $x$ and $H_k(\\hat{x})$ is its best $k$-sparse approximation. Our\nalgorithm is fully deterministic and only uses non-adaptive queries to $x$\n(i.e., all queries are determined and performed in parallel when the algorithm\nstarts).\n  An important technical tool that we use is a construction of nearly optimal\nand linear lossless condensers which is a careful instantiation of the GUV\ncondenser (Guruswami, Umans, Vadhan, JACM 2009). Moreover, we design a\ndeterministic and non-adaptive $\\ell_1/\\ell_1$ compressed sensing scheme based\non general lossless condensers that is equipped with a fast reconstruction\nalgorithm running in time $k^{1+\\alpha} (\\log N)^{O(1)}$ (for the GUV-based\ncondenser) and is of independent interest. Our scheme significantly simplifies\nand improves an earlier expander-based construction due to Berinde, Gilbert,\nIndyk, Karloff, Strauss (Allerton 2008).\n  Our methods use linear lossless condensers in a black box fashion; therefore,\nany future improvement on explicit constructions of such condensers would\nimmediately translate to improved parameters in our framework (potentially\nleading to $k (\\log N)^{O(1)}$ reconstruction time with a reduced exponent in\nthe poly-logarithmic factor, and eliminating the extra parameter $\\alpha$).\n  Finally, by allowing the algorithm to use randomness, while still using\nnon-adaptive queries, the running time of the algorithm can be improved to\n$\\tilde{O}(k \\log^3 N)$.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 20:22:27 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Cheraghchi", "Mahdi", ""], ["Indyk", "Piotr", ""]]}, {"id": "1504.07662", "submitter": "Dragomir Yankov", "authors": "Dragomir Yankov, Pavel Berkhin, Lihong Li", "title": "Evaluation of Explore-Exploit Policies in Multi-result Ranking Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the problem of using Explore-Exploit techniques to improve\nprecision in multi-result ranking systems such as web search, query\nautocompletion and news recommendation. Adopting an exploration policy directly\nonline, without understanding its impact on the production system, may have\nunwanted consequences - the system may sustain large losses, create user\ndissatisfaction, or collect exploration data which does not help improve\nranking quality. An offline framework is thus necessary to let us decide what\npolicy and how we should apply in a production environment to ensure positive\noutcome. Here, we describe such an offline framework.\n  Using the framework, we study a popular exploration policy - Thompson\nsampling. We show that there are different ways of implementing it in\nmulti-result ranking systems, each having different semantic interpretation and\nleading to different results in terms of sustained click-through-rate (CTR)\nloss and expected model improvement. In particular, we demonstrate that\nThompson sampling can act as an online learner optimizing CTR, which in some\ncases can lead to an interesting outcome: lift in CTR during exploration. The\nobservation is important for production systems as it suggests that one can get\nboth valuable exploration data to improve ranking performance on the long run,\nand at the same time increase CTR while exploration lasts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 21:16:07 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Yankov", "Dragomir", ""], ["Berkhin", "Pavel", ""], ["Li", "Lihong", ""]]}, {"id": "1504.07676", "submitter": "Matt Olson", "authors": "Abraham J. Wyner, Matthew Olson, Justin Bleich, David Mease", "title": "Explaining the Success of AdaBoost and Random Forests as Interpolating\n  Classifiers", "comments": "40 pages, 11 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large literature explaining why AdaBoost is a successful\nclassifier. The literature on AdaBoost focuses on classifier margins and\nboosting's interpretation as the optimization of an exponential likelihood\nfunction. These existing explanations, however, have been pointed out to be\nincomplete. A random forest is another popular ensemble method for which there\nis substantially less explanation in the literature. We introduce a novel\nperspective on AdaBoost and random forests that proposes that the two\nalgorithms work for similar reasons. While both classifiers achieve similar\npredictive accuracy, random forests cannot be conceived as a direct\noptimization procedure. Rather, random forests is a self-averaging,\ninterpolating algorithm which creates what we denote as a \"spikey-smooth\"\nclassifier, and we view AdaBoost in the same light. We conjecture that both\nAdaBoost and random forests succeed because of this mechanism. We provide a\nnumber of examples and some theoretical justification to support this\nexplanation. In the process, we question the conventional wisdom that suggests\nthat boosting algorithms for classification require regularization or early\nstopping and should be limited to low complexity classes of learners, such as\ndecision stumps. We conclude that boosting should be used like random forests:\nwith large decision trees and without direct regularization or early stopping.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 22:34:25 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 23:25:20 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Wyner", "Abraham J.", ""], ["Olson", "Matthew", ""], ["Bleich", "Justin", ""], ["Mease", "David", ""]]}, {"id": "1504.07720", "submitter": "Walid Krichene", "authors": "Walid Krichene", "title": "Dual Averaging on Compactly-Supported Distributions And Application to\n  No-Regret Learning on a Continuum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online learning problem on a continuum. A decision maker is\ngiven a compact feasible set $S$, and is faced with the following sequential\nproblem: at iteration~$t$, the decision maker chooses a distribution $x^{(t)}\n\\in \\Delta(S)$, then a loss function $\\ell^{(t)} : S \\to \\mathbb{R}_+$ is\nrevealed, and the decision maker incurs expected loss $\\langle \\ell^{(t)},\nx^{(t)} \\rangle = \\mathbb{E}_{s \\sim x^{(t)}} \\ell^{(t)}(s)$. We view the\nproblem as an online convex optimization problem on the space $\\Delta(S)$ of\nLebesgue-continnuous distributions on $S$. We prove a general regret bound for\nthe Dual Averaging method on $L^2(S)$, then prove that dual averaging with\n$\\omega$-potentials (a class of strongly convex regularizers) achieves\nsublinear regret when $S$ is uniformly fat (a condition weaker than convexity).\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 04:41:44 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Krichene", "Walid", ""]]}, {"id": "1504.07829", "submitter": "Samuele Soraggi", "authors": "Sara Rebagliati and Emanuela Sasso and Samuele Soraggi", "title": "Market forecasting using Hidden Markov Models", "comments": "This paper has been withdrawn by the author due to many errors and\n  not very precise results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working on the daily closing prices and logreturns, in this paper we deal\nwith the use of Hidden Markov Models (HMMs) to forecast the price of the\nEUR/USD Futures. The aim of our work is to understand how the HMMs describe\ndifferent financial time series depending on their structure. Subsequently, we\nanalyse the forecasting methods exposed in the previous literature, putting on\nevidence their pros and cons.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 12:21:49 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 11:59:23 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Rebagliati", "Sara", ""], ["Sasso", "Emanuela", ""], ["Soraggi", "Samuele", ""]]}, {"id": "1504.07865", "submitter": "Snehanshu Saha", "authors": "Snehanshu Saha, Surbhi Agrawal, Manikandan. R, Kakoli Bora, Swati\n  Routh, Anand Narasimhamurthy", "title": "ASTROMLSKIT: A New Statistical Machine Learning Toolkit: A Platform for\n  Data Analytics in Astronomy", "comments": "Habitability Catalog (HabCat), Supernova classification, data\n  analysis, Astroinformatics, Machine learning, ASTROMLS toolkit, Na\\\"ive\n  Bayes, SVD, PCA, Random Forest, SVM, Decision Tree, LDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astroinformatics is a new impact area in the world of astronomy, occasionally\ncalled the final frontier, where several astrophysicists, statisticians and\ncomputer scientists work together to tackle various data intensive astronomical\nproblems. Exponential growth in the data volume and increased complexity of the\ndata augments difficult questions to the existing challenges. Classical\nproblems in Astronomy are compounded by accumulation of astronomical volume of\ncomplex data, rendering the task of classification and interpretation\nincredibly laborious. The presence of noise in the data makes analysis and\ninterpretation even more arduous. Machine learning algorithms and data analytic\ntechniques provide the right platform for the challenges posed by these\nproblems. A diverse range of open problem like star-galaxy separation,\ndetection and classification of exoplanets, classification of supernovae is\ndiscussed. The focus of the paper is the applicability and efficacy of various\nmachine learning algorithms like K Nearest Neighbor (KNN), random forest (RF),\ndecision tree (DT), Support Vector Machine (SVM), Na\\\"ive Bayes and Linear\nDiscriminant Analysis (LDA) in analysis and inference of the decision theoretic\nproblems in Astronomy. The machine learning algorithms, integrated into\nASTROMLSKIT, a toolkit developed in the course of the work, have been used to\nanalyze HabCat data and supernovae data. Accuracy has been found to be\nappreciably good.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 14:06:18 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Saha", "Snehanshu", ""], ["Agrawal", "Surbhi", ""], ["R", "Manikandan.", ""], ["Bora", "Kakoli", ""], ["Routh", "Swati", ""], ["Narasimhamurthy", "Anand", ""]]}, {"id": "1504.07968", "submitter": "Ubai Sandouk", "authors": "Ubai Sandouk and Ke Chen", "title": "Learning Contextualized Music Semantics from Tags via a Siamese Network", "comments": "20 pages. To appear in ACM TIST: Intelligent Music Systems and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music information retrieval faces a challenge in modeling contextualized\nmusical concepts formulated by a set of co-occurring tags. In this paper, we\ninvestigate the suitability of our recently proposed approach based on a\nSiamese neural network in fighting off this challenge. By means of tag features\nand probabilistic topic models, the network captures contextualized semantics\nfrom tags via unsupervised learning. This leads to a distributed semantics\nspace and a potential solution to the out of vocabulary problem which has yet\nto be sufficiently addressed. We explore the nature of the resultant\nmusic-based semantics and address computational needs. We conduct experiments\non three public music tag collections -namely, CAL500, MagTag5K and Million\nSong Dataset- and compare our approach to a number of state-of-the-art\nsemantics learning approaches. Comparative results suggest that this approach\noutperforms previous approaches in terms of semantic priming and music tag\ncompletion.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 19:05:06 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 16:46:27 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Sandouk", "Ubai", ""], ["Chen", "Ke", ""]]}, {"id": "1504.08021", "submitter": "Harshavardhan Sundar Mr.", "authors": "Harshavardhan Sundar and Thippur V. Sreenivas", "title": "Who Spoke What? A Latent Variable Framework for the Joint Decoding of\n  Multiple Speakers and their Keywords", "comments": "6 pages, 2 figures Submitted to : IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a latent variable (LV) framework to identify all\nthe speakers and their keywords given a multi-speaker mixture signal. We\nintroduce two separate LVs to denote active speakers and the keywords uttered.\nThe dependency of a spoken keyword on the speaker is modeled through a\nconditional probability mass function. The distribution of the mixture signal\nis expressed in terms of the LV mass functions and speaker-specific-keyword\nmodels. The proposed framework admits stochastic models, representing the\nprobability density function of the observation vectors given that a particular\nspeaker uttered a specific keyword, as speaker-specific-keyword models. The LV\nmass functions are estimated in a Maximum Likelihood framework using the\nExpectation Maximization (EM) algorithm. The active speakers and their keywords\nare detected as modes of the joint distribution of the two LVs. In mixture\nsignals, containing two speakers uttering the keywords simultaneously, the\nproposed framework achieves an accuracy of 82% for detecting both the speakers\nand their respective keywords, using Student's-t mixture models as\nspeaker-specific-keyword models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 20:56:42 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Sundar", "Harshavardhan", ""], ["Sreenivas", "Thippur V.", ""]]}, {"id": "1504.08022", "submitter": "Hongyu Guo Ph.D", "authors": "Hongyu Guo, Xiaodan Zhu, Martin Renqiang Min", "title": "A Deep Learning Model for Structured Outputs with High-order Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications are associated with structured data, where not\nonly input but also output has interplay. However, typical classification and\nregression models often lack the ability of simultaneously exploring high-order\ninteraction within input and that within output. In this paper, we present a\ndeep learning model aiming to generate a powerful nonlinear functional mapping\nfrom structured input to structured output. More specifically, we propose to\nintegrate high-order hidden units, guided discriminative pretraining, and\nhigh-order auto-encoders for this purpose. We evaluate the model with three\ndatasets, and obtain state-of-the-art performances among competitive methods.\nOur current work focuses on structured output regression, which is a less\nexplored area, although the model can be extended to handle structured label\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 20:58:52 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Guo", "Hongyu", ""], ["Zhu", "Xiaodan", ""], ["Min", "Martin Renqiang", ""]]}, {"id": "1504.08025", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein, Diederik P. Kingma", "title": "Note on Equivalence Between Recurrent Neural Network Time Series Models\n  and Variational Bayesian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe that the standard log likelihood training objective for a\nRecurrent Neural Network (RNN) model of time series data is equivalent to a\nvariational Bayesian training objective, given the proper choice of generative\nand inference models. This perspective may motivate extensions to both RNNs and\nvariational Bayesian models. We propose one such extension, where multiple\nparticles are used for the hidden state of an RNN, allowing a natural\nrepresentation of uncertainty or multimodality.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 21:08:52 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 22:38:46 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""], ["Kingma", "Diederik P.", ""]]}, {"id": "1504.08142", "submitter": "Haiping Lu", "authors": "Qiquan Shi and Haiping Lu", "title": "Semi-Orthogonal Multilinear PCA with Relaxed Start", "comments": "8 pages, 2 figures, to appear in Proceedings of the 24th\n  International Joint Conference on Artificial Intelligence (IJCAI 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is an unsupervised method for learning\nlow-dimensional features with orthogonal projections. Multilinear PCA methods\nextend PCA to deal with multidimensional data (tensors) directly via\ntensor-to-tensor projection or tensor-to-vector projection (TVP). However,\nunder the TVP setting, it is difficult to develop an effective multilinear PCA\nmethod with the orthogonality constraint. This paper tackles this problem by\nproposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA\nlearns low-dimensional features directly from tensors via TVP by imposing the\northogonality constraint in only one mode. This formulation results in more\ncaptured variance and more learned features than full orthogonality. For better\ngeneralization, we further introduce a relaxed start (RS) strategy to get\nSO-MPCA-RS by fixing the starting projection vectors, which increases the bias\nand reduces the variance of the learning model. Experiments on both face (2D)\nand gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing\nalgorithms on the whole, and the relaxed start strategy is also effective for\nother TVP-based PCA methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 09:40:09 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 01:40:27 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Shi", "Qiquan", ""], ["Lu", "Haiping", ""]]}, {"id": "1504.08167", "submitter": "Orly Avner", "authors": "Orly Avner and Shie Mannor", "title": "Multi-user lax communications: a multi-armed bandit approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by cognitive radio networks, we consider a setting where multiple\nusers share several channels modeled as a multi-user multi-armed bandit (MAB)\nproblem. The characteristics of each channel are unknown and are different for\neach user. Each user can choose between the channels, but her success depends\non the particular channel chosen as well as on the selections of other users:\nif two users select the same channel their messages collide and none of them\nmanages to send any data. Our setting is fully distributed, so there is no\ncentral control. As in many communication systems, the users cannot set up a\ndirect communication protocol, so information exchange must be limited to a\nminimum. We develop an algorithm for learning a stable configuration for the\nmulti-user MAB problem. We further offer both convergence guarantees and\nexperiments inspired by real communication networks, including comparison to\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 11:11:54 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 08:50:37 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Avner", "Orly", ""], ["Mannor", "Shie", ""]]}, {"id": "1504.08168", "submitter": "Jan \\v{Z}egklitz", "authors": "Jan \\v{Z}egklitz and Petr Po\\v{s}\\'ik", "title": "Model Selection and Overfitting in Genetic Programming: Empirical Study\n  [Extended Version]", "comments": "8 pages, 12 figures, full paper for GECCO 2015 (accepted as poster,\n  this is the original paper submitted to the conference); added subtitle and\n  removed copyright text at the first page, fixed some typography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Programming has been very successful in solving a large area of\nproblems but its use as a machine learning algorithm has been limited so far.\nOne of the reasons is the problem of overfitting which cannot be solved or\nsuppresed as easily as in more traditional approaches. Another problem, closely\nrelated to overfitting, is the selection of the final model from the\npopulation.\n  In this article we present our research that addresses both problems:\noverfitting and model selection. We compare several ways of dealing with\novefitting, based on Random Sampling Technique (RST) and on using a validation\nset, all with an emphasis on model selection. We subject each approach to a\nthorough testing on artificial and real--world datasets and compare them with\nthe standard approach, which uses the full training data, as a baseline.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 11:12:52 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 14:29:34 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["\u017degklitz", "Jan", ""], ["Po\u0161\u00edk", "Petr", ""]]}, {"id": "1504.08215", "submitter": "Tapani Raiko", "authors": "Antti Rasmus, Harri Valpola, Tapani Raiko", "title": "Lateral Connections in Denoising Autoencoders Support Supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how a deep denoising autoencoder with lateral connections can be used\nas an auxiliary unsupervised learning task to support supervised learning. The\nproposed model is trained to minimize simultaneously the sum of supervised and\nunsupervised cost functions by back-propagation, avoiding the need for\nlayer-wise pretraining. It improves the state of the art significantly in the\npermutation-invariant MNIST classification task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 13:26:46 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Rasmus", "Antti", ""], ["Valpola", "Harri", ""], ["Raiko", "Tapani", ""]]}, {"id": "1504.08219", "submitter": "Oisin Mac Aodha", "authors": "Oisin Mac Aodha and Neill D.F. Campbell and Jan Kautz and Gabriel J.\n  Brostow", "title": "Hierarchical Subquery Evaluation for Active Learning on a Graph", "comments": "CVPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train good supervised and semi-supervised object classifiers, it is\ncritical that we not waste the time of the human experts who are providing the\ntraining labels. Existing active learning strategies can have uneven\nperformance, being efficient on some datasets but wasteful on others, or\ninconsistent just between runs on the same dataset. We propose perplexity based\ngraph construction and a new hierarchical subquery evaluation algorithm to\ncombat this variability, and to release the potential of Expected Error\nReduction.\n  Under some specific circumstances, Expected Error Reduction has been one of\nthe strongest-performing informativeness criteria for active learning. Until\nnow, it has also been prohibitively costly to compute for sizeable datasets. We\ndemonstrate our highly practical algorithm, comparing it to other active\nlearning measures on classification datasets that vary in sparsity,\ndimensionality, and size. Our algorithm is consistent over multiple runs and\nachieves high accuracy, while querying the human expert for labels at a\nfrequency that matches their desired time budget.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 13:35:59 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Mac Aodha", "Oisin", ""], ["Campbell", "Neill D. F.", ""], ["Kautz", "Jan", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1504.08291", "submitter": "Raja Giryes", "authors": "Raja Giryes and Guillermo Sapiro and Alex M. Bronstein", "title": "Deep Neural Networks with Random Gaussian Weights: A Universal\n  Classification Strategy?", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": "10.1109/TSP.2016.2546221", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three important properties of a classification machinery are: (i) the system\npreserves the core information of the input data; (ii) the training examples\nconvey information about unseen data; and (iii) the system is able to treat\ndifferently points from different classes. In this work we show that these\nfundamental properties are satisfied by the architecture of deep neural\nnetworks. We formally prove that these networks with random Gaussian weights\nperform a distance-preserving embedding of the data, with a special treatment\nfor in-class and out-of-class data. Similar points at the input of the network\nare likely to have a similar output. The theoretical analysis of deep networks\nhere presented exploits tools used in the compressed sensing and dictionary\nlearning literature, thereby making a formal connection between these important\ntopics. The derived results allow drawing conclusions on the metric learning\nproperties of the network and their relation to its structure, as well as\nproviding bounds on the required size of the training set such that the\ntraining examples would represent faithfully the unseen data. The results are\nvalidated with state-of-the-art trained networks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 16:14:52 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 11:30:51 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 13:53:11 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2016 19:25:05 GMT"}, {"version": "v5", "created": "Mon, 14 Mar 2016 19:17:08 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1504.08363", "submitter": "Gautam Kamath", "authors": "Constantinos Daskalakis and Gautam Kamath and Christos Tzamos", "title": "On the Structure, Covering, and Learning of Poisson Multinomial\n  Distributions", "comments": "49 pages, extended abstract appeared in FOCS 2015", "journal-ref": null, "doi": "10.1109/FOCS.2015.77", "report-no": null, "categories": "cs.DS cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the\nsum of $n$ independent random vectors supported on the set ${\\cal\nB}_k=\\{e_1,\\ldots,e_k\\}$ of standard basis vectors in $\\mathbb{R}^k$. We prove\na structural characterization of these distributions, showing that, for all\n$\\varepsilon >0$, any $(n, k)$-Poisson multinomial random vector is\n$\\varepsilon$-close, in total variation distance, to the sum of a discretized\nmultidimensional Gaussian and an independent $(\\text{poly}(k/\\varepsilon),\nk)$-Poisson multinomial random vector. Our structural characterization extends\nthe multi-dimensional CLT of Valiant and Valiant, by simultaneously applying to\nall approximation requirements $\\varepsilon$. In particular, it overcomes\nfactors depending on $\\log n$ and, importantly, the minimum eigenvalue of the\nPMD's covariance matrix from the distance to a multidimensional Gaussian random\nvariable.\n  We use our structural characterization to obtain an $\\varepsilon$-cover, in\ntotal variation distance, of the set of all $(n, k)$-PMDs, significantly\nimproving the cover size of Daskalakis and Papadimitriou, and obtaining the\nsame qualitative dependence of the cover size on $n$ and $\\varepsilon$ as the\n$k=2$ cover of Daskalakis and Papadimitriou. We further exploit this structure\nto show that $(n,k)$-PMDs can be learned to within $\\varepsilon$ in total\nvariation distance from $\\tilde{O}_k(1/\\varepsilon^2)$ samples, which is\nnear-optimal in terms of dependence on $\\varepsilon$ and independent of $n$. In\nparticular, our result generalizes the single-dimensional result of Daskalakis,\nDiakonikolas, and Servedio for Poisson Binomials to arbitrary dimension.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 19:53:03 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 00:04:51 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 20:58:00 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Kamath", "Gautam", ""], ["Tzamos", "Christos", ""]]}]