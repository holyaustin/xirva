[{"id": "1008.0336", "submitter": "Rahul  Dwivedi Mr.", "authors": "Ankit Garg, Rahul Dwivedi, Krishna Asawa", "title": "Close Clustering Based Automated Color Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image-search approaches today are based on the text based tags\nassociated with the images which are mostly human generated and are subject to\nvarious kinds of errors. The results of a query to the image database thus can\noften be misleading and may not satisfy the requirements of the user. In this\nwork we propose our approach to automate this tagging process of images, where\nimage results generated can be fine filtered based on a probabilistic tagging\nmechanism. We implement a tool which helps to automate the tagging process by\nmaintaining a training database, wherein the system is trained to identify\ncertain set of input images, the results generated from which are used to\ncreate a probabilistic tagging mechanism. Given a certain set of segments in an\nimage it calculates the probability of presence of particular keywords. This\nprobability table is further used to generate the candidate tags for input\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 2 Aug 2010 16:30:02 GMT"}], "update_date": "2010-08-03", "authors_parsed": [["Garg", "Ankit", ""], ["Dwivedi", "Rahul", ""], ["Asawa", "Krishna", ""]]}, {"id": "1008.0528", "submitter": "Carsten Wiuf", "authors": "Georgiana Ifrim and Carsten Wiuf", "title": "Bounded Coordinate-Descent for Biological Sequence Classification in\n  High Dimensional Predictor Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for discriminative sequence classification where the\nlearner works directly in the high dimensional predictor space of all\nsubsequences in the training set. This is possible by employing a new\ncoordinate-descent algorithm coupled with bounding the magnitude of the\ngradient for selecting discriminative subsequences fast. We characterize the\nloss functions for which our generic learning algorithm can be applied and\npresent concrete implementations for logistic regression (binomial\nlog-likelihood loss) and support vector machines (squared hinge loss).\nApplication of our algorithm to protein remote homology detection and remote\nfold recognition results in performance comparable to that of state-of-the-art\nmethods (e.g., kernel support vector machines). Unlike state-of-the-art\nclassifiers, the resulting classification models are simply lists of weighted\ndiscriminative subsequences and can thus be interpreted and related to the\nbiological problem.\n", "versions": [{"version": "v1", "created": "Tue, 3 Aug 2010 12:10:40 GMT"}], "update_date": "2010-08-04", "authors_parsed": [["Ifrim", "Georgiana", ""], ["Wiuf", "Carsten", ""]]}, {"id": "1008.1398", "submitter": "Christian Walder Dr", "authors": "Christian Walder, Ricardo Henao, Morten M{\\o}rup, Lars Kai Hansen", "title": "Semi-Supervised Kernel PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present three generalisations of Kernel Principal Components Analysis\n(KPCA) which incorporate knowledge of the class labels of a subset of the data\npoints. The first, MV-KPCA, penalises within class variances similar to Fisher\ndiscriminant analysis. The second, LSKPCA is a hybrid of least squares\nregression and kernel PCA. The final LR-KPCA is an iteratively reweighted\nversion of the previous which achieves a sigmoid loss function on the labeled\npoints. We provide a theoretical risk bound as well as illustrative experiments\non real and toy data sets.\n", "versions": [{"version": "v1", "created": "Sun, 8 Aug 2010 11:25:12 GMT"}], "update_date": "2010-08-10", "authors_parsed": [["Walder", "Christian", ""], ["Henao", "Ricardo", ""], ["M\u00f8rup", "Morten", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1008.1566", "submitter": "Zhemin Zhu", "authors": "Zhemin Zhu, Djoerd Hiemstra, Peter Apers, Andreas Wombacher", "title": "Separate Training for Conditional Random Fields Using Co-occurrence Rate\n  Factorization", "comments": "10pages", "journal-ref": null, "doi": null, "report-no": "TR-CTIT-12-29", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard training method of Conditional Random Fields (CRFs) is very slow\nfor large-scale applications. As an alternative, piecewise training divides the\nfull graph into pieces, trains them independently, and combines the learned\nweights at test time. In this paper, we present \\emph{separate} training for\nundirected models based on the novel Co-occurrence Rate Factorization (CR-F).\nSeparate training is a local training method. In contrast to MEMMs, separate\ntraining is unaffected by the label bias problem. Experiments show that\nseparate training (i) is unaffected by the label bias problem; (ii) reduces the\ntraining time from weeks to seconds; and (iii) obtains competitive results to\nthe standard and piecewise training on linear-chain CRFs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Aug 2010 19:02:04 GMT"}, {"version": "v2", "created": "Tue, 10 Aug 2010 16:42:50 GMT"}, {"version": "v3", "created": "Tue, 28 Sep 2010 13:58:29 GMT"}, {"version": "v4", "created": "Sun, 1 May 2011 16:40:05 GMT"}, {"version": "v5", "created": "Tue, 4 Dec 2012 09:50:03 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Zhu", "Zhemin", ""], ["Hiemstra", "Djoerd", ""], ["Apers", "Peter", ""], ["Wombacher", "Andreas", ""]]}, {"id": "1008.1643", "submitter": "Ninan Sajeeth Philip", "authors": "Ninan Sajeeth Philip", "title": "A Learning Algorithm based on High School Teaching Wisdom", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A learning algorithm based on primary school teaching and learning is\npresented. The methodology is to continuously evaluate a student and to give\nthem training on the examples for which they repeatedly fail, until, they can\ncorrectly answer all types of questions. This incremental learning procedure\nproduces better learning curves by demanding the student to optimally dedicate\ntheir learning time on the failed examples. When used in machine learning, the\nalgorithm is found to train a machine on a data with maximum variance in the\nfeature space so that the generalization ability of the network improves. The\nalgorithm has interesting applications in data mining, model evaluations and\nrare objects discovery.\n", "versions": [{"version": "v1", "created": "Tue, 10 Aug 2010 07:44:08 GMT"}, {"version": "v2", "created": "Sun, 12 Dec 2010 06:13:31 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Philip", "Ninan Sajeeth", ""]]}, {"id": "1008.2159", "submitter": "Nicholas Harvey", "authors": "Maria-Florina Balcan and Nicholas J. A. Harvey", "title": "Submodular Functions: Learnability, Structure, and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions are discrete functions that model laws of diminishing\nreturns and enjoy numerous algorithmic applications. They have been used in\nmany areas, including combinatorial optimization, machine learning, and\neconomics. In this work we study submodular functions from a learning theoretic\nangle. We provide algorithms for learning submodular functions, as well as\nlower bounds on their learnability. In doing so, we uncover several novel\nstructural results revealing ways in which submodular functions can be both\nsurprisingly structured and surprisingly unstructured. We provide several\nconcrete implications of our work in other domains including algorithmic game\ntheory and combinatorial optimization.\n  At a technical level, this research combines ideas from many areas, including\nlearning theory (distributional learning and PAC-style analyses), combinatorics\nand optimization (matroids and submodular functions), and pseudorandomness\n(lossless expander graphs).\n", "versions": [{"version": "v1", "created": "Thu, 12 Aug 2010 16:15:47 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2012 00:43:13 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2012 02:04:42 GMT"}], "update_date": "2012-08-24", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Harvey", "Nicholas J. A.", ""]]}, {"id": "1008.3043", "submitter": "Karin Schnass", "authors": "Massimo Fornasier, Karin Schnass, Jan Vybiral", "title": "Learning Functions of Few Arbitrary Linear Parameters in High Dimensions", "comments": "31 pages, this version was accepted to Foundations of Computational\n  Mathematics, the final publication will be available on\n  http://www.springerlink.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let us assume that $f$ is a continuous function defined on the unit ball of\n$\\mathbb R^d$, of the form $f(x) = g (A x)$, where $A$ is a $k \\times d$ matrix\nand $g$ is a function of $k$ variables for $k \\ll d$. We are given a budget $m\n\\in \\mathbb N$ of possible point evaluations $f(x_i)$, $i=1,...,m$, of $f$,\nwhich we are allowed to query in order to construct a uniform approximating\nfunction. Under certain smoothness and variation assumptions on the function\n$g$, and an {\\it arbitrary} choice of the matrix $A$, we present in this paper\n  1. a sampling choice of the points $\\{x_i\\}$ drawn at random for each\nfunction approximation;\n  2. algorithms (Algorithm 1 and Algorithm 2) for computing the approximating\nfunction, whose complexity is at most polynomial in the dimension $d$ and in\nthe number $m$ of points.\n  Due to the arbitrariness of $A$, the choice of the sampling points will be\naccording to suitable random distributions and our results hold with\noverwhelming probability. Our approach uses tools taken from the {\\it\ncompressed sensing} framework, recent Chernoff bounds for sums of\npositive-semidefinite matrices, and classical stability bounds for invariant\nsubspaces of singular value decompositions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Aug 2010 08:36:21 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2012 18:52:44 GMT"}], "update_date": "2012-01-18", "authors_parsed": [["Fornasier", "Massimo", ""], ["Schnass", "Karin", ""], ["Vybiral", "Jan", ""]]}, {"id": "1008.3187", "submitter": "Raghu Meka", "authors": "Parikshit Gopalan, Adam Klivans, Raghu Meka", "title": "Polynomial-Time Approximation Schemes for Knapsack and Related Counting\n  Problems using Branching Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic, polynomial-time algorithm for approximately counting\nthe number of {0,1}-solutions to any instance of the knapsack problem. On an\ninstance of length n with total weight W and accuracy parameter eps, our\nalgorithm produces a (1 + eps)-multiplicative approximation in time poly(n,log\nW,1/eps). We also give algorithms with identical guarantees for general integer\nknapsack, the multidimensional knapsack problem (with a constant number of\nconstraints) and for contingency tables (with a constant number of rows).\nPreviously, only randomized approximation schemes were known for these problems\ndue to work by Morris and Sinclair and work by Dyer.\n  Our algorithms work by constructing small-width, read-once branching programs\nfor approximating the underlying solution space under a carefully chosen\ndistribution. As a byproduct of this approach, we obtain new query algorithms\nfor learning functions of k halfspaces with respect to the uniform distribution\non {0,1}^n. The running time of our algorithm is polynomial in the accuracy\nparameter eps. Previously even for the case of k=2, only algorithms with an\nexponential dependence on eps were known.\n", "versions": [{"version": "v1", "created": "Wed, 18 Aug 2010 23:45:28 GMT"}], "update_date": "2010-08-20", "authors_parsed": [["Gopalan", "Parikshit", ""], ["Klivans", "Adam", ""], ["Meka", "Raghu", ""]]}, {"id": "1008.3585", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh", "title": "Ultrametric and Generalized Ultrametric in Computational Logic and in\n  Data Analysis", "comments": "19 pp., 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following a review of metric, ultrametric and generalized ultrametric, we\nreview their application in data analysis. We show how they allow us to explore\nboth geometry and topology of information, starting with measured data. Some\nthemes are then developed based on the use of metric, ultrametric and\ngeneralized ultrametric in logic. In particular we study approximation chains\nin an ultrametric or generalized ultrametric context. Our aim in this work is\nto extend the scope of data analysis by facilitating reasoning based on the\ndata analysis; and to show how quantitative and qualitative data analysis can\nbe incorporated into logic programming.\n", "versions": [{"version": "v1", "created": "Fri, 20 Aug 2010 23:07:54 GMT"}], "update_date": "2010-08-24", "authors_parsed": [["Murtagh", "Fionn", ""]]}, {"id": "1008.3746", "submitter": "Takashi Shinzato", "authors": "Takashi Shinzato and Muneki Yasuda", "title": "Belief Propagation Algorithm for Portfolio Optimization Problems", "comments": "5 pages, 2 figures, to submit to EPL", "journal-ref": null, "doi": "10.1371/journal.pone.0134968", "report-no": null, "categories": "q-fin.PM cond-mat.stat-mech cs.LG math.OC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The typical behavior of optimal solutions to portfolio optimization problems\nwith absolute deviation and expected shortfall models using replica analysis\nwas pioneeringly estimated by S. Ciliberti and M. M\\'ezard [Eur. Phys. B. 57,\n175 (2007)]; however, they have not yet developed an approximate derivation\nmethod for finding the optimal portfolio with respect to a given return set. In\nthis study, an approximation algorithm based on belief propagation for the\nportfolio optimization problem is presented using the Bethe free energy\nformalism, and the consistency of the numerical experimental results of the\nproposed algorithm with those of replica analysis is confirmed. Furthermore,\nthe conjecture of H. Konno and H. Yamazaki, that the optimal solutions with the\nabsolute deviation model and with the mean-variance model have the same typical\nbehavior, is verified using replica analysis and the belief propagation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 23 Aug 2010 04:20:37 GMT"}, {"version": "v2", "created": "Thu, 9 Sep 2010 04:00:01 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Shinzato", "Takashi", ""], ["Yasuda", "Muneki", ""]]}, {"id": "1008.3829", "submitter": "Ilan Nehama", "authors": "Ilan Nehama", "title": "Approximate Judgement Aggregation", "comments": null, "journal-ref": null, "doi": "10.1007/s10472-013-9358-6", "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze judgement aggregation problems in which a group of\nagents independently votes on a set of complex propositions that has some\ninterdependency constraint between them(e.g., transitivity when describing\npreferences). We consider the issue of judgement aggregation from the\nperspective of approximation. That is, we generalize the previous results by\nstudying approximate judgement aggregation. We relax the main two constraints\nassumed in the current literature, Consistency and Independence and consider\nmechanisms that only approximately satisfy these constraints, that is, satisfy\nthem up to a small portion of the inputs. The main question we raise is whether\nthe relaxation of these notions significantly alters the class of satisfying\naggregation mechanisms. The recent works for preference aggregation of Kalai,\nMossel, and Keller fit into this framework. The main result of this paper is\nthat, as in the case of preference aggregation, in the case of a subclass of a\nnatural class of aggregation problems termed `truth-functional agendas', the\nset of satisfying aggregation mechanisms does not extend non-trivially when\nrelaxing the constraints. Our proof techniques involve Boolean Fourier\ntransform and analysis of voter influences for voting protocols. The question\nwe raise for Approximate Aggregation can be stated in terms of Property\nTesting. For instance, as a corollary from our result we get a generalization\nof the classic result for property testing of linearity of Boolean functions.\n  An updated version (RePEc:huj:dispap:dp574R) is available at\nhttp://www.ratio.huji.ac.il/dp_files/dp574R.pdf\n", "versions": [{"version": "v1", "created": "Mon, 23 Aug 2010 14:26:46 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2011 12:34:24 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2011 20:58:26 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Nehama", "Ilan", ""]]}, {"id": "1008.4000", "submitter": "Dacheng Tao", "authors": "Tianyi Zhou, Dacheng Tao, Xindong Wu", "title": "NESVM: a Fast Gradient Method for Support Vector Machines", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Support vector machines (SVMs) are invaluable tools for many practical\napplications in artificial intelligence, e.g., classification and event\nrecognition. However, popular SVM solvers are not sufficiently efficient for\napplications with a great deal of samples as well as a large number of\nfeatures. In this paper, thus, we present NESVM, a fast gradient SVM solver\nthat can optimize various SVM models, e.g., classical SVM, linear programming\nSVM and least square SVM. Compared against SVM-Perf\n\\cite{SVM_Perf}\\cite{PerfML} (its convergence rate in solving the dual SVM is\nupper bounded by $\\mathcal O(1/\\sqrt{k})$, wherein $k$ is the number of\niterations.) and Pegasos \\cite{Pegasos} (online SVM that converges at rate\n$\\mathcal O(1/k)$ for the primal SVM), NESVM achieves the optimal convergence\nrate at $\\mathcal O(1/k^{2})$ and a linear time complexity. In particular,\nNESVM smoothes the non-differentiable hinge loss and $\\ell_1$-norm in the\nprimal SVM. Then the optimal gradient method without any line search is adopted\nto solve the optimization. In each iteration round, the current gradient and\nhistorical gradients are combined to determine the descent direction, while the\nLipschitz constant determines the step size. Only two matrix-vector\nmultiplications are required in each iteration round. Therefore, NESVM is more\nefficient than existing SVM solvers. In addition, NESVM is available for both\nlinear and nonlinear kernels. We also propose \"homotopy NESVM\" to accelerate\nNESVM by dynamically decreasing the smooth parameter and using the continuation\nmethod. Our experiments on census income categorization, indoor/outdoor scene\nclassification, event recognition and scene recognition suggest the efficiency\nand the effectiveness of NESVM. The MATLAB code of NESVM will be available on\nour website for further assessment.\n", "versions": [{"version": "v1", "created": "Tue, 24 Aug 2010 10:02:01 GMT"}], "update_date": "2011-10-02", "authors_parsed": [["Zhou", "Tianyi", ""], ["Tao", "Dacheng", ""], ["Wu", "Xindong", ""]]}, {"id": "1008.4220", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt, LIENS)", "title": "Structured sparsity-inducing norms through submodular functions", "comments": null, "journal-ref": "NIPS, Canada (2010)", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse methods for supervised learning aim at finding good linear predictors\nfrom as few variables as possible, i.e., with small cardinality of their\nsupports. This combinatorial selection problem is often turned into a convex\noptimization problem by replacing the cardinality function by its convex\nenvelope (tightest convex lower bound), in this case the L1-norm. In this\npaper, we investigate more general set-functions than the cardinality, that may\nincorporate prior knowledge or structural constraints which are common in many\napplications: namely, we show that for nondecreasing submodular set-functions,\nthe corresponding convex envelope can be obtained from its \\lova extension, a\ncommon tool in submodular analysis. This defines a family of polyhedral norms,\nfor which we provide generic algorithmic tools (subgradients and proximal\noperators) and theoretical results (conditions for support recovery or\nhigh-dimensional inference). By selecting specific submodular functions, we can\ngive a new interpretation to known norms, such as those based on\nrank-statistics or grouped norms with potentially overlapping groups; we also\ndefine new norms, in particular ones that can be used as non-factorial priors\nfor supervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 07:28:08 GMT"}, {"version": "v2", "created": "Wed, 22 Sep 2010 03:11:25 GMT"}, {"version": "v3", "created": "Fri, 12 Nov 2010 14:51:23 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt, LIENS"]]}, {"id": "1008.4232", "submitter": "Vladimir Vyugin", "authors": "Vladimir V. V'yugin", "title": "Online Learning in Case of Unbounded Losses Using the Follow Perturbed\n  Leader Algorithm", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the sequential prediction problem with expert advice is\nconsidered for the case where losses of experts suffered at each step cannot be\nbounded in advance. We present some modification of Kalai and Vempala algorithm\nof following the perturbed leader where weights depend on past losses of the\nexperts. New notions of a volume and a scaled fluctuation of a game are\nintroduced. We present a probabilistic algorithm protected from unrestrictedly\nlarge one-step losses. This algorithm has the optimal performance in the case\nwhen the scaled fluctuations of one-step losses of experts of the pool tend to\nzero.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 09:09:29 GMT"}], "update_date": "2010-08-26", "authors_parsed": [["V'yugin", "Vladimir V.", ""]]}, {"id": "1008.4406", "submitter": "Fangwen Fu", "authors": "Fangwen Fu, and Mihaela van der Schaar", "title": "Structural Solutions to Dynamic Scheduling for Multimedia Transmission\n  in Unknown Wireless Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a systematic solution to the problem of scheduling\ndelay-sensitive media data for transmission over time-varying wireless\nchannels. We first formulate the dynamic scheduling problem as a Markov\ndecision process (MDP) that explicitly considers the users' heterogeneous\nmultimedia data characteristics (e.g. delay deadlines, distortion impacts and\ndependencies etc.) and time-varying channel conditions, which are not\nsimultaneously considered in state-of-the-art packet scheduling algorithms.\nThis formulation allows us to perform foresighted decisions to schedule\nmultiple data units for transmission at each time in order to optimize the\nlong-term utilities of the multimedia applications. The heterogeneity of the\nmedia data enables us to express the transmission priorities between the\ndifferent data units as a priority graph, which is a directed acyclic graph\n(DAG). This priority graph provides us with an elegant structure to decompose\nthe multi-data unit foresighted decision at each time into multiple single-data\nunit foresighted decisions which can be performed sequentially, from the high\npriority data units to the low priority data units, thereby significantly\nreducing the computation complexity. When the statistical knowledge of the\nmultimedia data characteristics and channel conditions is unknown a priori, we\ndevelop a low-complexity online learning algorithm to update the value\nfunctions which capture the impact of the current decision on the future\nutility. The simulation results show that the proposed solution significantly\noutperforms existing state-of-the-art scheduling solutions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 23:06:39 GMT"}], "update_date": "2010-08-27", "authors_parsed": [["Fu", "Fangwen", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1008.4532", "submitter": "Wouter Koolen", "authors": "Wouter M. Koolen and Tim van Erven", "title": "Switching between Hidden Markov Models using Fixed Share", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In prediction with expert advice the goal is to design online prediction\nalgorithms that achieve small regret (additional loss on the whole data)\ncompared to a reference scheme. In the simplest such scheme one compares to the\nloss of the best expert in hindsight. A more ambitious goal is to split the\ndata into segments and compare to the best expert on each segment. This is\nappropriate if the nature of the data changes between segments. The standard\nfixed-share algorithm is fast and achieves small regret compared to this\nscheme.\n  Fixed share treats the experts as black boxes: there are no assumptions about\nhow they generate their predictions. But if the experts are learning, the\nfollowing question arises: should the experts learn from all data or only from\ndata in their own segment? The original algorithm naturally addresses the first\ncase. Here we consider the second option, which is more appropriate exactly\nwhen the nature of the data changes between segments. In general extending\nfixed share to this second case will slow it down by a factor of T on T\noutcomes. We show, however, that no such slowdown is necessary if the experts\nare hidden Markov models.\n", "versions": [{"version": "v1", "created": "Thu, 26 Aug 2010 15:36:22 GMT"}], "update_date": "2010-08-27", "authors_parsed": [["Koolen", "Wouter M.", ""], ["van Erven", "Tim", ""]]}, {"id": "1008.4654", "submitter": "Wouter Koolen", "authors": "Wouter M. Koolen and Tim van Erven", "title": "Freezing and Sleeping: Tracking Experts that Learn by Evolving Past\n  Posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem posed by Freund is how to efficiently track a small pool of experts\nout of a much larger set. This problem was solved when Bousquet and Warmuth\nintroduced their mixing past posteriors (MPP) algorithm in 2001.\n  In Freund's problem the experts would normally be considered black boxes.\nHowever, in this paper we re-examine Freund's problem in case the experts have\ninternal structure that enables them to learn. In this case the problem has two\npossible interpretations: should the experts learn from all data or only from\nthe subsequence on which they are being tracked? The MPP algorithm solves the\nfirst case. Our contribution is to generalise MPP to address the second option.\nThe results we obtain apply to any expert structure that can be formalised\nusing (expert) hidden Markov models. Curiously enough, for our interpretation\nthere are \\emph{two} natural reference schemes: freezing and sleeping. For each\nscheme, we provide an efficient prediction strategy and prove the relevant loss\nbound.\n", "versions": [{"version": "v1", "created": "Fri, 27 Aug 2010 06:53:28 GMT"}], "update_date": "2010-08-30", "authors_parsed": [["Koolen", "Wouter M.", ""], ["van Erven", "Tim", ""]]}, {"id": "1008.4669", "submitter": "Md. Saiful Islam", "authors": "Md. Saiful Islam and Md. Iftekharul Amin", "title": "An Architecture of Active Learning SVMs with Relevance Feedback for\n  Classifying E-mail", "comments": "7 pages, 2 figures", "journal-ref": "Journal of Computer Science (IBAIS University), Vol. 1, No. 1, pp.\n  15-18, 2007", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have proposed an architecture of active learning SVMs with\nrelevance feedback (RF)for classifying e-mail. This architecture combines both\nactive learning strategies where instead of using a randomly selected training\nset, the learner has access to a pool of unlabeled instances and can request\nthe labels of some number of them and relevance feedback where if any mail\nmisclassified then the next set of support vectors will be different from the\npresent set otherwise the next set will not change. Our proposed architecture\nwill ensure that a legitimate e-mail will not be dropped in the event of\noverflowing mailbox. The proposed architecture also exhibits dynamic updating\ncharacteristics making life as difficult for the spammer as possible.\n", "versions": [{"version": "v1", "created": "Fri, 27 Aug 2010 09:06:29 GMT"}], "update_date": "2010-08-30", "authors_parsed": [["Islam", "Md. Saiful", ""], ["Amin", "Md. Iftekharul", ""]]}, {"id": "1008.4973", "submitter": "Nabin Malakar", "authors": "N. K. Malakar and K. H. Knuth", "title": "Entropy-Based Search Algorithm for Experimental Design", "comments": "8 pages, 3 figures. To appear in the proceedings of MaxEnt 2010, held\n  in Chamonix, France", "journal-ref": null, "doi": "10.1063/1.3573612", "report-no": null, "categories": "stat.ML cs.LG physics.comp-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific method relies on the iterated processes of inference and\ninquiry. The inference phase consists of selecting the most probable models\nbased on the available data; whereas the inquiry phase consists of using what\nis known about the models to select the most relevant experiment. Optimizing\ninquiry involves searching the parameterized space of experiments to select the\nexperiment that promises, on average, to be maximally informative. In the case\nwhere it is important to learn about each of the model parameters, the\nrelevance of an experiment is quantified by Shannon entropy of the distribution\nof experimental outcomes predicted by a probable set of models. If the set of\npotential experiments is described by many parameters, we must search this\nhigh-dimensional entropy space. Brute force search methods will be slow and\ncomputationally expensive. We present an entropy-based search algorithm, called\nnested entropy sampling, to select the most informative experiment for\nefficient experimental design. This algorithm is inspired by Skilling's nested\nsampling algorithm used in inference and borrows the concept of a rising\nthreshold while a set of experiment samples are maintained. We demonstrate that\nthis algorithm not only selects highly relevant experiments, but also is more\nefficient than brute force search. Such entropic search techniques promise to\ngreatly benefit autonomous experimental design.\n", "versions": [{"version": "v1", "created": "Sun, 29 Aug 2010 23:37:19 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Malakar", "N. K.", ""], ["Knuth", "K. H.", ""]]}, {"id": "1008.5078", "submitter": "Joel Ratsaby", "authors": "Joel Ratsaby", "title": "Prediction by Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that text compression can be achieved by predicting the next\nsymbol in the stream of text data based on the history seen up to the current\nsymbol. The better the prediction the more skewed the conditional probability\ndistribution of the next symbol and the shorter the codeword that needs to be\nassigned to represent this next symbol. What about the opposite direction ?\nsuppose we have a black box that can compress text stream. Can it be used to\npredict the next symbol in the stream ? We introduce a criterion based on the\nlength of the compressed data and use it to predict the next symbol. We examine\nempirically the prediction error rate and its dependency on some compression\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 30 Aug 2010 13:21:49 GMT"}], "update_date": "2010-08-31", "authors_parsed": [["Ratsaby", "Joel", ""]]}, {"id": "1008.5090", "submitter": "Francesco Dinuzzo", "authors": "Francesco Dinuzzo", "title": "Fixed-point and coordinate descent algorithms for regularized kernel\n  methods", "comments": null, "journal-ref": null, "doi": "10.1109/TNN.2011.2164096", "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two general classes of optimization algorithms for\nkernel methods with convex loss function and quadratic norm regularization, and\nanalyze their convergence. The first approach, based on fixed-point iterations,\nis simple to implement and analyze, and can be easily parallelized. The second,\nbased on coordinate descent, exploits the structure of additively separable\nloss functions to compute solutions of line searches in closed form. Instances\nof these general classes of algorithms are already incorporated into state of\nthe art machine learning software for large scale problems. We start from a\nsolution characterization of the regularized problem, obtained using\nsub-differential calculus and resolvents of monotone operators, that holds for\ngeneral convex loss functions regardless of differentiability. The two\nmethodologies described in the paper can be regarded as instances of non-linear\nJacobi and Gauss-Seidel algorithms, and are both well-suited to solve large\nscale problems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Aug 2010 14:39:57 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Dinuzzo", "Francesco", ""]]}, {"id": "1008.5105", "submitter": "Vladimir Pestov", "authors": "Vladimir Pestov", "title": "Indexability, concentration, and VC theory", "comments": "17 pages, final submission to J. Discrete Algorithms (an expanded,\n  improved and corrected version of the SISAP'2010 invited paper, this e-print,\n  v3)", "journal-ref": "J. Discrete Algorithms 13 (2012), pp. 2-18", "doi": "10.1016/j.jda.2011.10.002", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Degrading performance of indexing schemes for exact similarity search in high\ndimensions has long since been linked to histograms of distributions of\ndistances and other 1-Lipschitz functions getting concentrated. We discuss this\nobservation in the framework of the phenomenon of concentration of measure on\nthe structures of high dimension and the Vapnik-Chervonenkis theory of\nstatistical learning.\n", "versions": [{"version": "v1", "created": "Mon, 30 Aug 2010 16:09:24 GMT"}, {"version": "v2", "created": "Tue, 31 Aug 2010 17:58:42 GMT"}, {"version": "v3", "created": "Wed, 1 Sep 2010 11:15:00 GMT"}, {"version": "v4", "created": "Mon, 10 Jan 2011 01:08:44 GMT"}, {"version": "v5", "created": "Sat, 21 May 2011 20:48:26 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Pestov", "Vladimir", ""]]}, {"id": "1008.5133", "submitter": "Farnood Merrikh-Bayat", "authors": "Farnood Merrikh-Bayat, Saeed Bagheri-Shouraki, and Ali Rohani", "title": "Memristor Crossbar-based Hardware Implementation of IDS Method", "comments": "16 pages, 13 figures, Submitted to IEEE Transaction on Fuzzy Systems", "journal-ref": null, "doi": "10.1109/TFUZZ.2011.2160024", "report-no": null, "categories": "cs.LG cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ink Drop Spread (IDS) is the engine of Active Learning Method (ALM), which is\nthe methodology of soft computing. IDS, as a pattern-based processing unit,\nextracts useful information from a system subjected to modeling. In spite of\nits excellent potential in solving problems such as classification and modeling\ncompared to other soft computing tools, finding its simple and fast hardware\nimplementation is still a challenge. This paper describes a new hardware\nimplementation of IDS method based on the memristor crossbar structure. In\naddition of simplicity, being completely real-time, having low latency and the\nability to continue working after the occurrence of power breakdown are some of\nthe advantages of our proposed circuit.\n", "versions": [{"version": "v1", "created": "Sun, 22 Aug 2010 16:44:23 GMT"}, {"version": "v2", "created": "Thu, 2 Sep 2010 15:56:15 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Merrikh-Bayat", "Farnood", ""], ["Bagheri-Shouraki", "Saeed", ""], ["Rohani", "Ali", ""]]}, {"id": "1008.5204", "submitter": "Qihang Lin", "authors": "Qihang Lin, Xi Chen and Javier Pena", "title": "A Smoothing Stochastic Gradient Method for Composite Optimization", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the unconstrained optimization problem whose objective function\nis composed of a smooth and a non-smooth conponents where the smooth component\nis the expectation a random function. This type of problem arises in some\ninteresting applications in machine learning. We propose a stochastic gradient\ndescent algorithm for this class of optimization problem. When the non-smooth\ncomponent has a particular structure, we propose another stochastic gradient\ndescent algorithm by incorporating a smoothing method into our first algorithm.\nThe proofs of the convergence rates of these two algorithms are given and we\nshow the numerical performance of our algorithm by applying them to regularized\nlinear regression problems with different sets of synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 02:42:32 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2011 18:14:40 GMT"}], "update_date": "2011-07-01", "authors_parsed": [["Lin", "Qihang", ""], ["Chen", "Xi", ""], ["Pena", "Javier", ""]]}, {"id": "1008.5209", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Rocquencourt, LIENS), Rodolphe Jenatton (INRIA\n  Rocquencourt, LIENS), Guillaume Obozinski (INRIA Rocquencourt, LIENS),\n  Francis Bach (INRIA Rocquencourt, LIENS)", "title": "Network Flow Algorithms for Structured Sparsity", "comments": "accepted for publication in Adv. Neural Information Processing\n  Systems, 2010", "journal-ref": null, "doi": null, "report-no": "RR-7372", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of learning problems that involve a structured\nsparsity-inducing norm defined as the sum of $\\ell_\\infty$-norms over groups of\nvariables. Whereas a lot of effort has been put in developing fast optimization\nmethods when the groups are disjoint or embedded in a specific hierarchical\nstructure, we address here the case of general overlapping groups. To this end,\nwe show that the corresponding optimization problem is related to network flow\noptimization. More precisely, the proximal problem associated with the norm we\nconsider is dual to a quadratic min-cost flow problem. We propose an efficient\nprocedure which computes its solution exactly in polynomial time. Our algorithm\nscales up to millions of variables, and opens up a whole new range of\napplications for structured sparse models. We present several experiments on\nimage and video data, demonstrating the applicability and scalability of our\napproach for various problems.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 03:39:49 GMT"}], "update_date": "2010-09-02", "authors_parsed": [["Mairal", "Julien", "", "INRIA Rocquencourt, LIENS"], ["Jenatton", "Rodolphe", "", "INRIA\n  Rocquencourt, LIENS"], ["Obozinski", "Guillaume", "", "INRIA Rocquencourt, LIENS"], ["Bach", "Francis", "", "INRIA Rocquencourt, LIENS"]]}, {"id": "1008.5231", "submitter": "Konstantinos Slavakis", "authors": "Konstantinos Slavakis and Isao Yamada", "title": "The adaptive projected subgradient method constrained by families of\n  quasi-nonexpansive mappings and its application to online learning", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online, i.e., time-adaptive, inverse problems in signal processing and\nmachine learning fall under the wide umbrella of the asymptotic minimization of\na sequence of non-negative, convex, and continuous functions. To incorporate\na-priori knowledge into the design, the asymptotic minimization task is usually\nconstrained on a fixed closed convex set, which is dictated by the available\na-priori information. To increase versatility towards the usage of the\navailable information, the present manuscript extends the Adaptive Projected\nSubgradient Method (APSM) by introducing an algorithmic scheme which\nincorporates a-priori knowledge in the design via a sequence of strongly\nattracting quasi-nonexpansive mappings in a real Hilbert space. In such a way,\nthe benefits offered to online learning tasks by the proposed method unfold in\ntwo ways: 1) the rich class of quasi-nonexpansive mappings provides a plethora\nof ways to cast a-priori knowledge, and 2) by introducing a sequence of such\nmappings, the proposed scheme is able to capture the time-varying nature of\na-priori information. The convergence properties of the algorithm are studied,\nseveral special cases of the method with wide applicability are shown, and the\npotential of the proposed scheme is demonstrated by considering an increasingly\nimportant, nowadays, online sparse system/signal recovery task.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 07:07:27 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2011 07:31:07 GMT"}, {"version": "v3", "created": "Fri, 5 Aug 2011 16:03:03 GMT"}], "update_date": "2011-08-08", "authors_parsed": [["Slavakis", "Konstantinos", ""], ["Yamada", "Isao", ""]]}, {"id": "1008.5325", "submitter": "Danny Bickson", "authors": "Danny Bickson and Carlos Guestrin", "title": "Inference with Multivariate Heavy-Tails in Linear Models", "comments": "In Neural Information Processing System (NIPS) 2010, Dec. 2010,\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed distributions naturally occur in many real life problems.\nUnfortunately, it is typically not possible to compute inference in closed-form\nin graphical models which involve such heavy-tailed distributions.\n  In this work, we propose a novel simple linear graphical model for\nindependent latent random variables, called linear characteristic model (LCM),\ndefined in the characteristic function domain. Using stable distributions, a\nheavy-tailed family of distributions which is a generalization of Cauchy,\nL\\'evy and Gaussian distributions, we show for the first time, how to compute\nboth exact and approximate inference in such a linear multivariate graphical\nmodel. LCMs are not limited to stable distributions, in fact LCMs are always\ndefined for any random variables (discrete, continuous or a mixture of both).\n  We provide a realistic problem from the field of computer networks to\ndemonstrate the applicability of our construction. Other potential application\nis iterative decoding of linear channels with non-Gaussian noise.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 14:31:57 GMT"}, {"version": "v2", "created": "Fri, 5 Nov 2010 15:26:53 GMT"}, {"version": "v3", "created": "Mon, 8 Nov 2010 16:14:02 GMT"}, {"version": "v4", "created": "Mon, 21 Mar 2011 15:54:54 GMT"}], "update_date": "2011-03-22", "authors_parsed": [["Bickson", "Danny", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1008.5372", "submitter": "Yong Zhang", "authors": "Zhaosong Lu and Yong Zhang", "title": "Penalty Decomposition Methods for $L0$-Norm Minimization", "comments": "This paper has been withdrawn by the author because an updated\n  version has been resubmitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.IT cs.LG cs.NA math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider general l0-norm minimization problems, that is, the\nproblems with l0-norm appearing in either objective function or constraint. In\nparticular, we first reformulate the l0-norm constrained problem as an\nequivalent rank minimization problem and then apply the penalty decomposition\n(PD) method proposed in [33] to solve the latter problem. By utilizing the\nspecial structures, we then transform all matrix operations of this method to\nvector operations and obtain a PD method that only involves vector operations.\nUnder some suitable assumptions, we establish that any accumulation point of\nthe sequence generated by the PD method satisfies a first-order optimality\ncondition that is generally stronger than one natural optimality condition. We\nfurther extend the PD method to solve the problem with the l0-norm appearing in\nobjective function. Finally, we test the performance of our PD methods by\napplying them to compressed sensing, sparse logistic regression and sparse\ninverse covariance selection. The computational results demonstrate that our\nmethods generally outperform the existing methods in terms of solution quality\nand/or speed.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 17:24:31 GMT"}, {"version": "v2", "created": "Fri, 11 May 2012 17:12:02 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "1008.5373", "submitter": "Yong Zhang", "authors": "Zhaosong Lu and Yong Zhang", "title": "Penalty Decomposition Methods for Rank Minimization", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA cs.SY q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider general rank minimization problems with rank\nappearing in either objective function or constraint. We first establish that a\nclass of special rank minimization problems has closed-form solutions. Using\nthis result, we then propose penalty decomposition methods for general rank\nminimization problems in which each subproblem is solved by a block coordinate\ndescend method. Under some suitable assumptions, we show that any accumulation\npoint of the sequence generated by the penalty decomposition methods satisfies\nthe first-order optimality conditions of a nonlinear reformulation of the\nproblems. Finally, we test the performance of our methods by applying them to\nthe matrix completion and nearest low-rank correlation matrix problems. The\ncomputational results demonstrate that our methods are generally comparable or\nsuperior to the existing methods in terms of solution quality.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 17:25:01 GMT"}, {"version": "v2", "created": "Mon, 22 Nov 2010 22:29:44 GMT"}, {"version": "v3", "created": "Thu, 17 May 2012 16:23:59 GMT"}, {"version": "v4", "created": "Tue, 29 May 2012 16:08:51 GMT"}], "update_date": "2012-05-30", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "1008.5386", "submitter": "Ricardo Silva", "authors": "Ricardo Silva and Charles Blundell and Yee Whye Teh", "title": "Mixed Cumulative Distribution Networks", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs (DAGs) are a popular framework to express\nmultivariate probability distributions. Acyclic directed mixed graphs (ADMGs)\nare generalizations of DAGs that can succinctly capture much richer sets of\nconditional independencies, and are especially useful in modeling the effects\nof latent variables implicitly. Unfortunately there are currently no good\nparameterizations of general ADMGs. In this paper, we apply recent work on\ncumulative distribution networks and copulas to propose one one general\nconstruction for ADMG models. We consider a simple parameter estimation\napproach, and report some encouraging experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 18:51:43 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Silva", "Ricardo", ""], ["Blundell", "Charles", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1008.5390", "submitter": "Hesam Dashti", "authors": "Hesam T. Dashti, Jernej Tonejc, Adel Ardalan, Alireza F. Siahpirani,\n  Sabrina Guettes, Zohreh Sharif, Liya Wang, Amir H. Assadi", "title": "Applications of Machine Learning Methods to Quantifying Phenotypic\n  Traits that Distinguish the Wild Type from the Mutant Arabidopsis Thaliana\n  Seedlings during Root Gravitropism", "comments": "International Conference on Bioinformatics and Computational Biology,\n  WorldComp 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Post-genomic research deals with challenging problems in screening genomes of\norganisms for particular functions or potential for being the targets of\ngenetic engineering for desirable biological features. 'Phenotyping' of wild\ntype and mutants is a time-consuming and costly effort by many individuals.\nThis article is a preliminary progress report in research on large-scale\nautomation of phenotyping steps (imaging, informatics and data analysis) needed\nto study plant gene-proteins networks that influence growth and development of\nplants. Our results undermine the significance of phenotypic traits that are\nimplicit in patterns of dynamics in plant root response to sudden changes of\nits environmental conditions, such as sudden re-orientation of the root tip\nagainst the gravity vector. Including dynamic features besides the common\nmorphological ones has paid off in design of robust and accurate machine\nlearning methods to automate a typical phenotyping scenario, i.e. to\ndistinguish the wild type from the mutants.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 18:54:33 GMT"}], "update_date": "2010-09-06", "authors_parsed": [["Dashti", "Hesam T.", ""], ["Tonejc", "Jernej", ""], ["Ardalan", "Adel", ""], ["Siahpirani", "Alireza F.", ""], ["Guettes", "Sabrina", ""], ["Sharif", "Zohreh", ""], ["Wang", "Liya", ""], ["Assadi", "Amir H.", ""]]}]