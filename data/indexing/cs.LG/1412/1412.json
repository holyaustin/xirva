[{"id": "1412.0100", "submitter": "Stefan Mathe", "authors": "Stefan Mathe, Cristian Sminchisescu", "title": "Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised\n  Detection in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art visual recognition and detection systems increasingly rely\non large amounts of training data and complex classifiers. Therefore it becomes\nincreasingly expensive both to manually annotate datasets and to keep running\ntimes at levels acceptable for practical applications. In this paper, we\npropose two solutions to address these issues. First, we introduce a weakly\nsupervised, segmentation-based approach to learn accurate detectors and image\nclassifiers from weak supervisory signals that provide only approximate\nconstraints on target localization. We illustrate our system on the problem of\naction detection in static images (Pascal VOC Actions 2012), using human visual\nsearch patterns as our training signal. Second, inspired from the\nsaccade-and-fixate operating principle of the human visual system, we use\nreinforcement learning techniques to train efficient search models for\ndetection. Our sequential method is weakly supervised and general (it does not\nrequire eye movements), finds optimal search strategies for any given detection\nconfidence function and achieves performance similar to exhaustive sliding\nwindow search at a fraction of its computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 12:18:14 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Mathe", "Stefan", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1412.0156", "submitter": "Francis Bach", "authors": "Alexandre D\\'efossez (LIENS, INRIA Paris - Rocquencourt), Francis Bach\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Constant Step Size Least-Mean-Square: Bias-Variance Trade-offs and\n  Optimal Sampling Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the least-squares regression problem and provide a detailed\nasymptotic analysis of the performance of averaged constant-step-size\nstochastic gradient descent (a.k.a. least-mean-squares). In the strongly-convex\ncase, we provide an asymptotic expansion up to explicit exponentially decaying\nterms. Our analysis leads to new insights into stochastic approximation\nalgorithms: (a) it gives a tighter bound on the allowed step-size; (b) the\ngeneralization error may be divided into a variance term which is decaying as\nO(1/n), independently of the step-size $\\gamma$, and a bias term that decays as\nO(1/$\\gamma$ 2 n 2); (c) when allowing non-uniform sampling, the choice of a\ngood sampling density depends on whether the variance or bias terms dominate.\nIn particular, when the variance term dominates, optimal sampling densities do\nnot lead to much gain, while when the bias term dominates, we can choose larger\nstep-sizes that leads to significant improvements.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 21:29:15 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["D\u00e9fossez", "Alexandre", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1412.0180", "submitter": "Dileep Kalathil", "authors": "Dileep Kalathil, Vivek S. Borkar, Rahul Jain", "title": "Empirical Q-Value Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new simple and natural algorithm for learning the optimal\nQ-value function of a discounted-cost Markov Decision Process (MDP) when the\ntransition kernels are unknown. Unlike the classical learning algorithms for\nMDPs, such as Q-learning and actor-critic algorithms, this algorithm doesn't\ndepend on a stochastic approximation-based method. We show that our algorithm,\nwhich we call the empirical Q-value iteration (EQVI) algorithm, converges to\nthe optimal Q-value function. We also give a rate of convergence or a\nnon-asymptotic sample complexity bound, and also show that an asynchronous (or\nonline) version of the algorithm will also work. Preliminary experimental\nresults suggest a faster rate of convergence to a ball park estimate for our\nalgorithm compared to stochastic approximation-based algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 06:06:23 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 05:24:11 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 02:57:33 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Kalathil", "Dileep", ""], ["Borkar", "Vivek S.", ""], ["Jain", "Rahul", ""]]}, {"id": "1412.0233", "submitter": "Anna Choromanska", "authors": "Anna Choromanska, Mikael Henaff, Michael Mathieu, G\\'erard Ben Arous,\n  Yann LeCun", "title": "The Loss Surfaces of Multilayer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the connection between the highly non-convex loss function of a\nsimple model of the fully-connected feed-forward neural network and the\nHamiltonian of the spherical spin-glass model under the assumptions of: i)\nvariable independence, ii) redundancy in network parametrization, and iii)\nuniformity. These assumptions enable us to explain the complexity of the fully\ndecoupled neural network through the prism of the results from random matrix\ntheory. We show that for large-size decoupled networks the lowest critical\nvalues of the random loss function form a layered structure and they are\nlocated in a well-defined band lower-bounded by the global minimum. The number\nof local minima outside that band diminishes exponentially with the size of the\nnetwork. We empirically verify that the mathematical model exhibits similar\nbehavior as the computer simulations, despite the presence of high dependencies\nin real networks. We conjecture that both simulated annealing and SGD converge\nto the band of low critical points, and that all critical points found there\nare local minima of high quality measured by the test error. This emphasizes a\nmajor difference between large- and small-size networks where for the latter\npoor quality local minima have non-zero probability of being recovered.\nFinally, we prove that recovering the global minimum becomes harder as the\nnetwork size increases and that it is in practice irrelevant as global minimum\noften leads to overfitting.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 15:48:16 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 21:46:57 GMT"}, {"version": "v3", "created": "Wed, 21 Jan 2015 22:25:26 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Choromanska", "Anna", ""], ["Henaff", "Mikael", ""], ["Mathieu", "Michael", ""], ["Arous", "G\u00e9rard Ben", ""], ["LeCun", "Yann", ""]]}, {"id": "1412.0436", "submitter": "Luis Torgo", "authors": "Luis Torgo", "title": "An Infra-Structure for Performance Estimation and Experimental\n  Comparison of Predictive Models in R", "comments": "Updated to version 1.0.2 of the R package. Added a small section on\n  package installation. Made explicit the reference to the R package version\n  number within the document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.SE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes an infra-structure provided by the R package\nperformanceEstimation that allows to estimate the predictive performance of\ndifferent approaches (workflows) to predictive tasks. The infra-structure is\ngeneric in the sense that it can be used to estimate the values of any\nperformance metrics, for any workflow on different predictive tasks, namely,\nclassification, regression and time series tasks. The package also includes\nseveral standard workflows that allow users to easily set up their experiments\nlimiting the amount of work and information they need to provide. The overall\ngoal of the infra-structure provided by our package is to facilitate the task\nof estimating the predictive performance of different modeling approaches to\npredictive tasks in the R environment.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 11:35:47 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 18:13:14 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 09:40:18 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2015 15:03:45 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Torgo", "Luis", ""]]}, {"id": "1412.0620", "submitter": "Anil Aswani", "authors": "Anil Aswani", "title": "Low-Rank Approximation and Completion of Positive Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike the matrix case, computing low-rank approximations of tensors is\nNP-hard and numerically ill-posed in general. Even the best rank-1\napproximation of a tensor is NP-hard. In this paper, we use convex optimization\nto develop polynomial-time algorithms for low-rank approximation and completion\nof positive tensors. Our approach is to use algebraic topology to define a new\n(numerically well-posed) decomposition for positive tensors, which we show is\nequivalent to the standard tensor decomposition in important cases. Though\ncomputing this decomposition is a nonconvex optimization problem, we prove it\ncan be exactly reformulated as a convex optimization problem. This allows us to\nconstruct polynomial-time randomized algorithms for computing this\ndecomposition and for solving low-rank tensor approximation problems. Among the\nconsequences is that best rank-1 approximations of positive tensors can be\ncomputed in polynomial time. Our framework is next extended to the tensor\ncompletion problem, where noisy entries of a tensor are observed and then used\nto estimate missing entries. We provide a polynomial-time algorithm that for\nspecific cases requires a polynomial (in tensor order) number of measurements,\nin contrast to existing approaches that require an exponential number of\nmeasurements. These algorithms are extended to exploit sparsity in the tensor\nto reduce the number of measurements needed. We conclude by providing a novel\ninterpretation of statistical regression problems with categorical variables as\ntensor completion problems, and numerical examples with synthetic data and data\nfrom a bioengineered metabolic network show the improved performance of our\napproach on this problem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 20:06:39 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 17:55:12 GMT"}, {"version": "v3", "created": "Fri, 9 Jan 2015 19:55:33 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2015 08:14:41 GMT"}, {"version": "v5", "created": "Sat, 9 Jul 2016 20:15:45 GMT"}, {"version": "v6", "created": "Tue, 13 Sep 2016 18:25:42 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Aswani", "Anil", ""]]}, {"id": "1412.1058", "submitter": "Rie Johnson", "authors": "Rie Johnson and Tong Zhang", "title": "Effective Use of Word Order for Text Categorization with Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) is a neural network that can make use of\nthe internal structure of data such as the 2D structure of image data. This\npaper studies CNN on text categorization to exploit the 1D structure (namely,\nword order) of text data for accurate prediction. Instead of using\nlow-dimensional word vectors as input as is often done, we directly apply CNN\nto high-dimensional text data, which leads to directly learning embedding of\nsmall text regions for use in classification. In addition to a straightforward\nadaptation of CNN from image to text, a simple but new variation which employs\nbag-of-word conversion in the convolution layer is proposed. An extension to\ncombine multiple convolution layers is also explored for higher accuracy. The\nexperiments demonstrate the effectiveness of our approach in comparison with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 16:19:51 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 12:59:35 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1412.1074", "submitter": "Alexandre Drouin", "authors": "Alexandre Drouin, S\\'ebastien Gigu\\`ere, Vladana Sagatovich, Maxime\n  D\\'eraspe, Fran\\c{c}ois Laviolette, Mario Marchand, Jacques Corbeil", "title": "Learning interpretable models of phenotypes from whole genome sequences\n  with the Set Covering Machine", "comments": "Presented at Machine Learning in Computational Biology 2014,\n  Montr\\'eal, Qu\\'ebec, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased affordability of whole genome sequencing has motivated its use\nfor phenotypic studies. We address the problem of learning interpretable models\nfor discrete phenotypes from whole genomes. We propose a general approach that\nrelies on the Set Covering Machine and a k-mer representation of the genomes.\nWe show results for the problem of predicting the resistance of Pseudomonas\nAeruginosa, an important human pathogen, against 4 antibiotics. Our results\ndemonstrate that extremely sparse models which are biologically relevant can be\nlearnt using this approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 13:26:50 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Drouin", "Alexandre", ""], ["Gigu\u00e8re", "S\u00e9bastien", ""], ["Sagatovich", "Vladana", ""], ["D\u00e9raspe", "Maxime", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""]]}, {"id": "1412.1114", "submitter": "Marc Claesen", "authors": "Marc Claesen, Jaak Simm, Dusan Popovic, Yves Moreau, Bart De Moor", "title": "Easy Hyperparameter Search Using Optunity", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optunity is a free software package dedicated to hyperparameter optimization.\nIt contains various types of solvers, ranging from undirected methods to direct\nsearch, particle swarm and evolutionary optimization. The design focuses on\nease of use, flexibility, code clarity and interoperability with existing\nsoftware in all machine learning environments. Optunity is written in Python\nand contains interfaces to environments such as R and MATLAB. Optunity uses a\nBSD license and is freely available online at http://www.optunity.net.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 21:55:44 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Claesen", "Marc", ""], ["Simm", "Jaak", ""], ["Popovic", "Dusan", ""], ["Moreau", "Yves", ""], ["De Moor", "Bart", ""]]}, {"id": "1412.1138", "submitter": "Ben Fulcher", "authors": "B. D. Fulcher, A. E. Georgieva, C. W. G. Redman, Nick S. Jones", "title": "Highly comparative fetal heart rate analysis", "comments": "7 pages, 4 figures", "journal-ref": "Fulcher, B. D., Georgieva, A., Redman, C. W., & Jones, N. S.\n  (2012). Highly comparative fetal heart rate analysis (pp. 3135-3138).\n  Presented at the 34th Annual International Conference of the IEEE EMBS, San\n  Diego, CA, USA", "doi": "10.1109/EMBC.2012.6346629", "report-no": null, "categories": "cs.LG cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A database of fetal heart rate (FHR) time series measured from 7221 patients\nduring labor is analyzed with the aim of learning the types of features of\nthese recordings that are informative of low cord pH. Our 'highly comparative'\nanalysis involves extracting over 9000 time-series analysis features from each\nFHR time series, including measures of autocorrelation, entropy, distribution,\nand various model fits. This diverse collection of features was developed in\nprevious work, and is publicly available. We describe five features that most\naccurately classify a balanced training set of 59 'low pH' and 59 'normal pH'\nFHR recordings. We then describe five of the features with the strongest linear\ncorrelation to cord pH across the full dataset of FHR time series. The features\nidentified in this work may be used as part of a system for guiding\nintervention during labor in future. This work successfully demonstrates the\nutility of comparing across a large, interdisciplinary literature on\ntime-series analysis to automatically contribute new scientific results for\nspecific biomedical signal processing challenges.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 00:00:42 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Fulcher", "B. D.", ""], ["Georgieva", "A. E.", ""], ["Redman", "C. W. G.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1412.1193", "submitter": "James Martens", "authors": "James Martens", "title": "New insights and perspectives on the natural gradient method", "comments": "Minor corrections from previous version and fixed typos. Official\n  JMLR version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural gradient descent is an optimization method traditionally motivated\nfrom the perspective of information geometry, and works well for many\napplications as an alternative to stochastic gradient descent. In this paper we\ncritically analyze this method and its properties, and show how it can be\nviewed as a type of 2nd-order optimization method, with the Fisher information\nmatrix acting as a substitute for the Hessian. In many important cases, the\nFisher information matrix is shown to be equivalent to the Generalized\nGauss-Newton matrix, which both approximates the Hessian, but also has certain\nproperties that favor its use over the Hessian. This perspective turns out to\nhave significant implications for the design of a practical and robust natural\ngradient optimizer, as it motivates the use of techniques like trust regions\nand Tikhonov regularization. Additionally, we make a series of contributions to\nthe understanding of natural gradient and 2nd-order methods, including: a\nthorough analysis of the convergence speed of stochastic natural gradient\ndescent (and more general stochastic 2nd-order methods) as applied to convex\nquadratics, a critical examination of the oft-used \"empirical\" approximation of\nthe Fisher matrix, and an analysis of the (approximate) parameterization\ninvariance property possessed by natural gradient methods (which we show also\nholds for certain other curvature, but notably not the Hessian).\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 05:21:13 GMT"}, {"version": "v10", "created": "Sun, 7 Jun 2020 22:48:03 GMT"}, {"version": "v11", "created": "Sat, 19 Sep 2020 15:16:47 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 02:31:33 GMT"}, {"version": "v3", "created": "Wed, 11 Feb 2015 00:30:02 GMT"}, {"version": "v4", "created": "Wed, 8 Apr 2015 08:52:47 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2015 00:54:03 GMT"}, {"version": "v6", "created": "Tue, 3 May 2016 23:43:13 GMT"}, {"version": "v7", "created": "Mon, 30 May 2016 21:09:07 GMT"}, {"version": "v8", "created": "Mon, 13 Mar 2017 13:27:59 GMT"}, {"version": "v9", "created": "Tue, 21 Nov 2017 12:15:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Martens", "James", ""]]}, {"id": "1412.1271", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Deep Distributed Random Samplings for Supervised Learning: An\n  Alternative to Random Forests?", "comments": "This paper has been withdrawn by the author. The idea is wrong and is\n  no longer to be posed on site. The paper will no longer be updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In (\\cite{zhang2014nonlinear,zhang2014nonlinear2}), we have viewed machine\nlearning as a coding and dimensionality reduction problem, and further proposed\na simple unsupervised dimensionality reduction method, entitled deep\ndistributed random samplings (DDRS). In this paper, we further extend it to\nsupervised learning incrementally. The key idea here is to incorporate label\ninformation into the coding process by reformulating that each center in DDRS\nhas multiple output units indicating which class the center belongs to. The\nsupervised learning method seems somewhat similar with random forests\n(\\cite{breiman2001random}), here we emphasize their differences as follows. (i)\nEach layer of our method considers the relationship between part of the data\npoints in training data with all training data points, while random forests\nfocus on building each decision tree on only part of training data points\nindependently. (ii) Our method builds gradually-narrowed network by sampling\nless and less data points, while random forests builds gradually-narrowed\nnetwork by merging subclasses. (iii) Our method is trained more straightforward\nfrom bottom layer to top layer, while random forests build each tree from top\nlayer to bottom layer by splitting. (iv) Our method encodes output targets\nimplicitly in sparse codes, while random forests encode output targets by\nremembering the class attributes of the activated nodes. Therefore, our method\nis a simpler, more straightforward, and maybe a better alternative choice,\nthough both methods use two very basic elements---randomization and nearest\nneighbor optimization---as the core. This preprint is used to protect the\nincremental idea from (\\cite{zhang2014nonlinear,zhang2014nonlinear2}). Full\nempirical evaluation will be announced carefully later.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 10:57:35 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 19:23:17 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1412.1353", "submitter": "Anastasia Pentina", "authors": "Anastasia Pentina and Viktoriia Sharmanska and Christoph H. Lampert", "title": "Curriculum Learning of Multiple Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing information between multiple tasks enables algorithms to achieve good\ngeneralization performance even from small amounts of training data. However,\nin a realistic scenario of multi-task learning not all tasks are equally\nrelated to each other, hence it could be advantageous to transfer information\nonly between the most related tasks. In this work we propose an approach that\nprocesses multiple tasks in a sequence with sharing between subsequent tasks\ninstead of solving all tasks jointly. Subsequently, we address the question of\ncurriculum learning of tasks, i.e. finding the best order of tasks to be\nlearned. Our approach is based on a generalization bound criterion for choosing\nthe task order that optimizes the average expected classification performance\nover all tasks. Our experimental results show that learning multiple related\ntasks sequentially can be more effective than learning them jointly, the order\nin which tasks are being solved affects the overall performance, and that our\nmodel is able to automatically discover the favourable order of tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 15:02:51 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Pentina", "Anastasia", ""], ["Sharmanska", "Viktoriia", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1412.1443", "submitter": "Guy Bresler", "authors": "Guy Bresler, David Gamarnik, and Devavrat Shah", "title": "Structure learning of antiferromagnetic Ising models", "comments": "15 pages. NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the computational complexity of learning the\ngraph structure underlying a discrete undirected graphical model from i.i.d.\nsamples. We first observe that the notoriously difficult problem of learning\nparities with noise can be captured as a special case of learning graphical\nmodels. This leads to an unconditional computational lower bound of $\\Omega\n(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree\n$d$, for the class of so-called statistical algorithms recently introduced by\nFeldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime\nrequired to exhaustively search over neighborhoods cannot be significantly\nimproved without restricting the class of models.\n  Aside from structural assumptions on the graph such as it being a tree,\nhypertree, tree-like, etc., many recent papers on structure learning assume\nthat the model has the correlation decay property. Indeed, focusing on\nferromagnetic Ising models, Bento and Montanari (2009) showed that all known\nlow-complexity algorithms fail to learn simple graphs when the interaction\nstrength exceeds a number related to the correlation decay threshold. Our\nsecond set of results gives a class of repelling (antiferromagnetic) models\nthat have the opposite behavior: very strong interaction allows efficient\nlearning in time $O(p^2)$. We provide an algorithm whose performance\ninterpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the\nrepulsion.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 19:08:55 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Bresler", "Guy", ""], ["Gamarnik", "David", ""], ["Shah", "Devavrat", ""]]}, {"id": "1412.1454", "submitter": "Ciprian Chelba", "authors": "Noam Shazeer, Joris Pelemans, Ciprian Chelba", "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": "Google Research Publication Id: 43222", "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel family of language model (LM) estimation techniques named\nSparse Non-negative Matrix (SNM) estimation. A first set of experiments\nempirically evaluating it on the One Billion Word Benchmark shows that SNM\n$n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN)\nmodels. When using skip-gram features the models are able to match the\nstate-of-the-art recurrent neural network (RNN) LMs; combining the two modeling\ntechniques yields the best known result on the benchmark. The computational\nadvantages of SNM over both maximum entropy and RNN LM estimation are probably\nits main strength, promising an approach that has the same flexibility in\ncombining arbitrary features effectively and yet should scale to very large\namounts of data as gracefully as $n$-gram LMs do.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 19:42:12 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2015 20:35:52 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Shazeer", "Noam", ""], ["Pelemans", "Joris", ""], ["Chelba", "Ciprian", ""]]}, {"id": "1412.1463", "submitter": "Am\\'elie Rolland", "authors": "S\\'ebastien Gigu\\`ere, Am\\'elie Rolland, Fran\\c{c}ois Laviolette and\n  Mario Marchand", "title": "On the String Kernel Pre-Image Problem with Applications in Drug\n  Discovery", "comments": "Peer-reviewed and accepted for presentation at Machine Learning in\n  Computational Biology 2014, Montr\\'eal, Qu\\'ebec, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pre-image problem has to be solved during inference by most structured\noutput predictors. For string kernels, this problem corresponds to finding the\nstring associated to a given input. An algorithm capable of solving or finding\ngood approximations to this problem would have many applications in\ncomputational biology and other fields. This work uses a recent result on\ncombinatorial optimization of linear predictors based on string kernels to\ndevelop, for the pre-image, a low complexity upper bound valid for many string\nkernels. This upper bound is used with success in a branch and bound searching\nalgorithm. Applications and results in the discovery of druggable peptides are\npresented and discussed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 20:33:57 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 02:51:56 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Gigu\u00e8re", "S\u00e9bastien", ""], ["Rolland", "Am\u00e9lie", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""]]}, {"id": "1412.1523", "submitter": "Bicheng Ying", "authors": "Bicheng Ying and Ali H. Sayed", "title": "Information Exchange and Learning Dynamics over Weakly-Connected\n  Adaptive Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper examines the learning mechanism of adaptive agents over\nweakly-connected graphs and reveals an interesting behavior on how information\nflows through such topologies. The results clarify how asymmetries in the\nexchange of data can mask local information at certain agents and make them\ntotally dependent on other agents. A leader-follower relationship develops with\nthe performance of some agents being fully determined by the performance of\nother agents that are outside their domain of influence. This scenario can\narise, for example, due to intruder attacks by malicious agents or as the\nresult of failures by some critical links. The findings in this work help\nexplain why strong-connectivity of the network topology, adaptation of the\ncombination weights, and clustering of agents are important ingredients to\nequalize the learning abilities of all agents against such disturbances. The\nresults also clarify how weak-connectivity can be helpful in reducing the\neffect of outlier data on learning performance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 00:01:52 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2015 20:48:53 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Ying", "Bicheng", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1412.1574", "submitter": "Liming Zhao", "authors": "Liming Zhao, Xi Li, Jun Xiao, Fei Wu, Yueting Zhuang", "title": "Metric Learning Driven Multi-Task Structured Output Optimization for\n  Robust Keypoint Tracking", "comments": "Accepted by AAAI-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important and challenging problem in computer vision and graphics,\nkeypoint-based object tracking is typically formulated in a spatio-temporal\nstatistical learning framework. However, most existing keypoint trackers are\nincapable of effectively modeling and balancing the following three aspects in\na simultaneous manner: temporal model coherence across frames, spatial model\nconsistency within frames, and discriminative feature construction. To address\nthis issue, we propose a robust keypoint tracker based on spatio-temporal\nmulti-task structured output optimization driven by discriminative metric\nlearning. Consequently, temporal model coherence is characterized by multi-task\nstructured keypoint model learning over several adjacent frames, while spatial\nmodel consistency is modeled by solving a geometric verification based\nstructured learning problem. Discriminative feature construction is enabled by\nmetric learning to ensure the intra-class compactness and inter-class\nseparability. Finally, the above three modules are simultaneously optimized in\na joint learning scheme. Experimental results have demonstrated the\neffectiveness of our tracker.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 07:42:21 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Zhao", "Liming", ""], ["Li", "Xi", ""], ["Xiao", "Jun", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1412.1576", "submitter": "Xun Zheng", "authors": "Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang Wei, Xun Zheng,\n  Eric P. Xing, Tie-Yan Liu, Wei-Ying Ma", "title": "LightLDA: Big Topic Models on Modest Compute Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building large-scale machine learning (ML) programs, such as big topic\nmodels or deep neural nets, one usually assumes such tasks can only be\nattempted with industrial-sized clusters with thousands of nodes, which are out\nof reach for most practitioners or academic researchers. We consider this\nchallenge in the context of topic modeling on web-scale corpora, and show that\nwith a modest cluster of as few as 8 machines, we can train a topic model with\n1 million topics and a 1-million-word vocabulary (for a total of 1 trillion\nparameters), on a document collection with 200 billion tokens -- a scale not\nyet reported even with thousands of machines. Our major contributions include:\n1) a new, highly efficient O(1) Metropolis-Hastings sampling algorithm, whose\nrunning cost is (surprisingly) agnostic of model size, and empirically\nconverges nearly an order of magnitude faster than current state-of-the-art\nGibbs samplers; 2) a structure-aware model-parallel scheme, which leverages\ndependencies within the topic model, yielding a sampling strategy that is\nfrugal on machine memory and network communication; 3) a differential\ndata-structure for model storage, which uses separate data structures for high-\nand low-frequency words to allow extremely large models to fit in memory, while\nmaintaining high inference speed; and 4) a bounded asynchronous data-parallel\nscheme, which allows efficient distributed processing of massive data via a\nparameter server. Our distribution strategy is an instance of the\nmodel-and-data-parallel programming model underlying the Petuum framework for\ngeneral distributed ML, and was implemented on top of the Petuum open-source\nsystem. We provide experimental evidence showing how this development puts\nmassive models within reach on a small cluster while still enjoying\nproportional time cost reductions with increasing cluster size, in comparison\nwith alternative options.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 07:49:12 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Yuan", "Jinhui", ""], ["Gao", "Fei", ""], ["Ho", "Qirong", ""], ["Dai", "Wei", ""], ["Wei", "Jinliang", ""], ["Zheng", "Xun", ""], ["Xing", "Eric P.", ""], ["Liu", "Tie-Yan", ""], ["Ma", "Wei-Ying", ""]]}, {"id": "1412.1587", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Ronen Eldan", "title": "The entropic barrier: a simple and optimal universal self-concordant\n  barrier", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the Cram\\'er transform of the uniform measure on a convex body\nin $\\mathbb{R}^n$ is a $(1+o(1)) n$-self-concordant barrier, improving a\nseminal result of Nesterov and Nemirovski. This gives the first explicit\nconstruction of a universal barrier for convex bodies with optimal\nself-concordance parameter. The proof is based on basic geometry of log-concave\ndistributions, and elementary duality in exponential families.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 08:45:04 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 01:20:03 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2015 00:12:22 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Eldan", "Ronen", ""]]}, {"id": "1412.1602", "submitter": "Jan Chorowski", "authors": "Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio", "title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent\n  NN: First Results", "comments": "As accepted to: Deep Learning and Representation Learning Workshop,\n  NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We replace the Hidden Markov Model (HMM) which is traditionally used in in\ncontinuous speech recognition with a bi-directional recurrent neural network\nencoder coupled to a recurrent neural network decoder that directly emits a\nstream of phonemes. The alignment between the input and output sequences is\nestablished using an attention mechanism: the decoder emits each symbol based\non a context created with a subset of input symbols elected by the attention\nmechanism. We report initial results demonstrating that this new approach\nachieves phoneme error rates that are comparable to the state-of-the-art\nHMM-based decoders, on the TIMIT dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 10:00:19 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Chorowski", "Jan", ""], ["Bahdanau", "Dzmitry", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.1619", "submitter": "Ilja Kuzborskij", "authors": "Ilja Kuzborskij, Francesco Orabona", "title": "Fast Rates by Transferring from Auxiliary Hypotheses", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-016-5594-4", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the learning setting where, in addition to the\ntraining set, the learner receives a collection of auxiliary hypotheses\noriginating from other tasks. We focus on a broad class of ERM-based linear\nalgorithms that can be instantiated with any non-negative smooth loss function\nand any strongly convex regularizer. We establish generalization and excess\nrisk bounds, showing that, if the algorithm is fed with a good combination of\nsource hypotheses, generalization happens at the fast rate $\\mathcal{O}(1/m)$\ninstead of the usual $\\mathcal{O}(1/\\sqrt{m})$. On the other hand, if the\nsource hypotheses combination is a misfit for the target task, we recover the\nusual learning rate. As a byproduct of our study, we also prove a new bound on\nthe Rademacher complexity of the smooth loss class under weaker assumptions\ncompared to previous works.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 11:01:11 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 10:32:37 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2015 17:34:49 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Kuzborskij", "Ilja", ""], ["Orabona", "Francesco", ""]]}, {"id": "1412.1628", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Sunggyun Park, Joon-Young Lee, In So Kweon", "title": "Fisher Kernel for Deep Neural Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to image representation based on low-level local descriptors, deep\nneural activations of Convolutional Neural Networks (CNNs) are richer in\nmid-level representation, but poorer in geometric invariance properties. In\nthis paper, we present a straightforward framework for better image\nrepresentation by combining the two approaches. To take advantages of both\nrepresentations, we propose an efficient method to extract a fair amount of\nmulti-scale dense local activations from a pre-trained CNN. We then aggregate\nthe activations by Fisher kernel framework, which has been modified with a\nsimple scale-wise normalization essential to make it suitable for CNN\nactivations. Replacing the direct use of a single activation vector with our\nrepresentation demonstrates significant performance improvements: +17.76 (Acc.)\non MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that\nour proposal can be used as a primary image representation for better\nperformances in visual recognition tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 11:30:57 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 07:16:18 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Yoo", "Donggeun", ""], ["Park", "Sunggyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1412.1740", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Nicholas I. Kolkin, Stephen Tyree, Kilian Q.\n  Weinberger", "title": "Image Data Compression for Covariance and Histogram Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance and histogram image descriptors provide an effective way to\ncapture information about images. Both excel when used in combination with\nspecial purpose distance metrics. For covariance descriptors these metrics\nmeasure the distance along the non-Euclidean Riemannian manifold of symmetric\npositive definite matrices. For histogram descriptors the Earth Mover's\ndistance measures the optimal transport between two histograms. Although more\nprecise, these distance metrics are very expensive to compute, making them\nimpractical in many applications, even for data sets of only a few thousand\nexamples. In this paper we present two methods to compress the size of\ncovariance and histogram datasets with only marginal increases in test error\nfor k-nearest neighbor classification. Specifically, we show that we can reduce\ndata sets to 16% and in some cases as little as 2% of their original size,\nwhile approximately matching the test error of kNN classification on the full\ntraining set. In fact, because the compressed set is learned in a supervised\nfashion, it sometimes even outperforms the full data set, while requiring only\na fraction of the space and drastically reducing test-time computation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 17:22:22 GMT"}, {"version": "v2", "created": "Sat, 23 May 2015 17:07:59 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kusner", "Matt J.", ""], ["Kolkin", "Nicholas I.", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1412.1788", "submitter": "Francis Bach", "authors": "Felipe Yanez (LIENS, INRIA Paris - Rocquencourt), Francis Bach (LIENS,\n  INRIA Paris - Rocquencourt)", "title": "Primal-Dual Algorithms for Non-negative Matrix Factorization with the\n  Kullback-Leibler Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) approximates a given matrix as a\nproduct of two non-negative matrices. Multiplicative algorithms deliver\nreliable results, but they show slow convergence for high-dimensional data and\nmay be stuck away from local minima. Gradient descent methods have better\nbehavior, but only apply to smooth losses such as the least-squares loss. In\nthis article, we propose a first-order primal-dual algorithm for non-negative\ndecomposition problems (where one factor is fixed) with the KL divergence,\nbased on the Chambolle-Pock algorithm. All required computations may be\nobtained in closed form and we provide an efficient heuristic way to select\nstep-sizes. By using alternating optimization, our algorithm readily extends to\nNMF and, on synthetic examples, face recognition or music source separation\ndatasets, it is either faster than existing algorithms, or leads to improved\nlocal optima, or both.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 19:56:02 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Yanez", "Felipe", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS,\n  INRIA Paris - Rocquencourt"]]}, {"id": "1412.1866", "submitter": "Jakub Mare\\v{c}ek", "authors": "Catherine Kerr, Terri Hoare, Paula Carroll, Jakub Marecek", "title": "Integer-Programming Ensemble of Temporal-Relations Classifiers", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery, 2020", "doi": "10.1007/s10618-019-00671-x", "report-no": null, "categories": "cs.CL cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction and understanding of temporal events and their relations are\nmajor challenges in natural language processing. Processing text on a\nsentence-by-sentence or expression-by-expression basis often fails, in part due\nto the challenge of capturing the global consistency of the text. We present an\nensemble method, which reconciles the outputs of multiple classifiers of\ntemporal expressions across the text using integer programming. Computational\nexperiments show that the ensemble improves upon the best individual results\nfrom two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and\nSemEval-2016 Task 12 (Clinical TempEval).\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 00:30:09 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 17:35:34 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 22:53:55 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 22:15:58 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Kerr", "Catherine", ""], ["Hoare", "Terri", ""], ["Carroll", "Paula", ""], ["Marecek", "Jakub", ""]]}, {"id": "1412.1947", "submitter": "Aditya AV Sastry Mr.", "authors": "Aditya AV Sastry and Kalyan Netti", "title": "A parallel sampling based clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The problem of automatically clustering data is an age old problem. People\nhave created numerous algorithms to tackle this problem. The execution time of\nany of this algorithm grows with the number of input points and the number of\ncluster centers required. To reduce the number of input points we could average\nthe points locally and use the means or the local centers as the input for\nclustering. However since the required number of local centers is very high,\nrunning the clustering algorithm on the entire dataset to obtain these\nrepresentational points is very time consuming. To remedy this problem, in this\npaper we are proposing two subclustering schemes where by we subdivide the\ndataset into smaller sets and run the clustering algorithm on the smaller\ndatasets to obtain the required number of datapoints to run our clustering\nalgorithm with. As we are subdividing the given dataset, we could run\nclustering algorithm on each smaller piece of the dataset in parallel. We found\nthat both parallel and serial execution of this method to be much faster than\nthe original clustering algorithm and error in running the clustering algorithm\non a reduced set to be very less.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 10:50:31 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Sastry", "Aditya AV", ""], ["Netti", "Kalyan", ""]]}, {"id": "1412.2066", "submitter": "Charless Fowlkes", "authors": "Shaofei Wang and Charless C. Fowlkes", "title": "Learning Multi-target Tracking with Quadratic Object Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a model for multi-target tracking based on associating\ncollections of candidate detections across frames of a video. In order to model\npairwise interactions between different tracks, such as suppression of\noverlapping tracks and contextual cues about co-occurence of different objects,\nwe augment a standard min-cost flow objective with quadratic terms between\ndetection variables. We learn the parameters of this model using structured\nprediction and a loss function which approximates the multi-target tracking\naccuracy. We evaluate two different approaches to finding an optimal set of\ntracks under model objective based on an LP relaxation and a novel greedy\nextension to dynamic programming that handles pairwise interactions. We find\nthe greedy algorithm achieves equivalent performance to the LP relaxation while\nbeing 2-7x faster than a commercial solver. The resulting model with learned\nparameters outperforms existing methods across several categories on the KITTI\ntracking benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 17:04:35 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 05:25:44 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Wang", "Shaofei", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1412.2106", "submitter": "Wojciech Kot{\\l}owski", "authors": "Wojciech Kot{\\l}owski", "title": "Consistent optimization of AMS by logistic loss minimization", "comments": "9 pages, HEPML workshop at NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we theoretically justify an approach popular among\nparticipants of the Higgs Boson Machine Learning Challenge to optimize\napproximate median significance (AMS). The approach is based on the following\ntwo-stage procedure. First, a real-valued function is learned by minimizing a\nsurrogate loss for binary classification, such as logistic loss, on the\ntraining sample. Then, a threshold is tuned on a separate validation sample, by\ndirect optimization of AMS. We show that the regret of the resulting\n(thresholded) classifier measured with respect to the squared AMS, is\nupperbounded by the regret of the underlying real-valued function measured with\nrespect to the logistic loss. Hence, we prove that minimizing logistic\nsurrogate is a consistent method of optimizing AMS.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 19:28:15 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Kot\u0142owski", "Wojciech", ""]]}, {"id": "1412.2113", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Makoto Yamada, Dawei Yin, Yi Chang", "title": "Consistent Collective Matrix Completion under Joint Low Rank Structure", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the collective matrix completion problem of jointly recovering a\ncollection of matrices with shared structure from partial (and potentially\nnoisy) observations. To ensure well--posedness of the problem, we impose a\njoint low rank structure, wherein each component matrix is low rank and the\nlatent space of the low rank factors corresponding to each entity is shared\nacross the entire collection. We first develop a rigorous algebra for\nrepresenting and manipulating collective--matrix structure, and identify\nsufficient conditions for consistent estimation of collective matrices. We then\npropose a tractable convex estimator for solving the collective matrix\ncompletion problem, and provide the first non--trivial theoretical guarantees\nfor consistency of collective matrix completion. We show that under reasonable\nassumptions stated in Section 3.1, with high probability, the proposed\nestimator exactly recovers the true matrices whenever sample complexity\nrequirements dictated by Theorem 1 are met. The sample complexity requirement\nderived in the paper are optimum up to logarithmic factors, and significantly\nimprove upon the requirements obtained by trivial extensions of standard matrix\ncompletion. Finally, we propose a scalable approximate algorithm to solve the\nproposed convex program, and corroborate our results through simulated\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 19:42:22 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 04:42:13 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 21:18:44 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Yamada", "Makoto", ""], ["Yin", "Dawei", ""], ["Chang", "Yi", ""]]}, {"id": "1412.2196", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, Zhouchen Lin, Chao Zhang, Junbin Gao", "title": "Relations among Some Low Rank Subspace Recovery Models", "comments": "Submitted to Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering intrinsic low dimensional subspaces from data distributed on them\nis a key preprocessing step to many applications. In recent years, there has\nbeen a lot of work that models subspace recovery as low rank minimization\nproblems. We find that some representative models, such as Robust Principal\nComponent Analysis (R-PCA), Robust Low Rank Representation (R-LRR), and Robust\nLatent Low Rank Representation (R-LatLRR), are actually deeply connected. More\nspecifically, we discover that once a solution to one of the models is\nobtained, we can obtain the solutions to other models in closed-form\nformulations. Since R-PCA is the simplest, our discovery makes it the center of\nlow rank subspace recovery models. Our work has two important implications.\nFirst, R-PCA has a solid theoretical foundation. Under certain conditions, we\ncould find better solutions to these low rank models at overwhelming\nprobabilities, although these models are non-convex. Second, we can obtain\nsignificantly faster algorithms for these models by solving R-PCA first. The\ncomputation cost can be further cut by applying low complexity randomized\nalgorithms, e.g., our novel $\\ell_{2,1}$ filtering algorithm, to R-PCA.\nExperiments verify the advantages of our algorithms over other state-of-the-art\nones that are based on the alternating direction method.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 03:25:12 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Zhang", "Hongyang", ""], ["Lin", "Zhouchen", ""], ["Zhang", "Chao", ""], ["Gao", "Junbin", ""]]}, {"id": "1412.2231", "submitter": "Canyi Lu", "authors": "Canyi Lu, Changbo Zhu, Chunyan Xu, Shuicheng Yan, Zhouchen Lin", "title": "Generalized Singular Value Thresholding", "comments": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the Generalized Singular Value Thresholding (GSVT) operator\n${\\text{Prox}}_{g}^{{\\sigma}}(\\cdot)$, \\begin{equation*}\n  {\\text{Prox}}_{g}^{{\\sigma}}(B)=\\arg\\min\\limits_{X}\\sum_{i=1}^{m}g(\\sigma_{i}(X))\n+ \\frac{1}{2}||X-B||_{F}^{2}, \\end{equation*} associated with a nonconvex\nfunction $g$ defined on the singular values of $X$. We prove that GSVT can be\nobtained by performing the proximal operator of $g$ (denoted as\n$\\text{Prox}_g(\\cdot)$) on the singular values since $\\text{Prox}_g(\\cdot)$ is\nmonotone when $g$ is lower bounded. If the nonconvex $g$ satisfies some\nconditions (many popular nonconvex surrogate functions, e.g., $\\ell_p$-norm,\n$0<p<1$, of $\\ell_0$-norm are special cases), a general solver to find\n$\\text{Prox}_g(b)$ is proposed for any $b\\geq0$. GSVT greatly generalizes the\nknown Singular Value Thresholding (SVT) which is a basic subroutine in many\nconvex low rank minimization methods. We are able to solve the nonconvex low\nrank minimization problem by using GSVT in place of SVT.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 13:08:29 GMT"}, {"version": "v2", "created": "Sun, 27 May 2018 05:56:25 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Lu", "Canyi", ""], ["Zhu", "Changbo", ""], ["Xu", "Chunyan", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1412.2302", "submitter": "Weiguang Ding", "authors": "Weiguang Ding, Ruoyan Wang, Fei Mao, Graham Taylor", "title": "Theano-based Large-Scale Visual Recognition with Multiple GPUs", "comments": "ICLR 2015 workshop camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012)\nimplementation and its naive data parallelism on multiple GPUs. Our performance\non 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014)\nrun on 1 GPU. To the best of our knowledge, this is the first open-source\nPython-based AlexNet implementation to-date.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 01:12:10 GMT"}, {"version": "v2", "created": "Thu, 1 Jan 2015 07:37:05 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 04:53:46 GMT"}, {"version": "v4", "created": "Mon, 6 Apr 2015 20:46:11 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Ding", "Weiguang", ""], ["Wang", "Ruoyan", ""], ["Mao", "Fei", ""], ["Taylor", "Graham", ""]]}, {"id": "1412.2309", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka and Pietro Perona and Frederick Eberhardt", "title": "Visual Causal Feature Learning", "comments": "Accepted at UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a rigorous definition of the visual cause of a behavior that is\nbroadly applicable to the visually driven behavior in humans, animals, neurons,\nrobots and other perceiving systems. Our framework generalizes standard\naccounts of causal learning to settings in which the causal variables need to\nbe constructed from micro-variables. We prove the Causal Coarsening Theorem,\nwhich allows us to gain causal knowledge from observational data with minimal\nexperimental effort. The theorem provides a connection to standard inference\ntechniques in machine learning that identify features of an image that\ncorrelate with, but may not cause, the target behavior. Finally, we propose an\nactive learning scheme to learn a manipulator function that performs optimal\nmanipulations on the image to automatically identify the visual cause of a\ntarget behavior. We illustrate our inference and learning algorithms in\nexperiments based on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 03:13:27 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 22:35:30 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Perona", "Pietro", ""], ["Eberhardt", "Frederick", ""]]}, {"id": "1412.2314", "submitter": "Bo Waggoner", "authors": "Bo Waggoner", "title": "$\\ell_p$ Testing and Learning of Discrete Distributions", "comments": "This is the full version of the paper appearing at ITCS 2015. Two\n  columns. 24 pages, of which 14 appendix", "journal-ref": null, "doi": "10.1145/2688073.2688095", "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic problems of testing uniformity of and learning a discrete\ndistribution, given access to independent samples from it, are examined under\ngeneral $\\ell_p$ metrics. The intuitions and results often contrast with the\nclassic $\\ell_1$ case. For $p > 1$, we can learn and test with a number of\nsamples that is independent of the support size of the distribution: With an\n$\\ell_p$ tolerance $\\epsilon$, $O(\\max\\{ \\sqrt{1/\\epsilon^q}, 1/\\epsilon^2 \\})$\nsamples suffice for testing uniformity and $O(\\max\\{ 1/\\epsilon^q,\n1/\\epsilon^2\\})$ samples suffice for learning, where $q=p/(p-1)$ is the\nconjugate of $p$. As this parallels the intuition that $O(\\sqrt{n})$ and $O(n)$\nsamples suffice for the $\\ell_1$ case, it seems that $1/\\epsilon^q$ acts as an\nupper bound on the \"apparent\" support size.\n  For some $\\ell_p$ metrics, uniformity testing becomes easier over larger\nsupports: a 6-sided die requires fewer trials to test for fairness than a\n2-sided coin, and a card-shuffler requires fewer trials than the die. In fact,\nthis inverse dependence on support size holds if and only if $p > \\frac{4}{3}$.\nThe uniformity testing algorithm simply thresholds the number of \"collisions\"\nor \"coincidences\" and has an optimal sample complexity up to constant factors\nfor all $1 \\leq p \\leq 2$. Another algorithm gives order-optimal sample\ncomplexity for $\\ell_{\\infty}$ uniformity testing. Meanwhile, the most natural\nlearning algorithm is shown to have order-optimal sample complexity for all\n$\\ell_p$ metrics.\n  The author thanks Cl\\'{e}ment Canonne for discussions and contributions to\nthis work.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 03:57:29 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 17:53:34 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 13:34:20 GMT"}, {"version": "v4", "created": "Sat, 21 Mar 2015 17:30:44 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Waggoner", "Bo", ""]]}, {"id": "1412.2404", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju", "title": "Dimensionality Reduction with Subspace Structure Preservation", "comments": "Published in NIPS 2014; v2: minor updates to the algorithm and added\n  a few lines addressing application to large-scale/high-dimensional data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling data as being sampled from a union of independent subspaces has been\nwidely applied to a number of real world applications. However, dimensionality\nreduction approaches that theoretically preserve this independence assumption\nhave not been well studied. Our key contribution is to show that $2K$\nprojection vectors are sufficient for the independence preservation of any $K$\nclass data sampled from a union of independent subspaces. It is this\nnon-trivial observation that we use for designing our dimensionality reduction\ntechnique. In this paper, we propose a novel dimensionality reduction algorithm\nthat theoretically preserves this structure for a given dataset. We support our\ntheoretical analysis with empirical results on both synthetic and real world\ndata achieving \\textit{state-of-the-art} results compared to popular\ndimensionality reduction techniques.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 22:02:33 GMT"}, {"version": "v2", "created": "Sun, 31 May 2015 22:30:47 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 23:11:46 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Arpit", "Devansh", ""], ["Nwogu", "Ifeoma", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1412.2432", "submitter": "Edward Meeds", "authors": "Edward Meeds and Remco Hendriks and Said Al Faraby and Magiel Bruntink\n  and Max Welling", "title": "MLitB: Machine Learning in the Browser", "comments": "Revised for PeerJ Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With few exceptions, the field of Machine Learning (ML) research has largely\nignored the browser as a computational engine. Beyond an educational resource\nfor ML, the browser has vast potential to not only improve the state-of-the-art\nin ML research, but also, inexpensively and on a massive scale, to bring\nsophisticated ML learning and prediction to the public at large. This paper\nintroduces MLitB, a prototype ML framework written entirely in JavaScript,\ncapable of performing large-scale distributed computing with heterogeneous\nclasses of devices. The development of MLitB has been driven by several\nunderlying objectives whose aim is to make ML learning and usage ubiquitous (by\nusing ubiquitous compute devices), cheap and effortlessly distributed, and\ncollaborative. This is achieved by allowing every internet capable device to\nrun training algorithms and predictive models with no software installation and\nby saving models in universally readable formats. Our prototype library is\ncapable of training deep neural networks with synchronized, distributed\nstochastic gradient descent. MLitB offers several important opportunities for\nnovel ML research, including: development of distributed learning algorithms,\nadvancement of web GPU algorithms, novel field and mobile applications, privacy\npreserving computing, and green grid-computing. MLitB is available as open\nsource software.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 02:23:40 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 13:11:41 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Meeds", "Edward", ""], ["Hendriks", "Remco", ""], ["Faraby", "Said Al", ""], ["Bruntink", "Magiel", ""], ["Welling", "Max", ""]]}, {"id": "1412.2457", "submitter": "Thomas Steinke", "authors": "Mark Bun and Thomas Steinke", "title": "Weighted Polynomial Approximations: Limits for Learning and\n  Pseudorandomness", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial approximations to boolean functions have led to many positive\nresults in computer science. In particular, polynomial approximations to the\nsign function underly algorithms for agnostically learning halfspaces, as well\nas pseudorandom generators for halfspaces. In this work, we investigate the\nlimits of these techniques by proving inapproximability results for the sign\nfunction.\n  Firstly, the polynomial regression algorithm of Kalai et al. (SIAM J. Comput.\n2008) shows that halfspaces can be learned with respect to log-concave\ndistributions on $\\mathbb{R}^n$ in the challenging agnostic learning model. The\npower of this algorithm relies on the fact that under log-concave\ndistributions, halfspaces can be approximated arbitrarily well by low-degree\npolynomials. We ask whether this technique can be extended beyond log-concave\ndistributions, and establish a negative result. We show that polynomials of any\ndegree cannot approximate the sign function to within arbitrarily low error for\na large class of non-log-concave distributions on the real line, including\nthose with densities proportional to $\\exp(-|x|^{0.99})$.\n  Secondly, we investigate the derandomization of Chernoff-type concentration\ninequalities. Chernoff-type tail bounds on sums of independent random variables\nhave pervasive applications in theoretical computer science. Schmidt et al.\n(SIAM J. Discrete Math. 1995) showed that these inequalities can be established\nfor sums of random variables with only $O(\\log(1/\\delta))$-wise independence,\nfor a tail probability of $\\delta$. We show that their results are tight up to\nconstant factors.\n  These results rely on techniques from weighted approximation theory, which\nstudies how well functions on the real line can be approximated by polynomials\nunder various distributions. We believe that these techniques will have further\napplications in other areas of computer science.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 05:58:56 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Bun", "Mark", ""], ["Steinke", "Thomas", ""]]}, {"id": "1412.2485", "submitter": "Vikram Nathan", "authors": "Vikram Nathan, Sharath Raghvendra", "title": "Accurate Streaming Support Vector Machines", "comments": "2 figures, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely-used tool for binary classification is the Support Vector Machine\n(SVM), a supervised learning technique that finds the \"maximum margin\" linear\nseparator between the two classes. While SVMs have been well studied in the\nbatch (offline) setting, there is considerably less work on the streaming\n(online) setting, which requires only a single pass over the data using\nsub-linear space. Existing streaming algorithms are not yet competitive with\nthe batch implementation. In this paper, we use the formulation of the SVM as a\nminimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based\noff of the blurred ball cover originally proposed by Agarwal and Sharathkumar.\nOur implementation consistently outperforms existing streaming SVM approaches\nand provides higher accuracies than libSVM on several datasets, thus making it\ncompetitive with the standard SVM batch implementation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 08:46:07 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Nathan", "Vikram", ""], ["Raghvendra", "Sharath", ""]]}, {"id": "1412.2689", "submitter": "Afdel Karim", "authors": "Ali Aajli, Karim Afdel", "title": "A New Approach of Learning Hierarchy Construction Based on Fuzzy Logic", "comments": "58-66", "journal-ref": "Journal of Engineering Research and Applications,ISSN : 2248-9622,\n  Vol. 4, Issue 10( Part - 3), October 2014, pp.58-66", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In recent years, adaptive learning systems rely increasingly on learning\nhierarchy to customize the educational logic developed in their courses. Most\napproaches do not consider that the relationships of prerequisites between the\nskills are fuzzy relationships. In this article, we describe a new approach of\na practical application of fuzzy logic techniques to the construction of\nlearning hierarchies. For this, we use a learning hierarchy predefined by one\nor more experts of a specific field. However, the relationships of\nprerequisites between the skills in the learning hierarchy are not definitive\nand they are fuzzy relationships. Indeed, we measure relevance degree of all\nrelationships existing in this learning hierarchy and we try to answer to the\nfollowing question: Is the relationships of prerequisites predefined in initial\nlearning hierarchy are correctly established or not?\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 18:34:07 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Aajli", "Ali", ""], ["Afdel", "Karim", ""]]}, {"id": "1412.2693", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi and Anima Anandkumar", "title": "Provable Methods for Training Neural Networks with Sparse Connectivity", "comments": "Accepted for presentation at Neural Information Processing\n  Systems(NIPS) 2014 Deep Learning workshop and Accepted as a workshop\n  contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide novel guaranteed approaches for training feedforward neural\nnetworks with sparse connectivity. We leverage on the techniques developed\npreviously for learning linear networks and show that they can also be\neffectively adopted to learn non-linear networks. We operate on the moments\ninvolving label and the score function of the input, and show that their\nfactorization provably yields the weight matrix of the first layer of a deep\nnetwork under mild conditions. In practice, the output of our method can be\nemployed as effective initializers for gradient descent.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 18:45:22 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 20:40:26 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 11:38:16 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2015 11:23:38 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1412.2812", "submitter": "Ehsan Khoddam", "authors": "Ivan Titov and Ehsan Khoddam", "title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error\n  Minimization Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to unsupervised estimation of feature-rich\nsemantic role labeling models. Our model consists of two components: (1) an\nencoding component: a semantic role labeling model which predicts roles given a\nrich set of syntactic and lexical features; (2) a reconstruction component: a\ntensor factorization model which relies on roles to predict argument fillers.\nWhen the components are estimated jointly to minimize errors in argument\nreconstruction, the induced roles largely correspond to roles defined in\nannotated resources. Our method performs on par with most accurate role\ninduction methods on English and German, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 23:40:41 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Titov", "Ivan", ""], ["Khoddam", "Ehsan", ""]]}, {"id": "1412.2859", "submitter": "James P. Crutchfield", "authors": "Sarah Marzen and James P. Crutchfield", "title": "Circumventing the Curse of Dimensionality in Prediction: Causal\n  Rate-Distortion for Infinite-Order Markov Processes", "comments": "25 pages, 14 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/cn.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG nlin.CD q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive rate-distortion analysis suffers from the curse of dimensionality:\nclustering arbitrarily long pasts to retain information about arbitrarily long\nfutures requires resources that typically grow exponentially with length. The\nchallenge is compounded for infinite-order Markov processes, since conditioning\non finite sequences cannot capture all of their past dependencies. Spectral\narguments show that algorithms which cluster finite-length sequences fail\ndramatically when the underlying process has long-range temporal correlations\nand can fail even for processes generated by finite-memory hidden Markov\nmodels. We circumvent the curse of dimensionality in rate-distortion analysis\nof infinite-order processes by casting predictive rate-distortion objective\nfunctions in terms of the forward- and reverse-time causal states of\ncomputational mechanics. Examples demonstrate that the resulting causal\nrate-distortion theory substantially improves current predictive\nrate-distortion analyses.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 05:23:27 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Marzen", "Sarah", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1412.2863", "submitter": "Majid Janzamin", "authors": "Majid Janzamin, Hanie Sedghi, Anima Anandkumar", "title": "Score Function Features for Discriminative Learning: Matrix and Tensor\n  Framework", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning forms the cornerstone for tackling challenging learning\nproblems in domains such as speech, computer vision and natural language\nprocessing. In this paper, we consider a novel class of matrix and\ntensor-valued features, which can be pre-trained using unlabeled samples. We\npresent efficient algorithms for extracting discriminative information, given\nthese pre-trained features and labeled samples for any related task. Our class\nof features are based on higher-order score functions, which capture local\nvariations in the probability density function of the input. We establish a\ntheoretical framework to characterize the nature of discriminative information\nthat can be extracted from score-function features, when used in conjunction\nwith labeled samples. We employ efficient spectral decomposition algorithms (on\nmatrices and tensors) for extracting discriminative components. The advantage\nof employing tensor-valued features is that we can extract richer\ndiscriminative information in the form of an overcomplete representations.\nThus, we present a novel framework for employing generative models of the input\nfor discriminative learning.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 06:22:19 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 19:12:31 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Janzamin", "Majid", ""], ["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1412.2929", "submitter": "Yao-Hsiang Yang", "authors": "Yao-Hsiang Yang, Lu-Hung Chen, Chieh-Chih Wang, and Chu-Song Chen", "title": "Bayesian Fisher's Discriminant for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian framework of Gaussian process in order to extend\nFisher's discriminant to classify functional data such as spectra and images.\nThe probability structure for our extended Fisher's discriminant is explicitly\nformulated, and we utilize the smoothness assumptions of functional data as\nprior probabilities. Existing methods which directly employ the smoothness\nassumption of functional data can be shown as special cases within this\nframework given corresponding priors while their estimates of the unknowns are\none-step approximations to the proposed MAP estimates. Empirical results on\nvarious simulation studies and different real applications show that the\nproposed method significantly outperforms the other Fisher's discriminant\nmethods for functional data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 12:01:03 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Yang", "Yao-Hsiang", ""], ["Chen", "Lu-Hung", ""], ["Wang", "Chieh-Chih", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1412.2954", "submitter": "Ying Xiao", "authors": "Santosh S. Vempala and Ying Xiao", "title": "Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample\n  Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, general technique for reducing the sample complexity of\nmatrix and tensor decomposition algorithms applied to distributions. We use the\ntechnique to give a polynomial-time algorithm for standard ICA with sample\ncomplexity nearly linear in the dimension, thereby improving substantially on\nprevious bounds. The analysis is based on properties of random polynomials,\nnamely the spacings of an ensemble of polynomials. Our technique also applies\nto other applications of tensor decompositions, including spherical Gaussian\nmixture models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 13:33:04 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 14:31:06 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 19:00:25 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Vempala", "Santosh S.", ""], ["Xiao", "Ying", ""]]}, {"id": "1412.3046", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi, Majid Janzamin, Anima Anandkumar", "title": "Provable Tensor Methods for Learning Mixtures of Generalized Linear\n  Models", "comments": "To appear in Proceeding of AI and Statistics (AISTATS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning mixtures of generalized linear models\n(GLM) which arise in classification and regression problems. Typical learning\napproaches such as expectation maximization (EM) or variational Bayes can get\nstuck in spurious local optima. In contrast, we present a tensor decomposition\nmethod which is guaranteed to correctly recover the parameters. The key insight\nis to employ certain feature transformations of the input, which depend on the\ninput generative model. Specifically, we employ score function tensors of the\ninput and compute their cross-correlation with the response variable. We\nestablish that the decomposition of this tensor consistently recovers the\nparameters, under mild non-degeneracy conditions. We demonstrate that the\ncomputational and sample complexity of our method is a low order polynomial of\nthe input and the latent dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 18:27:48 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 04:43:36 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 06:54:54 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2016 00:19:00 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Sedghi", "Hanie", ""], ["Janzamin", "Majid", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1412.3078", "submitter": "Jun Wei Ng", "authors": "Jun Wei Ng and Marc Peter Deisenroth", "title": "Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a practical and scalable Gaussian process model for large-scale\nnonlinear probabilistic regression. Our mixture-of-experts model is\nconceptually simple and hierarchically recombines computations for an overall\napproximation of a full Gaussian process. Closed-form and distributed\ncomputations allow for efficient and massive parallelisation while keeping the\nmemory consumption small. Given sufficient computing resources, our model can\nhandle arbitrarily large data sets, without explicit sparse approximations. We\nprovide strong experimental evidence that our model can be applied to large\ndata sets of sizes far beyond millions. Hence, our model has the potential to\nlay the foundation for general large-scale Gaussian process research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 20:03:06 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Ng", "Jun Wei", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1412.3100", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer", "title": "Semi-Supervised Learning with Heterophily", "comments": "17 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a family of linear inference algorithms that generalize existing\ngraph-based label propagation algorithms by allowing them to propagate\ngeneralized assumptions about \"attraction\" or \"compatibility\" between classes\nof neighboring nodes (in particular those that involve heterophily between\nnodes where \"opposites attract\"). We thus call this formulation Semi-Supervised\nLearning with Heterophily (SSLH) and show how it generalizes and improves upon\na recently proposed approach called Linearized Belief Propagation (LinBP).\nImportantly, our framework allows us to reduce the problem of estimating the\nrelative compatibility between nodes from partially labeled graph to a simple\noptimization problem. The result is a very fast algorithm that -- despite its\nsimplicity -- is surprisingly effective: we can classify unlabeled nodes within\nthe same graph in the same time as LinBP but with a superior accuracy and\ndespite our algorithm not knowing the compatibilities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 20:58:45 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 02:27:12 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Gatterbauer", "Wolfgang", ""]]}, {"id": "1412.3121", "submitter": "Seungwhan Moon", "authors": "Seungwhan Moon and Suyoun Kim and Haohan Wang", "title": "Multimodal Transfer Deep Learning with Applications in Audio-Visual\n  Recognition", "comments": "6 pages, MMML workshop at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a transfer deep learning (TDL) framework that can transfer the\nknowledge obtained from a single-modal neural network to a network with a\ndifferent modality. Specifically, we show that we can leverage speech data to\nfine-tune the network trained for video recognition, given an initial set of\naudio-video parallel dataset within the same semantics. Our approach first\nlearns the analogy-preserving embeddings between the abstract representations\nlearned from intermediate layers of each network, allowing for semantics-level\ntransfer between the source and target modalities. We then apply our neural\nnetwork operation that fine-tunes the target network with the additional\nknowledge transferred from the source network, while keeping the topology of\nthe target network unchanged. While we present an audio-visual recognition task\nas an application of our approach, our framework is flexible and thus can work\nwith any multimodal dataset, or with any already-existing deep networks that\nshare the common underlying semantics. In this work in progress report, we aim\nto provide comprehensive results of different configurations of the proposed\napproach on two widely used audio-visual datasets, and we discuss potential\napplications of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 21:12:19 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 19:56:41 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Moon", "Seungwhan", ""], ["Kim", "Suyoun", ""], ["Wang", "Haohan", ""]]}, {"id": "1412.3276", "submitter": "Christos Dimitrakakis", "authors": "Emmanouil G. Androulakis, Christos Dimitrakakis", "title": "Generalised Entropy MDPs and Minimax Regret", "comments": "7 pages, NIPS workshop \"From bad models to good policies\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods suffer from the problem of how to specify prior beliefs. One\ninteresting idea is to consider worst-case priors. This requires solving a\nstochastic zero-sum game. In this paper, we extend well-known results from\nbandit theory in order to discover minimax-Bayes policies and discuss when they\nare practical.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 12:28:34 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Androulakis", "Emmanouil G.", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1412.3352", "submitter": "Neda Pourali", "authors": "Neda Pourali", "title": "Web image annotation by diffusion maps manifold learning algorithm", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": "10.5121/ijfcst.2014.4606", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image annotation is one of the most challenging problems in machine\nvision areas. The goal of this task is to predict number of keywords\nautomatically for images captured in real data. Many methods are based on\nvisual features in order to calculate similarities between image samples. But\nthe computation cost of these approaches is very high. These methods require\nmany training samples to be stored in memory. To lessen this burden, a number\nof techniques have been developed to reduce the number of features in a\ndataset. Manifold learning is a popular approach to nonlinear dimensionality\nreduction. In this paper, we investigate Diffusion maps manifold learning\nmethod for web image auto-annotation task. Diffusion maps manifold learning\nmethod is used to reduce the dimension of some visual features. Extensive\nexperiments and analysis on NUS-WIDE-LITE web image dataset with different\nvisual features show how this manifold learning dimensionality reduction method\ncan be applied effectively to image annotation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 10:38:28 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Pourali", "Neda", ""]]}, {"id": "1412.3397", "submitter": "Gang Chen", "authors": "Gang Chen, Ran Xu and Sargur Srihari", "title": "Sequential Labeling with online Deep Learning", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Deep learning has attracted great attention recently and yielded the state of\nthe art performance in dimension reduction and classification problems.\nHowever, it cannot effectively handle the structured output prediction, e.g.\nsequential labeling. In this paper, we propose a deep learning structure, which\ncan learn discriminative features for sequential labeling problems. More\nspecifically, we add the inter-relationship between labels in our deep learning\nstructure, in order to incorporate the context information from the sequential\ndata. Thus, our model is more powerful than linear Conditional Random Fields\n(CRFs) because the objective function learns latent non-linear features so that\ntarget labeling can be better predicted. We pretrain the deep structure with\nstacked restricted Boltzmann machines (RBMs) for feature learning and optimize\nour objective function with online learning algorithm, a mixture of perceptron\ntraining and stochastic gradient descent. We test our model on different\nchallenge tasks, and show that our model outperforms significantly over the\ncompletive baselines.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 18:16:12 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2015 20:38:28 GMT"}, {"version": "v3", "created": "Mon, 4 May 2015 01:41:46 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Chen", "Gang", ""], ["Xu", "Ran", ""], ["Srihari", "Sargur", ""]]}, {"id": "1412.3409", "submitter": "Christopher Clark", "authors": "Christopher Clark and Amos Storkey", "title": "Teaching Deep Convolutional Neural Networks to Play Go", "comments": "9 pages, 8 figures, 5 tables. Corrected typos, minor adjustment to\n  table format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mastering the game of Go has remained a long standing challenge to the field\nof AI. Modern computer Go systems rely on processing millions of possible\nfuture positions to play well, but intuitively a stronger and more 'humanlike'\nway to play the game would be to rely on pattern recognition abilities rather\nthen brute force computation. Following this sentiment, we train deep\nconvolutional neural networks to play Go by training them to predict the moves\nmade by expert Go players. To solve this problem we introduce a number of novel\ntechniques, including a method of tying weights in the network to 'hard code'\nsymmetries that are expect to exist in the target function, and demonstrate in\nan ablation study they considerably improve performance. Our final networks are\nable to achieve move prediction accuracies of 41.1% and 44.4% on two different\nGo datasets, surpassing previous state of the art on this task by significant\nmargins. Additionally, while previous move prediction programs have not yielded\nstrong Go playing programs, we show that the networks trained in this work\nacquired high levels of skill. Our convolutional neural networks can\nconsistently defeat the well known Go program GNU Go, indicating it is state of\nthe art among programs that do not use Monte Carlo Tree Search. It is also able\nto win some games against state of the art Go playing program Fuego while using\na fraction of the play time. This success at playing Go indicates high level\nprinciples of the game were learned.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 18:59:43 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 10:31:31 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Clark", "Christopher", ""], ["Storkey", "Amos", ""]]}, {"id": "1412.3411", "submitter": "Jacquelyn Shelton", "authors": "Jacquelyn A. Shelton, Jan Gasthaus, Zhenwen Dai, Joerg Luecke, Arthur\n  Gretton", "title": "GP-select: Accelerating EM using adaptive subspace preselection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric procedure to achieve fast inference in generative\ngraphical models when the number of latent states is very large. The approach\nis based on iterative latent variable preselection, where we alternate between\nlearning a 'selection function' to reveal the relevant latent variables, and\nuse this to obtain a compact approximation of the posterior distribution for\nEM; this can make inference possible where the number of possible latent states\nis e.g. exponential in the number of latent variables, whereas an exact\napproach would be computationally unfeasible. We learn the selection function\nentirely from the observed data and current EM state via Gaussian process\nregression. This is by contrast with earlier approaches, where selection\nfunctions were manually-designed for each problem setting. We show that our\napproach performs as well as these bespoke selection functions on a wide\nvariety of inference problems: in particular, for the challenging case of a\nhierarchical model for object localization with occlusion, we achieve results\nthat match a customized state-of-the-art selection method, at a far lower\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 19:04:52 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 08:20:25 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Shelton", "Jacquelyn A.", ""], ["Gasthaus", "Jan", ""], ["Dai", "Zhenwen", ""], ["Luecke", "Joerg", ""], ["Gretton", "Arthur", ""]]}, {"id": "1412.3489", "submitter": "Nathan Wiebe", "authors": "Nathan Wiebe, Ashish Kapoor, Krysta M. Svore", "title": "Quantum Deep Learning", "comments": "34 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has had a profound impact on machine learning\nand artificial intelligence. At the same time, algorithms for quantum computers\nhave been shown to efficiently solve some problems that are intractable on\nconventional, classical computers. We show that quantum computing not only\nreduces the time required to train a deep restricted Boltzmann machine, but\nalso provides a richer and more comprehensive framework for deep learning than\nclassical computing and leads to significant improvements in the optimization\nof the underlying objective function. Our quantum methods also permit efficient\ntraining of full Boltzmann machines and multi-layer, fully connected models and\ndo not have well known classical counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 23:05:16 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 00:20:28 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Wiebe", "Nathan", ""], ["Kapoor", "Ashish", ""], ["Svore", "Krysta M.", ""]]}, {"id": "1412.3555", "submitter": "Junyoung Chung", "authors": "Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence\n  Modeling", "comments": "Presented in NIPS 2014 Deep Learning and Representation Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we compare different types of recurrent units in recurrent\nneural networks (RNNs). Especially, we focus on more sophisticated units that\nimplement a gating mechanism, such as a long short-term memory (LSTM) unit and\na recently proposed gated recurrent unit (GRU). We evaluate these recurrent\nunits on the tasks of polyphonic music modeling and speech signal modeling. Our\nexperiments revealed that these advanced recurrent units are indeed better than\nmore traditional recurrent units such as tanh units. Also, we found GRU to be\ncomparable to LSTM.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 06:46:53 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Chung", "Junyoung", ""], ["Gulcehre", "Caglar", ""], ["Cho", "KyungHyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.3635", "submitter": "Maria Schuld", "authors": "Maria Schuld, Ilya Sinayskiy and Francesco Petruccione", "title": "Simulating a perceptron on a quantum computer", "comments": "11 pages, 6 figures, accepted by Physics Letters A", "journal-ref": "Physics Letters A, 379, pp. 660-663 (2015)", "doi": "10.1016/j.physleta.2014.11.061", "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptrons are the basic computational unit of artificial neural networks,\nas they model the activation mechanism of an output neuron due to incoming\nsignals from its neighbours. As linear classifiers, they play an important role\nin the foundations of machine learning. In the context of the emerging field of\nquantum machine learning, several attempts have been made to develop a\ncorresponding unit using quantum information theory. Based on the quantum phase\nestimation algorithm, this paper introduces a quantum perceptron model\nimitating the step-activation function of a classical perceptron. This scheme\nrequires resources in $\\mathcal{O}(n)$ (where $n$ is the size of the input) and\npromises efficient applications for more complex structures such as trainable\nquantum neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 12:49:36 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Schuld", "Maria", ""], ["Sinayskiy", "Ilya", ""], ["Petruccione", "Francesco", ""]]}, {"id": "1412.3684", "submitter": "Soren Goyal", "authors": "Soren Goyal, Paul Benjamin", "title": "Object Recognition Using Deep Neural Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of objects using Deep Neural Networks is an active area of\nresearch and many breakthroughs have been made in the last few years. The paper\nattempts to indicate how far this field has progressed. The paper briefly\ndescribes the history of research in Neural Networks and describe several of\nthe recent advances in this field. The performances of recently developed\nNeural Network Algorithm over benchmark datasets have been tabulated. Finally,\nsome the applications of this field have been provided.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 18:23:13 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Goyal", "Soren", ""], ["Benjamin", "Paul", ""]]}, {"id": "1412.3705", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Venkatesh Saligrama", "title": "A Topic Modeling Approach to Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a topic modeling approach to the prediction of preferences in\npairwise comparisons. We develop a new generative model for pairwise\ncomparisons that accounts for multiple shared latent rankings that are\nprevalent in a population of users. This new model also captures inconsistent\nuser behavior in a natural way. We show how the estimation of latent rankings\nin the new generative model can be formally reduced to the estimation of topics\nin a statistically equivalent topic modeling problem. We leverage recent\nadvances in the topic modeling literature to develop an algorithm that can\nlearn shared latent rankings with provable consistency as well as sample and\ncomputational complexity guarantees. We demonstrate that the new approach is\nempirically competitive with the current state-of-the-art approaches in\npredicting preferences on some semi-synthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:15:53 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 22:01:20 GMT"}, {"version": "v3", "created": "Sun, 25 Jan 2015 22:32:10 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1412.3708", "submitter": "Marc Goessling", "authors": "Marc Goessling and Yali Amit", "title": "Compact Compositional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning compact and interpretable representations is a very natural task,\nwhich has not been solved satisfactorily even for simple binary datasets. In\nthis paper, we review various ways of composing experts for binary data and\nargue that competitive forms of interaction are best suited to learn\nlow-dimensional representations. We propose a new composition rule that\ndiscourages experts from focusing on similar structures and that penalizes\nopposing votes strongly so that abstaining from voting becomes more attractive.\nWe also introduce a novel sequential initialization procedure, which is based\non a process of oversimplification and correction. Experiments show that with\nour approach very intuitive models can be learned.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:19:56 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 19:23:27 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2015 22:02:42 GMT"}, {"version": "v4", "created": "Sat, 29 Oct 2016 22:49:39 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Goessling", "Marc", ""], ["Amit", "Yali", ""]]}, {"id": "1412.3714", "submitter": "Jiwei Li", "authors": "Jiwei Li", "title": "Feature Weight Tuning for Recursive Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses how a recursive neural network model can automatically\nleave out useless information and emphasize important evidence, in other words,\nto perform \"weight tuning\" for higher-level representation acquisition. We\npropose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural\nNetwork (BENN), which automatically control how much one specific unit\ncontributes to the higher-level representation. The proposed model can be\nviewed as incorporating a more powerful compositional function for embedding\nacquisition in recursive neural networks. Experimental results demonstrate the\nsignificant improvement over standard neural models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:35:27 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 00:57:57 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Li", "Jiwei", ""]]}, {"id": "1412.3773", "submitter": "Joris Mooij", "authors": "Joris M. Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler,\n  Bernhard Sch\\\"olkopf", "title": "Distinguishing cause from effect using observational data: methods and\n  benchmarks", "comments": "101 pages, second revision submitted to Journal of Machine Learning\n  Research", "journal-ref": "Journal of Machine Learning Research 17(32):1-102, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of causal relationships from purely observational data is a\nfundamental problem in science. The most elementary form of such a causal\ndiscovery problem is to decide whether X causes Y or, alternatively, Y causes\nX, given joint observations of two variables X, Y. An example is to decide\nwhether altitude causes temperature, or vice versa, given only joint\nmeasurements of both variables. Even under the simplifying assumptions of no\nconfounding, no feedback loops, and no selection bias, such bivariate causal\ndiscovery problems are challenging. Nevertheless, several approaches for\naddressing those problems have been proposed in recent years. We review two\nfamilies of such methods: Additive Noise Methods (ANM) and Information\nGeometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs\nthat consists of data for 100 different cause-effect pairs selected from 37\ndatasets from various domains (e.g., meteorology, biology, medicine,\nengineering, economy, etc.) and motivate our decisions regarding the \"ground\ntruth\" causal directions of all pairs. We evaluate the performance of several\nbivariate causal discovery methods on these real-world benchmark data and in\naddition on artificially simulated data. Our empirical results on real-world\ndata indicate that certain methods are indeed able to distinguish cause from\neffect using only purely observational data, although more benchmark data would\nbe needed to obtain statistically significant conclusions. One of the best\nperforming methods overall is the additive-noise method originally proposed by\nHoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of\n0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of\nthis work we prove the consistency of that method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 19:34:39 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 14:51:36 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2015 11:37:57 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Mooij", "Joris M.", ""], ["Peters", "Jonas", ""], ["Janzing", "Dominik", ""], ["Zscheischler", "Jakob", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1412.3919", "submitter": "Alexandre Abraham", "authors": "Alexandre Abraham (NEUROSPIN, INRIA Saclay - Ile de France), Fabian\n  Pedregosa (INRIA Saclay - Ile de France), Michael Eickenberg (LNAO, INRIA\n  Saclay - Ile de France), Philippe Gervais (NEUROSPIN, INRIA Saclay - Ile de\n  France, LNAO), Andreas Muller, Jean Kossaifi, Alexandre Gramfort (NEUROSPIN,\n  LTCI), Bertrand Thirion (NEUROSPIN, INRIA Saclay - Ile de France), G\\\"ael\n  Varoquaux (NEUROSPIN, INRIA Saclay - Ile de France, LNAO)", "title": "Machine Learning for Neuroimaging with Scikit-Learn", "comments": "Frontiers in neuroscience, Frontiers Research Foundation, 2013, pp.15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine learning methods are increasingly used for neuroimaging\ndata analysis. Their main virtue is their ability to model high-dimensional\ndatasets, e.g. multivariate analysis of activation images or resting-state time\nseries. Supervised learning is typically used in decoding or encoding settings\nto relate brain images to behavioral or clinical observations, while\nunsupervised learning can uncover hidden structures in sets of images (e.g.\nresting state functional MRI) or find sub-populations in large cohorts. By\nconsidering different functional neuroimaging applications, we illustrate how\nscikit-learn, a Python machine learning library, can be used to perform some\nkey analysis steps. Scikit-learn contains a very large set of statistical\nlearning algorithms, both supervised and unsupervised, and its application to\nneuroimaging data provides a versatile tool to study the brain.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 08:38:35 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Abraham", "Alexandre", "", "NEUROSPIN, INRIA Saclay - Ile de France"], ["Pedregosa", "Fabian", "", "INRIA Saclay - Ile de France"], ["Eickenberg", "Michael", "", "LNAO, INRIA\n  Saclay - Ile de France"], ["Gervais", "Philippe", "", "NEUROSPIN, INRIA Saclay - Ile de\n  France, LNAO"], ["Muller", "Andreas", "", "NEUROSPIN,\n  LTCI"], ["Kossaifi", "Jean", "", "NEUROSPIN,\n  LTCI"], ["Gramfort", "Alexandre", "", "NEUROSPIN,\n  LTCI"], ["Thirion", "Bertrand", "", "NEUROSPIN, INRIA Saclay - Ile de France"], ["Varoquaux", "G\u00e4el", "", "NEUROSPIN, INRIA Saclay - Ile de France, LNAO"]]}, {"id": "1412.3922", "submitter": "Kunal Dutta", "authors": "Kunal Dutta, Arijit Ghosh", "title": "Size sensitive packing number for Hamming cube and its consequences", "comments": "At the time of submission, we have become aware of a similar packing\n  result proven simultaneously by Ezra. However, we note that our proof of the\n  main packing lemma is quite different from hers. Also, the focus of our paper\n  is on discrepancy bounds and sampling complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CG cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a size-sensitive version of Haussler's Packing\nlemma~\\cite{Haussler92spherepacking} for set-systems with bounded primal\nshatter dimension, which have an additional {\\em size-sensitive property}. This\nanswers a question asked by Ezra~\\cite{Ezra-sizesendisc-soda-14}. We also\npartially address another point raised by Ezra regarding overcounting of sets\nin her chaining procedure. As a consequence of these improvements, we get an\nimprovement on the size-sensitive discrepancy bounds for set systems with the\nabove property. Improved bounds on the discrepancy for these special set\nsystems also imply an improvement in the sizes of {\\em relative $(\\varepsilon,\n\\delta)$-approximations} and $(\\nu, \\alpha)$-samples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 08:46:28 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Dutta", "Kunal", ""], ["Ghosh", "Arijit", ""]]}, {"id": "1412.4005", "submitter": "Jerome Bobin", "authors": "Jerome Bobin and Jeremy Rapin and Anthony Larue and Jean-Luc Starck", "title": "Sparsity and adaptivity for the blind separation of partially correlated\n  sources", "comments": "submitted to IEEE Transactions on signal processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2391071", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation (BSS) is a very popular technique to analyze\nmultichannel data. In this context, the data are modeled as the linear\ncombination of sources to be retrieved. For that purpose, standard BSS methods\nall rely on some discrimination principle, whether it is statistical\nindependence or morphological diversity, to distinguish between the sources.\nHowever, dealing with real-world data reveals that such assumptions are rarely\nvalid in practice: the signals of interest are more likely partially\ncorrelated, which generally hampers the performances of standard BSS methods.\nIn this article, we introduce a novel sparsity-enforcing BSS method coined\nAdaptive Morphological Component Analysis (AMCA), which is designed to retrieve\nsparse and partially correlated sources. More precisely, it makes profit of an\nadaptive re-weighting scheme to favor/penalize samples based on their level of\ncorrelation. Extensive numerical experiments have been carried out which show\nthat the proposed method is robust to the partial correlation of sources while\nstandard BSS techniques fail. The AMCA algorithm is evaluated in the field of\nastrophysics for the separation of physical components from microwave data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 14:41:14 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Bobin", "Jerome", ""], ["Rapin", "Jeremy", ""], ["Larue", "Anthony", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1412.4080", "submitter": "Antoine Bonnefoy", "authors": "Antoine Bonnefoy, Valentin Emiya, Liva Ralaivola, R\\'emi Gribonval\n  (INRIA - IRISA)", "title": "Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and\n  Group-Lasso", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2447503", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent computational strategies based on screening tests have been proposed\nto accelerate algorithms addressing penalized sparse regression problems such\nas the Lasso. Such approaches build upon the idea that it is worth dedicating\nsome small computational effort to locate inactive atoms and remove them from\nthe dictionary in a preprocessing stage so that the regression algorithm\nworking with a smaller dictionary will then converge faster to the solution of\nthe initial problem. We believe that there is an even more efficient way to\nscreen the dictionary and obtain a greater acceleration: inside each iteration\nof the regression algorithm, one may take advantage of the algorithm\ncomputations to obtain a new screening test for free with increasing screening\neffects along the iterations. The dictionary is henceforth dynamically screened\ninstead of being screened statically, once and for all, before the first\niteration. We formalize this dynamic screening principle in a general\nalgorithmic scheme and apply it by embedding inside a number of first-order\nalgorithms adapted existing screening tests to solve the Lasso or new screening\ntests to solve the Group-Lasso. Computational gains are assessed in a large set\nof experiments on synthetic data as well as real-world sounds and images. They\nshow both the screening efficiency and the gain in terms running times.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 18:39:56 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Bonnefoy", "Antoine", "", "INRIA - IRISA"], ["Emiya", "Valentin", "", "INRIA - IRISA"], ["Ralaivola", "Liva", "", "INRIA - IRISA"], ["Gribonval", "R\u00e9mi", "", "INRIA - IRISA"]]}, {"id": "1412.4182", "submitter": "Stefan Wager", "authors": "Jacob Steinhardt, Stefan Wager, and Percy Liang", "title": "The Statistics of Streaming Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse analogue to stochastic gradient descent that is\nguaranteed to perform well under similar conditions to the lasso. In the linear\nregression setup with irrepresentable noise features, our algorithm recovers\nthe support set of the optimal parameter vector with high probability, and\nachieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),\nwhere k is the sparsity of the solution, d is the number of features, and T is\nthe number of training examples. Meanwhile, our algorithm does not require any\nmore computational resources than stochastic gradient descent. In our\nexperiments, we find that our method substantially out-performs existing\nstreaming algorithms on both real and simulated data.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 02:32:06 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Wager", "Stefan", ""], ["Liang", "Percy", ""]]}, {"id": "1412.4186", "submitter": "Eugene Borovikov", "authors": "Eugene Borovikov", "title": "An Evaluation of Support Vector Machines as a Pattern Recognition Tool", "comments": "A short (6 page) report on evaluation of the SVM as a pattern\n  classification tool, as of 1999", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The purpose of this report is in examining the generalization performance of\nSupport Vector Machines (SVM) as a tool for pattern recognition and object\nclassification. The work is motivated by the growing popularity of the method\nthat is claimed to guarantee a good generalization performance for the task in\nhand. The method is implemented in MATLAB. SVMs based on various kernels are\ntested for classifying data from various domains.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 03:33:13 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Borovikov", "Eugene", ""]]}, {"id": "1412.4385", "submitter": "Yi Yang", "authors": "Yi Yang and Jacob Eisenstein", "title": "Unsupervised Domain Adaptation with Feature Embeddings", "comments": "For more details, please refer to the long version of this paper:\n  http://www.cc.gatech.edu/~jeisenst/papers/yang-naacl-2015.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is the dominant technique for unsupervised domain\nadaptation, but existing approaches often require the specification of \"pivot\nfeatures\" that generalize across domains, which are selected by task-specific\nheuristics. We show that a novel but simple feature embedding approach provides\nbetter performance, by exploiting the feature template structure common in NLP\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 17:44:58 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 19:35:12 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 01:44:48 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Yang", "Yi", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1412.4446", "submitter": "Pascal Germain", "authors": "Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\\c{c}ois Laviolette,\n  Mario Marchand", "title": "Domain-Adversarial Neural Networks", "comments": "The first version of this paper was accepted at the \"Second Workshop\n  on Transfer and Multi-Task Learning: Theory meets Practice\" (NIPS 2014,\n  Montreal, Canada). See: https://sites.google.com/site/multitaskwsnips2014/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new representation learning algorithm suited to the context of\ndomain adaptation, in which data at training and test time come from similar\nbut different distributions. Our algorithm is directly inspired by theory on\ndomain adaptation suggesting that, for effective domain transfer to be\nachieved, predictions must be made based on a data representation that cannot\ndiscriminate between the training (source) and test (target) domains. We\npropose a training objective that implements this idea in the context of a\nneural network, whose hidden layer is trained to be predictive of the\nclassification task, but uninformative as to the domain of the input. Our\nexperiments on a sentiment analysis classification benchmark, where the target\ndomain data available at training time is unlabeled, show that our neural\nnetwork for domain adaption algorithm has better performance than either a\nstandard neural network or an SVM, even if trained on input features extracted\nwith the state-of-the-art marginalized stacked denoising autoencoders of Chen\net al. (2012).\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 02:16:07 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 17:52:03 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Ajakan", "Hana", ""], ["Germain", "Pascal", ""], ["Larochelle", "Hugo", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""]]}, {"id": "1412.4564", "submitter": "Karel Lenc", "authors": "Andrea Vedaldi, Karel Lenc", "title": "MatConvNet - Convolutional Neural Networks for MATLAB", "comments": "Updated for release v1.0-beta20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for\nMATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.\nIt exposes the building blocks of CNNs as easy-to-use MATLAB functions,\nproviding routines for computing linear convolutions with filter banks, feature\npooling, and many more. In this manner, MatConvNet allows fast prototyping of\nnew CNN architectures; at the same time, it supports efficient computation on\nCPU and GPU allowing to train complex models on large datasets such as ImageNet\nILSVRC. This document provides an overview of CNNs and how they are implemented\nin MatConvNet and gives the technical details of each computational block in\nthe toolbox.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 12:23:35 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2015 15:35:25 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 14:31:06 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Vedaldi", "Andrea", ""], ["Lenc", "Karel", ""]]}, {"id": "1412.4659", "submitter": "Ju Sun", "authors": "Qing Qu, Ju Sun, John Wright", "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating\n  directions", "comments": "Accepted by IEEE Trans. Information Theory. The paper has been\n  revised by the reviewers' comments. The proofs have been streamlined", "journal-ref": "IEEE Transaction on Information Theory, 62(10):5855 - 5880, 2016", "doi": "10.1109/TIT.2016.2601599", "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to find the sparsest vector (direction) in a generic subspace\n$\\mathcal{S} \\subseteq \\mathbb{R}^p$ with $\\mathrm{dim}(\\mathcal{S})= n < p$?\nThis problem can be considered a homogeneous variant of the sparse recovery\nproblem, and finds connections to sparse dictionary learning, sparse PCA, and\nmany other problems in signal processing and machine learning. In this paper,\nwe focus on a **planted sparse model** for the subspace: the target sparse\nvector is embedded in an otherwise random subspace. Simple convex heuristics\nfor this planted recovery problem provably break down when the fraction of\nnonzero entries in the target sparse vector substantially exceeds\n$O(1/\\sqrt{n})$. In contrast, we exhibit a relatively simple nonconvex approach\nbased on alternating directions, which provably succeeds even when the fraction\nof nonzero entries is $\\Omega(1)$. To the best of our knowledge, this is the\nfirst practical algorithm to achieve linear scaling under the planted sparse\nmodel. Empirically, our proposed algorithm also succeeds in more challenging\ndata models, e.g., sparse dictionary learning.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 16:27:29 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 03:23:33 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 00:54:41 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Qu", "Qing", ""], ["Sun", "Ju", ""], ["Wright", "John", ""]]}, {"id": "1412.4736", "submitter": "Phil Long", "authors": "David P. Helmbold and Philip M. Long", "title": "On the Inductive Bias of Dropout", "comments": null, "journal-ref": "Journal of Machine Learning Research, 16, 3403-3454 (2015). (See\n  http://jmlr.org/papers/volume16/helmbold15a/helmbold15a.pdf.)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a simple but effective technique for learning in neural networks\nand other settings. A sound theoretical understanding of dropout is needed to\ndetermine when dropout should be applied and how to use it most effectively. In\nthis paper we continue the exploration of dropout as a regularizer pioneered by\nWager, et.al. We focus on linear classification where a convex proxy to the\nmisclassification loss (i.e. the logistic loss used in logistic regression) is\nminimized. We show: (a) when the dropout-regularized criterion has a unique\nminimizer, (b) when the dropout-regularization penalty goes to infinity with\nthe weights, and when it remains bounded, (c) that the dropout regularization\ncan be non-monotonic as individual weights increase from 0, and (d) that the\ndropout regularization penalty may not be convex. This last point is\nparticularly surprising because the combination of dropout regularization with\nany convex loss proxy is always a convex function.\n  In order to contrast dropout regularization with $L_2$ regularization, we\nformalize the notion of when different sources are more compatible with\ndifferent regularizers. We then exhibit distributions that are provably more\ncompatible with dropout regularization than $L_2$ regularization, and vice\nversa. These sources provide additional insight into how the inductive biases\nof dropout and $L_2$ regularization differ. We provide some similar results for\n$L_1$ regularization.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 19:40:46 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 02:58:50 GMT"}, {"version": "v3", "created": "Mon, 22 Dec 2014 22:22:30 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 18:59:20 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1412.4863", "submitter": "Changsheng Li", "authors": "Changsheng Li and Qingshan Liu and Weishan Dong and Xin Zhang and Lin\n  Yang", "title": "Max-Margin based Discriminative Feature Learning", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new max-margin based discriminative feature\nlearning method. Specifically, we aim at learning a low-dimensional feature\nrepresentation, so as to maximize the global margin of the data and make the\nsamples from the same class as close as possible. In order to enhance the\nrobustness to noise, a $l_{2,1}$ norm constraint is introduced to make the\ntransformation matrix in group sparsity. In addition, for multi-class\nclassification tasks, we further intend to learn and leverage the correlation\nrelationships among multiple class tasks for assisting in learning\ndiscriminative features. The experimental results demonstrate the power of the\nproposed method against the related state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 02:55:01 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 02:43:47 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Li", "Changsheng", ""], ["Liu", "Qingshan", ""], ["Dong", "Weishan", ""], ["Zhang", "Xin", ""], ["Yang", "Lin", ""]]}, {"id": "1412.4864", "submitter": "Philip Bachman", "authors": "Philip Bachman and Ouais Alsharif and Doina Precup", "title": "Learning with Pseudo-Ensembles", "comments": "To appear in Advances in Neural Information Processing Systems 27\n  (NIPS 2014), Advances in Neural Information Processing Systems 27, Dec. 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the notion of a pseudo-ensemble, a (possibly infinite)\ncollection of child models spawned from a parent model by perturbing it\naccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep\nneural network trains a pseudo-ensemble of child subnetworks generated by\nrandomly masking nodes in the parent network. We present a novel regularizer\nbased on making the behavior of a pseudo-ensemble robust with respect to the\nnoise process generating it. In the fully-supervised setting, our regularizer\nmatches the performance of dropout. But, unlike dropout, our regularizer\nnaturally extends to the semi-supervised setting, where it produces\nstate-of-the-art results. We provide a case study in which we transform the\nRecursive Neural Tensor Network of (Socher et. al, 2013) into a\npseudo-ensemble, which significantly improves its performance on a real-world\nsentiment analysis benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 02:55:05 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Bachman", "Philip", ""], ["Alsharif", "Ouais", ""], ["Precup", "Doina", ""]]}, {"id": "1412.4986", "submitter": "Hsiang-Fu Yu", "authors": "Hsiang-Fu Yu and Cho-Jui Hsieh and Hyokun Yun and S.V.N Vishwanathan\n  and Inderjit S. Dhillon", "title": "A Scalable Asynchronous Distributed Algorithm for Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful topic models with massive document collections which\ncontain millions of documents and billions of tokens is challenging because of\ntwo reasons: First, one needs to deal with a large number of topics (typically\nin the order of thousands). Second, one needs a scalable and efficient way of\ndistributing the computation across multiple machines. In this paper we present\na novel algorithm F+Nomad LDA which simultaneously tackles both these problems.\nIn order to handle large number of topics we use an appropriately modified\nFenwick tree. This data structure allows us to sample from a multinomial\ndistribution over $T$ items in $O(\\log T)$ time. Moreover, when topic counts\nchange the data structure can be updated in $O(\\log T)$ time. In order to\ndistribute the computation across multiple processor we present a novel\nasynchronous framework inspired by the Nomad algorithm of\n\\cite{YunYuHsietal13}. We show that F+Nomad LDA significantly outperform\nstate-of-the-art on massive problems which involve millions of documents,\nbillions of words, and thousands of topics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 12:52:50 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Yu", "Hsiang-Fu", ""], ["Hsieh", "Cho-Jui", ""], ["Yun", "Hyokun", ""], ["Vishwanathan", "S. V. N", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1412.5068", "submitter": "Shixiang Gu", "authors": "Shixiang Gu, Luca Rigazio", "title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown deep neural networks (DNNs) to be highly susceptible to\nwell-designed, small perturbations at the input layer, or so-called adversarial\nexamples. Taking images as an example, such distortions are often\nimperceptible, but can result in 100% mis-classification for a state of the art\nDNN. We study the structure of adversarial examples and explore network\ntopology, pre-processing and training strategies to improve the robustness of\nDNNs. We perform various experiments to assess the removability of adversarial\nexamples by corrupting with additional noise and pre-processing with denoising\nautoencoders (DAEs). We find that DAEs can remove substantial amounts of the\nadversarial noise. How- ever, when stacking the DAE with the original DNN, the\nresulting network can again be attacked by new adversarial examples with even\nsmaller distortion. As a solution, we propose Deep Contractive Network, a model\nwith a new end-to-end training procedure that includes a smoothness penalty\ninspired by the contractive autoencoder (CAE). This increases the network\nrobustness to adversarial examples, without a significant performance penalty.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 23:03:49 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 16:35:05 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 14:14:24 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2015 21:43:29 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Gu", "Shixiang", ""], ["Rigazio", "Luca", ""]]}, {"id": "1412.5083", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro, Alex Bronstein", "title": "Random Forests Can Hash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash codes are a very efficient data representation needed to be able to cope\nwith the ever growing amounts of data. We introduce a random forest semantic\nhashing scheme with information-theoretic code aggregation, showing for the\nfirst time how random forest, a technique that together with deep learning have\nshown spectacular results in classification, can also be extended to\nlarge-scale retrieval. Traditional random forest fails to enforce the\nconsistency of hashes generated from each tree for the same class data, i.e.,\nto preserve the underlying similarity, and it also lacks a principled way for\ncode aggregation across trees. We start with a simple hashing scheme, where\nindependently trained random trees in a forest are acting as hashing functions.\nWe the propose a subspace model as the splitting function, and show that it\nenforces the hash consistency in a tree for data from the same class. We also\nintroduce an information-theoretic approach for aggregating codes of individual\ntrees into a single hash code, producing a near-optimal unique hash for each\nclass. Experiments on large-scale public datasets are presented, showing that\nthe proposed approach significantly outperforms state-of-the-art hashing\nmethods for retrieval tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 17:02:18 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 18:26:12 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 01:00:24 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex", ""]]}, {"id": "1412.5104", "submitter": "Angjoo Kanazawa", "authors": "Angjoo Kanazawa, Abhishek Sharma, David Jacobs", "title": "Locally Scale-Invariant Convolutional Neural Networks", "comments": "Deep Learning and Representation Learning Workshop: NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (ConvNets) have shown excellent results on many\nvisual classification tasks. With the exception of ImageNet, these datasets are\ncarefully crafted such that objects are well-aligned at similar scales.\nNaturally, the feature learning problem gets more challenging as the amount of\nvariation in the data increases, as the models have to learn to be invariant to\ncertain changes in appearance. Recent results on the ImageNet dataset show that\ngiven enough data, ConvNets can learn such invariances producing very\ndiscriminative features [1]. But could we do more: use less parameters, less\ndata, learn more discriminative features, if certain invariances were built\ninto the learning process? In this paper we present a simple model that allows\nConvNets to learn features in a locally scale-invariant manner without\nincreasing the number of model parameters. We show on a modified MNIST dataset\nthat when faced with scale variation, building in scale-invariance allows\nConvNets to learn more discriminative features with reduced chances of\nover-fitting.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 18:09:34 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Kanazawa", "Angjoo", ""], ["Sharma", "Abhishek", ""], ["Jacobs", "David", ""]]}, {"id": "1412.5218", "submitter": "David Duvenaud", "authors": "Roger B. Grosse and David K. Duvenaud", "title": "Testing MCMC code", "comments": "Presented at the 2014 NIPS workshop on Software Engineering for\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilistic\nmodeling and inference, but are difficult to debug, and are prone to silent\nfailure if implemented naively. We outline several strategies for testing the\ncorrectness of MCMC algorithms. Specifically, we advocate writing code in a\nmodular way, where conditional probability calculations are kept separate from\nthe logic of the sampler. We discuss strategies for both unit testing and\nintegration testing. As a running example, we show how a Python implementation\nof Gibbs sampling for a mixture of Gaussians model can be tested.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 22:37:20 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Grosse", "Roger B.", ""], ["Duvenaud", "David K.", ""]]}, {"id": "1412.5236", "submitter": "Andrew Dai", "authors": "Andrew M. Dai, Amos J. Storkey", "title": "The supervised hierarchical Dirichlet process", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2315802", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the supervised hierarchical Dirichlet process (sHDP), a\nnonparametric generative model for the joint distribution of a group of\nobservations and a response variable directly associated with that whole group.\nWe compare the sHDP with another leading method for regression on grouped data,\nthe supervised latent Dirichlet allocation (sLDA) model. We evaluate our method\non two real-world classification problems and two real-world regression\nproblems. Bayesian nonparametric regression models based on the Dirichlet\nprocess, such as the Dirichlet process-generalised linear models (DP-GLM) have\npreviously been explored; these models allow flexibility in modelling nonlinear\nrelationships. However, until now, Hierarchical Dirichlet Process (HDP)\nmixtures have not seen significant use in supervised problems with grouped data\nsince a straightforward application of the HDP on the grouped data results in\nlearnt clusters that are not predictive of the responses. The sHDP solves this\nproblem by allowing for clusters to be learnt jointly from the group structure\nand from the label assigned to each group.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 01:16:31 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Dai", "Andrew M.", ""], ["Storkey", "Amos J.", ""]]}, {"id": "1412.5244", "submitter": "Yujia Li", "authors": "Yujia Li, Kevin Swersky, Richard Zemel", "title": "Learning unbiased features", "comments": "Published in NIPS 2014 Workshop on Transfer and Multitask Learning,\n  see http://nips.cc/Conferences/2014/Program/event.php?ID=4282", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key element in transfer learning is representation learning; if\nrepresentations can be developed that expose the relevant factors underlying\nthe data, then new tasks and domains can be learned readily based on mappings\nof these salient factors. We propose that an important aim for these\nrepresentations are to be unbiased. Different forms of representation learning\ncan be derived from alternative definitions of unwanted bias, e.g., bias to\nparticular tasks, domains, or irrelevant underlying data dimensions. One very\nuseful approach to estimating the amount of bias in a representation comes from\nmaximum mean discrepancy (MMD) [5], a measure of distance between probability\ndistributions. We are not the first to suggest that MMD can be a useful\ncriterion in developing representations that apply across multiple domains or\ntasks [1]. However, in this paper we describe a number of novel applications of\nthis criterion that we have devised, all based on the idea of developing\nunbiased representations. These formulations include: a standard domain\nadaptation framework; a method of learning invariant representations; an\napproach based on noise-insensitive autoencoders; and a novel form of\ngenerative model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 02:47:22 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Li", "Yujia", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard", ""]]}, {"id": "1412.5272", "submitter": "Qiang Wu", "authors": "Jun Fan and Ting Hu and Qiang Wu and Ding-Xuan Zhou", "title": "Consistency Analysis of an Empirical Minimum Error Entropy Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the consistency of an empirical minimum error entropy\n(MEE) algorithm in a regression setting. We introduce two types of consistency.\nThe error entropy consistency, which requires the error entropy of the learned\nfunction to approximate the minimum error entropy, is shown to be always true\nif the bandwidth parameter tends to 0 at an appropriate rate. The regression\nconsistency, which requires the learned function to approximate the regression\nfunction, however, is a complicated issue. We prove that the error entropy\nconsistency implies the regression consistency for homoskedastic models where\nthe noise is independent of the input variable. But for heteroskedastic models,\na counterexample is used to show that the two types of consistency do not\ncoincide. A surprising result is that the regression consistency is always\ntrue, provided that the bandwidth parameter tends to infinity at an appropriate\nrate. Regression consistency of two classes of special models is shown to hold\nwith fixed bandwidth parameter, which further illustrates the complexity of\nregression consistency of MEE. Fourier transform plays crucial roles in our\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 07:11:01 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Fan", "Jun", ""], ["Hu", "Ting", ""], ["Wu", "Qiang", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1412.5335", "submitter": "Gr\\'egoire Mesnil", "authors": "Gr\\'egoire Mesnil, Tomas Mikolov, Marc'Aurelio Ranzato, Yoshua Bengio", "title": "Ensemble of Generative and Discriminative Techniques for Sentiment\n  Analysis of Movie Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is a common task in natural language processing that aims\nto detect polarity of a text document (typically a consumer review). In the\nsimplest settings, we discriminate only between positive and negative\nsentiment, turning the task into a standard binary classification problem. We\ncompare several ma- chine learning approaches to this problem, and combine them\nto achieve the best possible results. We show how to use for this task the\nstandard generative lan- guage models, which are slightly complementary to the\nstate of the art techniques. We achieve strong results on a well-known dataset\nof IMDB movie reviews. Our results are easily reproducible, as we publish also\nthe code needed to repeat the experiments. This should simplify further advance\nof the state of the art, as other researchers can combine their techniques with\nours with little effort.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 11:02:04 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 14:17:16 GMT"}, {"version": "v3", "created": "Fri, 19 Dec 2014 11:36:14 GMT"}, {"version": "v4", "created": "Tue, 3 Feb 2015 20:03:35 GMT"}, {"version": "v5", "created": "Wed, 4 Feb 2015 05:17:55 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 14:26:14 GMT"}, {"version": "v7", "created": "Wed, 27 May 2015 06:40:09 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Mesnil", "Gr\u00e9goire", ""], ["Mikolov", "Tomas", ""], ["Ranzato", "Marc'Aurelio", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.5474", "submitter": "Jonghoon Jin", "authors": "Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello", "title": "Flattened Convolutional Neural Networks for Feedforward Acceleration", "comments": "International Conference on Learning Representations (ICLR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present flattened convolutional neural networks that are designed for fast\nfeedforward execution. The redundancy of the parameters, especially weights of\nthe convolutional filters in convolutional neural networks has been extensively\nstudied and different heuristics have been proposed to construct a low rank\nbasis of the filters after training. In this work, we train flattened networks\nthat consist of consecutive sequence of one-dimensional filters across all\ndirections in 3D space to obtain comparable performance as conventional\nconvolutional networks. We tested flattened model on different datasets and\nfound that the flattened layer can effectively substitute for the 3D filters\nwithout loss of accuracy. The flattened convolution pipelines provide around\ntwo times speed-up during feedforward pass compared to the baseline model due\nto the significant reduction of learning parameters. Furthermore, the proposed\nmethod does not require efforts in manual tuning or post processing once the\nmodel is trained.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 16:48:54 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:36:05 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2015 01:40:08 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2015 05:50:23 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Jin", "Jonghoon", ""], ["Dundar", "Aysegul", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1412.5567", "submitter": "Awni Hannun", "authors": "Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos,\n  Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates and\n  Andrew Y. Ng", "title": "Deep Speech: Scaling up end-to-end speech recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a state-of-the-art speech recognition system developed using\nend-to-end deep learning. Our architecture is significantly simpler than\ntraditional speech systems, which rely on laboriously engineered processing\npipelines; these traditional systems also tend to perform poorly when used in\nnoisy environments. In contrast, our system does not need hand-designed\ncomponents to model background noise, reverberation, or speaker variation, but\ninstead directly learns a function that is robust to such effects. We do not\nneed a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our\napproach is a well-optimized RNN training system that uses multiple GPUs, as\nwell as a set of novel data synthesis techniques that allow us to efficiently\nobtain a large amount of varied data for training. Our system, called Deep\nSpeech, outperforms previously published results on the widely studied\nSwitchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech\nalso handles challenging noisy environments better than widely used,\nstate-of-the-art commercial speech systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 20:39:45 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 21:36:13 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Hannun", "Awni", ""], ["Case", "Carl", ""], ["Casper", "Jared", ""], ["Catanzaro", "Bryan", ""], ["Diamos", "Greg", ""], ["Elsen", "Erich", ""], ["Prenger", "Ryan", ""], ["Satheesh", "Sanjeev", ""], ["Sengupta", "Shubho", ""], ["Coates", "Adam", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1412.5617", "submitter": "Shuang Song", "authors": "Shuang Song, Kamalika Chaudhuri, Anand D. Sarwate", "title": "Learning from Data with Heterogeneous Noise using SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning from data of variable quality that may be obtained from\ndifferent heterogeneous sources. Addressing learning from heterogeneous data in\nits full generality is a challenging problem. In this paper, we adopt instead a\nmodel in which data is observed through heterogeneous noise, where the noise\nlevel reflects the quality of the data source. We study how to use stochastic\ngradient algorithms to learn in this model. Our study is motivated by two\nconcrete examples where this problem arises naturally: learning with local\ndifferential privacy based on data from multiple sources with different privacy\nrequirements, and learning from data with labels of variable quality.\n  The main contribution of this paper is to identify how heterogeneous noise\nimpacts performance. We show that given two datasets with heterogeneous noise,\nthe order in which to use them in standard SGD depends on the learning rate. We\npropose a method for changing the learning rate as a function of the\nheterogeneity, and prove new regret bounds for our method in two cases of\ninterest. Experiments on real data show that our method performs better than\nusing a single learning rate and using only the less noisy of the two datasets\nwhen the noise level is low to moderate.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 21:15:06 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Song", "Shuang", ""], ["Chaudhuri", "Kamalika", ""], ["Sarwate", "Anand D.", ""]]}, {"id": "1412.5627", "submitter": "Fabricio Martins Lopes", "authors": "Bruno Mendes Moro Conque and Andr\\'e Yoshiaki Kashiwabara and\n  Fabr\\'icio Martins Lopes", "title": "Feature extraction from complex networks: A case of study in genomic\n  sequences classification", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new approach for classification of genomic sequences\nfrom measurements of complex networks and information theory. For this, it is\nconsidered the nucleotides, dinucleotides and trinucleotides of a genomic\nsequence. For each of them, the entropy, sum entropy and maximum entropy values\nare calculated.For each of them is also generated a network, in which the nodes\nare the nucleotides, dinucleotides or trinucleotides and its edges are\nestimated by observing the respective adjacency among them in the genomic\nsequence. In this way, it is generated three networks, for which measures of\ncomplex networks are extracted.These measures together with measures of\ninformation theory comprise a feature vector representing a genomic sequence.\nThus, the feature vector is used for classification by methods such as SVM,\nMultiLayer Perceptron, J48, IBK, Naive Bayes and Random Forest in order to\nevaluate the proposed approach.It was adopted coding sequences, intergenic\nsequences and TSS (Transcriptional Starter Sites) as datasets, for which the\nbetter results were obtained by the Random Forest with 91.2%, followed by J48\nwith 89.1% and SVM with 84.8% of accuracy. These results indicate that the new\napproach of feature extraction has its value, reaching good levels of\nclassification even considering only the genomic sequences, i.e., no other a\npriori knowledge about them is considered.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 21:31:51 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Conque", "Bruno Mendes Moro", ""], ["Kashiwabara", "Andr\u00e9 Yoshiaki", ""], ["Lopes", "Fabr\u00edcio Martins", ""]]}, {"id": "1412.5659", "submitter": "Nicholas Dronen", "authors": "Nicholas Dronen, Peter W. Foltz, Kyle Habermehl", "title": "Effective sampling for large-scale automated writing evaluation systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated writing evaluation (AWE) has been shown to be an effective\nmechanism for quickly providing feedback to students. It has already seen wide\nadoption in enterprise-scale applications and is starting to be adopted in\nlarge-scale contexts. Training an AWE model has historically required a single\nbatch of several hundred writing examples and human scores for each of them.\nThis requirement limits large-scale adoption of AWE since human-scoring essays\nis costly. Here we evaluate algorithms for ensuring that AWE models are\nconsistently trained using the most informative essays. Our results show how to\nminimize training set sizes while maximizing predictive performance, thereby\nreducing cost without unduly sacrificing accuracy. We conclude with a\ndiscussion of how to integrate this approach into large-scale AWE systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 22:41:14 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Dronen", "Nicholas", ""], ["Foltz", "Peter W.", ""], ["Habermehl", "Kyle", ""]]}, {"id": "1412.5673", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji and Jacob Eisenstein", "title": "Entity-Augmented Distributional Semantics for Discourse Relations", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse relations bind smaller linguistic elements into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it\nrequires understanding the semantics of the linked sentences. A more subtle\nchallenge is that it is not enough to represent the meaning of each sentence of\na discourse relation, because the relation may depend on links between\nlower-level elements, such as entity mentions. Our solution computes\ndistributional meaning representations by composition up the syntactic parse\ntree. A key difference from previous work on compositional distributional\nsemantics is that we also compute representations for entity mentions, using a\nnovel downward compositional pass. Discourse relations are predicted not only\nfrom the distributional representations of the sentences, but also of their\ncoreferent entity mentions. The resulting system obtains substantial\nimprovements over the previous state-of-the-art in predicting implicit\ndiscourse relations in the Penn Discourse Treebank.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 23:26:48 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 23:17:48 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 14:14:44 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Ji", "Yangfeng", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1412.5710", "submitter": "Jiaqi Zhao", "authors": "Jiaqi Zhao, Vitor Basto Fernandes, Licheng Jiao, Iryna Yevseyeva, Asep\n  Maulana, Rui Li, Thomas B\\\"ack, and Michael T. M. Emmerich", "title": "Multiobjective Optimization of Classifiers by Means of 3-D Convex Hull\n  Based Evolutionary Algorithm", "comments": "32 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Finding a good classifier is a multiobjective optimization problem with\ndifferent error rates and the costs to be minimized. The receiver operating\ncharacteristic is widely used in the machine learning community to analyze the\nperformance of parametric classifiers or sets of Pareto optimal classifiers. In\norder to directly compare two sets of classifiers the area (or volume) under\nthe convex hull can be used as a scalar indicator for the performance of a set\nof classifiers in receiver operating characteristic space.\n  Recently, the convex hull based multiobjective genetic programming algorithm\nwas proposed and successfully applied to maximize the convex hull area for\nbinary classification problems. The contribution of this paper is to extend\nthis algorithm for dealing with higher dimensional problem formulations. In\nparticular, we discuss problems where parsimony (or classifier complexity) is\nstated as a third objective and multi-class classification with three different\ntrue classification rates to be maximized.\n  The design of the algorithm proposed in this paper is inspired by\nindicator-based evolutionary algorithms, where first a performance indicator\nfor a solution set is established and then a selection operator is designed\nthat complies with the performance indicator. In this case, the performance\nindicator will be the volume under the convex hull. The algorithm is tested and\nanalyzed in a proof of concept study on different benchmarks that are designed\nfor measuring its capability to capture relevant parts of a convex hull.\n  Further benchmark and application studies on email classification and feature\nselection round up the analysis and assess robustness and usefulness of the new\nalgorithm in real world settings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 03:01:10 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Zhao", "Jiaqi", ""], ["Fernandes", "Vitor Basto", ""], ["Jiao", "Licheng", ""], ["Yevseyeva", "Iryna", ""], ["Maulana", "Asep", ""], ["Li", "Rui", ""], ["B\u00e4ck", "Thomas", ""], ["Emmerich", "Michael T. M.", ""]]}, {"id": "1412.5721", "submitter": "Edo Liberty", "authors": "Edo Liberty, Ram Sriharsha, Maxim Sviridenko", "title": "An Algorithm for Online K-Means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that one can be competitive with the k-means objective while\noperating online. In this model, the algorithm receives vectors v_1,...,v_n one\nby one in an arbitrary order. For each vector the algorithm outputs a cluster\nidentifier before receiving the next one. Our online algorithm generates ~O(k)\nclusters whose k-means cost is ~O(W*). Here, W* is the optimal k-means cost\nusing k clusters and ~O suppresses poly-logarithmic factors. We also show that,\nexperimentally, it is not much worse than k-means++ while operating in a\nstrictly more constrained computational model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 05:09:32 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 17:30:23 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Liberty", "Edo", ""], ["Sriharsha", "Ram", ""], ["Sviridenko", "Maxim", ""]]}, {"id": "1412.5732", "submitter": "Changsheng Li", "authors": "Changsheng Li and Fan Wei and Weishan Dong and Qingshan Liu and\n  Xiangfeng Wang and Xin Zhang", "title": "Dynamic Structure Embedded Online Multiple-Output Regression for Stream\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online multiple-output regression is an important machine learning technique\nfor modeling, predicting, and compressing multi-dimensional correlated data\nstreams. In this paper, we propose a novel online multiple-output regression\nmethod, called MORES, for stream data. MORES can \\emph{dynamically} learn the\nstructure of the coefficients change in each update step to facilitate the\nmodel's continuous refinement. We observe that limited expressive ability of\nthe regression model, especially in the preliminary stage of online update,\noften leads to the variables in the residual errors being dependent. In light\nof this point, MORES intends to \\emph{dynamically} learn and leverage the\nstructure of the residual errors to improve the prediction accuracy. Moreover,\nwe define three statistical variables to \\emph{exactly} represent all the seen\nsamples for \\emph{incrementally} calculating prediction loss in each online\nupdate round, which can avoid loading all the training data into memory for\nupdating model, and also effectively prevent drastic fluctuation of the model\nin the presence of noise. Furthermore, we introduce a forgetting factor to set\ndifferent weights on samples so as to track the data streams' evolving\ncharacteristics quickly from the latest samples. Experiments on one synthetic\ndataset and three real-world datasets validate the effectiveness of the\nproposed method. In addition, the update speed of MORES is at least 2000\nsamples processed per second on the three real-world datasets, more than 15\ntimes faster than the state-of-the-art online learning algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 06:37:50 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 03:00:55 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Li", "Changsheng", ""], ["Wei", "Fan", ""], ["Dong", "Weishan", ""], ["Liu", "Qingshan", ""], ["Wang", "Xiangfeng", ""], ["Zhang", "Xin", ""]]}, {"id": "1412.5744", "submitter": "Richard Golden Professor", "authors": "Richard M. Golden", "title": "Stochastic Descent Analysis of Representation Learning Algorithms", "comments": "Version: April 27, 2015. This paper has been withdrawn by the author\n  because of a minor problem with the proof which has since been corrected. The\n  revised manuscript will eventually be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although stochastic approximation learning methods have been widely used in\nthe machine learning literature for over 50 years, formal theoretical analyses\nof specific machine learning algorithms are less common because stochastic\napproximation theorems typically possess assumptions which are difficult to\ncommunicate and verify. This paper presents a new stochastic approximation\ntheorem for state-dependent noise with easily verifiable assumptions applicable\nto the analysis and design of important deep learning algorithms including:\nadaptive learning, contrastive divergence learning, stochastic descent\nexpectation maximization, and active learning.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 07:51:24 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 20:05:38 GMT"}, {"version": "v3", "created": "Sun, 22 Feb 2015 00:12:42 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2015 14:39:33 GMT"}, {"version": "v5", "created": "Tue, 7 Apr 2015 14:43:31 GMT"}, {"version": "v6", "created": "Mon, 27 Apr 2015 15:56:42 GMT"}, {"version": "v7", "created": "Wed, 19 Apr 2017 21:17:10 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Golden", "Richard M.", ""]]}, {"id": "1412.5896", "submitter": "Raja Giryes", "authors": "Raja Giryes and Guillermo Sapiro and Alex M. Bronstein", "title": "On the Stability of Deep Networks", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.NE math.IT math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the properties of deep neural networks (DNN) with\nrandom weights. We formally prove that these networks perform a\ndistance-preserving embedding of the data. Based on this we then draw\nconclusions on the size of the training data and the networks' structure. A\nlonger version of this paper with more results and details can be found in\n(Giryes et al., 2015). In particular, we formally prove in the longer version\nthat DNN with random Gaussian weights perform a distance-preserving embedding\nof the data, with a special treatment for in-class and out-of-class data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 15:40:03 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 20:52:41 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 14:38:57 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1412.5902", "submitter": "Teng Qiu", "authors": "Teng Qiu, Kaifu Yang, Chaoyi Li, Yongjie Li", "title": "Nearest Descent, In-Tree, and Clustering", "comments": "28 pages: text part(1-14), supplementary material(15-28)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a physically inspired graph-theoretical clustering\nmethod, which first makes the data points organized into an attractive graph,\ncalled In-Tree, via a physically inspired rule, called Nearest Descent (ND). In\nparticular, the rule of ND works to select the nearest node in the descending\ndirection of potential as the parent node of each node, which is in essence\ndifferent from the classical Gradient Descent or Steepest Descent. The\nconstructed In-Tree proves a very good candidate for clustering due to its\nparticular features and properties. In the In-Tree, the original clustering\nproblem is reduced to a problem of removing a very few of undesired edges from\nthis graph. Pleasingly, the undesired edges in In-Tree are so distinguishable\nthat they can be easily determined in either automatic or interactive way,\nwhich is in stark contrast to the cases in the widely used Minimal Spanning\nTree and k-nearest-neighbor graph. The cluster number in the proposed method\ncan be easily determined based on some intermediate plots, and the cluster\nassignment for each node is easily made by quickly searching its root node in\neach sub-graph (also an In-Tree). The proposed method is extensively evaluated\non both synthetic and real-world datasets. Overall, the proposed clustering\nmethod is a density-based one, but shows significant differences and advantages\nin comparison to the traditional ones. The proposed method is simple yet\nefficient and reliable, and is applicable to various datasets with diverse\nshapes, attributes and any high dimensionality\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 11:38:55 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 14:57:28 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Qiu", "Teng", ""], ["Yang", "Kaifu", ""], ["Li", "Chaoyi", ""], ["Li", "Yongjie", ""]]}, {"id": "1412.5949", "submitter": "Pengtao Xie", "authors": "Pengtao Xie and Eric Xing", "title": "Large Scale Distributed Distance Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large scale machine learning and data mining problems with high feature\ndimensionality, the Euclidean distance between data points can be\nuninformative, and Distance Metric Learning (DML) is often desired to learn a\nproper similarity measure (using side information such as example data pairs\nbeing similar or dissimilar). However, high dimensionality and large volume of\npairwise constraints in modern big data can lead to prohibitive computational\ncost for both the original DML formulation in Xing et al. (2002) and later\nextensions. In this paper, we present a distributed algorithm for DML, and a\nlarge-scale implementation on a parameter server architecture. Our approach\nbuilds on a parallelizable reformulation of Xing et al. (2002), and an\nasynchronous stochastic gradient descent optimization procedure. To our\nknowledge, this is the first distributed solution to DML, and we show that, on\na system with 256 CPU cores, our program is able to complete a DML task on a\ndataset with 1 million data points, 22-thousand features, and 200 million\nlabeled data pairs, in 15 hours; and the learned metric shows great\neffectiveness in properly measuring distances.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 17:14:34 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Xie", "Pengtao", ""], ["Xing", "Eric", ""]]}, {"id": "1412.5967", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Christoph Studer, Andrew E. Waters, Richard G. Baraniuk", "title": "Tag-Aware Ordinal Sparse Factor Analysis for Learning and Content\n  Analytics", "comments": null, "journal-ref": "In Proc. 6th Intl. Conf. on Educational Data Mining, pages 90-97,\n  July 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning offers novel ways and means to design personalized learning\nsystems wherein each student's educational experience is customized in real\ntime depending on their background, learning goals, and performance to date.\nSPARse Factor Analysis (SPARFA) is a novel framework for machine learning-based\nlearning analytics, which estimates a learner's knowledge of the concepts\nunderlying a domain, and content analytics, which estimates the relationships\namong a collection of questions and those concepts. SPARFA jointly learns the\nassociations among the questions and the concepts, learner concept knowledge\nprofiles, and the underlying question difficulties, solely based on the\ncorrect/incorrect graded responses of a population of learners to a collection\nof questions. In this paper, we extend the SPARFA framework significantly to\nenable: (i) the analysis of graded responses on an ordinal scale (partial\ncredit) rather than a binary scale (correct/incorrect); (ii) the exploitation\nof tags/labels for questions that partially describe the question{concept\nassociations. The resulting Ordinal SPARFA-Tag framework greatly enhances the\ninterpretability of the estimated concepts. We demonstrate using real\neducational data that Ordinal SPARFA-Tag outperforms both SPARFA and existing\ncollaborative filtering techniques in predicting missing learner responses.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 17:46:41 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Lan", "Andrew S.", ""], ["Studer", "Christoph", ""], ["Waters", "Andrew E.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1412.5968", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Christoph Studer, Richard G. Baraniuk", "title": "Quantized Matrix Completion for Personalized Learning", "comments": null, "journal-ref": "In Proc. 7th Intl. Conf. on Educational Data Mining, pages\n  280-283, July 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed SPARse Factor Analysis (SPARFA) framework for\npersonalized learning performs factor analysis on ordinal or binary-valued\n(e.g., correct/incorrect) graded learner responses to questions. The underlying\nfactors are termed \"concepts\" (or knowledge components) and are used for\nlearning analytics (LA), the estimation of learner concept-knowledge profiles,\nand for content analytics (CA), the estimation of question-concept associations\nand question difficulties. While SPARFA is a powerful tool for LA and CA, it\nrequires a number of algorithm parameters (including the number of concepts),\nwhich are difficult to determine in practice. In this paper, we propose\nSPARFA-Lite, a convex optimization-based method for LA that builds on matrix\ncompletion, which only requires a single algorithm parameter and enables us to\nautomatically identify the required number of concepts. Using a variety of\neducational datasets, we demonstrate that SPARFALite (i) achieves comparable\nperformance in predicting unobserved learner responses to existing methods,\nincluding item response theory (IRT) and SPARFA, and (ii) is computationally\nmore efficient.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 17:48:17 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Lan", "Andrew S.", ""], ["Studer", "Christoph", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1412.6018", "submitter": "Rachada Kongkachandra Asst.Prof.Dr.", "authors": "Sirisak Visessenee, Sanparith Marukatat, and Rachada Kongkachandra", "title": "Automatic Training Data Synthesis for Handwriting Recognition Using the\n  Structural Crossing-Over Technique", "comments": "8 pages, 6 figures", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 5, No. 5, September 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel technique called \"Structural Crossing-Over\" to\nsynthesize qualified data for training machine learning-based handwriting\nrecognition. The proposed technique can provide a greater variety of patterns\nof training data than the existing approaches such as elastic distortion and\ntangent-based affine transformation. A couple of training characters are\nchosen, then they are analyzed by their similar and different structures, and\nfinally are crossed over to generate the new characters. The experiments are\nset to compare the performances of tangent-based affine transformation and the\nproposed approach in terms of the variety of generated characters and percent\nof recognition errors. The standard MNIST corpus including 60,000 training\ncharacters and 10,000 test characters is employed in the experiments. The\nproposed technique uses 1,000 characters to synthesize 60,000 characters, and\nthen uses these data to train and test the benchmark handwriting recognition\nsystem that exploits Histogram of Gradient (HOG) as features and Support Vector\nMachine (SVM) as recognizer. The experimental result yields 8.06% of errors. It\nsignificantly outperforms the tangent-based affine transformation and the\noriginal MNIST training data, which are 11.74% and 16.55%, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 04:32:20 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Visessenee", "Sirisak", ""], ["Marukatat", "Sanparith", ""], ["Kongkachandra", "Rachada", ""]]}, {"id": "1412.6039", "submitter": "Xin Yuan", "authors": "Yunchen Pu, Xin Yuan and Lawrence Carin", "title": "Generative Deep Deconvolutional Learning", "comments": "21 pages, 9 figures, revised version for ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generative Bayesian model is developed for deep (multi-layer) convolutional\ndictionary learning. A novel probabilistic pooling operation is integrated into\nthe deep model, yielding efficient bottom-up and top-down probabilistic\nlearning. After learning the deep convolutional dictionary, testing is\nimplemented via deconvolutional inference. To speed up this inference, a new\nstatistical approach is proposed to project the top-layer dictionary elements\nto the data level. Following this, only one layer of deconvolution is required\nduring testing. Experimental results demonstrate powerful capabilities of the\nmodel to learn multi-layer features from images. Excellent classification\nresults are obtained on both the MNIST and Caltech 101 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 20:01:38 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 17:21:36 GMT"}, {"version": "v3", "created": "Sun, 22 Feb 2015 18:13:29 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Pu", "Yunchen", ""], ["Yuan", "Xin", ""], ["Carin", "Lawrence", ""]]}, {"id": "1412.6093", "submitter": "Raunaq Vohra", "authors": "Kratarth Goel and Raunaq Vohra", "title": "Learning Temporal Dependencies in Data Using a DBN-BLSTM", "comments": "6 pages, 2 figures, 1 table, ICLR 2015 conference track submission\n  under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of deep learning, it has been used to solve various problems\nusing many different architectures. The application of such deep architectures\nto auditory data is also not uncommon. However, these architectures do not\nalways adequately consider the temporal dependencies in data. We thus propose a\nnew generic architecture called the Deep Belief Network - Bidirectional Long\nShort-Term Memory (DBN-BLSTM) network that models sequences by keeping track of\nthe temporal information while enabling deep representations in the data. We\ndemonstrate this new architecture by applying it to the task of music\ngeneration and obtain state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 11:04:59 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 18:44:33 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Goel", "Kratarth", ""], ["Vohra", "Raunaq", ""]]}, {"id": "1412.6095", "submitter": "Ali Heydari", "authors": "Ali Heydari", "title": "Theoretical and Numerical Analysis of Approximate Dynamic Programming\n  with Approximation Errors", "comments": "This study is the counterpart of another work of the author\n  (arXiv:1412.5675) which was for value iterations with initial stabilizing\n  guess (with overlaps on Theorem 1 and Lemma 1). As for the revision on this\n  work, some steps of proofs are updated and an explanation about the\n  approximation error is included. Initial submission date: 12/18/2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is aimed at answering the famous question of how the approximation\nerrors at each iteration of Approximate Dynamic Programming (ADP) affect the\nquality of the final results considering the fact that errors at each iteration\naffect the next iteration. To this goal, convergence of Value Iteration scheme\nof ADP for deterministic nonlinear optimal control problems with undiscounted\ncost functions is investigated while considering the errors existing in\napproximating respective functions. The boundedness of the results around the\noptimal solution is obtained based on quantities which are known in a general\noptimal control problem and assumptions which are verifiable. Moreover, since\nthe presence of the approximation errors leads to the deviation of the results\nfrom optimality, sufficient conditions for stability of the system operated by\nthe result obtained after a finite number of value iterations, along with an\nestimation of its region of attraction, are derived in terms of a calculable\nupper bound of the control approximation error. Finally, the process of\nimplementation of the method on an orbital maneuver problem is investigated\nthrough which the assumptions made in the theoretical developments are verified\nand the sufficient conditions are applied for guaranteeing stability and near\noptimality.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 16:38:10 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 22:09:05 GMT"}, {"version": "v3", "created": "Fri, 15 May 2015 18:41:08 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Heydari", "Ali", ""]]}, {"id": "1412.6115", "submitter": "Yunchao Gong", "authors": "Yunchao Gong and Liu Liu and Ming Yang and Lubomir Bourdev", "title": "Compressing Deep Convolutional Networks using Vector Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) has become the most promising method\nfor object recognition, repeatedly demonstrating record breaking results for\nimage classification and object detection in recent years. However, a very deep\nCNN generally involves many layers with millions of parameters, making the\nstorage of the network model to be extremely large. This prohibits the usage of\ndeep CNNs on resource limited hardware, especially cell phones or other\nembedded devices. In this paper, we tackle this model storage issue by\ninvestigating information theoretical vector quantization methods for\ncompressing the parameters of CNNs. In particular, we have found in terms of\ncompressing the most storage demanding dense connected layers, vector\nquantization methods have a clear gain over existing matrix factorization\nmethods. Simply applying k-means clustering to the weights or conducting\nproduct quantization can lead to a very good balance between model size and\nrecognition accuracy. For the 1000-category classification task in the ImageNet\nchallenge, we are able to achieve 16-24 times compression of the network with\nonly 1% loss of classification accuracy using the state-of-the-art CNN.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 21:09:01 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Gong", "Yunchao", ""], ["Liu", "Liu", ""], ["Yang", "Ming", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1412.6141", "submitter": "Song-Ju Kim Dr.", "authors": "Song-Ju Kim, Masashi Aono, and Etsushi Nameda", "title": "Efficient Decision-Making by Volume-Conserving Physical Object", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": "10.1088/1367-2630/17/8/083023", "report-no": null, "categories": "cs.AI cs.LG nlin.AO physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that any physical object, as long as its volume is conserved\nwhen coupled with suitable operations, provides a sophisticated decision-making\ncapability. We consider the problem of finding, as accurately and quickly as\npossible, the most profitable option from a set of options that gives\nstochastic rewards. These decisions are made as dictated by a physical object,\nwhich is moved in a manner similar to the fluctuations of a rigid body in a\ntug-of-war game. Our analytical calculations validate statistical reasons why\nour method exhibits higher efficiency than conventional algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 08:23:13 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Kim", "Song-Ju", ""], ["Aono", "Masashi", ""], ["Nameda", "Etsushi", ""]]}, {"id": "1412.6177", "submitter": "Tomoki Tsuchida", "authors": "Tomoki Tsuchida and Garrison W. Cottrell", "title": "Example Selection For Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised learning, an unbiased uniform sampling strategy is typically\nused, in order that the learned features faithfully encode the statistical\nstructure of the training data. In this work, we explore whether active example\nselection strategies - algorithms that select which examples to use, based on\nthe current estimate of the features - can accelerate learning. Specifically,\nwe investigate effects of heuristic and saliency-inspired selection algorithms\non the dictionary learning task with sparse activations. We show that some\nselection algorithms do improve the speed of learning, and we speculate on why\nthey might work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 23:25:22 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 23:28:16 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 19:22:18 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Tsuchida", "Tomoki", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1412.6181", "submitter": "Ran Gilad-Bachrach", "authors": "Pengtao Xie and Misha Bilenko and Tom Finley and Ran Gilad-Bachrach\n  and Kristin Lauter and Michael Naehrig", "title": "Crypto-Nets: Neural Networks over Encrypted Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem we address is the following: how can a user employ a predictive\nmodel that is held by a third party, without compromising private information.\nFor example, a hospital may wish to use a cloud service to predict the\nreadmission risk of a patient. However, due to regulations, the patient's\nmedical files cannot be revealed. The goal is to make an inference using the\nmodel, without jeopardizing the accuracy of the prediction or the privacy of\nthe data.\n  To achieve high accuracy, we use neural networks, which have been shown to\noutperform other learning models for many tasks. To achieve the privacy\nrequirements, we use homomorphic encryption in the following protocol: the data\nowner encrypts the data and sends the ciphertexts to the third party to obtain\na prediction from a trained model. The model operates on these ciphertexts and\nsends back the encrypted prediction. In this protocol, not only the data\nremains private, even the values predicted are available only to the data\nowner.\n  Using homomorphic encryption and modifications to the activation functions\nand training algorithms of neural networks, we show that it is protocol is\npossible and may be feasible. This method paves the way to build a secure\ncloud-based neural network prediction services without invading users' privacy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 23:38:54 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 06:16:14 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Xie", "Pengtao", ""], ["Bilenko", "Misha", ""], ["Finley", "Tom", ""], ["Gilad-Bachrach", "Ran", ""], ["Lauter", "Kristin", ""], ["Naehrig", "Michael", ""]]}, {"id": "1412.6211", "submitter": "Qiang Wu", "authors": "Xianfeng Hu, Yang Wang and Qiang Wu", "title": "Multiple Authors Detection: A Quantitative Analysis of Dream of the Red\n  Chamber", "comments": null, "journal-ref": "Advances in Adaptive Data Analysis, Article ID 1450012 (18 pages),\n  2014", "doi": "10.1142/S1793536914500125", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the authorship controversy of Dream of the Red Chamber and the\napplication of machine learning in the study of literary stylometry, we develop\na rigorous new method for the mathematical analysis of authorship by testing\nfor a so-called chrono-divide in writing styles. Our method incorporates some\nof the latest advances in the study of authorship attribution, particularly\ntechniques from support vector machines. By introducing the notion of relative\nfrequency as a feature ranking metric our method proves to be highly effective\nand robust.\n  Applying our method to the Cheng-Gao version of Dream of the Red Chamber has\nled to convincing if not irrefutable evidence that the first $80$ chapters and\nthe last $40$ chapters of the book were written by two different authors.\nFurthermore, our analysis has unexpectedly provided strong support to the\nhypothesis that Chapter 67 was not the work of Cao Xueqin either.\n  We have also tested our method to the other three Great Classical Novels in\nChinese. As expected no chrono-divides have been found. This provides further\nevidence of the robustness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 04:31:11 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Hu", "Xianfeng", ""], ["Wang", "Yang", ""], ["Wu", "Qiang", ""]]}, {"id": "1412.6249", "submitter": "Min Lin", "authors": "Min Lin, Shuo Li, Xuan Luo, Shuicheng Yan", "title": "Purine: A bi-graph based deep learning framework", "comments": "Submitted to ICLR 2015 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel deep learning framework, termed Purine.\nIn Purine, a deep network is expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 08:20:10 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 03:18:46 GMT"}, {"version": "v3", "created": "Tue, 20 Jan 2015 02:17:39 GMT"}, {"version": "v4", "created": "Mon, 16 Mar 2015 16:13:46 GMT"}, {"version": "v5", "created": "Thu, 16 Apr 2015 13:09:33 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Lin", "Min", ""], ["Li", "Shuo", ""], ["Luo", "Xuan", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1412.6257", "submitter": "Alexander Kalmanovich", "authors": "Alexander Kalmanovich and Gal Chechik", "title": "Gradual training of deep denoising auto encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacked denoising auto encoders (DAEs) are well known to learn useful deep\nrepresentations, which can be used to improve supervised training by\ninitializing a deep network. We investigate a training scheme of a deep DAE,\nwhere DAE layers are gradually added and keep adapting as additional layers are\nadded. We show that in the regime of mid-sized datasets, this gradual training\nprovides a small but consistent improvement over stacked training in both\nreconstruction quality and classification error over stacked training on MNIST\nand CIFAR datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 09:30:33 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Kalmanovich", "Alexander", ""], ["Chechik", "Gal", ""]]}, {"id": "1412.6285", "submitter": "Gianluca Bontempi", "authors": "Gianluca Bontempi and Maxime Flauder", "title": "From dependency to causality: a machine learning approach", "comments": "submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between statistical dependency and causality lies at the\nheart of all statistical approaches to causal inference. Recent results in the\nChaLearn cause-effect pair challenge have shown that causal directionality can\nbe inferred with good accuracy also in Markov indistinguishable configurations\nthanks to data driven approaches. This paper proposes a supervised machine\nlearning approach to infer the existence of a directed causal link between two\nvariables in multivariate settings with $n>2$ variables. The approach relies on\nthe asymmetry of some conditional (in)dependence relations between the members\nof the Markov blankets of two variables causally connected. Our results show\nthat supervised learning methods may be successfully used to extract causal\ninformation on the basis of asymmetric statistical descriptors also for $n>2$\nvariate distributions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 10:50:14 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Bontempi", "Gianluca", ""], ["Flauder", "Maxime", ""]]}, {"id": "1412.6286", "submitter": "Wendelin B\\\"ohmer", "authors": "Wendelin B\\\"ohmer and Klaus Obermayer", "title": "Regression with Linear Factored Functions", "comments": "Under review as conference paper at ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications that use empirically estimated functions face a curse of\ndimensionality, because the integrals over most function classes must be\napproximated by sampling. This paper introduces a novel regression-algorithm\nthat learns linear factored functions (LFF). This class of functions has\nstructural properties that allow to analytically solve certain integrals and to\ncalculate point-wise products. Applications like belief propagation and\nreinforcement learning can exploit these properties to break the curse and\nspeed up computation. We derive a regularized greedy optimization scheme, that\nlearns factored basis functions during training. The novel regression algorithm\nperforms competitively to Gaussian processes on benchmark tasks, and the\nlearned LFF functions are with 4-9 factored basis functions on average very\ncompact.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 11:01:21 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 14:53:35 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 15:14:20 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["B\u00f6hmer", "Wendelin", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1412.6296", "submitter": "Jifeng Dai", "authors": "Jifeng Dai, Yang Lu, Ying-Nian Wu", "title": "Generative Modeling of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural networks (CNNs) have proven to be a powerful tool\nfor discriminative learning. Recently researchers have also started to show\ninterest in the generative aspects of CNNs in order to gain a deeper\nunderstanding of what they have learned and how to further improve them. This\npaper investigates generative modeling of CNNs. The main contributions include:\n(1) We construct a generative model for the CNN in the form of exponential\ntilting of a reference distribution. (2) We propose a generative gradient for\npre-training CNNs by a non-parametric importance sampling scheme, which is\nfundamentally different from the commonly used discriminative gradient, and yet\nhas the same computational architecture and cost as the latter. (3) We propose\na generative visualization method for the CNNs by sampling from an explicit\nparametric image distribution. The proposed visualization method can directly\ndraw synthetic samples for any given node in a trained CNN by the Hamiltonian\nMonte Carlo (HMC) algorithm, without resorting to any extra hold-out images.\nExperiments on the challenging ImageNet benchmark show that the proposed\ngenerative gradient pre-training consistently helps improve the performances of\nCNNs, and the proposed generative visualization method generates meaningful and\nvaried samples of synthetic images from a large-scale deep CNN.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 11:34:37 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 15:07:06 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Dai", "Jifeng", ""], ["Lu", "Yang", ""], ["Wu", "Ying-Nian", ""]]}, {"id": "1412.6388", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Ethem Alpayd{\\i}n", "title": "Distributed Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed budding tree is a decision tree algorithm in which every\nnode is part internal node and part leaf. This allows representing every\ndecision tree in a continuous parameter space, and therefore a budding tree can\nbe jointly trained with backpropagation, like a neural network. Even though\nthis continuity allows it to be used in hierarchical representation learning,\nthe learned representations are local: Activation makes a soft selection among\nall root-to-leaf paths in a tree. In this work we extend the budding tree and\npropose the distributed tree where the children use different and independent\nsplits and hence multiple paths in a tree can be traversed at the same time.\nThis ability to combine multiple paths gives the power of a distributed\nrepresentation, as in a traditional perceptron layer. We show that distributed\ntrees perform comparably or better than budding and traditional hard trees on\nclassification and regression tasks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:44:02 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Alpayd\u0131n", "Ethem", ""]]}, {"id": "1412.6418", "submitter": "Ehsan Khoddam Mohammadi", "authors": "Ivan Titov and Ehsan Khoddam", "title": "Inducing Semantic Representation from Text by Jointly Predicting and\n  Factorizing Relations", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new method to integrate two recent lines of work:\nunsupervised induction of shallow semantics (e.g., semantic roles) and\nfactorization of relations in text and knowledge bases. Our model consists of\ntwo components: (1) an encoding component: a semantic role labeling model which\npredicts roles given a rich set of syntactic and lexical features; (2) a\nreconstruction component: a tensor factorization model which relies on roles to\npredict argument fillers. When the components are estimated jointly to minimize\nerrors in argument reconstruction, the induced roles largely correspond to\nroles defined in annotated resources. Our method performs on par with most\naccurate role induction methods on English, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguage.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 16:30:33 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 12:16:56 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 10:24:27 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Titov", "Ivan", ""], ["Khoddam", "Ehsan", ""]]}, {"id": "1412.6451", "submitter": "Mark Wernsdorfer", "authors": "Mark Wernsdorfer, Ute Schmid", "title": "Grounding Hierarchical Reinforcement Learning Models for Knowledge\n  Transfer", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of deep machine learning enable to to reuse low-level representations\nefficiently for generating more abstract high-level representations.\nOriginally, deep learning has been applied passively (e.g., for classification\npurposes). Recently, it has been extended to estimate the value of actions for\nautonomous agents within the framework of reinforcement learning (RL). Explicit\nmodels of the environment can be learned to augment such a value function.\nAlthough \"flat\" connectionist methods have already been used for model-based\nRL, up to now, only model-free variants of RL have been equipped with methods\nfrom deep learning. We propose a variant of deep model-based RL that enables an\nagent to learn arbitrarily abstract hierarchical representations of its\nenvironment. In this paper, we present research on how such hierarchical\nrepresentations can be grounded in sensorimotor interaction between an agent\nand its environment.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 17:41:59 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Wernsdorfer", "Mark", ""], ["Schmid", "Ute", ""]]}, {"id": "1412.6452", "submitter": "Maria-Irina Nicolae", "authors": "Maria-Irina Nicolae, Marc Sebban, Amaury Habrard, \\'Eric Gaussier and\n  Massih-Reza Amini", "title": "Algorithmic Robustness for Learning via $(\\epsilon, \\gamma, \\tau)$-Good\n  Similarity Functions", "comments": "ICLR 2015 Workshop - accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of metric plays a key role in machine learning problems such as\nclassification, clustering or ranking. However, it is worth noting that there\nis a severe lack of theoretical guarantees that can be expected on the\ngeneralization capacity of the classifier associated to a given metric. The\ntheoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions\n(Balcan et al., 2008) has been one of the first attempts to draw a link between\nthe properties of a similarity function and those of a linear classifier making\nuse of it. In this paper, we extend and complete this theory by providing a new\ngeneralization bound for the associated classifier based on the algorithmic\nrobustness framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 17:43:26 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 15:36:14 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 11:10:43 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Nicolae", "Maria-Irina", ""], ["Sebban", "Marc", ""], ["Habrard", "Amaury", ""], ["Gaussier", "\u00c9ric", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1412.6493", "submitter": "Zichao Yang", "authors": "Zichao Yang and Alexander J. Smola and Le Song and Andrew Gordon\n  Wilson", "title": "A la Carte - Learning Fast Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods have great promise for learning rich statistical\nrepresentations of large modern datasets. However, compared to neural networks,\nkernel methods have been perceived as lacking in scalability and flexibility.\nWe introduce a family of fast, flexible, lightly parametrized and general\npurpose kernel learning methods, derived from Fastfood basis function\nexpansions. We provide mechanisms to learn the properties of groups of spectral\nfrequencies in these expansions, which require only O(mlogd) time and O(m)\nmemory, for m basis functions and d input dimensions. We show that the proposed\nmethods can learn a wide class of kernels, outperforming the alternatives in\naccuracy, speed, and memory consumption.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 19:27:21 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Yang", "Zichao", ""], ["Smola", "Alexander J.", ""], ["Song", "Le", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1412.6502", "submitter": "Siddharth Pramod", "authors": "Siddharth Pramod, Adam Page, Tinoosh Mohsenin and Tim Oates", "title": "Detecting Epileptic Seizures from EEG Data using Neural Networks", "comments": "This paper has been withdrawn by the authors due to an error\n  discovered in the experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of neural networks trained with dropout in predicting\nepileptic seizures from electroencephalographic data (scalp EEG). The input to\nthe neural network is a 126 feature vector containing 9 features for each of\nthe 14 EEG channels obtained over 1-second, non-overlapping windows. The models\nin our experiments achieved high sensitivity and specificity on patient records\nnot used in the training process. This is demonstrated using\nleave-one-out-cross-validation across patient records, where we hold out one\npatient's record as the test set and use all other patients' records for\ntraining; repeating this procedure for all patients in the database.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:00:38 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 01:20:42 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 00:42:42 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2015 15:24:05 GMT"}, {"version": "v5", "created": "Thu, 6 Dec 2018 03:15:14 GMT"}, {"version": "v6", "created": "Mon, 4 Feb 2019 00:41:35 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Pramod", "Siddharth", ""], ["Page", "Adam", ""], ["Mohsenin", "Tinoosh", ""], ["Oates", "Tim", ""]]}, {"id": "1412.6506", "submitter": "Pengtao Xie", "authors": "Pengtao Xie and Eric Xing", "title": "Cauchy Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) has wide applications in machine learning,\ntext mining and computer vision. Classical PCA based on a Gaussian noise model\nis fragile to noise of large magnitude. Laplace noise assumption based PCA\nmethods cannot deal with dense noise effectively. In this paper, we propose\nCauchy Principal Component Analysis (Cauchy PCA), a very simple yet effective\nPCA method which is robust to various types of noise. We utilize Cauchy\ndistribution to model noise and derive Cauchy PCA under the maximum likelihood\nestimation (MLE) framework with low rank constraint. Our method can robustly\nestimate the low rank matrix regardless of whether noise is large or small,\ndense or sparse. We analyze the robustness of Cauchy PCA from a robust\nstatistics view and present an efficient singular value projection optimization\nmethod. Experimental results on both simulated data and real applications\ndemonstrate the robustness of Cauchy PCA to various noise patterns.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:06:02 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Xie", "Pengtao", ""], ["Xing", "Eric", ""]]}, {"id": "1412.6514", "submitter": "Majid Janzamin", "authors": "Majid Janzamin and Hanie Sedghi and Anima Anandkumar", "title": "Score Function Features for Discriminative Learning", "comments": "Accepted as a workshop contribution at ICLR 2015. A longer version of\n  this work is also available on arXiv: http://arxiv.org/abs/1412.2863", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning forms the cornerstone for tackling challenging learning\nproblems in domains such as speech, computer vision and natural language\nprocessing. In this paper, we consider a novel class of matrix and\ntensor-valued features, which can be pre-trained using unlabeled samples. We\npresent efficient algorithms for extracting discriminative information, given\nthese pre-trained features and labeled samples for any related task. Our class\nof features are based on higher-order score functions, which capture local\nvariations in the probability density function of the input. We establish a\ntheoretical framework to characterize the nature of discriminative information\nthat can be extracted from score-function features, when used in conjunction\nwith labeled samples. We employ efficient spectral decomposition algorithms (on\nmatrices and tensors) for extracting discriminative components. The advantage\nof employing tensor-valued features is that we can extract richer\ndiscriminative information in the form of an overcomplete representations.\nThus, we present a novel framework for employing generative models of the input\nfor discriminative learning.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:18:36 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2015 18:46:19 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Janzamin", "Majid", ""], ["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1412.6544", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe", "title": "Qualitatively characterizing neural network optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks involves solving large-scale non-convex optimization\nproblems. This task has long been believed to be extremely difficult, with fear\nof local minima and other obstacles motivating a variety of schemes to improve\noptimization, such as unsupervised pretraining. However, modern neural networks\nare able to achieve negligible training error on complex tasks, using only\ndirect training with stochastic gradient descent. We introduce a simple\nanalysis technique to look for evidence that such networks are overcoming local\noptima. We find that, in fact, on a straight path from initialization to\nsolution, a variety of state of the art neural networks never encounter any\nsignificant obstacles.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 21:55:01 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 23:59:32 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 05:25:15 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2015 18:28:17 GMT"}, {"version": "v5", "created": "Fri, 20 Mar 2015 20:08:54 GMT"}, {"version": "v6", "created": "Thu, 21 May 2015 21:44:31 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Vinyals", "Oriol", ""], ["Saxe", "Andrew M.", ""]]}, {"id": "1412.6547", "submitter": "Paul Mineiro", "authors": "Paul Mineiro and Nikos Karampatziakis", "title": "Fast Label Embeddings via Randomized Linear Algebra", "comments": "To appear in the proceedings of the ECML/PKDD 2015 conference.\n  Reference implementation available at https://github.com/pmineiro/randembed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern multiclass and multilabel problems are characterized by\nincreasingly large output spaces. For these problems, label embeddings have\nbeen shown to be a useful primitive that can improve computational and\nstatistical efficiency. In this work we utilize a correspondence between rank\nconstrained estimation and low dimensional label embeddings that uncovers a\nfast label embedding algorithm which works in both the multiclass and\nmultilabel settings. The result is a randomized algorithm whose running time is\nexponentially faster than naive algorithms. We demonstrate our techniques on\ntwo large-scale public datasets, from the Large Scale Hierarchical Text\nChallenge and the Open Directory Project, where we obtain state of the art\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 22:09:35 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 23:29:44 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 16:11:14 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2015 23:24:53 GMT"}, {"version": "v5", "created": "Mon, 13 Apr 2015 00:29:44 GMT"}, {"version": "v6", "created": "Mon, 15 Jun 2015 18:07:20 GMT"}, {"version": "v7", "created": "Sun, 5 Jul 2015 15:38:11 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Mineiro", "Paul", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1412.6550", "submitter": "Adriana Romero", "authors": "Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine\n  Chassang, Carlo Gatta and Yoshua Bengio", "title": "FitNets: Hints for Thin Deep Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While depth tends to improve network performances, it also makes\ngradient-based training more difficult since deeper networks tend to be more\nnon-linear. The recently proposed knowledge distillation approach is aimed at\nobtaining small and fast-to-execute models, and it has shown that a student\nnetwork could imitate the soft output of a larger teacher network or ensemble\nof networks. In this paper, we extend this idea to allow the training of a\nstudent that is deeper and thinner than the teacher, using not only the outputs\nbut also the intermediate representations learned by the teacher as hints to\nimprove the training process and final performance of the student. Because the\nstudent intermediate hidden layer will generally be smaller than the teacher's\nintermediate hidden layer, additional parameters are introduced to map the\nstudent hidden layer to the prediction of the teacher hidden layer. This allows\none to train deeper students that can generalize better or run faster, a\ntrade-off that is controlled by the chosen student capacity. For example, on\nCIFAR-10, a deep student network with almost 10.4 times less parameters\noutperforms a larger, state-of-the-art teacher network.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 22:40:51 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 20:56:15 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 18:44:36 GMT"}, {"version": "v4", "created": "Fri, 27 Mar 2015 11:52:28 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Romero", "Adriana", ""], ["Ballas", "Nicolas", ""], ["Kahou", "Samira Ebrahimi", ""], ["Chassang", "Antoine", ""], ["Gatta", "Carlo", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.6553", "submitter": "Vadim Lebedev", "authors": "Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor\n  Lempitsky", "title": "Speeding-up Convolutional Neural Networks Using Fine-tuned\n  CP-Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple two-step approach for speeding up convolution layers\nwithin large convolutional neural networks based on tensor decomposition and\ndiscriminative fine-tuning. Given a layer, we use non-linear least squares to\ncompute a low-rank CP-decomposition of the 4D convolution kernel tensor into a\nsum of a small number of rank-one tensors. At the second step, this\ndecomposition is used to replace the original convolutional layer with a\nsequence of four convolutional layers with small kernels. After such\nreplacement, the entire network is fine-tuned on the training data using\nstandard backpropagation process.\n  We evaluate this approach on two CNNs and show that it is competitive with\nprevious approaches, leading to higher obtained CPU speedups at the cost of\nlower accuracy drops for the smaller of the two networks. Thus, for the\n36-class character classification CNN, our approach obtains a 8.5x CPU speedup\nof the whole network with only minor accuracy drop (1% from 91% to 90%). For\nthe standard ImageNet architecture (AlexNet), the approach speeds up the second\nconvolution layer by a factor of 4x at the cost of $1\\%$ increase of the\noverall top-5 classification error.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 23:02:43 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 21:57:37 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 11:40:54 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Lebedev", "Vadim", ""], ["Ganin", "Yaroslav", ""], ["Rakhuba", "Maksim", ""], ["Oseledets", "Ivan", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1412.6558", "submitter": "David Sussillo", "authors": "David Sussillo, L.F. Abbott", "title": "Random Walk Initialization for Training Very Deep Feedforward Networks", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training very deep networks is an important open problem in machine learning.\nOne of many difficulties is that the norm of the back-propagated error gradient\ncan grow or decay exponentially. Here we show that training very deep\nfeed-forward networks (FFNs) is not as difficult as previously thought. Unlike\nwhen back-propagation is applied to a recurrent network, application to an FFN\namounts to multiplying the error gradient by a different random matrix at each\nlayer. We show that the successive application of correctly scaled random\nmatrices to an initial vector results in a random walk of the log of the norm\nof the resulting vectors, and we compute the scaling that makes this walk\nunbiased. The variance of the random walk grows only linearly with network\ndepth and is inversely proportional to the size of each layer. Practically,\nthis implies a gradient whose log-norm scales with the square root of the\nnetwork depth and shows that the vanishing gradient problem can be mitigated by\nincreasing the width of the layers. Mathematical analyses and experimental\nresults using stochastic gradient descent to optimize tasks related to the\nMNIST and TIMIT datasets are provided to support these claims. Equations for\nthe optimal matrix scaling are provided for the linear and ReLU cases.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 23:24:53 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 21:28:29 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 22:28:32 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Sussillo", "David", ""], ["Abbott", "L. F.", ""]]}, {"id": "1412.6563", "submitter": "David Warde-Farley", "authors": "David Warde-Farley, Andrew Rabinovich, Dragomir Anguelov", "title": "Self-informed neural network structure learning", "comments": "Updated with accepted workshop contribution header", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of large scale, multi-label visual recognition with a\nlarge number of possible classes. We propose a method for augmenting a trained\nneural network classifier with auxiliary capacity in a manner designed to\nsignificantly improve upon an already well-performing model, while minimally\nimpacting its computational footprint. Using the predictions of the network\nitself as a descriptor for assessing visual similarity, we define a\npartitioning of the label space into groups of visually similar entities. We\nthen augment the network with auxilliary hidden layer pathways with\nconnectivity only to these groups of label units. We report a significant\nimprovement in mean average precision on a large-scale object recognition task\nwith the augmented model, while increasing the number of multiply-adds by less\nthan 3%.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 00:05:57 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 21:35:29 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Warde-Farley", "David", ""], ["Rabinovich", "Andrew", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1412.6564", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison, Aja Huang, Ilya Sutskever, David Silver", "title": "Move Evaluation in Go Using Deep Convolutional Neural Networks", "comments": "Minor edits and included captures in Figure 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game of Go is more challenging than other board games, due to the\ndifficulty of constructing a position or move evaluation function. In this\npaper we investigate whether deep convolutional networks can be used to\ndirectly represent and learn this knowledge. We train a large 12-layer\nconvolutional neural network by supervised learning from a database of human\nprofessional games. The network correctly predicts the expert move in 55% of\npositions, equalling the accuracy of a 6 dan human player. When the trained\nconvolutional network was used directly to play games of Go, without any\nsearch, it beat the traditional search program GnuGo in 97% of games, and\nmatched the performance of a state-of-the-art Monte-Carlo tree search that\nsimulates a million positions per move.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 00:31:30 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 19:03:34 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Maddison", "Chris J.", ""], ["Huang", "Aja", ""], ["Sutskever", "Ilya", ""], ["Silver", "David", ""]]}, {"id": "1412.6568", "submitter": "Georgiana Dinu", "authors": "Georgiana Dinu, Angeliki Lazaridou, Marco Baroni", "title": "Improving zero-shot learning by mitigating the hubness problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The zero-shot paradigm exploits vector-based word representations extracted\nfrom text corpora with unsupervised methods to learn general mapping functions\nfrom other feature spaces onto word space, where the words associated to the\nnearest neighbours of the mapped vectors are used as their linguistic labels.\nWe show that the neighbourhoods of the mapped elements are strongly polluted by\nhubs, vectors that tend to be near a high proportion of items, pushing their\ncorrect labels down the neighbour list. After illustrating the problem\nempirically, we propose a simple method to correct it by taking the proximity\ndistribution of potential neighbours across many mapped vectors into account.\nWe show that this correction leads to consistent improvements in realistic\nzero-shot experiments in the cross-lingual, image labeling and image retrieval\ndomains.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 01:03:46 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 14:15:13 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2015 13:10:07 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Dinu", "Georgiana", ""], ["Lazaridou", "Angeliki", ""], ["Baroni", "Marco", ""]]}, {"id": "1412.6572", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy", "title": "Explaining and Harnessing Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several machine learning models, including neural networks, consistently\nmisclassify adversarial examples---inputs formed by applying small but\nintentionally worst-case perturbations to examples from the dataset, such that\nthe perturbed input results in the model outputting an incorrect answer with\nhigh confidence. Early attempts at explaining this phenomenon focused on\nnonlinearity and overfitting. We argue instead that the primary cause of neural\nnetworks' vulnerability to adversarial perturbation is their linear nature.\nThis explanation is supported by new quantitative results while giving the\nfirst explanation of the most intriguing fact about them: their generalization\nacross architectures and training sets. Moreover, this view yields a simple and\nfast method of generating adversarial examples. Using this approach to provide\nexamples for adversarial training, we reduce the test set error of a maxout\nnetwork on the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 01:17:12 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 17:25:05 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2015 20:19:16 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Shlens", "Jonathon", ""], ["Szegedy", "Christian", ""]]}, {"id": "1412.6577", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Claire Cardie", "title": "Modeling Compositionality with Multiplicative Recurrent Neural Networks", "comments": "10 pages, 2 figures, published at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 01:53:22 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 08:35:22 GMT"}, {"version": "v3", "created": "Sat, 2 May 2015 20:22:32 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Cardie", "Claire", ""]]}, {"id": "1412.6581", "submitter": "Joost van Amersfoort B.Sc.", "authors": "Otto Fabius, Joost R. van Amersfoort", "title": "Variational Recurrent Auto-Encoders", "comments": "Accepted at ICLR workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 02:07:07 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 15:04:29 GMT"}, {"version": "v3", "created": "Wed, 4 Feb 2015 11:21:39 GMT"}, {"version": "v4", "created": "Wed, 18 Feb 2015 15:42:44 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2015 14:01:18 GMT"}, {"version": "v6", "created": "Mon, 15 Jun 2015 12:35:11 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Fabius", "Otto", ""], ["van Amersfoort", "Joost R.", ""]]}, {"id": "1412.6583", "submitter": "Brian Cheung", "authors": "Brian Cheung, Jesse A. Livezey, Arjun K. Bansal, Bruno A. Olshausen", "title": "Discovering Hidden Factors of Variation in Deep Networks", "comments": "Presented at International Conference on Learning Representations\n  2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enjoyed a great deal of success because of its ability to\nlearn useful features for tasks such as classification. But there has been less\nexploration in learning the factors of variation apart from the classification\nsignal. By augmenting autoencoders with simple regularization terms during\ntraining, we demonstrate that standard deep architectures can discover and\nexplicitly represent factors of variation beyond those relevant for\ncategorization. We introduce a cross-covariance penalty (XCov) as a method to\ndisentangle factors like handwriting style for digits and subject identity in\nfaces. We demonstrate this on the MNIST handwritten digit database, the Toronto\nFaces Database (TFD) and the Multi-PIE dataset by generating manipulated\ninstances of the data. Furthermore, we demonstrate these deep networks can\nextrapolate `hidden' variation in the supervised signal.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 02:52:03 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:41:40 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 17:15:02 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 06:47:48 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Cheung", "Brian", ""], ["Livezey", "Jesse A.", ""], ["Bansal", "Arjun K.", ""], ["Olshausen", "Bruno A.", ""]]}, {"id": "1412.6586", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mohammad Javad Shafiee, Parthipan Siva, and Xiao Yu\n  Wang", "title": "A deep-structured fully-connected random field model for structured\n  inference", "comments": "Accepted, 13 pages", "journal-ref": "IEEE Access Journal, vol. 3, pp. 469-477, 2015", "doi": "10.1109/ACCESS.2015.2425304", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant interest in the use of fully-connected graphical\nmodels and deep-structured graphical models for the purpose of structured\ninference. However, fully-connected and deep-structured graphical models have\nbeen largely explored independently, leaving the unification of these two\nconcepts ripe for exploration. A fundamental challenge with unifying these two\ntypes of models is in dealing with computational complexity. In this study, we\ninvestigate the feasibility of unifying fully-connected and deep-structured\nmodels in a computationally tractable manner for the purpose of structured\ninference. To accomplish this, we introduce a deep-structured fully-connected\nrandom field (DFRF) model that integrates a series of intermediate sparse\nauto-encoding layers placed between state layers to significantly reduce\ncomputational complexity. The problem of image segmentation was used to\nillustrate the feasibility of using the DFRF for structured inference in a\ncomputationally tractable manner. Results in this study show that it is\nfeasible to unify fully-connected and deep-structured models in a\ncomputationally tractable manner for solving structured inference problems such\nas image segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 03:02:32 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 21:34:22 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 23:05:49 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wong", "Alexander", ""], ["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Wang", "Xiao Yu", ""]]}, {"id": "1412.6596", "submitter": "Scott Reed", "authors": "Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru\n  Erhan, Andrew Rabinovich", "title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art deep learning systems for visual object recognition\nand detection use purely supervised training with regularization such as\ndropout to avoid overfitting. The performance depends critically on the amount\nof labeled examples, and in current practice the labels are assumed to be\nunambiguous and accurate. However, this assumption often does not hold; e.g. in\nrecognition, class labels may be missing; in detection, objects in the image\nmay not be localized; and in general, the labeling may be subjective. In this\nwork we propose a generic way to handle noisy and incomplete labeling by\naugmenting the prediction objective with a notion of consistency. We consider a\nprediction consistent if the same prediction is made given similar percepts,\nwhere the notion of similarity is between deep network features computed from\nthe input data. In experiments we demonstrate that our approach yields\nsubstantial robustness to label noise on several datasets. On MNIST handwritten\ndigits, we show that our model is robust to label corruption. On the Toronto\nFace Database, we show that our model handles well the case of subjective\nlabels in emotion recognition, achieving state-of-the- art results, and can\nalso benefit from unlabeled face images with no modification to our method. On\nthe ILSVRC2014 detection challenge data, we show that our approach extends to\nvery deep networks, high resolution images and structured outputs, and results\nin improved scalable detection.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:11:33 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 22:30:39 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2015 19:48:37 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Reed", "Scott", ""], ["Lee", "Honglak", ""], ["Anguelov", "Dragomir", ""], ["Szegedy", "Christian", ""], ["Erhan", "Dumitru", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1412.6597", "submitter": "Tom Paine", "authors": "Tom Le Paine, Pooya Khorrami, Wei Han, Thomas S. Huang", "title": "An Analysis of Unsupervised Pre-training in Light of Recent Advances", "comments": "Accepted as a workshop contribution to ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks perform well on object recognition because of a\nnumber of recent advances: rectified linear units (ReLUs), data augmentation,\ndropout, and large labelled datasets. Unsupervised data has been proposed as\nanother way to improve performance. Unfortunately, unsupervised pre-training is\nnot used by state-of-the-art methods leading to the following question: Is\nunsupervised pre-training still useful given recent advances? If so, when? We\nanswer this in three parts: we 1) develop an unsupervised method that\nincorporates ReLUs and recent unsupervised regularization techniques, 2)\nanalyze the benefits of unsupervised pre-training compared to data augmentation\nand dropout on CIFAR-10 while varying the ratio of unsupervised to supervised\nsamples, 3) verify our findings on STL-10. We discover unsupervised\npre-training, as expected, helps when the ratio of unsupervised to supervised\nsamples is high, and surprisingly, hurts when the ratio is low. We also use\nunsupervised pre-training with additional color augmentation to achieve near\nstate-of-the-art performance on STL-10.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:20:55 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 22:03:40 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 21:05:34 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 21:26:31 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Paine", "Tom Le", ""], ["Khorrami", "Pooya", ""], ["Han", "Wei", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1412.6598", "submitter": "Sobhan Naderi Parizi", "authors": "Sobhan Naderi Parizi, Andrea Vedaldi, Andrew Zisserman and Pedro\n  Felzenszwalb", "title": "Automatic Discovery and Optimization of Parts for Image Classification", "comments": "19 pages, template changed to camera ready version, 1 reference\n  added, 1 reference fixed, Fig. 3, 4 updated (larger text)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-based representations have been shown to be very useful for image\nclassification. Learning part-based models is often viewed as a two-stage\nproblem. First, a collection of informative parts is discovered, using\nheuristics that promote part distinctiveness and diversity, and then\nclassifiers are trained on the vector of part responses. In this paper we unify\nthe two stages and learn the image classifiers and a set of shared parts\njointly. We generate an initial pool of parts by randomly sampling part\ncandidates and selecting a good subset using L1/L2 regularization. All steps\nare driven \"directly\" by the same objective namely the classification loss on a\ntraining set. This lets us do away with engineered heuristics. We also\nintroduce the notion of \"negative parts\", intended as parts that are negatively\ncorrelated with one or more classes. Negative parts are complementary to the\nparts discovered by other methods, which look only for positive correlations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:25:34 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 20:13:40 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Parizi", "Sobhan Naderi", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""], ["Felzenszwalb", "Pedro", ""]]}, {"id": "1412.6599", "submitter": "Kevin Bache", "authors": "Kevin Bache, Dennis DeCoste, Padhraic Smyth", "title": "Hot Swapping for Online Adaptation of Optimization Hyperparameters", "comments": "Submission to ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general framework for online adaptation of optimization\nhyperparameters by `hot swapping' their values during learning. We investigate\nthis approach in the context of adaptive learning rate selection using an\nexplore-exploit strategy from the multi-armed bandit literature. Experiments on\na benchmark neural network show that the hot swapping approach leads to\nconsistently better solutions compared to well-known alternatives such as\nAdaDelta and stochastic gradient with exhaustive hyperparameter search.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:36:28 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 05:18:18 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 23:28:01 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Bache", "Kevin", ""], ["DeCoste", "Dennis", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1412.6601", "submitter": "Afroze Ibrahim Baqapuri", "authors": "Afroze Ibrahim Baqapuri and Ilya Trofimov", "title": "Using Neural Networks for Click Prediction of Sponsored Search", "comments": "updated typos, and removed conference header", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sponsored search is a multi-billion dollar industry and makes up a major\nsource of revenue for search engines (SE). click-through-rate (CTR) estimation\nplays a crucial role for ads selection, and greatly affects the SE revenue,\nadvertiser traffic and user experience. We propose a novel architecture for\nsolving CTR prediction problem by combining artificial neural networks (ANN)\nwith decision trees. First we compare ANN with respect to other popular machine\nlearning models being used for this task. Then we go on to combine ANN with\nMatrixNet (proprietary implementation of boosted trees) and evaluate the\nperformance of the system as a whole. The results show that our approach\nprovides significant improvement over existing models.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:44:00 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 08:15:40 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2015 23:30:51 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Baqapuri", "Afroze Ibrahim", ""], ["Trofimov", "Ilya", ""]]}, {"id": "1412.6604", "submitter": "Marc'Aurelio Ranzato", "authors": "MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan\n  Collobert, Sumit Chopra", "title": "Video (language) modeling: a baseline for generative models of natural\n  videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a strong baseline model for unsupervised feature learning using\nvideo data. By learning to predict missing frames or extrapolate future frames\nfrom an input video sequence, the model discovers both spatial and temporal\ncorrelations which are useful to represent complex deformations and motion\npatterns. The models we propose are largely borrowed from the language modeling\nliterature, and adapted to the vision domain by quantizing the space of image\npatches into a large dictionary. We demonstrate the approach on both a filling\nand a generation task. For the first time, we show that, after training on\nnatural videos, such a model can predict non-trivial motions over short video\nsequences.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:05:51 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 01:49:29 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2015 15:03:04 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2015 03:39:55 GMT"}, {"version": "v5", "created": "Wed, 4 May 2016 14:01:42 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Ranzato", "MarcAurelio", ""], ["Szlam", "Arthur", ""], ["Bruna", "Joan", ""], ["Mathieu", "Michael", ""], ["Collobert", "Ronan", ""], ["Chopra", "Sumit", ""]]}, {"id": "1412.6606", "submitter": "Roy Frostig", "authors": "Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford", "title": "Competing with the Empirical Risk Minimizer in a Single Pass", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many estimation problems, e.g. linear and logistic regression, we wish to\nminimize an unknown objective given only unbiased samples of the objective\nfunction. Furthermore, we aim to achieve this using as few samples as possible.\nIn the absence of computational constraints, the minimizer of a sample average\nof observed data -- commonly referred to as either the empirical risk minimizer\n(ERM) or the $M$-estimator -- is widely regarded as the estimation strategy of\nchoice due to its desirable statistical convergence properties. Our goal in\nthis work is to perform as well as the ERM, on every problem, while minimizing\nthe use of computational resources such as running time and space usage.\n  We provide a simple streaming algorithm which, under standard regularity\nassumptions on the underlying problem, enjoys the following properties:\n  * The algorithm can be implemented in linear time with a single pass of the\nobserved data, using space linear in the size of a single sample.\n  * The algorithm achieves the same statistical rate of convergence as the\nempirical risk minimizer on every problem, even considering constant factors.\n  * The algorithm's performance depends on the initial error at a rate that\ndecreases super-polynomially.\n  * The algorithm is easily parallelizable.\n  Moreover, we quantify the (finite-sample) rate at which the algorithm becomes\ncompetitive with the ERM.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:14:13 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 19:13:05 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Frostig", "Roy", ""], ["Ge", "Rong", ""], ["Kakade", "Sham M.", ""], ["Sidford", "Aaron", ""]]}, {"id": "1412.6610", "submitter": "Daniel Jiwoong  Im", "authors": "Daniel Jiwoong Im and Graham W. Taylor", "title": "Scoring and Classifying with Gated Auto-encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-encoders are perhaps the best-known non-probabilistic methods for\nrepresentation learning. They are conceptually simple and easy to train. Recent\ntheoretical work has shed light on their ability to capture manifold structure,\nand drawn connections to density modelling. This has motivated researchers to\nseek ways of auto-encoder scoring, which has furthered their use in\nclassification. Gated auto-encoders (GAEs) are an interesting and flexible\nextension of auto-encoders which can learn transformations among different\nimages or pixel covariances within images. However, they have been much less\nstudied, theoretically or empirically. In this work, we apply a dynamical\nsystems view to GAEs, deriving a scoring function, and drawing connections to\nRestricted Boltzmann Machines. On a set of deep learning benchmarks, we also\ndemonstrate their effectiveness for single and multi-label classification.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:46:05 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 18:05:21 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 16:35:39 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 18:15:16 GMT"}, {"version": "v5", "created": "Mon, 15 Jun 2015 00:25:47 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1412.6614", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ryota Tomioka, Nathan Srebro", "title": "In Search of the Real Inductive Bias: On the Role of Implicit\n  Regularization in Deep Learning", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present experiments demonstrating that some other form of capacity\ncontrol, different from network size, plays a central role in learning\nmultilayer feed-forward networks. We argue, partially through analogy to matrix\nfactorization, that this is an inductive bias that can help shed light on deep\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 06:52:25 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 21:00:09 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 18:51:37 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2015 18:48:31 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Tomioka", "Ryota", ""], ["Srebro", "Nathan", ""]]}, {"id": "1412.6615", "submitter": "Levent Sagun", "authors": "Levent Sagun, V. Ugur Guney, Gerard Ben Arous, Yann LeCun", "title": "Explorations on high dimensional landscapes", "comments": "11 pages, 8 figures, workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding minima of a real valued non-convex function over a high dimensional\nspace is a major challenge in science. We provide evidence that some such\nfunctions that are defined on high dimensional domains have a narrow band of\nvalues whose pre-image contains the bulk of its critical points. This is in\ncontrast with the low dimensional picture in which this band is wide. Our\nsimulations agree with the previous theoretical work on spin glasses that\nproves the existence of such a band when the dimension of the domain tends to\ninfinity. Furthermore our experiments on teacher-student networks with the\nMNIST dataset establish a similar phenomenon in deep networks. We finally\nobserve that both the gradient descent and the stochastic gradient descent\nmethods can reach this level within the same number of steps.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 06:57:12 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 01:29:56 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 10:08:16 GMT"}, {"version": "v4", "created": "Mon, 6 Apr 2015 21:47:50 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Sagun", "Levent", ""], ["Guney", "V. Ugur", ""], ["Arous", "Gerard Ben", ""], ["LeCun", "Yann", ""]]}, {"id": "1412.6616", "submitter": "Abram Demski", "authors": "Abram Demski, Volkan Ustun, Paul Rosenbloom, Cody Kommers", "title": "Outperforming Word2Vec on Analogy Tasks with Random Projections", "comments": "This paper has been withdrawn due to problems pointed out in review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed vector representation based on a simplification of\nthe BEAGLE system, designed in the context of the Sigma cognitive architecture.\nOur method does not require gradient-based training of neural networks, matrix\ndecompositions as with LSA, or convolutions as with BEAGLE. All that is\ninvolved is a sum of random vectors and their pointwise products. Despite the\nsimplicity of this technique, it gives state-of-the-art results on analogy\nproblems, in most cases better than Word2Vec. To explain this success, we\ninterpret it as a dimension reduction via random projection.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:07:29 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 17:31:58 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Demski", "Abram", ""], ["Ustun", "Volkan", ""], ["Rosenbloom", "Paul", ""], ["Kommers", "Cody", ""]]}, {"id": "1412.6617", "submitter": "Daniel Jiwoong  Im", "authors": "Daniel Jiwoong Im, Ethan Buchman, Graham W. Taylor", "title": "Understanding Minimum Probability Flow for RBMs Under Various Kinds of\n  Dynamics", "comments": "Nine pages including the reference page plus one page appendix.\n  Appeared at ICLR2015 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-based models are popular in machine learning due to the elegance of\ntheir formulation and their relationship to statistical physics. Among these,\nthe Restricted Boltzmann Machine (RBM), and its staple training algorithm\ncontrastive divergence (CD), have been the prototype for some recent\nadvancements in the unsupervised training of deep neural networks. However, CD\nhas limited theoretical motivation, and can in some cases produce undesirable\nbehavior. Here, we investigate the performance of Minimum Probability Flow\n(MPF) learning for training RBMs. Unlike CD, with its focus on approximating an\nintractable partition function via Gibbs sampling, MPF proposes a tractable,\nconsistent, objective function defined in terms of a Taylor expansion of the KL\ndivergence with respect to sampling dynamics. Here we propose a more general\nform for the sampling dynamics in MPF, and explore the consequences of\ndifferent choices for these dynamics for training RBMs. Experimental results\nshow MPF outperforming CD for various RBM configurations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:08:37 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2015 16:03:15 GMT"}, {"version": "v3", "created": "Mon, 12 Jan 2015 16:27:21 GMT"}, {"version": "v4", "created": "Sun, 1 Mar 2015 17:37:09 GMT"}, {"version": "v5", "created": "Thu, 2 Apr 2015 20:10:35 GMT"}, {"version": "v6", "created": "Tue, 7 Apr 2015 20:57:05 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Buchman", "Ethan", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1412.6618", "submitter": "Martin Kiefel", "authors": "Martin Kiefel, Varun Jampani and Peter V. Gehler", "title": "Permutohedral Lattice CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a convolutional layer that is able to process sparse\ninput features. As an example, for image recognition problems this allows an\nefficient filtering of signals that do not lie on a dense grid (like pixel\nposition), but of more general features (such as color values). The presented\nalgorithm makes use of the permutohedral lattice data structure. The\npermutohedral lattice was introduced to efficiently implement a bilateral\nfilter, a commonly used image processing operation. Its use allows for a\ngeneralization of the convolution type found in current (spatial) convolutional\nnetwork architectures.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:08:54 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 14:16:58 GMT"}, {"version": "v3", "created": "Sun, 3 May 2015 11:26:34 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Kiefel", "Martin", ""], ["Jampani", "Varun", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1412.6621", "submitter": "Arnab Paul", "authors": "Arnab Paul, Suresh Venkatasubramanian", "title": "Why does Deep Learning work? - A perspective from Group Theory", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called pre-training: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:28:46 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 02:22:01 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 07:19:35 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Paul", "Arnab", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1412.6622", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Nir Ailon", "title": "Deep metric learning using Triplet network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has proven itself as a successful set of models for learning\nuseful semantic representations of data. These, however, are mostly implicitly\nlearned as part of a classification task. In this paper we propose the triplet\nnetwork model, which aims to learn useful representations by distance\ncomparisons. A similar model was defined by Wang et al. (2014), tailor made for\nlearning a ranking for image information retrieval. Here we demonstrate using\nvarious datasets that our model learns a better representation than that of its\nimmediate competitor, the Siamese network. We also discuss future possible\nusage as a framework for unsupervised learning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:34:50 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 17:28:52 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 09:00:25 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 15:35:35 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Hoffer", "Elad", ""], ["Ailon", "Nir", ""]]}, {"id": "1412.6623", "submitter": "Luke Vilnis", "authors": "Luke Vilnis, Andrew McCallum", "title": "Word Representations via Gaussian Embedding", "comments": "12 pages, published as conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current work in lexical distributed representations maps each word to a point\nvector in low-dimensional space. Mapping instead to a density provides many\ninteresting advantages, including better capturing uncertainty about a\nrepresentation and its relationships, expressing asymmetries more naturally\nthan dot product or cosine similarity, and enabling more expressive\nparameterization of decision boundaries. This paper advocates for density-based\ndistributed embeddings and presents a method for learning representations in\nthe space of Gaussian distributions. We compare performance on various word\nembedding benchmarks, investigate the ability of these embeddings to model\nentailment and other asymmetric relationships, and explore novel properties of\nthe representation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:42:40 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 14:24:28 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 18:19:11 GMT"}, {"version": "v4", "created": "Fri, 1 May 2015 10:14:58 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Vilnis", "Luke", ""], ["McCallum", "Andrew", ""]]}, {"id": "1412.6630", "submitter": "Jan Rudy", "authors": "Jan Rudy, Weiguang Ding, Daniel Jiwoong Im, Graham W. Taylor", "title": "Neural Network Regularization via Robust Weight Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is essential when training large neural networks. As deep\nneural networks can be mathematically interpreted as universal function\napproximators, they are effective at memorizing sampling noise in the training\ndata. This results in poor generalization to unseen data. Therefore, it is no\nsurprise that a new regularization technique, Dropout, was partially\nresponsible for the now-ubiquitous winning entry to ImageNet 2012 by the\nUniversity of Toronto. Currently, Dropout (and related methods such as\nDropConnect) are the most effective means of regularizing large neural\nnetworks. These amount to efficiently visiting a large number of related models\nat training time, while aggregating them to a single predictor at test time.\nThe proposed FaMe model aims to apply a similar strategy, yet learns a\nfactorization of each weight matrix such that the factors are robust to noise.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:59:14 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 13:28:46 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Rudy", "Jan", ""], ["Ding", "Weiguang", ""], ["Im", "Daniel Jiwoong", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1412.6632", "submitter": "Junhua Mao", "authors": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille", "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "comments": "Add a simple strategy to boost the performance of image captioning\n  task significantly. More details are shown in Section 8 of the paper. The\n  code and related data are available at https://github.com/mjhucla/mRNN-CR ;.\n  arXiv admin note: substantial text overlap with arXiv:1410.1090", "journal-ref": "ICLR 2015", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel image captions. It directly models the probability\ndistribution of generating a word given previous words and an image. Image\ncaptions are generated by sampling from this distribution. The model consists\nof two sub-networks: a deep recurrent neural network for sentences and a deep\nconvolutional network for images. These two sub-networks interact with each\nother in a multimodal layer to form the whole m-RNN model. The effectiveness of\nour model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K,\nFlickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In\naddition, we apply the m-RNN model to retrieval tasks for retrieving images or\nsentences, and achieves significant performance improvement over the\nstate-of-the-art methods which directly optimize the ranking objective function\nfor retrieval. The project page of this work is:\nwww.stat.ucla.edu/~junhua.mao/m-RNN.html .\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 08:10:04 GMT"}, {"version": "v2", "created": "Fri, 26 Dec 2014 08:24:11 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 04:17:48 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 21:03:35 GMT"}, {"version": "v5", "created": "Thu, 11 Jun 2015 15:26:58 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Wei", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Huang", "Zhiheng", ""], ["Yuille", "Alan", ""]]}, {"id": "1412.6645", "submitter": "Gabriel Synnaeve", "authors": "Gabriel Synnaeve, Emmanuel Dupoux", "title": "Weakly Supervised Multi-Embeddings Learning of Acoustic Models", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained a Siamese network with multi-task same/different information on a\nspeech dataset, and found that it was possible to share a network for both\ntasks without a loss in performance. The first task was to discriminate between\ntwo same or different words, and the second was to discriminate between two\nsame or different talkers.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 11:54:41 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 10:09:09 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 12:35:32 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Synnaeve", "Gabriel", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "1412.6650", "submitter": "Alex Ter-Sarkisov", "authors": "Aram Ter-Sarkisov, Holger Schwenk, Loic Barrault and Fethi Bougares", "title": "Incremental Adaptation Strategies for Neural Network Language Models", "comments": "accepted as workshop paper at ACL-IJCNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is today acknowledged that neural network language models outperform\nbackoff language models in applications like speech recognition or statistical\nmachine translation. However, training these models on large amounts of data\ncan take several days. We present efficient techniques to adapt a neural\nnetwork language model to new data. Instead of training a completely new model\nor relying on mixture approaches, we propose two new methods: continued\ntraining on resampled data or insertion of adaptation layers. We present\nexperimental results in an CAT environment where the post-edits of professional\ntranslators are used to improve an SMT system. Both methods are very fast and\nachieve significant improvements without overfitting the small adaptation data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 13:06:05 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 13:43:19 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 11:36:36 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2015 14:54:51 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Ter-Sarkisov", "Aram", ""], ["Schwenk", "Holger", ""], ["Barrault", "Loic", ""], ["Bougares", "Fethi", ""]]}, {"id": "1412.6651", "submitter": "Sixin Zhang Sixin Zhang", "authors": "Sixin Zhang, Anna Choromanska, Yann LeCun", "title": "Deep learning with Elastic Averaging SGD", "comments": "NIPS2015 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of stochastic optimization for deep learning in the\nparallel computing environment under communication constraints. A new algorithm\nis proposed in this setting where the communication and coordination of work\namong concurrent processes (local workers), is based on an elastic force which\nlinks the parameters they compute with a center variable stored by the\nparameter server (master). The algorithm enables the local workers to perform\nmore exploration, i.e. the algorithm allows the local variables to fluctuate\nfurther from the center variable by reducing the amount of communication\nbetween local workers and the master. We empirically demonstrate that in the\ndeep learning setting, due to the existence of many local optima, allowing more\nexploration can lead to the improved performance. We propose synchronous and\nasynchronous variants of the new algorithm. We provide the stability analysis\nof the asynchronous variant in the round-robin scheme and compare it with the\nmore common parallelized method ADMM. We show that the stability of EASGD is\nguaranteed when a simple stability condition is satisfied, which is not the\ncase for ADMM. We additionally propose the momentum-based version of our\nalgorithm that can be applied in both synchronous and asynchronous settings.\nAsynchronous variant of the algorithm is applied to train convolutional neural\nnetworks for image classification on the CIFAR and ImageNet datasets.\nExperiments demonstrate that the new algorithm accelerates the training of deep\narchitectures compared to DOWNPOUR and other common baseline approaches and\nfurthermore is very communication efficient.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 13:22:23 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 20:50:02 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 01:18:40 GMT"}, {"version": "v4", "created": "Wed, 25 Feb 2015 19:00:29 GMT"}, {"version": "v5", "created": "Wed, 29 Apr 2015 11:56:24 GMT"}, {"version": "v6", "created": "Sat, 6 Jun 2015 00:20:58 GMT"}, {"version": "v7", "created": "Sat, 8 Aug 2015 02:52:48 GMT"}, {"version": "v8", "created": "Sun, 25 Oct 2015 12:12:52 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Zhang", "Sixin", ""], ["Choromanska", "Anna", ""], ["LeCun", "Yann", ""]]}, {"id": "1412.6734", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Panos Toulis, Shie Mannor, Edoardo M. Airoldi", "title": "Implicit Temporal Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, the TD($\\lambda$) algorithm is a fundamental\npolicy evaluation method with an efficient online implementation that is\nsuitable for large-scale problems. One practical drawback of TD($\\lambda$) is\nits sensitivity to the choice of the step-size. It is an empirically well-known\nfact that a large step-size leads to fast convergence, at the cost of higher\nvariance and risk of instability. In this work, we introduce the implicit\nTD($\\lambda$) algorithm which has the same function and computational cost as\nTD($\\lambda$), but is significantly more stable. We provide a theoretical\nexplanation of this stability and an empirical evaluation of implicit\nTD($\\lambda$) on typical benchmark tasks. Our results show that implicit\nTD($\\lambda$) outperforms standard TD($\\lambda$) and a state-of-the-art method\nthat automatically tunes the step-size, and thus shows promise for wide\napplicability.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 06:53:15 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Tamar", "Aviv", ""], ["Toulis", "Panos", ""], ["Mannor", "Shie", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1412.6741", "submitter": "Cheuk Ting Li", "authors": "Kim-Hung Li and Cheuk Ting Li", "title": "Locally Weighted Learning for Naive Bayes Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a consequence of the strong and usually violated conditional independence\nassumption (CIA) of naive Bayes (NB) classifier, the performance of NB becomes\nless and less favorable compared to sophisticated classifiers when the sample\nsize increases. We learn from this phenomenon that when the size of the\ntraining data is large, we should either relax the assumption or apply NB to a\n\"reduced\" data set, say for example use NB as a local model. The latter\napproach trades the ignored information for the robustness to the model\nassumption. In this paper, we consider using NB as a model for locally weighted\ndata. A special weighting function is designed so that if CIA holds for the\nunweighted data, it also holds for the weighted data. The new method is\nintuitive and capable of handling class imbalance. It is theoretically more\nsound than the locally weighted learners of naive Bayes that base\nclassification only on the $k$ nearest neighbors. Empirical study shows that\nthe new method with appropriate choice of parameter outperforms seven existing\nclassifiers of similar nature.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 08:30:57 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Li", "Kim-Hung", ""], ["Li", "Cheuk Ting", ""]]}, {"id": "1412.6752", "submitter": "Abdulrahman Ibraheem", "authors": "Abdulrahman Oladipupo Ibraheem", "title": "Correlation of Data Reconstruction Error and Shrinkages in Pair-wise\n  Distances under Principal Component Analysis (PCA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this on-going work, I explore certain theoretical and empirical\nimplications of data transformations under the PCA. In particular, I state and\nprove three theorems about PCA, which I paraphrase as follows: 1). PCA without\ndiscarding eigenvector rows is injective, but looses this injectivity when\neigenvector rows are discarded 2). PCA without discarding eigen- vector rows\npreserves pair-wise distances, but tends to cause pair-wise distances to shrink\nwhen eigenvector rows are discarded. 3). For any pair of points, the shrinkage\nin pair-wise distance is bounded above by an L1 norm reconstruction error\nassociated with the points. Clearly, 3). suggests that there might exist some\ncorrelation between shrinkages in pair-wise distances and mean square\nreconstruction error which is defined as the sum of those eigenvalues\nassociated with the discarded eigenvectors. I therefore decided to perform\nnumerical experiments to obtain the corre- lation between the sum of those\neigenvalues and shrinkages in pair-wise distances. In addition, I have also\nperformed some experiments to check respectively the effect of the sum of those\neigenvalues and the effect of the shrinkages on classification accuracies under\nthe PCA map. So far, I have obtained the following results on some publicly\navailable data from the UCI Machine Learning Repository: 1). There seems to be\na strong cor- relation between the sum of those eigenvalues associated with\ndiscarded eigenvectors and shrinkages in pair-wise distances. 2). Neither the\nsum of those eigenvalues nor pair-wise distances have any strong correlations\nwith classification accuracies. 1\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 09:50:32 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Ibraheem", "Abdulrahman Oladipupo", ""]]}, {"id": "1412.6785", "submitter": "Sotetsu Koyamada", "authors": "Sotetsu Koyamada and Masanori Koyama and Ken Nakae and Shin Ishii", "title": "Principal Sensitivity Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-18038-0_48", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm (Principal Sensitivity Analysis; PSA) to analyze\nthe knowledge of the classifier obtained from supervised machine learning\ntechniques. In particular, we define principal sensitivity map (PSM) as the\ndirection on the input space to which the trained classifier is most sensitive,\nand use analogously defined k-th PSM to define a basis for the input space. We\ntrain neural networks with artificial data and real data, and apply the\nalgorithm to the obtained supervised classifiers. We then visualize the PSMs to\ndemonstrate the PSA's ability to decompose the knowledge acquired by the\ntrained classifiers.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 13:40:29 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2015 16:18:47 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Koyamada", "Sotetsu", ""], ["Koyama", "Masanori", ""], ["Nakae", "Ken", ""], ["Ishii", "Shin", ""]]}, {"id": "1412.6806", "submitter": "Alexey Dosovitskiy", "authors": "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin\n  Riedmiller", "title": "Striving for Simplicity: The All Convolutional Net", "comments": "accepted to ICLR-2015 workshop track; no changes other than style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding -- and building on other recent work for\nfinding simple network structures -- we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n\"deconvolution approach\" for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 16:16:37 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 21:44:06 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 07:58:17 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Springenberg", "Jost Tobias", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1412.6808", "submitter": "Waheed Bajwa", "authors": "Tong Wu and Waheed U. Bajwa", "title": "Learning the nonlinear geometry of high-dimensional data: Models and\n  algorithms", "comments": "Extended version of the journal paper accepted for publication in\n  IEEE Trans. Signal Processing (20 pages, 7 figures, 4 tables)", "journal-ref": "IEEE Trans. Signal Processing, vol. 63, no. 23, pp. 6229-6244,\n  Dec. 2015", "doi": "10.1109/TSP.2015.2469637", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern information processing relies on the axiom that high-dimensional data\nlie near low-dimensional geometric structures. This paper revisits the problem\nof data-driven learning of these geometric structures and puts forth two new\nnonlinear geometric models for data describing \"related\" objects/phenomena. The\nfirst one of these models straddles the two extremes of the subspace model and\nthe union-of-subspaces model, and is termed the metric-constrained\nunion-of-subspaces (MC-UoS) model. The second one of these models---suited for\ndata drawn from a mixture of nonlinear manifolds---generalizes the kernel\nsubspace model, and is termed the metric-constrained kernel union-of-subspaces\n(MC-KUoS) model. The main contributions of this paper in this regard include\nthe following. First, it motivates and formalizes the problems of MC-UoS and\nMC-KUoS learning. Second, it presents algorithms that efficiently learn an\nMC-UoS or an MC-KUoS underlying data of interest. Third, it extends these\nalgorithms to the case when parts of the data are missing. Last, but not least,\nit reports the outcomes of a series of numerical experiments involving both\nsynthetic and real data that demonstrate the superiority of the proposed\ngeometric models and learning algorithms over existing approaches in the\nliterature. These experiments also help clarify the connections between this\nwork and the literature on (subspace and kernel k-means) clustering.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 16:40:31 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 02:12:06 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Wu", "Tong", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1412.6815", "submitter": "Misha Denil", "authors": "Misha Denil and Alban Demiraj and Nando de Freitas", "title": "Extraction of Salient Sentences from Labelled Documents", "comments": "arXiv admin note: substantial text overlap with arXiv:1406.3830", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical convolutional document model with an architecture\ndesigned to support introspection of the document structure. Using this model,\nwe show how to use visualisation techniques from the computer vision literature\nto identify and extract topic-relevant sentences.\n  We also introduce a new scalable evaluation technique for automatic sentence\nextraction systems that avoids the need for time consuming human annotation of\nvalidation data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 17:38:19 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 23:57:08 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Denil", "Misha", ""], ["Demiraj", "Alban", ""], ["de Freitas", "Nando", ""]]}, {"id": "1412.6821", "submitter": "Roland Kwitt", "authors": "Jan Reininghaus, Stefan Huber, Ulrich Bauer, Roland Kwitt", "title": "A Stable Multi-Scale Kernel for Topological Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis offers a rich source of valuable information to\nstudy vision problems. Yet, so far we lack a theoretically sound connection to\npopular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In\nthis work, we establish such a connection by designing a multi-scale kernel for\npersistence diagrams, a stable summary representation of topological features\nin data. We show that this kernel is positive definite and prove its stability\nwith respect to the 1-Wasserstein distance. Experiments on two benchmark\ndatasets for 3D shape classification/retrieval and texture recognition show\nconsiderable performance gains of the proposed method compared to an\nalternative approach that is based on the recently introduced persistence\nlandscapes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 19:17:08 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Reininghaus", "Jan", ""], ["Huber", "Stefan", ""], ["Bauer", "Ulrich", ""], ["Kwitt", "Roland", ""]]}, {"id": "1412.6830", "submitter": "Forest Agostinelli", "authors": "Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi", "title": "Learning Activation Functions to Improve Deep Neural Networks", "comments": "Accepted as a workshop paper contribution at the International\n  Conference on Learning Representations (ICLR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks typically have a fixed, non-linear activation\nfunction at each neuron. We have designed a novel form of piecewise linear\nactivation function that is learned independently for each neuron using\ngradient descent. With this adaptive activation function, we are able to\nimprove upon deep neural network architectures composed of static rectified\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\nboson decay modes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 20:20:21 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 21:44:41 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 08:05:02 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Agostinelli", "Forest", ""], ["Hoffman", "Matthew", ""], ["Sadowski", "Peter", ""], ["Baldi", "Pierre", ""]]}, {"id": "1412.6857", "submitter": "Tyng-Luh Liu", "authors": "Jyh-Jing Hwang and Tyng-Luh Liu", "title": "Contour Detection Using Cost-Sensitive Convolutional Neural Networks", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of contour detection via per-pixel classifications of\nedge point. To facilitate the process, the proposed approach leverages with\nDenseNet, an efficient implementation of multiscale convolutional neural\nnetworks (CNNs), to extract an informative feature vector for each pixel and\nuses an SVM classifier to accomplish contour detection. The main challenge lies\nin adapting a pre-trained per-image CNN model for yielding per-pixel image\nfeatures. We propose to base on the DenseNet architecture to achieve pixelwise\nfine-tuning and then consider a cost-sensitive strategy to further improve the\nlearning with a small dataset of edge and non-edge image patches. In the\nexperiment of contour detection, we look into the effectiveness of combining\nper-pixel features from different CNN layers and obtain comparable performances\nto the state-of-the-art on BSDS500.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 01:16:50 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 14:37:27 GMT"}, {"version": "v3", "created": "Thu, 15 Jan 2015 15:01:16 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 07:37:54 GMT"}, {"version": "v5", "created": "Tue, 12 May 2015 08:42:42 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Hwang", "Jyh-Jing", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1412.6881", "submitter": "Jinseok Nam", "authors": "Jinseok Nam and Johannes F\\\"urnkranz", "title": "On Learning Vector Representations in Hierarchical Label Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in multi-label classification is to capture label\npatterns or underlying structures that have an impact on such patterns. This\npaper addresses one such problem, namely how to exploit hierarchical structures\nover labels. We present a novel method to learn vector representations of a\nlabel space given a hierarchy of labels and label co-occurrence patterns. Our\nexperimental results demonstrate qualitatively that the proposed method is able\nto learn regularities among labels by exploiting a label hierarchy as well as\nlabel co-occurrences. It highlights the importance of the hierarchical\ninformation in order to obtain regularities which facilitate analogical\nreasoning over a label space. We also experimentally illustrate the dependency\nof the learned representations on the label hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 06:12:06 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 17:12:04 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 19:23:23 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Nam", "Jinseok", ""], ["F\u00fcrnkranz", "Johannes", ""]]}, {"id": "1412.6885", "submitter": "Jun Yuan", "authors": "Jun Yuan, Bingbing Ni, Ashraf A.Kassim", "title": "Half-CNN: A General Framework for Whole-Image Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Network (CNN) has achieved great success in image\nclassification. The classification model can also be utilized at image or patch\nlevel for many other applications, such as object detection and segmentation.\nIn this paper, we propose a whole-image CNN regression model, by removing the\nfull connection layer and training the network with continuous feature maps.\nThis is a generic regression framework that fits many applications. We\ndemonstrate this method through two tasks: simultaneous face detection &\nsegmentation, and scene saliency prediction. The result is comparable with\nother models in the respective fields, using only a small scale network. Since\nthe regression model is trained on corresponding image / feature map pairs,\nthere are no requirements on uniform input size as opposed to the\nclassification model. Our framework avoids classifier design, a process that\nmay introduce too much manual intervention in model development. Yet, it is\nhighly correlated to the classification network and offers some in-deep review\nof CNN structures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 06:43:58 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Yuan", "Jun", ""], ["Ni", "Bingbing", ""], ["Kassim", "Ashraf A.", ""]]}, {"id": "1412.6980", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P. Kingma and Jimmy Ba", "title": "Adam: A Method for Stochastic Optimization", "comments": "Published as a conference paper at the 3rd International Conference\n  for Learning Representations, San Diego, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 13:54:29 GMT"}, {"version": "v2", "created": "Sat, 17 Jan 2015 20:26:06 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 21:04:48 GMT"}, {"version": "v4", "created": "Tue, 3 Mar 2015 17:51:27 GMT"}, {"version": "v5", "created": "Thu, 23 Apr 2015 16:46:07 GMT"}, {"version": "v6", "created": "Tue, 23 Jun 2015 19:57:17 GMT"}, {"version": "v7", "created": "Mon, 20 Jul 2015 09:43:23 GMT"}, {"version": "v8", "created": "Thu, 23 Jul 2015 20:27:47 GMT"}, {"version": "v9", "created": "Mon, 30 Jan 2017 01:27:54 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Kingma", "Diederik P.", ""], ["Ba", "Jimmy", ""]]}, {"id": "1412.7003", "submitter": "Shin-ichi Maeda", "authors": "Shin-ichi Maeda", "title": "A Bayesian encourages dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is one of the key techniques to prevent the learning from\noverfitting. It is explained that dropout works as a kind of modified L2\nregularization. Here, we shed light on the dropout from Bayesian standpoint.\nBayesian interpretation enables us to optimize the dropout rate, which is\nbeneficial for learning of weight parameters and prediction after learning. The\nexperiment result also encourages the optimization of the dropout.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:46:26 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 06:17:16 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 06:32:47 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Maeda", "Shin-ichi", ""]]}, {"id": "1412.7004", "submitter": "Pranava Swaroop Madhyastha", "authors": "Pranava Swaroop Madhyastha, Xavier Carreras, Ariadna Quattoni", "title": "Tailoring Word Embeddings for Bilexical Predictions: An Experimental\n  Comparison", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We investigate the problem of inducing word embeddings that are tailored for\na particular bilexical relation. Our learning algorithm takes an existing\nlexical vector space and compresses it such that the resulting word embeddings\nare good predictors for a target bilexical relation. In experiments we show\nthat task-specific embeddings can benefit both the quality and efficiency in\nlexical prediction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:49:19 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 17:33:57 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Madhyastha", "Pranava Swaroop", ""], ["Carreras", "Xavier", ""], ["Quattoni", "Ariadna", ""]]}, {"id": "1412.7006", "submitter": "Vivek Venugopalan", "authors": "Michael Giering, Vivek Venugopalan, Kishore Reddy", "title": "Multi-modal Sensor Registration for Vehicle Perception via Deep Neural\n  Networks", "comments": "7 pages, double column, IEEE format, accepted at IEEE HPEC 2015", "journal-ref": null, "doi": "10.1109/HPEC.2015.7322485", "report-no": "1214_v3", "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to simultaneously leverage multiple modes of sensor information\nis critical for perception of an automated vehicle's physical surroundings.\nSpatio-temporal alignment of registration of the incoming information is often\na prerequisite to analyzing the fused data. The persistence and reliability of\nmulti-modal registration is therefore the key to the stability of decision\nsupport systems ingesting the fused information. LiDAR-video systems like on\nthose many driverless cars are a common example of where keeping the LiDAR and\nvideo channels registered to common physical features is important. We develop\na deep learning method that takes multiple channels of heterogeneous data, to\ndetect the misalignment of the LiDAR-video inputs. A number of variations were\ntested on the Ford LiDAR-video driving test data set and will be discussed. To\nthe best of our knowledge the use of multi-modal deep convolutional neural\nnetworks for dynamic real-time LiDAR-video registration has not been presented.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:54:53 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 01:14:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Giering", "Michael", ""], ["Venugopalan", "Vivek", ""], ["Reddy", "Kishore", ""]]}, {"id": "1412.7007", "submitter": "Vivek Venugopalan", "authors": "Soumik Sarkar, Vivek Venugopalan, Kishore Reddy, Michael Giering,\n  Julian Ryde, Navdeep Jaitly", "title": "Occlusion Edge Detection in RGB-D Frames using Deep Convolutional\n  Networks", "comments": "7 pages, double column, IEEE HPEC 2015 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion edges in images which correspond to range discontinuity in the\nscene from the point of view of the observer are an important prerequisite for\nmany vision and mobile robot tasks. Although they can be extracted from range\ndata however extracting them from images and videos would be extremely\nbeneficial. We trained a deep convolutional neural network (CNN) to identify\nocclusion edges in images and videos with both RGB-D and RGB inputs. The use of\nCNN avoids hand-crafting of features for automatically isolating occlusion\nedges and distinguishing them from appearance edges. Other than quantitative\nocclusion edge detection results, qualitative results are provided to\ndemonstrate the trade-off between high resolution analysis and frame-level\ncomputation time which is critical for real-time robotics applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:55:17 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 12:50:48 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2015 01:07:23 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Sarkar", "Soumik", ""], ["Venugopalan", "Vivek", ""], ["Reddy", "Kishore", ""], ["Giering", "Michael", ""], ["Ryde", "Julian", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1412.7009", "submitter": "Jan Rudy", "authors": "Jan Rudy, Graham Taylor", "title": "Generative Class-conditional Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Bengio et al. (2013) proposes a sampling procedure for\ndenoising autoencoders which involves learning the transition operator of a\nMarkov chain. The transition operator is typically unimodal, which limits its\ncapacity to model complex data. In order to perform efficient sampling from\nconditional distributions, we extend this work, both theoretically and\nalgorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is\nable to generate convincing class-conditional samples when trained on both the\nMNIST and TFD datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:57:05 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 00:16:55 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 01:54:33 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Rudy", "Jan", ""], ["Taylor", "Graham", ""]]}, {"id": "1412.7022", "submitter": "Joan Bruna", "authors": "Pablo Sprechmann, Joan Bruna, Yann LeCun", "title": "Audio Source Separation with Discriminative Scattering Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we describe an ongoing line of research for solving\nsingle-channel source separation problems. Many monaural signal decomposition\ntechniques proposed in the literature operate on a feature space consisting of\na time-frequency representation of the input data. A challenge faced by these\napproaches is to effectively exploit the temporal dependencies of the signals\nat scales larger than the duration of a time-frame. In this work we propose to\ntackle this problem by modeling the signals using a time-frequency\nrepresentation with multiple temporal resolutions. The proposed representation\nconsists of a pyramid of wavelet scattering operators, which generalizes\nConstant Q Transforms (CQT) with extra layers of convolution and complex\nmodulus. We first show that learning standard models with this multi-resolution\nsetting improves source separation results over fixed-resolution methods. As\nstudy case, we use Non-Negative Matrix Factorizations (NMF) that has been\nwidely considered in many audio application. Then, we investigate the inclusion\nof the proposed multi-resolution setting into a discriminative training regime.\nWe discuss several alternatives using different deep neural network\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:15:44 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 23:54:06 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 02:24:14 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Sprechmann", "Pablo", ""], ["Bruna", "Joan", ""], ["LeCun", "Yann", ""]]}, {"id": "1412.7024", "submitter": "Matthieu Courbariaux", "authors": "Matthieu Courbariaux, Yoshua Bengio and Jean-Pierre David", "title": "Training deep neural networks with low precision multiplications", "comments": "10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multipliers are the most space and power-hungry arithmetic operators of the\ndigital implementation of deep neural networks. We train a set of\nstate-of-the-art neural networks (Maxout networks) on three benchmark datasets:\nMNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:\nfloating point, fixed point and dynamic fixed point. For each of those datasets\nand for each of those formats, we assess the impact of the precision of the\nmultiplications on the final error after training. We find that very low\nprecision is sufficient not just for running trained networks but also for\ntraining them. For example, it is possible to train Maxout networks with 10\nbits multiplications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:22:45 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 18:05:12 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 00:26:12 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2015 22:52:43 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2015 01:00:44 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Courbariaux", "Matthieu", ""], ["Bengio", "Yoshua", ""], ["David", "Jean-Pierre", ""]]}, {"id": "1412.7026", "submitter": "Aditya Joshi", "authors": "Aditya Joshi, Johan Halseth, Pentti Kanerva", "title": "Language Recognition using Random Indexing", "comments": "7 pages, 1 figures, 2 tables, ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Indexing is a simple implementation of Random Projections with a wide\nrange of applications. It can solve a variety of problems with good accuracy\nwithout introducing much complexity. Here we use it for identifying the\nlanguage of text samples. We present a novel method of generating language\nrepresentation vectors using letter blocks. Further, we show that the method is\neasily implemented and requires little computational power and space.\nExperiments on a number of model parameters illustrate certain properties about\nhigh dimensional sparse vector representations of data. Proof of statistically\nrelevant language vectors are shown through the extremely high success of\nvarious language recognition tasks. On a difficult data set of 21,000 short\nsentences from 21 different languages, our model performs a language\nrecognition task and achieves 97.8% accuracy, comparable to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:34:43 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 08:02:49 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Joshi", "Aditya", ""], ["Halseth", "Johan", ""], ["Kanerva", "Pentti", ""]]}, {"id": "1412.7028", "submitter": "Jo\\\"el Legrand", "authors": "Jo\\\"el Legrand and Ronan Collobert", "title": "Joint RNN-Based Greedy Parsing and Word Composition", "comments": "Published as a conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The greedy parser and\nthe compositional procedure are jointly trained, and tightly depends on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. We reach F1 performance on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser. We provide a fully functional\nimplementation of the method described in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:40:31 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 17:39:39 GMT"}, {"version": "v3", "created": "Thu, 8 Jan 2015 15:04:34 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 21:57:49 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Legrand", "Jo\u00ebl", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.7054", "submitter": "Pierre Sermanet", "authors": "Pierre Sermanet, Andrea Frome, Esteban Real", "title": "Attention for Fine-Grained Categorization", "comments": "ICLR 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents experiments extending the work of Ba et al. (2014) on\nrecurrent neural models for attention into less constrained visual\nenvironments, specifically fine-grained categorization on the Stanford Dogs\ndata set. In this work we use an RNN of the same structure but substitute a\nmore powerful visual network and perform large-scale pre-training of the visual\nnetwork outside of the attention RNN. Most work in attention models to date\nfocuses on tasks with toy or more constrained visual environments, whereas we\npresent results for fine-grained categorization better than the\nstate-of-the-art GoogLeNet classification model. We show that our model learns\nto direct high resolution attention to the most discriminative regions without\nany spatial supervision such as bounding boxes, and it is able to discriminate\nfine-grained dog breeds moderately well even when given only an initial\nlow-resolution context image and narrow, inexpensive glimpses at faces and fur\npatterns. This and similar attention models have the major advantage of being\ntrained end-to-end, as opposed to other current detection and recognition\npipelines with hand-engineered components where information is lost. While our\nmodel is state-of-the-art, further work is needed to fully leverage the\nsequential input.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:06:07 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 00:15:45 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2015 01:45:56 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Sermanet", "Pierre", ""], ["Frome", "Andrea", ""], ["Real", "Esteban", ""]]}, {"id": "1412.7056", "submitter": "Shuchin Aeron", "authors": "Eric Kernfeld and Shuchin Aeron and Misha Kilmer", "title": "Clustering multi-way data: a novel algebraic approach", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a method for unsupervised clustering of two-way\n(matrix) data by combining two recent innovations from different fields: the\nSparse Subspace Clustering (SSC) algorithm [10], which groups points coming\nfrom a union of subspaces into their respective subspaces, and the t-product\n[18], which was introduced to provide a matrix-like multiplication for third\norder tensors. Our algorithm is analogous to SSC in that an \"affinity\" between\ndifferent data points is built using a sparse self-representation of the data.\nUnlike SSC, we employ the t-product in the self-representation. This allows us\nmore flexibility in modeling; infact, SSC is a special case of our method. When\nusing the t-product, three-way arrays are treated as matrices whose elements\n(scalars) are n-tuples or tubes. Convolutions take the place of scalar\nmultiplication. This framework allows us to embed the 2-D data into a\nvector-space-like structure called a free module over a commutative ring. These\nfree modules retain many properties of complex inner-product spaces, and we\nleverage that to provide theoretical guarantees on our algorithm. We show that\ncompared to vector-space counterparts, SSmC achieves higher accuracy and better\nable to cluster data with less preprocessing in some image clustering problems.\nIn particular we show the performance of the proposed method on Weizmann face\ndatabase, the Extended Yale B Face database and the MNIST handwritten digits\ndatabase.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:06:44 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 02:19:29 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Kernfeld", "Eric", ""], ["Aeron", "Shuchin", ""], ["Kilmer", "Misha", ""]]}, {"id": "1412.7062", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin\n  Murphy and Alan L. Yuille", "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully\n  Connected CRFs", "comments": "14 pages. Updated related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) have recently shown state of the\nart performance in high level vision tasks, such as image classification and\nobject detection. This work brings together methods from DCNNs and\nprobabilistic graphical models for addressing the task of pixel-level\nclassification (also called \"semantic image segmentation\"). We show that\nresponses at the final layer of DCNNs are not sufficiently localized for\naccurate object segmentation. This is due to the very invariance properties\nthat make DCNNs good for high level tasks. We overcome this poor localization\nproperty of deep networks by combining the responses at the final DCNN layer\nwith a fully connected Conditional Random Field (CRF). Qualitatively, our\n\"DeepLab\" system is able to localize segment boundaries at a level of accuracy\nwhich is beyond previous methods. Quantitatively, our method sets the new\nstate-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching\n71.6% IOU accuracy in the test set. We show how these results can be obtained\nefficiently: Careful network re-purposing and a novel application of the 'hole'\nalgorithm from the wavelet community allow dense computation of neural net\nresponses at 8 frames per second on a modern GPU.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:18:33 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 18:21:29 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 17:14:50 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 04:00:08 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Papandreou", "George", ""], ["Kokkinos", "Iasonas", ""], ["Murphy", "Kevin", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1412.7063", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran", "title": "Diverse Embedding Neural Network Language Models", "comments": "Under review as workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Diverse Embedding Neural Network (DENN), a novel architecture for\nlanguage models (LMs). A DENNLM projects the input word history vector onto\nmultiple diverse low-dimensional sub-spaces instead of a single\nhigher-dimensional sub-space as in conventional feed-forward neural network\nLMs. We encourage these sub-spaces to be diverse during network training\nthrough an augmented loss function. Our language modeling experiments on the\nPenn Treebank data set show the performance benefit of using a DENNLM.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:19:56 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 21:33:46 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 19:53:21 GMT"}, {"version": "v4", "created": "Wed, 25 Feb 2015 21:55:15 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2015 20:07:50 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1412.7091", "submitter": "Pascal Vincent", "authors": "Pascal Vincent, Alexandre de Br\\'ebisson, Xavier Bouthillier", "title": "Efficient Exact Gradient Update for training Deep Networks with Very\n  Large Sparse Targets", "comments": "15 pages technical report version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important class of problems involves training deep neural networks with\nsparse prediction targets of very high dimension D. These occur naturally in\ne.g. neural language models or the learning of word-embeddings, often posed as\npredicting the probability of next words among a vocabulary of size D (e.g. 200\n000). Computing the equally large, but typically non-sparse D-dimensional\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\nincurs a prohibitive O(Dd) computational cost for each example, as does\nupdating the D x d output weight matrix and computing the gradient needed for\nbackpropagation to previous layers. While efficient handling of large sparse\nnetwork inputs is trivial, the case of large sparse targets is not, and has\nthus so far been sidestepped with approximate alternatives such as hierarchical\nsoftmax or sampling-based approximations during training. In this work we\ndevelop an original algorithmic approach which, for a family of loss functions\nthat includes squared error and spherical softmax, can compute the exact loss,\ngradient update for the output weights, and gradient for backpropagation, all\nin O(d^2) per example instead of O(Dd), remarkably without ever computing the\nD-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e.\ntwo orders of magnitude for typical sizes, for that critical part of the\ncomputations that often dominates the training time in this kind of network\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 18:51:08 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 04:02:12 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2015 01:27:13 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Vincent", "Pascal", ""], ["de Br\u00e9bisson", "Alexandre", ""], ["Bouthillier", "Xavier", ""]]}, {"id": "1412.7110", "submitter": "Dimitri Palaz", "authors": "Dimitri Palaz, Mathew Magimai Doss and Ronan Collobert", "title": "Learning linearly separable features for speech recognition using\n  convolutional neural networks", "comments": "Final version for ICLR 2015 Workshop; Revisions according to reviews.\n  Revised Section 4.5. Add references and correct typos. Submitted for ICLR\n  2015 conference track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition systems usually rely on spectral-based features,\nsuch as MFCC of PLP. These features are extracted based on prior knowledge such\nas, speech perception or/and speech production. Recently, convolutional neural\nnetworks have been shown to be able to estimate phoneme conditional\nprobabilities in a completely data-driven manner, i.e. using directly temporal\nraw speech signal as input. This system was shown to yield similar or better\nperformance than HMM/ANN based system on phoneme recognition task and on large\nscale continuous speech recognition task, using less parameters. Motivated by\nthese studies, we investigate the use of simple linear classifier in the\nCNN-based framework. Thus, the network learns linearly separable features from\nraw speech. We show that such system yields similar or better performance than\nMLP based system using cepstral-based features as input.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 19:46:01 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 13:46:10 GMT"}, {"version": "v3", "created": "Fri, 23 Jan 2015 10:44:21 GMT"}, {"version": "v4", "created": "Thu, 26 Feb 2015 19:51:35 GMT"}, {"version": "v5", "created": "Fri, 27 Feb 2015 16:31:32 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 08:29:14 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Palaz", "Dimitri", ""], ["Doss", "Mathew Magimai", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.7122", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko", "title": "Learning Deep Object Detectors from 3D Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced 3D CAD models are becoming easily accessible online, and can\npotentially generate an infinite number of training images for almost any\nobject category.We show that augmenting the training data of contemporary Deep\nConvolutional Neural Net (DCNN) models with such synthetic data can be\neffective, especially when real training data is limited or not well matched to\nthe target domain. Most freely available CAD models capture 3D shape but are\noften missing other low level cues, such as realistic object texture, pose, or\nbackground. In a detailed analysis, we use synthetic CAD-rendered images to\nprobe the ability of DCNN to learn without these cues, with surprising\nfindings. In particular, we show that when the DCNN is fine-tuned on the target\ndetection task, it exhibits a large degree of invariance to missing low-level\ncues, but, when pretrained on generic ImageNet classification, it learns better\nwhen the low-level cues are simulated. We show that our synthetic DCNN training\napproach significantly outperforms previous methods on the PASCAL VOC2007\ndataset when learning in the few-shot scenario and improves performance in a\ndomain shift scenario on the Office benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:10:31 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 23:44:24 GMT"}, {"version": "v3", "created": "Tue, 19 May 2015 17:56:07 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2015 01:01:39 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Peng", "Xingchao", ""], ["Sun", "Baochen", ""], ["Ali", "Karim", ""], ["Saenko", "Kate", ""]]}, {"id": "1412.7144", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Evan Shelhamer, Jonathan Long and Trevor Darrell", "title": "Fully Convolutional Multi-Class Multiple Instance Learning", "comments": "in ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) can reduce the need for costly annotation in\ntasks such as semantic segmentation by weakening the required degree of\nsupervision. We propose a novel MIL formulation of multi-class semantic\nsegmentation learning by a fully convolutional network. In this setting, we\nseek to learn a semantic segmentation model from just weak image-level labels.\nThe model is trained end-to-end to jointly optimize the representation while\ndisambiguating the pixel-image label assignment. Fully convolutional training\naccepts inputs of any size, does not need object proposal pre-processing, and\noffers a pixelwise loss map for selecting latent instances. Our multi-class MIL\nloss exploits the further supervision given by images with multiple labels. We\nevaluate this approach through preliminary experiments on the PASCAL VOC\nsegmentation challenge.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:49:54 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 01:17:59 GMT"}, {"version": "v3", "created": "Sat, 7 Feb 2015 02:12:26 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2015 05:31:10 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Pathak", "Deepak", ""], ["Shelhamer", "Evan", ""], ["Long", "Jonathan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1412.7149", "submitter": "Zichao Yang", "authors": "Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex\n  Smola, Le Song, Ziyu Wang", "title": "Deep Fried Convnets", "comments": "svd experiments included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully connected layers of a deep convolutional neural network typically\ncontain over 90% of the network parameters, and consume the majority of the\nmemory required to store the network parameters. Reducing the number of\nparameters while preserving essentially the same predictive performance is\ncritically important for operating deep neural networks in memory constrained\nenvironments such as GPUs or embedded devices.\n  In this paper we show how kernel methods, in particular a single Fastfood\nlayer, can be used to replace all fully connected layers in a deep\nconvolutional neural network. This novel Fastfood layer is also end-to-end\ntrainable in conjunction with convolutional layers, allowing us to combine them\ninto a new architecture, named deep fried convolutional networks, which\nsubstantially reduces the memory footprint of convolutional networks trained on\nMNIST and ImageNet with no drop in predictive performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:53:30 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:17:24 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2015 21:30:55 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2015 20:17:26 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yang", "Zichao", ""], ["Moczulski", "Marcin", ""], ["Denil", "Misha", ""], ["de Freitas", "Nando", ""], ["Smola", "Alex", ""], ["Song", "Le", ""], ["Wang", "Ziyu", ""]]}, {"id": "1412.7155", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Lisa Anne Hendricks, Trevor Darrell", "title": "Learning Compact Convolutional Neural Networks with Nested Dropout", "comments": "4 pages, 2 figures. Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, nested dropout was proposed as a method for ordering representation\nunits in autoencoders by their information content, without diminishing\nreconstruction cost. However, it has only been applied to training\nfully-connected autoencoders in an unsupervised setting. We explore the impact\nof nested dropout on the convolutional layers in a CNN trained by\nbackpropagation, investigating whether nested dropout can provide a simple and\nsystematic way to determine the optimal representation size with respect to the\ndesired accuracy and desired task and data complexity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:59:58 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 01:47:57 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 00:07:59 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 06:11:22 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Finn", "Chelsea", ""], ["Hendricks", "Lisa Anne", ""], ["Darrell", "Trevor", ""]]}, {"id": "1412.7156", "submitter": "Ludovic Denoyer", "authors": "Gabriella Contardo and Ludovic Denoyer and Thierry Artieres", "title": "Representation Learning for cold-start recommendation", "comments": "Accepted as workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard approach to Collaborative Filtering (CF), i.e. prediction of user\nratings on items, relies on Matrix Factorization techniques. Representations\nfor both users and items are computed from the observed ratings and used for\nprediction. Unfortunatly, these transductive approaches cannot handle the case\nof new users arriving in the system, with no known rating, a problem known as\nuser cold-start. A common approach in this context is to ask these incoming\nusers for a few initialization ratings. This paper presents a model to tackle\nthis twofold problem of (i) finding good questions to ask, (ii) building\nefficient representations from this small amount of information. The model can\nalso be used in a more standard (warm) context. Our approach is evaluated on\nthe classical CF problem and on the cold-start problem on four different\ndatasets showing its ability to improve baseline performance in both cases.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 21:58:06 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 18:56:23 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2015 09:59:25 GMT"}, {"version": "v4", "created": "Wed, 8 Apr 2015 15:37:19 GMT"}, {"version": "v5", "created": "Mon, 22 Jun 2015 14:01:33 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Contardo", "Gabriella", ""], ["Denoyer", "Ludovic", ""], ["Artieres", "Thierry", ""]]}, {"id": "1412.7180", "submitter": "Yishu Miao", "authors": "Yishu Miao, Ziyu Wang, Phil Blunsom", "title": "Bayesian Optimisation for Machine Translation", "comments": "Bayesian optimisation workshop, NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents novel Bayesian optimisation algorithms for minimum error\nrate training of statistical machine translation systems. We explore two\nclasses of algorithms for efficiently exploring the translation space, with the\nfirst based on N-best lists and the second based on a hypergraph representation\nthat compactly represents an exponential number of translation options. Our\nalgorithms exhibit faster convergence and are capable of obtaining lower error\nrates than the existing translation model specific approaches, all within a\ngeneric Bayesian optimisation framework. Further more, we also introduce a\nrandom embedding algorithm to scale our approach to sparse high dimensional\nfeature sets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 21:44:00 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Miao", "Yishu", ""], ["Wang", "Ziyu", ""], ["Blunsom", "Phil", ""]]}, {"id": "1412.7190", "submitter": "Mathieu Aubry", "authors": "Francisco Massa, Mathieu Aubry, Renaud Marlet", "title": "Convolutional Neural Networks for joint object detection and pose\n  estimation: A comparative study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the application of convolutional neural networks for\njointly detecting objects depicted in still images and estimating their 3D\npose. We identify different feature representations of oriented objects, and\nenergies that lead a network to learn this representations. The choice of the\nrepresentation is crucial since the pose of an object has a natural, continuous\nstructure while its category is a discrete variable. We evaluate the different\napproaches on the joint object detection and pose estimation task of the\nPascal3D+ benchmark using Average Viewpoint Precision. We show that a\nclassification approach on discretized viewpoints achieves state-of-the-art\nperformance for joint object detection and pose estimation, and significantly\noutperforms existing baselines on this benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 22:26:26 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 16:43:41 GMT"}, {"version": "v3", "created": "Sat, 7 Feb 2015 05:27:24 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 05:15:45 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Massa", "Francisco", ""], ["Aubry", "Mathieu", ""], ["Marlet", "Renaud", ""]]}, {"id": "1412.7193", "submitter": "Giljin Jang", "authors": "Giljin Jang, Han-Gyu Kim, Yung-Hwan Oh", "title": "Audio Source Separation Using a Deep Autoencoder", "comments": "3 pages, 4 figures, ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for unsupervised audio source\nseparation using a deep autoencoder. The characteristics of unknown source\nsignals mixed in the mixed input is automatically by properly configured\nautoencoders implemented by a network with many layers, and separated by\nclustering the coefficient vectors in the code layer. By investigating the\nweight vectors to the final target, representation layer, the primitive\ncomponents of the audio signals in the frequency domain are observed. By\nclustering the activation coefficients in the code layer, the previously\nunknown source signals are segregated. The original source sounds are then\nseparated and reconstructed by using code vectors which belong to different\nclusters. The restored sounds are not perfect but yield promising results for\nthe possibility in the success of many practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 22:38:06 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Jang", "Giljin", ""], ["Kim", "Han-Gyu", ""], ["Oh", "Yung-Hwan", ""]]}, {"id": "1412.7210", "submitter": "Antti Rasmus", "authors": "Antti Rasmus, Tapani Raiko, Harri Valpola", "title": "Denoising autoencoder with modulated lateral connections learns\n  invariant representations of natural images", "comments": "Presentation at ICLR 2015 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 23:36:15 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 12:17:51 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 20:56:43 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2015 15:49:16 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Rasmus", "Antti", ""], ["Raiko", "Tapani", ""], ["Valpola", "Harri", ""]]}, {"id": "1412.7215", "submitter": "Saghar Hosseini", "authors": "Saghar Hosseini, Airlie Chapman, and Mehran Mesbahi", "title": "Online Distributed Optimization on Dynamic Networks", "comments": "Submitted to The IEEE Transactions on Automatic Control, 2014", "journal-ref": null, "doi": "10.1109/TAC.2016.2525928", "report-no": null, "categories": "math.OC cs.DS cs.LG cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a distributed optimization scheme over a network of\nagents in the presence of cost uncertainties and over switching communication\ntopologies. Inspired by recent advances in distributed convex optimization, we\npropose a distributed algorithm based on a dual sub-gradient averaging. The\nobjective of this algorithm is to minimize a cost function cooperatively.\nFurthermore, the algorithm changes the weights on the communication links in\nthe network to adapt to varying reliability of neighboring agents. A\nconvergence rate analysis as a function of the underlying network topology is\nthen presented, followed by simulation results for representative classes of\nsensor networks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 23:57:30 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Hosseini", "Saghar", ""], ["Chapman", "Airlie", ""], ["Mesbahi", "Mehran", ""]]}, {"id": "1412.7259", "submitter": "Dong Wang", "authors": "Dong Wang and Xiaoyang Tan", "title": "Unsupervised Feature Learning with C-SVDDNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of learning feature representation\nfrom unlabeled data using a single-layer K-means network. A K-means network\nmaps the input data into a feature representation by finding the nearest\ncentroid for each input point, which has attracted researchers' great attention\nrecently due to its simplicity, effectiveness, and scalability. However, one\ndrawback of this feature mapping is that it tends to be unreliable when the\ntraining data contains noise. To address this issue, we propose a SVDD based\nfeature learning algorithm that describes the density and distribution of each\ncluster from K-means with an SVDD ball for more robust feature representation.\nFor this purpose, we present a new SVDD algorithm called C-SVDD that centers\nthe SVDD ball towards the mode of local density of each cluster, and we show\nthat the objective of C-SVDD can be solved very efficiently as a linear\nprogramming problem. Additionally, traditional unsupervised feature learning\nmethods usually take an average or sum of local representations to obtain\nglobal representation which ignore spatial relationship among them. To use\nspatial information we propose a global representation with a variant of SIFT\ndescriptor. The architecture is also extended with multiple receptive field\nscales and multiple pooling sizes. Extensive experiments on several popular\nobject recognition benchmarks, such as STL-10, MINST, Holiday and Copydays\nshows that the proposed C-SVDDNet method yields comparable or better\nperformance than that of the previous state of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 05:56:50 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 05:31:03 GMT"}, {"version": "v3", "created": "Fri, 29 May 2015 09:50:54 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Wang", "Dong", ""], ["Tan", "Xiaoyang", ""]]}, {"id": "1412.7272", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Emre Neftci and Gert Cauwenberghs", "title": "Learning Non-deterministic Representations with Energy-based Ensembles", "comments": "9 pages, 3 figures, ICLR-15 workshop contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a generative model is to capture the distribution underlying the\ndata, typically through latent variables. After training, these variables are\noften used as a new representation, more effective than the original features\nin a variety of learning tasks. However, the representations constructed by\ncontemporary generative models are usually point-wise deterministic mappings\nfrom the original feature space. Thus, even with representations robust to\nclass-specific transformations, statistically driven models trained on them\nwould not be able to generalize when the labeled data is scarce. Inspired by\nthe stochasticity of the synaptic connections in the brain, we introduce\nEnergy-based Stochastic Ensembles. These ensembles can learn non-deterministic\nrepresentations, i.e., mappings from the feature space to a family of\ndistributions in the latent space. These mappings are encoded in a distribution\nover a (possibly infinite) collection of models. By conditionally sampling\nmodels from the ensemble, we obtain multiple representations for every input\nexample and effectively augment the data. We propose an algorithm similar to\ncontrastive divergence for training restricted Boltzmann stochastic ensembles.\nFinally, we demonstrate the concept of the stochastic representations on a\nsynthetic dataset as well as test them in the one-shot learning scenario on\nMNIST.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 07:06:55 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 10:04:49 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Neftci", "Emre", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1412.7384", "submitter": "Peng Yang", "authors": "Peng Yang, Xiaoquan Su, Le Ou-Yang, Hon-Nian Chua, Xiao-Li Li, Kang\n  Ning", "title": "Microbial community pattern detection in human body habitats via\n  ensemble clustering framework", "comments": "BMC Systems Biology 2014", "journal-ref": "BMC Systems Biology 2014, 8(Suppl 4):S7", "doi": "10.1186/1752-0509-8-S4-S7", "report-no": null, "categories": "q-bio.QM cs.CE cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human habitat is a host where microbial species evolve, function, and\ncontinue to evolve. Elucidating how microbial communities respond to human\nhabitats is a fundamental and critical task, as establishing baselines of human\nmicrobiome is essential in understanding its role in human disease and health.\nHowever, current studies usually overlook a complex and interconnected\nlandscape of human microbiome and limit the ability in particular body habitats\nwith learning models of specific criterion. Therefore, these methods could not\ncapture the real-world underlying microbial patterns effectively. To obtain a\ncomprehensive view, we propose a novel ensemble clustering framework to mine\nthe structure of microbial community pattern on large-scale metagenomic data.\nParticularly, we first build a microbial similarity network via integrating\n1920 metagenomic samples from three body habitats of healthy adults. Then a\nnovel symmetric Nonnegative Matrix Factorization (NMF) based ensemble model is\nproposed and applied onto the network to detect clustering pattern. Extensive\nexperiments are conducted to evaluate the effectiveness of our model on\nderiving microbial community with respect to body habitat and host gender. From\nclustering results, we observed that body habitat exhibits a strong bound but\nnon-unique microbial structural patterns. Meanwhile, human microbiome reveals\ndifferent degree of structural variations over body habitat and host gender. In\nsummary, our ensemble clustering framework could efficiently explore integrated\nclustering results to accurately identify microbial communities, and provide a\ncomprehensive view for a set of microbial communities. Such trends depict an\nintegrated biography of microbial communities, which offer a new insight\ntowards uncovering pathogenic model of human microbiome.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 12:52:45 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 02:03:08 GMT"}, {"version": "v3", "created": "Sun, 4 Jan 2015 05:37:42 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Yang", "Peng", ""], ["Su", "Xiaoquan", ""], ["Ou-Yang", "Le", ""], ["Chua", "Hon-Nian", ""], ["Li", "Xiao-Li", ""], ["Ning", "Kang", ""]]}, {"id": "1412.7419", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Marcin Moczulski and Yoshua Bengio", "title": "ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient", "comments": "8 pages, 3 figures, ICLR workshop submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stochastic gradient algorithms have been the main focus of large-scale\nlearning problems and they led to important successes in machine learning. The\nconvergence of SGD depends on the careful choice of learning rate and the\namount of the noise in stochastic estimates of the gradients. In this paper, we\npropose a new adaptive learning rate algorithm, which utilizes curvature\ninformation for automatically tuning the learning rates. The information about\nthe element-wise curvature of the loss function is estimated from the local\nstatistics of the stochastic first order gradients. We further propose a new\nvariance reduction technique to speed up the convergence. In our preliminary\nexperiments with deep neural networks, we obtained better performance compared\nto the popular stochastic gradient algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 15:55:08 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 19:15:01 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 00:46:01 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2015 18:52:17 GMT"}, {"version": "v5", "created": "Sun, 1 Nov 2015 03:05:18 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Moczulski", "Marcin", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.7449", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever,\n  Geoffrey Hinton", "title": "Grammar as a Foreign Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic constituency parsing is a fundamental problem in natural language\nprocessing and has been the subject of intensive research and engineering for\ndecades. As a result, the most accurate parsers are domain specific, complex,\nand inefficient. In this paper we show that the domain agnostic\nattention-enhanced sequence-to-sequence model achieves state-of-the-art results\non the most widely used syntactic constituency parsing dataset, when trained on\na large synthetic corpus that was annotated using existing parsers. It also\nmatches the performance of standard parsers when trained only on a small\nhuman-annotated dataset, which shows that this model is highly data-efficient,\nin contrast to sequence-to-sequence models without the attention mechanism. Our\nparser is also fast, processing over a hundred sentences per second with an\nunoptimized CPU implementation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 17:16:24 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 03:16:54 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2015 22:41:07 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Vinyals", "Oriol", ""], ["Kaiser", "Lukasz", ""], ["Koo", "Terry", ""], ["Petrov", "Slav", ""], ["Sutskever", "Ilya", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1412.7477", "submitter": "Tuo Zhao", "authors": "Tuo Zhao, Han Liu, Tong Zhang", "title": "Pathwise Coordinate Optimization for Sparse Learning: Algorithm and\n  Theory", "comments": "Accepted by the Annals of Statistics, 2016+", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pathwise coordinate optimization is one of the most important\ncomputational frameworks for high dimensional convex and nonconvex sparse\nlearning problems. It differs from the classical coordinate optimization\nalgorithms in three salient features: {\\it warm start initialization}, {\\it\nactive set updating}, and {\\it strong rule for coordinate preselection}. Such a\ncomplex algorithmic structure grants superior empirical performance, but also\nposes significant challenge to theoretical analysis. To tackle this long\nlasting problem, we develop a new theory showing that these three features play\npivotal roles in guaranteeing the outstanding statistical and computational\nperformance of the pathwise coordinate optimization framework. Particularly, we\nanalyze the existing pathwise coordinate optimization algorithms and provide\nnew theoretical insights into them. The obtained insights further motivate the\ndevelopment of several modifications to improve the pathwise coordinate\noptimization framework, which guarantees linear convergence to a unique sparse\nlocal optimum with optimal statistical properties in parameter estimation and\nsupport recovery. This is the first result on the computational and statistical\nguarantees of the pathwise coordinate optimization framework in high\ndimensions. Thorough numerical experiments are provided to support our theory.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 19:12:24 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 21:48:23 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2015 21:57:51 GMT"}, {"version": "v4", "created": "Wed, 3 Aug 2016 17:00:55 GMT"}, {"version": "v5", "created": "Mon, 2 Jan 2017 18:36:23 GMT"}, {"version": "v6", "created": "Wed, 1 Feb 2017 20:41:32 GMT"}, {"version": "v7", "created": "Thu, 9 Feb 2017 18:19:38 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Zhao", "Tuo", ""], ["Liu", "Han", ""], ["Zhang", "Tong", ""]]}, {"id": "1412.7479", "submitter": "Sudheendra Vijayanarasimhan", "authors": "Sudheendra Vijayanarasimhan and Jonathon Shlens and Rajat Monga and\n  Jay Yagnik", "title": "Deep Networks With Large Output Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been extremely successful at various image, speech,\nvideo recognition tasks because of their ability to model deep structures\nwithin the data. However, they are still prohibitively expensive to train and\napply for problems containing millions of classes in the output layer. Based on\nthe observation that the key computation common to most neural network layers\nis a vector/matrix product, we propose a fast locality-sensitive hashing\ntechnique to approximate the actual dot product enabling us to scale up the\ntraining and inference to millions of output classes. We evaluate our technique\non three diverse large-scale recognition tasks and show that our approach can\ntrain large-scale models at a faster rate (in terms of steps/total time)\ncompared to baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 19:22:59 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 18:45:36 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 01:12:58 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 19:53:21 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Vijayanarasimhan", "Sudheendra", ""], ["Shlens", "Jonathon", ""], ["Monga", "Rajat", ""], ["Yagnik", "Jay", ""]]}, {"id": "1412.7489", "submitter": "Yongxin Yang", "authors": "Yongxin Yang and Timothy M. Hospedales", "title": "A Unified Perspective on Multi-Domain and Multi-Task Learning", "comments": "9 pages, Accepted to ICLR 2015 Conference Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a new neural-network based perspective on\nmulti-task learning (MTL) and multi-domain learning (MDL). By introducing the\nconcept of a semantic descriptor, this framework unifies MDL and MTL as well as\nencompassing various classic and recent MTL/MDL algorithms by interpreting them\nas different ways of constructing semantic descriptors. Our interpretation\nprovides an alternative pipeline for zero-shot learning (ZSL), where a model\nfor a novel class can be constructed without training data. Moreover, it leads\nto a new and practically relevant problem setting of zero-shot domain\nadaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model\nfor an unseen domain can be generated by its semantic descriptor. Experiments\nacross this range of problems demonstrate that our framework outperforms a\nvariety of alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 19:50:21 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 15:10:46 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2015 15:29:50 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1412.7522", "submitter": "Orhan Firat", "authors": "Orhan Firat, Emre Aksan, Ilke Oztekin, Fatos T. Yarman Vural", "title": "Learning Deep Temporal Representations for Brain Decoding", "comments": "This paper has been withdrawn for a revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Functional magnetic resonance imaging produces high dimensional data, with a\nless then ideal number of labelled samples for brain decoding tasks (predicting\nbrain states). In this study, we propose a new deep temporal convolutional\nneural network architecture with spatial pooling for brain decoding which aims\nto reduce dimensionality of feature space along with improved classification\nperformance. Temporal representations (filters) for each layer of the\nconvolutional model are learned by leveraging unlabelled fMRI data in an\nunsupervised fashion with regularized autoencoders. Learned temporal\nrepresentations in multiple levels capture the regularities in the temporal\ndomain and are observed to be a rich bank of activation patterns which also\nexhibit similarities to the actual hemodynamic responses. Further, spatial\npooling layers in the convolutional architecture reduce the dimensionality\nwithout losing excessive information. By employing the proposed temporal\nconvolutional architecture with spatial pooling, raw input fMRI data is mapped\nto a non-linear, highly-expressive and low-dimensional feature space where the\nfinal classification is conducted. In addition, we propose a simple heuristic\napproach for hyper-parameter tuning when no validation data is available.\nProposed method is tested on a ten class recognition memory experiment with\nnine subjects. The results support the efficiency and potential of the proposed\nmodel, compared to the baseline multi-voxel pattern analysis techniques.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 20:53:09 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 10:31:13 GMT"}, {"version": "v3", "created": "Thu, 25 Dec 2014 07:43:42 GMT"}, {"version": "v4", "created": "Mon, 12 Jan 2015 05:03:58 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Firat", "Orhan", ""], ["Aksan", "Emre", ""], ["Oztekin", "Ilke", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1412.7525", "submitter": "Dong-Hyun Lee", "authors": "Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, Yoshua Bengio", "title": "Difference Target Propagation", "comments": "13 pages, 8 figures, Accepted in ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Back-propagation has been the workhorse of recent successes of deep learning\nbut it relies on infinitesimal effects (partial derivatives) in order to\nperform credit assignment. This could become a serious issue as one considers\ndeeper and more non-linear functions, e.g., consider the extreme case of\nnonlinearity where the relation between parameters and cost is actually\ndiscrete. Inspired by the biological implausibility of back-propagation, a few\napproaches have been proposed in the past that could play a similar credit\nassignment role. In this spirit, we explore a novel approach to credit\nassignment in deep networks that we call target propagation. The main idea is\nto compute targets rather than gradients, at each layer. Like gradients, they\nare propagated backwards. In a way that is related but different from\npreviously proposed proxies for back-propagation which rely on a backwards\nnetwork with symmetric weights, target propagation relies on auto-encoders at\neach layer. Unlike back-propagation, it can be applied even when units exchange\nstochastic bits rather than real numbers. We show that a linear correction for\nthe imperfectness of the auto-encoders, called difference target propagation,\nis very effective to make target propagation actually work, leading to results\ncomparable to back-propagation for deep networks with discrete and continuous\nunits and denoising auto-encoders and achieving state of the art for stochastic\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 20:57:59 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 16:54:57 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2015 01:01:54 GMT"}, {"version": "v4", "created": "Sat, 14 Nov 2015 07:05:40 GMT"}, {"version": "v5", "created": "Wed, 25 Nov 2015 02:30:41 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Lee", "Dong-Hyun", ""], ["Zhang", "Saizheng", ""], ["Fischer", "Asja", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.7580", "submitter": "Nicolas Vasilache", "authors": "Nicolas Vasilache, Jeff Johnson, Michael Mathieu, Soumith Chintala,\n  Serkan Piantino, Yann LeCun", "title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation", "comments": "Camera ready for ICLR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the performance profile of Convolutional Neural Network training\non the current generation of NVIDIA Graphics Processing Units. We introduce two\nnew Fast Fourier Transform convolution implementations: one based on NVIDIA's\ncuFFT library, and another based on a Facebook authored FFT implementation,\nfbfft, that provides significant speedups over cuFFT (over 1.5x) for whole\nCNNs. Both of these convolution implementations are available in open source,\nand are faster than NVIDIA's cuDNN implementation for many common convolutional\nlayers (up to 23.5x for some synthetic kernel configurations). We discuss\ndifferent performance regimes of convolutions, comparing areas where\nstraightforward time domain convolutions outperform Fourier frequency domain\nconvolutions. Details on algorithmic applications of NVIDIA GPU hardware\nspecifics in the implementation of fbfft are also provided.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 01:31:36 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 16:55:04 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 20:01:00 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Vasilache", "Nicolas", ""], ["Johnson", "Jeff", ""], ["Mathieu", "Michael", ""], ["Chintala", "Soumith", ""], ["Piantino", "Serkan", ""], ["LeCun", "Yann", ""]]}, {"id": "1412.7584", "submitter": "Zhanglong Ji", "authors": "Zhanglong Ji, Zachary C. Lipton, Charles Elkan", "title": "Differential Privacy and Machine Learning: a Survey and Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of machine learning is to extract useful information from data,\nwhile privacy is preserved by concealing information. Thus it seems hard to\nreconcile these competing interests. However, they frequently must be balanced\nwhen mining sensitive data. For example, medical research represents an\nimportant application where it is necessary both to extract useful information\nand protect patient privacy. One way to resolve the conflict is to extract\ngeneral characteristics of whole populations without disclosing the private\ninformation of individuals.\n  In this paper, we consider differential privacy, one of the most popular and\npowerful definitions of privacy. We explore the interplay between machine\nlearning and differential privacy, namely privacy-preserving machine learning\nalgorithms and learning-based data release mechanisms. We also describe some\ntheoretical results that address what can be learned differentially privately\nand upper bounds of loss functions for differentially private algorithms.\n  Finally, we present some open questions, including how to incorporate public\ndata, how to deal with missing data in private datasets, and whether, as the\nnumber of observed samples grows arbitrarily large, differentially private\nmachine learning algorithms can be achieved at no cost to utility as compared\nto corresponding non-differentially private algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 01:51:06 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Ji", "Zhanglong", ""], ["Lipton", "Zachary C.", ""], ["Elkan", "Charles", ""]]}, {"id": "1412.7625", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "An Effective Semi-supervised Divisive Clustering Algorithm", "comments": "8 pages, 4 figures, a new (6th) member of the in-tree clustering\n  family", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Nowadays, data are generated massively and rapidly from scientific fields as\nbioinformatics, neuroscience and astronomy to business and engineering fields.\nCluster analysis, as one of the major data analysis tools, is therefore more\nsignificant than ever. We propose in this work an effective Semi-supervised\nDivisive Clustering algorithm (SDC). Data points are first organized by a\nminimal spanning tree. Next, this tree structure is transitioned to the in-tree\nstructure, and then divided into sub-trees under the supervision of the labeled\ndata, and in the end, all points in the sub-trees are directly associated with\nspecific cluster centers. SDC is fully automatic, non-iterative, involving no\nfree parameter, insensitive to noise, able to detect irregularly shaped cluster\nstructures, applicable to the data sets of high dimensionality and different\nattributes. The power of SDC is demonstrated on several datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 08:55:50 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2015 09:35:39 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1412.7659", "submitter": "Taco Cohen", "authors": "Taco S. Cohen and Max Welling", "title": "Transformation Properties of Learned Visual Representations", "comments": "T.S. Cohen & M. Welling, Transformation Properties of Learned Visual\n  Representations. In International Conference on Learning Representations\n  (ICLR), 2015", "journal-ref": "Proceedings of the International Conference on Learning\n  Representations, 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a three-dimensional object moves relative to an observer, a change\noccurs on the observer's image plane and in the visual representation computed\nby a learned model. Starting with the idea that a good visual representation is\none that transforms linearly under scene motions, we show, using the theory of\ngroup representations, that any such representation is equivalent to a\ncombination of the elementary irreducible representations. We derive a striking\nrelationship between irreducibility and the statistical dependency structure of\nthe representation, by showing that under restricted conditions, irreducible\nrepresentations are decorrelated. Under partial observability, as induced by\nthe perspective projection of a scene onto the image plane, the motion group\ndoes not have a linear action on the space of images, so that it becomes\nnecessary to perform inference over a latent representation that does transform\nlinearly. This idea is demonstrated in a model of rotating NORB objects that\nemploys a latent representation of the non-commutative 3D rotation group SO(3).\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 13:19:20 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 04:46:00 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 21:20:04 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1412.7725", "submitter": "Zhicheng Yan", "authors": "Zhicheng Yan and Hao Zhang and Baoyuan Wang and Sylvain Paris and\n  Yizhou Yu", "title": "Automatic Photo Adjustment Using Deep Neural Networks", "comments": "TOG minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo retouching enables photographers to invoke dramatic visual impressions\nby artistically enhancing their photos through stylistic color and tone\nadjustments. However, it is also a time-consuming and challenging task that\nrequires advanced skills beyond the abilities of casual photographers. Using an\nautomated algorithm is an appealing alternative to manual work but such an\nalgorithm faces many hurdles. Many photographic styles rely on subtle\nadjustments that depend on the image content and even its semantics. Further,\nthese adjustments are often spatially varying. Because of these\ncharacteristics, existing automatic algorithms are still limited and cover only\na subset of these challenges. Recently, deep machine learning has shown unique\nabilities to address hard problems that resisted machine algorithms for long.\nThis motivated us to explore the use of deep learning in the context of photo\nediting. In this paper, we explain how to formulate the automatic photo\nadjustment problem in a way suitable for this approach. We also introduce an\nimage descriptor that accounts for the local semantics of an image. Our\nexperiments demonstrate that our deep learning formulation applied using these\ndescriptors successfully capture sophisticated photographic styles. In\nparticular and unlike previous techniques, it can model local adjustments that\ndepend on the image semantics. We show on several examples that this yields\nresults that are qualitatively and quantitatively better than previous work.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 17:51:17 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 03:49:35 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Yan", "Zhicheng", ""], ["Zhang", "Hao", ""], ["Wang", "Baoyuan", ""], ["Paris", "Sylvain", ""], ["Yu", "Yizhou", ""]]}, {"id": "1412.7753", "submitter": "Tomas Mikolov", "authors": "Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu,\n  Marc'Aurelio Ranzato", "title": "Learning Longer Memory in Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network is a powerful model that learns temporal patterns in\nsequential data. For a long time, it was believed that recurrent networks are\ndifficult to train using simple optimizers, such as stochastic gradient\ndescent, due to the so-called vanishing gradient problem. In this paper, we\nshow that learning longer term patterns in real data, such as in natural\nlanguage, is perfectly possible using gradient descent. This is achieved by\nusing a slight structural modification of the simple recurrent neural network\narchitecture. We encourage some of the hidden units to change their state\nslowly by making part of the recurrent weight matrix close to identity, thus\nforming kind of a longer term memory. We evaluate our model in language\nmodeling experiments, where we obtain similar performance to the much more\ncomplex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber,\n1997).\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 20:58:18 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 23:37:58 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Mikolov", "Tomas", ""], ["Joulin", "Armand", ""], ["Chopra", "Sumit", ""], ["Mathieu", "Michael", ""], ["Ranzato", "Marc'Aurelio", ""]]}, {"id": "1412.7755", "submitter": "Jimmy Ba", "authors": "Jimmy Ba, Volodymyr Mnih, Koray Kavukcuoglu", "title": "Multiple Object Recognition with Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an attention-based model for recognizing multiple objects in\nimages. The proposed model is a deep recurrent neural network trained with\nreinforcement learning to attend to the most relevant regions of the input\nimage. We show that the model learns to both localize and recognize multiple\nobjects despite being given only class labels during training. We evaluate the\nmodel on the challenging task of transcribing house number sequences from\nGoogle Street View images and show that it is both more accurate than the\nstate-of-the-art convolutional networks and uses fewer parameters and less\ncomputation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 20:58:23 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 16:49:23 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Ba", "Jimmy", ""], ["Mnih", "Volodymyr", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1412.7828", "submitter": "S{\\o}ren S{\\o}nderby", "authors": "S{\\o}ren Kaae S{\\o}nderby and Ole Winther", "title": "Protein Secondary Structure Prediction with Long Short Term Memory\n  Networks", "comments": "v2: adds larger network with slightly better results, update author\n  affiliations", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of protein secondary structure from the amino acid sequence is a\nclassical bioinformatics problem. Common methods use feed forward neural\nnetworks or SVMs combined with a sliding window, as these models does not\nnaturally handle sequential data. Recurrent neural networks are an\ngeneralization of the feed forward neural network that naturally handle\nsequential data. We use a bidirectional recurrent neural network with long\nshort term memory cells for prediction of secondary structure and evaluate\nusing the CB513 dataset. On the secondary structure 8-class problem we report\nbetter performance (0.674) than state of the art (0.664). Our model includes\nfeed forward networks between the long short term memory cells, a path that can\nbe further explored.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 14:27:42 GMT"}, {"version": "v2", "created": "Sun, 4 Jan 2015 19:44:17 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Winther", "Ole", ""]]}, {"id": "1412.7839", "submitter": "Waheed Bajwa", "authors": "Haroon Raja and Waheed U. Bajwa", "title": "Cloud K-SVD: A Collaborative Dictionary Learning Algorithm for Big,\n  Distributed Data", "comments": "Accepted for Publication in IEEE Trans. Signal Processing (2015); 16\n  pages, 3 figures", "journal-ref": "IEEE Trans. Signal Processing, vol. 64, no. 1, pp. 173-188, Jan.\n  2016", "doi": "10.1109/TSP.2015.2472372", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of data-adaptive representations for big,\ndistributed data. It is assumed that a number of geographically-distributed,\ninterconnected sites have massive local data and they are interested in\ncollaboratively learning a low-dimensional geometric structure underlying these\ndata. In contrast to previous works on subspace-based data representations,\nthis paper focuses on the geometric structure of a union of subspaces (UoS). In\nthis regard, it proposes a distributed algorithm---termed cloud K-SVD---for\ncollaborative learning of a UoS structure underlying distributed data of\ninterest. The goal of cloud K-SVD is to learn a common overcomplete dictionary\nat each individual site such that every sample in the distributed data can be\nrepresented through a small number of atoms of the learned dictionary. Cloud\nK-SVD accomplishes this goal without requiring exchange of individual samples\nbetween sites. This makes it suitable for applications where sharing of raw\ndata is discouraged due to either privacy concerns or large volumes of data.\nThis paper also provides an analysis of cloud K-SVD that gives insights into\nits properties as well as deviations of the dictionaries learned at individual\nsites from a centralized solution in terms of different measures of\nlocal/global data and topology of interconnections. Finally, the paper\nnumerically illustrates the efficacy of cloud K-SVD on real and synthetic\ndistributed data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 17:01:52 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 21:27:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Raja", "Haroon", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1412.7868", "submitter": "P.K. Srijith", "authors": "P. K. Srijith, P. Balamurugan and Shirish Shevade", "title": "Gaussian Process Pseudo-Likelihood Models for Sequence Labeling", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several machine learning problems arising in natural language processing can\nbe modeled as a sequence labeling problem. We provide Gaussian process models\nbased on pseudo-likelihood approximation to perform sequence labeling. Gaussian\nprocesses (GPs) provide a Bayesian approach to learning in a kernel based\nframework. The pseudo-likelihood model enables one to capture long range\ndependencies among the output components of the sequence without becoming\ncomputationally intractable. We use an efficient variational Gaussian\napproximation method to perform inference in the proposed model. We also\nprovide an iterative algorithm which can effectively make use of the\ninformation from the neighboring labels to perform prediction. The ability to\ncapture long range dependencies makes the proposed approach useful for a wide\nrange of sequence labeling problems. Numerical experiments on some sequence\nlabeling data sets demonstrate the usefulness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 21:59:46 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 05:41:08 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Srijith", "P. K.", ""], ["Balamurugan", "P.", ""], ["Shevade", "Shirish", ""]]}, {"id": "1412.7927", "submitter": "Raunaq Vohra", "authors": "Kratarth Goel, Raunaq Vohra, and J.K. Sahoo", "title": "Polyphonic Music Generation by Modeling Temporal Dependencies Using a\n  RNN-DBN", "comments": "8 pages, A4, 1 figure, 1 table, ICANN 2014 oral presentation. arXiv\n  admin note: text overlap with arXiv:1206.6392 by other authors", "journal-ref": "Lecture Notes in Computer Science Volume 8681, 2014, pp 217-224", "doi": "10.1007/978-3-319-11179-7_28", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generic technique to model temporal dependencies\nand sequences using a combination of a recurrent neural network and a Deep\nBelief Network. Our technique, RNN-DBN, is an amalgamation of the memory state\nof the RNN that allows it to provide temporal information and a multi-layer DBN\nthat helps in high level representation of the data. This makes RNN-DBNs ideal\nfor sequence generation. Further, the use of a DBN in conjunction with the RNN\nmakes this model capable of significantly more complex data representation than\nan RBM. We apply this technique to the task of polyphonic music generation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 11:08:42 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Goel", "Kratarth", ""], ["Vohra", "Raunaq", ""], ["Sahoo", "J. K.", ""]]}, {"id": "1412.7934", "submitter": "Raunaq Vohra", "authors": "Kratarth Goel, Raunaq Vohra, and Ainesh Bakshi", "title": "A Novel Feature Selection and Extraction Technique for Classification", "comments": "2 pages, 2 tables, published at IEEE SMC 2014", "journal-ref": "IEEE Xplore, Proceedings of IEEE SMC 2014, pages 4033 - 4034", "doi": "10.1109/SMC.2014.6974562", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a versatile technique for the purpose of feature\nselection and extraction - Class Dependent Features (CDFs). We use CDFs to\nimprove the accuracy of classification and at the same time control\ncomputational expense by tackling the curse of dimensionality. In order to\ndemonstrate the generality of this technique, it is applied to handwritten\ndigit recognition and text categorization.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 12:57:58 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Goel", "Kratarth", ""], ["Vohra", "Raunaq", ""], ["Bakshi", "Ainesh", ""]]}, {"id": "1412.7938", "submitter": "Shusen Wang", "authors": "Shusen Wang, Tong Zhang, Zhihua Zhang", "title": "Adjusting Leverage Scores by Row Weighting: A Practical Approach to\n  Coherent Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix completion is an important problem with extensive real-world\napplications. When observations are uniformly sampled from the underlying\nmatrix entries, existing methods all require the matrix to be incoherent. This\npaper provides the first working method for coherent matrix completion under\nthe standard uniform sampling model. Our approach is based on the weighted\nnuclear norm minimization idea proposed in several recent work, and our key\ncontribution is a practical method to compute the weighting matrices so that\nthe leverage scores become more uniform after weighting. Under suitable\nconditions, we are able to derive theoretical results, showing the\neffectiveness of our approach. Experiments on synthetic data show that our\napproach recovers highly coherent matrices with high precision, whereas the\nstandard unweighted method fails even on noise-free data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 13:56:02 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 12:28:34 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Wang", "Shusen", ""], ["Zhang", "Tong", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1412.7978", "submitter": "Daniel Kovach Jr.", "authors": "Daniel Kovach", "title": "The Computational Theory of Intelligence: Information Entropy", "comments": "Published at\n  http://www.scirp.org/journal/PaperInformation.aspx?PaperID=50204", "journal-ref": "International Journal of Modern Nonlinear Theory and Application,\n  3, 182-190 (2014)", "doi": "10.4236/ijmnta.2014.34020", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an information theoretic approach to the concept of\nintelligence in the computational sense. We introduce a probabilistic framework\nfrom which computational intelligence is shown to be an entropy minimizing\nprocess at the local level. Using this new scheme, we develop a simple data\ndriven clustering example and discuss its applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 07:41:45 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Kovach", "Daniel", ""]]}, {"id": "1412.7990", "submitter": "Ernesto Diaz-Aviles", "authors": "Ernesto Diaz-Aviles, Hoang Thanh Lam, Fabio Pinelli, Stefano Braghin,\n  Yiannis Gkoufas, Michele Berlingerio, and Francesco Calabrese", "title": "Predicting User Engagement in Twitter with Collaborative Ranking", "comments": "RecSysChallenge'14 at RecSys 2014, October 10, 2014, Foster City, CA,\n  USA", "journal-ref": "In Proceedings of the 2014 Recommender Systems Challenge\n  (RecSysChallenge'14). ACM, New York, NY, USA, , Pages 41 , 6 pages", "doi": "10.1145/2668067.2668072", "report-no": null, "categories": "cs.IR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering (CF) is a core component of popular web-based\nservices such as Amazon, YouTube, Netflix, and Twitter. Most applications use\nCF to recommend a small set of items to the user. For instance, YouTube\npresents to a user a list of top-n videos she would likely watch next based on\nher rating and viewing history. Current methods of CF evaluation have been\nfocused on assessing the quality of a predicted rating or the ranking\nperformance for top-n recommended items. However, restricting the recommender\nsystem evaluation to these two aspects is rather limiting and neglects other\ndimensions that could better characterize a well-perceived recommendation. In\nthis paper, instead of optimizing rating or top-n recommendation, we focus on\nthe task of predicting which items generate the highest user engagement. In\nparticular, we use Twitter as our testbed and cast the problem as a\nCollaborative Ranking task where the rich features extracted from the metadata\nof the tweets help to complement the transaction information limited to user\nids, item ids, ratings and timestamps. We learn a scoring function that\ndirectly optimizes the user engagement in terms of nDCG@10 on the predicted\nranking. Experiments conducted on an extended version of the MovieTweetings\ndataset, released as part of the RecSys Challenge 2014, show the effectiveness\nof our approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 21:00:14 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Diaz-Aviles", "Ernesto", ""], ["Lam", "Hoang Thanh", ""], ["Pinelli", "Fabio", ""], ["Braghin", "Stefano", ""], ["Gkoufas", "Yiannis", ""], ["Berlingerio", "Michele", ""], ["Calabrese", "Francesco", ""]]}, {"id": "1412.8060", "submitter": "Zheng Qu", "authors": "Zheng Qu and Peter Richt\\'arik", "title": "Coordinate Descent with Arbitrary Sampling I: Algorithms and Complexity", "comments": "32 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of minimizing the sum of a smooth convex function and a\nconvex block-separable regularizer and propose a new randomized coordinate\ndescent method, which we call ALPHA. Our method at every iteration updates a\nrandom subset of coordinates, following an arbitrary distribution. No\ncoordinate descent methods capable to handle an arbitrary sampling have been\nstudied in the literature before for this problem. ALPHA is a remarkably\nflexible algorithm: in special cases, it reduces to deterministic and\nrandomized methods such as gradient descent, coordinate descent, parallel\ncoordinate descent and distributed coordinate descent -- both in nonaccelerated\nand accelerated variants. The variants with arbitrary (or importance) sampling\nare new. We provide a complexity analysis of ALPHA, from which we deduce as a\ndirect corollary complexity bounds for its many variants, all matching or\nimproving best known bounds.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 15:28:26 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 16:09:21 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1412.8063", "submitter": "Zheng Qu", "authors": "Zheng Qu and Peter Richt\\'arik", "title": "Coordinate Descent with Arbitrary Sampling II: Expected Separable\n  Overapproximation", "comments": "29 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and complexity analysis of randomized coordinate descent methods,\nand in particular of variants which update a random subset (sampling) of\ncoordinates in each iteration, depends on the notion of expected separable\noverapproximation (ESO). This refers to an inequality involving the objective\nfunction and the sampling, capturing in a compact way certain smoothness\nproperties of the function in a random subspace spanned by the sampled\ncoordinates. ESO inequalities were previously established for special classes\nof samplings only, almost invariably for uniform samplings. In this paper we\ndevelop a systematic technique for deriving these inequalities for a large\nclass of functions and for arbitrary samplings. We demonstrate that one can\nrecover existing ESO results using our general approach, which is based on the\nstudy of eigenvalues associated with samplings and the data describing the\nfunction.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 15:39:30 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 19:49:20 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1412.8109", "submitter": "Anis Charrada", "authors": "Anis Charrada, Abdelaziz Samet", "title": "Complex support vector machines regression for robust channel estimation\n  in LTE downlink system", "comments": "13 pages Vol.4, IJCNC (2012) No.1, January 2012. arXiv admin note:\n  substantial text overlap with arXiv:1109.0895", "journal-ref": null, "doi": "10.5121/ijcnc.2012.4115", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of channel estimation for LTE Downlink system in\nthe environment of high mobility presenting non-Gaussian impulse noise\ninterfering with reference signals is faced. The estimation of the frequency\nselective time varying multipath fading channel is performed by using a channel\nestimator based on a nonlinear complex Support Vector Machine Regression (SVR)\nwhich is applied to Long Term Evolution (LTE) downlink. The estimation\nalgorithm makes use of the pilot signals to estimate the total frequency\nresponse of the highly selective fading multipath channel. Thus, the algorithm\nmaps trained data into a high dimensional feature space and uses the structural\nrisk minimization principle to carry out the regression estimation for the\nfrequency response function of the fading channel. The obtained results show\nthe effectiveness of the proposed method which has better performance than the\nconventional Least Squares (LS) and Decision Feedback methods to track the\nvariations of the fading multipath channel.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 19:51:35 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Charrada", "Anis", ""], ["Samet", "Abdelaziz", ""]]}, {"id": "1412.8147", "submitter": "Saeed Parseh", "authors": "Saeed Parseh and Ahmad Baraani", "title": "Improving Persian Document Classification Using Semantic Relations\n  between Words", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase of information, document classification as one of the\nmethods of text mining, plays vital role in many management and organizing\ninformation. Document classification is the process of assigning a document to\none or more predefined category labels. Document classification includes\ndifferent parts such as text processing, term selection, term weighting and\nfinal classification. The accuracy of document classification is very\nimportant. Thus improvement in each part of classification should lead to\nbetter results and higher precision. Term weighting has a great impact on the\naccuracy of the classification. Most of the existing weighting methods exploit\nthe statistical information of terms in documents and do not consider semantic\nrelations between words. In this paper, an automated document classification\nsystem is presented that uses a novel term weighting method based on semantic\nrelations between terms. To evaluate the proposed method, three standard\nPersian corpuses are used. Experiment results show 2 to 4 percent improvement\nin classification accuracy compared with the best previous designed system for\nPersian documents.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 10:56:53 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Parseh", "Saeed", ""], ["Baraani", "Ahmad", ""]]}, {"id": "1412.8291", "submitter": "Christian Osendorfer", "authors": "Maximilian Karl and Christian Osendorfer", "title": "Improving approximate RPCA with a k-sparsity prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A process centric view of robust PCA (RPCA) allows its fast approximate\nimplementation based on a special form o a deep neural network with weights\nshared across all layers. However, empirically this fast approximation to RPCA\nfails to find representations that are parsemonious. We resolve these bad local\nminima by relaxing the elementwise L1 and L2 priors and instead utilize a\nstructure inducing k-sparsity prior. In a discriminative classification task\nthe newly learned representations outperform these from the original\napproximate RPCA formulation significantly.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 09:51:20 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Karl", "Maximilian", ""], ["Osendorfer", "Christian", ""]]}, {"id": "1412.8293", "submitter": "Jiyan Yang", "authors": "Haim Avron, Vikas Sindhwani, Jiyan Yang, Michael Mahoney", "title": "Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels", "comments": "A short version of this paper has been presented in ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of improving the efficiency of randomized Fourier\nfeature maps to accelerate training and testing speed of kernel methods on\nlarge datasets. These approximate feature maps arise as Monte Carlo\napproximations to integral representations of shift-invariant kernel functions\n(e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo\n(QMC) approximations instead, where the relevant integrands are evaluated on a\nlow-discrepancy sequence of points as opposed to random point sets as in the\nMonte Carlo approach. We derive a new discrepancy measure called box\ndiscrepancy based on theoretical characterizations of the integration error\nwith respect to a given sequence. We then propose to learn QMC sequences\nadapted to our setting based on explicit box discrepancy minimization. Our\ntheoretical analyses are complemented with empirical results that demonstrate\nthe effectiveness of classical and adaptive QMC techniques for this problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 10:00:39 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2015 07:20:00 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Avron", "Haim", ""], ["Sindhwani", "Vikas", ""], ["Yang", "Jiyan", ""], ["Mahoney", "Michael", ""]]}, {"id": "1412.8307", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell, Migel D. Tissera, Tony Vladusich, Andr\\'e van\n  Schaik, and Jonathan Tapson", "title": "Fast, simple and accurate handwritten digit classification by training\n  shallow neural network classifiers with the 'extreme learning machine'\n  algorithm", "comments": "Accepted for publication; 9 pages of text, 6 figures and 1 table", "journal-ref": null, "doi": "10.1371/journal.pone.0134254", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in training deep (multi-layer) architectures have inspired a\nrenaissance in neural network use. For example, deep convolutional networks are\nbecoming the default option for difficult tasks on large datasets, such as\nimage and speech recognition. However, here we show that error rates below 1%\non the MNIST handwritten digit benchmark can be replicated with shallow\nnon-convolutional neural networks. This is achieved by training such networks\nusing the 'Extreme Learning Machine' (ELM) approach, which also enables a very\nrapid training time (~10 minutes). Adding distortions, as is common practise\nfor MNIST, reduces error rates even further. Our methods are also shown to be\ncapable of achieving less than 5.5% error rates on the NORB image database. To\nachieve these results, we introduce several enhancements to the standard ELM\nalgorithm, which individually and in combination can significantly improve\nperformance. The main innovation is to ensure each hidden-unit operates only on\na randomly sized and positioned patch of each image. This form of random\n`receptive field' sampling of the input ensures the input weight matrix is\nsparse, with about 90% of weights equal to zero. Furthermore, combining our\nmethods with a small number of iterations of a single-batch backpropagation\nmethod can significantly reduce the number of hidden-units required to achieve\na particular performance. Our close to state-of-the-art results for MNIST and\nNORB suggest that the ease of use and accuracy of the ELM algorithm for\ndesigning a single-hidden-layer neural network classifier should cause it to be\ngiven greater consideration either as a standalone method for simpler problems,\nor as the final classification stage in deep neural networks applied to more\ndifficult problems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 11:14:59 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 08:28:03 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["McDonnell", "Mark D.", ""], ["Tissera", "Migel D.", ""], ["Vladusich", "Tony", ""], ["van Schaik", "Andr\u00e9", ""], ["Tapson", "Jonathan", ""]]}, {"id": "1412.8380", "submitter": "Hidetoshi Shimodaira", "authors": "Hidetoshi Shimodaira", "title": "A simple coding for cross-domain matching with dimension reduction via\n  spectral graph embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data vectors are obtained from multiple domains. They are feature vectors of\nimages or vector representations of words. Domains may have different numbers\nof data vectors with different dimensions. These data vectors from multiple\ndomains are projected to a common space by linear transformations in order to\nsearch closely related vectors across domains. We would like to find projection\nmatrices to minimize distances between closely related data vectors. This\nformulation of cross-domain matching is regarded as an extension of the\nspectral graph embedding to multi-domain setting, and it includes several\nmultivariate analysis methods of statistics such as multiset canonical\ncorrelation analysis, correspondence analysis, and principal component\nanalysis. Similar approaches are very popular recently in pattern recognition\nand vision. In this paper, instead of proposing a novel method, we will\nintroduce an embarrassingly simple idea of coding the data vectors for\nexplaining all the above mentioned approaches. A data vector is concatenated\nwith zero vectors from all other domains to make an augmented vector. The\ncross-domain matching is solved by applying the single-domain version of\nspectral graph embedding to these augmented vectors of all the domains. An\ninteresting connection to the classical associative memory model of neural\nnetworks is also discussed by noticing a coding for association. A\ncross-validation method for choosing the dimension of the common space and a\nregularization parameter will be discussed in an illustrative numerical\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 16:08:27 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2015 18:38:26 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Shimodaira", "Hidetoshi", ""]]}, {"id": "1412.8439", "submitter": "Sewoong Oh", "authors": "Giulia Fanti, Peter Kairouz, Sewoong Oh, Pramod Viswanath", "title": "Spy vs. Spy: Rumor Source Obfuscation", "comments": "14 pages 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymous messaging platforms, such as Secret and Whisper, have emerged as\nimportant social media for sharing one's thoughts without the fear of being\njudged by friends, family, or the public. Further, such anonymous platforms are\ncrucial in nations with authoritarian governments; the right to free expression\nand sometimes the personal safety of the author of the message depend on\nanonymity. Whether for fear of judgment or personal endangerment, it is crucial\nto keep anonymous the identity of the user who initially posted a sensitive\nmessage. In this paper, we consider an adversary who observes a snapshot of the\nspread of a message at a certain time. Recent advances in rumor source\ndetection shows that the existing messaging protocols are vulnerable against\nsuch an adversary. We introduce a novel messaging protocol, which we call\nadaptive diffusion, and show that it spreads the messages fast and achieves a\nperfect obfuscation of the source when the underlying contact network is an\ninfinite regular tree: all users with the message are nearly equally likely to\nhave been the origin of the message. Experiments on a sampled Facebook network\nshow that it effectively hides the location of the source even when the graph\nis finite, irregular and has cycles. We further consider a stronger adversarial\nmodel where a subset of colluding users track the reception of messages. We\nshow that the adaptive diffusion provides a strong protection of the anonymity\nof the source even under this scenario.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 19:47:16 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 20:20:06 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2015 03:50:07 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Fanti", "Giulia", ""], ["Kairouz", "Peter", ""], ["Oh", "Sewoong", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1412.8493", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "An ADMM algorithm for solving a proximal bound-constrained quadratic\n  program", "comments": "5 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:1405.5960", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a proximal operator given by a quadratic function subject to\nbound constraints and give an optimization algorithm using the alternating\ndirection method of multipliers (ADMM). The algorithm is particularly efficient\nto solve a collection of proximal operators that share the same quadratic form,\nor if the quadratic program is the relaxation of a binary quadratic problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 21:35:19 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1412.8534", "submitter": "Mehdi Sajjadi", "authors": "Mehdi Sajjadi, Mojtaba Seyedhosseini, Tolga Tasdizen", "title": "Disjunctive Normal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are powerful pattern classifiers; however, they\nhave been surpassed in accuracy by methods such as support vector machines and\nrandom forests that are also easier to use and faster to train.\nBackpropagation, which is used to train artificial neural networks, suffers\nfrom the herd effect problem which leads to long training times and limit\nclassification accuracy. We use the disjunctive normal form and approximate the\nboolean conjunction operations with products to construct a novel network\narchitecture. The proposed model can be trained by minimizing an error function\nand it allows an effective and intuitive initialization which solves the\nherd-effect problem associated with backpropagation. This leads to state-of-the\nart classification accuracy and fast training times. In addition, our model can\nbe jointly optimized with convolutional features in an unified structure\nleading to state-of-the-art results on computer vision problems with fast\nconvergence rates. A GPU implementation of LDNN with optional convolutional\nfeatures is also available\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 02:17:30 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Sajjadi", "Mehdi", ""], ["Seyedhosseini", "Mojtaba", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1412.8566", "submitter": "Roger Grosse", "authors": "Yuri Burda and Roger B. Grosse and Ruslan Salakhutdinov", "title": "Accurate and Conservative Estimates of MRF Log-likelihood using Reverse\n  Annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random fields (MRFs) are difficult to evaluate as generative models\nbecause computing the test log-probabilities requires the intractable partition\nfunction. Annealed importance sampling (AIS) is widely used to estimate MRF\npartition functions, and often yields quite accurate results. However, AIS is\nprone to overestimate the log-likelihood with little indication that anything\nis wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lower\nbound on the log-likelihood of an approximation to the original MRF model.\nRAISE requires only the same MCMC transition operators as standard AIS.\nExperimental results indicate that RAISE agrees closely with AIS\nlog-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the\nside of underestimating, rather than overestimating, the log-likelihood.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 06:13:10 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Burda", "Yuri", ""], ["Grosse", "Roger B.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1412.8690", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, SIERRA)", "title": "Breaking the Curse of Dimensionality with Convex Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider neural networks with a single hidden layer and non-decreasing\nhomogeneous activa-tion functions like the rectified linear units. By letting\nthe number of hidden units grow unbounded and using classical non-Euclidean\nregularization tools on the output weights, we provide a detailed theoretical\nanalysis of their generalization performance, with a study of both the\napproximation and the estimation errors. We show in particular that they are\nadaptive to unknown underlying linear structures, such as the dependence on the\nprojection of the input variables onto a low-dimensional subspace. Moreover,\nwhen using sparsity-inducing norms on the input weights, we show that\nhigh-dimensional non-linear variable selection may be achieved, without any\nstrong assumption regarding the data and with a total number of variables\npotentially exponential in the number of ob-servations. In addition, we provide\na simple geometric interpretation to the non-convex problem of addition of a\nnew unit, which is the core potentially hard computational element in the\nframework of learning from continuously many basis functions. We provide simple\nconditions for convex relaxations to achieve the same generalization error\nbounds, even when constant-factor approxi-mations cannot be found (e.g.,\nbecause it is NP-hard such as for the zero-homogeneous activation function). We\nwere not able to find strong enough convex relaxations and leave open the\nexistence or non-existence of polynomial-time algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 17:08:00 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 08:38:03 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Bach", "Francis", "", "LIENS, SIERRA"]]}]