[{"id": "1808.00004", "submitter": "Kaige Yang Mr", "authors": "Kaige Yang and Laura Toni", "title": "Graph-Based Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study recommendation systems modelled as contextual\nmulti-armed bandit (MAB) problems. We propose a graph-based recommendation\nsystem that learns and exploits the geometry of the user space to create\nmeaningful clusters in the user domain. This reduces the dimensionality of the\nrecommendation problem while preserving the accuracy of MAB. We then study the\neffect of graph sparsity and clusters size on the MAB performance and provide\nexhaustive simulation results both in synthetic and in real-case datasets.\nSimulation results show improvements with respect to state-of-the-art MAB\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 14:16:54 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Yang", "Kaige", ""], ["Toni", "Laura", ""]]}, {"id": "1808.00020", "submitter": "Bogdan Mazoure", "authors": "Thang Doan, Joao Monteiro, Isabela Albuquerque, Bogdan Mazoure, Audrey\n  Durand, Joelle Pineau, R Devon Hjelm", "title": "On-line Adaptative Curriculum Learning for GANs", "comments": "Accepted to the Thirty-Third AAAI Conference On Artificial\n  Intelligence, 2019 (Added 128x128 CelebA samples to the end of the appendix)", "journal-ref": "Proceedings of 33rd AAAI Conference on Artificial Intelligence\n  (AAAI 2019)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) can successfully approximate a\nprobability distribution and produce realistic samples. However, open questions\nsuch as sufficient convergence conditions and mode collapse still persist. In\nthis paper, we build on existing work in the area by proposing a novel\nframework for training the generator against an ensemble of discriminator\nnetworks, which can be seen as a one-student/multiple-teachers setting. We\nformalize this problem within the full-information adversarial bandit\nframework, where we evaluate the capability of an algorithm to select mixtures\nof discriminators for providing the generator with feedback during learning. To\nthis end, we propose a reward function which reflects the progress made by the\ngenerator and dynamically update the mixture weights allocated to each\ndiscriminator. We also draw connections between our algorithm and stochastic\noptimization methods and then show that existing approaches using multiple\ndiscriminators in literature can be recovered from our framework. We argue that\nless expressive discriminators are smoother and have a general coarse grained\nview of the modes map, which enforces the generator to cover a wide portion of\nthe data distribution support. On the other hand, highly expressive\ndiscriminators ensure samples quality. Finally, experimental results show that\nour approach improves samples quality and diversity over existing baselines by\neffectively learning a curriculum. These results also support the claim that\nweaker discriminators have higher entropy improving modes coverage. Keywords:\nmultiple discriminators, curriculum learning, multiple resolutions\ndiscriminators, multi-armed bandits, generative adversarial networks, smooth\ndiscriminators, multi-discriminator gan training, multiple experts.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 18:34:56 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 22:56:38 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 15:52:10 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2018 17:17:59 GMT"}, {"version": "v5", "created": "Wed, 12 Dec 2018 17:58:03 GMT"}, {"version": "v6", "created": "Mon, 11 Mar 2019 17:15:48 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Doan", "Thang", ""], ["Monteiro", "Joao", ""], ["Albuquerque", "Isabela", ""], ["Mazoure", "Bogdan", ""], ["Durand", "Audrey", ""], ["Pineau", "Joelle", ""], ["Hjelm", "R Devon", ""]]}, {"id": "1808.00022", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou and Ronald Poppe", "title": "Analyzing Human-Human Interactions: A Survey", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2019.102799", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many videos depict people, and it is their interactions that inform us of\ntheir activities, relation to one another and the cultural and social setting.\nWith advances in human action recognition, researchers have begun to address\nthe automated recognition of these human-human interactions from video. The\nmain challenges stem from dealing with the considerable variation in recording\nsetting, the appearance of the people depicted and the coordinated performance\nof their interaction. This survey provides a summary of these challenges and\ndatasets to address these, followed by an in-depth discussion of relevant\nvision-based recognition and detection methods. We focus on recent, promising\nwork based on deep learning and convolutional neural networks (CNNs). Finally,\nwe outline directions to overcome the limitations of the current\nstate-of-the-art to analyze and, eventually, understand social human actions.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 18:37:41 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 09:24:43 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Stergiou", "Alexandros", ""], ["Poppe", "Ronald", ""]]}, {"id": "1808.00033", "submitter": "Mengnan Du", "authors": "Mengnan Du, Ninghao Liu, Xia Hu", "title": "Techniques for Interpretable Machine Learning", "comments": "Accepted by Communications of the ACM (CACM), Review Article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretable machine learning tackles the important problem that humans\ncannot understand the behaviors of complex machine learning models and how\nthese models arrive at a particular decision. Although many approaches have\nbeen proposed, a comprehensive understanding of the achievements and challenges\nis still lacking. We provide a survey covering existing techniques to increase\nthe interpretability of machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as designing\nuser-friendly explanations and developing comprehensive evaluation metrics to\nfurther push forward the area of interpretable machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 19:14:39 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 17:24:35 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 20:44:37 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Du", "Mengnan", ""], ["Liu", "Ninghao", ""], ["Hu", "Xia", ""]]}, {"id": "1808.00036", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia, Christian F. Beckmann, Andre F. Marquand", "title": "Scalable Multi-Task Gaussian Process Tensor Regression for Normative\n  Modeling of Structured Variation in Neuroimaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most brain disorders are very heterogeneous in terms of their underlying\nbiology and developing analysis methods to model such heterogeneity is a major\nchallenge. A promising approach is to use probabilistic regression methods to\nestimate normative models of brain function using (f)MRI data then use these to\nmap variation across individuals in clinical populations (e.g., via anomaly\ndetection). To fully capture individual differences, it is crucial to\nstatistically model the patterns of correlation across different brain regions\nand individuals. However, this is very challenging for neuroimaging data\nbecause of high-dimensionality and highly structured patterns of correlation\nacross multiple axes. Here, we propose a general and flexible multi-task\nlearning framework to address this problem. Our model uses a tensor-variate\nGaussian process in a Bayesian mixed-effects model and makes use of Kronecker\nalgebra and a low-rank approximation to scale efficiently to multi-way\nneuroimaging data at the whole brain level. On a publicly available clinical\nfMRI dataset, we show that our computationally affordable approach\nsubstantially improves detection sensitivity over both a mass-univariate\nnormative model and a classifier that --unlike our approach-- has full access\nto the clinical labels.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 19:26:59 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 16:30:17 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Kia", "Seyed Mostafa", ""], ["Beckmann", "Christian F.", ""], ["Marquand", "Andre F.", ""]]}, {"id": "1808.00046", "submitter": "Mandar Gogate", "authors": "Ahsan Adeel, Mandar Gogate, Amir Hussain, William M. Whitmer", "title": "Lip-Reading Driven Deep Learning Approach for Speech Enhancement", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel lip-reading driven deep learning framework for\nspeech enhancement. The proposed approach leverages the complementary strengths\nof both deep learning and analytical acoustic modelling (filtering based\napproach) as compared to recently published, comparatively simpler benchmark\napproaches that rely only on deep learning. The proposed audio-visual (AV)\nspeech enhancement framework operates at two levels. In the first level, a\nnovel deep learning-based lip-reading regression model is employed. In the\nsecond level, lip-reading approximated clean-audio features are exploited,\nusing an enhanced, visually-derived Wiener filter (EVWF), for the clean audio\npower spectrum estimation. Specifically, a stacked long-short-term memory\n(LSTM) based lip-reading regression model is designed for clean audio features\nestimation using only temporal visual features considering different number of\nprior visual frames. For clean speech spectrum estimation, a new\nfilterbank-domain EVWF is formulated, which exploits estimated speech features.\nThe proposed EVWF is compared with conventional Spectral Subtraction and\nLog-Minimum Mean-Square Error methods using both ideal AV mapping and LSTM\ndriven AV mapping. The potential of the proposed speech enhancement framework\nis evaluated under different dynamic real-world commercially-motivated\nscenarios (e.g. cafe, public transport, pedestrian area) at different SNR\nlevels (ranging from low to high SNRs) using benchmark Grid and ChiME3 corpora.\nFor objective testing, perceptual evaluation of speech quality is used to\nevaluate the quality of restored speech. For subjective testing, the standard\nmean-opinion-score method is used with inferential statistics. Comparative\nsimulation results demonstrate significant lip-reading and speech enhancement\nimprovement in terms of both speech quality and speech intelligibility.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 19:50:13 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Adeel", "Ahsan", ""], ["Gogate", "Mandar", ""], ["Hussain", "Amir", ""], ["Whitmer", "William M.", ""]]}, {"id": "1808.00058", "submitter": "Abolfazl Razi", "authors": "Han Peng, Abolfazl Razi, Fatemeh Afghah, Jonathan Ashdown", "title": "A Unified Framework for Joint Mobility Prediction and Object Profiling\n  of Drones in UAV Networks", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, using a network of autonomous and cooperative unmanned\naerial vehicles (UAVs) without command and communication from the ground\nstation has become more imperative, in particular in search-and-rescue\noperations, disaster management, and other applications where human\nintervention is limited. In such scenarios, UAVs can make more efficient\ndecisions if they acquire more information about the mobility, sensing and\nactuation capabilities of their neighbor nodes. In this paper, we develop an\nunsupervised online learning algorithm for joint mobility prediction and object\nprofiling of UAVs to facilitate control and communication protocols. The\nproposed method not only predicts the future locations of the surrounding\nflying objects, but also classifies them into different groups with similar\nlevels of maneuverability (e.g. rotatory, and fixed-wing UAVs) without prior\nknowledge about these classes. This method is flexible in admitting new object\ntypes with unknown mobility profiles, thereby applicable to emerging flying\nAd-hoc networks with heterogeneous nodes.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 20:08:34 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Peng", "Han", ""], ["Razi", "Abolfazl", ""], ["Afghah", "Fatemeh", ""], ["Ashdown", "Jonathan", ""]]}, {"id": "1808.00060", "submitter": "Mandar Gogate", "authors": "Mandar Gogate, Ahsan Adeel, Ricard Marxer, Jon Barker, Amir Hussain", "title": "DNN driven Speaker Independent Audio-Visual Mask Estimation for Speech\n  Separation", "comments": "Accepted for Interspeech 2018, 5 pages, 4 figures", "journal-ref": null, "doi": "10.21437/Interspeech.2018-2516", "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human auditory cortex excels at selectively suppressing background noise to\nfocus on a target speaker. The process of selective attention in the brain is\nknown to contextually exploit the available audio and visual cues to better\nfocus on target speaker while filtering out other noises. In this study, we\npropose a novel deep neural network (DNN) based audiovisual (AV) mask\nestimation model. The proposed AV mask estimation model contextually integrates\nthe temporal dynamics of both audio and noise-immune visual features for\nimproved mask estimation and speech separation. For optimal AV features\nextraction and ideal binary mask (IBM) estimation, a hybrid DNN architecture is\nexploited to leverages the complementary strengths of a stacked long short term\nmemory (LSTM) and convolution LSTM network. The comparative simulation results\nin terms of speech quality and intelligibility demonstrate significant\nperformance improvement of our proposed AV mask estimation model as compared to\naudio-only and visual-only mask estimation approaches for both speaker\ndependent and independent scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 20:12:15 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Gogate", "Mandar", ""], ["Adeel", "Ahsan", ""], ["Marxer", "Ricard", ""], ["Barker", "Jon", ""], ["Hussain", "Amir", ""]]}, {"id": "1808.00068", "submitter": "Javad Rahimipour Anaraki", "authors": "Javad Rahimipour Anaraki, Saeed Samet, Mahdi Eftekhari, Chang Wook Ahn", "title": "A Fuzzy-Rough based Binary Shuffled Frog Leaping Algorithm for Feature\n  Selection", "comments": "24 pages, 11 Tables, 1 figure", "journal-ref": null, "doi": "10.5281/zenodo.1474575", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection and attribute reduction are crucial problems, and widely\nused techniques in the field of machine learning, data mining and pattern\nrecognition to overcome the well-known phenomenon of the Curse of\nDimensionality, by either selecting a subset of features or removing unrelated\nones. This paper presents a new feature selection method that efficiently\ncarries out attribute reduction, thereby selecting the most informative\nfeatures of a dataset. It consists of two components: 1) a measure for feature\nsubset evaluation, and 2) a search strategy. For the evaluation measure, we\nhave employed the fuzzy-rough dependency degree (FRFDD) in the lower\napproximation-based fuzzy-rough feature selection (L-FRFS) due to its\neffectiveness in feature selection. As for the search strategy, a new version\nof a binary shuffled frog leaping algorithm is proposed (B-SFLA). The new\nfeature selection method is obtained by hybridizing the B-SFLA with the FRDD.\nNon-parametric statistical tests are conducted to compare the proposed approach\nwith several existing methods over twenty two datasets, including nine high\ndimensional and large ones, from the UCI repository. The experimental results\ndemonstrate that the B-SFLA approach significantly outperforms other\nmetaheuristic methods in terms of the number of selected features and the\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 20:38:19 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Anaraki", "Javad Rahimipour", ""], ["Samet", "Saeed", ""], ["Eftekhari", "Mahdi", ""], ["Ahn", "Chang Wook", ""]]}, {"id": "1808.00076", "submitter": "Gabriel de Souza Pereira Moreira", "authors": "Gabriel de Souza P. Moreira, Felipe Ferreira, Adilson Marques da Cunha", "title": "News Session-Based Recommendations using Deep Neural Networks", "comments": "Accepted for the Third Workshop on Deep Learning for Recommender\n  Systems - DLRS 2018, October 02-07, 2018, Vancouver, Canada.\n  https://recsys.acm.org/recsys18/dlrs/", "journal-ref": null, "doi": "10.1145/3270323.3270328", "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News recommender systems are aimed to personalize users experiences and help\nthem to discover relevant articles from a large and dynamic search space.\nTherefore, news domain is a challenging scenario for recommendations, due to\nits sparse user profiling, fast growing number of items, accelerated item's\nvalue decay, and users preferences dynamic shift. Some promising results have\nbeen recently achieved by the usage of Deep Learning techniques on Recommender\nSystems, specially for item's feature extraction and for session-based\nrecommendations with Recurrent Neural Networks. In this paper, it is proposed\nan instantiation of the CHAMELEON -- a Deep Learning Meta-Architecture for News\nRecommender Systems. This architecture is composed of two modules, the first\nresponsible to learn news articles representations, based on their text and\nmetadata, and the second module aimed to provide session-based recommendations\nusing Recurrent Neural Networks. The recommendation task addressed in this work\nis next-item prediction for users sessions: \"what is the next most likely\narticle a user might read in a session?\" Users sessions context is leveraged by\nthe architecture to provide additional information in such extreme cold-start\nscenario of news recommendation. Users' behavior and item features are both\nmerged in an hybrid recommendation approach. A temporal offline evaluation\nmethod is also proposed as a complementary contribution, for a more realistic\nevaluation of such task, considering dynamic factors that affect global\nreadership interests like popularity, recency, and seasonality. Experiments\nwith an extensive number of session-based recommendation methods were performed\nand the proposed instantiation of CHAMELEON meta-architecture obtained a\nsignificant relative improvement in top-n accuracy and ranking metrics (10% on\nHit Rate and 13% on MRR) over the best benchmark methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 21:15:54 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 01:02:00 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2018 03:09:58 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Moreira", "Gabriel de Souza P.", ""], ["Ferreira", "Felipe", ""], ["da Cunha", "Adilson Marques", ""]]}, {"id": "1808.00079", "submitter": "Jianwei Feng", "authors": "Jianwei Feng and Dong Huang", "title": "Optimal Gradient Checkpoint Search for Arbitrary Computation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks(DNNs) require huge GPU memory when training on modern\nimage/video databases. Unfortunately, the GPU memory is physically finite,\nwhich limits the image resolutions and batch sizes that could be used in\ntraining for better DNN performance. Unlike solutions that require physically\nupgrade GPUs, the Gradient CheckPointing(GCP) training trades computation for\nmore memory beyond existing GPU hardware. GCP only stores a subset of\nintermediate tensors, called Gradient Checkpoints (GCs), during forward. Then\nduring backward, extra local forwards are conducted to compute the missing\ntensors. The total training memory cost becomes the sum of (1) the memory cost\nof the gradient checkpoints and (2) the maximum memory cost of local forwards.\nTo achieve maximal memory cut-offs, one needs optimal algorithms to select GCs.\nExisting GCP approaches rely on either manual input of GCs or heuristics-based\nGC search on Linear Computation Graphs (LCGs), and cannot apply to Arbitrary\nComputation Graphs(ACGs). In this paper, we present theories and optimal\nalgorithms on GC selection that, for the first time, are applicable to ACGs and\nachieve the maximal memory cut-offs. Extensive experiments show that our\napproach not only outperforms existing approaches (only applicable on LCGs),\nand is applicable to a vast family of LCG and ACG networks, such as Alexnet,\nVGG, ResNet, Densenet, Inception Net and highly complicated DNNs by Network\nArchitecture Search. Our work enables GCP training on ACGs, and cuts off up-to\n80% of training memory with a moderate time overhead (~30%-50%). Codes are\navailable\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 21:52:08 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 21:43:43 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 01:27:21 GMT"}, {"version": "v4", "created": "Mon, 23 Sep 2019 23:18:18 GMT"}, {"version": "v5", "created": "Tue, 2 Mar 2021 03:38:11 GMT"}, {"version": "v6", "created": "Thu, 18 Mar 2021 04:32:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Feng", "Jianwei", ""], ["Huang", "Dong", ""]]}, {"id": "1808.00087", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang and Borja Balle and Shiva Kasiviswanathan", "title": "Subsampled R\\'enyi Differential Privacy and Analytical Moments\n  Accountant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of subsampling in differential privacy (DP), a question\nthat is the centerpiece behind many successful differentially private machine\nlearning algorithms. Specifically, we provide a tight upper bound on the\nR\\'enyi Differential Privacy (RDP) (Mironov, 2017) parameters for algorithms\nthat: (1) subsample the dataset, and then (2) applies a randomized mechanism M\nto the subsample, in terms of the RDP parameters of M and the subsampling\nprobability parameter. Our results generalize the moments accounting technique,\ndeveloped by Abadi et al. (2016) for the Gaussian mechanism, to any subsampled\nRDP mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 22:13:39 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 06:28:59 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Balle", "Borja", ""], ["Kasiviswanathan", "Shiva", ""]]}, {"id": "1808.00098", "submitter": "Fenglei Fan", "authors": "Fenglei Fan, Jinjun Xiong, Ge Wang", "title": "Universal Approximation with Quadratic Deep Networks", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has achieved huge successes in many important\napplications. In our previous studies, we proposed quadratic/second-order\nneurons and deep quadratic neural networks. In a quadratic neuron, the inner\nproduct of a vector of data and the corresponding weights in a conventional\nneuron is replaced with a quadratic function. The resultant quadratic neuron\nenjoys an enhanced expressive capability over the conventional neuron. However,\nhow quadratic neurons improve the expressing capability of a deep quadratic\nnetwork has not been studied up to now, preferably in relation to that of a\nconventional neural network. Regarding this, we ask four basic questions in\nthis paper: (1) for the one-hidden-layer network structure, is there any\nfunction that a quadratic network can approximate much more efficiently than a\nconventional network? (2) for the same multi-layer network structure, is there\nany function that can be expressed by a quadratic network but cannot be\nexpressed with conventional neurons in the same structure? (3) Does a quadratic\nnetwork give a new insight into universal approximation? (4) To approximate the\nsame class of functions with the same error bound, is a quantized quadratic\nnetwork able to enjoy a lower number of weights than a quantized conventional\nnetwork? Our main contributions are the four interconnected theorems shedding\nlight upon these four questions and demonstrating the merits of a quadratic\nnetwork in terms of expressive efficiency, unique capability, compact\narchitecture and computational capacity respectively.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 22:41:20 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 21:24:58 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 13:25:47 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Fan", "Fenglei", ""], ["Xiong", "Jinjun", ""], ["Wang", "Ge", ""]]}, {"id": "1808.00111", "submitter": "Tim Leathart", "authors": "Tim Leathart, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer", "title": "Probability Calibration Trees", "comments": "Proceedings of the 9th Asian Conference on Machine Learning", "journal-ref": "Leathart, T., Frank, E., Holmes, G., & Pfahringer, B. (2017).\n  Probability calibration trees. In Proceedings of the 9th Asian Conference on\n  Machine Learning (pp. 145-160)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining accurate and well calibrated probability estimates from classifiers\nis useful in many applications, for example, when minimising the expected cost\nof classifications. Existing methods of calibrating probability estimates are\napplied globally, ignoring the potential for improvements by applying a more\nfine-grained model. We propose probability calibration trees, a modification of\nlogistic model trees that identifies regions of the input space in which\ndifferent probability calibration models are learned to improve performance. We\ncompare probability calibration trees to two widely used calibration\nmethods---isotonic regression and Platt scaling---and show that our method\nresults in lower root mean squared error on average than both methods, for\nestimates produced by a variety of base learners.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 23:37:21 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 04:00:23 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Leathart", "Tim", ""], ["Frank", "Eibe", ""], ["Holmes", "Geoffrey", ""], ["Pfahringer", "Bernhard", ""]]}, {"id": "1808.00113", "submitter": "Sumeet Singh", "authors": "Sumeet Singh, Vikas Sindhwani, Jean-Jacques E. Slotine, Marco Pavone", "title": "Learning Stabilizable Dynamical Systems via Control Contraction Metrics", "comments": "To appear at WAFR 2018. v2: re-structured Sections 3 & 4 to improve\n  clarity; expanded discussion on limitations & future work in Section 5; added\n  details on training & validation, significantly expanded experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG cs.RO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for learning stabilizable nonlinear dynamical\nsystems for continuous control tasks in robotics. The key idea is to develop a\nnew control-theoretic regularizer for dynamics fitting rooted in the notion of\nstabilizability, which guarantees that the learned system can be accompanied by\na robust controller capable of stabilizing any open-loop trajectory that the\nsystem may generate. By leveraging tools from contraction theory, statistical\nlearning, and convex optimization, we provide a general and tractable\nsemi-supervised algorithm to learn stabilizable dynamics, which can be applied\nto complex underactuated systems. We validated the proposed algorithm on a\nsimulated planar quadrotor system and observed notably improved trajectory\ngeneration and tracking performance with the control-theoretic regularized\nmodel over models learned using traditional regression techniques, especially\nwhen using a small number of demonstration examples. The results presented\nillustrate the need to infuse standard model-based reinforcement learning\nalgorithms with concepts drawn from nonlinear control theory for improved\nreliability.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 23:46:50 GMT"}, {"version": "v2", "created": "Sun, 11 Nov 2018 03:38:16 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Singh", "Sumeet", ""], ["Sindhwani", "Vikas", ""], ["Slotine", "Jean-Jacques E.", ""], ["Pavone", "Marco", ""]]}, {"id": "1808.00123", "submitter": "Ting Wang", "authors": "Yujie Ji, Xinyang Zhang, Ting Wang", "title": "EagleEye: Attack-Agnostic Defense against Adversarial Inputs (Technical\n  Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are inherently vulnerable to adversarial inputs:\nsuch maliciously crafted samples trigger DNNs to misbehave, leading to\ndetrimental consequences for DNN-powered systems. The fundamental challenges of\nmitigating adversarial inputs stem from their adaptive and variable nature.\nExisting solutions attempt to improve DNN resilience against specific attacks;\nyet, such static defenses can often be circumvented by adaptively engineered\ninputs or by new attack variants.\n  Here, we present EagleEye, an attack-agnostic adversarial tampering analysis\nengine for DNN-powered systems. Our design exploits the {\\em minimality\nprinciple} underlying many attacks: to maximize the attack's evasiveness, the\nadversary often seeks the minimum possible distortion to convert genuine inputs\nto adversarial ones. We show that this practice entails the distinct\ndistributional properties of adversarial inputs in the input space. By\nleveraging such properties in a principled manner, EagleEye effectively\ndiscriminates adversarial inputs and even uncovers their correct classification\noutputs. Through extensive empirical evaluation using a range of benchmark\ndatasets and DNN models, we validate EagleEye's efficacy. We further\ninvestigate the adversary's possible countermeasures, which implies a difficult\ndilemma for her: to evade EagleEye's detection, excessive distortion is\nnecessary, thereby significantly reducing the attack's evasiveness regarding\nother detection mechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 00:59:08 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Ji", "Yujie", ""], ["Zhang", "Xinyang", ""], ["Wang", "Ting", ""]]}, {"id": "1808.00131", "submitter": "Xingwei Hu Dr", "authors": "Xingwei Hu", "title": "A Theory of Dichotomous Valuation with Applications to Variable\n  Selection", "comments": "74 pages, 3 figures, 3 tables, 4 algorithms, 12 theorems, and 14\n  proofs", "journal-ref": "Econometric Reviews, 2020", "doi": "10.1080/07474938.2020.1735750", "report-no": null, "categories": "stat.ML cs.LG econ.EM math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An econometric or statistical model may undergo a marginal gain if we admit a\nnew variable to the model, and a marginal loss if we remove an existing\nvariable from the model. Assuming equality of opportunity among all candidate\nvariables, we derive a valuation framework by the expected marginal gain and\nmarginal loss in all potential modeling scenarios. However, marginal gain and\nloss are not symmetric; thus, we introduce three unbiased solutions. When used\nin variable selection, our new approaches significantly outperform several\npopular methods used in practice. The results also explore some novel traits of\nthe Shapley value.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 01:30:22 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 20:42:09 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 19:07:46 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2020 14:35:59 GMT"}, {"version": "v5", "created": "Fri, 13 Mar 2020 13:25:34 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Hu", "Xingwei", ""]]}, {"id": "1808.00158", "submitter": "Mirco Ravanelli", "authors": "Mirco Ravanelli, Yoshua Bengio", "title": "Speaker Recognition from Raw Waveform with SincNet", "comments": "In Proceedings of SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is progressively gaining popularity as a viable alternative to\ni-vectors for speaker recognition. Promising results have been recently\nobtained with Convolutional Neural Networks (CNNs) when fed by raw speech\nsamples directly. Rather than employing standard hand-crafted features, the\nlatter CNNs learn low-level speech representations from waveforms, potentially\nallowing the network to better capture important narrow-band speaker\ncharacteristics such as pitch and formants. Proper design of the neural network\nis crucial to achieve this goal. This paper proposes a novel CNN architecture,\ncalled SincNet, that encourages the first convolutional layer to discover more\nmeaningful filters. SincNet is based on parametrized sinc functions, which\nimplement band-pass filters. In contrast to standard CNNs, that learn all\nelements of each filter, only low and high cutoff frequencies are directly\nlearned from data with the proposed method. This offers a very compact and\nefficient way to derive a customized filter bank specifically tuned for the\ndesired application. Our experiments, conducted on both speaker identification\nand speaker verification tasks, show that the proposed architecture converges\nfaster and performs better than a standard CNN on raw waveforms.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 16:27:19 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2018 19:19:14 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 15:52:10 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ravanelli", "Mirco", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1808.00177", "submitter": "OpenAI OpenAI", "authors": "OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal\n  Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert,\n  Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter\n  Welinder, Lilian Weng, Wojciech Zaremba", "title": "Learning Dexterous In-Hand Manipulation", "comments": "Making OpenAI the first author. We wish this paper to be cited as\n  \"Learning Dexterous In-Hand Manipulation\" by OpenAI et al. We are replicating\n  the approach from the physics community: arXiv:1812.06489", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use reinforcement learning (RL) to learn dexterous in-hand manipulation\npolicies which can perform vision-based object reorientation on a physical\nShadow Dexterous Hand. The training is performed in a simulated environment in\nwhich we randomize many of the physical properties of the system like friction\ncoefficients and an object's appearance. Our policies transfer to the physical\nrobot despite being trained entirely in simulation. Our method does not rely on\nany human demonstrations, but many behaviors found in human manipulation emerge\nnaturally, including finger gaiting, multi-finger coordination, and the\ncontrolled use of gravity. Our results were obtained using the same distributed\nRL system that was used to train OpenAI Five. We also include a video of our\nresults: https://youtu.be/jwSbzNHGflM\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 06:02:36 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 09:08:32 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 02:26:22 GMT"}, {"version": "v4", "created": "Wed, 16 Jan 2019 01:52:36 GMT"}, {"version": "v5", "created": "Fri, 18 Jan 2019 23:26:53 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["OpenAI", "", ""], ["Andrychowicz", "Marcin", ""], ["Baker", "Bowen", ""], ["Chociej", "Maciek", ""], ["Jozefowicz", "Rafal", ""], ["McGrew", "Bob", ""], ["Pachocki", "Jakub", ""], ["Petron", "Arthur", ""], ["Plappert", "Matthias", ""], ["Powell", "Glenn", ""], ["Ray", "Alex", ""], ["Schneider", "Jonas", ""], ["Sidor", "Szymon", ""], ["Tobin", "Josh", ""], ["Welinder", "Peter", ""], ["Weng", "Lilian", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1808.00191", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh", "title": "Graph R-CNN for Scene Graph Generation", "comments": "16 pages, ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel scene graph generation model called Graph R-CNN, that is\nboth effective and efficient at detecting objects and their relations in\nimages. Our model contains a Relation Proposal Network (RePN) that efficiently\ndeals with the quadratic number of potential relations between objects in an\nimage. We also propose an attentional Graph Convolutional Network (aGCN) that\neffectively captures contextual information between objects and relations.\nFinally, we introduce a new evaluation metric that is more holistic and\nrealistic than existing metrics. We report state-of-the-art performance on\nscene graph generation as evaluated using both existing and our proposed\nmetrics.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 06:50:19 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Yang", "Jianwei", ""], ["Lu", "Jiasen", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1808.00193", "submitter": "Chen Yukang", "authors": "Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, Chang Huang,\n  Lisen Mu, Xinggang Wang", "title": "Reinforced Evolutionary Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is an important yet challenging task in\nnetwork design due to its high computational consumption. To address this\nissue, we propose the Reinforced Evolutionary Neural Architecture Search (RE-\nNAS), which is an evolutionary method with the reinforced mutation for NAS. Our\nmethod integrates reinforced mutation into an evolution algorithm for neural\narchitecture exploration, in which a mutation controller is introduced to learn\nthe effects of slight modifications and make mutation actions. The reinforced\nmutation controller guides the model population to evolve efficiently.\nFurthermore, as child models can inherit parameters from their parents during\nevolution, our method requires very limited computational resources. In\nexperiments, we conduct the proposed search method on CIFAR-10 and obtain a\npowerful network architecture, RENASNet. This architecture achieves a\ncompetitive result on CIFAR-10. The explored network architecture is\ntransferable to ImageNet and achieves a new state-of-the-art accuracy, i.e.,\n75.7% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test\nits performance on semantic segmentation with DeepLabv3 on the PASCAL VOC.\nRENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83%\nmIOU without being pre-trained on COCO.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 06:53:53 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 02:09:29 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 11:12:02 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chen", "Yukang", ""], ["Meng", "Gaofeng", ""], ["Zhang", "Qian", ""], ["Xiang", "Shiming", ""], ["Huang", "Chang", ""], ["Mu", "Lisen", ""], ["Wang", "Xinggang", ""]]}, {"id": "1808.00196", "submitter": "Yang Wang", "authors": "Jiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, David S. Ebert", "title": "Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of\n  Machine Learning Models", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2018.2864499", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation and diagnosis of machine learning models have gained renewed\ninterest in recent years with breakthroughs in new approaches. We present\nManifold, a framework that utilizes visual analysis techniques to support\ninterpretation, debugging, and comparison of machine learning models in a more\ntransparent and interactive manner. Conventional techniques usually focus on\nvisualizing the internal logic of a specific model type (i.e., deep neural\nnetworks), lacking the ability to extend to a more complex scenario where\ndifferent model types are integrated. To this end, Manifold is designed as a\ngeneric framework that does not rely on or access the internal logic of the\nmodel and solely observes the input (i.e., instances or features) and the\noutput (i.e., the predicted result and probability distribution). We describe\nthe workflow of Manifold as an iterative process consisting of three major\nphases that are commonly involved in the model development and diagnosis\nprocess: inspection (hypothesis), explanation (reasoning), and refinement\n(verification). The visual components supporting these tasks include a\nscatterplot-based visual summary that overviews the models' outcome and a\ncustomizable tabular view that reveals feature discrimination. We demonstrate\ncurrent applications of the framework on the classification and regression\ntasks and discuss other potential machine learning use scenarios where Manifold\ncan be applied.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:04:08 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Zhang", "Jiawei", ""], ["Wang", "Yang", ""], ["Molino", "Piero", ""], ["Li", "Lezhi", ""], ["Ebert", "David S.", ""]]}, {"id": "1808.00197", "submitter": "Jerome Darmont", "authors": "Ayb\\\"uk\\\"e Ozt\\\"urk (ERIC, ArAr), St\\'ephane Lallich (ERIC),\n  J\\'er\\^ome Darmont (ERIC), Sylvie Yona Waksman (ArAr)", "title": "MaxMin Linear Initialization for Fuzzy C-Means", "comments": null, "journal-ref": "IBaI. 14th International Conference on Machine Learning and Data\n  Mining (MLDM 2018), Jul 2018, New York, United States. Springer, Lecture\n  Notes in Artificial Intelligence, 10934-10935, 2018, Machine Learning and\n  Data Mining in Pattern Recognition. http://www.mldm.de", "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an extensive research area in data science. The aim of\nclustering is to discover groups and to identify interesting patterns in\ndatasets. Crisp (hard) clustering considers that each data point belongs to one\nand only one cluster. However, it is inadequate as some data points may belong\nto several clusters, as is the case in text categorization. Thus, we need more\nflexible clustering. Fuzzy clustering methods, where each data point can belong\nto several clusters, are an interesting alternative. Yet, seeding iterative\nfuzzy algorithms to achieve high quality clustering is an issue. In this paper,\nwe propose a new linear and efficient initialization algorithm MaxMin Linear to\ndeal with this problem. Then, we validate our theoretical results through\nextensive experiments on a variety of numerical real-world and artificial\ndatasets. We also test several validity indices, including a new validity index\nthat we propose, Transformed Standardized Fuzzy Difference (TSFD).\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:07:15 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Ozt\u00fcrk", "Ayb\u00fck\u00eb", "", "ERIC, ArAr"], ["Lallich", "St\u00e9phane", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Waksman", "Sylvie Yona", "", "ArAr"]]}, {"id": "1808.00198", "submitter": "Moa Johansson", "authors": "Agrin Hilmkil, Oscar Ivarsson, Moa Johansson, Dan Kuylenstierna, Teun\n  van Erp", "title": "Towards Machine Learning on data from Professional Cyclists", "comments": "Accepted for the 12th World Congress on Performance Analysis of\n  Sports, Opatija, Croatia, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Professional sports are developing towards increasingly scientific training\nmethods with increasing amounts of data being collected from laboratory tests,\ntraining sessions and competitions. In cycling, it is standard to equip\nbicycles with small computers recording data from sensors such as power-meters,\nin addition to heart-rate, speed, altitude etc. Recently, machine learning\ntechniques have provided huge success in a wide variety of areas where large\namounts of data (big data) is available. In this paper, we perform a pilot\nexperiment on machine learning to model physical response in elite cyclists. As\na first experiment, we show that it is possible to train a LSTM machine\nlearning algorithm to predict the heart-rate response of a cyclist during a\ntraining session. This work is a promising first step towards developing more\nelaborate models based on big data and machine learning to capture performance\naspects of athletes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:13:16 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Hilmkil", "Agrin", ""], ["Ivarsson", "Oscar", ""], ["Johansson", "Moa", ""], ["Kuylenstierna", "Dan", ""], ["van Erp", "Teun", ""]]}, {"id": "1808.00200", "submitter": "Chu Wang", "authors": "Chu Wang, Yan-Ming Zhang, Cheng-Lin Liu", "title": "Anomaly Detection via Minimum Likelihood Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection aims to detect abnormal events by a model of normality. It\nplays an important role in many domains such as network intrusion detection,\ncriminal activity identity and so on. With the rapidly growing size of\naccessible training data and high computation capacities, deep learning based\nanomaly detection has become more and more popular. In this paper, a new\ndomain-based anomaly detection method based on generative adversarial networks\n(GAN) is proposed. Minimum likelihood regularization is proposed to make the\ngenerator produce more anomalies and prevent it from converging to normal data\ndistribution. Proper ensemble of anomaly scores is shown to improve the\nstability of discriminator effectively. The proposed method has achieved\nsignificant improvement than other anomaly detection methods on Cifar10 and UCI\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:14:57 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Wang", "Chu", ""], ["Zhang", "Yan-Ming", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1808.00209", "submitter": "Mir Khan", "authors": "Mir Khan, Heikki Huttunen, Jani Boutellier", "title": "Binarized Convolutional Neural Networks for Efficient Inference on GPUs", "comments": "IEEE EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have recently achieved significant\nbreakthroughs in various image classification tasks. However, they are\ncomputationally expensive,which can make their feasible mplementation on\nembedded and low-power devices difficult. In this paper convolutional neural\nnetwork binarization is implemented on GPU-based platforms for real-time\ninference on resource constrained devices. In binarized networks, all weights\nand intermediate computations between layers are quantized to +1 and -1,\nallowing multiplications and additions to be replaced with bit-wise operations\nbetween 32-bit words. This representation completely eliminates the need for\nfloating point multiplications and additions and decreases both the\ncomputational load and the memory footprint compared to a full-precision\nnetwork implemented in floating point, making it well-suited for\nresource-constrained environments. We compare the performance of our\nimplementation with an equivalent floating point implementation on one desktop\nand two embedded GPU platforms. Our implementation achieves a maximum speed up\nof 7. 4X with only 4.4% loss in accuracy compared to a reference\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:48:26 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Khan", "Mir", ""], ["Huttunen", "Heikki", ""], ["Boutellier", "Jani", ""]]}, {"id": "1808.00232", "submitter": "Zhaoran Wang", "authors": "Yuan Xie, Boyi Liu, Qiang Liu, Zhaoran Wang, Yuan Zhou and Jian Peng", "title": "Off-Policy Evaluation and Learning from Logged Bandit Feedback: Error\n  Reduction via Surrogate Policy", "comments": "27 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning from a batch of logged bandit feedback, the discrepancy between\nthe policy to be learned and the off-policy training data imposes statistical\nand computational challenges. Unlike classical supervised learning and online\nlearning settings, in batch contextual bandit learning, one only has access to\na collection of logged feedback from the actions taken by a historical policy,\nand expect to learn a policy that takes good actions in possibly unseen\ncontexts. Such a batch learning setting is ubiquitous in online and interactive\nsystems, such as ad platforms and recommendation systems. Existing approaches\nbased on inverse propensity weights, such as Inverse Propensity Scoring (IPS)\nand Policy Optimizer for Exponential Models (POEM), enjoy unbiasedness but\noften suffer from large mean squared error. In this work, we introduce a new\napproach named Maximum Likelihood Inverse Propensity Scoring (MLIPS) for batch\nlearning from logged bandit feedback. Instead of using the given historical\npolicy as the proposal in inverse propensity weights, we estimate a maximum\nlikelihood surrogate policy based on the logged action-context pairs, and then\nuse this surrogate policy as the proposal. We prove that MLIPS is\nasymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared\nerror than IPS. Such an error reduction phenomenon is somewhat surprising as\nthe estimated surrogate policy is less accurate than the given historical\npolicy. Results on multi-label classification problems and a large- scale ad\nplacement dataset demonstrate the empirical effectiveness of MLIPS.\nFurthermore, the proposed surrogate policy technique is complementary to\nexisting error reduction techniques, and when combined, is able to consistently\nboost the performance of several widely used approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 08:56:32 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Xie", "Yuan", ""], ["Liu", "Boyi", ""], ["Liu", "Qiang", ""], ["Wang", "Zhaoran", ""], ["Zhou", "Yuan", ""], ["Peng", "Jian", ""]]}, {"id": "1808.00245", "submitter": "Dmitry Rokhlin B.", "authors": "Dmitry B. Rokhlin", "title": "Robbins-Monro conditions for persistent exploration learning strategies", "comments": "9 pages, a typo in the title is corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate simple assumptions, implying the Robbins-Monro conditions for\nthe $Q$-learning algorithm with the local learning rate, depending on the\nnumber of visits of a particular state-action pair (local clock) and the number\nof iteration (global clock). It is assumed that the Markov decision process is\ncommunicating and the learning policy ensures the persistent exploration. The\nrestrictions are imposed on the functional dependence of the learning rate on\nthe local and global clocks. The result partially confirms the conjecture of\nBradkte (1994).\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 09:43:50 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 10:06:20 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 18:56:13 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Rokhlin", "Dmitry B.", ""]]}, {"id": "1808.00260", "submitter": "Andreas Kamilaris", "authors": "Andreas Kamilaris", "title": "A Review on the Application of Natural Computing in Environmental\n  Informatics", "comments": "Proc. of EnviroInfo 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural computing offers new opportunities to understand, model and analyze\nthe complexity of the physical and human-created environment. This paper\nexamines the application of natural computing in environmental informatics, by\ninvestigating related work in this research field. Various nature-inspired\ntechniques are presented, which have been employed to solve different relevant\nproblems. Advantages and disadvantages of these techniques are discussed,\ntogether with analysis of how natural computing is generally used in\nenvironmental research.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 10:53:01 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Kamilaris", "Andreas", ""]]}, {"id": "1808.00265", "submitter": "Yundong Zhang", "authors": "Yundong Zhang, Juan Carlos Niebles, Alvaro Soto", "title": "Interpretable Visual Question Answering by Visual Grounding from\n  Attention Supervision Mining", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key aspect of VQA models that are interpretable is their ability to ground\ntheir answers to relevant regions in the image. Current approaches with this\ncapability rely on supervised learning and human annotated groundings to train\nattention mechanisms inside the VQA architecture. Unfortunately, obtaining\nhuman annotations specific for visual grounding is difficult and expensive. In\nthis work, we demonstrate that we can effectively train a VQA architecture with\ngrounding supervision that can be automatically obtained from available region\ndescriptions and object annotations. We also show that our model trained with\nthis mined supervision generates visual groundings that achieve a higher\ncorrelation with respect to manually-annotated groundings, meanwhile achieving\nstate-of-the-art VQA accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 11:06:08 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Zhang", "Yundong", ""], ["Niebles", "Juan Carlos", ""], ["Soto", "Alvaro", ""]]}, {"id": "1808.00300", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Carl Doersch and Adam Santoro and Peter\n  Battaglia", "title": "Learning Visual Question Answering by Bootstrapping Hard Attention", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms in biological perception are thought to select subsets\nof perceptual information for more sophisticated processing which would be\nprohibitive to perform on all sensory inputs. In computer vision, however,\nthere has been relatively little exploration of hard attention, where some\ninformation is selectively ignored, in spite of the success of soft attention,\nwhere information is re-weighted and aggregated, but never filtered out. Here,\nwe introduce a new approach for hard attention and find it achieves very\ncompetitive performance on a recently-released visual question answering\ndatasets, equalling and in some cases surpassing similar soft attention\narchitectures while entirely ignoring some features. Even though the hard\nattention mechanism is thought to be non-differentiable, we found that the\nfeature magnitudes correlate with semantic relevance, and provide a useful\nsignal for our mechanism's attentional selection criterion. Because hard\nattention selects important features of the input information, it can also be\nmore efficient than analogous soft attention mechanisms. This is especially\nimportant for recent approaches that use non-local pairwise operations, whereby\ncomputational and memory costs are quadratic in the size of the set of\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 12:39:43 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Doersch", "Carl", ""], ["Santoro", "Adam", ""], ["Battaglia", "Peter", ""]]}, {"id": "1808.00309", "submitter": "Alma Eguizabal", "authors": "Alma Eguizabal, Peter J. Schreier and David Ram\\'irez", "title": "Model-order selection in statistical shape models", "comments": "To appear in 2018 IEEE International Workshop on Machine Learning for\n  Signal Processing, Sept.\\ 17--20, 2018, Aalborg, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical shape models enhance machine learning algorithms providing prior\ninformation about deformation. A Point Distribution Model (PDM) is a popular\nlandmark-based statistical shape model for segmentation. It requires choosing a\nmodel order, which determines how much of the variation seen in the training\ndata is accounted for by the PDM. A good choice of the model order depends on\nthe number of training samples and the noise level in the training data set.\nYet the most common approach for choosing the model order simply keeps a\npredetermined percentage of the total shape variation. In this paper, we\npresent a technique for choosing the model order based on information-theoretic\ncriteria, and we show empirical evidence that the model order chosen by this\ntechnique provides a good trade-off between over- and underfitting.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 13:13:46 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Eguizabal", "Alma", ""], ["Schreier", "Peter J.", ""], ["Ram\u00edrez", "David", ""]]}, {"id": "1808.00331", "submitter": "Jiyang Xie", "authors": "Jiyang Xie, Jiaxin Guo, Zhanyu Ma, Jing-Hao Xue, Qie Sun, Hailong Li,\n  Jun Guo", "title": "SEA: A Combined Model for Heat Demand Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heat demand prediction is a prominent research topic in the area of\nintelligent energy networks. It has been well recognized that periodicity is\none of the important characteristics of heat demand. Seasonal-trend\ndecomposition based on LOESS (STL) algorithm can analyze the periodicity of a\nheat demand series, and decompose the series into seasonal and trend\ncomponents. Then, predicting the seasonal and trend components respectively,\nand combining their predictions together as the heat demand prediction is a\npossible way to predict heat demand. In this paper, STL-ENN-ARIMA (SEA), a\ncombined model, was proposed based on the combination of the Elman neural\nnetwork (ENN) and the autoregressive integrated moving average (ARIMA) model,\nwhich are commonly applied to heat demand prediction. ENN and ARIMA are used to\npredict seasonal and trend components, respectively. Experimental results\ndemonstrate that the proposed SEA model has a promising performance.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 06:26:06 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Xie", "Jiyang", ""], ["Guo", "Jiaxin", ""], ["Ma", "Zhanyu", ""], ["Xue", "Jing-Hao", ""], ["Sun", "Qie", ""], ["Li", "Hailong", ""], ["Guo", "Jun", ""]]}, {"id": "1808.00337", "submitter": "Miklas S. Kristoffersen", "authors": "Miklas S. Kristoffersen, Sven E. Shepstone, Zheng-Hua Tan", "title": "The Importance of Context When Recommending TV Content: Dataset and\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2019.2944214", "report-no": null, "categories": "cs.IR cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Home entertainment systems feature in a variety of usage scenarios with one\nor more simultaneous users, for whom the complexity of choosing media to\nconsume has increased rapidly over the last decade. Users' decision processes\nare complex and highly influenced by contextual settings, but data supporting\nthe development and evaluation of context-aware recommender systems are scarce.\nIn this paper we present a dataset of self-reported TV consumption enriched\nwith contextual information of viewing situations. We show how choice of genre\nassociates with, among others, the number of present users and users' attention\nlevels. Furthermore, we evaluate the performance of predicting chosen genres\ngiven different configurations of contextual information, and compare the\nresults to contextless predictions. The results suggest that including\ncontextual features in the prediction cause notable improvements, and both\ntemporal and social context show significant contributions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 11:17:43 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 10:44:34 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kristoffersen", "Miklas S.", ""], ["Shepstone", "Sven E.", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "1808.00361", "submitter": "Jonathan Connell", "authors": "Jonathan Connell, Benjamin Herta", "title": "Structured Differential Learning for Automatic Threshold Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": "IBM Research Report RC25144", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a technique that can automatically tune the parameters of a\nrule-based computer vision system comprised of thresholds, combinational logic,\nand time constants. This lets us retain the flexibility and perspicacity of a\nconventionally structured system while allowing us to perform approximate\ngradient descent using labeled data. While this is only a heuristic procedure,\nas far as we are aware there is no other efficient technique for tuning such\nsystems. We describe the components of the system and the associated supervised\nlearning mechanism. We also demonstrate the utility of the algorithm by\ncomparing its performance versus hand tuning for an automotive headlight\ncontroller. Despite having over 100 parameters, the method is able to\nprofitably adjust the system values given just the desired output for a number\nof videos.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:13:28 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Connell", "Jonathan", ""], ["Herta", "Benjamin", ""]]}, {"id": "1808.00380", "submitter": "Anant Raj", "authors": "Anant Raj, Ho Chung Leon Law, Dino Sejdinovic and Mijung Park", "title": "A Differentially Private Kernel Two-Sample Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel two-sample testing is a useful statistical tool in determining whether\ndata samples arise from different distributions without imposing any parametric\nassumptions on those distributions. However, raw data samples can expose\nsensitive information about individuals who participate in scientific studies,\nwhich makes the current tests vulnerable to privacy breaches. Hence, we design\na new framework for kernel two-sample testing conforming to differential\nprivacy constraints, in order to guarantee the privacy of subjects in the data.\nUnlike existing differentially private parametric tests that simply add noise\nto data, kernel-based testing imposes a challenge due to a complex dependence\nof test statistics on the raw data, as these statistics correspond to\nestimators of distances between representations of probability measures in\nHilbert spaces. Our approach considers finite dimensional approximations to\nthose representations. As a result, a simple chi-squared test is obtained,\nwhere a test statistic depends on a mean and covariance of empirical\ndifferences between the samples, which we perturb for a privacy guarantee. We\ninvestigate the utility of our framework in two realistic settings and conclude\nthat our method requires only a relatively modest increase in sample size to\nachieve a similar level of power to the non-private tests in both settings.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:38:08 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Raj", "Anant", ""], ["Law", "Ho Chung Leon", ""], ["Sejdinovic", "Dino", ""], ["Park", "Mijung", ""]]}, {"id": "1808.00387", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang, Alexander Rakhlin", "title": "Just Interpolate: Kernel \"Ridgeless\" Regression Can Generalize", "comments": "28 pages, 8 figures", "journal-ref": "The Annals of Statistics 48 (2020) 1329-1347", "doi": "10.1214/19-AOS1849", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the absence of explicit regularization, Kernel \"Ridgeless\" Regression with\nnonlinear kernels has the potential to fit the training data perfectly. It has\nbeen observed empirically, however, that such interpolated solutions can still\ngeneralize well on test data. We isolate a phenomenon of implicit\nregularization for minimum-norm interpolated solutions which is due to a\ncombination of high dimensionality of the input data, curvature of the kernel\nfunction, and favorable geometric properties of the data such as an eigenvalue\ndecay of the empirical covariance and kernel matrices. In addition to deriving\na data-dependent upper bound on the out-of-sample error, we present\nexperimental evidence suggesting that the phenomenon occurs in the MNIST\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:50:23 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 03:53:50 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1808.00408", "submitter": "Alpha Albert Lee", "authors": "Simon Becker and Yao Zhang and Alpha A. Lee", "title": "Geometry of energy landscapes and the optimizability of deep neural\n  networks", "comments": null, "journal-ref": "Phys. Rev. Lett. 124, 108301 (2020)", "doi": "10.1103/PhysRevLett.124.108301", "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are workhorse models in machine learning with multiple\nlayers of non-linear functions composed in series. Their loss function is\nhighly non-convex, yet empirically even gradient descent minimisation is\nsufficient to arrive at accurate and predictive models. It is hitherto unknown\nwhy are deep neural networks easily optimizable. We analyze the energy\nlandscape of a spin glass model of deep neural networks using random matrix\ntheory and algebraic geometry. We analytically show that the multilayered\nstructure holds the key to optimizability: Fixing the number of parameters and\nincreasing network depth, the number of stationary points in the loss function\ndecreases, minima become more clustered in parameter space, and the tradeoff\nbetween the depth and width of minima becomes less severe. Our analytical\nresults are numerically verified through comparison with neural networks\ntrained on a set of classical benchmark datasets. Our model uncovers generic\ndesign principles of machine learning models.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 16:33:20 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Becker", "Simon", ""], ["Zhang", "Yao", ""], ["Lee", "Alpha A.", ""]]}, {"id": "1808.00418", "submitter": "Fabrice Daniel", "authors": "Marc Velay and Fabrice Daniel", "title": "Stock Chart Pattern recognition with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study evaluates the performances of CNN and LSTM for recognizing common\ncharts patterns in a stock historical data. It presents two common patterns,\nthe method used to build the training set, the neural networks architectures\nand the accuracies obtained.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:00:43 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Velay", "Marc", ""], ["Daniel", "Fabrice", ""]]}, {"id": "1808.00423", "submitter": "Fabrice Daniel", "authors": "Marc Velay and Fabrice Daniel", "title": "Seq2Seq and Multi-Task Learning for joint intent and content extraction\n  for domain specific interpreters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study evaluates the performances of an LSTM network for detecting and\nextracting the intent and content of com- mands for a financial chatbot. It\npresents two techniques, sequence to sequence learning and Multi-Task Learning,\nwhich might improve on the previous task.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:04:48 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Velay", "Marc", ""], ["Daniel", "Fabrice", ""]]}, {"id": "1808.00441", "submitter": "Pere Gim\\'enez-Febrer", "authors": "Pere Gim\\'enez-Febrer, Alba Pag\\`es-Zamora, Georgios B. Giannakis", "title": "Matrix completion and extrapolation via kernel regression", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2932875", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion and extrapolation (MCEX) are dealt with here over\nreproducing kernel Hilbert spaces (RKHSs) in order to account for prior\ninformation present in the available data. Aiming at a faster and\nlow-complexity solver, the task is formulated as a kernel ridge regression. The\nresultant MCEX algorithm can also afford online implementation, while the class\nof kernel functions also encompasses several existing approaches to MC with\nprior information. Numerical tests on synthetic and real datasets show that the\nnovel approach performs faster than widespread methods such as alternating\nleast squares (ALS) or stochastic gradient descent (SGD), and that the recovery\nerror is reduced, especially when dealing with noisy data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:41:23 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 13:49:04 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Gim\u00e9nez-Febrer", "Pere", ""], ["Pag\u00e8s-Zamora", "Alba", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1808.00496", "submitter": "Ini Oguntola", "authors": "Ini Oguntola, Subby Olubeko, Christopher Sweeney", "title": "SlimNets: An Exploration of Deep Model Compression and Acceleration", "comments": "To be published in 2018 IEEE High Performance Extreme Computing\n  Conference (HPEC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved increasingly accurate results on a wide\nvariety of complex tasks. However, much of this improvement is due to the\ngrowing use and availability of computational resources (e.g use of GPUs, more\nlayers, more parameters, etc). Most state-of-the-art deep networks, despite\nperforming well, over-parameterize approximate functions and take a significant\namount of time to train. With increased focus on deploying deep neural networks\non resource constrained devices like smart phones, there has been a push to\nevaluate why these models are so resource hungry and how they can be made more\nefficient. This work evaluates and compares three distinct methods for deep\nmodel compression and acceleration: weight pruning, low rank factorization, and\nknowledge distillation. Comparisons on VGG nets trained on CIFAR10 show that\neach of the models on their own are effective, but that the true power lies in\ncombining them. We show that by combining pruning and knowledge distillation\nmethods we can create a compressed network 85 times smaller than the original,\nall while retaining 96% of the original model's accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 18:28:12 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Oguntola", "Ini", ""], ["Olubeko", "Subby", ""], ["Sweeney", "Christopher", ""]]}, {"id": "1808.00516", "submitter": "Hossein Nourkhiz Mahjoub", "authors": "Hossein Nourkhiz Mahjoub, Amin Tahmasbi-Sarvestani, Hadi Kazemi, Yaser\n  P. Fallah", "title": "A Learning-Based Framework for Two-Dimensional Vehicle Maneuver\n  Prediction over V2V Networks", "comments": null, "journal-ref": "2017 IEEE Cyber Science and Technology Congress(CyberSciTech),\n  Orlando, FL, 2017, pp. 156-163", "doi": "10.1109/DASC-PICom-DataCom-CyberSciTec.2017.39", "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situational awareness in vehicular networks could be substantially improved\nutilizing reliable trajectory prediction methods. More precise situational\nawareness, in turn, results in notably better performance of critical safety\napplications, such as Forward Collision Warning (FCW), as well as comfort\napplications like Cooperative Adaptive Cruise Control (CACC). Therefore,\nvehicle trajectory prediction problem needs to be deeply investigated in order\nto come up with an end to end framework with enough precision required by the\nsafety applications' controllers. This problem has been tackled in the\nliterature using different methods. However, machine learning, which is a\npromising and emerging field with remarkable potential for time series\nprediction, has not been explored enough for this purpose. In this paper, a\ntwo-layer neural network-based system is developed which predicts the future\nvalues of vehicle parameters, such as velocity, acceleration, and yaw rate, in\nthe first layer and then predicts the two-dimensional, i.e. longitudinal and\nlateral, trajectory points based on the first layer's outputs. The performance\nof the proposed framework has been evaluated in realistic cut-in scenarios from\nSafety Pilot Model Deployment (SPMD) dataset and the results show a noticeable\nimprovement in the prediction accuracy in comparison with the kinematics model\nwhich is the dominant employed model by the automotive industry. Both ideal and\nnonideal communication circumstances have been investigated for our system\nevaluation. For non-ideal case, an estimation step is included in the framework\nbefore the parameter prediction block to handle the drawbacks of packet drops\nor sensor failures and reconstruct the time series of vehicle parameters at a\ndesirable frequency.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 19:12:22 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Mahjoub", "Hossein Nourkhiz", ""], ["Tahmasbi-Sarvestani", "Amin", ""], ["Kazemi", "Hadi", ""], ["Fallah", "Yaser P.", ""]]}, {"id": "1808.00523", "submitter": "Zachariah Carmichael", "authors": "Zachariah Carmichael, Humza Syed, Stuart Burtner, Dhireesha\n  Kudithipudi", "title": "Mod-DeepESN: Modular Deep Echo State Network", "comments": "4 pages, Cognitive Computational Neuroscience (CCN) 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuro-inspired recurrent neural network algorithms, such as echo state\nnetworks, are computationally lightweight and thereby map well onto untethered\ndevices. The baseline echo state network algorithms are shown to be efficient\nin solving small-scale spatio-temporal problems. However, they underperform for\ncomplex tasks that are characterized by multi-scale structures. In this\nresearch, an intrinsic plasticity-infused modular deep echo state network\narchitecture is proposed to solve complex and multiple timescale temporal\ntasks. It outperforms state-of-the-art for time series prediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 19:28:03 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 20:37:31 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Carmichael", "Zachariah", ""], ["Syed", "Humza", ""], ["Burtner", "Stuart", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1808.00525", "submitter": "Jinseok Kim", "authors": "Jinseok Kim and Jenna Kim", "title": "The impact of imbalanced training data on machine learning for author\n  name disambiguation", "comments": "17 pages, 3 figures, and 3 tables", "journal-ref": "Kim, J. & Kim, J. (2018). The impact of imbalanced training data\n  on machine learning for author name disambiguation. Scientometrics", "doi": "10.1007/s11192-018-2865-9", "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised machine learning for author name disambiguation, negative\ntraining data are often dominantly larger than positive training data. This\npaper examines how the ratios of negative to positive training data can affect\nthe performance of machine learning algorithms to disambiguate author names in\nbibliographic records. On multiple labeled datasets, three classifiers -\nLogistic Regression, Na\\\"ive Bayes, and Random Forest - are trained through\nrepresentative features such as coauthor names, and title words extracted from\nthe same training data but with various positive-negative training data ratios.\nResults show that increasing negative training data can improve disambiguation\nperformance but with a few percent of performance gains and sometimes degrade\nit. Logistic Regression and Na\\\"ive Bayes learn optimal disambiguation models\neven with a base ratio (1:1) of positive and negative training data. Also, the\nperformance improvement by Random Forest tends to quickly saturate roughly\nafter 1:10 ~ 1:15. These findings imply that contrary to the common practice\nusing all training data, name disambiguation algorithms can be trained using\npart of negative training data without degrading much disambiguation\nperformance while increasing computational efficiency. This study calls for\nmore attention from author name disambiguation scholars to methods for machine\nlearning from imbalanced data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 14:29:27 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 02:59:12 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Kim", "Jinseok", ""], ["Kim", "Jenna", ""]]}, {"id": "1808.00527", "submitter": "Carlos Sarraute", "authors": "Jorge Brea, Javier Burroni, Carlos Sarraute", "title": "Inference of Users Demographic Attributes based on Homophily in\n  Communication Networks", "comments": "Published in Fourth Conference on the Scientific Analysis of Mobile\n  Phone Datasets (NetMob), MIT Media Lab, Cambridge, USA, 8 April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over the past decade, mobile phones have become prevalent in all parts of the\nworld, across all demographic backgrounds. Mobile phones are used by men and\nwomen across a wide age range in both developed and developing countries.\nConsequently, they have become one of the most important mechanisms for social\ninteraction within a population, making them an increasingly important source\nof information to understand human demographics and human behaviour.\n  In this work we combine two sources of information: communication logs from a\nmajor mobile operator in a Latin American country, and information on the\ndemographics of a subset of the users population. This allows us to perform an\nobservational study of mobile phone usage, differentiated by age groups\ncategories. This study is interesting in its own right, since it provides\nknowledge on the structure and demographics of the mobile phone market in the\nstudied country.\n  We then tackle the problem of inferring the age group for all users in the\nnetwork. We present here an exclusively graph-based inference method relying\nsolely on the topological structure of the mobile network, together with a\ntopological analysis of the performance of the algorithm. The equations for our\nalgorithm can be described as a diffusion process with two added properties:\n(i) memory of its initial state, and (ii) the information is propagated as a\nprobability vector for each node attribute (instead of the value of the\nattribute itself). Our algorithm can successfully infer different age groups\nwithin the network population given known values for a subset of nodes (seed\nnodes). Most interestingly, we show that by carefully analysing the topological\nrelationships between correctly predicted nodes and the seed nodes, we can\ncharacterize particular subsets of nodes for which our inference method has\nsignificantly higher accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 19:32:31 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Brea", "Jorge", ""], ["Burroni", "Javier", ""], ["Sarraute", "Carlos", ""]]}, {"id": "1808.00529", "submitter": "Si Liu", "authors": "Si Liu, Risheek Garrepalli, Thomas G. Dietterich, Alan Fern, Dan\n  Hendrycks", "title": "Open Category Detection with PAC Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Open category detection is the problem of detecting \"alien\" test instances\nthat belong to categories or classes that were not present in the training\ndata. In many applications, reliably detecting such aliens is central to\nensuring the safety and accuracy of test set predictions. Unfortunately, there\nare no algorithms that provide theoretical guarantees on their ability to\ndetect aliens under general assumptions. Further, while there are algorithms\nfor open category detection, there are few empirical results that directly\nreport alien detection rates. Thus, there are significant theoretical and\nempirical gaps in our understanding of open category detection. In this paper,\nwe take a step toward addressing this gap by studying a simple, but\npractically-relevant variant of open category detection. In our setting, we are\nprovided with a \"clean\" training set that contains only the target categories\nof interest and an unlabeled \"contaminated\" training set that contains a\nfraction $\\alpha$ of alien examples. Under the assumption that we know an upper\nbound on $\\alpha$, we develop an algorithm with PAC-style guarantees on the\nalien detection rate, while aiming to minimize false alarms. Empirical results\non synthetic and standard benchmark datasets demonstrate the regimes in which\nthe algorithm can be effective and provide a baseline for further advancements.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 19:41:04 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Liu", "Si", ""], ["Garrepalli", "Risheek", ""], ["Dietterich", "Thomas G.", ""], ["Fern", "Alan", ""], ["Hendrycks", "Dan", ""]]}, {"id": "1808.00560", "submitter": "Kai Chen", "authors": "Kai Chen, Perry Groot, Jinsong Chen, and Elena Marchiori", "title": "Spectral Mixture Kernels with Time and Phase Delay Dependencies", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral Mixture (SM) kernels form a powerful class of kernels for Gaussian\nprocesses, capable to discover patterns, extrapolate, and model negative\ncovariances. Being a linear superposition of quasi-periodical Gaussian\ncomponents, an SM kernel does not explicitly model dependencies between\ncomponents. In this paper we investigate the benefits of modeling explicitly\ntime and phase delay dependencies between components in an AM kernel. We\nanalyze the presence of statistical dependencies between components using\nGaussian conditionals and posterior covariance and use this framework to\nmotivate the proposed SM kernel extension, called Spectral Mixture kernel with\ntime and phase delay Dependencies (SMD). SMD is constructed in two steps:\nfirst, time delay and phase delay are incorporated into each base component;\nnext, cross-convolution between a base component and the reversed complex\nconjugate of another base component is performed which yields a complex-valued\nand positive definite kernel representing correlations between base components.\nThe number of hyper-parameters of SMD, except the time and phase delay ones,\nremains equal to that of the SM kernel. We perform a thorough comparative\nexperimental analysis of SMD on synthetic and real-life data sets. Results\nindicate the beneficial effect of modeling time and phase delay dependencies\nbetween base components, notably for natural phenomena involving little or no\ninfluence from human activity.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 20:55:54 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2018 11:50:23 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 21:37:31 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 09:05:19 GMT"}, {"version": "v5", "created": "Sun, 14 Oct 2018 20:26:09 GMT"}, {"version": "v6", "created": "Fri, 16 Aug 2019 19:18:41 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Chen", "Kai", ""], ["Groot", "Perry", ""], ["Chen", "Jinsong", ""], ["Marchiori", "Elena", ""]]}, {"id": "1808.00563", "submitter": "Anirudh Raju", "authors": "Anirudh Raju, Sankaran Panchapagesan, Xing Liu, Arindam Mandal, Nikko\n  Strom", "title": "Data Augmentation for Robust Keyword Spotting under Playback\n  Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate on-device keyword spotting (KWS) with low false accept and false\nreject rate is crucial to customer experience for far-field voice control of\nconversational agents. It is particularly challenging to maintain low false\nreject rate in real world conditions where there is (a) ambient noise from\nexternal sources such as TV, household appliances, or other speech that is not\ndirected at the device (b) imperfect cancellation of the audio playback from\nthe device, resulting in residual echo, after being processed by the Acoustic\nEcho Cancellation (AEC) system. In this paper, we propose a data augmentation\nstrategy to improve keyword spotting performance under these challenging\nconditions. The training set audio is artificially corrupted by mixing in music\nand TV/movie audio, at different signal to interference ratios. Our results\nshow that we get around 30-45% relative reduction in false reject rates, at a\nrange of false alarm rates, under audio playback from such devices.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 21:00:50 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Raju", "Anirudh", ""], ["Panchapagesan", "Sankaran", ""], ["Liu", "Xing", ""], ["Mandal", "Arindam", ""], ["Strom", "Nikko", ""]]}, {"id": "1808.00564", "submitter": "Adam Gonczarek", "authors": "Piotr Klukowski and Adam Gonczarek", "title": "Towards fully automated protein structure elucidation with NMR\n  spectroscopy", "comments": null, "journal-ref": "3rd International Workshop on Biomedical Informatics with\n  Optimization and Machine Learning. ICML 2018", "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear magnetic resonance (NMR) spectroscopy is one of the leading\ntechniques for protein studies. The method features a number of properties,\nallowing to explain macromolecular interactions mechanistically and resolve\nstructures with atomic resolution. However, due to laborious data analysis, a\nfull potential of NMR spectroscopy remains unexploited. Here we present an\napproach aiming at automation of two major bottlenecks in the analysis\npipeline, namely, peak picking and chemical shift assignment. Our approach\ncombines deep learning, non-parametric models and combinatorial optimization,\nand is able to detect signals of interest in a multidimensional NMR data with\nhigh accuracy and match them with atoms in medium-length protein sequences,\nwhich is a preliminary step to solve protein spatial structure.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 10:38:29 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Klukowski", "Piotr", ""], ["Gonczarek", "Adam", ""]]}, {"id": "1808.00590", "submitter": "Yang Zhang", "authors": "Lucjan Hanzlik, Yang Zhang, Kathrin Grosse, Ahmed Salem, Max Augustin,\n  Michael Backes, Mario Fritz", "title": "MLCapsule: Guarded Offline Deployment of Machine Learning as a Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of machine learning (ML) techniques, ML as a service\nhas become increasingly popular. In this setting, an ML model resides on a\nserver and users can query it with their data via an API. However, if the\nuser's input is sensitive, sending it to the server is undesirable and\nsometimes even legally not possible. Equally, the service provider does not\nwant to share the model by sending it to the client for protecting its\nintellectual property and pay-per-query business model.\n  In this paper, we propose MLCapsule, a guarded offline deployment of machine\nlearning as a service. MLCapsule executes the model locally on the user's side\nand therefore the data never leaves the client. Meanwhile, MLCapsule offers the\nservice provider the same level of control and security of its model as the\ncommonly used server-side execution. In addition, MLCapsule is applicable to\noffline applications that require local execution. Beyond protecting against\ndirect model access, we couple the secure offline deployment with defenses\nagainst advanced attacks on machine learning models such as model stealing,\nreverse engineering, and membership inference.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 22:45:48 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 17:02:07 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Hanzlik", "Lucjan", ""], ["Zhang", "Yang", ""], ["Grosse", "Kathrin", ""], ["Salem", "Ahmed", ""], ["Augustin", "Max", ""], ["Backes", "Michael", ""], ["Fritz", "Mario", ""]]}, {"id": "1808.00601", "submitter": "Francesco Lomio", "authors": "Francesco Lomio, Ricardo Farinha, Mauri Laasonen, Heikki Huttunen", "title": "Classification of Building Information Model (BIM) Structures with Deep\n  Learning", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": "10.1109/EUVIP.2018.8611701", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study an application of machine learning to the construction\nindustry and we use classical and modern machine learning methods to categorize\nimages of building designs into three classes: Apartment building, Industrial\nbuilding or Other. No real images are used, but only images extracted from\nBuilding Information Model (BIM) software, as these are used by the\nconstruction industry to store building designs. For this task, we compared\nfour different methods: the first is based on classical machine learning, where\nHistogram of Oriented Gradients (HOG) was used for feature extraction and a\nSupport Vector Machine (SVM) for classification; the other three methods are\nbased on deep learning, covering common pre-trained networks as well as ones\ndesigned from scratch. To validate the accuracy of the models, a database of\n240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and\nabove 89% for the neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 23:56:28 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Lomio", "Francesco", ""], ["Farinha", "Ricardo", ""], ["Laasonen", "Mauri", ""], ["Huttunen", "Heikki", ""]]}, {"id": "1808.00616", "submitter": "Daniel L. Pimentel-Alarc\\'on", "authors": "Daniel L. Pimentel-Alarc\\'on", "title": "Mixture Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completing a data matrix X has become an ubiquitous problem in modern data\nscience, with applications in recommender systems, computer vision, and\nnetworks inference, to name a few. One typical assumption is that X is\nlow-rank. A more general model assumes that each column of X corresponds to one\nof several low-rank matrices. This paper generalizes these models to what we\ncall mixture matrix completion (MMC): the case where each entry of X\ncorresponds to one of several low-rank matrices. MMC is a more accurate model\nfor recommender systems, and brings more flexibility to other completion and\nclustering problems. We make four fundamental contributions about this new\nmodel. First, we show that MMC is theoretically possible (well-posed). Second,\nwe give its precise information-theoretic identifiability conditions. Third, we\nderive the sample complexity of MMC. Finally, we give a practical algorithm for\nMMC with performance comparable to the state-of-the-art for simpler related\nproblems, both on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 01:09:24 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Pimentel-Alarc\u00f3n", "Daniel L.", ""]]}, {"id": "1808.00628", "submitter": "Daniel L. Pimentel-Alarc\\'on", "authors": "Daniel L. Pimentel-Alarc\\'on, Usman Mahmood", "title": "Fusion Subspace Clustering: Full and Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern inference and learning often hinge on identifying low-dimensional\nstructures that approximate large scale data. Subspace clustering achieves this\nthrough a union of linear subspaces. However, in contemporary applications data\nis increasingly often incomplete, rendering standard (full-data) methods\ninapplicable. On the other hand, existing incomplete-data methods present major\ndrawbacks, like lifting an already high-dimensional problem, or requiring a\nsuper polynomial number of samples. Motivated by this, we introduce a new\nsubspace clustering algorithm inspired by fusion penalties. The main idea is to\npermanently assign each datum to a subspace of its own, and minimize the\ndistance between the subspaces of all data, so that subspaces of the same\ncluster get fused together. Our approach is entirely new to both, full and\nmissing data, and unlike other methods, it directly allows noise, it requires\nno liftings, it allows low, high, and even full-rank data, it approaches\noptimal (information-theoretic) sampling rates, and it does not rely on other\nmethods such as low-rank matrix completion to handle missing data. Furthermore,\nour extensive experiments on both real and synthetic data show that our\napproach performs comparably to the state-of-the-art with complete data, and\ndramatically better if data is missing.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 01:54:15 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Pimentel-Alarc\u00f3n", "Daniel L.", ""], ["Mahmood", "Usman", ""]]}, {"id": "1808.00629", "submitter": "Farhad Shakerin", "authors": "Farhad Shakerin, Gopal Gupta", "title": "Induction of Non-Monotonic Logic Programs to Explain Boosted Tree Models\n  Using LIME", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a heuristic based algorithm to induce \\textit{nonmonotonic} logic\nprograms that will explain the behavior of XGBoost trained classifiers. We use\nthe technique based on the LIME approach to locally select the most important\nfeatures contributing to the classification decision. Then, in order to explain\nthe model's global behavior, we propose the LIME-FOLD algorithm ---a\nheuristic-based inductive logic programming (ILP) algorithm capable of learning\nnon-monotonic logic programs---that we apply to a transformed dataset produced\nby LIME. Our proposed approach is agnostic to the choice of the ILP algorithm.\nOur experiments with UCI standard benchmarks suggest a significant improvement\nin terms of classification evaluation metrics. Meanwhile, the number of induced\nrules dramatically decreases compared to ALEPH, a state-of-the-art ILP system.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 01:54:26 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 18:30:40 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Shakerin", "Farhad", ""], ["Gupta", "Gopal", ""]]}, {"id": "1808.00667", "submitter": "Ekram Hossain", "authors": "K. I. Ahmed, H. Tabassum, and E. Hossain", "title": "Deep Learning for Radio Resource Allocation in Multi-Cell Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased complexity and heterogeneity of emerging 5G and beyond 5G (B5G)\nwireless networks will require a paradigm shift from traditional resource\nallocation mechanisms. Deep learning (DL) is a powerful tool where a\nmulti-layer neural network can be trained to model a resource management\nalgorithm using network data.Therefore, resource allocation decisions can be\nobtained without intensive online computations which would be required\notherwise for the solution of resource allocation problems. In this context,\nthis article focuses on the application of DL to obtain solutions for the radio\nresource allocation problems in multi-cell networks. Starting with a brief\noverview of a deep neural network (DNN) as a DL model, relevant DNN\narchitectures and the data training procedure, we provide an overview of\nexisting state-of-the-art applying DL in the context of radio resource\nallocation. A qualitative comparison is provided in terms of their objectives,\ninputs/outputs, learning and data training methods. Then, we present a\nsupervised DL model to solve the sub-band and power allocation problem in a\nmulti-cell network. Using the data generated by a genetic algorithm, we first\ntrain the model and then test the accuracy of the proposed model in predicting\nthe resource allocation solutions. Simulation results show that the trained DL\nmodel is able to provide the desired optimal solution 86.3% of time.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 04:55:25 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Ahmed", "K. I.", ""], ["Tabassum", "H.", ""], ["Hossain", "E.", ""]]}, {"id": "1808.00668", "submitter": "Takuya Isomura", "authors": "Takuya Isomura, Taro Toyoizumi", "title": "On the achievability of blind source separation for high-dimensional\n  nonlinear source mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many years, a combination of principal component analysis (PCA) and\nindependent component analysis (ICA) has been used for blind source separation\n(BSS). However, it remains unclear why these linear methods work well with\nreal-world data that involve nonlinear source mixtures. This work theoretically\nvalidates that a cascade of linear PCA and ICA can solve a nonlinear BSS\nproblem accurately -- when the sensory inputs are generated from hidden sources\nvia nonlinear mappings with sufficient dimensionality. Our proposed theorem,\ntermed the asymptotic linearization theorem, theoretically guarantees that\napplying linear PCA to the inputs can reliably extract a subspace spanned by\nthe linear projections from every hidden source as the major components -- and\nthus projecting the inputs onto their major eigenspace can effectively recover\na linear transformation of the hidden sources. Then, subsequent application of\nlinear ICA can separate all the true independent hidden sources accurately.\nZero-element-wise-error nonlinear BSS is asymptotically attained when the\nsource dimensionality is large and the input dimensionality is sufficiently\nlarger than the source dimensionality. Our proposed theorem is validated\nanalytically and numerically. Moreover, the same computation can be performed\nby using Hebbian-like plasticity rules, implying the biological plausibility of\nthis nonlinear BSS strategy. Our results highlight the utility of linear PCA\nand ICA for accurately and reliably recovering nonlinearly mixed sources -- and\nfurther suggest the importance of employing sensors with sufficient\ndimensionality to identify true hidden sources of real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 04:58:49 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 04:51:06 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 11:12:16 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Isomura", "Takuya", ""], ["Toyoizumi", "Taro", ""]]}, {"id": "1808.00720", "submitter": "Stephen Bonner", "authors": "David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, Alexandros\n  Karatzoglou", "title": "RecoGym: A Reinforcement Learning Environment for the problem of Product\n  Recommendation in Online Advertising", "comments": "Accepted at the REVEAL workshop at the Twelfth ACM Conference on\n  Recommender Systems (RecSys '18), October 2--7, 2018, Vancouver, BC, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems are becoming ubiquitous in many settings and take many\nforms, from product recommendation in e-commerce stores, to query suggestions\nin search engines, to friend recommendation in social networks. Current\nresearch directions which are largely based upon supervised learning from\nhistorical data appear to be showing diminishing returns with a lot of\npractitioners report a discrepancy between improvements in offline metrics for\nsupervised learning and the online performance of the newly proposed models.\nOne possible reason is that we are using the wrong paradigm: when looking at\nthe long-term cycle of collecting historical performance data, creating a new\nversion of the recommendation model, A/B testing it and then rolling it out. We\nsee that there a lot of commonalities with the reinforcement learning (RL)\nsetup, where the agent observes the environment and acts upon it in order to\nchange its state towards better states (states with higher rewards). To this\nend we introduce RecoGym, an RL environment for recommendation, which is\ndefined by a model of user traffic patterns on e-commerce and the users\nresponse to recommendations on the publisher websites. We believe that this is\nan important step forward for the field of recommendation systems research,\nthat could open up an avenue of collaboration between the recommender systems\nand reinforcement learning communities and lead to better alignment between\noffline and online performance metrics.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 09:13:18 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 11:58:09 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Rohde", "David", ""], ["Bonner", "Stephen", ""], ["Dunlop", "Travis", ""], ["Vasile", "Flavian", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1808.00741", "submitter": "Vladimir V'yugin", "authors": "Vladimir V'yugin and Vladimir Trunov", "title": "Online Aggregation of Unbounded Losses Using Shifting Experts with\n  Confidence", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-018-5751-z", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the setting of sequential prediction based on shifting experts and\non a \"smooth\" version of the method of specialized experts. To aggregate\nexperts predictions, we use the AdaHedge algorithm, which is a version of the\nHedge algorithm with adaptive learning rate, and extend it by the\nmeta-algorithm Fixed Share. Due to this, we combine the advantages of both\nalgorithms: (1) we use the shifting regret which is a more optimal\ncharacteristic of the algorithm; (2) regret bounds are valid in the case of\nsigned unbounded losses of the experts. Also, (3) we incorporate in this scheme\na \"smooth\" version of the method of specialized experts which allows us to make\nmore flexible and accurate predictions. All results are obtained in the\nadversarial setting -- no assumptions are made about the nature of data source.\nWe present results of numerical experiments for short-term forecasting of\nelectricity consumption based on a real data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 09:57:29 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 15:02:36 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 12:03:37 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["V'yugin", "Vladimir", ""], ["Trunov", "Vladimir", ""]]}, {"id": "1808.00758", "submitter": "Bo Yang", "authors": "Bo Yang, Sen Wang, Andrew Markham, Niki Trigoni", "title": "Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D\n  Reconstruction", "comments": "IJCV 2019. Code and data are available at\n  https://github.com/Yang7879/AttSets", "journal-ref": null, "doi": "10.1007/s11263-019-01217-w", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering an underlying 3D shape from a set of\nimages. Existing learning based approaches usually resort to recurrent neural\nnets, e.g., GRU, or intuitive pooling operations, e.g., max/mean poolings, to\nfuse multiple deep features encoded from input images. However, GRU based\napproaches are unable to consistently estimate 3D shapes given different\npermutations of the same set of input images as the recurrent unit is\npermutation variant. It is also unlikely to refine the 3D shape given more\nimages due to the long-term memory loss of GRU. Commonly used pooling\napproaches are limited to capturing partial information, e.g., max/mean values,\nignoring other valuable features. In this paper, we present a new feed-forward\nneural module, named AttSets, together with a dedicated training algorithm,\nnamed FASet, to attentively aggregate an arbitrarily sized deep feature set for\nmulti-view 3D reconstruction. The AttSets module is permutation invariant,\ncomputationally efficient and flexible to implement, while the FASet algorithm\nenables the AttSets based network to be remarkably robust and generalize to an\narbitrary number of input images. We thoroughly evaluate FASet and the\nproperties of AttSets on multiple large public datasets. Extensive experiments\nshow that AttSets together with FASet algorithm significantly outperforms\nexisting aggregation approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 11:09:13 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 06:32:40 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yang", "Bo", ""], ["Wang", "Sen", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1808.00783", "submitter": "Peter M. Roth", "authors": "Mina Basirat and Peter M. Roth", "title": "The Quest for the Golden Activation Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have been shown to be beneficial for a variety of tasks,\nin particular allowing for end-to-end learning and reducing the requirement for\nmanual design decisions. However, still many parameters have to be chosen in\nadvance, also raising the need to optimize them. One important, but often\nignored system parameter is the selection of a proper activation function.\nThus, in this paper we target to demonstrate the importance of activation\nfunctions in general and show that for different tasks different activation\nfunctions might be meaningful. To avoid the manual design or selection of\nactivation functions, we build on the idea of genetic algorithms to learn the\nbest activation function for a given task. In addition, we introduce two new\nactivation functions, ELiSH and HardELiSH, which can easily be incorporated in\nour framework. In this way, we demonstrate for three different image\nclassification benchmarks that different activation functions are learned, also\nshowing improved results compared to typically used baselines.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 12:44:09 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Basirat", "Mina", ""], ["Roth", "Peter M.", ""]]}, {"id": "1808.00803", "submitter": "Zhanyu Ma", "authors": "Jiyang Xie, Zeyu Song, Yupeng Li, and Zhanyu Ma", "title": "Mobile big data analysis with machine learning", "comments": "Version 0.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates to identify the requirement and the development of\nmachine learning-based mobile big data analysis through discussing the insights\nof challenges in the mobile big data (MBD). Furthermore, it reviews the\nstate-of-the-art applications of data analysis in the area of MBD. Firstly, we\nintroduce the development of MBD. Secondly, the frequently adopted methods of\ndata analysis are reviewed. Three typical applications of MBD analysis, namely\nwireless channel modeling, human online and offline behavior analysis, and\nspeech recognition in the internet of vehicles, are introduced respectively.\nFinally, we summarize the main challenges and future development directions of\nmobile big data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 13:31:23 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 09:38:45 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Xie", "Jiyang", ""], ["Song", "Zeyu", ""], ["Li", "Yupeng", ""], ["Ma", "Zhanyu", ""]]}, {"id": "1808.00814", "submitter": "Zhanyu Ma", "authors": "Zhanyu Ma", "title": "Classification of EEG Signal based on non-Gaussian Neutral Vector", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the design of brain-computer interface systems, classification of\nElectroencephalogram (EEG) signals is the essential part and a challenging\ntask. Recently, as the marginalized discrete wavelet transform (mDWT)\nrepresentations can reveal features related to the transient nature of the EEG\nsignals, the mDWT coefficients have been frequently used in EEG signal\nclassification. In our previous work, we have proposed a super-Dirichlet\ndistribution-based classifier, which utilized the nonnegative and sum-to-one\nproperties of the mDWT coefficients. The proposed classifier performed better\nthan the state-of-the-art support vector machine-based classifier. In this\npaper, we further study the neutrality of the mDWT coefficients. Assuming the\nmDWT vector coefficients to be a neutral vector, we transform them non-linearly\ninto a set of independent scalar coefficients. Feature selection strategy is\nproposed on the transformed feature domain. Experimental results show that the\nfeature selection strategy helps improving the classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 13:48:48 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 09:35:12 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Ma", "Zhanyu", ""]]}, {"id": "1808.00818", "submitter": "Zhanyu Ma", "authors": "Zhanyu Ma", "title": "Dirichlet Mixture Model based VQ Performance Prediction for Line\n  Spectral Frequency", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we continue our previous work on the Dirichlet mixture model\n(DMM)-based VQ to derive the performance bound of the LSF VQ. The LSF\nparameters are transformed into the $\\Delta$LSF domain and the underlying\ndistribution of the $\\Delta$LSF parameters are modelled by a DMM with finite\nnumber of mixture components. The quantization distortion, in terms of the mean\nsquared error (MSE), is calculated with the high rate theory. The mapping\nrelation between the perceptually motivated log spectral distortion (LSD) and\nthe MSE is empirically approximated by a polynomial. With this mapping\nfunction, the minimum required bit rate for transparent coding of the LSF is\nestimated.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 14:02:50 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 09:28:29 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Ma", "Zhanyu", ""]]}, {"id": "1808.00831", "submitter": "Christian Donner", "authors": "Christian Donner, Manfred Opper", "title": "Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes", "comments": "34 pages; 6 figures", "journal-ref": "Journal of Machine Learning Research, year 2018, volume 19,number\n  67, pages 1-34", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approximate Bayesian inference approach for estimating the\nintensity of an inhomogeneous Poisson process, where the intensity function is\nmodelled using a Gaussian process (GP) prior via a sigmoid link function.\nAugmenting the model using a latent marked Poisson process and P\\'olya--Gamma\nrandom variables we obtain a representation of the likelihood which is\nconjugate to the GP prior. We estimate the posterior using a variational\nfree--form mean field optimisation together with the framework of sparse GPs.\nFurthermore, as alternative approximation we suggest a sparse Laplace's method\nfor the posterior, for which an efficient expectation--maximisation algorithm\nis derived to find the posterior's mode. Both algorithms compare well against\nexact inference obtained by a Markov Chain Monte Carlo sampler and standard\nvariational Gauss approach solving the same model, while being one order of\nmagnitude faster. Furthermore, the performance and speed of our method is\ncompetitive with that of another recently proposed Poisson process model based\non a quadratic link function, while not being limited to GPs with squared\nexponential kernels and rectangular domains.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 14:32:58 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 07:12:55 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Donner", "Christian", ""], ["Opper", "Manfred", ""]]}, {"id": "1808.00845", "submitter": "Jiaxin Cai", "authors": "Jiaxin Cai, Xin Tang", "title": "RGB Video Based Tennis Action Recognition Using a Deep Historical Long\n  Short-Term Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition has attracted increasing attention from RGB input in\ncomputer vision partially due to potential applications on somatic simulation\nand statistics of sport such as virtual tennis game and tennis techniques and\ntactics analysis by video. Recently, deep learning based methods have achieved\npromising performance for action recognition. In this paper, we propose\nweighted Long Short-Term Memory adopted with convolutional neural network\nrepresentations for three dimensional tennis shots recognition. First, the\nlocal two-dimensional convolutional neural network spatial representations are\nextracted from each video frame individually using a pre-trained Inception\nnetwork. Then, a weighted Long Short-Term Memory decoder is introduced to take\nthe output state at time t and the historical embedding feature at time t-1 to\ngenerate feature vector using a score weighting scheme. Finally, we use the\nadopted CNN and weighted LSTM to map the original visual features into a vector\nspace to generate the spatial-temporal semantical description of visual\nsequences and classify the action video content. Experiments on the benchmark\ndemonstrate that our method using only simple raw RGB video can achieve better\nperformance than the state-of-the-art baselines for tennis shot recognition.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 14:58:51 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 14:28:06 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Cai", "Jiaxin", ""], ["Tang", "Xin", ""]]}, {"id": "1808.00876", "submitter": "Che-Wei Huang", "authors": "Che-Wei Huang and Shrikanth S. Narayanan", "title": "Normalization Before Shaking Toward Learning Symmetrically Distributed\n  Representation Without Margin in Speech Emotion Recognition", "comments": "Submission to The IEEE Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is crucial to the success of many practical deep learning\nmodels, in particular in a more often than not scenario where there are only a\nfew to a moderate number of accessible training samples. In addition to weight\ndecay, data augmentation and dropout, regularization based on multi-branch\narchitectures, such as Shake-Shake regularization, has been proven successful\nin many applications and attracted more and more attention. However, beyond\nmodel-based representation augmentation, it is unclear how Shake-Shake\nregularization helps to provide further improvement on classification tasks,\nlet alone the baffling interaction between batch normalization and shaking. In\nthis work, we present our investigation on Shake-Shake regularization, drawing\nconnections to the vicinal risk minimization principle and discriminative\nfeature learning in verification tasks. Furthermore, we identify a strong\nresemblance between batch normalized residual blocks and batch normalized\nrecurrent neural networks, where both of them share a similar convergence\nbehavior, which could be mitigated by a proper initialization of batch\nnormalization. Based on the findings, our experiments on speech emotion\nrecognition demonstrate simultaneously an improvement on the classification\naccuracy and a reduction on the generalization gap both with statistical\nsignificance.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 15:57:57 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 23:21:54 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Huang", "Che-Wei", ""], ["Narayanan", "Shrikanth S.", ""]]}, {"id": "1808.00878", "submitter": "Hazrat Ali", "authors": "Hazrat Ali, Adnan Ali Awan, Sanaullah Khan, Omer Shafique, Atiq ur\n  Rahman, Shahid Khan", "title": "Supervised classification for object identification in urban areas using\n  satellite imagery", "comments": "2018 International Conference on Computing, Mathematics and\n  Engineering Technologies (iCoMET)", "journal-ref": "H. Ali et al., 2018 International Conference on Computing,\n  Mathematics and Engineering Technologies (iCoMET), Sukkur, 2018, pp. 1-4", "doi": "10.1109/ICOMET.2018.8346383", "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a useful method to achieve classification in satellite\nimagery. The approach is based on pixel level study employing various features\nsuch as correlation, homogeneity, energy and contrast. In this study gray-scale\nimages are used for training the classification model. For supervised\nclassification, two classification techniques are employed namely the Support\nVector Machine (SVM) and the Naive Bayes. With textural features used for\ngray-scale images, Naive Bayes performs better with an overall accuracy of 76%\ncompared to 68% achieved by SVM. The computational time is evaluated while\nperforming the experiment with two different window sizes i.e., 50x50 and\n70x70. The required computational time on a single image is found to be 27\nseconds for a window size of 70x70 and 45 seconds for a window size of 50x50.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 16:00:32 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Ali", "Hazrat", ""], ["Awan", "Adnan Ali", ""], ["Khan", "Sanaullah", ""], ["Shafique", "Omer", ""], ["Rahman", "Atiq ur", ""], ["Khan", "Shahid", ""]]}, {"id": "1808.00892", "submitter": "Hirokazu Kameoka", "authors": "Hirokazu Kameoka, Li Li, Shota Inoue, Shoji Makino", "title": "Semi-blind source separation with multichannel variational autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multichannel source separation technique called the\nmultichannel variational autoencoder (MVAE) method, which uses a conditional\nVAE (CVAE) to model and estimate the power spectrograms of the sources in a\nmixture. By training the CVAE using the spectrograms of training examples with\nsource-class labels, we can use the trained decoder distribution as a universal\ngenerative model capable of generating spectrograms conditioned on a specified\nclass label. By treating the latent space variables and the class label as the\nunknown parameters of this generative model, we can develop a\nconvergence-guaranteed semi-blind source separation algorithm that consists of\niteratively estimating the power spectrograms of the underlying sources as well\nas the separation matrices. In experimental evaluations, our MVAE produced\nbetter separation performance than a baseline method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 16:30:51 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 23:04:04 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2018 07:29:03 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Kameoka", "Hirokazu", ""], ["Li", "Li", ""], ["Inoue", "Shota", ""], ["Makino", "Shoji", ""]]}, {"id": "1808.00911", "submitter": "Adrian Alan Pol", "authors": "Adrian Alan Pol, Gianluca Cerminara, Cecile Germain, Maurizio Pierini,\n  Agrima Seth", "title": "Detector monitoring with artificial neural networks at the CMS\n  experiment at the CERN Large Hadron Collider", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG physics.ins-det stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable data quality monitoring is a key asset in delivering collision data\nsuitable for physics analysis in any modern large-scale High Energy Physics\nexperiment. This paper focuses on the use of artificial neural networks for\nsupervised and semi-supervised problems related to the identification of\nanomalies in the data collected by the CMS muon detectors. We use deep neural\nnetworks to analyze LHC collision data, represented as images organized\ngeographically. We train a classifier capable of detecting the known anomalous\nbehaviors with unprecedented efficiency and explore the usage of convolutional\nautoencoders to extend anomaly detection capabilities to unforeseen failure\nmodes. A generalization of this strategy could pave the way to the automation\nof the data quality assessment process for present and future high-energy\nphysics experiments.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 12:24:43 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Pol", "Adrian Alan", ""], ["Cerminara", "Gianluca", ""], ["Germain", "Cecile", ""], ["Pierini", "Maurizio", ""], ["Seth", "Agrima", ""]]}, {"id": "1808.00924", "submitter": "Spencer M. Richards", "authors": "Spencer M. Richards, Felix Berkenkamp, Andreas Krause", "title": "The Lyapunov Neural Network: Adaptive Stability Certification for Safe\n  Learning of Dynamical Systems", "comments": "Proc. of the 2nd Conference on Robot Learning (CoRL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms have shown considerable prowess in simulation by allowing\nrobots to adapt to uncertain environments and improve their performance.\nHowever, such algorithms are rarely used in practice on safety-critical\nsystems, since the learned policy typically does not yield any safety\nguarantees. That is, the required exploration may cause physical harm to the\nrobot or its environment. In this paper, we present a method to learn accurate\nsafety certificates for nonlinear, closed-loop dynamical systems. Specifically,\nwe construct a neural network Lyapunov function and a training algorithm that\nadapts it to the shape of the largest safe region in the state space. The\nalgorithm relies only on knowledge of inputs and outputs of the dynamics,\nrather than on any specific model structure. We demonstrate our method by\nlearning the safe region of attraction for a simulated inverted pendulum.\nFurthermore, we discuss how our method can be used in safe learning algorithms\ntogether with statistical models of dynamical systems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 17:20:04 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 17:51:03 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Richards", "Spencer M.", ""], ["Berkenkamp", "Felix", ""], ["Krause", "Andreas", ""]]}, {"id": "1808.00928", "submitter": "Debidatta Dwibedi", "authors": "Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet", "title": "Learning Actionable Representations from Visual Observations", "comments": "This work is accepted in IROS 2018. Project website:\n  https://sites.google.com/view/actionablerepresentations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore a new approach for robots to teach themselves about\nthe world simply by observing it. In particular we investigate the\neffectiveness of learning task-agnostic representations for continuous control\ntasks. We extend Time-Contrastive Networks (TCN) that learn from visual\nobservations by embedding multiple frames jointly in the embedding space as\nopposed to a single frame. We show that by doing so, we are now able to encode\nboth position and velocity attributes significantly more accurately. We test\nthe usefulness of this self-supervised approach in a reinforcement learning\nsetting. We show that the representations learned by agents observing\nthemselves take random actions, or other agents perform tasks successfully, can\nenable the learning of continuous control policies using algorithms like\nProximal Policy Optimization (PPO) using only the learned embeddings as input.\nWe also demonstrate significant improvements on the real-world Pouring dataset\nwith a relative error reduction of 39.4% for motion attributes and 11.1% for\nstatic attributes compared to the single-frame baseline. Video results are\navailable at https://sites.google.com/view/actionablerepresentations .\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 17:24:54 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 16:03:59 GMT"}, {"version": "v3", "created": "Sat, 2 Feb 2019 23:09:02 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Dwibedi", "Debidatta", ""], ["Tompson", "Jonathan", ""], ["Lynch", "Corey", ""], ["Sermanet", "Pierre", ""]]}, {"id": "1808.00931", "submitter": "Mamikon Gulian", "authors": "Mamikon Gulian, Maziar Raissi, Paris Perdikaris, George Karniadakis", "title": "Machine Learning of Space-Fractional Differential Equations", "comments": "26 pages, 10 figures. In v2, a minor change to the formatting of a\n  handful of references was made in the bibliography; the main text was\n  unchanged. In v3, minor improvements were made to the exposition; more\n  details about motivation, examples, optimization, and relation to previous\n  works were given", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven discovery of \"hidden physics\" -- i.e., machine learning of\ndifferential equation models underlying observed data -- has recently been\napproached by embedding the discovery problem into a Gaussian Process\nregression of spatial data, treating and discovering unknown equation\nparameters as hyperparameters of a modified \"physics informed\" Gaussian Process\nkernel. This kernel includes the parametrized differential operators applied to\na prior covariance kernel. We extend this framework to linear space-fractional\ndifferential equations. The methodology is compatible with a wide variety of\nfractional operators in $\\mathbb{R}^d$ and stationary covariance kernels,\nincluding the Matern class, and can optimize the Matern parameter during\ntraining. We provide a user-friendly and feasible way to perform fractional\nderivatives of kernels, via a unified set of d-dimensional Fourier integral\nformulas amenable to generalized Gauss-Laguerre quadrature.\n  The implementation of fractional derivatives has several benefits. First, it\nallows for discovering fractional-order PDEs for systems characterized by heavy\ntails or anomalous diffusion, bypassing the analytical difficulty of fractional\ncalculus. Data sets exhibiting such features are of increasing prevalence in\nphysical and financial domains. Second, a single fractional-order archetype\nallows for a derivative of arbitrary order to be learned, with the order itself\nbeing a parameter in the regression. This is advantageous even when used for\ndiscovering integer-order equations; the user is not required to assume a\n\"dictionary\" of derivatives of various orders, and directly controls the\nparsimony of the models being discovered. We illustrate on several examples,\nincluding fractional-order interpolation of advection-diffusion and modeling\nrelative stock performance in the S&P 500 with alpha-stable motion via a\nfractional diffusion equation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 17:31:54 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 15:47:44 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 04:24:07 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Gulian", "Mamikon", ""], ["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George", ""]]}, {"id": "1808.00934", "submitter": "Enayat Ullah", "authors": "Enayat Ullah, Poorya Mianjy, Teodor V. Marinov, Raman Arora", "title": "Streaming Kernel PCA with $\\tilde{O}(\\sqrt{n})$ Random Features", "comments": "Advances in Neural Information Processing Systems (NIPS), 2018. 42\n  pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical and computational aspects of kernel principal\ncomponent analysis using random Fourier features and show that under mild\nassumptions, $O(\\sqrt{n} \\log n)$ features suffices to achieve\n$O(1/\\epsilon^2)$ sample complexity. Furthermore, we give a memory efficient\nstreaming algorithm based on classical Oja's algorithm that achieves this rate.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 17:41:15 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 21:22:58 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Ullah", "Enayat", ""], ["Mianjy", "Poorya", ""], ["Marinov", "Teodor V.", ""], ["Arora", "Raman", ""]]}, {"id": "1808.00935", "submitter": "Chaosheng Dong", "authors": "Chaosheng Dong, Bo Zeng", "title": "Inferring Parameters Through Inverse Multiobjective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of human's decisions that are observed, inverse optimization has\nbeen developed and utilized to infer the underlying decision making problem.\nThe majority of existing studies assumes that the decision making problem is\nwith a single objective function, and attributes data divergence to noises,\nerrors or bounded rationality, which, however, could lead to a corrupted\ninference when decisions are tradeoffs among multiple criteria. In this paper,\nwe take a data-driven approach and design a more sophisticated inverse\noptimization formulation to explicitly infer parameters of a multiobjective\ndecision making problem from noisy observations. This framework, together with\nour mathematical analyses and advanced algorithm developments, demonstrates a\nstrong capacity in estimating critical parameters, decoupling \"interpretable\"\ncomponents from noises or errors, deriving the denoised \\emph{optimal}\ndecisions, and ensuring statistical significance. In particular, for the whole\ndecision maker population, if suitable conditions hold, we will be able to\nunderstand the overall diversity and the distribution of their preferences over\nmultiple criteria, which is important when a precise inference on every single\ndecision maker is practically unnecessary or infeasible. Numerical results on a\nlarge number of experiments are reported to confirm the effectiveness of our\nunique inverse optimization model and the computational efficacy of the\ndeveloped algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 17:41:56 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Dong", "Chaosheng", ""], ["Zeng", "Bo", ""]]}, {"id": "1808.00961", "submitter": "Zhanyu Ma", "authors": "Jiyang Xie, Zhanyu Ma and Jun Guo", "title": "Impacts of Weather Conditions on District Heat System", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using artificial neural network for the prediction of heat demand has\nattracted more and more attention. Weather conditions, such as ambient\ntemperature, wind speed and direct solar irradiance, have been identified as\nkey input parameters. In order to further improve the model accuracy, it is of\ngreat importance to understand the influence of different parameters. Based on\nan Elman neural network (ENN), this paper investigates the impact of direct\nsolar irradiance and wind speed on predicting the heat demand of a district\nheating network. Results show that including wind speed can generally result in\na lower overall mean absolute percentage error (MAPE) (6.43%) than including\ndirect solar irradiance (6.47%); while including direct solar irradiance can\nachieve a lower maximum absolute deviation (71.8%) than including wind speed\n(81.53%). In addition, even though including both wind speed and direct solar\nirradiance shows the best overall performance (MAPE=6.35%).\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 13:52:01 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 09:07:20 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Xie", "Jiyang", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""]]}, {"id": "1808.00973", "submitter": "Kyle S. Cranmer", "authors": "Markus Stoye, Johann Brehmer, Gilles Louppe, Juan Pavez, and Kyle\n  Cranmer", "title": "Likelihood-free inference with an improved cross-entropy estimator", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG hep-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend recent work (Brehmer, et. al., 2018) that use neural networks as\nsurrogate models for likelihood-free inference. As in the previous work, we\nexploit the fact that the joint likelihood ratio and joint score, conditioned\non both observed and latent variables, can often be extracted from an implicit\ngenerative model or simulator to augment the training data for these surrogate\nmodels. We show how this augmented training data can be used to provide a new\ncross-entropy estimator, which provides improved sample efficiency compared to\nprevious loss functions exploiting this augmented training data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 18:00:08 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Stoye", "Markus", ""], ["Brehmer", "Johann", ""], ["Louppe", "Gilles", ""], ["Pavez", "Juan", ""], ["Cranmer", "Kyle", ""]]}, {"id": "1808.01006", "submitter": "Kilol Gupta", "authors": "Kilol Gupta, Mukund Yelahanka Raghuprasad, Pankhuri Kumar", "title": "A Hybrid Variational Autoencoder for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's day and age when almost every industry has an online presence with\nusers interacting in online marketplaces, personalized recommendations have\nbecome quite important. Traditionally, the problem of collaborative filtering\nhas been tackled using Matrix Factorization which is linear in nature. We\nextend the work of [11] on using variational autoencoders (VAEs) for\ncollaborative filtering with implicit feedback by proposing a hybrid,\nmulti-modal approach. Our approach combines movie embeddings (learned from a\nsibling VAE network) with user ratings from the Movielens 20M dataset and\napplies it to the task of movie recommendation. We empirically show how the VAE\nnetwork is empowered by incorporating movie embeddings. We also visualize movie\nand user embeddings by clustering their latent representations obtained from a\nVAE.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 06:57:11 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 23:31:19 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Gupta", "Kilol", ""], ["Raghuprasad", "Mukund Yelahanka", ""], ["Kumar", "Pankhuri", ""]]}, {"id": "1808.01026", "submitter": "Sobhan Soleymani", "authors": "Sobhan Soleymani, Ali Dabouei, Seyed Mehdi Iranmanesh, Hadi Kazemi,\n  Jeremy Dawson, and Nasser M. Nasrabadi", "title": "Prosodic-Enhanced Siamese Convolutional Neural Networks for Cross-Device\n  Text-Independent Speaker Verification", "comments": "Accepted in 9th IEEE International Conference on Biometrics: Theory,\n  Applications, and Systems (BTAS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel cross-device text-independent speaker verification\narchitecture is proposed. Majority of the state-of-the-art deep architectures\nthat are used for speaker verification tasks consider Mel-frequency cepstral\ncoefficients. In contrast, our proposed Siamese convolutional neural network\narchitecture uses Mel-frequency spectrogram coefficients to benefit from the\ndependency of the adjacent spectro-temporal features. Moreover, although\nspectro-temporal features have proved to be highly reliable in speaker\nverification models, they only represent some aspects of short-term acoustic\nlevel traits of the speaker's voice. However, the human voice consists of\nseveral linguistic levels such as acoustic, lexicon, prosody, and phonetics,\nthat can be utilized in speaker verification models. To compensate for these\ninherited shortcomings in spectro-temporal features, we propose to enhance the\nproposed Siamese convolutional neural network architecture by deploying a\nmultilayer perceptron network to incorporate the prosodic, jitter, and shimmer\nfeatures. The proposed end-to-end verification architecture performs feature\nextraction and verification simultaneously. This proposed architecture displays\nsignificant improvement over classical signal processing approaches and deep\nalgorithms for forensic cross-device speaker verification.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 19:21:59 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Soleymani", "Sobhan", ""], ["Dabouei", "Ali", ""], ["Iranmanesh", "Seyed Mehdi", ""], ["Kazemi", "Hadi", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1808.01048", "submitter": "Hanwei Wu", "authors": "Hanwei Wu and Markus Flierl", "title": "Variational Information Bottleneck on Vector Quantized Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide an information-theoretic interpretation of the\nVector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss\nfunction of the original VQ-VAE can be derived from the variational\ndeterministic information bottleneck (VDIB) principle. On the other hand, the\nVQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as\nan approximation to the variational information bottleneck(VIB) principle.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 23:30:50 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wu", "Hanwei", ""], ["Flierl", "Markus", ""]]}, {"id": "1808.01095", "submitter": "Doris Xin", "authors": "Doris Xin, Litian Ma, Jialin Liu, Stephen Macke, Shuchen Song, Aditya\n  Parameswaran", "title": "Helix: Accelerating Human-in-the-loop Machine Learning", "comments": null, "journal-ref": null, "doi": "10.14778/3229863.3236234", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data application developers and data scientists spend an inordinate amount of\ntime iterating on machine learning (ML) workflows -- by modifying the data\npre-processing, model training, and post-processing steps -- via\ntrial-and-error to achieve the desired model performance. Existing work on\naccelerating machine learning focuses on speeding up one-shot execution of\nworkflows, failing to address the incremental and dynamic nature of typical ML\ndevelopment. We propose Helix, a declarative machine learning system that\naccelerates iterative development by optimizing workflow execution end-to-end\nand across iterations. Helix minimizes the runtime per iteration via program\nanalysis and intelligent reuse of previous results, which are selectively\nmaterialized -- trading off the cost of materialization for potential future\nbenefits -- to speed up future iterations. Additionally, Helix offers a\ngraphical interface to visualize workflow DAGs and compare versions to\nfacilitate iterative development. Through two ML applications, in\nclassification and in structured prediction, attendees will experience the\nsuccinctness of Helix programming interface and the speed and ease of iterative\ndevelopment using Helix. In our evaluations, Helix achieved up to an order of\nmagnitude reduction in cumulative run time compared to state-of-the-art machine\nlearning tools.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 06:02:46 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Xin", "Doris", ""], ["Ma", "Litian", ""], ["Liu", "Jialin", ""], ["Macke", "Stephen", ""], ["Song", "Shuchen", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1808.01126", "submitter": "Gilles Kratzer", "authors": "Gilles Kratzer and Reinhard Furrer", "title": "Information-Theoretic Scoring Rules to Learn Additive Bayesian Network\n  Applied to Epidemiology", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network modelling is a well adapted approach to study messy and\nhighly correlated datasets which are very common in, e.g., systems\nepidemiology. A popular approach to learn a Bayesian network from an\nobservational datasets is to identify the maximum a posteriori network in a\nsearch-and-score approach. Many scores have been proposed both Bayesian or\nfrequentist based. In an applied perspective, a suitable approach would allow\nmultiple distributions for the data and is robust enough to run autonomously. A\npromising framework to compute scores are generalized linear models. Indeed,\nthere exists fast algorithms for estimation and many tailored solutions to\ncommon epidemiological issues. The purpose of this paper is to present an R\npackage abn that has an implementation of multiple frequentist scores and some\nrealistic simulations that show its usability and performance. It includes\nfeatures to deal efficiently with data separation and adjustment which are very\ncommon in systems epidemiology.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 09:22:46 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Kratzer", "Gilles", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1808.01128", "submitter": "Abhai Kollara Dilip", "authors": "Abhai Kollara Dilip, Kamal Raj K, Malaikannan Sankarasubbu", "title": "PHI Scrubber: A Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidentiality of patient information is an essential part of Electronic\nHealth Record System. Patient information, if exposed, can cause a serious\ndamage to the privacy of individuals receiving healthcare. Hence it is\nimportant to remove such details from physician notes. A system is proposed\nwhich consists of a deep learning model where a de-convolutional neural network\nand bi-directional LSTM-CNN is used along with regular expressions to recognize\nand eliminate the individually identifiable information. This information is\nthen removed from a medical practitioner's data which further allows the fair\nusage of such information among researchers and in clinical trials.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 09:34:20 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Dilip", "Abhai Kollara", ""], ["K", "Kamal Raj", ""], ["Sankarasubbu", "Malaikannan", ""]]}, {"id": "1808.01132", "submitter": "Kai Chen", "authors": "Kai Chen, Perry Groot, Jinsong Chen, and Elena Marchiori", "title": "Generalized Spectral Mixture Kernels for Multi-Task Gaussian Processes", "comments": "17 pages, 33 figures. arXiv admin note: text overlap with\n  arXiv:1808.02266", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Task Gaussian processes (MTGPs) have shown a significant progress both\nin expressiveness and interpretation of the relatedness between different\ntasks: from linear combinations of independent single-output Gaussian processes\n(GPs), through the direct modeling of the cross-covariances such as spectral\nmixture kernels with phase shift, to the design of multivariate covariance\nfunctions based on spectral mixture kernels which model delays among tasks in\naddition to phase differences, and which provide a parametric interpretation of\nthe relatedness across tasks. In this paper we further extend expressiveness\nand interpretability of MTGPs models and introduce a new family of kernels\ncapable to model nonlinear correlations between tasks as well as dependencies\nbetween spectral mixtures, including time and phase delay. Specifically, we use\ngeneralized convolution spectral mixture kernels for modeling dependencies at\nspectral mixture level, and coupling coregionalization for discovering task\nlevel correlations. The proposed kernels for MTGP are validated on artificial\ndata and compared with existing MTGPs methods on three real-world experiments.\nResults indicate the benefits of our more expressive representation with\nrespect to performance and interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 09:42:45 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 15:33:19 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 16:53:35 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 08:51:41 GMT"}, {"version": "v5", "created": "Sun, 14 Oct 2018 13:34:31 GMT"}, {"version": "v6", "created": "Mon, 17 Dec 2018 11:24:48 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Chen", "Kai", ""], ["Groot", "Perry", ""], ["Chen", "Jinsong", ""], ["Marchiori", "Elena", ""]]}, {"id": "1808.01145", "submitter": "Eva Garc\\'ia-Mart\\'in", "authors": "Eva Garc\\'ia-Mart\\'in, Niklas Lavesson, H{\\aa}kan Grahn, Emiliano\n  Casalicchio, Veselka Boeva", "title": "Hoeffding Trees with nmin adaptation", "comments": "Accepted at: The 5th IEEE International Conference on Data Science\n  and Advanced Analytics (DSAA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning software accounts for a significant amount of energy\nconsumed in data centers. These algorithms are usually optimized towards\npredictive performance, i.e. accuracy, and scalability. This is the case of\ndata stream mining algorithms. Although these algorithms are adaptive to the\nincoming data, they have fixed parameters from the beginning of the execution.\nWe have observed that having fixed parameters lead to unnecessary computations,\nthus making the algorithm energy inefficient. In this paper we present the nmin\nadaptation method for Hoeffding trees. This method adapts the value of the nmin\nparameter, which significantly affects the energy consumption of the algorithm.\nThe method reduces unnecessary computations and memory accesses, thus reducing\nthe energy, while the accuracy is only marginally affected. We experimentally\ncompared VFDT (Very Fast Decision Tree, the first Hoeffding tree algorithm) and\nCVFDT (Concept-adapting VFDT) with the VFDT-nmin (VFDT with nmin adaptation).\nThe results show that VFDT-nmin consumes up to 27% less energy than the\nstandard VFDT, and up to 92% less energy than CVFDT, trading off a few percent\nof accuracy in a few datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 10:24:32 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Garc\u00eda-Mart\u00edn", "Eva", ""], ["Lavesson", "Niklas", ""], ["Grahn", "H\u00e5kan", ""], ["Casalicchio", "Emiliano", ""], ["Boeva", "Veselka", ""]]}, {"id": "1808.01153", "submitter": "Konda Reddy Mopuri", "authors": "Konda Reddy Mopuri, Phani Krishna Uppala, and R. Venkatesh Babu", "title": "Ask, Acquire, and Attack: Data-free UAP Generation using Class\n  Impressions", "comments": "Accepted in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are susceptible to input specific noise, called\nadversarial perturbations. Moreover, there exist input-agnostic noise, called\nUniversal Adversarial Perturbations (UAP) that can affect inference of the\nmodels over most input samples. Given a model, there exist broadly two\napproaches to craft UAPs: (i) data-driven: that require data, and (ii)\ndata-free: that do not require data samples. Data-driven approaches require\nactual samples from the underlying data distribution and craft UAPs with high\nsuccess (fooling) rate. However, data-free approaches craft UAPs without\nutilizing any data samples and therefore result in lesser success rates. In\nthis paper, for data-free scenarios, we propose a novel approach that emulates\nthe effect of data samples with class impressions in order to craft UAPs using\ndata-driven objectives. Class impression for a given pair of category and model\nis a generic representation (in the input space) of the samples belonging to\nthat category. Further, we present a neural network based generative model that\nutilizes the acquired class impressions to learn crafting UAPs. Experimental\nevaluation demonstrates that the learned generative model, (i) readily crafts\nUAPs via simple feed-forwarding through neural network layers, and (ii)\nachieves state-of-the-art success rates for data-free scenario and closer to\nthat for data-driven setting without actually utilizing any data samples.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 11:02:26 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Uppala", "Phani Krishna", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1808.01174", "submitter": "Daniel Jakubovitz", "authors": "Daniel Jakubovitz, Raja Giryes, Miguel R. D. Rodrigues", "title": "Generalization Error in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have lately shown great performance in various fields\nsuch as computer vision, speech recognition, speech translation, and natural\nlanguage processing. However, alongside their state-of-the-art performance, it\nis still generally unclear what is the source of their generalization ability.\nThus, an important question is what makes deep neural networks able to\ngeneralize well from the training set to new data. In this article, we provide\nan overview of the existing theory and bounds for the characterization of the\ngeneralization error of deep neural networks, combining both classical and more\nrecent theoretical and empirical results.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 12:57:12 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 13:34:09 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 15:25:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Jakubovitz", "Daniel", ""], ["Giryes", "Raja", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1808.01175", "submitter": "Muhammed Tarik Altuncu", "authors": "M. Tarik Altuncu, Sophia N. Yaliraki, Mauricio Barahona", "title": "Content-driven, unsupervised clustering of news articles through\n  multiscale graph partitioning", "comments": "8 pages; 5 figures; To present at KDD 2018: Data Science, Journalism\n  & Media workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion in the amount of news and journalistic content being generated\nacross the globe, coupled with extended and instantaneous access to information\nthrough online media, makes it difficult and time-consuming to monitor news\ndevelopments and opinion formation in real time. There is an increasing need\nfor tools that can pre-process, analyse and classify raw text to extract\ninterpretable content; specifically, identifying topics and content-driven\ngroupings of articles. We present here such a methodology that brings together\npowerful vector embeddings from Natural Language Processing with tools from\nGraph Theory that exploit diffusive dynamics on graphs to reveal natural\npartitions across scales. Our framework uses a recent deep neural network text\nanalysis methodology (Doc2vec) to represent text in vector form and then\napplies a multi-scale community detection method (Markov Stability) to\npartition a similarity graph of document vectors. The method allows us to\nobtain clusters of documents with similar content, at different levels of\nresolution, in an unsupervised manner. We showcase our approach with the\nanalysis of a corpus of 9,000 news articles published by Vox Media over one\nyear. Our results show consistent groupings of documents according to content\nwithout a priori assumptions about the number or type of clusters to be found.\nThe multilevel clustering reveals a quasi-hierarchy of topics and subtopics\nwith increased intelligibility and improved topic coherence as compared to\nexternal taxonomy services and standard topic detection methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 12:57:15 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Altuncu", "M. Tarik", ""], ["Yaliraki", "Sophia N.", ""], ["Barahona", "Mauricio", ""]]}, {"id": "1808.01181", "submitter": "Jakub Mare\\v{c}ek", "authors": "Jakub Marecek and Tigran Tchrakian", "title": "Robust Spectral Filtering and Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a setting, where the output of a linear dynamical system (LDS)\nis, with an unknown but fixed probability, replaced by noise. There, we present\na robust method for the prediction of the outputs of the LDS and identification\nof the samples of noise, and prove guarantees on its statistical performance.\nOne application lies in anomaly detection: the samples of noise, unlikely to\nhave been generated by the dynamics, can be flagged to operators of the system\nfor further study.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 13:25:39 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Marecek", "Jakub", ""], ["Tchrakian", "Tigran", ""]]}, {"id": "1808.01184", "submitter": "Alexander Broad", "authors": "Alexander Broad, Ian Abraham, Todd Murphey, Brenna Argall", "title": "Structured Neural Network Dynamics for Model-based Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a structured neural network architecture that is inspired by\nlinear time-varying dynamical systems. The network is designed to mimic the\nproperties of linear dynamical systems which makes analysis and control simple.\nThe architecture facilitates the integration of learned system models with\ngradient-based model predictive control algorithms, and removes the requirement\nof computing potentially costly derivatives online. We demonstrate the efficacy\nof this modeling technique in computing autonomous control policies through\nevaluation in a variety of standard continuous control domains.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 13:36:38 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Broad", "Alexander", ""], ["Abraham", "Ian", ""], ["Murphey", "Todd", ""], ["Argall", "Brenna", ""]]}, {"id": "1808.01199", "submitter": "Harold Soh", "authors": "Vinh Vo Thanh, Harold Soh", "title": "Generation Meets Recommendation: Proposing Novel Items for Groups of\n  Users", "comments": "20 pages, 6 figures, Accepted to Recsys'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a movie studio aiming to produce a set of new movies for summer\nrelease: What types of movies it should produce? Who would the movies appeal\nto? How many movies should it make? Similar issues are encountered by a variety\nof organizations, e.g., mobile-phone manufacturers and online magazines, who\nhave to create new (non-existent) items to satisfy groups of users with\ndifferent preferences. In this paper, we present a joint problem formalization\nof these interrelated issues, and propose generative methods that address these\nquestions simultaneously. Specifically, we leverage the latent space obtained\nby training a deep generative model---the Variational Autoencoder (VAE)---via a\nloss function that incorporates both rating performance and item reconstruction\nterms. We then apply a greedy search algorithm that utilizes this learned\nlatent space to jointly obtain K plausible new items, and user groups that\nwould find the items appealing. An evaluation of our methods on a synthetic\ndataset indicates that our approach is able to generate novel items similar to\nhighly-desirable unobserved items. As case studies on real-world data, we\napplied our method on the MART abstract art and Movielens Tag Genome dataset,\nwhich resulted in promising results: small and diverse sets of novel items.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 08:26:19 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Thanh", "Vinh Vo", ""], ["Soh", "Harold", ""]]}, {"id": "1808.01204", "submitter": "Yingyu Liang", "authors": "Yuanzhi Li and Yingyu Liang", "title": "Learning Overparameterized Neural Networks via Stochastic Gradient\n  Descent on Structured Data", "comments": "NeurIPS'18 version. Appendix updated, additional experimental results\n  added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have many successful applications, while much less\ntheoretical understanding has been gained. Towards bridging this gap, we study\nthe problem of learning a two-layer overparameterized ReLU neural network for\nmulti-class classification via stochastic gradient descent (SGD) from random\ninitialization. In the overparameterized setting, when the data comes from\nmixtures of well-separated distributions, we prove that SGD learns a network\nwith a small generalization error, albeit the network has enough capacity to\nfit arbitrary labels. Furthermore, the analysis provides interesting insights\ninto several aspects of learning neural networks and can be verified based on\nempirical studies on synthetic data and on the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 14:28:12 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 16:43:28 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 17:30:17 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""]]}, {"id": "1808.01280", "submitter": "ShihChung Lo Ph.D.", "authors": "ShihChung B. Lo, Ph.D., Matthew T. Freedman, M.D., Seong K. Mun,\n  Ph.D., and Heang-Ping Chan, Ph.D", "title": "Geared Rotationally Identical and Invariant Convolutional Neural Network\n  Systems", "comments": "14 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theorems and techniques to form different types of transformationally\ninvariant processing and to produce the same output quantitatively based on\neither transformationally invariant operators or symmetric operations have\nrecently been introduced by the authors. In this study, we further propose to\ncompose a geared rotationally identical CNN system (GRI-CNN) with a small step\nangle by connecting networks of participated processes at the first flatten\nlayer. Using an ordinary CNN structure as a base, requirements for constructing\na GRI-CNN include the use of either symmetric input vector or kernels with an\nangle increment that can form a complete cycle as a \"gearwheel\". Four basic\nGRI-CNN structures were studied. Each of them can produce quantitatively\nidentical output results when a rotation angle of the input vector is evenly\ndivisible by the step angle of the gear. Our study showed when an input vector\nrotated with an angle does not match to a step angle, the GRI-CNN can also\nproduce a highly consistent result. With a design of using an ultra-fine\ngear-tooth step angle (e.g., 1 degree or 0.1 degree), all four GRI-CNN systems\ncan be constructed virtually isotropically.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 02:27:40 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 15:08:37 GMT"}, {"version": "v3", "created": "Fri, 10 Aug 2018 11:26:09 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Lo", "ShihChung B.", ""], ["D.", "Ph.", ""], ["Freedman", "Matthew T.", ""], ["D.", "M.", ""], ["Mun", "Seong K.", ""], ["D.", "Ph.", ""], ["Chan", "Heang-Ping", ""], ["D", "Ph.", ""]]}, {"id": "1808.01345", "submitter": "Daniele Ramazzotti", "authors": "Paolo Cazzaniga and Marco S. Nobile and Daniele Ramazzotti", "title": "Investigating the performance of multi-objective optimization when\n  learning Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Networks have been widely used in the last decades in many fields,\nto describe statistical dependencies among random variables. In general,\nlearning the structure of such models is a problem with considerable\ntheoretical interest that poses many challenges. On the one hand, it is a\nwell-known NP-complete problem, practically hardened by the huge search space\nof possible solutions. On the other hand, the phenomenon of I-equivalence,\ni.e., different graphical structures underpinning the same set of statistical\ndependencies, may lead to multimodal fitness landscapes further hindering\nmaximum likelihood approaches to solve the task. In particular, we exploit the\nNSGA-II multi-objective optimization procedure in order to explicitly account\nfor both the likelihood of a solution and the number of selected arcs, by\nsetting these as the two objective functions of the method. The aim of this\nwork is to investigate the behavior of NSGA-II and analyse the quality of its\nsolutions. We thus thoroughly examined the optimization results obtained on a\nwide set of simulated data, by considering both the goodness of the inferred\nsolutions in terms of the objective functions values achieved, and by comparing\nthe retrieved structures with the ground truth, i.e., the networks used to\ngenerate the target data. Our results show that NSGA-II can converge to\nsolutions characterized by better likelihood and less arcs than classic\napproaches, although paradoxically characterized in many cases by a lower\nsimilarity with the target network.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 20:22:57 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 22:05:15 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Cazzaniga", "Paolo", ""], ["Nobile", "Marco S.", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1808.01346", "submitter": "Francisco Gonzalez", "authors": "Francisco J. Gonzalez, Maciej Balajewicz", "title": "Deep convolutional recurrent autoencoders for learning low-dimensional\n  feature dynamics of fluid systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.LG physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model reduction of high-dimensional dynamical systems alleviates\ncomputational burdens faced in various tasks from design optimization to model\npredictive control. One popular model reduction approach is based on projecting\nthe governing equations onto a subspace spanned by basis functions obtained\nfrom the compression of a dataset of solution snapshots. However, this method\nis intrusive since the projection requires access to the system operators.\nFurther, some systems may require special treatment of nonlinearities to ensure\ncomputational efficiency or additional modeling to preserve stability. In this\nwork we propose a deep learning-based strategy for nonlinear model reduction\nthat is inspired by projection-based model reduction where the idea is to\nidentify some optimal low-dimensional representation and evolve it in time. Our\napproach constructs a modular model consisting of a deep convolutional\nautoencoder and a modified LSTM network. The deep convolutional autoencoder\nreturns a low-dimensional representation in terms of coordinates on some\nexpressive nonlinear data-supporting manifold. The dynamics on this manifold\nare then modeled by the modified LSTM network in a computationally efficient\nmanner. An offline unsupervised training strategy that exploits the model\nmodularity is also developed. We demonstrate our model on three illustrative\nexamples each highlighting the model's performance in prediction tasks for\nfluid systems with large parameter-variations and its stability in long-term\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 20:32:32 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 23:42:08 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Gonzalez", "Francisco J.", ""], ["Balajewicz", "Maciej", ""]]}, {"id": "1808.01355", "submitter": "Arunava Chakravarty", "authors": "Arunava Chakravarty, Jayanthi Sivswamy", "title": "A Deep Learning based Joint Segmentation and Classification Framework\n  for Glaucoma Assesment in Retinal Color Fundus Images", "comments": "8 pages, submitted to the REFUGE glaucoma segmentation grand\n  challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Computer Aided diagnostic tools can be used for the early detection\nof glaucoma to prevent irreversible vision loss. In this work, we present a\nMulti-task Convolutional Neural Network (CNN) that jointly segments the Optic\nDisc (OD), Optic Cup (OC) and predicts the presence of glaucoma in color fundus\nimages. The CNN utilizes a combination of image appearance features and\nstructural features obtained from the OD-OC segmentation to obtain a robust\nprediction. The use of fewer network parameters and the sharing of the CNN\nfeatures for multiple related tasks ensures the good generalizability of the\narchitecture, allowing it to be trained on small training sets. The\ncross-testing performance of the proposed method on an independent validation\nset acquired using a different camera and image resolution was found to be good\nwith an average dice score of 0.92 for OD, 0.84 for OC and AUC of 0.95 on the\ntask of glaucoma classification illustrating its potential as a mass screening\ntool for the early detection of glaucoma.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 09:12:37 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Chakravarty", "Arunava", ""], ["Sivswamy", "Jayanthi", ""]]}, {"id": "1808.01357", "submitter": "Mirco Planamente", "authors": "Mirco Planamente, Mohammad Reza Loghmani and Barbara Caputo", "title": "A recurrent multi-scale approach to RBG-D Object Recognition", "comments": "Master thesis extracted from the paper arXiv:1806.01673 submitted to\n  accv 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological development aims to produce generations of increasingly\nefficient robots able to perform complex tasks. This requires considerable\nefforts, from the scientific community, to find new algorithms that solve\ncomputer vision problems, such as object recognition. The diffusion of RGB-D\ncameras directed the study towards the research of new architectures able to\nexploit the RGB and Depth information. The project that is developed in this\nthesis concerns the realization of a new end-to-end architecture for the\nrecognition of RGB-D objects called RCFusion. Our method generates compact and\nhighly discriminative multi-modal features by combining complementary RGB and\ndepth information representing different levels of abstraction. We evaluate our\nmethod on standard object recognition datasets, RGB-D Object Dataset and\nJHUIT-50. The experiments performed show that our method outperforms the\nexisting approaches and establishes new state-of-the-art results for both\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 08:15:06 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 15:37:49 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 16:50:09 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Planamente", "Mirco", ""], ["Loghmani", "Mohammad Reza", ""], ["Caputo", "Barbara", ""]]}, {"id": "1808.01358", "submitter": "Hiroki Ohashi", "authors": "Hiroki Ohashi, Mohammad Al-Naser, Sheraz Ahmed, Katsuyuki Nakamura,\n  Takuto Sato, Andreas Dengel", "title": "Attributes' Importance for Zero-Shot Pose-Classification Based on\n  Wearable Sensors", "comments": "The paper was published at Sensors, an open access journal\n  (http://www.mdpi.com/1424-8220/18/8/2485). This article belongs to the\n  Special Issue Artificial Intelligence and Machine Learning in Sensors\n  Networks", "journal-ref": "Sensors 2018, 18, 2485", "doi": "10.3390/s18082485", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple yet effective method for improving the\nperformance of zero-shot learning (ZSL). ZSL classifies instances of unseen\nclasses, from which no training data is available, by utilizing the attributes\nof the classes. Conventional ZSL methods have equally dealt with all the\navailable attributes, but this sometimes causes misclassification. This is\nbecause an attribute that is effective for classifying instances of one class\nis not always effective for another class. In this case, a metric of\nclassifying the latter class can be undesirably influenced by the irrelevant\nattribute. This paper solves this problem by taking the importance of each\nattribute for each class into account when calculating the metric. In addition\nto the proposal of this new method, this paper also contributes by providing a\ndataset for pose classification based on wearable sensors, named HDPoseDS. It\ncontains 22 classes of poses performed by 10 subjects with 31 IMU sensors\nacross full body. To the best of our knowledge, it is the richest\nwearable-sensor dataset especially in terms of sensor density, and thus it is\nsuitable for studying zero-shot pose/action recognition. The presented method\nwas evaluated on HDPoseDS and outperformed relative improvement of 5.9% in\ncomparison to the best baseline method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 07:51:01 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ohashi", "Hiroki", ""], ["Al-Naser", "Mohammad", ""], ["Ahmed", "Sheraz", ""], ["Nakamura", "Katsuyuki", ""], ["Sato", "Takuto", ""], ["Dengel", "Andreas", ""]]}, {"id": "1808.01371", "submitter": "Nikolai Yakovenko", "authors": "Raul Puri, Robert Kirby, Nikolai Yakovenko and Bryan Catanzaro", "title": "Large Scale Language Modeling: Converging on 40GB of Text in Four Hours", "comments": "8 pages; To appear in High Performance Machine Learning Workshop\n  (HPML) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown how to train Convolutional Neural Networks (CNNs)\nrapidly on large image datasets, then transfer the knowledge gained from these\nmodels to a variety of tasks. Following [Radford 2017], in this work, we\ndemonstrate similar scalability and transfer for Recurrent Neural Networks\n(RNNs) for Natural Language tasks. By utilizing mixed precision arithmetic and\na 32k batch size distributed across 128 NVIDIA Tesla V100 GPUs, we are able to\ntrain a character-level 4096-dimension multiplicative LSTM (mLSTM) for\nunsupervised text reconstruction over 3 epochs of the 40 GB Amazon Reviews\ndataset in four hours. This runtime compares favorably with previous work\ntaking one month to train the same size and configuration for one epoch over\nthe same dataset. Converging large batch RNN models can be challenging. Recent\nwork has suggested scaling the learning rate as a function of batch size, but\nwe find that simply scaling the learning rate as a function of batch size leads\neither to significantly worse convergence or immediate divergence for this\nproblem. We provide a learning rate schedule that allows our model to converge\nwith a 32k batch size. Since our model converges over the Amazon Reviews\ndataset in hours, and our compute requirement of 128 Tesla V100 GPUs, while\nsubstantial, is commercially available, this work opens up large scale\nunsupervised NLP training to most commercial applications and deep learning\nresearchers. A model can be trained over most public or private text datasets\novernight.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 21:44:29 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 01:59:23 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Puri", "Raul", ""], ["Kirby", "Robert", ""], ["Yakovenko", "Nikolai", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1808.01400", "submitter": "Uri Alon", "authors": "Uri Alon, Shaked Brody, Omer Levy, Eran Yahav", "title": "code2seq: Generating Sequences from Structured Representations of Code", "comments": "Accepted to ICLR'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate natural language sequences from source code snippets\nhas a variety of applications such as code summarization, documentation, and\nretrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine\ntranslation (NMT), have achieved state-of-the-art performance on these tasks by\ntreating source code as a sequence of tokens. We present ${\\rm {\\scriptsize\nCODE2SEQ}}$: an alternative approach that leverages the syntactic structure of\nprogramming languages to better encode source code. Our model represents a code\nsnippet as the set of compositional paths in its abstract syntax tree (AST) and\nuses attention to select the relevant paths while decoding. We demonstrate the\neffectiveness of our approach for two tasks, two programming languages, and\nfour datasets of up to $16$M examples. Our model significantly outperforms\nprevious models that were specifically designed for programming languages, as\nwell as state-of-the-art NMT models. An interactive online demo of our model is\navailable at http://code2seq.org. Our code, data and trained models are\navailable at http://github.com/tech-srl/code2seq.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 01:26:07 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 19:15:15 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 22:20:49 GMT"}, {"version": "v4", "created": "Fri, 25 Jan 2019 21:54:12 GMT"}, {"version": "v5", "created": "Wed, 6 Feb 2019 08:27:31 GMT"}, {"version": "v6", "created": "Thu, 21 Feb 2019 14:12:56 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Alon", "Uri", ""], ["Brody", "Shaked", ""], ["Levy", "Omer", ""], ["Yahav", "Eran", ""]]}, {"id": "1808.01410", "submitter": "Daisy Stanton", "authors": "Daisy Stanton, Yuxuan Wang, RJ Skerry-Ryan", "title": "Predicting Expressive Speaking Style From Text In End-To-End Speech\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Style Tokens (GSTs) are a recently-proposed method to learn latent\ndisentangled representations of high-dimensional data. GSTs can be used within\nTacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to\nuncover expressive factors of variation in speaking style. In this work, we\nintroduce the Text-Predicted Global Style Token (TP-GST) architecture, which\ntreats GST combination weights or style embeddings as \"virtual\" speaking style\nlabels within Tacotron. TP-GST learns to predict stylistic renderings from text\nalone, requiring neither explicit labels during training nor auxiliary inputs\nfor inference. We show that, when trained on a dataset of expressive speech,\nour system generates audio with more pitch and energy variation than two\nstate-of-the-art baseline models. We further demonstrate that TP-GSTs can\nsynthesize speech with background noise removed, and corroborate these analyses\nwith positive results on human-rated listener preference audiobook tasks.\nFinally, we demonstrate that multi-speaker TP-GST models successfully factorize\nspeaker identity and speaking style. We provide a website with audio samples\nfor each of our findings.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 02:21:07 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Stanton", "Daisy", ""], ["Wang", "Yuxuan", ""], ["Skerry-Ryan", "RJ", ""]]}, {"id": "1808.01486", "submitter": "Wei Cui", "authors": "Wei Cui, Kaiming Shen, Wei Yu", "title": "Spatial Deep Learning for Wireless Scheduling", "comments": "This paper is the full version of the paper presented at IEEE Global\n  Communications Conference 2018. It includes 15 pages and 12 figures", "journal-ref": "IEEE J. Sel. Areas in Commun. 37 (2019) 1248-1261", "doi": "10.1109/JSAC.2019.2904352", "report-no": null, "categories": "eess.SP cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal scheduling of interfering links in a dense wireless network with\nfull frequency reuse is a challenging task. The traditional method involves\nfirst estimating all the interfering channel strengths then optimizing the\nscheduling based on the model. This model-based method is however resource\nintensive and computationally hard because channel estimation is expensive in\ndense networks; furthermore, finding even a locally optimal solution of the\nresulting optimization problem may be computationally complex. This paper shows\nthat by using a deep learning approach, it is possible to bypass the channel\nestimation and to schedule links efficiently based solely on the geographic\nlocations of the transmitters and the receivers, due to the fact that in many\npropagation environments, the wireless channel strength is largely a function\nof the distance dependent path-loss. This is accomplished by unsupervised\ntraining over randomly deployed networks, and by using a novel neural network\narchitecture that computes the geographic spatial convolutions of the\ninterfering or interfered neighboring nodes along with subsequent multiple\nfeedback stages to learn the optimum solution. The resulting neural network\ngives near-optimal performance for sum-rate maximization and is capable of\ngeneralizing to larger deployment areas and to deployments of different link\ndensities. Moreover, to provide fairness, this paper proposes a novel\nscheduling approach that utilizes the sum-rate optimal scheduling algorithm\nover judiciously chosen subsets of links for maximizing a proportional fairness\nobjective over the network. The proposed approach shows highly competitive and\ngeneralizable network utility maximization results.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 14:03:10 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 01:19:09 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 15:23:30 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Cui", "Wei", ""], ["Shen", "Kaiming", ""], ["Yu", "Wei", ""]]}, {"id": "1808.01517", "submitter": "Simon Koppers", "authors": "Simon Koppers and Dorit Merhof", "title": "DELIMIT PyTorch - An extension for Deep Learning in Diffusion Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DELIMIT is a framework extension for deep learning in diffusion imaging,\nwhich extends the basic framework PyTorch towards spherical signals. Based on\nseveral novel layers, deep learning can be applied to spherical diffusion\nimaging data in a very convenient way. First, two spherical harmonic\ninterpolation layers are added to the extension, which allow to transform the\nsignal from spherical surface space into the spherical harmonic space, and vice\nversa. In addition, a local spherical convolution layer is introduced that adds\nthe possibility to include gradient neighborhood information within the\nnetwork. Furthermore, these extensions can also be utilized for the\npreprocessing of diffusion signals.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 18:26:24 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Koppers", "Simon", ""], ["Merhof", "Dorit", ""]]}, {"id": "1808.01524", "submitter": "Prashnna Gyawali", "authors": "Prashnna K Gyawali, B. Milan Horacek, John L. Sapp, and Linwei Wang", "title": "Learning disentangled representation from 12-lead electrograms:\n  application in localizing the origin of Ventricular Tachycardia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing availability of electrocardiogram (ECG) data has motivated the\nuse of data-driven models for automating various clinical tasks based on ECG\ndata. The development of subject-specific models are limited by the cost and\ndifficulty of obtaining sufficient training data for each individual. The\nalternative of population model, however, faces challenges caused by the\nsignificant inter-subject variations within the ECG data. We address this\nchallenge by investigating for the first time the problem of learning\nrepresentations for clinically-informative variables while disentangling other\nfactors of variations within the ECG data. In this work, we present a\nconditional variational autoencoder (VAE) to extract the subject-specific\nadjustment to the ECG data, conditioned on task-specific representations\nlearned from a deterministic encoder. To encourage the representation for\ninter-subject variations to be independent from the task-specific\nrepresentation, maximum mean discrepancy is used to match all the moments\nbetween the distributions learned by the VAE conditioning on the code from the\ndeterministic encoder. The learning of the task-specific representation is\nregularized by a weak supervision in the form of contrastive regularization. We\napply the proposed method to a novel yet important clinical task of classifying\nthe origin of ventricular tachycardia (VT) into pre-defined segments,\ndemonstrating the efficacy of the proposed method against the standard VAE.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 19:34:39 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gyawali", "Prashnna K", ""], ["Horacek", "B. Milan", ""], ["Sapp", "John L.", ""], ["Wang", "Linwei", ""]]}, {"id": "1808.01527", "submitter": "Kobi Cohen", "authors": "Anton Puzanov and Kobi Cohen", "title": "Deep Reinforcement One-Shot Learning for Artificially Intelligent\n  Classification Systems", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a sharp rise in networking applications, in\nwhich significant events need to be classified but only a few training\ninstances are available. These are known as cases of one-shot learning.\nExamples include analyzing network traffic under zero-day attacks, and computer\nvision tasks by sensor networks deployed in the field. To handle this\nchallenging task, organizations often use human analysts to classify events\nunder high uncertainty. Existing algorithms use a threshold-based mechanism to\ndecide whether to classify an object automatically or send it to an analyst for\ndeeper inspection. However, this approach leads to a significant waste of\nresources since it does not take the practical temporal constraints of system\nresources into account. Our contribution is threefold. First, we develop a\nnovel Deep Reinforcement One-shot Learning (DeROL) framework to address this\nchallenge. The basic idea of the DeROL algorithm is to train a deep-Q network\nto obtain a policy which is oblivious to the unseen classes in the testing\ndata. Then, in real-time, DeROL maps the current state of the one-shot learning\nprocess to operational actions based on the trained deep-Q network, to maximize\nthe objective function. Second, we develop the first open-source software for\npractical artificially intelligent one-shot classification systems with limited\nresources for the benefit of researchers in related fields. Third, we present\nan extensive experimental study using the OMNIGLOT dataset for computer vision\ntasks and the UNSW-NB15 dataset for intrusion detection tasks that demonstrates\nthe versatility and efficiency of the DeROL framework.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 20:13:35 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Puzanov", "Anton", ""], ["Cohen", "Kobi", ""]]}, {"id": "1808.01531", "submitter": "Ian Gemp", "authors": "Ian Gemp, Sridhar Mahadevan", "title": "Global Convergence to the Equilibrium of GANs using Variational\n  Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In optimization, the negative gradient of a function denotes the direction of\nsteepest descent. Furthermore, traveling in any direction orthogonal to the\ngradient maintains the value of the function. In this work, we show that these\northogonal directions that are ignored by gradient descent can be critical in\nequilibrium problems. Equilibrium problems have drawn heightened attention in\nmachine learning due to the emergence of the Generative Adversarial Network\n(GAN). We use the framework of Variational Inequalities to analyze popular\ntraining algorithms for a fundamental GAN variant: the Wasserstein\nLinear-Quadratic GAN. We show that the steepest descent direction causes\ndivergence from the equilibrium, and convergence to the equilibrium is achieved\nthrough following a particular orthogonal direction. We call this successful\ntechnique Crossing-the-Curl, named for its mathematical derivation as well as\nits intuition: identify the game's axis of rotation and move \"across\" space in\nthe direction towards smaller \"curling\".\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 20:47:44 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 19:41:27 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 14:04:54 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Gemp", "Ian", ""], ["Mahadevan", "Sridhar", ""]]}, {"id": "1808.01535", "submitter": "Huan Song", "authors": "Huan Song, Megan Willi, Jayaraman J. Thiagarajan, Visar Berisha,\n  Andreas Spanias", "title": "Triplet Network with Attention for Speaker Diarization", "comments": "Interspeech2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automatic speech processing systems, speaker diarization is a crucial\nfront-end component to separate segments from different speakers. Inspired by\nthe recent success of deep neural networks (DNNs) in semantic inferencing,\ntriplet loss-based architectures have been successfully used for this problem.\nHowever, existing work utilizes conventional i-vectors as the input\nrepresentation and builds simple fully connected networks for metric learning,\nthus not fully leveraging the modeling power of DNN architectures. This paper\ninvestigates the importance of learning effective representations from the\nsequences directly in metric learning pipelines for speaker diarization. More\nspecifically, we propose to employ attention models to learn embeddings and the\nmetric jointly in an end-to-end fashion. Experiments are conducted on the\nCALLHOME conversational speech corpus. The diarization results demonstrate\nthat, besides providing a unified model, the proposed approach achieves\nimproved performance when compared against existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 21:10:03 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Song", "Huan", ""], ["Willi", "Megan", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Berisha", "Visar", ""], ["Spanias", "Andreas", ""]]}, {"id": "1808.01550", "submitter": "Dimitrios Stamoulis", "authors": "Dimitrios Stamoulis, Ting-Wu Chin, Anand Krishnan Prakash, Haocheng\n  Fang, Sribhuvan Sajja, Mitchell Bognar, Diana Marculescu", "title": "Designing Adaptive Neural Networks for Energy-Constrained Image\n  Classification", "comments": "This conference paper will appear in the proceedings of ICCAD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As convolutional neural networks (CNNs) enable state-of-the-art computer\nvision applications, their high energy consumption has emerged as a key\nimpediment to their deployment on embedded and mobile devices. Towards\nefficient image classification under hardware constraints, prior work has\nproposed adaptive CNNs, i.e., systems of networks with different accuracy and\ncomputation characteristics, where a selection scheme adaptively selects the\nnetwork to be evaluated for each input image. While previous efforts have\ninvestigated different network selection schemes, we find that they do not\nnecessarily result in energy savings when deployed on mobile systems. The key\nlimitation of existing methods is that they learn only how data should be\nprocessed among the CNNs and not the network architectures, with each network\nbeing treated as a blackbox.\n  To address this limitation, we pursue a more powerful design paradigm where\nthe architecture settings of the CNNs are treated as hyper-parameters to be\nglobally optimized. We cast the design of adaptive CNNs as a hyper-parameter\noptimization problem with respect to energy, accuracy, and communication\nconstraints imposed by the mobile device. To efficiently solve this problem, we\nadapt Bayesian optimization to the properties of the design space, reaching\nnear-optimal configurations in few tens of function evaluations. Our method\nreduces the energy consumed for image classification on a mobile device by up\nto 6x, compared to the best previously published work that uses CNNs as\nblackboxes. Finally, we evaluate two image classification practices, i.e.,\nclassifying all images locally versus over the cloud under energy and\ncommunication constraints.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 02:01:31 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 00:47:43 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Stamoulis", "Dimitrios", ""], ["Chin", "Ting-Wu", ""], ["Prakash", "Anand Krishnan", ""], ["Fang", "Haocheng", ""], ["Sajja", "Sribhuvan", ""], ["Bognar", "Mitchell", ""], ["Marculescu", "Diana", ""]]}, {"id": "1808.01552", "submitter": "Leye Wang", "authors": "Leye Wang, Bin Guo, Qiang Yang", "title": "Smart City Development with Urban Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the smart city development levels of different cities are still\nunbalanced. For a large number of cities which just started development, the\ngovernments will face a critical cold-start problem: 'how to develop a new\nsmart city service with limited data?'. To address this problem, transfer\nlearning can be leveraged to accelerate the smart city development, which we\nterm the urban transfer learning paradigm. This article investigates the common\nprocess of urban transfer learning, aiming to provide city planners and\nrelevant practitioners with guidelines on how to apply this novel learning\nparadigm. Our guidelines include common transfer strategies to take, general\nsteps to follow, and case studies in public safety, transportation management,\netc. We also summarize a few research opportunities and expect this article can\nattract more researchers to study urban transfer learning.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 02:28:27 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 03:42:02 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Wang", "Leye", ""], ["Guo", "Bin", ""], ["Yang", "Qiang", ""]]}, {"id": "1808.01560", "submitter": "Hyeong Kyu Choi", "authors": "Hyeong Kyu Choi", "title": "Stock Price Correlation Coefficient Prediction with ARIMA-LSTM Hybrid\n  Model", "comments": "I'd appreciate any kind of comments on my work. Feel free to email\n  me!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the price correlation of two assets for future time periods is\nimportant in portfolio optimization. We apply LSTM recurrent neural networks\n(RNN) in predicting the stock price correlation coefficient of two individual\nstocks. RNNs are competent in understanding temporal dependencies. The use of\nLSTM cells further enhances its long term predictive properties. To encompass\nboth linearity and nonlinearity in the model, we adopt the ARIMA model as well.\nThe ARIMA model filters linear tendencies in the data and passes on the\nresidual value to the LSTM model. The ARIMA LSTM hybrid model is tested against\nother traditional predictive financial models such as the full historical\nmodel, constant correlation model, single index model and the multi group\nmodel. In our empirical study, the predictive ability of the ARIMA-LSTM model\nturned out superior to all other financial models by a significant scale. Our\nwork implies that it is worth considering the ARIMA LSTM model to forecast\ncorrelation coefficient for portfolio optimization.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 05:10:26 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 07:43:41 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 06:29:21 GMT"}, {"version": "v4", "created": "Mon, 27 Aug 2018 05:30:05 GMT"}, {"version": "v5", "created": "Mon, 1 Oct 2018 11:08:13 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Choi", "Hyeong Kyu", ""]]}, {"id": "1808.01574", "submitter": "Marco Duarte", "authors": "Siwei Feng, Han Yu, and Marco F. Duarte", "title": "Autoencoder Based Sample Selection for Self-Taught Learning", "comments": "38 pages, 4 figures, to appear in Elsevier Knowledge-Based Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-taught learning is a technique that uses a large number of unlabeled\ndata as source samples to improve the task performance on target samples.\nCompared with other transfer learning techniques, self-taught learning can be\napplied to a broader set of scenarios due to the loose restrictions on the\nsource data. However, knowledge transferred from source samples that are not\nsufficiently related to the target domain may negatively influence the target\nlearner, which is referred to as negative transfer. In this paper, we propose a\nmetric for the relevance between a source sample and the target samples. To be\nmore specific, both source and target samples are reconstructed through a\nsingle-layer autoencoder with a linear relationship between source samples and\nreconstructed target samples being simultaneously enforced. An\n$\\ell_{2,1}$-norm sparsity constraint is imposed on the transformation matrix\nto identify source samples relevant to the target domain. Source domain samples\nthat are deemed relevant are assigned pseudo-labels reflecting their relevance\nto target domain samples, and are combined with target samples in order to\nprovide an expanded training set for classifier training. Local data structures\nare also preserved during source sample selection through spectral graph\nanalysis. Promising results in extensive experiments show the advantages of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 07:45:06 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 04:06:05 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Feng", "Siwei", ""], ["Yu", "Han", ""], ["Duarte", "Marco F.", ""]]}, {"id": "1808.01591", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta and Hinrich Sch\\\"utze", "title": "LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse\n  Semantic Accumulation and Example to Pattern Transformation", "comments": "2018 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP2018) workshop on Analyzing and Interpreting Neural Networks for NLP\n  (BlackBoxNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are temporal networks and cumulative in\nnature that have shown promising results in various natural language processing\ntasks. Despite their success, it still remains a challenge to understand their\nhidden behavior. In this work, we analyze and interpret the cumulative nature\nof RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation\n(LISA) for explaining decisions and detecting the most likely (i.e., saliency)\npatterns that the network relies on while decision making. We demonstrate (1)\nLISA: \"How an RNN accumulates or builds semantics during its sequential\nprocessing for a given text example and expected response\" (2) Example2pattern:\n\"How the saliency patterns look like for each category in the data according to\nthe network in decision making\". We analyse the sensitiveness of RNNs about\ndifferent inputs to check the increase or decrease in prediction scores and\nfurther extract the saliency patterns learned by the network. We employ two\nrelation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to\nexplain RNN predictions via the LISA and example2pattern.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 09:50:47 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gupta", "Pankaj", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1808.01595", "submitter": "Simon Koppers", "authors": "Simon Koppers, Luke Bloy, Jeffrey I. Berman, Chantal M.W. Tax, J.\n  Christopher Edgar and Dorit Merhof", "title": "Spherical Harmonic Residual Network for Diffusion Signal Harmonization", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion imaging is an important method in the field of neuroscience, as it\nis sensitive to changes within the tissue microstructure of the human brain.\nHowever, a major challenge when using MRI to derive quantitative measures is\nthat the use of different scanners, as used in multi-site group studies,\nintroduces measurement variability. This can lead to an increased variance in\nquantitative metrics, even if the same brain is scanned.\n  Contrary to the assumption that these characteristics are comparable and\nsimilar, small changes in these values are observed in many clinical studies,\nhence harmonization of the signals is essential.\n  In this paper, we present a method that does not require additional\npreprocessing, such as segmentation or registration, and harmonizes the signal\nbased on a deep learning residual network. For this purpose, a training\ndatabase is required, which consist of the same subjects, scanned on different\nscanners.\n  The results show that harmonized signals are significantly more similar to\nthe ground truth signal compared to no harmonization, but also improve in\ncomparison to another deep learning method. The same effect is also\ndemonstrated in commonly used metrics derived from the diffusion MRI signal.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 11:05:23 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Koppers", "Simon", ""], ["Bloy", "Luke", ""], ["Berman", "Jeffrey I.", ""], ["Tax", "Chantal M. W.", ""], ["Edgar", "J. Christopher", ""], ["Merhof", "Dorit", ""]]}, {"id": "1808.01614", "submitter": "Rick Salay", "authors": "Rick Salay, Krzysztof Czarnecki", "title": "Using Machine Learning Safely in Automotive Software: An Assessment and\n  Adaption of Software Process Requirements in ISO 26262", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of machine learning (ML) is on the rise in many sectors of software\ndevelopment, and automotive software development is no different. In\nparticular, Advanced Driver Assistance Systems (ADAS) and Automated Driving\nSystems (ADS) are two areas where ML plays a significant role. In automotive\ndevelopment, safety is a critical objective, and the emergence of standards\nsuch as ISO 26262 has helped focus industry practices to address safety in a\nsystematic and consistent way. Unfortunately, these standards were not designed\nto accommodate technologies such as ML or the type of functionality that is\nprovided by an ADS and this has created a conflict between the need to innovate\nand the need to improve safety. In this report, we take steps to address this\nconflict by doing a detailed assessment and adaption of ISO 26262 for ML,\nspecifically in the context of supervised learning. First we analyze the key\nfactors that are the source of the conflict. Then we assess each software\ndevelopment process requirement (Part 6 of ISO 26262) for applicability to ML.\nWhere there are gaps, we propose new requirements to address the gaps. Finally\nwe discuss the application of this adapted and extended variant of Part 6 to ML\ndevelopment scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 13:40:22 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Salay", "Rick", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1808.01616", "submitter": "Hongzhi Wang", "authors": "Zhemin Liu and Feng Xiong and Kaifa Zou and Hongzhi Wang", "title": "Predicting Learning Status in MOOCs using LSTM", "comments": "arXiv admin note: text overlap with arXiv:1402.1128 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time and open online course resources of MOOCs have attracted a large\nnumber of learners in recent years. However, many new questions were emerging\nabout the high dropout rate of learners. For MOOCs platform, predicting the\nlearning status of MOOCs learners in real time with high accuracy is the\ncrucial task, and it also help improve the quality of MOOCs teaching. The\nprediction task in this paper is inherently a time series prediction problem,\nand can be treated as time series classification problem, hence this paper\nproposed a prediction model based on RNNLSTMs and optimization techniques which\ncan be used to predict learners' learning status. Using datasets provided by\nChinese University MOOCs as the inputs of model, the average accuracy of\nmodel's outputs was about 90%.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 14:06:18 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Liu", "Zhemin", ""], ["Xiong", "Feng", ""], ["Zou", "Kaifa", ""], ["Wang", "Hongzhi", ""]]}, {"id": "1808.01630", "submitter": "Zhijian Ou", "authors": "Zhijian Ou", "title": "A Review of Learning with Deep Generative Models from Perspective of\n  Graphical Modeling", "comments": "add SN-GANs, SA-GANs, conditional generation (cGANs, AC-GANs). arXiv\n  admin note: text overlap with arXiv:1606.00709, arXiv:1801.03558 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document aims to provide a review on learning with deep generative\nmodels (DGMs), which is an highly-active area in machine learning and more\ngenerally, artificial intelligence. This review is not meant to be a tutorial,\nbut when necessary, we provide self-contained derivations for completeness.\nThis review has two features. First, though there are different perspectives to\nclassify DGMs, we choose to organize this review from the perspective of\ngraphical modeling, because the learning methods for directed DGMs and\nundirected DGMs are fundamentally different. Second, we differentiate model\ndefinitions from model learning algorithms, since different learning algorithms\ncan be applied to solve the learning problem on the same model, and an\nalgorithm can be applied to learn different models. We thus separate model\ndefinition and model learning, with more emphasis on reviewing, differentiating\nand connecting different learning algorithms. We also discuss promising future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 14:51:07 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 03:40:32 GMT"}, {"version": "v3", "created": "Sat, 1 Sep 2018 16:29:56 GMT"}, {"version": "v4", "created": "Wed, 27 Mar 2019 01:55:56 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Ou", "Zhijian", ""]]}, {"id": "1808.01642", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad and Daoqiang Zhang", "title": "Multi-Objective Cognitive Model: a supervised approach for multi-subject\n  fMRI analysis", "comments": "Neuroinformatics, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to decode the human brain, Multivariate Pattern (MVP) classification\ngenerates cognitive models by using functional Magnetic Resonance Imaging\n(fMRI) datasets. As a standard pipeline in the MVP analysis, brain patterns in\nmulti-subject fMRI dataset must be mapped to a shared space and then a\nclassification model is generated by employing the mapped patterns. However,\nthe MVP models may not provide stable performance on a new fMRI dataset because\nthe standard pipeline uses disjoint steps for generating these models. Indeed,\neach step in the pipeline includes an objective function with independent\noptimization approach, where the best solution of each step may not be optimum\nfor the next steps. For tackling the mentioned issue, this paper introduces the\nMulti-Objective Cognitive Model (MOCM) that utilizes an integrated objective\nfunction for MVP analysis rather than just using those disjoint steps. For\nsolving the integrated problem, we proposed a customized multi-objective\noptimization approach, where all possible solutions are firstly generated, and\nthen our method ranks and selects the robust solutions as the final results.\nEmpirical studies confirm that the proposed method can generate superior\nperformance in comparison with other techniques.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 16:19:56 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1808.01650", "submitter": "Deepak Gupta", "authors": "Deepak Gupta, Sarah Kohail and Pushpak Bhattacharyya", "title": "Combining Graph-based Dependency Features with Convolutional Neural\n  Network for Answer Triggering", "comments": "19th International Conference on Computational Linguistics and\n  Intelligent Text Processing (CICLing 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answer triggering is the task of selecting the best-suited answer for a given\nquestion from a set of candidate answers if exists. In this paper, we present a\nhybrid deep learning model for answer triggering, which combines several\ndependency graph based alignment features, namely graph edit distance,\ngraph-based similarity and dependency graph coverage, with dense vector\nembeddings from a Convolutional Neural Network (CNN). Our experiments on the\nWikiQA dataset show that such a combination can more accurately trigger a\ncandidate answer compared to the previous state-of-the-art models. Comparative\nstudy on WikiQA dataset shows 5.86% absolute F-score improvement at the\nquestion level.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 16:44:25 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gupta", "Deepak", ""], ["Kohail", "Sarah", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1808.01664", "submitter": "Kaidi Xu", "authors": "Kaidi Xu, Sijia Liu, Pu Zhao, Pin-Yu Chen, Huan Zhang, Quanfu Fan,\n  Deniz Erdogmus, Yanzhi Wang, Xue Lin", "title": "Structured Adversarial Attack: Towards General Implementation and Better\n  Interpretability", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When generating adversarial examples to attack deep neural networks (DNNs),\nLp norm of the added perturbation is usually used to measure the similarity\nbetween original image and adversarial example. However, such adversarial\nattacks perturbing the raw input spaces may fail to capture structural\ninformation hidden in the input. This work develops a more general attack\nmodel, i.e., the structured attack (StrAttack), which explores group sparsity\nin adversarial perturbations by sliding a mask through images aiming for\nextracting key spatial structures. An ADMM (alternating direction method of\nmultipliers)-based framework is proposed that can split the original problem\ninto a sequence of analytically solvable subproblems and can be generalized to\nimplement other attacking methods. Strong group sparsity is achieved in\nadversarial perturbations even with the same level of Lp norm distortion as the\nstate-of-the-art attacks. We demonstrate the effectiveness of StrAttack by\nextensive experimental results onMNIST, CIFAR-10, and ImageNet. We also show\nthat StrAttack provides better interpretability (i.e., better correspondence\nwith discriminative image regions)through adversarial saliency map (Papernot et\nal., 2016b) and class activation map(Zhou et al., 2016).\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 18:06:37 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 03:52:24 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 21:36:46 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Xu", "Kaidi", ""], ["Liu", "Sijia", ""], ["Zhao", "Pu", ""], ["Chen", "Pin-Yu", ""], ["Zhang", "Huan", ""], ["Fan", "Quanfu", ""], ["Erdogmus", "Deniz", ""], ["Wang", "Yanzhi", ""], ["Lin", "Xue", ""]]}, {"id": "1808.01684", "submitter": "Hongbao Zhang", "authors": "Hongbao Zhang, Pengtao Xie, Eric Xing", "title": "Missing Value Imputation Based on Deep Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing values widely exist in many real-world datasets, which hinders the\nperforming of advanced data analytics. Properly filling these missing values is\ncrucial but challenging, especially when the missing rate is high. Many\napproaches have been proposed for missing value imputation (MVI), but they are\nmostly heuristics-based, lacking a principled foundation and do not perform\nsatisfactorily in practice. In this paper, we propose a probabilistic framework\nbased on deep generative models for MVI. Under this framework, imputing the\nmissing entries amounts to seeking a fixed-point solution between two\nconditional distributions defined on the missing entries and latent variables\nrespectively. These distributions are parameterized by deep neural networks\n(DNNs) which possess high approximation power and can capture the nonlinear\nrelationships between missing entries and the observed values. The learning of\nweight parameters of DNNs is performed by maximizing an approximation of the\nlog-likelihood of observed values. We conducted extensive evaluation on 13\ndatasets and compared with 11 baselines methods, where our methods largely\noutperforms the baselines.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 21:06:05 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zhang", "Hongbao", ""], ["Xie", "Pengtao", ""], ["Xing", "Eric", ""]]}, {"id": "1808.01686", "submitter": "Henry Kvinge", "authors": "Henry Kvinge, Elin Farnell, Michael Kirby, Chris Peterson", "title": "Too many secants: a hierarchical approach to secant-based dimensionality\n  reduction on large data sets", "comments": "To appear in the Proceedings of the 2018 IEEE High Performance\n  Extreme Computing Conference, Waltham, MA USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question in many data analysis settings is the problem of\ndiscerning the \"natural\" dimension of a data set. That is, when a data set is\ndrawn from a manifold (possibly with noise), a meaningful aspect of the data is\nthe dimension of that manifold. Various approaches exist for estimating this\ndimension, such as the method of Secant-Avoidance Projection (SAP).\nIntuitively, the SAP algorithm seeks to determine a projection which best\npreserves the lengths of all secants between points in a data set; by applying\nthe algorithm to find the best projections to vector spaces of various\ndimensions, one may infer the dimension of the manifold of origination. That\nis, one may learn the dimension at which it is possible to construct a\ndiffeomorphic copy of the data in a lower-dimensional Euclidean space. Using\nWhitney's embedding theorem, we can relate this information to the natural\ndimension of the data. A drawback of the SAP algorithm is that a data set with\n$T$ points has $O(T^2)$ secants, making the computation and storage of all\nsecants infeasible for very large data sets. In this paper, we propose a novel\nalgorithm that generalizes the SAP algorithm with an emphasis on addressing\nthis issue. That is, we propose a hierarchical secant-based\ndimensionality-reduction method, which can be employed for data sets where\nexplicitly calculating all secants is not feasible.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 21:27:32 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Kvinge", "Henry", ""], ["Farnell", "Elin", ""], ["Kirby", "Michael", ""], ["Peterson", "Chris", ""]]}, {"id": "1808.01687", "submitter": "Micol Marchetti-Bowick", "authors": "Micol Marchetti-Bowick, Benjamin J. Lengerich, Ankur P. Parikh, Eric\n  P. Xing", "title": "Hybrid Subspace Learning for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high-dimensional data setting, in which p >> n, is a challenging\nstatistical paradigm that appears in many real-world problems. In this setting,\nlearning a compact, low-dimensional representation of the data can\nsubstantially help distinguish signal from noise. One way to achieve this goal\nis to perform subspace learning to estimate a small set of latent features that\ncapture the majority of the variance in the original data. Most existing\nsubspace learning models, such as PCA, assume that the data can be fully\nrepresented by its embedding in one or more latent subspaces. However, in this\nwork, we argue that this assumption is not suitable for many high-dimensional\ndatasets; often only some variables can easily be projected to a\nlow-dimensional space. We propose a hybrid dimensionality reduction technique\nin which some features are mapped to a low-dimensional subspace while others\nremain in the original space. Our model leads to more accurate estimation of\nthe latent space and lower reconstruction error. We present a simple\noptimization procedure for the resulting biconvex problem and show synthetic\ndata results that demonstrate the advantages of our approach over existing\nmethods. Finally, we demonstrate the effectiveness of this method for\nextracting meaningful features from both gene expression and video background\nsubtraction datasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 21:28:10 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Marchetti-Bowick", "Micol", ""], ["Lengerich", "Benjamin J.", ""], ["Parikh", "Ankur P.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1808.01739", "submitter": "L.A. Prashanth", "authors": "Ravi Kumar Kolla, Prashanth L.A., Sanjay P. Bhat and Krishna\n  Jagannathan", "title": "Concentration bounds for empirical conditional value-at-risk: The\n  unbounded case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several real-world applications involving decision making under\nuncertainty, the traditional expected value objective may not be suitable, as\nit may be necessary to control losses in the case of a rare but extreme event.\nConditional Value-at-Risk (CVaR) is a popular risk measure for modeling the\naforementioned objective. We consider the problem of estimating CVaR from\ni.i.d. samples of an unbounded random variable, which is either sub-Gaussian or\nsub-exponential. We derive a novel one-sided concentration bound for a natural\nsample-based CVaR estimator in this setting. Our bound relies on a\nconcentration result for a quantile-based estimator for Value-at-Risk (VaR),\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 06:02:15 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Kolla", "Ravi Kumar", ""], ["A.", "Prashanth L.", ""], ["Bhat", "Sanjay P.", ""], ["Jagannathan", "Krishna", ""]]}, {"id": "1808.01743", "submitter": "Marinka Zitnik", "authors": "Marinka Zitnik and Blaz Zupan", "title": "NIMFA: A Python Library for Nonnegative Matrix Factorization", "comments": null, "journal-ref": "Journal of Machine Learning Research 13 (2012) 849-853", "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NIMFA is an open-source Python library that provides a unified interface to\nnonnegative matrix factorization algorithms. It includes implementations of\nstate-of-the-art factorization methods, initialization approaches, and quality\nscoring. It supports both dense and sparse matrix representation. NIMFA's\ncomponent-based implementation and hierarchical design should help the users to\nemploy already implemented techniques or design and code new strategies for\nmatrix factorization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 06:28:35 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zitnik", "Marinka", ""], ["Zupan", "Blaz", ""]]}, {"id": "1808.01753", "submitter": "Vivek B S", "authors": "Vivek B.S., Konda Reddy Mopuri, and R. Venkatesh Babu", "title": "Gray-box Adversarial Training", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial samples are perturbed inputs crafted to mislead the machine\nlearning systems. A training mechanism, called adversarial training, which\npresents adversarial samples along with clean samples has been introduced to\nlearn robust models. In order to scale adversarial training for large datasets,\nthese perturbations can only be crafted using fast and simple methods (e.g.,\ngradient ascent). However, it is shown that adversarial training converges to a\ndegenerate minimum, where the model appears to be robust by generating weaker\nadversaries. As a result, the models are vulnerable to simple black-box\nattacks. In this paper we, (i) demonstrate the shortcomings of existing\nevaluation policy, (ii) introduce novel variants of white-box and black-box\nattacks, dubbed gray-box adversarial attacks\" based on which we propose novel\nevaluation method to assess the robustness of the learned models, and (iii)\npropose a novel variant of adversarial training, named Graybox Adversarial\nTraining\" that uses intermediate versions of the models to seed the\nadversaries. Experimental evaluation demonstrates that the models trained using\nour method exhibit better robustness compared to both undefended and\nadversarially trained model\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 07:26:44 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["S.", "Vivek B.", ""], ["Mopuri", "Konda Reddy", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1808.01813", "submitter": "Ronald Ortner", "authors": "Ronald Ortner", "title": "Regret Bounds for Reinforcement Learning via Markov Chain Concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple optimistic algorithm for which it is easy to derive regret\nbounds of $\\tilde{O}(\\sqrt{t_{\\rm mix} SAT})$ after $T$ steps in uniformly\nergodic Markov decision processes with $S$ states, $A$ actions, and mixing time\nparameter $t_{\\rm mix}$. These bounds are the first regret bounds in the\ngeneral, non-episodic setting with an optimal dependence on all given\nparameters. They could only be improved by using an alternative mixing time\nparameter.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 10:34:40 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2019 07:56:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ortner", "Ronald", ""]]}, {"id": "1808.01842", "submitter": "Ashkan Norouzi-Fard", "authors": "Ashkan Norouzi-Fard, Jakub Tarnawski, Slobodan Mitrovi\\'c, Amir\n  Zandieh, Aida Mousavifar, and Ola Svensson", "title": "Beyond $1/2$-Approximation for Submodular Maximization on Massive Data\n  Streams", "comments": null, "journal-ref": "Proc. of 35th International Conference on Machine Learning (ICML),\n  2018, pages 3829-3838", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in machine learning and data mining, such as data diversification,\nnon-parametric learning, kernel machines, clustering etc., require extracting a\nsmall but representative summary from a massive dataset. Often, such problems\ncan be posed as maximizing a submodular set function subject to a cardinality\nconstraint. We consider this question in the streaming setting, where elements\narrive over time at a fast pace and thus we need to design an efficient,\nlow-memory algorithm. One such method, proposed by Badanidiyuru et al. (2014),\nalways finds a $0.5$-approximate solution. Can this approximation factor be\nimproved? We answer this question affirmatively by designing a new algorithm\nSALSA for streaming submodular maximization. It is the first low-memory,\nsingle-pass algorithm that improves the factor $0.5$, under the natural\nassumption that elements arrive in a random order. We also show that this\nassumption is necessary, i.e., that there is no such algorithm with better than\n$0.5$-approximation when elements arrive in arbitrary order. Our experiments\ndemonstrate that SALSA significantly outperforms the state of the art in\napplications related to exemplar-based clustering, social graph analysis, and\nrecommender systems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 12:23:42 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Norouzi-Fard", "Ashkan", ""], ["Tarnawski", "Jakub", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Zandieh", "Amir", ""], ["Mousavifar", "Aida", ""], ["Svensson", "Ola", ""]]}, {"id": "1808.01857", "submitter": "Quentin Berthet", "authors": "Quentin Berthet, Varun Kanade", "title": "Statistical Windows in Testing for the Initial Distribution of a\n  Reversible Markov Chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of hypothesis testing between two discrete\ndistributions, where we only have access to samples after the action of a known\nreversible Markov chain, playing the role of noise. We derive\ninstance-dependent minimax rates for the sample complexity of this problem, and\nshow how its dependence in time is related to the spectral properties of the\nMarkov chain. We show that there exists a wide statistical window, in terms of\nsample complexity for hypothesis testing between different pairs of initial\ndistributions. We illustrate these results in several concrete examples.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 12:54:04 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Berthet", "Quentin", ""], ["Kanade", "Varun", ""]]}, {"id": "1808.01916", "submitter": "Murali Karthick Baskar", "authors": "Murali Karthick Baskar, Martin Karafiat, Lukas Burget, Karel Vesely,\n  Frantisek Grezl and Jan Honza Cernocky", "title": "Residual Memory Networks: Feed-forward approach to learn long temporal\n  dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep recurrent neural network (RNN) architectures is complicated due\nto the increased network complexity. This disrupts the learning of higher order\nabstracts using deep RNN. In case of feed-forward networks training deep\nstructures is simple and faster while learning long-term temporal information\nis not possible. In this paper we propose a residual memory neural network\n(RMN) architecture to model short-time dependencies using deep feed-forward\nlayers having residual and time delayed connections. The residual connection\npaves way to construct deeper networks by enabling unhindered flow of gradients\nand the time delay units capture temporal information with shared weights. The\nnumber of layers in RMN signifies both the hierarchical processing depth and\ntemporal depth. The computational complexity in training RMN is significantly\nless when compared to deep recurrent networks. RMN is further extended as\nbi-directional RMN (BRMN) to capture both past and future information.\nExperimental analysis is done on AMI corpus to substantiate the capability of\nRMN in learning long-term information and hierarchical information. Recognition\nperformance of RMN trained with 300 hours of Switchboard corpus is compared\nwith various state-of-the-art LVCSR systems. The results indicate that RMN and\nBRMN gains 6 % and 3.8 % relative improvement over LSTM and BLSTM networks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 14:00:40 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Baskar", "Murali Karthick", ""], ["Karafiat", "Martin", ""], ["Burget", "Lukas", ""], ["Vesely", "Karel", ""], ["Grezl", "Frantisek", ""], ["Cernocky", "Jan Honza", ""]]}, {"id": "1808.01944", "submitter": "Nicolo' Savioli", "authors": "Nicol\\'o Savioli, Giovanni Montana, Pablo Lamata", "title": "V-FCNN: Volumetric Fully Convolution Neural Network For Automatic Atrial\n  Segmentation", "comments": "9 pages, 4 figures, In Proceedings of MICCAI 2018 Atrial Segmentation\n  Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Atrial Fibrillation (AF) is a common electro-physiological cardiac disorder\nthat causes changes in the anatomy of the atria. A better characterization of\nthese changes is desirable for the definition of clinical biomarkers,\nfurthermore, thus there is a need for its fully automatic segmentation from\nclinical images. In this work, we present an architecture based on\n3D-convolution kernels, a Volumetric Fully Convolution Neural Network (V-FCNN),\nable to segment the entire volume in a one-shot, and consequently integrate the\nimplicit spatial redundancy present in high-resolution images. A loss function\nbased on the mixture of both Mean Square Error (MSE) and Dice Loss (DL) is\nused, in an attempt to combine the ability to capture the bulk shape as well as\nthe reduction of local errors products by over-segmentation. Results\ndemonstrate a reasonable performance in the middle region of the atria along\nwith the impact of the challenges of capturing the variability of the pulmonary\nveins or the identification of the valve plane that separates the atria to the\nventricle. A final dice of $92.5\\%$ in $54$ patients ($4752$ atria test slices\nin total) is shown.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 14:51:33 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 09:27:11 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Savioli", "Nicol\u00f3", ""], ["Montana", "Giovanni", ""], ["Lamata", "Pablo", ""]]}, {"id": "1808.01960", "submitter": "Dror Freirich", "authors": "Dror Freirich and Ron Meir and Aviv Tamar", "title": "Distributional Multivariate Policy Evaluation and Exploration with the\n  Bellman GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed distributional approach to reinforcement learning\n(DiRL) is centered on learning the distribution of the reward-to-go, often\nreferred to as the value distribution. In this work, we show that the\ndistributional Bellman equation, which drives DiRL methods, is equivalent to a\ngenerative adversarial network (GAN) model. In this formulation, DiRL can be\nseen as learning a deep generative model of the value distribution, driven by\nthe discrepancy between the distribution of the current value, and the\ndistribution of the sum of current reward and next value. We use this insight\nto propose a GAN-based approach to DiRL, which leverages the strengths of GANs\nin learning distributions of high-dimensional data. In particular, we show that\nour GAN approach can be used for DiRL with multivariate rewards, an important\nsetting which cannot be tackled with prior methods. The multivariate setting\nalso allows us to unify learning the distribution of values and state\ntransitions, and we exploit this idea to devise a novel exploration method that\nis driven by the discrepancy in estimating both values and states.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 15:22:13 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Freirich", "Dror", ""], ["Meir", "Ron", ""], ["Tamar", "Aviv", ""]]}, {"id": "1808.01974", "submitter": "Chuanqi Tan", "authors": "Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang and\n  Chunfang Liu", "title": "A Survey on Deep Transfer Learning", "comments": "The 27th International Conference on Artificial Neural Networks\n  (ICANN 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a new classification platform, deep learning has recently received\nincreasing attention from researchers and has been successfully applied to many\ndomains. In some domains, like bioinformatics and robotics, it is very\ndifficult to construct a large-scale well-annotated dataset due to the expense\nof data acquisition and costly annotation, which limits its development.\nTransfer learning relaxes the hypothesis that the training data must be\nindependent and identically distributed (i.i.d.) with the test data, which\nmotivates us to use transfer learning to solve the problem of insufficient\ntraining data. This survey focuses on reviewing the current researches of\ntransfer learning by using deep neural network and its applications. We defined\ndeep transfer learning, category and review the recent research works based on\nthe techniques used in deep transfer learning.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:06:43 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Tan", "Chuanqi", ""], ["Sun", "Fuchun", ""], ["Kong", "Tao", ""], ["Zhang", "Wenchang", ""], ["Yang", "Chao", ""], ["Liu", "Chunfang", ""]]}, {"id": "1808.01975", "submitter": "Pascal Fernsel", "authors": "Pascal Fernsel, Peter Maass", "title": "A Survey on Surrogate Approaches to Non-negative Matrix Factorization", "comments": "37 pages, 6 figures. Submitted to the Vietnam Journal of Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in hyperspectral imaging we investigate methods for\napproximating a high-dimensional non-negative matrix $\\mathbf{\\mathit{Y}}$ by a\nproduct of two lower-dimensional, non-negative matrices $\\mathbf{\\mathit{K}}$\nand $\\mathbf{\\mathit{X}}.$ This so-called non-negative matrix factorization is\nbased on defining suitable Tikhonov functionals, which combine a discrepancy\nmeasure for $\\mathbf{\\mathit{Y}}\\approx\\mathbf{\\mathit{KX}}$ with penalty terms\nfor enforcing additional properties of $\\mathbf{\\mathit{K}}$ and\n$\\mathbf{\\mathit{X}}$. The minimization is based on alternating minimization\nwith respect to $\\mathbf{\\mathit{K}}$ or $\\mathbf{\\mathit{X}}$, where in each\niteration step one replaces the original Tikhonov functional by a locally\ndefined surrogate functional. The choice of surrogate functionals is crucial:\nIt should allow a comparatively simple minimization and simultaneously its\nfirst order optimality condition should lead to multiplicative update rules,\nwhich automatically preserve non-negativity of the iterates. We review the most\nstandard construction principles for surrogate functionals for Frobenius-norm\nand Kullback-Leibler discrepancy measures. We extend the known surrogate\nconstructions by a general framework, which allows to add a large variety of\npenalty terms. The paper finishes by deriving the corresponding alternating\nminimization schemes explicitely and by applying these methods to MALDI imaging\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:12:04 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 12:17:16 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Fernsel", "Pascal", ""], ["Maass", "Peter", ""]]}, {"id": "1808.01976", "submitter": "Jonas Rauber", "authors": "Wieland Brendel, Jonas Rauber, Alexey Kurakin, Nicolas Papernot, Behar\n  Veliqi, Marcel Salath\\'e, Sharada P. Mohanty, Matthias Bethge", "title": "Adversarial Vision Challenge", "comments": "https://www.crowdai.org/challenges/adversarial-vision-challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NIPS 2018 Adversarial Vision Challenge is a competition to facilitate\nmeasurable progress towards robust machine vision models and more generally\napplicable adversarial attacks. This document is an updated version of our\ncompetition proposal that was accepted in the competition track of 32nd\nConference on Neural Information Processing Systems (NIPS 2018).\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:13:43 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 18:21:49 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Brendel", "Wieland", ""], ["Rauber", "Jonas", ""], ["Kurakin", "Alexey", ""], ["Papernot", "Nicolas", ""], ["Veliqi", "Behar", ""], ["Salath\u00e9", "Marcel", ""], ["Mohanty", "Sharada P.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1808.01990", "submitter": "Fatih Cakir", "authors": "Fatih Cakir, Kun He, Stan Sclaroff", "title": "Hashing with Binary Matrix Pursuit", "comments": "23 pages, 4 figures. In Proceedings of European Conference on\n  Computer Vision (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose theoretical and empirical improvements for two-stage hashing\nmethods. We first provide a theoretical analysis on the quality of the binary\ncodes and show that, under mild assumptions, a residual learning scheme can\nconstruct binary codes that fit any neighborhood structure with arbitrary\naccuracy. Secondly, we show that with high-capacity hash functions such as\nCNNs, binary code inference can be greatly simplified for many standard\nneighborhood definitions, yielding smaller optimization problems and more\nrobust codes. Incorporating our findings, we propose a novel two-stage hashing\nmethod that significantly outperforms previous hashing studies on widely used\nimage retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:51:36 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Cakir", "Fatih", ""], ["He", "Kun", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1808.01992", "submitter": "Zhiding Yu", "authors": "Zhiding Yu, Weiyang Liu, Yang Zou, Chen Feng, Srikumar Ramalingam, B.\n  V. K. Vijaya Kumar, Jan Kautz", "title": "Simultaneous Edge Alignment and Learning", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection is among the most fundamental vision problems for its role in\nperceptual grouping and its wide applications. Recent advances in\nrepresentation learning have led to considerable improvements in this area.\nMany state of the art edge detection models are learned with fully\nconvolutional networks (FCNs). However, FCN-based edge learning tends to be\nvulnerable to misaligned labels due to the delicate structure of edges. While\nsuch problem was considered in evaluation benchmarks, similar issue has not\nbeen explicitly addressed in general edge learning. In this paper, we show that\nlabel misalignment can cause considerably degraded edge learning quality, and\naddress this issue by proposing a simultaneous edge alignment and learning\nframework. To this end, we formulate a probabilistic model where edge alignment\nis treated as latent variable optimization, and is learned end-to-end during\nnetwork training. Experiments show several applications of this work, including\nimproved edge detection with state of the art performance, and automatic\nrefinement of noisy annotations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:58:42 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 09:42:05 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 05:36:51 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Yu", "Zhiding", ""], ["Liu", "Weiyang", ""], ["Zou", "Yang", ""], ["Feng", "Chen", ""], ["Ramalingam", "Srikumar", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Kautz", "Jan", ""]]}, {"id": "1808.02016", "submitter": "Abduallah Mohamed", "authors": "Abduallah A. Mohamed and Christian Claudel", "title": "MCRM: Mother Compact Recurrent Memory", "comments": "Submitted to AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTMs and GRUs are the most common recurrent neural network architectures\nused to solve temporal sequence problems. The two architectures have differing\ndata flows dealing with a common component called the cell state (also referred\nto as the memory). We attempt to enhance the memory by presenting a\nmodification that we call the Mother Compact Recurrent Memory (MCRM). MCRMs are\na type of a nested LSTM-GRU architecture where the cell state is the GRU hidden\nstate. The concatenation of the forget gate and input gate interactions from\nthe LSTM are considered an input to the GRU cell. Because MCRMs has this type\nof nesting, MCRMs have a compact memory pattern consisting of neurons that acts\nexplicitly in both long-term and short-term fashions. For some specific tasks,\nempirical results show that MCRMs outperform previously used architectures.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 15:48:39 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 06:00:41 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 21:37:25 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Mohamed", "Abduallah A.", ""], ["Claudel", "Christian", ""]]}, {"id": "1808.02017", "submitter": "Daniele Ramazzotti", "authors": "Daniele Ramazzotti and Peter Clardy and Leo Anthony Celi and David J.\n  Stone and Robert S. Rudin", "title": "Withholding or withdrawing invasive interventions may not accelerate\n  time to death among dying ICU patients", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0212439", "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We considered observational data available from the MIMIC-III open-access ICU\ndatabase and collected within a study period between year 2002 up to 2011. If a\npatient had multiple admissions to the ICU during the 30 days before death,\nonly the first stay was analyzed, leading to a final set of 6,436 unique ICU\nadmissions during the study period. We tested two hypotheses: (i)\nadministration of invasive intervention during the ICU stay immediately\npreceding end-of-life would decrease over the study time period and (ii)\ntime-to-death from ICU admission would also decrease, due to the decrease in\ninvasive intervention administration. To investigate the latter hypothesis, we\nperformed a subgroups analysis by considering patients with lowest and highest\nseverity. To do so, we stratified the patients based on their SAPS I scores,\nand we considered patients within the first and the third tertiles of the\nscore. We then assessed differences in trends within these groups between years\n2002-05 vs. 2008-11.\n  Comparing the period 2002-2005 vs. 2008-2011, we found a reduction in\nendotracheal ventilation among patients who died within 30 days of ICU\nadmission (120.8 vs. 68.5 hours for the lowest severity patients, p<0.001; 47.7\nvs. 46.0 hours for the highest severity patients, p=0.004). This is explained\nin part by an increase in the use of non-invasive ventilation. Comparing the\nperiod 2002-2005 vs. 2008-2011, we found a reduction in the use of vasopressors\nand inotropes among patients with the lowest severity who died within 30 days\nof ICU admission (41.8 vs. 36.2 hours, p<0.001) but not among those with the\nhighest severity. Despite a reduction in the use of invasive interventions, we\ndid not find a reduction in the time to death between 2002-2005 vs. 2008-2011\n(7.8 days vs. 8.2 days for the lowest severity patients, p=0.32; 2.1 days vs.\n2.0 days for the highest severity patients, p=0.74).\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 17:50:41 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 22:13:42 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Ramazzotti", "Daniele", ""], ["Clardy", "Peter", ""], ["Celi", "Leo Anthony", ""], ["Stone", "David J.", ""], ["Rudin", "Robert S.", ""]]}, {"id": "1808.02024", "submitter": "Quang-VInh Dang", "authors": "Quang-Vinh Dang", "title": "Outlier detection on network flow analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to be able to detect and classify malicious network traffic\nflows such as DDoS attacks from benign flows. Normally the task is performed by\nusing supervised classification algorithms. In this paper we analyze the usage\nof outlier detection algorithms for the network traffic classification problem.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 14:15:46 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Dang", "Quang-Vinh", ""]]}, {"id": "1808.02026", "submitter": "Nutan Chen", "authors": "Nutan Chen, Alexej Klushyn, Alexandros Paraschos, Djalel Benbouzid,\n  Patrick van der Smagt", "title": "Active Learning based on Data Uncertainty and Model Sensitivity", "comments": "Published on 2018 IEEE/RSJ International Conference on Intelligent\n  Robots and System", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots can rapidly acquire new skills from demonstrations. However, during\ngeneralisation of skills or transitioning across fundamentally different\nskills, it is unclear whether the robot has the necessary knowledge to perform\nthe task. Failing to detect missing information often leads to abrupt movements\nor to collisions with the environment. Active learning can quantify the\nuncertainty of performing the task and, in general, locate regions of missing\ninformation. We introduce a novel algorithm for active learning and demonstrate\nits utility for generating smooth trajectories. Our approach is based on deep\ngenerative models and metric learning in latent spaces. It relies on the\nJacobian of the likelihood to detect non-smooth transitions in the latent\nspace, i.e., transitions that lead to abrupt changes in the movement of the\nrobot. When non-smooth transitions are detected, our algorithm asks for an\nadditional demonstration from that specific region. The newly acquired\nknowledge modifies the data manifold and allows for learning a latent\nrepresentation for generating smooth movements. We demonstrate the efficacy of\nour approach on generalising elementary skills, transitioning across different\nskills, and implicitly avoiding collisions with the environment. For our\nexperiments, we use a simulated pendulum where we observe its motion from\nimages and a 7-DoF anthropomorphic arm.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 21:21:48 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Chen", "Nutan", ""], ["Klushyn", "Alexej", ""], ["Paraschos", "Alexandros", ""], ["Benbouzid", "Djalel", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1808.02078", "submitter": "Francisco Ruiz", "authors": "Michalis K. Titsias and Francisco J. R. Ruiz", "title": "Unbiased Implicit Variational Inference", "comments": "9 pages, 3 figures", "journal-ref": "Artificial Intelligence and Statistics (AISTATS 2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop unbiased implicit variational inference (UIVI), a method that\nexpands the applicability of variational inference by defining an expressive\nvariational family. UIVI considers an implicit variational distribution\nobtained in a hierarchical manner using a simple reparameterizable distribution\nwhose variational parameters are defined by arbitrarily flexible deep neural\nnetworks. Unlike previous works, UIVI directly optimizes the evidence lower\nbound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on\nseveral models, including Bayesian multinomial logistic regression and\nvariational autoencoders, and show that UIVI achieves both tighter ELBO and\nbetter predictive performance than existing approaches at a similar\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 19:28:26 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 19:35:20 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 18:53:59 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Ruiz", "Francisco J. R.", ""]]}, {"id": "1808.02083", "submitter": "Victor Minden", "authors": "Andrea Giovannucci and Victor Minden and Cengiz Pehlevan and Dmitri B.\n  Chklovskii", "title": "Efficient Principal Subspace Projection of Streaming Data Through Fast\n  Similarity Matching", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data problems frequently require processing datasets in a streaming\nfashion, either because all data are available at once but collectively are\nlarger than available memory or because the data intrinsically arrive one data\npoint at a time and must be processed online. Here, we introduce a\ncomputationally efficient version of similarity matching, a framework for\nonline dimensionality reduction that incrementally estimates the top\nK-dimensional principal subspace of streamed data while keeping in memory only\nthe last sample and the current iterate. To assess the performance of our\napproach, we construct and make public a test suite containing both a synthetic\ndata generator and the infrastructure to test online dimensionality reduction\nalgorithms on real datasets, as well as performant implementations of our\nalgorithm and competing algorithms with similar aims. Among the algorithms\nconsidered we find our approach to be competitive, performing among the best on\nboth synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 19:37:56 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Giovannucci", "Andrea", ""], ["Minden", "Victor", ""], ["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1808.02093", "submitter": "Dj Strouse", "authors": "DJ Strouse, Max Kleiman-Weiner, Josh Tenenbaum, Matt Botvinick, David\n  Schwab", "title": "Learning to Share and Hide Intentions using Information Regularization", "comments": "Presented at the 32nd Conference on Neural Information Processing\n  Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.LG cs.MA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to cooperate with friends and compete with foes is a key component\nof multi-agent reinforcement learning. Typically to do so, one requires access\nto either a model of or interaction with the other agent(s). Here we show how\nto learn effective strategies for cooperation and competition in an asymmetric\ninformation game with no such model or interaction. Our approach is to\nencourage an agent to reveal or hide their intentions using an\ninformation-theoretic regularizer. We consider both the mutual information\nbetween goal and action given state, as well as the mutual information between\ngoal and state. We show how to optimize these regularizers in a way that is\neasy to integrate with policy gradient reinforcement learning. Finally, we\ndemonstrate that cooperative (competitive) policies learned with our approach\nlead to more (less) reward for a second agent in two simple asymmetric\ninformation games.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 20:10:27 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 23:54:47 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Strouse", "DJ", ""], ["Kleiman-Weiner", "Max", ""], ["Tenenbaum", "Josh", ""], ["Botvinick", "Matt", ""], ["Schwab", "David", ""]]}, {"id": "1808.02096", "submitter": "Huiguang He", "authors": "Changde Du, Changying Du, Hao Wang, Jinpeng Li, Wei-Long Zheng,\n  Bao-Liang Lu, Huiguang He", "title": "Semi-supervised Deep Generative Modelling of Incomplete Multi-Modality\n  Emotional Data", "comments": "arXiv admin note: text overlap with arXiv:1704.07548, 2018 ACM\n  Multimedia Conference (MM'18)", "journal-ref": null, "doi": "10.1145/3240508.3240528", "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are threefold challenges in emotion recognition. First, it is difficult\nto recognize human's emotional states only considering a single modality.\nSecond, it is expensive to manually annotate the emotional data. Third,\nemotional data often suffers from missing modalities due to unforeseeable\nsensor malfunction or configuration issues. In this paper, we address all these\nproblems under a novel multi-view deep generative framework. Specifically, we\npropose to model the statistical relationships of multi-modality emotional data\nusing multiple modality-specific generative networks with a shared latent\nspace. By imposing a Gaussian mixture assumption on the posterior approximation\nof the shared latent variables, our framework can learn the joint deep\nrepresentation from multiple modalities and evaluate the importance of each\nmodality simultaneously. To solve the labeled-data-scarcity problem, we extend\nour multi-view model to semi-supervised learning scenario by casting the\nsemi-supervised classification problem as a specialized missing data imputation\ntask. To address the missing-modality problem, we further extend our\nsemi-supervised multi-view model to deal with incomplete data, where a missing\nview is treated as a latent variable and integrated out during inference. This\nway, the proposed overall framework can utilize all available (both labeled and\nunlabeled, as well as both complete and incomplete) data to improve its\ngeneralization ability. The experiments conducted on two real multi-modal\nemotion datasets demonstrated the superiority of our framework.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 07:07:36 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Du", "Changde", ""], ["Du", "Changying", ""], ["Wang", "Hao", ""], ["Li", "Jinpeng", ""], ["Zheng", "Wei-Long", ""], ["Lu", "Bao-Liang", ""], ["He", "Huiguang", ""]]}, {"id": "1808.02113", "submitter": "Cynthia Freeman", "authors": "Cynthia Freeman, Jonathan Merriman, Abhinav Aggarwal, Ian Beaver,\n  Abdullah Mueen", "title": "Paying Attention to Attention: Highlighting Influential Samples in\n  Sequential Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In (Yang et al. 2016), a hierarchical attention network (HAN) is created for\ndocument classification. The attention layer can be used to visualize text\ninfluential in classifying the document, thereby explaining the model's\nprediction. We successfully applied HAN to a sequential analysis task in the\nform of real-time monitoring of turn taking in conversations. However, we\ndiscovered instances where the attention weights were uniform at the stopping\npoint (indicating all turns were equivalently influential to the classifier),\npreventing meaningful visualization for real-time human review or classifier\nimprovement. We observed that attention weights for turns fluctuated as the\nconversations progressed, indicating turns had varying influence based on\nconversation state. Leveraging this observation, we develop a method to create\nmore informative real-time visuals (as confirmed by human reviewers) in cases\nof uniform attention weights using the changes in turn importance as a\nconversation progresses over time.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 21:05:55 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Freeman", "Cynthia", ""], ["Merriman", "Jonathan", ""], ["Aggarwal", "Abhinav", ""], ["Beaver", "Ian", ""], ["Mueen", "Abdullah", ""]]}, {"id": "1808.02123", "submitter": "Nandini Ramanan", "authors": "Nandini Ramanan, Gautam Kunapuli, Tushar Khot, Bahare Fatemi, Seyed\n  Mehran Kazemi, David Poole, Kristian Kersting, Sriraam Natarajan", "title": "Structure Learning for Relational Logistic Regression: An Ensemble\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning Relational Logistic Regression (RLR).\nUnlike standard logistic regression, the features of RLRs are first-order\nformulae with associated weight vectors instead of scalar weights. We turn the\nproblem of learning RLR to learning these vector-weighted formulae and develop\na learning algorithm based on the recently successful functional-gradient\nboosting methods for probabilistic logic models. We derive the functional\ngradients and show how weights can be learned simultaneously in an efficient\nmanner. Our empirical evaluation on standard and novel data sets demonstrates\nthe superiority of our approach over other methods for learning RLR.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 21:27:05 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Ramanan", "Nandini", ""], ["Kunapuli", "Gautam", ""], ["Khot", "Tushar", ""], ["Fatemi", "Bahare", ""], ["Kazemi", "Seyed Mehran", ""], ["Poole", "David", ""], ["Kersting", "Kristian", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1808.02129", "submitter": "Daniele Ramazzotti", "authors": "Francesco Bonchi and Francesco Gullo and Bud Mishra and Daniele\n  Ramazzotti", "title": "Probabilistic Causal Analysis of Social Influence", "comments": null, "journal-ref": "CIKM 18, October 22-26, 2018, Torino, Italy", "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mastering the dynamics of social influence requires separating, in a database\nof information propagation traces, the genuine causal processes from temporal\ncorrelation, i.e., homophily and other spurious causes. However, most studies\nto characterize social influence, and, in general, most data-science analyses\nfocus on correlations, statistical independence, or conditional independence.\nOnly recently, there has been a resurgence of interest in \"causal data\nscience\", e.g., grounded on causality theories. In this paper we adopt a\nprincipled causal approach to the analysis of social influence from\ninformation-propagation data, rooted in the theory of probabilistic causation.\n  Our approach consists of two phases. In the first one, in order to avoid the\npitfalls of misinterpreting causation when the data spans a mixture of several\nsubtypes (\"Simpson's paradox\"), we partition the set of propagation traces into\ngroups, in such a way that each group is as less contradictory as possible in\nterms of the hierarchical structure of information propagation. To achieve this\ngoal, we borrow the notion of \"agony\" and define the Agony-bounded Partitioning\nproblem, which we prove being hard, and for which we develop two efficient\nalgorithms with approximation guarantees. In the second phase, for each group\nfrom the first phase, we apply a constrained MLE approach to ultimately learn a\nminimal causal topology. Experiments on synthetic data show that our method is\nable to retrieve the genuine causal arcs w.r.t. a ground-truth generative\nmodel. Experiments on real data show that, by focusing only on the extracted\ncausal structures instead of the whole social graph, the effectiveness of\npredicting influence spread is significantly improved.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 21:44:06 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 23:31:44 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Bonchi", "Francesco", ""], ["Gullo", "Francesco", ""], ["Mishra", "Bud", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1808.02169", "submitter": "Xuanqing Liu", "authors": "Xuanqing Liu, Cho-Jui Hsieh", "title": "Fast Variance Reduction Method with Stochastic Batch Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a family of variance reduction methods with randomized\nbatch size---at each step, the algorithm first randomly chooses the batch size\nand then selects a batch of samples to conduct a variance-reduced stochastic\nupdate. We give the linear convergence rate for this framework for composite\nfunctions, and show that the optimal strategy to achieve the optimal\nconvergence rate per data access is to always choose batch size of 1, which is\nequivalent to the SAGA algorithm. However, due to the presence of cache/disk IO\neffect in computer architecture, the number of data access cannot reflect the\nrunning time because of 1) random memory access is much slower than sequential\naccess, 2) when data is too big to fit into memory, disk seeking takes even\nlonger time. After taking these into account, choosing batch size of $1$ is no\nlonger optimal, so we propose a new algorithm called SAGA++ and show how to\ncalculate the optimal average batch size theoretically. Our algorithm\noutperforms SAGA and other existing batched and stochastic solvers on real\ndatasets. In addition, we also conduct a precise analysis to compare different\nupdate rules for variance reduction methods, showing that SAGA++ converges\nfaster than SVRG in theory.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 00:49:24 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Liu", "Xuanqing", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1808.02174", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Cody Freitag, Himanshu Tyagi", "title": "Test without Trust: Optimal Locally Private Distribution Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DM cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distribution testing when the samples can only be\naccessed using a locally differentially private mechanism and focus on two\nrepresentative testing questions of identity (goodness-of-fit) and independence\ntesting for discrete distributions. We are concerned with two settings: First,\nwhen we insist on using an already deployed, general-purpose locally\ndifferentially private mechanism such as the popular RAPPOR or the recently\nintroduced Hadamard Response for collecting data, and must build our tests\nbased on the data collected via this mechanism; and second, when no such\nrestriction is imposed, and we can design a bespoke mechanism specifically for\ntesting. For the latter purpose, we introduce the Randomized Aggregated Private\nTesting Optimal Response (RAPTOR) mechanism which is remarkably simple and\nrequires only one bit of communication per sample.\n  We propose tests based on these mechanisms and analyze their sample\ncomplexities. Each proposed test can be implemented efficiently. In each case\n(barring one), we complement our performance bounds for algorithms with\ninformation-theoretic lower bounds and establish sample optimality of our\nproposed algorithm. A peculiar feature that emerges is that our sample-optimal\nalgorithm based on RAPTOR uses public-coins, and any test based on RAPPOR or\nHadamard Response, which are both private-coin mechanisms, requires\nsignificantly more samples.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 01:18:57 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Freitag", "Cody", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1808.02180", "submitter": "Dacheng Tao", "authors": "Fengxiang He, Tongliang Liu, Geoffrey I Webb, and Dacheng Tao", "title": "Instance-Dependent PU Learning by Bayesian Optimal Relabeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning from positive and unlabelled data, it is a strong assumption\nthat the positive observations are randomly sampled from the distribution of\n$X$ conditional on $Y = 1$, where X stands for the feature and Y the label.\nMost existing algorithms are optimally designed under the assumption. However,\nfor many real-world applications, the observed positive examples are dependent\non the conditional probability $P(Y = 1|X)$ and should be sampled biasedly. In\nthis paper, we assume that a positive example with a higher $P(Y = 1|X)$ is\nmore likely to be labelled and propose a probabilistic-gap based PU learning\nalgorithms. Specifically, by treating the unlabelled data as noisy negative\nexamples, we could automatically label a group positive and negative examples\nwhose labels are identical to the ones assigned by a Bayesian optimal\nclassifier with a consistency guarantee. The relabelled examples have a biased\ndomain, which is remedied by the kernel mean matching technique. The proposed\nalgorithm is model-free and thus do not have any parameters to tune.\nExperimental results demonstrate that our method works well on both generated\nand real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 01:47:57 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 02:47:49 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["He", "Fengxiang", ""], ["Liu", "Tongliang", ""], ["Webb", "Geoffrey I", ""], ["Tao", "Dacheng", ""]]}, {"id": "1808.02213", "submitter": "Paul Atzberger", "authors": "Paul J. Atzberger", "title": "Importance of the Mathematical Foundations of Machine Learning Methods\n  for Scientific and Engineering Applications", "comments": "Position Paper at SciML2018 Workshop, US Department of Energy,\n  January 2018, (two-page limit)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of recent interest in adopting machine learning methods\nfor scientific and engineering applications. This has in large part been\ninspired by recent successes and advances in the domains of Natural Language\nProcessing (NLP) and Image Classification (IC). However, scientific and\nengineering problems have their own unique characteristics and requirements\nraising new challenges for effective design and deployment of machine learning\napproaches. There is a strong need for further mathematical developments on the\nfoundations of machine learning methods to increase the level of rigor of\nemployed methods and to ensure more reliable and interpretable results. Also as\nreported in the recent literature on state-of-the-art results and indicated by\nthe No Free Lunch Theorems of statistical learning theory incorporating some\nform of inductive bias and domain knowledge is essential to success.\nConsequently, even for existing and widely used methods there is a strong need\nfor further mathematical work to facilitate ways to incorporate prior\nscientific knowledge and related inductive biases into learning frameworks and\nalgorithms. We briefly discuss these topics and discuss some ideas proceeding\nin this direction.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 05:09:13 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Atzberger", "Paul J.", ""]]}, {"id": "1808.02227", "submitter": "Rad Niazadeh", "authors": "Moses Charikar, Vaggos Chatziafratis, Rad Niazadeh", "title": "Hierarchical Clustering better than Average-Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Clustering (HC) is a widely studied problem in exploratory data\nanalysis, usually tackled by simple agglomerative procedures like\naverage-linkage, single-linkage or complete-linkage. In this paper we focus on\ntwo objectives, introduced recently to give insight into the performance of\naverage-linkage clustering: a similarity based HC objective proposed by\n[Moseley and Wang, 2017] and a dissimilarity based HC objective proposed by\n[Cohen-Addad et al., 2018]. In both cases, we present tight counterexamples\nshowing that average-linkage cannot obtain better than 1/3 and 2/3\napproximations respectively (in the worst-case), settling an open question\nraised in [Moseley and Wang, 2017]. This matches the approximation ratio of a\nrandom solution, raising a natural question: can we beat average-linkage for\nthese objectives? We answer this in the affirmative, giving two new algorithms\nbased on semidefinite programming with provably better guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 06:47:01 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Charikar", "Moses", ""], ["Chatziafratis", "Vaggos", ""], ["Niazadeh", "Rad", ""]]}, {"id": "1808.02229", "submitter": "Guangxu Zhu", "authors": "Jiayao Zhang and Guangxu Zhu and Robert W. Heath Jr. and Kaibin Huang", "title": "Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep\n  Learning", "comments": "Submitted to IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning algorithms have been adopted in a range of\nsignal-processing applications spanning computer vision, natural language\nprocessing, and artificial intelligence. Many relevant problems involve\nsubspace-structured features, orthogonality constrained or low-rank constrained\nobjective functions, or subspace distances. These mathematical characteristics\nare expressed naturally using the Grassmann manifold. Unfortunately, this fact\nis not yet explored in many traditional learning algorithms. In the last few\nyears, there have been growing interests in studying Grassmann manifold to\ntackle new learning problems. Such attempts have been reassured by substantial\nperformance improvements in both classic learning and learning using deep\nneural networks. We term the former as shallow and the latter deep Grassmannian\nlearning. The aim of this paper is to introduce the emerging area of\nGrassmannian learning by surveying common mathematical problems and primary\nsolution approaches, and overviewing various applications. We hope to inspire\npractitioners in different fields to adopt the powerful tool of Grassmannian\nlearning in their research.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 06:54:06 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 02:19:08 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Zhang", "Jiayao", ""], ["Zhu", "Guangxu", ""], ["Heath", "Robert W.", "Jr."], ["Huang", "Kaibin", ""]]}, {"id": "1808.02234", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Dianhui Wang", "title": "Deep Stacked Stochastic Configuration Networks for Lifelong Learning of\n  Non-Stationary Data Streams", "comments": "This paper has been published in Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of SCN offers a fast framework with universal approximation\nguarantee for lifelong learning of non-stationary data streams. Its adaptive\nscope selection property enables for proper random generation of hidden unit\nparameters advancing conventional randomized approaches constrained with a\nfixed scope of random parameters. This paper proposes deep stacked stochastic\nconfiguration network (DSSCN) for continual learning of non-stationary data\nstreams which contributes two major aspects: 1) DSSCN features a\nself-constructing methodology of deep stacked network structure where hidden\nunit and hidden layer are extracted automatically from continuously generated\ndata streams; 2) the concept of SCN is developed to randomly assign inverse\ncovariance matrix of multivariate Gaussian function in the hidden node addition\nstep bypassing its computationally prohibitive tuning phase. Numerical\nevaluation and comparison with prominent data stream algorithms under two\nprocedures: periodic hold-out and prequential test-then-train processes\ndemonstrate the advantage of proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 07:15:54 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 15:44:25 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 13:24:25 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Wang", "Dianhui", ""]]}, {"id": "1808.02237", "submitter": "Behrooz Azarkhalili", "authors": "Behrooz Azarkhalili, Ali Saberi, Hamidreza Chitsaz, and Ali\n  Sharifi-Zarchi", "title": "DeePathology: Deep Multi-Task Learning for Inferring Molecular Pathology\n  from Cancer Transcriptome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite great advances, molecular cancer pathology is often limited to the\nuse of a small number of biomarkers rather than the whole transcriptome, partly\ndue to computational challenges. Here, we introduce a novel architecture of\nDeep Neural Networks (DNNs) that is capable of simultaneous inference of\nvarious properties of biological samples, through multi-task and transfer\nlearning. It encodes the whole transcription profile into a strikingly\nlow-dimensional latent vector of size 8, and then recovers mRNA and miRNA\nexpression profiles, tissue and disease type from this vector. This latent\nspace is significantly better than the original gene expression profiles for\ndiscriminating samples based on their tissue and disease. We employed this\narchitecture on mRNA transcription profiles of 10787 clinical samples from 34\nclasses (one healthy and 33 different types of cancer) from 27 tissues. Our\nmethod significantly outperforms prior works and classical machine learning\napproaches in predicting tissue-of-origin, normal or disease state and cancer\ntype of each sample. For tissues with more than one type of cancer, it reaches\n99.4\\% accuracy in identifying the correct cancer subtype. We also show this\nsystem is very robust against noise and missing values. Collectively, our\nresults highlight applications of artificial intelligence in molecular cancer\npathology and oncological research. DeePathology is freely available at\n\\url{https://github.com/SharifBioinf/DeePathology}.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 07:26:39 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 13:41:13 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Azarkhalili", "Behrooz", ""], ["Saberi", "Ali", ""], ["Chitsaz", "Hamidreza", ""], ["Sharifi-Zarchi", "Ali", ""]]}, {"id": "1808.02240", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura, Deniz Gunduz, Sennur Ulukus", "title": "Speeding Up Distributed Gradient Descent by Utilizing Non-persistent\n  Stragglers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed gradient descent (DGD) is an efficient way of implementing\ngradient descent (GD), especially for large data sets, by dividing the\ncomputation tasks into smaller subtasks and assigning to different computing\nservers (CSs) to be executed in parallel. In standard parallel execution,\nper-iteration waiting time is limited by the execution time of the straggling\nservers. Coded DGD techniques have been introduced recently, which can tolerate\nstraggling servers via assigning redundant computation tasks to the CSs. In\nmost of the existing DGD schemes, either with coded computation or coded\ncommunication, the non-straggling CSs transmit one message per iteration once\nthey complete all their assigned computation tasks. However, although the\nstraggling servers cannot complete all their assigned tasks, they are often\nable to complete a certain portion of them. In this paper, we allow multiple\ntransmissions from each CS at each iteration in order to make sure a maximum\nnumber of completed computations can be reported to the aggregating server\n(AS), including the straggling servers. We numerically show that the average\ncompletion time per iteration can be reduced significantly by slightly\nincreasing the communication load per server.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 07:49:25 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 21:16:43 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 14:30:26 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Ozfatura", "Emre", ""], ["Gunduz", "Deniz", ""], ["Ulukus", "Sennur", ""]]}, {"id": "1808.02266", "submitter": "Kai Chen", "authors": "Kai Chen, Perry Groot, Jinsong Chen, and Elena Marchiori", "title": "Multi-Output Convolution Spectral Mixture for Gaussian Processes", "comments": "14 pages, 26 figures. arXiv admin note: text overlap with\n  arXiv:1808.01132", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output Gaussian processes (MOGPs) are recently extended by using\nspectral mixture kernel, which enables expressively pattern extrapolation with\na strong interpretation. In particular, Multi-Output Spectral Mixture kernel\n(MOSM) is a recent, powerful state of the art method. However, MOSM cannot\nreduce to the ordinary spectral mixture kernel (SM) when using a single\nchannel. Moreover, when the spectral density of different channels is either\nvery close or very far from each other in the frequency domain, MOSM generates\nunreasonable scale effects on cross weights which produces an incorrect\ndescription of the channel correlation structure. In this paper, we tackle\nthese drawbacks and introduce a principled multi-output convolution spectral\nmixture kernel (MOCSM) framework. In our framework, we model channel\ndependencies through convolution of time and phase delayed spectral mixtures\nbetween different channels. Results of extensive experiments on synthetic and\nreal datasets demontrate the advantages of MOCSM and its state of the art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 09:01:05 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 15:09:32 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 16:51:38 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 08:42:17 GMT"}, {"version": "v5", "created": "Sun, 14 Oct 2018 13:37:35 GMT"}, {"version": "v6", "created": "Mon, 17 Dec 2018 11:04:44 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Chen", "Kai", ""], ["Groot", "Perry", ""], ["Chen", "Jinsong", ""], ["Marchiori", "Elena", ""]]}, {"id": "1808.02334", "submitter": "Herman Shen", "authors": "Sharad Rawat and M.H. Herman Shen", "title": "A novel topology design approach using an integrated deep learning\n  network architecture", "comments": "15 pages, 6 Figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topology design optimization offers tremendous opportunity in design and\nmanufacturing freedoms by designing and producing a part from the ground-up\nwithout a meaningful initial design as required by conventional shape design\noptimization approaches. Ideally, with adequate problem statements, to\nformulate and solve the topology design problem using a standard topology\noptimization process, such as SIMP (Simplified Isotropic Material with\nPenalization) is possible. In reality, an estimated over thousands of design\niterations is often required for just a few design variables, the conventional\noptimization approach is in general impractical or computationally unachievable\nfor real world applications significantly diluting the development of the\ntopology optimization technology. There is, therefore, a need for a different\napproach that will be able to optimize the initial design topology effectively\nand rapidly. Therefore, this work presents a new topology design procedure to\ngenerate optimal structures using an integrated Generative Adversarial Networks\n(GANs) and convolutional neural network architecture.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 19:10:39 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 15:56:05 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Rawat", "Sharad", ""], ["Shen", "M. H. Herman", ""]]}, {"id": "1808.02357", "submitter": "Shayan Gharib", "authors": "Shayan Gharib, Honain Derrar, Daisuke Niizumi, Tuukka Senttula, Janne\n  Tommola, Toni Heittola, Tuomas Virtanen, Heikki Huttunen", "title": "Acoustic Scene Classification: A Competition Review", "comments": "This work has been accepted in IEEE International Workshop on Machine\n  Learning for Signal Processing (MLSP 2018). Copyright may be transferred\n  without notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of acoustic scene classification, i.e.,\ncategorization of audio sequences into mutually exclusive classes based on\ntheir spectral content. We describe the methods and results discovered during a\ncompetition organized in the context of a graduate machine learning course;\nboth by the students and external participants. We identify the most suitable\nmethods and study the impact of each by performing an ablation study of the\nmixture of approaches. We also compare the results with a neural network\nbaseline, and show the improvement over that. Finally, we discuss the impact of\nusing a competition as a part of a university course, and justify its\nimportance in the curriculum based on student feedback.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 07:40:17 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Gharib", "Shayan", ""], ["Derrar", "Honain", ""], ["Niizumi", "Daisuke", ""], ["Senttula", "Tuukka", ""], ["Tommola", "Janne", ""], ["Heittola", "Toni", ""], ["Virtanen", "Tuomas", ""], ["Huttunen", "Heikki", ""]]}, {"id": "1808.02394", "submitter": "Woongsup Lee", "authors": "Woongsup Lee, Ohyun Jo, Minhoe Kim", "title": "Application of End-to-End Deep Learning in Wireless Communications\n  Systems", "comments": "This work has been submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a potential paradigm changer for the design of wireless\ncommunications systems (WCS), from conventional handcrafted schemes based on\nsophisticated mathematical models with assumptions to autonomous schemes based\non the end-to-end deep learning using a large number of data. In this article,\nwe present a basic concept of the deep learning and its application to WCS by\ninvestigating the resource allocation (RA) scheme based on a deep neural\nnetwork (DNN) where multiple goals with various constraints can be satisfied\nthrough the end-to-end deep learning. Especially, the optimality and\nfeasibility of the DNN based RA are verified through simulation. Then, we\ndiscuss the technical challenges regarding the application of deep learning in\nWCS.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 14:20:10 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Lee", "Woongsup", ""], ["Jo", "Ohyun", ""], ["Kim", "Minhoe", ""]]}, {"id": "1808.02433", "submitter": "Francois Fagan", "authors": "Francois Fagan and Garud Iyengar", "title": "Robust Implicit Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguably the biggest challenge in applying neural networks is tuning the\nhyperparameters, in particular the learning rate. The sensitivity to the\nlearning rate is due to the reliance on backpropagation to train the network.\nIn this paper we present the first application of Implicit Stochastic Gradient\nDescent (ISGD) to train neural networks, a method known in convex optimization\nto be unconditionally stable and robust to the learning rate. Our key\ncontribution is a novel layer-wise approximation of ISGD which makes its\nupdates tractable for neural networks. Experiments show that our method is more\nrobust to high learning rates and generally outperforms standard\nbackpropagation on a variety of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 15:59:08 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Fagan", "Francois", ""], ["Iyengar", "Garud", ""]]}, {"id": "1808.02435", "submitter": "Luisa Isabel Mart\\'inez Merino", "authors": "Martine Labb\\'e, Luisa I. Mart\\'inez-Merino, Antonio M.\n  Rodr\\'iguez-Ch\\'ia", "title": "Mixed Integer Linear Programming for Feature Selection in Support Vector\n  Machine", "comments": "37 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on support vector machine (SVM) with feature selection. A\nMILP formulation is proposed for the problem. The choice of suitable features\nto construct the separating hyperplanes has been modelled in this formulation\nby including a budget constraint that sets in advance a limit on the number of\nfeatures to be used in the classification process. We propose both an exact and\na heuristic procedure to solve this formulation in an efficient way. Finally,\nthe validation of the model is done by checking it with some well-known data\nsets and comparing it with classical classification methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 15:59:17 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Labb\u00e9", "Martine", ""], ["Mart\u00ednez-Merino", "Luisa I.", ""], ["Rodr\u00edguez-Ch\u00eda", "Antonio M.", ""]]}, {"id": "1808.02458", "submitter": "Yannai A. Gonczarowski", "authors": "Yannai A. Gonczarowski and S. Matthew Weinberg", "title": "The Sample Complexity of Up-to-$\\varepsilon$ Multi-Dimensional Revenue\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sample complexity of revenue maximization for multiple\nbidders in unrestricted multi-dimensional settings. Specifically, we study the\nstandard model of $n$ additive bidders whose values for $m$ heterogeneous items\nare drawn independently. For any such instance and any $\\varepsilon>0$, we show\nthat it is possible to learn an $\\varepsilon$-Bayesian Incentive Compatible\nauction whose expected revenue is within $\\varepsilon$ of the optimal\n$\\varepsilon$-BIC auction from only polynomially many samples.\n  Our fully nonparametric approach is based on ideas that hold quite generally,\nand completely sidestep the difficulty of characterizing optimal (or\nnear-optimal) auctions for these settings. Therefore, our results easily extend\nto general multi-dimensional settings, including valuations that are not\nnecessarily even subadditive, and arbitrary allocation constraints. For the\ncases of a single bidder and many goods, or a single parameter (good) and many\nbidders, our analysis yields exact incentive compatibility (and for the latter\nalso computational efficiency). Although the single-parameter case is already\nwell-understood, our corollary for this case extends slightly the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 16:59:16 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 12:02:46 GMT"}, {"version": "v3", "created": "Sun, 6 Sep 2020 21:07:13 GMT"}, {"version": "v4", "created": "Fri, 9 Apr 2021 21:23:15 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gonczarowski", "Yannai A.", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1808.02474", "submitter": "Yuhong Guo", "authors": "Meng Ye and Yuhong Guo", "title": "Multi-Label Zero-Shot Learning with Transfer-Aware Label Embedding\n  Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning transfers knowledge from seen classes to novel unseen\nclasses to reduce human labor of labelling data for building new classifiers.\nMuch effort on zero-shot learning however has focused on the standard\nmulti-class setting, the more challenging multi-label zero-shot problem has\nreceived limited attention. In this paper we propose a transfer-aware embedding\nprojection approach to tackle multi-label zero-shot learning. The approach\nprojects the label embedding vectors into a low-dimensional space to induce\nbetter inter-label relationships and explicitly facilitate information transfer\nfrom seen labels to unseen labels, while simultaneously learning a max-margin\nmulti-label classifier with the projected label embeddings. Auxiliary\ninformation can be conveniently incorporated to guide the label embedding\nprojection to further improve label relation structures for zero-shot knowledge\ntransfer. We conduct experiments for zero-shot multi-label image\nclassification. The results demonstrate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 17:48:40 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Ye", "Meng", ""], ["Guo", "Yuhong", ""]]}, {"id": "1808.02480", "submitter": "Golan Pundak", "authors": "Golan Pundak, Tara N. Sainath, Rohit Prabhavalkar, Anjuli Kannan, Ding\n  Zhao", "title": "Deep context: end-to-end contextual speech recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automatic speech recognition (ASR) what a user says depends on the\nparticular context she is in. Typically, this context is represented as a set\nof word n-grams. In this work, we present a novel, all-neural, end-to-end (E2E)\nASR sys- tem that utilizes such context. Our approach, which we re- fer to as\nContextual Listen, Attend and Spell (CLAS) jointly- optimizes the ASR\ncomponents along with embeddings of the context n-grams. During inference, the\nCLAS system can be presented with context phrases which might contain out-of-\nvocabulary (OOV) terms not seen during training. We com- pare our proposed\nsystem to a more traditional contextualiza- tion approach, which performs\nshallow-fusion between inde- pendently trained LAS and contextual n-gram models\nduring beam search. Across a number of tasks, we find that the pro- posed CLAS\nsystem outperforms the baseline method by as much as 68% relative WER,\nindicating the advantage of joint optimization over individually trained\ncomponents. Index Terms: speech recognition, sequence-to-sequence models,\nlisten attend and spell, LAS, attention, embedded speech recognition.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 21:23:21 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Pundak", "Golan", ""], ["Sainath", "Tara N.", ""], ["Prabhavalkar", "Rohit", ""], ["Kannan", "Anjuli", ""], ["Zhao", "Ding", ""]]}, {"id": "1808.02510", "submitter": "Giannis Nikolentzos", "authors": "Giannis Nikolentzos, Michalis Vazirgiannis", "title": "Message Passing Graph Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph kernels have recently emerged as a promising approach for tackling the\ngraph similarity and learning tasks at the same time. In this paper, we propose\na general framework for designing graph kernels. The proposed framework\ncapitalizes on the well-known message passing scheme on graphs. The kernels\nderived from the framework consist of two components. The first component is a\nkernel between vertices, while the second component is a kernel between graphs.\nThe main idea behind the proposed framework is that the representations of the\nvertices are implicitly updated using an iterative procedure. Then, these\nrepresentations serve as the building blocks of a kernel that compares pairs of\ngraphs. We derive four instances of the proposed framework, and show through\nextensive experiments that these instances are competitive with\nstate-of-the-art methods in various tasks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 18:37:13 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Nikolentzos", "Giannis", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1808.02513", "submitter": "Babak Zamirai", "authors": "Parker Hill, Babak Zamirai, Shengshuo Lu, Yu-Wei Chao, Michael\n  Laurenzano, Mehrzad Samadi, Marios Papaefthymiou, Scott Mahlke, Thomas\n  Wenisch, Jia Deng, Lingjia Tang, Jason Mars", "title": "Rethinking Numerical Representations for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With ever-increasing computational demand for deep learning, it is critical\nto investigate the implications of the numeric representation and precision of\nDNN model weights and activations on computational efficiency. In this work, we\nexplore unconventional narrow-precision floating-point representations as it\nrelates to inference accuracy and efficiency to steer the improved design of\nfuture DNN platforms. We show that inference using these custom numeric\nrepresentations on production-grade DNNs, including GoogLeNet and VGG, achieves\nan average speedup of 7.6x with less than 1% degradation in inference accuracy\nrelative to a state-of-the-art baseline platform representing the most\nsophisticated hardware using single-precision floating point. To facilitate the\nuse of such customized precision, we also present a novel technique that\ndrastically reduces the time required to derive the optimal precision\nconfiguration.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 18:42:09 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Hill", "Parker", ""], ["Zamirai", "Babak", ""], ["Lu", "Shengshuo", ""], ["Chao", "Yu-Wei", ""], ["Laurenzano", "Michael", ""], ["Samadi", "Mehrzad", ""], ["Papaefthymiou", "Marios", ""], ["Mahlke", "Scott", ""], ["Wenisch", "Thomas", ""], ["Deng", "Jia", ""], ["Tang", "Lingjia", ""], ["Mars", "Jason", ""]]}, {"id": "1808.02546", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, Silvio Lattanzi, Vahab Mirrokni", "title": "Parallel and Streaming Algorithms for K-Core Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-core decomposition is a fundamental primitive in many machine\nlearning and data mining applications. We present the first distributed and the\nfirst streaming algorithms to compute and maintain an approximate $k$-core\ndecomposition with provable guarantees. Our algorithms achieve rigorous bounds\non space complexity while bounding the number of passes or number of rounds of\ncomputation. We do so by presenting a new powerful sketching technique for\n$k$-core decomposition, and then by showing it can be computed efficiently in\nboth streaming and MapReduce models. Finally, we confirm the effectiveness of\nour sketching technique empirically on a number of publicly available graphs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 20:31:39 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 22:24:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Lattanzi", "Silvio", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1808.02602", "submitter": "Jette Henderson", "authors": "Jette Henderson, Bradley A. Malin, Joyce C. Ho, Joydeep Ghosh", "title": "PIVETed-Granite: Computational Phenotypes through Constrained Tensor\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been recently shown that sparse, nonnegative tensor factorization of\nmulti-modal electronic health record data is a promising approach to\nhigh-throughput computational phenotyping. However, such approaches typically\ndo not leverage available domain knowledge while extracting the phenotypes;\nhence, some of the suggested phenotypes may not map well to clinical concepts\nor may be very similar to other suggested phenotypes. To address these issues,\nwe present a novel, automatic approach called PIVETed-Granite that mines\nexisting biomedical literature (PubMed) to obtain cannot-link constraints that\nare then used as side-information during a tensor-factorization based\ncomputational phenotyping process. The resulting improvements are clearly\nobserved in experiments using a large dataset from VUMC to identify phenotypes\nfor hypertensive patients.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 02:16:13 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Henderson", "Jette", ""], ["Malin", "Bradley A.", ""], ["Ho", "Joyce C.", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1808.02610", "submitter": "Jianbo Chen", "authors": "Jianbo Chen, Le Song, Martin J. Wainwright, Michael I. Jordan", "title": "L-Shapley and C-Shapley: Efficient Model Interpretation for Structured\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study instancewise feature importance scoring as a method for model\ninterpretation. Any such method yields, for each predicted instance, a vector\nof importance scores associated with the feature vector. Methods based on the\nShapley score have been proposed as a fair way of computing feature\nattributions of this kind, but incur an exponential complexity in the number of\nfeatures. This combinatorial explosion arises from the definition of the\nShapley value and prevents these methods from being scalable to large data sets\nand complex models. We focus on settings in which the data have a graph\nstructure, and the contribution of features to the target variable is\nwell-approximated by a graph-structured factorization. In such settings, we\ndevelop two algorithms with linear complexity for instancewise feature\nimportance scoring. We establish the relationship of our methods to the Shapley\nvalue and another closely related concept known as the Myerson value from\ncooperative game theory. We demonstrate on both language and image data that\nour algorithms compare favorably with other methods for model interpretation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 03:10:24 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Chen", "Jianbo", ""], ["Song", "Le", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1808.02633", "submitter": "Liting Sun", "authors": "Liting Sun, Wei Zhan, Masayoshi Tomizuka, and Anca D. Dragan", "title": "Courteous Autonomous Cars", "comments": "International Conference on Intelligent Robots (IROS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, autonomous cars optimize for a combination of safety, efficiency,\nand driving quality. But as we get better at this optimization, we start seeing\nbehavior go from too conservative to too aggressive. The car's behavior exposes\nthe incentives we provide in its cost function. In this work, we argue for cars\nthat are not optimizing a purely selfish cost, but also try to be courteous to\nother interactive drivers. We formalize courtesy as a term in the objective\nthat measures the increase in another driver's cost induced by the autonomous\ncar's behavior. Such a courtesy term enables the robot car to be aware of\npossible irrationality of the human behavior, and plan accordingly. We analyze\nthe effect of courtesy in a variety of scenarios. We find, for example, that\ncourteous robot cars leave more space when merging in front of a human driver.\nMoreover, we find that such a courtesy term can help explain real human driver\nbehavior on the NGSIM dataset.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 05:45:24 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 02:47:58 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Sun", "Liting", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1808.02651", "submitter": "Hsueh-Ti Derek Liu", "authors": "Hsueh-Ti Derek Liu, Michael Tao, Chun-Liang Li, Derek Nowrouzezahrai,\n  Alec Jacobson", "title": "Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically\n  Differentiable Renderer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning image classifiers are vulnerable to adversarial\nattacks, inputs with perturbations designed to intentionally trigger\nmisclassification. Current adversarial methods directly alter pixel colors and\nevaluate against pixel norm-balls: pixel perturbations smaller than a specified\nmagnitude, according to a measurement norm. This evaluation, however, has\nlimited practical utility since perturbations in the pixel space do not\ncorrespond to underlying real-world phenomena of image formation that lead to\nthem and has no security motivation attached. Pixels in natural images are\nmeasurements of light that has interacted with the geometry of a physical\nscene. As such, we propose the direct perturbation of physical parameters that\nunderly image formation: lighting and geometry. As such, we propose a novel\nevaluation measure, parametric norm-balls, by directly perturbing physical\nparameters that underly image formation. One enabling contribution we present\nis a physically-based differentiable renderer that allows us to propagate pixel\ngradients to the parametric space of lighting and geometry. Our approach\nenables physically-based adversarial attacks, and our differentiable renderer\nleverages models from the interactive rendering literature to balance the\nperformance and accuracy trade-offs necessary for a memory-efficient and\nscalable adversarial data augmentation workflow.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 08:01:18 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 22:12:01 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Liu", "Hsueh-Ti Derek", ""], ["Tao", "Michael", ""], ["Li", "Chun-Liang", ""], ["Nowrouzezahrai", "Derek", ""], ["Jacobson", "Alec", ""]]}, {"id": "1808.02776", "submitter": "Antonela Tommasel", "authors": "J. Andr\\'es D\\'iaz-Pace and Antonela Tommasel and Daniela Godoy", "title": "Can Network Analysis Techniques help to Predict Design Dependencies? An\n  Initial Study", "comments": "Accepted at ICSA 2018", "journal-ref": null, "doi": "10.1109/ICSA-C.2018.00025", "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree of dependencies among the modules of a software system is a key\nattribute to characterize its design structure and its ability to evolve over\ntime. Several design problems are often correlated with undesired dependencies\namong modules. Being able to anticipate those problems is important for\ndevelopers, so they can plan early for maintenance and refactoring efforts.\nHowever, existing tools are limited to detecting undesired dependencies once\nthey appeared in the system. In this work, we investigate whether module\ndependencies can be predicted (before they actually appear). Since the module\nstructure can be regarded as a network, i.e, a dependency graph, we leverage on\nnetwork features to analyze the dynamics of such a structure. In particular, we\napply link prediction techniques for this task. We conducted an evaluation on\ntwo Java projects across several versions, using link prediction and machine\nlearning techniques, and assessed their performance for identifying new\ndependencies from a project version to the next one. The results, although\npreliminary, show that the link prediction approach is feasible for package\ndependencies. Also, this work opens opportunities for further development of\nsoftware-specific strategies for dependency prediction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 13:35:44 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["D\u00edaz-Pace", "J. Andr\u00e9s", ""], ["Tommasel", "Antonela", ""], ["Godoy", "Daniela", ""]]}, {"id": "1808.02814", "submitter": "Berkin Bilgic", "authors": "Berkin Bilgic, Itthi Chatnuntawech, Mary Kate Manhard, Qiyuan Tian,\n  Congyu Liao, Stephen F. Cauley, Susie Y. Huang, Jonathan R. Polimeni,\n  Lawrence L. Wald, Kawin Setsompop", "title": "Highly Accelerated Multishot EPI through Synergistic Machine Learning\n  and Joint Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To introduce a combined machine learning (ML) and physics-based\nimage reconstruction framework that enables navigator-free, highly accelerated\nmultishot echo planar imaging (msEPI), and demonstrate its application in\nhigh-resolution structural and diffusion imaging.\n  Methods: Singleshot EPI is an efficient encoding technique, but does not lend\nitself well to high-resolution imaging due to severe distortion artifacts and\nblurring. While msEPI can mitigate these artifacts, high-quality msEPI has been\nelusive because of phase mismatch arising from shot-to-shot variations which\npreclude the combination of the multiple-shot data into a single image. We\nemploy deep learning to obtain an interim image with minimal artifacts, which\npermits estimation of image phase variations due to shot-to-shot changes. These\nvariations are then included in a Joint Virtual Coil Sensitivity Encoding\n(JVC-SENSE) reconstruction to utilize data from all shots and improve upon the\nML solution.\n  Results: Our combined ML + physics approach enabled Rinplane x MultiBand (MB)\n= 8x2-fold acceleration using 2 EPI-shots for multi-echo imaging, so that\nwhole-brain T2 and T2* parameter maps could be derived from an 8.3 sec\nacquisition at 1x1x3mm3 resolution. This has also allowed high-resolution\ndiffusion imaging with high geometric fidelity using 5-shots at Rinplane x MB =\n9x2-fold acceleration. To make these possible, we extended the state-of-the-art\nMUSSELS reconstruction technique to Simultaneous MultiSlice (SMS) encoding and\nused it as an input to our ML network.\n  Conclusion: Combination of ML and JVC-SENSE enabled navigator-free msEPI at\nhigher accelerations than previously possible while using fewer shots, with\nreduced vulnerability to poor generalizability and poor acceptance of\nend-to-end ML approaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 15:15:29 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 15:52:17 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 20:20:46 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Bilgic", "Berkin", ""], ["Chatnuntawech", "Itthi", ""], ["Manhard", "Mary Kate", ""], ["Tian", "Qiyuan", ""], ["Liao", "Congyu", ""], ["Cauley", "Stephen F.", ""], ["Huang", "Susie Y.", ""], ["Polimeni", "Jonathan R.", ""], ["Wald", "Lawrence L.", ""], ["Setsompop", "Kawin", ""]]}, {"id": "1808.02822", "submitter": "Maximilian Alber", "authors": "Maximilian Alber, Irwan Bello, Barret Zoph, Pieter-Jan Kindermans,\n  Prajit Ramachandran, Quoc Le", "title": "Backprop Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The back-propagation algorithm is the cornerstone of deep learning. Despite\nits importance, few variations of the algorithm have been attempted. This work\npresents an approach to discover new variations of the back-propagation\nequation. We use a domain specific lan- guage to describe update equations as a\nlist of primitive functions. An evolution-based method is used to discover new\npropagation rules that maximize the generalization per- formance after a few\nepochs of training. We find several update equations that can train faster with\nshort training times than standard back-propagation, and perform similar as\nstandard back-propagation at convergence.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 15:23:14 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Alber", "Maximilian", ""], ["Bello", "Irwan", ""], ["Zoph", "Barret", ""], ["Kindermans", "Pieter-Jan", ""], ["Ramachandran", "Prajit", ""], ["Le", "Quoc", ""]]}, {"id": "1808.02838", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far and Emina Soljanin", "title": "On the Effect of Task-to-Worker Assignment in Distributed Computing\n  Systems with Stragglers", "comments": "Accepted at the 56th Annual Allerton Conference on Communication,\n  Control, and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the expected completion time of some recently proposed algorithms\nfor distributed computing which redundantly assign computing tasks to multiple\nmachines in order to tolerate a certain number of machine failures. We\nanalytically show that not only the amount of redundancy but also the\ntask-to-machine assignments affect the latency in a distributed system. We\nstudy systems with a fixed number of computing tasks that are split in possibly\noverlapping batches, and independent exponentially distributed machine service\ntimes. We show that, for such systems, the uniform replication of non-\noverlapping (disjoint) batches of computing tasks achieves the minimum expected\ncomputing time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 15:51:18 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""]]}, {"id": "1808.02871", "submitter": "L.A. Prashanth", "authors": "Prashanth L A, Shalabh Bhatnagar, Nirav Bhavsar, Michael Fu and Steven\n  I. Marcus", "title": "Random directions stochastic approximation with deterministic\n  perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce deterministic perturbation schemes for the recently proposed\nrandom directions stochastic approximation (RDSA) [17], and propose new\nfirst-order and second-order algorithms. In the latter case, these are the\nfirst second-order algorithms to incorporate deterministic perturbations. We\nshow that the gradient and/or Hessian estimates in the resulting algorithms\nwith deterministic perturbations are asymptotically unbiased, so that the\nalgorithms are provably convergent. Furthermore, we derive convergence rates to\nestablish the superiority of the first-order and second-order algorithms, for\nthe special case of a convex and quadratic optimization problem, respectively.\nNumerical experiments are used to validate the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 17:26:37 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 17:51:04 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["A", "Prashanth L", ""], ["Bhatnagar", "Shalabh", ""], ["Bhavsar", "Nirav", ""], ["Fu", "Michael", ""], ["Marcus", "Steven I.", ""]]}, {"id": "1808.02932", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga and Chris H. Wiggins", "title": "Nonparametric Gaussian Mixture Models for the Multi-Armed Contextual\n  Bandit", "comments": "The software used for this study is publicly available at\n  https://github.com/iurteaga/bandits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here adopt Bayesian nonparametric mixture models to extend multi-armed\nbandits in general, and Thompson sampling in particular, to scenarios where\nthere is reward model uncertainty. In the stochastic multi-armed bandit, where\nan agent must learn a policy that maximizes long term payoff, the reward for\nthe selected action is generated from an unknown distribution. Thompson\nsampling is a generative and interpretable multi-armed bandit algorithm that\nhas been shown both to perform well in practice, and to enjoy optimality\nproperties for certain reward functions. Nevertheless, Thompson sampling\nrequires knowledge of the true reward model, for calculation of expected\nrewards and sampling from its parameter posterior. In this work, we extend\nThompson sampling to complex scenarios where there is model uncertainty, by\nadopting a very flexible set of reward distributions: Bayesian nonparametric\nGaussian mixture models. The generative process of Bayesian nonparametric\nmixtures naturally aligns with the Bayesian modeling of multi-armed bandits:\nthe nonparametric model autonomously determines its complexity as new rewards\nare observed for the played arms. By characterizing each arm's reward\ndistribution with independent nonparametric mixture models, the proposed method\nsequentially learns the model that best approximates the true underlying reward\ndistribution, achieving successful performance in complex -- not in the\nexponential family -- bandits. Our contribution is valuable for practical\nscenarios, as it avoids stringent case-by-case model specifications and\nhyperparameter tuning, yet attains reduced regret in diverse bandit settings.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 20:40:15 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 18:13:54 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 22:02:51 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1808.02933", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga and Chris H. Wiggins", "title": "(Sequential) Importance Sampling Bandits", "comments": "The software used for this study is publicly available at\n  https://github.com/iurteaga/bandits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work extends existing multi-armed bandit (MAB) algorithms beyond their\noriginal settings by leveraging advances in sequential Monte Carlo (SMC)\nmethods from the approximate inference community. We leverage Monte Carlo\nestimation and, in particular, the flexibility of (sequential) importance\nsampling to allow for accurate estimation of the statistics of interest within\nthe MAB problem. The MAB is a sequential allocation task where the goal is to\nlearn a policy that maximizes long term payoff, where only the reward of the\nexecuted action is observed; i.e., sequential optimal decisions are made, while\nsimultaneously learning how the world operates. In the stochastic setting, the\nreward for each action is generated from an unknown distribution. To decide the\nnext optimal action to take, one must compute sufficient statistics of this\nunknown reward distribution, e.g., upper-confidence bounds (UCB), or\nexpectations in Thompson sampling. Closed-form expressions for these statistics\nof interest are analytically intractable except for simple cases. By combining\nSMC methods --- which estimate posterior densities and expectations in\nprobabilistic models that are analytically intractable --- with Bayesian\nstate-of-the-art MAB algorithms, we extend their applicability to complex\nmodels: those for which sampling may be performed even if analytic computation\nof summary statistics is infeasible --- nonlinear reward functions and dynamic\nbandits. We combine SMC both for Thompson sampling and upper confident\nbound-based (Bayes-UCB) policies, and study different bandit models: classic\nBernoulli and Gaussian distributed cases, as well as dynamic and context\ndependent linear-Gaussian, logistic and categorical-softmax rewards.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 20:40:42 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 18:52:44 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 20:31:32 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1808.02939", "submitter": "Yuan Gong", "authors": "Yuan Gong, Christian Poellabauer", "title": "Towards Learning Fine-Grained Disentangled Representations from Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representations of high-dimensional data is currently\nan active research area. However, compared to the field of computer vision,\nless work has been done for speech processing. In this paper, we provide a\nreview of two representative efforts on this topic and propose the novel\nconcept of fine-grained disentangled speech representation learning.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 20:59:26 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Gong", "Yuan", ""], ["Poellabauer", "Christian", ""]]}, {"id": "1808.02941", "submitter": "Xiangyi Chen", "authors": "Xiangyi Chen, Sijia Liu, Ruoyu Sun, Mingyi Hong", "title": "On the Convergence of A Class of Adam-Type Algorithms for Non-Convex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a class of adaptive gradient based momentum algorithms\nthat update the search directions and learning rates simultaneously using past\ngradients. This class, which we refer to as the \"Adam-type\", includes the\npopular algorithms such as the Adam, AMSGrad and AdaGrad. Despite their\npopularity in training deep neural networks, the convergence of these\nalgorithms for solving nonconvex problems remains an open question. This paper\nprovides a set of mild sufficient conditions that guarantee the convergence for\nthe Adam-type methods. We prove that under our derived conditions, these\nmethods can achieve the convergence rate of order $O(\\log{T}/\\sqrt{T})$ for\nnonconvex stochastic optimization. We show the conditions are essential in the\nsense that violating them may make the algorithm diverge. Moreover, we propose\nand analyze a class of (deterministic) incremental adaptive gradient\nalgorithms, which has the same $O(\\log{T}/\\sqrt{T})$ convergence rate. Our\nstudy could also be extended to a broader class of adaptive gradient methods in\nmachine learning and optimization.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 21:14:07 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2019 00:48:35 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Chen", "Xiangyi", ""], ["Liu", "Sijia", ""], ["Sun", "Ruoyu", ""], ["Hong", "Mingyi", ""]]}, {"id": "1808.02956", "submitter": "Dongrui Wu", "authors": "Chenfeng Guo and Dongrui Wu", "title": "Feature Dimensionality Reduction for Video Affect Classification: A\n  Comparative Study", "comments": "1st Asian Affective Computing and Intelligent Interaction Conference,\n  Beijing, May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective computing has become a very important research area in\nhuman-machine interaction. However, affects are subjective, subtle, and\nuncertain. So, it is very difficult to obtain a large number of labeled\ntraining samples, compared with the number of possible features we could\nextract. Thus, dimensionality reduction is critical in affective computing.\nThis paper presents our preliminary study on dimensionality reduction for\naffect classification. Five popular dimensionality reduction approaches are\nintroduced and compared. Experiments on the DEAP dataset showed that no\napproach can universally outperform others, and performing classification using\nthe raw features directly may not always be a bad choice.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:29:45 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Guo", "Chenfeng", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.03001", "submitter": "Mathukumalli Vidyasagar", "authors": "Mahsa Lotfi and Mathukumalli Vidyasagar", "title": "Compressed Sensing Using Binary Matrices of Nearly Optimal Dimensions", "comments": "28 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of compressed sensing using binary\nmeasurement matrices and $\\ell_1$-norm minimization (basis pursuit) as the\nrecovery algorithm. We derive new upper and lower bounds on the number of\nmeasurements to achieve robust sparse recovery with binary matrices. We\nestablish sufficient conditions for a column-regular binary matrix to satisfy\nthe robust null space property (RNSP) and show that the associated sufficient\nconditions % sparsity bounds for robust sparse recovery obtained using the RNSP\nare better by a factor of $(3 \\sqrt{3})/2 \\approx 2.6$ compared to the\nsufficient conditions obtained using the restricted isometry property (RIP).\nNext we derive universal \\textit{lower} bounds on the number of measurements\nthat any binary matrix needs to have in order to satisfy the weaker sufficient\ncondition based on the RNSP and show that bipartite graphs of girth six are\noptimal. Then we display two classes of binary matrices, namely parity check\nmatrices of array codes and Euler squares, which have girth six and are nearly\noptimal in the sense of almost satisfying the lower bound. In principle,\nrandomly generated Gaussian measurement matrices are \"order-optimal\". So we\ncompare the phase transition behavior of the basis pursuit formulation using\nbinary array codes and Gaussian matrices and show that (i) there is essentially\nno difference between the phase transition boundaries in the two cases and (ii)\nthe CPU time of basis pursuit with binary matrices is hundreds of times faster\nthan with Gaussian matrices and the storage requirements are less. Therefore it\nis suggested that binary matrices are a viable alternative to Gaussian matrices\nfor compressed sensing using basis pursuit. \\end{abstract}\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 02:50:24 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 16:47:42 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2020 16:17:36 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lotfi", "Mahsa", ""], ["Vidyasagar", "Mathukumalli", ""]]}, {"id": "1808.03030", "submitter": "Ruiyi Zhang", "authors": "Ruiyi Zhang, Changyou Chen, Chunyuan Li, Lawrence Carin", "title": "Policy Optimization as Wasserstein Gradient Flows", "comments": "Accepted by ICML 2018; Initial version on Deep Reinforcement Learning\n  Symposium at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy optimization is a core component of reinforcement learning (RL), and\nmost existing RL methods directly optimize parameters of a policy based on\nmaximizing the expected total reward, or its surrogate. Though often achieving\nencouraging empirical success, its underlying mathematical principle on {\\em\npolicy-distribution} optimization is unclear. We place policy optimization into\nthe space of probability measures, and interpret it as Wasserstein gradient\nflows. On the probability-measure space, under specified circumstances, policy\noptimization becomes a convex problem in terms of distribution optimization. To\nmake optimization feasible, we develop efficient algorithms by numerically\nsolving the corresponding discrete gradient flows. Our technique is applicable\nto several RL settings, and is related to many state-of-the-art\npolicy-optimization algorithms. Empirical results verify the effectiveness of\nour framework, often obtaining better performance compared to related\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 06:19:39 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Zhang", "Ruiyi", ""], ["Chen", "Changyou", ""], ["Li", "Chunyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1808.03064", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist", "title": "Gradient and Newton Boosting for Classification and Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting algorithms are frequently used in applied data science and in\nresearch. To date, the distinction between boosting with either gradient\ndescent or second-order Newton updates is often not made in both applied and\nmethodological research, and it is thus implicitly assumed that the difference\nis irrelevant. The goal of this article is to clarify this situation. In\nparticular, we present gradient and Newton boosting, as well as a hybrid\nvariant of the two, in a unified framework. We compare these boosting\nalgorithms with trees as base learners using various datasets and loss\nfunctions. Our experiments show that Newton boosting outperforms gradient and\nhybrid gradient-Newton boosting in terms of predictive accuracy on the majority\nof datasets. We also present evidence that the reason for this is not faster\nconvergence of Newton boosting. In addition, we introduce a novel tuning\nparameter for tree-based Newton boosting which is interpretable and important\nfor predictive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 09:19:34 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 06:30:10 GMT"}, {"version": "v3", "created": "Tue, 12 Feb 2019 08:34:25 GMT"}, {"version": "v4", "created": "Fri, 1 Mar 2019 13:39:52 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2020 07:43:19 GMT"}, {"version": "v6", "created": "Mon, 5 Oct 2020 06:11:27 GMT"}, {"version": "v7", "created": "Tue, 20 Oct 2020 07:07:29 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Sigrist", "Fabio", ""]]}, {"id": "1808.03096", "submitter": "Mohammad Etemad", "authors": "Mohammad Etemad, Amilcar Soares Junior, Stan Matwin", "title": "On feature selection and evaluation of transportation mode prediction\n  strategies", "comments": "arXiv admin note: substantial text overlap with arXiv:1807.10876", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation modes prediction is a fundamental task for decision making in\nsmart cities and traffic management systems. Traffic policies designed based on\ntrajectory mining can save money and time for authorities and the public. It\nmay reduce the fuel consumption and commute time and moreover, may provide more\npleasant moments for residents and tourists. Since the number of features that\nmay be used to predict a user transportation mode can be substantial, finding a\nsubset of features that maximizes a performance measure is worth investigating.\nIn this work, we explore wrapper and information retrieval methods to find the\nbest subset of trajectory features. After finding the best classifier and the\nbest feature subset, our results were compared with two related papers that\napplied deep learning methods and the results showed that our framework\nachieved better performance. Furthermore, two types of cross-validation\napproaches were investigated, and the performance results show that the random\ncross-validation method provides optimistic results.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 11:27:11 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 20:29:07 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Etemad", "Mohammad", ""], ["Junior", "Amilcar Soares", ""], ["Matwin", "Stan", ""]]}, {"id": "1808.03114", "submitter": "Alex B\\\"auerle", "authors": "Alex B\\\"auerle, Heiko Neumann and Timo Ropinski", "title": "Classifier-Guided Visual Correction of Noisy Labels for Image\n  Classification Tasks", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.13973", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training data plays an essential role in modern applications of machine\nlearning. However, gathering labeled training data is time-consuming.\nTherefore, labeling is often outsourced to less experienced users, or\ncompletely automated. This can introduce errors, which compromise valuable\ntraining data, and lead to suboptimal training results. We thus propose a novel\napproach that uses the power of pretrained classifiers to visually guide users\nto noisy labels, and let them interactively check error candidates, to\niteratively improve the training data set. To systematically investigate\ntraining data, we propose a categorization of labeling errors into three\ndifferent types, based on an analysis of potential pitfalls in label\nacquisition processes. For each of these types, we present approaches to\ndetect, reason about, and resolve error candidates, as we propose measures and\nvisual guidance techniques to support machine learning users. Our approach has\nbeen used to spot errors in well-known machine learning benchmark data sets,\nand we tested its usability during a user evaluation. While initially developed\nfor images, the techniques presented in this paper are independent of the\nclassification algorithm, and can also be extended to many other types of\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 12:34:33 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 12:07:13 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 12:20:21 GMT"}, {"version": "v4", "created": "Mon, 6 Apr 2020 13:55:12 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["B\u00e4uerle", "Alex", ""], ["Neumann", "Heiko", ""], ["Ropinski", "Timo", ""]]}, {"id": "1808.03147", "submitter": "Jacob Turner", "authors": "Gianluca Micchi, Saeid Soheily-Khah, and Jacob Turner", "title": "A New Optimization Layer for Real-Time Bidding Advertising Campaigns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is relatively easy to start an online advertising campaign,\nobtaining a high Key Performance Indicator (KPI) can be challenging. A large\nbody of work on this subject has already been performed and platforms known as\nDSPs are available on the market that deal with such an optimization. From the\nadvertiser's point of view, each DSP is a different black box, with its pros\nand cons, that needs to be configured. In order to take advantage of the pros\nof every DSP, advertisers are well-advised to use a combination of them when\nsetting up their campaigns. In this paper, we propose an algorithm for\nadvertisers to add an optimization layer on top of DSPs. The algorithm we\nintroduce, called SKOTT, maximizes the chosen KPI by optimally configuring the\nDSPs and putting them in competition with each other. SKOTT is a highly\nspecialized iterative algorithm loosely based on gradient descent that is made\nup of three independent sub-routines, each dealing with a different problem:\npartitioning the budget, setting the desired average bid, and preventing\nunder-delivery. In particular, one of the novelties of our approach lies in our\ntaking the perspective of the advertisers rather than the DSPs. Synthetic\nmarket data is used to evaluate the efficiency of SKOTT against other\nstate-of-the-art approaches adapted from similar problems. The results\nillustrate the benefits of our proposals, which greatly outperforms the other\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 19:04:40 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Micchi", "Gianluca", ""], ["Soheily-Khah", "Saeid", ""], ["Turner", "Jacob", ""]]}, {"id": "1808.03216", "submitter": "Bruno Sudret", "authors": "E. Torre, S. Marelli, P. Embrechts, B. Sudret", "title": "Data-driven polynomial chaos expansion for machine learning regression", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.03.039", "report-no": "RSUQ-2018-005B", "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a regression technique for data-driven problems based on\npolynomial chaos expansion (PCE). PCE is a popular technique in the field of\nuncertainty quantification (UQ), where it is typically used to replace a\nrunnable but expensive computational model subject to random inputs with an\ninexpensive-to-evaluate polynomial function. The metamodel obtained enables a\nreliable estimation of the statistics of the output, provided that a suitable\nprobabilistic model of the input is available. Machine learning (ML) regression\nis a research field that focuses on providing purely data-driven input-output\nmaps, with the focus on pointwise prediction accuracy. We show that a PCE\nmetamodel purely trained on data can yield pointwise predictions whose accuracy\nis comparable to that of other ML regression models, such as neural networks\nand support vector machines. The comparisons are performed on benchmark\ndatasets available from the literature. The methodology also enables the\nquantification of the output uncertainties, and is robust to noise.\nFurthermore, it enjoys additional desirable properties, such as good\nperformance for small training sets and simplicity of construction, with only\nlittle parameter tuning required.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 16:11:31 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 15:22:47 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Torre", "E.", ""], ["Marelli", "S.", ""], ["Embrechts", "P.", ""], ["Sudret", "B.", ""]]}, {"id": "1808.03227", "submitter": "Mahtab Ahmed", "authors": "Mahtab Ahmed, Jumayel Islam, Muhammad Rifayat Samee, Robert E. Mercer", "title": "Identifying Protein-Protein Interaction using Tree LSTM and Structured\n  Attention", "comments": "9 Pages, 2 Figures, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying interactions between proteins is important to understand\nunderlying biological processes. Extracting a protein-protein interaction (PPI)\nfrom the raw text is often very difficult. Previous supervised learning methods\nhave used handcrafted features on human-annotated data sets. In this paper, we\npropose a novel tree recurrent neural network with structured attention\narchitecture for doing PPI. Our architecture achieves state of the art results\n(precision, recall, and F1-score) on the AIMed and BioInfer benchmark data\nsets. Moreover, our models achieve a significant improvement over previous best\nmodels without any explicit feature extraction. Our experimental results show\nthat traditional recurrent networks have inferior performance compared to tree\nrecurrent networks for the supervised PPI problem.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 19:08:12 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Ahmed", "Mahtab", ""], ["Islam", "Jumayel", ""], ["Samee", "Muhammad Rifayat", ""], ["Mercer", "Robert E.", ""]]}, {"id": "1808.03230", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi, Natesh S. Pillai, and Aaron Smith", "title": "Does Hamiltonian Monte Carlo mix faster than a random walk on multimodal\n  densities?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of\nMarkov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity\nof HMC algorithms is their excellent performance as the dimension $d$ of the\ntarget becomes large: under conditions that are satisfied for many common\nstatistical models, optimally-tuned HMC algorithms have a running time that\nscales like $d^{0.25}$. In stark contrast, the running time of the usual\nRandom-Walk Metropolis (RWM) algorithm, optimally tuned, scales like $d$. This\nsuperior scaling of the HMC algorithm with dimension is attributed to the fact\nthat it, unlike RWM, incorporates the gradient information in the proposal\ndistribution. In this paper, we investigate a different scaling question: does\nHMC beat RWM for highly $\\textit{multimodal}$ targets? We find that the answer\nis often $\\textit{no}$. We compute the spectral gaps for both the algorithms\nfor a specific class of multimodal target densities, and show that they are\nidentical. The key reason is that, within one mode, the gradient is effectively\nignorant about other modes, thus negating the advantage the HMC algorithm\nenjoys in unimodal targets. We also give heuristic arguments suggesting that\nthe above observation may hold quite generally. Our main tool for answering\nthis question is a novel simple formula for the conductance of HMC using\nLiouville's theorem. This result allows us to compute the spectral gap of HMC\nalgorithms, for both the classical HMC with isotropic momentum and the recent\nRiemannian HMC, for multimodal targets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 16:46:51 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 16:20:07 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Mangoubi", "Oren", ""], ["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1808.03233", "submitter": "Chengrun Yang", "authors": "Chengrun Yang, Yuji Akimoto, Dae Won Kim, Madeleine Udell", "title": "OBOE: Collaborative Filtering for AutoML Model Selection", "comments": null, "journal-ref": null, "doi": "10.1145/3292500.3330909", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm selection and hyperparameter tuning remain two of the most\nchallenging tasks in machine learning. Automated machine learning (AutoML)\nseeks to automate these tasks to enable widespread use of machine learning by\nnon-experts. This paper introduces OBOE, a collaborative filtering method for\ntime-constrained model selection and hyperparameter tuning. OBOE forms a matrix\nof the cross-validated errors of a large number of supervised learning models\n(algorithms together with hyperparameters) on a large number of datasets, and\nfits a low rank model to learn the low-dimensional feature vectors for the\nmodels and datasets that best predict the cross-validated errors. To find\npromising models for a new dataset, OBOE runs a set of fast but informative\nalgorithms on the new dataset and uses their cross-validated errors to infer\nthe feature vector for the new dataset. OBOE can find good models under\nconstraints on the number of models fit or the total time budget. To this end,\nthis paper develops a new heuristic for active learning in time-constrained\nmatrix completion based on optimal experiment design. Our experiments\ndemonstrate that OBOE delivers state-of-the-art performance faster than\ncompeting approaches on a test bed of supervised learning problems. Moreover,\nthe success of the bilinear model used by OBOE suggests that AutoML may be\nsimpler than was previously understood.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 16:56:04 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 19:55:26 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Yang", "Chengrun", ""], ["Akimoto", "Yuji", ""], ["Kim", "Dae Won", ""], ["Udell", "Madeleine", ""]]}, {"id": "1808.03246", "submitter": "Jiajun Wu", "authors": "Anurag Ajay, Jiajun Wu, Nima Fazeli, Maria Bauza, Leslie P. Kaelbling,\n  Joshua B. Tenenbaum, Alberto Rodriguez", "title": "Augmenting Physical Simulators with Stochastic Neural Networks: Case\n  Study of Planar Pushing and Bouncing", "comments": "IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient, generalizable physical simulator with universal uncertainty\nestimates has wide applications in robot state estimation, planning, and\ncontrol. In this paper, we build such a simulator for two scenarios, planar\npushing and ball bouncing, by augmenting an analytical rigid-body simulator\nwith a neural network that learns to model uncertainty as residuals. Combining\nsymbolic, deterministic simulators with learnable, stochastic neural nets\nprovides us with expressiveness, efficiency, and generalizability\nsimultaneously. Our model outperforms both purely analytical and purely learned\nsimulators consistently on real, standard benchmarks. Compared with methods\nthat model uncertainty using Gaussian processes, our model runs much faster,\ngeneralizes better to new object shapes, and is able to characterize the\ncomplex distribution of object trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 17:30:48 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Ajay", "Anurag", ""], ["Wu", "Jiajun", ""], ["Fazeli", "Nima", ""], ["Bauza", "Maria", ""], ["Kaelbling", "Leslie P.", ""], ["Tenenbaum", "Joshua B.", ""], ["Rodriguez", "Alberto", ""]]}, {"id": "1808.03253", "submitter": "Adarsh Subbaswamy", "authors": "Adarsh Subbaswamy, Suchi Saria", "title": "Counterfactual Normalization: Proactively Addressing Dataset Shift and\n  Improving Reliability Using Causal Mechanisms", "comments": "Proceedings of the 34th Conference on Uncertainty in Artificial\n  Intelligence (UAI), 2018. Revised from print version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models can fail to generalize from training to deployment\nenvironments because of dataset shift, posing a threat to model reliability and\nthe safety of downstream decisions made in practice. Instead of using samples\nfrom the target distribution to reactively correct dataset shift, we use\ngraphical knowledge of the causal mechanisms relating variables in a prediction\nproblem to proactively remove relationships that do not generalize across\nenvironments, even when these relationships may depend on unobserved variables\n(violations of the \"no unobserved confounders\" assumption). To accomplish this,\nwe identify variables with unstable paths of statistical influence and remove\nthem from the model. We also augment the causal graph with latent\ncounterfactual variables that isolate unstable paths of statistical influence,\nallowing us to retain stable paths that would otherwise be removed. Our\nexperiments demonstrate that models that remove vulnerable variables and use\nestimates of the latent variables transfer better, often outperforming in the\ntarget domain despite some accuracy loss in the training domain.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 17:35:52 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Subbaswamy", "Adarsh", ""], ["Saria", "Suchi", ""]]}, {"id": "1808.03258", "submitter": "Haijun Yu", "authors": "Shanshan Tang and Haijun Yu", "title": "Application of Bounded Total Variation Denoising in Urban Traffic\n  Analysis", "comments": "7 figures, 3 tables, to appear on East Asian Journal on Applied\n  Mathematics", "journal-ref": "East Asian Journal on Applied Mathematics Vol.9, No.3, pp.\n  622-642, 2019", "doi": "10.4208/eajam.181118.250219", "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is believed that denoising is not always necessary in many big data\napplications, we show in this paper that denoising is helpful in urban traffic\nanalysis by applying the method of bounded total variation denoising to the\nurban road traffic prediction and clustering problem. We propose two\neasy-to-implement methods to estimate the noise strength parameter in the\ndenoising algorithm, and apply the denoising algorithm to GPS-based traffic\ndata from Beijing taxi system. For the traffic prediction problem, we combine\nneural network and history matching method for roads randomly chosen from an\nurban area of Beijing. Numerical experiments show that the predicting accuracy\nis improved significantly by applying the proposed bounded total variation\ndenoising algorithm. We also test the algorithm on clustering problem, where a\nrecently developed clustering analysis method is applied to more than one\nhundred urban road segments in Beijing based on their velocity profiles. Better\nclustering result is obtained after denoising.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 13:37:51 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 08:35:45 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Tang", "Shanshan", ""], ["Yu", "Haijun", ""]]}, {"id": "1808.03265", "submitter": "Qiwei Han", "authors": "Qiwei Han, Mengxin Ji, Inigo Martinez de Rituerto de Troya, Manas\n  Gaur, Leid Zejnilovic", "title": "A Hybrid Recommender System for Patient-Doctor Matchmaking in Primary\n  Care", "comments": "This paper is accepted at DSAA 2018 as a full paper, Proc. of the 5th\n  IEEE International Conference on Data Science and Advanced Analytics (DSAA),\n  Turin, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We partner with a leading European healthcare provider and design a mechanism\nto match patients with family doctors in primary care. We define the\nmatchmaking process for several distinct use cases given different levels of\navailable information about patients. Then, we adopt a hybrid recommender\nsystem to present each patient a list of family doctor recommendations. In\nparticular, we model patient trust of family doctors using a large-scale\ndataset of consultation histories, while accounting for the temporal dynamics\nof their relationships. Our proposed approach shows higher predictive accuracy\nthan both a heuristic baseline and a collaborative filtering approach, and the\nproposed trust measure further improves model performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 04:08:46 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 18:52:59 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Han", "Qiwei", ""], ["Ji", "Mengxin", ""], ["de Troya", "Inigo Martinez de Rituerto", ""], ["Gaur", "Manas", ""], ["Zejnilovic", "Leid", ""]]}, {"id": "1808.03277", "submitter": "Zecheng He", "authors": "Zecheng He, Tianwei Zhang, Ruby B. Lee", "title": "VerIDeep: Verifying Integrity of Deep Neural Networks through\n  Sensitive-Sample Fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become popular, and numerous cloud-based services are\nprovided to help customers develop and deploy deep learning applications.\nMeanwhile, various attack techniques have also been discovered to stealthily\ncompromise the model's integrity. When a cloud customer deploys a deep learning\nmodel in the cloud and serves it to end-users, it is important for him to be\nable to verify that the deployed model has not been tampered with, and the\nmodel's integrity is protected.\n  We propose a new low-cost and self-served methodology for customers to verify\nthat the model deployed in the cloud is intact, while having only black-box\naccess (e.g., via APIs) to the deployed model. Customers can detect arbitrary\nchanges to their deep learning models. Specifically, we define\n\\texttt{Sensitive-Sample} fingerprints, which are a small set of transformed\ninputs that make the model outputs sensitive to the model's parameters. Even\nsmall weight changes can be clearly reflected in the model outputs, and\nobserved by the customer. Our experiments on different types of model integrity\nattacks show that we can detect model integrity breaches with high accuracy\n($>$99\\%) and low overhead ($<$10 black-box model accesses).\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 17:10:21 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 01:02:04 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["He", "Zecheng", ""], ["Zhang", "Tianwei", ""], ["Lee", "Ruby B.", ""]]}, {"id": "1808.03298", "submitter": "Zhiyu Min", "authors": "Zhiyu Min, Dahua Lin", "title": "Probabilistic Ensemble of Collaborative Filters", "comments": "8 pages. In Proceedings of AAAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is an important technique for recommendation. Whereas\nit has been repeatedly shown to be effective in previous work, its performance\nremains unsatisfactory in many real-world applications, especially those where\nthe items or users are highly diverse. In this paper, we explore an\nensemble-based framework to enhance the capability of a recommender in handling\ndiverse data. Specifically, we formulate a probabilistic model which integrates\nthe items, the users, as well as the associations between them into a\ngenerative process. On top of this formulation, we further derive a progressive\nalgorithm to construct an ensemble of collaborative filters. In each iteration,\na new filter is derived from re-weighted entries and incorporated into the\nensemble. It is noteworthy that while the algorithmic procedure of our\nalgorithm is apparently similar to boosting, it is derived from an essentially\ndifferent formulation and thus differs in several key technical aspects. We\ntested the proposed method on three large datasets, and observed substantial\nimprovement over the state of the art, including L2Boost, an effective method\nbased on boosting.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 04:48:57 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 06:49:53 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Min", "Zhiyu", ""], ["Lin", "Dahua", ""]]}, {"id": "1808.03300", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang", "title": "$\\alpha$-Approximation Density-based Clustering of Multi-valued Objects", "comments": "This submission has been removed by arXiv administrators due to\n  copyright infringement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This submission has been removed by arXiv administrators due to copyright\ninfringement.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 18:43:18 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 17:15:44 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Zhang", "Zhilin", ""]]}, {"id": "1808.03305", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, Richard Zemel, John K. Tsotsos", "title": "The Elephant in the Room", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We showcase a family of common failures of state-of-the art object detectors.\nThese are obtained by replacing image sub-regions by another sub-image that\ncontains a trained object. We call this \"object transplanting\". Modifying an\nimage in this manner is shown to have a non-local impact on object detection.\nSlight changes in object position can affect its identity according to an\nobject detector as well as that of other objects in the image. We provide some\nanalysis and suggest possible reasons for the reported phenomena.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 18:58:59 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Zemel", "Richard", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1808.03314", "submitter": "Alex Sherstinsky", "authors": "Alex Sherstinsky", "title": "Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term\n  Memory (LSTM) Network", "comments": "43 pages, 10 figures, 78 references", "journal-ref": "Elsevier \"Physica D: Nonlinear Phenomena\" journal, Volume 404,\n  March 2020: Special Issue on Machine Learning and Dynamical Systems", "doi": "10.1016/j.physd.2019.132306", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of their effectiveness in broad practical applications, LSTM networks\nhave received a wealth of coverage in scientific journals, technical blogs, and\nimplementation guides. However, in most articles, the inference formulas for\nthe LSTM network and its parent, RNN, are stated axiomatically, while the\ntraining formulas are omitted altogether. In addition, the technique of\n\"unrolling\" an RNN is routinely presented without justification throughout the\nliterature. The goal of this paper is to explain the essential RNN and LSTM\nfundamentals in a single document. Drawing from concepts in signal processing,\nwe formally derive the canonical RNN formulation from differential equations.\nWe then propose and prove a precise statement, which yields the RNN unrolling\ntechnique. We also review the difficulties with training the standard RNN and\naddress them by transforming the RNN into the \"Vanilla LSTM\" network through a\nseries of logical arguments. We provide all equations pertaining to the LSTM\nsystem together with detailed descriptions of its constituent entities. Albeit\nunconventional, our choice of notation and the method for presenting the LSTM\nsystem emphasizes ease of understanding. As part of the analysis, we identify\nnew opportunities to enrich the LSTM system and incorporate these extensions\ninto the Vanilla LSTM network, producing the most general LSTM variant to date.\nThe target reader has already been exposed to RNNs and LSTM networks through\nnumerous available resources and is open to an alternative pedagogical\napproach. A Machine Learning practitioner seeking guidance for implementing our\nnew augmented LSTM model in software for experimentation and research will find\nthe insights and derivations in this tutorial valuable as well.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 19:31:42 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 16:05:02 GMT"}, {"version": "v3", "created": "Sat, 6 Oct 2018 20:58:56 GMT"}, {"version": "v4", "created": "Sun, 4 Nov 2018 07:53:54 GMT"}, {"version": "v5", "created": "Mon, 3 Feb 2020 05:01:36 GMT"}, {"version": "v6", "created": "Tue, 31 Mar 2020 18:59:23 GMT"}, {"version": "v7", "created": "Sun, 31 May 2020 18:33:31 GMT"}, {"version": "v8", "created": "Mon, 21 Dec 2020 08:31:01 GMT"}, {"version": "v9", "created": "Sun, 31 Jan 2021 08:24:26 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Sherstinsky", "Alex", ""]]}, {"id": "1808.03319", "submitter": "Upal Mahbub", "authors": "Upal Mahbub and Jukka Komulainen and Denzil Ferreira and Rama\n  Chellappa", "title": "Continuous Authentication of Smartphones Based on Application Usage", "comments": "14 pages, 7 figures, 7 tables,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An empirical investigation of active/continuous authentication for\nsmartphones is presented in this paper by exploiting users' unique application\nusage data, i.e., distinct patterns of use, modeled by a Markovian process.\nVariations of Hidden Markov Models (HMMs) are evaluated for continuous user\nverification, and challenges due to the sparsity of session-wise data, an\nexplosion of states, and handling unforeseen events in the test data are\ntackled. Unlike traditional approaches, the proposed formulation does not\ndepend on the top N-apps, rather uses the complete app-usage information to\nachieve low latency. Through experimentation, empirical assessment of the\nimpact of unforeseen events, i.e., unknown applications and unforeseen\nobservations, on user verification is done via a modified edit-distance\nalgorithm for simple sequence matching. It is found that for enhanced\nverification performance, unforeseen events should be incorporated in the\nmodels by adopting smoothing techniques with HMMs. For validation, extensive\nexperiments on two distinct datasets are performed. The marginal smoothing\ntechnique is the most effective for user verification in terms of equal error\nrate (EER) and with a sampling rate of 1/30s^{-1} and 30 minutes of historical\ndata, and the method is capable of detecting an intrusion within ~2.5 minutes\nof application use.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 03:24:22 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Mahbub", "Upal", ""], ["Komulainen", "Jukka", ""], ["Ferreira", "Denzil", ""], ["Chellappa", "Rama", ""]]}, {"id": "1808.03327", "submitter": "Shounak Datta", "authors": "Avisek Gupta, Shounak Datta, Swagatam Das", "title": "Fuzzy Clustering to Identify Clusters at Different Levels of Fuzziness:\n  An Evolutionary Multi-Objective Optimization Approach", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy clustering methods identify naturally occurring clusters in a dataset,\nwhere the extent to which different clusters are overlapped can differ. Most\nmethods have a parameter to fix the level of fuzziness. However, the\nappropriate level of fuzziness depends on the application at hand. This paper\npresents Entropy $c$-Means (ECM), a method of fuzzy clustering that\nsimultaneously optimizes two contradictory objective functions, resulting in\nthe creation of fuzzy clusters with different levels of fuzziness. This allows\nECM to identify clusters with different degrees of overlap. ECM optimizes the\ntwo objective functions using two multi-objective optimization methods,\nNon-dominated Sorting Genetic Algorithm II (NSGA-II), and Multiobjective\nEvolutionary Algorithm based on Decomposition (MOEA/D). We also propose a\nmethod to select a suitable trade-off clustering from the Pareto front.\nExperiments on challenging synthetic datasets as well as real-world datasets\nshow that ECM leads to better cluster detection compared to the conventional\nfuzzy clustering methods as well as previously used multi-objective methods for\nfuzzy clustering.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 19:48:41 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Gupta", "Avisek", ""], ["Datta", "Shounak", ""], ["Das", "Swagatam", ""]]}, {"id": "1808.03331", "submitter": "Daisy Yi Ding", "authors": "Daisy Yi Ding, Chlo\\'e Simpson, Stephen Pfohl, Dave C. Kale, Kenneth\n  Jung, Nigam H. Shah", "title": "The Effectiveness of Multitask Learning for Phenotyping with Electronic\n  Health Records Data", "comments": "Pacific Symposium on Biocomputing (PSB) 2019, Hawaii,\n  https://psb.stanford.edu/psb-online/; 13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic phenotyping is the task of ascertaining whether an individual has\na medical condition of interest by analyzing their medical record and is\nfoundational in clinical informatics. Increasingly, electronic phenotyping is\nperformed via supervised learning. We investigate the effectiveness of\nmultitask learning for phenotyping using electronic health records (EHR) data.\nMultitask learning aims to improve model performance on a target task by\njointly learning additional auxiliary tasks and has been used in disparate\nareas of machine learning. However, its utility when applied to EHR data has\nnot been established, and prior work suggests that its benefits are\ninconsistent. We present experiments that elucidate when multitask learning\nwith neural nets improves performance for phenotyping using EHR data relative\nto neural nets trained for a single phenotype and to well-tuned logistic\nregression baselines. We find that multitask neural nets consistently\noutperform single-task neural nets for rare phenotypes but underperform for\nrelatively more common phenotypes. The effect size increases as more auxiliary\ntasks are added. Moreover, multitask learning reduces the sensitivity of neural\nnets to hyperparameter settings for rare phenotypes. Last, we quantify\nphenotype complexity and find that neural nets trained with or without\nmultitask learning do not improve on simple baselines unless the phenotypes are\nsufficiently complex.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 20:08:13 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 00:04:46 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2019 01:44:50 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ding", "Daisy Yi", ""], ["Simpson", "Chlo\u00e9", ""], ["Pfohl", "Stephen", ""], ["Kale", "Dave C.", ""], ["Jung", "Kenneth", ""], ["Shah", "Nigam H.", ""]]}, {"id": "1808.03333", "submitter": "Ruocheng Guo", "authors": "Vineeth Rakesh and Ruocheng Guo and Raha Moraffah and Nitin Agarwal\n  and Huan Liu", "title": "Linked Causal Variational Autoencoder for Inferring Paired Spillover\n  Effects", "comments": "First two authors contributed equally, 4 pages, CIKM'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling spillover effects from observational data is an important problem in\neconomics, business, and other fields of research. % It helps us infer the\ncausality between two seemingly unrelated set of events. For example, if\nconsumer spending in the United States declines, it has spillover effects on\neconomies that depend on the U.S. as their largest export market. In this\npaper, we aim to infer the causation that results in spillover effects between\npairs of entities (or units), we call this effect as \\textit{paired spillover}.\nTo achieve this, we leverage the recent developments in variational inference\nand deep learning techniques to propose a generative model called Linked Causal\nVariational Autoencoder (LCVA). Similar to variational autoencoders (VAE), LCVA\nincorporates an encoder neural network to learn the latent attributes and a\ndecoder network to reconstruct the inputs. However, unlike VAE, LCVA treats the\n\\textit{latent attributes as confounders that are assumed to affect both the\ntreatment and the outcome of units}. Specifically, given a pair of units $u$\nand $\\bar{u}$, their individual treatment and outcomes, the encoder network of\nLCVA samples the confounders by conditioning on the observed covariates of $u$,\nthe treatments of both $u$ and $\\bar{u}$ and the outcome of $u$. Once inferred,\nthe latent attributes (or confounders) of $u$ captures the spillover effect of\n$\\bar{u}$ on $u$. Using a network of users from job training dataset (LaLonde\n(1986)) and co-purchase dataset from Amazon e-commerce domain, we show that\nLCVA is significantly more robust than existing methods in capturing spillover\neffects.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 20:11:09 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 00:34:57 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2018 03:29:23 GMT"}, {"version": "v4", "created": "Wed, 3 Oct 2018 17:47:09 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Rakesh", "Vineeth", ""], ["Guo", "Ruocheng", ""], ["Moraffah", "Raha", ""], ["Agarwal", "Nitin", ""], ["Liu", "Huan", ""]]}, {"id": "1808.03351", "submitter": "Trefor Evans", "authors": "Trefor W. Evans and Prasanth B. Nair", "title": "Exploiting Structure for Fast Kernel Learning", "comments": "Appears in the proceedings of the SIAM International Conference on\n  Data Mining (SDM), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two methods for exact Gaussian process (GP) inference and learning\non massive image, video, spatial-temporal, or multi-output datasets with\nmissing values (or \"gaps\") in the observed responses. The first method ignores\nthe gaps using sparse selection matrices and a highly effective low-rank\npreconditioner is introduced to accelerate computations. The second method\nintroduces a novel approach to GP training whereby response values are inferred\non the gaps before explicitly training the model. We find this second approach\nto be greatly advantageous for the class of problems considered. Both of these\nnovel approaches make extensive use of Kronecker matrix algebra to design\nmassively scalable algorithms which have low memory requirements. We\ndemonstrate exact GP inference for a spatial-temporal climate modelling problem\nwith 3.7 million training points as well as a video reconstruction problem with\n1 billion points.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 21:36:02 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Evans", "Trefor W.", ""], ["Nair", "Prasanth B.", ""]]}, {"id": "1808.03364", "submitter": "Matthew Harding", "authors": "Matthew Harding and Carlos Lamarche", "title": "A Panel Quantile Approach to Attrition Bias in Big Data: Evidence from a\n  Randomized Experiment", "comments": "JEL: C21, C23, C25, C55. Keywords: Attrition; Big Data; Quantile\n  regression; Individual Effects; Time-of-Day Pricing", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a quantile regression estimator for panel data models\nwith individual heterogeneity and attrition. The method is motivated by the\nfact that attrition bias is often encountered in Big Data applications. For\nexample, many users sign-up for the latest program but few remain active users\nseveral months later, making the evaluation of such interventions inherently\nvery challenging. Building on earlier work by Hausman and Wise (1979), we\nprovide a simple identification strategy that leads to a two-step estimation\nprocedure. In the first step, the coefficients of interest in the selection\nequation are consistently estimated using parametric or nonparametric methods.\nIn the second step, standard panel quantile methods are employed on a subset of\nweighted observations. The estimator is computationally easy to implement in\nBig Data applications with a large number of subjects. We investigate the\nconditions under which the parameter estimator is asymptotically Gaussian and\nwe carry out a series of Monte Carlo simulations to investigate the finite\nsample properties of the estimator. Lastly, using a simulation exercise, we\napply the method to the evaluation of a recent Time-of-Day electricity pricing\nexperiment inspired by the work of Aigner and Hausman (1980).\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 22:35:27 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Harding", "Matthew", ""], ["Lamarche", "Carlos", ""]]}, {"id": "1808.03388", "submitter": "Roozbeh Khodambashi", "authors": "Ningquan Wang, Ruxiu Liu, Roozbeh Khodambashi, Norh Asmare, and A.\n  Fatih Sarioglu", "title": "Code-division multiplexed resistive pulse sensor networks for\n  spatio-temporal detection of particles in microfluidic devices", "comments": "2017 IEEE 30th International Conference on Micro Electro Mechanical\n  Systems (MEMS)", "journal-ref": null, "doi": "10.1109/MEMSYS.2017.7863416", "report-no": null, "categories": "cs.ET cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial separation of suspended particles based on contrast in their physical\nor chemical properties forms the basis of various biological assays performed\non lab-on-achip devices. To electronically acquire this information, we have\nrecently introduced a microfluidic sensing platform, called Microfluidic CODES,\nwhich combines the resistive pulse sensing with the code division multiple\naccess in multiplexing a network of integrated electrical sensors. In this\npaper, we enhance the multiplexing capacity of the Microfluidic CODES by\nemploying sensors that generate non-orthogonal code waveforms and a new\ndecoding algorithm that combines machine learning techniques with minimum\nmean-squared error estimation. As a proof of principle, we fabricated a\nmicrofluidic device with a network of 10 code-multiplexed sensors and\ncharacterized it using cells suspended in phosphate buffer saline solution.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 01:58:27 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Wang", "Ningquan", ""], ["Liu", "Ruxiu", ""], ["Khodambashi", "Roozbeh", ""], ["Asmare", "Norh", ""], ["Sarioglu", "A. Fatih", ""]]}, {"id": "1808.03408", "submitter": "Li Shen", "authors": "Fangyu Zou, Li Shen, Zequn Jie, Ju Sun and Wei Liu", "title": "Weighted AdaGrad with Unified Momentum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating adaptive learning rate and momentum techniques into SGD leads to\na large class of efficiently accelerated adaptive stochastic algorithms, such\nas Nadam, AccAdaGrad, \\textit{etc}. In spite of their effectiveness in\npractice, there is still a large gap in their theories of convergences,\nespecially in the difficult non-convex stochastic setting. To fill this gap, we\npropose \\emph{weighted AdaGrad with unified momentum}, dubbed AdaUSM, which has\nthe main characteristics that (1) it incorporates a unified momentum scheme\nwhich covers both the heavy ball momentum and the Nesterov accelerated gradient\nmomentum; (2) it adopts a novel weighted adaptive learning rate that can unify\nthe learning rates of AdaGrad, AccAdaGrad, Adam, and RMSProp. Moreover, when we\ntake polynomially growing weights in AdaUSM, we obtain its\n$\\mathcal{O}(\\log(T)/\\sqrt{T})$ convergence rate in the non-convex stochastic\nsetting. We also show that the adaptive learning rates of Adam and RMSProp\ncorrespond to taking exponentially growing weights in AdaUSM, which thereby\nprovides a new perspesctive for understanding Adam and RMSProp. Lastly,\ncomparative experiments of AdaUSM against SGD with momentum, AdaGrad, AdaEMA,\nAdam, and AMSGrad on various deep learning models and datasets are also\nprovided.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 04:18:48 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 08:53:20 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 10:50:45 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Zou", "Fangyu", ""], ["Shen", "Li", ""], ["Jie", "Zequn", ""], ["Sun", "Ju", ""], ["Liu", "Wei", ""]]}, {"id": "1808.03420", "submitter": "Dharma Teja Vooturi", "authors": "Dharma Teja Vooturi, Dheevatsa Mudigere, Sasikanth Avancha", "title": "Hierarchical Block Sparse Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse deep neural networks(DNNs) are efficient in both memory and compute\nwhen compared to dense DNNs. But due to irregularity in computation of sparse\nDNNs, their efficiencies are much lower than that of dense DNNs on regular\nparallel hardware such as TPU. This inefficiency leads to poor/no performance\nbenefits for sparse DNNs. Performance issue for sparse DNNs can be alleviated\nby bringing structure to the sparsity and leveraging it for improving runtime\nefficiency. But such structural constraints often lead to suboptimal\naccuracies. In this work, we jointly address both accuracy and performance of\nsparse DNNs using our proposed class of sparse neural networks called HBsNN\n(Hierarchical Block sparse Neural Networks). For a given sparsity, HBsNN models\nachieve better runtime performance than unstructured sparse models and better\naccuracy than highly structured sparse models.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 05:53:12 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 02:05:11 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Vooturi", "Dharma Teja", ""], ["Mudigere", "Dheevatsa", ""], ["Avancha", "Sasikanth", ""]]}, {"id": "1808.03425", "submitter": "Jinfeng Zeng", "authors": "Jinfeng Zeng, Yufeng Wu, Jin-Guo Liu, Lei Wang, Jiangping Hu", "title": "Learning and Inference on Generative Adversarial Quantum Circuits", "comments": "7 pages, 6 figures", "journal-ref": "Phys. Rev. A 99, 052306 (2019)", "doi": "10.1103/PhysRevA.99.052306", "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum mechanics is inherently probabilistic in light of Born's rule. Using\nquantum circuits as probabilistic generative models for classical data exploits\ntheir superior expressibility and efficient direct sampling ability. However,\ntraining of quantum circuits can be more challenging compared to classical\nneural networks due to lack of efficient differentiable learning algorithm. We\ndevise an adversarial quantum-classical hybrid training scheme via coupling a\nquantum circuit generator and a classical neural network discriminator\ntogether. After training, the quantum circuit generative model can infer\nmissing data with quadratic speed up via amplitude amplification. We\nnumerically simulate the learning and inference of generative adversarial\nquantum circuit using the prototypical Bars-and-Stripes dataset. Generative\nadversarial quantum circuits is a fresh approach to machine learning which may\nenjoy the practically useful quantum advantage on near-term quantum devices.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 06:36:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Zeng", "Jinfeng", ""], ["Wu", "Yufeng", ""], ["Liu", "Jin-Guo", ""], ["Wang", "Lei", ""], ["Hu", "Jiangping", ""]]}, {"id": "1808.03504", "submitter": "Navid Tafaghodi Khajavi", "authors": "Navid Tafaghodi Khajavi and Anthony Kuh", "title": "Model Approximation Using Cascade of Tree Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a general, multistage framework for graphical model\napproximation using a cascade of models such as trees. In particular, we look\nat the problem of covariance matrix approximation for Gaussian distributions as\nlinear transformations of tree models. This is a new way to decompose the\ncovariance matrix. Here, we propose an algorithm which incorporates the\nCholesky factorization method to compute the decomposition matrix and thus can\napproximate a simple graphical model using a cascade of the Cholesky\nfactorization of the tree approximation transformations. The Cholesky\ndecomposition enables us to achieve a tree structure factor graph at each\ncascade stage of the algorithm which facilitates the use of the message passing\nalgorithm since the approximated graph has less loops compared to the original\ngraph. The overall graph is a cascade of factor graphs with each factor graph\nbeing a tree. This is a different perspective on the approximation model, and\nalgorithms such as Gaussian belief propagation can be used on this overall\ngraph. Here, we present theoretical result that guarantees the convergence of\nthe proposed model approximation using the cascade of tree decompositions. In\nthe simulations, we look at synthetic and real data and measure the performance\nof the proposed framework by comparing the KL divergences.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 12:21:47 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Khajavi", "Navid Tafaghodi", ""], ["Kuh", "Anthony", ""]]}, {"id": "1808.03566", "submitter": "Ahmad Hassanat", "authors": "Ahmad B. Hassanat", "title": "Greedy Algorithms for Approximating the Diameter of Machine Learning\n  Datasets in Multidimensional Euclidean Space", "comments": "15 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the diameter of a dataset in multidimensional Euclidean space is a\nwell-established problem, with well-known algorithms. However, most of the\nalgorithms found in the literature do not scale well with large values of data\ndimension, so the time complexity grows exponentially in most cases, which\nmakes these algorithms impractical. Therefore, we implemented 4 simple greedy\nalgorithms to be used for approximating the diameter of a multidimensional\ndataset; these are based on minimum/maximum l2 norms, hill climbing search,\nTabu search and Beam search approaches, respectively. The time complexity of\nthe implemented algorithms is near-linear, as they scale near-linearly with\ndata size and its dimensions. The results of the experiments (conducted on\ndifferent machine learning data sets) prove the efficiency of the implemented\nalgorithms and can therefore be recommended for finding the diameter to be used\nby different machine learning applications when needed.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 14:35:38 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Hassanat", "Ahmad B.", ""]]}, {"id": "1808.03578", "submitter": "Noah Frazier-Logue", "authors": "Noah Frazier-Logue and Stephen Jos\\'e Hanson", "title": "Dropout is a special case of the stochastic delta rule: faster and more\n  accurate deep learning", "comments": "6 pages, 7 figures; submitted to ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-layer neural networks have lead to remarkable performance on many kinds\nof benchmark tasks in text, speech and image processing. Nonlinear parameter\nestimation in hierarchical models is known to be subject to overfitting and\nmisspecification. One approach to these estimation and related problems (local\nminima, colinearity, feature discovery etc.) is called Dropout (Hinton, et al\n2012, Baldi et al 2016). The Dropout algorithm removes hidden units according\nto a Bernoulli random variable with probability $p$ prior to each update,\ncreating random \"shocks\" to the network that are averaged over updates. In this\npaper we will show that Dropout is a special case of a more general model\npublished originally in 1990 called the Stochastic Delta Rule, or SDR (Hanson,\n1990). SDR redefines each weight in the network as a random variable with mean\n$\\mu_{w_{ij}}$ and standard deviation $\\sigma_{w_{ij}}$. Each weight random\nvariable is sampled on each forward activation, consequently creating an\nexponential number of potential networks with shared weights. Both parameters\nare updated according to prediction error, thus resulting in weight noise\ninjections that reflect a local history of prediction error and local model\naveraging. SDR therefore implements a more sensitive local gradient-dependent\nsimulated annealing per weight converging in the limit to a Bayes optimal\nnetwork. Tests on standard benchmarks (CIFAR) using a modified version of\nDenseNet shows the SDR outperforms standard Dropout in test error by approx.\n$17\\%$ with DenseNet-BC 250 on CIFAR-100 and approx. $12-14\\%$ in smaller\nnetworks. We also show that SDR reaches the same accuracy that Dropout attains\nin 100 epochs in as few as 35 epochs.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 15:06:05 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 20:58:09 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Frazier-Logue", "Noah", ""], ["Hanson", "Stephen Jos\u00e9", ""]]}, {"id": "1808.03591", "submitter": "Lu\\'is Paulo Faina Garcia", "authors": "Ana C. Lorena, Lu\\'is P. F. Garcia, Jens Lehmann, Marcilio C. P. Souto\n  and Tin K. Ho", "title": "How Complex is your classification problem? A survey on measuring\n  classification complexity", "comments": "Survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characteristics extracted from the training datasets of classification\nproblems have proven to be effective predictors in a number of meta-analyses.\nAmong them, measures of classification complexity can be used to estimate the\ndifficulty in separating the data points into their expected classes.\nDescriptors of the spatial distribution of the data and estimates of the shape\nand size of the decision boundary are among the known measures for this\ncharacterization. This information can support the formulation of new\ndata-driven pre-processing and pattern recognition techniques, which can in\nturn be focused on challenges highlighted by such characteristics of the\nproblems. This paper surveys and analyzes measures which can be extracted from\nthe training datasets in order to characterize the complexity of the respective\nclassification problems. Their use in recent literature is also reviewed and\ndiscussed, allowing to prospect opportunities for future work in the area.\nFinally, descriptions are given on an R package named Extended Complexity\nLibrary (ECoL) that implements a set of complexity measures and is made\npublicly available.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 15:38:14 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 19:43:41 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 20:33:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lorena", "Ana C.", ""], ["Garcia", "Lu\u00eds P. F.", ""], ["Lehmann", "Jens", ""], ["Souto", "Marcilio C. P.", ""], ["Ho", "Tin K.", ""]]}, {"id": "1808.03601", "submitter": "Fan Yang", "authors": "Fan Yang, Zhiyuan Chen", "title": "Using Randomness to Improve Robustness of Machine-Learning Models\n  Against Evasion Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have been widely used in security applications such\nas intrusion detection, spam filtering, and virus or malware detection.\nHowever, it is well-known that adversaries are always trying to adapt their\nattacks to evade detection. For example, an email spammer may guess what\nfeatures spam detection models use and modify or remove those features to avoid\ndetection. There has been some work on making machine learning models more\nrobust to such attacks. However, one simple but promising approach called {\\em\nrandomization} is underexplored. This paper proposes a novel\nrandomization-based approach to improve robustness of machine learning models\nagainst evasion attacks. The proposed approach incorporates randomization into\nboth model training time and model application time (meaning when the model is\nused to detect attacks). We also apply this approach to random forest, an\nexisting ML method which already has some degree of randomness. Experiments on\nintrusion detection and spam filtering data show that our approach further\nimproves robustness of random-forest method. We also discuss how this approach\ncan be applied to other ML models.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 15:59:31 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Yang", "Fan", ""], ["Chen", "Zhiyuan", ""]]}, {"id": "1808.03604", "submitter": "Vikram Venkatraghavan", "authors": "Vikram Venkatraghavan, Esther E. Bron, Wiro J. Niessen, Stefan Klein\n  (for the Alzheimer's Disease Neuroimaging Initiative)", "title": "Disease Progression Timeline Estimation for Alzheimer's Disease using\n  Discriminative Event Based Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alzheimer's Disease (AD) is characterized by a cascade of biomarkers becoming\nabnormal, the pathophysiology of which is very complex and largely unknown.\nEvent-based modeling (EBM) is a data-driven technique to estimate the sequence\nin which biomarkers for a disease become abnormal based on cross-sectional\ndata. It can help in understanding the dynamics of disease progression and\nfacilitate early diagnosis and prognosis. In this work we propose a novel\ndiscriminative approach to EBM, which is shown to be more accurate than\nexisting state-of-the-art EBM methods. The method first estimates for each\nsubject an approximate ordering of events. Subsequently, the central ordering\nover all subjects is estimated by fitting a generalized Mallows model to these\napproximate subject-specific orderings. We also introduce the concept of\nrelative distance between events which helps in creating a disease progression\ntimeline. Subsequently, we propose a method to stage subjects by placing them\non the estimated disease progression timeline. We evaluated the proposed method\non Alzheimer's Disease Neuroimaging Initiative (ADNI) data and compared the\nresults with existing state-of-the-art EBM methods. We also performed extensive\nexperiments on synthetic data simulating the progression of Alzheimer's\ndisease. The event orderings obtained on ADNI data seem plausible and are in\nagreement with the current understanding of progression of AD. The proposed\npatient staging algorithm performed consistently better than that of\nstate-of-the-art EBM methods. Event orderings obtained in simulation\nexperiments were more accurate than those of other EBM methods and the\nestimated disease progression timeline was observed to correlate with the\ntimeline of actual disease progression. The results of these experiments are\nencouraging and suggest that discriminative EBM is a promising approach to\ndisease progression modeling.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 16:08:27 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Venkatraghavan", "Vikram", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Bron", "Esther E.", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Niessen", "Wiro J.", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Klein", "Stefan", "", "for the Alzheimer's Disease Neuroimaging Initiative"]]}, {"id": "1808.03620", "submitter": "Nikola Kovachki", "authors": "Nikola B. Kovachki, Andrew M. Stuart", "title": "Ensemble Kalman Inversion: A Derivative-Free Technique For Machine\n  Learning Tasks", "comments": "41 pages, 14 figures", "journal-ref": null, "doi": "10.1088/1361-6420/ab1c3a", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard probabilistic perspective on machine learning gives rise to\nempirical risk-minimization tasks that are frequently solved by stochastic\ngradient descent (SGD) and variants thereof. We present a formulation of these\ntasks as classical inverse or filtering problems and, furthermore, we propose\nan efficient, gradient-free algorithm for finding a solution to these problems\nusing ensemble Kalman inversion (EKI). Applications of our approach include\noffline and online supervised learning with deep neural networks, as well as\ngraph-based semi-supervised learning. The essence of the EKI procedure is an\nensemble based approximate gradient descent in which derivatives are replaced\nby differences from within the ensemble. We suggest several modifications to\nthe basic method, derived from empirically successful heuristics developed in\nthe context of SGD. Numerical results demonstrate wide applicability and\nrobustness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 16:55:33 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kovachki", "Nikola B.", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1808.03679", "submitter": "Jianchao Lee", "authors": "Jianchao Lee, Qiannan Duan, Sifan Bi, Ruen Luo, Yachao Lian, Hanqiang\n  Liu, Ruixing Tian, Jiayuan Chen, Guodong Ma, Jinhong Gao, Zhaoyi Xu", "title": "Machine Learning Promoting Extreme Simplification of Spectroscopy\n  Equipment", "comments": "This is the second version. On pages 7 through 8, we have added a new\n  case about the spectral properties of mixtures. Specifically, paragraph 1 on\n  page 8 and Fig.7 is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectroscopy measurement is one of main pathways for exploring and\nunderstanding the nature. Today, it seems that racing artificial intelligence\nwill remould its styles. The algorithms contained in huge neural networks are\ncapable of substituting many of expensive and complex components of spectrum\ninstruments. In this work, we presented a smart machine learning strategy on\nthe measurement of absorbance curves, and also initially verified that an\nexceedingly-simplified equipment is sufficient to meet the needs for this\nstrategy. Further, with its simplicity, the setup is expected to infiltrate\ninto many scientific areas in versatile forms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 01:15:51 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 01:15:58 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Lee", "Jianchao", ""], ["Duan", "Qiannan", ""], ["Bi", "Sifan", ""], ["Luo", "Ruen", ""], ["Lian", "Yachao", ""], ["Liu", "Hanqiang", ""], ["Tian", "Ruixing", ""], ["Chen", "Jiayuan", ""], ["Ma", "Guodong", ""], ["Gao", "Jinhong", ""], ["Xu", "Zhaoyi", ""]]}, {"id": "1808.03698", "submitter": "Marcelo Medeiros", "authors": "Yuri Fonseca, Marcelo Medeiros, Gabriel Vasconcelos, Alvaro Veiga", "title": "BooST: Boosting Smooth Trees for Partial Effect Estimation in Nonlinear\n  Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new machine learning (ML) model for nonlinear\nregression called the Boosted Smooth Transition Regression Trees (BooST), which\nis a combination of boosting algorithms with smooth transition regression\ntrees. The main advantage of the BooST model is the estimation of the\nderivatives (partial effects) of very general nonlinear models. Therefore, the\nmodel can provide more interpretation about the mapping between the covariates\nand the dependent variable than other tree-based models, such as Random\nForests. We present several examples with both simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 20:37:52 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 00:48:14 GMT"}, {"version": "v3", "created": "Sat, 15 Dec 2018 13:41:37 GMT"}, {"version": "v4", "created": "Sun, 9 Jun 2019 14:37:04 GMT"}, {"version": "v5", "created": "Tue, 28 Jul 2020 00:27:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Fonseca", "Yuri", ""], ["Medeiros", "Marcelo", ""], ["Vasconcelos", "Gabriel", ""], ["Veiga", "Alvaro", ""]]}, {"id": "1808.03703", "submitter": "Daniel Kondratyuk", "authors": "Daniel Kondratyuk, Tom\\'a\\v{s} Gaven\\v{c}iak, Milan Straka, Jan\n  Haji\\v{c}", "title": "LemmaTag: Jointly Tagging and Lemmatizing for Morphologically-Rich\n  Languages with BRNNs", "comments": "8 pages, 3 figures. Submitted to EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LemmaTag, a featureless neural network architecture that jointly\ngenerates part-of-speech tags and lemmas for sentences by using bidirectional\nRNNs with character-level and word-level embeddings. We demonstrate that both\ntasks benefit from sharing the encoding part of the network, predicting tag\nsubcategories, and using the tagger output as an input to the lemmatizer. We\nevaluate our model across several languages with complex morphology, which\nsurpasses state-of-the-art accuracy in both part-of-speech tagging and\nlemmatization in Czech, German, and Arabic.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 20:46:32 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 16:36:40 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Kondratyuk", "Daniel", ""], ["Gaven\u010diak", "Tom\u00e1\u0161", ""], ["Straka", "Milan", ""], ["Haji\u010d", "Jan", ""]]}, {"id": "1808.03715", "submitter": "Sageev Oore", "authors": "Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, Karen Simonyan", "title": "This Time with Feeling: Learning Expressive Musical Performance", "comments": "Includes links to urls for audio samples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music generation has generally been focused on either creating scores or\ninterpreting them. We discuss differences between these two problems and\npropose that, in fact, it may be valuable to work in the space of direct $\\it\nperformance$ generation: jointly predicting the notes $\\it and$ $\\it also$\ntheir expressive timing and dynamics. We consider the significance and\nqualities of the data set needed for this. Having identified both a problem\ndomain and characteristics of an appropriate data set, we show an LSTM-based\nrecurrent network model that subjectively performs quite well on this task.\nCritically, we provide generated examples. We also include feedback from\nprofessional composers and musicians about some of these examples.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 21:53:51 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Oore", "Sageev", ""], ["Simon", "Ian", ""], ["Dieleman", "Sander", ""], ["Eck", "Douglas", ""], ["Simonyan", "Karen", ""]]}, {"id": "1808.03726", "submitter": "Muhao Chen", "authors": "Muhao Chen, Yingtao Tian, Haochen Chen, Kai-Wei Chang, Steven Skiena,\n  Carlo Zaniolo", "title": "Learning to Represent Bilingual Dictionaries", "comments": "CoNLL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilingual word embeddings have been widely used to capture the similarity of\nlexical semantics in different human languages. However, many applications,\nsuch as cross-lingual semantic search and question answering, can be largely\nbenefited from the cross-lingual correspondence between sentences and lexicons.\nTo bridge this gap, we propose a neural embedding model that leverages\nbilingual dictionaries. The proposed model is trained to map the literal word\ndefinitions to the cross-lingual target words, for which we explore with\ndifferent sentence encoding techniques. To enhance the learning process on\nlimited resources, our model adopts several critical learning strategies,\nincluding multi-task learning on different bridges of languages, and joint\nlearning of the dictionary model with a bilingual word embedding model.\nExperimental evaluation focuses on two applications. The results of the\ncross-lingual reverse dictionary retrieval task show our model's promising\nability of comprehending bilingual concepts based on descriptions, and\nhighlight the effectiveness of proposed learning strategies in improving\nperformance. Meanwhile, our model effectively addresses the bilingual\nparaphrase identification problem and significantly outperforms previous\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 23:21:07 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 10:14:33 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 20:14:24 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Chen", "Muhao", ""], ["Tian", "Yingtao", ""], ["Chen", "Haochen", ""], ["Chang", "Kai-Wei", ""], ["Skiena", "Steven", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1808.03733", "submitter": "Yuanfeng Song", "authors": "Di Jiang, Yuanfeng Song, Rongzhong Lian, Siqi Bao, Jinhua Peng, Huang\n  He, and Hua Wu", "title": "Familia: A Configurable Topic Modeling Framework for Industrial Text\n  Engineering", "comments": "21 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, a variety of topic models have been proposed for text\nengineering. However, except Probabilistic Latent Semantic Analysis (PLSA) and\nLatent Dirichlet Allocation (LDA), most of existing topic models are seldom\napplied or considered in industrial scenarios. This phenomenon is caused by the\nfact that there are very few convenient tools to support these topic models so\nfar. Intimidated by the demanding expertise and labor of designing and\nimplementing parameter inference algorithms, software engineers are prone to\nsimply resort to PLSA/LDA, without considering whether it is proper for their\nproblem at hand or not. In this paper, we propose a configurable topic modeling\nframework named Familia, in order to bridge the huge gap between academic\nresearch fruits and current industrial practice. Familia supports an important\nline of topic models that are widely applicable in text engineering scenarios.\nIn order to relieve burdens of software engineers without knowledge of Bayesian\nnetworks, Familia is able to conduct automatic parameter inference for a\nvariety of topic models. Simply through changing the data organization of\nFamilia, software engineers are able to easily explore a broad spectrum of\nexisting topic models or even design their own topic models, and find the one\nthat best suits the problem at hand. With its superior extendability, Familia\nhas a novel sampling mechanism that strikes balance between effectiveness and\nefficiency of parameter inference. Furthermore, Familia is essentially a big\ntopic modeling framework that supports parallel parameter inference and\ndistributed parameter storage. The utilities and necessity of Familia are\ndemonstrated in real-life industrial applications. Familia would significantly\nenlarge software engineers' arsenal of topic models and pave the way for\nutilizing highly customized topic models in real-life problems.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 01:14:50 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 06:26:12 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Jiang", "Di", ""], ["Song", "Yuanfeng", ""], ["Lian", "Rongzhong", ""], ["Bao", "Siqi", ""], ["Peng", "Jinhua", ""], ["He", "Huang", ""], ["Wu", "Hua", ""]]}, {"id": "1808.03737", "submitter": "Kan Ren", "authors": "Kan Ren, Yuchen Fang, Weinan Zhang, Shuhao Liu, Jiajun Li, Ya Zhang,\n  Yong Yu, Jun Wang", "title": "Learning Multi-touch Conversion Attribution with Dual-attention\n  Mechanisms for Online Advertising", "comments": "10 pages, 11 figures; CIKM 2018", "journal-ref": null, "doi": "10.1145/3269206.3271677", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online advertising, the Internet users may be exposed to a sequence of\ndifferent ad campaigns, i.e., display ads, search, or referrals from multiple\nchannels, before led up to any final sales conversion and transaction. For both\ncampaigners and publishers, it is fundamentally critical to estimate the\ncontribution from ad campaign touch-points during the customer journey\n(conversion funnel) and assign the right credit to the right ad exposure\naccordingly. However, the existing research on the multi-touch attribution\nproblem lacks a principled way of utilizing the users' pre-conversion actions\n(i.e., clicks), and quite often fails to model the sequential patterns among\nthe touch points from a user's behavior data. To make it worse, the current\nindustry practice is merely employing a set of arbitrary rules as the\nattribution model, e.g., the popular last-touch model assigns 100% credit to\nthe final touch-point regardless of actual attributions. In this paper, we\npropose a Dual-attention Recurrent Neural Network (DARNN) for the multi-touch\nattribution problem. It learns the attribution values through an attention\nmechanism directly from the conversion estimation objective. To achieve this,\nwe utilize sequence-to-sequence prediction for user clicks, and combine both\npost-view and post-click attribution patterns together for the final conversion\nestimation. To quantitatively benchmark attribution models, we also propose a\nnovel yet practical attribution evaluation scheme through the proxy of budget\nallocation (under the estimated attributions) over ad channels. The\nexperimental results on two real datasets demonstrate the significant\nperformance gains of our attribution model against the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 01:58:19 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 01:31:31 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Ren", "Kan", ""], ["Fang", "Yuchen", ""], ["Zhang", "Weinan", ""], ["Liu", "Shuhao", ""], ["Li", "Jiajun", ""], ["Zhang", "Ya", ""], ["Yu", "Yong", ""], ["Wang", "Jun", ""]]}, {"id": "1808.03749", "submitter": "Hongyang Li", "authors": "Hongyang Li, Xiaoyang Guo, Bo Dai, Wanli Ouyang, Xiaogang Wang", "title": "Neural Network Encapsulation", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A capsule is a collection of neurons which represents different variants of a\npattern in the network. The routing scheme ensures only certain capsules which\nresemble lower counterparts in the higher layer should be activated. However,\nthe computational complexity becomes a bottleneck for scaling up to larger\nnetworks, as lower capsules need to correspond to each and every higher\ncapsule. To resolve this limitation, we approximate the routing process with\ntwo branches: a master branch which collects primary information from its\ndirect contact in the lower layer and an aide branch that replenishes master\nbased on pattern variants encoded in other lower capsules. Compared with\nprevious iterative and unsupervised routing scheme, these two branches are\ncommunicated in a fast, supervised and one-time pass fashion. The complexity\nand runtime of the model are therefore decreased by a large margin. Motivated\nby the routing to make higher capsule have agreement with lower capsule, we\nextend the mechanism as a compensation for the rapid loss of information in\nnearby layers. We devise a feedback agreement unit to send back higher capsules\nas feedback. It could be regarded as an additional regularization to the\nnetwork. The feedback agreement is achieved by comparing the optimal transport\ndivergence between two distributions (lower and higher capsules). Such an\nadd-on witnesses a unanimous gain in both capsule and vanilla networks. Our\nproposed EncapNet performs favorably better against previous state-of-the-arts\non CIFAR10/100, SVHN and a subset of ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 04:36:53 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Li", "Hongyang", ""], ["Guo", "Xiaoyang", ""], ["Dai", "Bo", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1808.03752", "submitter": "Kai Wang", "authors": "Kai Wang and Yu Liu and Xiujuan Xu and Dan Lin", "title": "Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph Embedding (KGE) aims to represent entities and relations of\nknowledge graph in a low-dimensional continuous vector space. Recent works\nfocus on incorporating structural knowledge with additional information, such\nas entity descriptions, relation paths and so on. However, common used\nadditional information usually contains plenty of noise, which makes it hard to\nlearn valuable representation. In this paper, we propose a new kind of\nadditional information, called entity neighbors, which contain both semantic\nand topological features about given entity. We then develop a deep memory\nnetwork model to encode information from neighbors. Employing a gating\nmechanism, representations of structure and neighbors are integrated into a\njoint representation. The experimental results show that our model outperforms\nexisting KGE methods utilizing entity descriptions and achieves\nstate-of-the-art metrics on 4 datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 05:05:06 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wang", "Kai", ""], ["Liu", "Yu", ""], ["Xu", "Xiujuan", ""], ["Lin", "Dan", ""]]}, {"id": "1808.03753", "submitter": "Chris Mattmann", "authors": "Chris A. Mattmann, Sujen Shah, Brian Wilson", "title": "MARVIN: An Open Machine Learning Corpus and Environment for Automated\n  Machine Learning Primitive Annotation and Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this demo paper, we introduce the DARPA D3M program for automatic machine\nlearning (ML) and JPL's MARVIN tool that provides an environment to locate,\nannotate, and execute machine learning primitives for use in ML pipelines.\nMARVIN is a web-based application and associated back-end interface written in\nPython that enables composition of ML pipelines from hundreds of primitives\nfrom the world of Scikit-Learn, Keras, DL4J and other widely used libraries.\nMARVIN allows for the creation of Docker containers that run on Kubernetes\nclusters within DARPA to provide an execution environment for automated machine\nlearning. MARVIN currently contains over 400 datasets and challenge problems\nfrom a wide array of ML domains including routine classification and regression\nto advanced video/image classification and remote sensing.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 05:05:26 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Mattmann", "Chris A.", ""], ["Shah", "Sujen", ""], ["Wilson", "Brian", ""]]}, {"id": "1808.03793", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta and Florian Buettner and Hinrich Sch\\\"utze", "title": "Document Informed Neural Autoregressive Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context information around words helps in determining their actual meaning,\nfor example \"networks\" used in contexts of artificial neural networks or\nbiological neuron networks. Generative topic models infer topic-word\ndistributions, taking no or only little context into account. Here, we extend a\nneural autoregressive topic model to exploit the full context information\naround words in a document in a language modeling fashion. This results in an\nimproved performance in terms of generalization, interpretability and\napplicability. We apply our modeling approach to seven data sets from various\ndomains and demonstrate that our approach consistently outperforms\nstateof-the-art generative topic models. With the learned representations, we\nshow on an average a gain of 9.6% (0.57 Vs 0.52) in precision at retrieval\nfraction 0.02 and 7.2% (0.582 Vs 0.543) in F1 for text categorization.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 12:16:09 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Gupta", "Pankaj", ""], ["Buettner", "Florian", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1808.03835", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen", "title": "jLDADMM: A Java package for the LDA and DMM topic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we present jLDADMM---an easy-to-use Java toolkit\nfor conventional topic models. jLDADMM is released to provide alternatives for\ntopic modeling on normal or short texts. It provides implementations of the\nLatent Dirichlet Allocation topic model and the one-topic-per-document\nDirichlet Multinomial Mixture model (i.e. mixture of unigrams), using collapsed\nGibbs sampling. In addition, jLDADMM supplies a document clustering evaluation\nto compare topic models. jLDADMM is open-source and available to download at:\nhttps://github.com/datquocnguyen/jLDADMM\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 16:47:58 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Nguyen", "Dat Quoc", ""]]}, {"id": "1808.03843", "submitter": "Wei Tan", "authors": "Wei Tan, Shiyu Chang, Liana Fong, Cheng Li, Zijun Wang, Liangliang Cao", "title": "Matrix Factorization on GPUs with Memory Optimization and Approximate\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) discovers latent features from observations, which\nhas shown great promises in the fields of collaborative filtering, data\ncompression, feature extraction, word embedding, etc. While many\nproblem-specific optimization techniques have been proposed, alternating least\nsquare (ALS) remains popular due to its general applicability e.g. easy to\nhandle positive-unlabeled inputs, fast convergence and parallelization\ncapability. Current MF implementations are either optimized for a single\nmachine or with a need of a large computer cluster but still are insufficient.\nThis is because a single machine provides limited compute power for large-scale\ndata while multiple machines suffer from the network communication bottleneck.\n  To address the aforementioned challenge, accelerating ALS on graphics\nprocessing units (GPUs) is a promising direction. We propose the novel approach\nin enhancing the MF efficiency via both memory optimization and approximate\ncomputing. The former exploits GPU memory hierarchy to increase data reuse,\nwhile the later reduces unnecessary computing without hurting the convergence\nof learning algorithms. Extensive experiments on large-scale datasets show that\nour solution not only outperforms the competing CPU solutions by a large margin\nbut also has a 2x-4x performance gain compared to the state-of-the-art GPU\nsolutions. Our implementations are open-sourced and publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 17:36:10 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Tan", "Wei", ""], ["Chang", "Shiyu", ""], ["Fong", "Liana", ""], ["Li", "Cheng", ""], ["Wang", "Zijun", ""], ["Cao", "Liangliang", ""]]}, {"id": "1808.03856", "submitter": "Thomas M\\\"uller", "authors": "Thomas M\\\"uller, Brian McWilliams, Fabrice Rousselle, Markus Gross,\n  Jan Nov\\'ak", "title": "Neural Importance Sampling", "comments": "19 pages, 15 figures. Accepted for publication in ACM Transactions on\n  Graphics; presented at SIGGRAPH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use deep neural networks for generating samples in Monte Carlo\nintegration. Our work is based on non-linear independent components estimation\n(NICE), which we extend in numerous ways to improve performance and enable its\napplication to integration problems. First, we introduce piecewise-polynomial\ncoupling transforms that greatly increase the modeling power of individual\ncoupling layers. Second, we propose to preprocess the inputs of neural networks\nusing one-blob encoding, which stimulates localization of computation and\nimproves inference. Third, we derive a gradient-descent-based optimization for\nthe KL and the $\\chi^2$ divergence for the specific application of Monte Carlo\nintegration with unnormalized stochastic estimates of the target distribution.\nOur approach enables fast and accurate inference and efficient sample\ngeneration independently of the dimensionality of the integration domain. We\nshow its benefits on generating natural images and in two applications to\nlight-transport simulation: first, we demonstrate learning of joint\npath-sampling densities in the primary sample space and importance sampling of\nmulti-dimensional path prefixes thereof. Second, we use our technique to\nextract conditional directional densities driven by the product of incident\nillumination and the BSDF in the rendering equation, and we leverage the\ndensities for path guiding. In all applications, our approach yields on-par or\nhigher performance than competing techniques at equal sample count.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 20:12:49 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 15:16:55 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 06:59:09 GMT"}, {"version": "v4", "created": "Mon, 27 May 2019 07:57:34 GMT"}, {"version": "v5", "created": "Tue, 3 Sep 2019 07:18:51 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["M\u00fcller", "Thomas", ""], ["McWilliams", "Brian", ""], ["Rousselle", "Fabrice", ""], ["Gross", "Markus", ""], ["Nov\u00e1k", "Jan", ""]]}, {"id": "1808.03857", "submitter": "Aadirupa Saha", "authors": "Aadirupa Saha, Arun Rajkumar", "title": "Ranking with Features: Algorithm and A Graph Theoretic Analysis", "comments": "Bug in one proof, wrong result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of ranking a set of items from pairwise comparisons\nin the presence of features associated with the items. Recent works have\nestablished that $O(n\\log(n))$ samples are needed to rank well when there is no\nfeature information present. However, this might be sub-optimal in the presence\nof associated features. We introduce a new probabilistic preference model\ncalled feature-Bradley-Terry-Luce (f-BTL) model that generalizes the standard\nBTL model to incorporate feature information. We present a new least squares\nbased algorithm called fBTL-LS which we show requires much lesser than\n$O(n\\log(n))$ pairs to obtain a good ranking -- precisely our new sample\ncomplexity bound is of $O(\\alpha\\log \\alpha)$, where $\\alpha$ denotes the\nnumber of `independent items' of the set, in general $\\alpha << n$. Our\nanalysis is novel and makes use of tools from classical graph matching theory\nto provide tighter bounds that sheds light on the true complexity of the\nranking problem, capturing the item dependencies in terms of their feature\nrepresentations. This was not possible with earlier matrix completion based\ntools used for this problem. We also prove an information theoretic lower bound\non the required sample complexity for recovering the underlying ranking, which\nessentially shows the tightness of our proposed algorithms. The efficacy of our\nproposed algorithms are validated through extensive experimental evaluations on\na variety of synthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 20:15:28 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 15:53:42 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Saha", "Aadirupa", ""], ["Rajkumar", "Arun", ""]]}, {"id": "1808.03873", "submitter": "Tianlin Liu", "authors": "Tianlin Liu", "title": "A Consistent Method for Learning OOMs from Asymptotically Stationary\n  Time Series Data Containing Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the traditional framework of spectral learning of stochastic time series\nmodels, model parameters are estimated based on trajectories of fully recorded\nobservations. However, real-world time series data often contain missing\nvalues, and worse, the distributions of missingness events over time are often\nnot independent of the visible process. Recently, a spectral OOM learning\nalgorithm for time series with missing data was introduced and proved to be\nconsistent, albeit under quite strong conditions. Here we refine the algorithm\nand prove that the original strong conditions can be very much relaxed. We\nvalidate our theoretical findings by numerical experiments, showing that the\nalgorithm can consistently handle missingness patterns whose dynamic interacts\nwith the visible process.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 22:28:11 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 10:17:37 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Liu", "Tianlin", ""]]}, {"id": "1808.03880", "submitter": "Yaron Singer", "authors": "Eric Balkanski and Yaron Singer", "title": "Parallelization does not Accelerate Convex Optimization: Adaptivity\n  Lower Bounds for Non-smooth Convex Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the limitations of parallelization in convex\noptimization. A convenient approach to study parallelization is through the\nprism of \\emph{adaptivity} which is an information theoretic measure of the\nparallel runtime of an algorithm [BS18]. Informally, adaptivity is the number\nof sequential rounds an algorithm needs to make when it can execute\npolynomially-many queries in parallel at every round. For combinatorial\noptimization with black-box oracle access, the study of adaptivity has recently\nled to exponential accelerations in parallel runtime and the natural question\nis whether dramatic accelerations are achievable for convex optimization.\n  For the problem of minimizing a non-smooth convex function $f:[0,1]^n\\to\n\\mathbb{R}$ over the unit Euclidean ball, we give a tight lower bound that\nshows that even when $\\texttt{poly}(n)$ queries can be executed in parallel,\nthere is no randomized algorithm with $\\tilde{o}(n^{1/3})$ rounds of adaptivity\nthat has convergence rate that is better than those achievable with a\none-query-per-round algorithm. A similar lower bound was obtained by Nemirovski\n[Nem94], however that result holds for the $\\ell_{\\infty}$-setting instead of\n$\\ell_2$. In addition, we also show a tight lower bound that holds for\nLipschitz and strongly convex functions.\n  At the time of writing this manuscript we were not aware of Nemirovski's\nresult. The construction we use is similar to the one in [Nem94], though our\nanalysis is different. Due to the close relationship between this work and\n[Nem94], we view the research contribution of this manuscript limited and it\nshould serve as an instructful approach to understanding lower bounds for\nparallel optimization.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 01:56:17 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 19:29:57 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Balkanski", "Eric", ""], ["Singer", "Yaron", ""]]}, {"id": "1808.03894", "submitter": "Reza Ghaeini", "authors": "Reza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli", "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study\n  on Natural Language Inference", "comments": "11 pages, 11 figures, accepted as a short paper at EMNLP 2018", "journal-ref": "EMNLP 2018", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have achieved remarkable success in natural language\ninference (NLI) tasks. While these models are widely explored, they are hard to\ninterpret and it is often unclear how and why they actually work. In this\npaper, we take a step toward explaining such deep learning based models through\na case study on a popular neural model for NLI. In particular, we propose to\ninterpret the intermediate layers of NLI models by visualizing the saliency of\nattention and LSTM gating signals. We present several examples for which our\nmethods are able to reveal interesting insights and identify the critical\ninformation contributing to the model decisions.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 05:42:26 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Ghaeini", "Reza", ""], ["Fern", "Xiaoli Z.", ""], ["Tadepalli", "Prasad", ""]]}, {"id": "1808.03908", "submitter": "Xiaoyu Du", "authors": "Xiangnan He, Zhankui He, Xiaoyu Du and Tat-Seng Chua", "title": "Adversarial Personalized Ranking for Recommendation", "comments": "SIGIR 2018", "journal-ref": null, "doi": "10.1145/3209978.3209981", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Item recommendation is a personalized ranking task. To this end, many\nrecommender systems optimize models with pairwise ranking objectives, such as\nthe Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) ---\nthe most widely used model in recommendation --- as a demonstration, we show\nthat optimizing it with BPR leads to a recommender model that is not robust. In\nparticular, we find that the resultant model is highly vulnerable to\nadversarial perturbations on its model parameters, which implies the possibly\nlarge error in generalization.\n  To enhance the robustness of a recommender model and thus improve its\ngeneralization performance, we propose a new optimization framework, namely\nAdversarial Personalized Ranking (APR). In short, our APR enhances the pairwise\nranking method BPR by performing adversarial training. It can be interpreted as\nplaying a minimax game, where the minimization of the BPR objective function\nmeanwhile defends an adversary, which adds adversarial perturbations on model\nparameters to maximize the BPR objective function. To illustrate how it works,\nwe implement APR on MF by adding adversarial perturbations on the embedding\nvectors of users and items. Extensive experiments on three public real-world\ndatasets demonstrate the effectiveness of APR --- by optimizing MF with APR, it\noutperforms BPR with a relative improvement of 11.2% on average and achieves\nstate-of-the-art performance for item recommendation. Our implementation is\navailable at: https://github.com/hexiangnan/adversarial_personalized_ranking.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 08:26:25 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["He", "Xiangnan", ""], ["He", "Zhankui", ""], ["Du", "Xiaoyu", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1808.03912", "submitter": "Xiaoyu Du", "authors": "Xiangnan He, Xiaoyu Du, Xiang Wang, Feng Tian, Jinhui Tang and\n  Tat-Seng Chua", "title": "Outer Product-based Neural Collaborative Filtering", "comments": "IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we contribute a new multi-layer neural network architecture\nnamed ONCF to perform collaborative filtering. The idea is to use an outer\nproduct to explicitly model the pairwise correlations between the dimensions of\nthe embedding space. In contrast to existing neural recommender models that\ncombine user embedding and item embedding via a simple concatenation or\nelement-wise product, our proposal of using outer product above the embedding\nlayer results in a two-dimensional interaction map that is more expressive and\nsemantically plausible. Above the interaction map obtained by outer product, we\npropose to employ a convolutional neural network to learn high-order\ncorrelations among embedding dimensions. Extensive experiments on two public\nimplicit feedback data demonstrate the effectiveness of our proposed ONCF\nframework, in particular, the positive effect of using outer product to model\nthe correlations between embedding dimensions in the low level of multi-layer\nneural recommender model. The experiment codes are available at:\nhttps://github.com/duxy-me/ConvNCF\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 08:56:06 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["He", "Xiangnan", ""], ["Du", "Xiaoyu", ""], ["Wang", "Xiang", ""], ["Tian", "Feng", ""], ["Tang", "Jinhui", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1808.03920", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Ziyin Liu, Amir Zadeh, Louis-Philippe Morency", "title": "Multimodal Language Analysis with Recurrent Multistage Fusion", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational modeling of human multimodal language is an emerging research\narea in natural language processing spanning the language, visual and acoustic\nmodalities. Comprehending multimodal language requires modeling not only the\ninteractions within each modality (intra-modal interactions) but more\nimportantly the interactions between modalities (cross-modal interactions). In\nthis paper, we propose the Recurrent Multistage Fusion Network (RMFN) which\ndecomposes the fusion problem into multiple stages, each of them focused on a\nsubset of multimodal signals for specialized, effective fusion. Cross-modal\ninteractions are modeled using this multistage fusion approach which builds\nupon intermediate representations of previous stages. Temporal and intra-modal\ninteractions are modeled by integrating our proposed fusion approach with a\nsystem of recurrent neural networks. The RMFN displays state-of-the-art\nperformance in modeling human multimodal language across three public datasets\nrelating to multimodal sentiment analysis, emotion recognition, and speaker\ntraits recognition. We provide visualizations to show that each stage of fusion\nfocuses on a different subset of multimodal signals, learning increasingly\ndiscriminative multimodal representations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 10:04:45 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Liang", "Paul Pu", ""], ["Liu", "Ziyin", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1808.03926", "submitter": "Adnan Akhundov", "authors": "Adnan Akhundov, Dietrich Trautmann, Georg Groh", "title": "Sequence Labeling: A Practical Approach", "comments": "For the source code and detailed experimental results, see\n  http://github.com/aakhundov/sequence-labeling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a practical approach to solving sequence labeling problem assuming\nunavailability of domain expertise and scarcity of informational and\ncomputational resources. To this end, we utilize a universal end-to-end\nBi-LSTM-based neural sequence labeling model applicable to a wide range of NLP\ntasks and languages. The model combines morphological, semantic, and structural\ncues extracted from data to arrive at informed predictions. The model's\nperformance is evaluated on eight benchmark datasets (covering three tasks:\nPOS-tagging, NER, and Chunking, and four languages: English, German, Dutch, and\nSpanish). We observe state-of-the-art results on four of them: CoNLL-2012\n(English NER), CoNLL-2002 (Dutch NER), GermEval 2014 (German NER), Tiger Corpus\n(German POS-tagging), and competitive performance on the rest.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 11:53:04 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Akhundov", "Adnan", ""], ["Trautmann", "Dietrich", ""], ["Groh", "Georg", ""]]}, {"id": "1808.03944", "submitter": "Chengjia Wang", "authors": "Chengjia Wang, Gillian Macnaught, Giorgos Papanastasiou, Tom\n  MacGillivray, and David Newby", "title": "Unsupervised learning for cross-domain medical image synthesis using\n  deformation invariant cycle consistency networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the cycle-consistent generative adversarial networks (CycleGAN) has\nbeen widely used for synthesis of multi-domain medical images. The\ndomain-specific nonlinear deformations captured by CycleGAN make the\nsynthesized images difficult to be used for some applications, for example,\ngenerating pseudo-CT for PET-MR attenuation correction. This paper presents a\ndeformation-invariant CycleGAN (DicycleGAN) method using deformable\nconvolutional layers and new cycle-consistency losses. Its robustness dealing\nwith data that suffer from domain-specific nonlinear deformations has been\nevaluated through comparison experiments performed on a multi-sequence brain MR\ndataset and a multi-modality abdominal dataset. Our method has displayed its\nability to generate synthesized data that is aligned with the source while\nmaintaining a proper quality of signal compared to CycleGAN-generated data. The\nproposed model also obtained comparable performance with CycleGAN when data\nfrom the source and target domains are alignable through simple affine\ntransformations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 13:49:19 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wang", "Chengjia", ""], ["Macnaught", "Gillian", ""], ["Papanastasiou", "Giorgos", ""], ["MacGillivray", "Tom", ""], ["Newby", "David", ""]]}, {"id": "1808.03953", "submitter": "Adeel Pervez", "authors": "Adeel Pervez", "title": "A Fourier View of REINFORCE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a connection between the Fourier spectrum of Boolean functions and\nthe REINFORCE gradient estimator for binary latent variable models. We show\nthat REINFORCE estimates (up to a factor) the degree-1 Fourier coefficients of\na Boolean function. Using this connection we offer a new perspective on\nvariance reduction in gradient estimation for latent variable models: namely,\nthat variance reduction involves eliminating or reducing Fourier coefficients\nthat do not have degree 1. We then use this connection to develop low-variance\nunbiased gradient estimators for binary latent variable models such as sigmoid\nbelief networks. The estimator is based upon properties of the noise operator\nfrom Boolean Fourier theory and involves a sample-dependent baseline added to\nthe REINFORCE estimator in a way that keeps the estimator unbiased. The\nbaseline can be plugged into existing gradient estimators for further variance\nreduction.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 15:14:04 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Pervez", "Adeel", ""]]}, {"id": "1808.03958", "submitter": "Zhaofei Yu", "authors": "Shanshan Jia and Zhaofei Yu and Arno Onken and Yonghong Tian and\n  Tiejun Huang and Jian K. Liu", "title": "Neural System Identification with Spike-triggered Non-negative Matrix\n  Factorization", "comments": "updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuronal circuits formed in the brain are complex with intricate connection\npatterns. Such complexity is also observed in the retina as a relatively simple\nneuronal circuit. A retinal ganglion cell receives excitatory inputs from\nneurons in previous layers as driving forces to fire spikes. Analytical methods\nare required that can decipher these components in a systematic manner.\nRecently a method termed spike-triggered non-negative matrix factorization\n(STNMF) has been proposed for this purpose. In this study, we extend the scope\nof the STNMF method. By using the retinal ganglion cell as a model system, we\nshow that STNMF can detect various computational properties of upstream bipolar\ncells, including spatial receptive field, temporal filter, and transfer\nnonlinearity. In addition, we recover synaptic connection strengths from the\nweight matrix of STNMF. Furthermore, we show that STNMF can separate spikes of\na ganglion cell into a few subsets of spikes where each subset is contributed\nby one presynaptic bipolar cell. Taken together, these results corroborate that\nSTNMF is a useful method for deciphering the structure of neuronal circuits.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 15:37:40 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 05:09:10 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 03:02:36 GMT"}, {"version": "v4", "created": "Sun, 1 Mar 2020 12:14:30 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Jia", "Shanshan", ""], ["Yu", "Zhaofei", ""], ["Onken", "Arno", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""], ["Liu", "Jian K.", ""]]}, {"id": "1808.03965", "submitter": "Hongyang Gao", "authors": "Hongyang Gao, Zhengyang Wang, Shuiwang Ji", "title": "Large-Scale Learnable Graph Convolutional Networks", "comments": null, "journal-ref": "In Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining (pp. 1416-1424). ACM (2018)", "doi": "10.1145/3219819.3219947", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved great success on grid-like\ndata such as images, but face tremendous challenges in learning from more\ngeneric data such as graphs. In CNNs, the trainable local filters enable the\nautomatic extraction of high-level features. The computation with filters\nrequires a fixed number of ordered units in the receptive fields. However, the\nnumber of neighboring units is neither fixed nor are they ordered in generic\ngraphs, thereby hindering the applications of convolutional operations. Here,\nwe address these challenges by proposing the learnable graph convolutional\nlayer (LGCL). LGCL automatically selects a fixed number of neighboring nodes\nfor each feature based on value ranking in order to transform graph data into\ngrid-like structures in 1-D format, thereby enabling the use of regular\nconvolutional operations on generic graphs. To enable model training on\nlarge-scale graphs, we propose a sub-graph training method to reduce the\nexcessive memory and computational resource requirements suffered by prior\nmethods on graph convolutions. Our experimental results on node classification\ntasks in both transductive and inductive learning settings demonstrate that our\nmethods can achieve consistently better performance on the Cora, Citeseer,\nPubmed citation network, and protein-protein interaction network datasets. Our\nresults also indicate that the proposed methods using sub-graph training\nstrategy are more efficient as compared to prior approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 16:22:12 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Gao", "Hongyang", ""], ["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1808.03987", "submitter": "Abhirami Harilal", "authors": "Shamik Ghosh, Abhirami Harilal, A. R. Sahasransu, Ritesh Kumar Singh\n  and Satyaki Bhattacharya", "title": "A simulation study to distinguish prompt photon from $\\pi^0$ and beam\n  halo in a granular calorimeter using deep networks", "comments": null, "journal-ref": null, "doi": "10.1088/1748-0221/14/01/P01011", "report-no": null, "categories": "physics.ins-det cs.LG hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a hadron collider environment identification of prompt photons originating\nin a hard partonic scattering process and rejection of non-prompt photons\ncoming from hadronic jets or from beam related sources, is the first step for\nstudy of processes with photons in final state. Photons coming from decay of\n$\\pi_0$'s produced inside a hadronic jet and photons produced in catastrophic\nbremsstrahlung by beam halo muons are two major sources of non-prompt photons.\nIn this paper the potential of deep learning methods for separating the prompt\nphotons from beam halo and $\\pi^0$'s in the electromagnetic calorimeter of a\ncollider detector is investigated, using an approximate description of the CMS\ndetector. It is shown that, using only calorimetric information as images with\na Convolutional Neural Network, beam halo (and $\\pi^{0}$) can be separated from\nphoton with 99.96\\% (97.7\\%) background rejection for 99.00\\% (90.0\\%) signal\nefficiency which is much better than traditionally employed variables.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 19:04:29 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 19:56:55 GMT"}, {"version": "v3", "created": "Sat, 22 Dec 2018 13:17:24 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ghosh", "Shamik", ""], ["Harilal", "Abhirami", ""], ["Sahasransu", "A. R.", ""], ["Singh", "Ritesh Kumar", ""], ["Bhattacharya", "Satyaki", ""]]}, {"id": "1808.04008", "submitter": "Aadirupa Saha", "authors": "Aadirupa Saha and Aditya Gopalan", "title": "PAC Battling Bandits in the Plackett-Luce Model", "comments": "In 30th International Conference on Algorithmic Learning Theory\n  (ALT), 2019. (45 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the probably approximately correct (PAC) \\emph{Battling-Bandit}\nproblem with the Plackett-Luce (PL) subset choice model--an online learning\nframework where at each trial the learner chooses a subset of $k$ arms from a\nfixed set of $n$ arms, and subsequently observes a stochastic feedback\nindicating preference information of the items in the chosen subset, e.g., the\nmost preferred item or ranking of the top $m$ most preferred items etc. The\nobjective is to identify a near-best item in the underlying PL model with high\nconfidence. This generalizes the well-studied PAC \\emph{Dueling-Bandit} problem\nover $n$ arms, which aims to recover the \\emph{best-arm} from pairwise\npreference information, and is known to require $O(\\frac{n}{\\epsilon^2} \\ln\n\\frac{1}{\\delta})$ sample complexity \\citep{Busa_pl,Busa_top}. We study the\nsample complexity of this problem under various feedback models: (1) Winner of\nthe subset (WI), and (2) Ranking of top-$m$ items (TR) for $2\\le m \\le k$. We\nshow, surprisingly, that with winner information (WI) feedback over subsets of\nsize $2 \\leq k \\leq n$, the best achievable sample complexity is still $O\\left(\n\\frac{n}{\\epsilon^2} \\ln \\frac{1}{\\delta}\\right)$, independent of $k$, and the\nsame as that in the Dueling Bandit setting ($k=2$). For the more general\ntop-$m$ ranking (TR) feedback model, we show a significantly smaller lower\nbound on sample complexity of $\\Omega\\bigg( \\frac{n}{m\\epsilon^2} \\ln\n\\frac{1}{\\delta}\\bigg)$, which suggests a multiplicative reduction by a factor\n${m}$ owing to the additional information revealed from preferences among $m$\nitems instead of just $1$. We also propose two algorithms for the PAC problem\nwith the TR feedback model with optimal (upto logarithmic factors) sample\ncomplexity guarantees, establishing the increase in statistical efficiency from\nexploiting rank-ordered feedback.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 22:23:43 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 08:46:23 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 21:45:58 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Saha", "Aadirupa", ""], ["Gopalan", "Aditya", ""]]}, {"id": "1808.04022", "submitter": "Thach Le Nguyen", "authors": "Thach Le Nguyen and Severin Gsponer and Iulia Ilie and Georgiana Ifrim", "title": "Interpretable Time Series Classification using All-Subsequence Learning\n  and Symbolic Representations in Time and Frequency Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time series classification literature has expanded rapidly over the last\ndecade, with many new classification approaches published each year. The\nresearch focus has mostly been on improving the accuracy and efficiency of\nclassifiers, while their interpretability has been somewhat neglected.\nClassifier interpretability has become a critical constraint for many\napplication domains and the introduction of the 'right to explanation' GDPR EU\nlegislation in May 2018 is likely to further emphasize the importance of\nexplainable learning algorithms. In this work we analyse the state-of-the-art\nfor time series classification, and propose new algorithms that aim to maintain\nthe classifier accuracy and efficiency, but keep interpretability as a key\ndesign constraint. We present new time series classification algorithms that\nadvance the state-of-the-art by implementing the following three key ideas: (1)\nMultiple resolutions of symbolic approximations: we combine symbolic\nrepresentations obtained using different parameters; (2) Multiple domain\nrepresentations: we combine symbolic approximations in time (e.g., SAX) and\nfrequency (e.g., SFA) domains; (3) Efficient navigation of a huge\nsymbolic-words space: we adapt a symbolic sequence classifier named SEQL, to\nmake it work with multiple domain representations (e.g., SAX-SEQL, SFA-SEQL),\nand use its greedy feature selection strategy to effectively filter the best\nfeatures for each representation. We show that a multi-resolution multi-domain\nlinear classifier, SAX-SFA-SEQL, achieves a similar accuracy to the\nstate-of-the-art COTE ensemble, and to a recent deep learning method (FCN), but\nuses a fraction of the time required by either COTE or FCN. We discuss the\naccuracy, efficiency and interpretability of our proposed algorithms. To\nfurther analyse the interpretability aspect of our classifiers, we present a\ncase study on an ecology benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 23:47:10 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Nguyen", "Thach Le", ""], ["Gsponer", "Severin", ""], ["Ilie", "Iulia", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "1808.04096", "submitter": "H\\'el\\`ene Plisnier", "authors": "H\\'el\\`ene Plisnier, Denis Steckelmacher, Tim Brys, Diederik M.\n  Roijers, Ann Now\\'e", "title": "Directed Policy Gradient for Safe Reinforcement Learning with Human\n  Advice", "comments": "Accepted at the European Workshop on Reinforcement Learning 2018\n  (EWRL14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many currently deployed Reinforcement Learning agents work in an environment\nshared with humans, be them co-workers, users or clients. It is desirable that\nthese agents adjust to people's preferences, learn faster thanks to their help,\nand act safely around them. We argue that most current approaches that learn\nfrom human feedback are unsafe: rewarding or punishing the agent a-posteriori\ncannot immediately prevent it from wrong-doing. In this paper, we extend Policy\nGradient to make it robust to external directives, that would otherwise break\nthe fundamentally on-policy nature of Policy Gradient. Our technique, Directed\nPolicy Gradient (DPG), allows a teacher or backup policy to override the agent\nbefore it acts undesirably, while allowing the agent to leverage human advice\nor directives to learn faster. Our experiments demonstrate that DPG makes the\nagent learn much faster than reward-based approaches, while requiring an order\nof magnitude less advice.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 08:12:22 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Plisnier", "H\u00e9l\u00e8ne", ""], ["Steckelmacher", "Denis", ""], ["Brys", "Tim", ""], ["Roijers", "Diederik M.", ""], ["Now\u00e9", "Ann", ""]]}, {"id": "1808.04127", "submitter": "Robert Schwarzenberg", "authors": "David Harbecke, Robert Schwarzenberg, Christoph Alt", "title": "Learning Explanations from Language Data", "comments": "Appears in 2018 EMNLP Workshop on Analyzing and Interpreting Neural\n  Networks for NLP (BlackboxNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PatternAttribution is a recent method, introduced in the vision domain, that\nexplains classifications of deep neural networks. We demonstrate that it also\ngenerates meaningful interpretations in the language domain.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 09:51:46 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Harbecke", "David", ""], ["Schwarzenberg", "Robert", ""], ["Alt", "Christoph", ""]]}, {"id": "1808.04152", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu, and Josef Kittler", "title": "Learning Discriminative Hashing Codes for Cross-Modal Retrieval based on\n  Multi-view Features", "comments": "28 pages, 10 figures, 13 tables. The paper is under consideration at\n  Pattern Analysis and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing techniques have been applied broadly in retrieval tasks due to their\nlow storage requirements and high speed of processing. Many hashing methods\nbased on a single view have been extensively studied for information retrieval.\nHowever, the representation capacity of a single view is insufficient and some\ndiscriminative information is not captured, which results in limited\nimprovement. In this paper, we employ multiple views to represent images and\ntexts for enriching the feature information. Our framework exploits the\ncomplementary information among multiple views to better learn the\ndiscriminative compact hash codes. A discrete hashing learning framework that\njointly performs classifier learning and subspace learning is proposed to\ncomplete multiple search tasks simultaneously. Our framework includes two\nstages, namely a kernelization process and a quantization process.\nKernelization aims to find a common subspace where multi-view features can be\nfused. The quantization stage is designed to learn discriminative unified\nhashing codes. Extensive experiments are performed on single-label datasets\n(WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE) and the\nexperimental results indicate the superiority of our method compared with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 11:18:49 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 06:11:35 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 08:36:38 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1808.04216", "submitter": "Tobias Backes", "authors": "Tobias Backes", "title": "Effective Unsupervised Author Disambiguation with Relative Frequencies", "comments": "Proceedings of JCDL 2018", "journal-ref": null, "doi": "10.1145/3197026.3197036", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work addresses the problem of author name homonymy in the Web of\nScience. Aiming for an efficient, simple and straightforward solution, we\nintroduce a novel probabilistic similarity measure for author name\ndisambiguation based on feature overlap. Using the researcher-ID available for\na subset of the Web of Science, we evaluate the application of this measure in\nthe context of agglomeratively clustering author mentions. We focus on a\nconcise evaluation that shows clearly for which problem setups and at which\ntime during the clustering process our approach works best. In contrast to most\nother works in this field, we are sceptical towards the performance of author\nname disambiguation methods in general and compare our approach to the trivial\nsingle-cluster baseline. Our results are presented separately for each correct\nclustering size as we can explain that, when treating all cases together, the\ntrivial baseline and more sophisticated approaches are hardly distinguishable\nin terms of evaluation results. Our model shows state-of-the-art performance\nfor all correct clustering sizes without any discriminative training and with\ntuning only one convergence parameter.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 10:09:54 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Backes", "Tobias", ""]]}, {"id": "1808.04228", "submitter": "Zhan Yang", "authors": "Zhan Yang, Osolo Ian Raymond, ChengYuan Zhang, Ying Wan, Jun Long", "title": "DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human\n  Activity Recognition", "comments": "19 pages, 5 figures, 6 tables, accepted by IEEE Access", "journal-ref": "IEEE ACCESS, vol. 6, pp. 56750-56764, 2018", "doi": "10.1109/ACCESS.2018.2873315", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) are currently popular in human\nactivity recognition applications. However, in the face of modern artificial\nintelligence sensor-based games, many research achievements cannot be\npractically applied on portable devices. DCNNs are typically resource-intensive\nand too large to be deployed on portable devices, thus this limits the\npractical application of complex activity detection. In addition, since\nportable devices do not possess high-performance Graphic Processing Units\n(GPUs), there is hardly any improvement in Action Game (ACT) experience.\nBesides, in order to deal with multi-sensor collaboration, all previous human\nactivity recognition models typically treated the representations from\ndifferent sensor signal sources equally. However, distinct types of activities\nshould adopt different fusion strategies. In this paper, a novel scheme is\nproposed. This scheme is used to train 2-bit Convolutional Neural Networks with\nweights and activations constrained to {-0.5,0,0.5}. It takes into account the\ncorrelation between different sensor signal sources and the activity types.\nThis model, which we refer to as DFTerNet, aims at producing a more reliable\ninference and better trade-offs for practical applications. Our basic idea is\nto exploit quantization of weights and activations directly in pre-trained\nfilter banks and adopt dynamic fusion strategies for different activity types.\nExperiments demonstrate that by using dynamic fusion strategy can exceed the\nbaseline model performance by up to ~5% on activity recognition like\nOPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we\nwere able to achieve performances closer to that of full-precision counterpart.\nThese results were also verified using the UniMiB-SHAR dataset. In addition,\nthe proposed method can achieve ~9x acceleration on CPUs and ~11x memory\nsaving.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 02:33:34 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 04:47:24 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Yang", "Zhan", ""], ["Raymond", "Osolo Ian", ""], ["Zhang", "ChengYuan", ""], ["Wan", "Ying", ""], ["Long", "Jun", ""]]}, {"id": "1808.04244", "submitter": "Dongrui Wu", "authors": "Dongrui Wu and Jian Huang", "title": "Affect Estimation in 3D Space Using Multi-Task Active Learning for\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquisition of labeled training samples for affective computing is usually\ncostly and time-consuming, as affects are intrinsically subjective, subtle and\nuncertain, and hence multiple human assessors are needed to evaluate each\naffective sample. Particularly, for affect estimation in the 3D space of\nvalence, arousal and dominance, each assessor has to perform the evaluations in\nthree dimensions, which makes the labeling problem even more challenging. Many\nsophisticated machine learning approaches have been proposed to reduce the data\nlabeling requirement in various other domains, but so far few have considered\naffective computing. This paper proposes two multi-task active learning for\nregression approaches, which select the most beneficial samples to label, by\nconsidering the three affect primitives simultaneously. Experimental results on\nthe VAM corpus demonstrated that our optimal sample selection approaches can\nresult in better estimation performance than random selection and several\ntraditional single-task active learning approaches. Thus, they can help\nalleviate the data labeling problem in affective computing, i.e., better\nestimation performance can be obtained from fewer labeling queries.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:39:46 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 19:58:15 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Wu", "Dongrui", ""], ["Huang", "Jian", ""]]}, {"id": "1808.04245", "submitter": "Dongrui Wu", "authors": "Dongrui Wu and Chin-Teng Lin and Jian Huang", "title": "Active Learning for Regression Using Greedy Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression problems are pervasive in real-world applications. Generally a\nsubstantial amount of labeled samples are needed to build a regression model\nwith good generalization ability. However, many times it is relatively easy to\ncollect a large number of unlabeled samples, but time-consuming or expensive to\nlabel them. Active learning for regression (ALR) is a methodology to reduce the\nnumber of labeled samples, by selecting the most beneficial ones to label,\ninstead of random selection. This paper proposes two new ALR approaches based\non greedy sampling (GS). The first approach (GSy) selects new samples to\nincrease the diversity in the output space, and the second (iGS) selects new\nsamples to increase the diversity in both input and output spaces. Extensive\nexperiments on 12 UCI and CMU StatLib datasets from various domains, and on 15\nsubjects on EEG-based driver drowsiness estimation, verified their\neffectiveness and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:29:19 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wu", "Dongrui", ""], ["Lin", "Chin-Teng", ""], ["Huang", "Jian", ""]]}, {"id": "1808.04256", "submitter": "Chenyu You", "authors": "Chenyu You, Guang Li, Yi Zhang, Xiaoliu Zhang, Hongming Shan,\n  Shenghong Ju, Zhen Zhao, Zhuiyang Zhang, Wenxiang Cong, Michael W. Vannier,\n  Punam K. Saha, Ge Wang", "title": "CT Super-resolution GAN Constrained by the Identical, Residual, and\n  Cycle Learning Ensemble(GAN-CIRCLE)", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging 2019", "doi": "10.1109/TMI.2019.2922960", "report-no": "TMI-2019-0250", "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) is widely used in screening, diagnosis, and\nimage-guided therapy for both clinical and research purposes. Since CT involves\nionizing radiation, an overarching thrust of related technical research is\ndevelopment of novel methods enabling ultrahigh quality imaging with fine\nstructural details while reducing the X-ray radiation. In this paper, we\npresent a semi-supervised deep learning approach to accurately recover\nhigh-resolution (HR) CT images from low-resolution (LR) counterparts.\nSpecifically, with the generative adversarial network (GAN) as the building\nblock, we enforce the cycle-consistency in terms of the Wasserstein distance to\nestablish a nonlinear end-to-end mapping from noisy LR input images to denoised\nand deblurred HR outputs. We also include the joint constraints in the loss\nfunction to facilitate structural preservation. In this deep imaging process,\nwe incorporate deep convolutional neural network (CNN), residual learning, and\nnetwork in network techniques for feature extraction and restoration. In\ncontrast to the current trend of increasing network depth and complexity to\nboost the CT imaging performance, which limit its real-world applications by\nimposing considerable computational and memory overheads, we apply a parallel\n$1\\times1$ CNN to compress the output of the hidden layer and optimize the\nnumber of layers and the number of filters for each convolutional layer.\nQuantitative and qualitative evaluations demonstrate that our proposed model is\naccurate, efficient and robust for super-resolution (SR) image restoration from\nnoisy LR input images. In particular, we validate our composite SR networks on\nthree large-scale CT datasets, and obtain promising results as compared to the\nother state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 05:33:23 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 17:56:08 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 20:28:30 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["You", "Chenyu", ""], ["Li", "Guang", ""], ["Zhang", "Yi", ""], ["Zhang", "Xiaoliu", ""], ["Shan", "Hongming", ""], ["Ju", "Shenghong", ""], ["Zhao", "Zhen", ""], ["Zhang", "Zhuiyang", ""], ["Cong", "Wenxiang", ""], ["Vannier", "Michael W.", ""], ["Saha", "Punam K.", ""], ["Wang", "Ge", ""]]}, {"id": "1808.04258", "submitter": "Chao Ma", "authors": "Chao Ma, Jianchun Wang, Weinan E", "title": "Model Reduction with Memory and the Machine Learning of Dynamical\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known Mori-Zwanzig theory tells us that model reduction leads to\nmemory effect. For a long time, modeling the memory effect accurately and\nefficiently has been an important but nearly impossible task in developing a\ngood reduced model. In this work, we explore a natural analogy between\nrecurrent neural networks and the Mori-Zwanzig formalism to establish a\nsystematic approach for developing reduced models with memory. Two training\nmodels-a direct training model and a dynamically coupled training model-are\nproposed and compared. We apply these methods to the Kuramoto-Sivashinsky\nequation and the Navier-Stokes equation. Numerical experiments show that the\nproposed method can produce reduced model with good performance on both\nshort-term prediction and long-term statistical properties.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 07:16:49 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Ma", "Chao", ""], ["Wang", "Jianchun", ""], ["E", "Weinan", ""]]}, {"id": "1808.04260", "submitter": "Maximilian Alber", "authors": "Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam\n  H\\\"agele, Kristof T. Sch\\\"utt, Gr\\'egoire Montavon, Wojciech Samek,\n  Klaus-Robert M\\\"uller, Sven D\\\"ahne, Pieter-Jan Kindermans", "title": "iNNvestigate neural networks!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks have revolutionized many application\ndomains of machine learning and are key components of many critical decision or\npredictive processes. Therefore, it is crucial that domain specialists can\nunderstand and analyze actions and pre- dictions, even of the most complex\nneural network architectures. Despite these arguments neural networks are often\ntreated as black boxes. In the attempt to alleviate this short- coming many\nanalysis methods were proposed, yet the lack of reference implementations often\nmakes a systematic comparison between the methods a major effort. The presented\nlibrary iNNvestigate addresses this by providing a common interface and\nout-of-the- box implementation for many analysis methods, including the\nreference implementation for PatternNet and PatternAttribution as well as for\nLRP-methods. To demonstrate the versatility of iNNvestigate, we provide an\nanalysis of image classifications for variety of state-of-the-art neural\nnetwork architectures.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 14:23:15 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Alber", "Maximilian", ""], ["Lapuschkin", "Sebastian", ""], ["Seegerer", "Philipp", ""], ["H\u00e4gele", "Miriam", ""], ["Sch\u00fctt", "Kristof T.", ""], ["Montavon", "Gr\u00e9goire", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""], ["D\u00e4hne", "Sven", ""], ["Kindermans", "Pieter-Jan", ""]]}, {"id": "1808.04262", "submitter": "Anvar Kurmukov", "authors": "Anvar Kurmukov and Ayagoz Mussabayeva and Yulia Denisova and Daniel\n  Moyer and Boris Gutman", "title": "Connectivity-Driven Brain Parcellation via Consensus Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two related methods for deriving connectivity-based brain atlases\nfrom individual connectomes. The proposed methods exploit a previously proposed\ndense connectivity representation, termed continuous connectivity, by first\nperforming graph-based hierarchical clustering of individual brains, and\nsubsequently aggregating the individual parcellations into a consensus\nparcellation. The search for consensus minimizes the sum of cluster membership\ndistances, effectively estimating a pseudo-Karcher mean of individual\nparcellations. We assess the quality of our parcellations using (1)\nKullback-Liebler and Jensen-Shannon divergence with respect to the dense\nconnectome representation, (2) inter-hemispheric symmetry, and (3) performance\nof the simplified connectome in a biological sex classification task. We find\nthat the parcellation based-atlas computed using a greedy search at a\nhierarchical depth 3 outperforms all other parcellation-based atlases as well\nas the standard Dessikan-Killiany anatomical atlas in all three assessments.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 08:54:31 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Kurmukov", "Anvar", ""], ["Mussabayeva", "Ayagoz", ""], ["Denisova", "Yulia", ""], ["Moyer", "Daniel", ""], ["Gutman", "Boris", ""]]}, {"id": "1808.04281", "submitter": "Giorgio Gnecco", "authors": "Falco J. Bargagli-Stoffi, Giorgio Gnecco", "title": "Estimating Heterogeneous Causal Effects in the Presence of Irregular\n  Assignment Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a link between causal inference and machine learning\ntechniques - specifically, Classification and Regression Trees (CART) - in\nobservational studies where the receipt of the treatment is not randomized, but\nthe assignment to the treatment can be assumed to be randomized (irregular\nassignment mechanism). The paper contributes to the growing applied machine\nlearning literature on causal inference, by proposing a modified version of the\nCausal Tree (CT) algorithm to draw causal inference from an irregular\nassignment mechanism. The proposed method is developed by merging the CT\napproach with the instrumental variable framework to causal inference, hence\nthe name Causal Tree with Instrumental Variable (CT-IV). As compared to CT, the\nmain strength of CT-IV is that it can deal more efficiently with the\nheterogeneity of causal effects, as demonstrated by a series of numerical\nresults obtained on synthetic data. Then, the proposed algorithm is used to\nevaluate a public policy implemented by the Tuscan Regional Administration\n(Italy), which aimed at easing the access to credit for small firms. In this\ncontext, CT-IV breaks fresh ground for target-based policies, identifying\ninteresting heterogeneous causal effects.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:07:55 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 22:23:04 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Bargagli-Stoffi", "Falco J.", ""], ["Gnecco", "Giorgio", ""]]}, {"id": "1808.04287", "submitter": "Paul Jasek", "authors": "Paul Jasek and Bernard Abayowa", "title": "Visual Sensor Network Reconfiguration with Deep Reinforcement Learning", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for reconfiguration of dynamic visual sensor networks\nwith deep reinforcement learning (RL). Our RL agent uses a modified\nasynchronous advantage actor-critic framework and the recently proposed\nRelational Network module at the foundation of its network architecture. To\naddress the issue of sample inefficiency in current approaches to model-free\nreinforcement learning, we train our system in an abstract simulation\nenvironment that represents inputs from a dynamic scene. Our system is\nvalidated using inputs from a real-world scenario and preexisting object\ndetection and tracking algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:24:01 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Jasek", "Paul", ""], ["Abayowa", "Bernard", ""]]}, {"id": "1808.04293", "submitter": "Jiakai Wei", "authors": "Jiakai Wei", "title": "Fast, Better Training Trick -- Random Gradient", "comments": "arXiv admin note: text overlap with arXiv:1708.07120 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will show an unprecedented method to accelerate training\nand improve performance, which called random gradient (RG). This method can be\neasier to the training of any model without extra calculation cost, we use\nImage classification, Semantic segmentation, and GANs to confirm this method\ncan improve speed which is training model in computer vision. The central idea\nis using the loss multiplied by a random number to random reduce the\nback-propagation gradient. We can use this method to produce a better result in\nPascal VOC, Cifar, Cityscapes datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:37:29 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Wei", "Jiakai", ""]]}, {"id": "1808.04295", "submitter": "Zhiqin Xu", "authors": "Zhiqin John Xu", "title": "Understanding training and generalization in deep learning by Fourier\n  analysis", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: It is still an open research area to theoretically understand why\nDeep Neural Networks (DNNs)---equipped with many more parameters than training\ndata and trained by (stochastic) gradient-based methods---often achieve\nremarkably low generalization error. Contribution: We study DNN training by\nFourier analysis. Our theoretical framework explains: i) DNN with (stochastic)\ngradient-based methods often endows low-frequency components of the target\nfunction with a higher priority during the training; ii) Small initialization\nleads to good generalization ability of DNN while preserving the DNN's ability\nto fit any function. These results are further confirmed by experiments of DNNs\nfitting the following datasets, that is, natural images, one-dimensional\nfunctions and MNIST dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:40:41 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 21:01:43 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 01:14:01 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 03:20:18 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Xu", "Zhiqin John", ""]]}, {"id": "1808.04302", "submitter": "Maciej  Skorski", "authors": "Maciej Skorski", "title": "Simple Root Cause Analysis by Separable Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Root Cause Analysis for Anomalies is challenging because of the trade-off\nbetween the accuracy and its explanatory friendliness, required for industrial\napplications. In this paper we propose a framework for simple and friendly RCA\nwithin the Bayesian regime under certain restrictions (that Hessian at the mode\nis diagonal, here referred to as \\emph{separability}) imposed on the predictive\nposterior. We show that this assumption is satisfied for important base models,\nincluding Multinomal, Dirichlet-Multinomial and Naive Bayes. To demonstrate the\nusefulness of the framework, we embed it into the Bayesian Net and validate on\nweb server error logs (real world data set).\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:54:50 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "1808.04308", "submitter": "Sebastian Lapuschkin", "authors": "Fabian Horst, Sebastian Lapuschkin, Wojciech Samek, Klaus-Robert\n  M\\\"uller and Wolfgang I. Sch\\\"ollhorn", "title": "Explaining the Unique Nature of Individual Gait Patterns with Deep\n  Learning", "comments": "17 pages (23 pages including references, 24 pages including\n  references and auxiliary statements, 33 pages including references, auxiliary\n  statements and and supplementary material). 5 figures, 3 tables, 4\n  supplementary figures, 9 supplementary tables. Accepted for publication at\n  Scientific Reports: https://doi.org/10.1038/s41598-019-38748-8", "journal-ref": null, "doi": "10.1038/s41598-019-38748-8", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) techniques such as (deep) artificial neural networks\n(DNN) are solving very successfully a plethora of tasks and provide new\npredictive models for complex physical, chemical, biological and social\nsystems. However, in most cases this comes with the disadvantage of acting as a\nblack box, rarely providing information about what made them arrive at a\nparticular prediction. This black box aspect of ML techniques can be\nproblematic especially in medical diagnoses, so far hampering a clinical\nacceptance. The present paper studies the uniqueness of individual gait\npatterns in clinical biomechanics using DNNs. By attributing portions of the\nmodel predictions back to the input variables (ground reaction forces and\nfull-body joint angles), the Layer-Wise Relevance Propagation (LRP) technique\nreliably demonstrates which variables at what time windows of the gait cycle\nare most relevant for the characterisation of gait patterns from a certain\nindividual. By measuring the time-resolved contribution of each input variable\nto the prediction of ML techniques such as DNNs, our method describes the first\ngeneral framework that enables to understand and interpret non-linear ML\nmethods in (biomechanical) gait analysis and thereby supplies a powerful tool\nfor analysis, diagnosis and treatment of human gait.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 16:04:34 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 13:26:45 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Horst", "Fabian", ""], ["Lapuschkin", "Sebastian", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Sch\u00f6llhorn", "Wolfgang I.", ""]]}, {"id": "1808.04311", "submitter": "Xiaofan Zhang", "authors": "Junsong Wang, Qiuwen Lou, Xiaofan Zhang, Chao Zhu, Yonghua Lin, Deming\n  Chen", "title": "Design Flow of Accelerating Hybrid Extremely Low Bit-width Neural\n  Network in Embedded FPGA", "comments": "Accepted by International Conference on Field-Programmable Logic and\n  Applications (FPL'2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network accelerators with low latency and low energy consumption are\ndesirable for edge computing. To create such accelerators, we propose a design\nflow for accelerating the extremely low bit-width neural network (ELB-NN) in\nembedded FPGAs with hybrid quantization schemes. This flow covers both network\ntraining and FPGA-based network deployment, which facilitates the design space\nexploration and simplifies the tradeoff between network accuracy and\ncomputation efficiency. Using this flow helps hardware designers to deliver a\nnetwork accelerator in edge devices under strict resource and power\nconstraints. We present the proposed flow by supporting hybrid ELB settings\nwithin a neural network. Results show that our design can deliver very high\nperformance peaking at 10.3 TOPS and classify up to 325.3 image/s/watt while\nrunning large-scale neural networks for less than 5W using embedded FPGA. To\nthe best of our knowledge, it is the most energy efficient solution in\ncomparison to GPU or other FPGA implementations reported so far in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 09:24:57 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 18:16:49 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Wang", "Junsong", ""], ["Lou", "Qiuwen", ""], ["Zhang", "Xiaofan", ""], ["Zhu", "Chao", ""], ["Lin", "Yonghua", ""], ["Chen", "Deming", ""]]}, {"id": "1808.04327", "submitter": "Alireza Yazdani", "authors": "Maziar Raissi, Alireza Yazdani, George Em Karniadakis", "title": "Hidden Fluid Mechanics: A Navier-Stokes Informed Deep Learning Framework\n  for Assimilating Flow Visualization Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present hidden fluid mechanics (HFM), a physics informed deep learning\nframework capable of encoding an important class of physical laws governing\nfluid motions, namely the Navier-Stokes equations. In particular, we seek to\nleverage the underlying conservation laws (i.e., for mass, momentum, and\nenergy) to infer hidden quantities of interest such as velocity and pressure\nfields merely from spatio-temporal visualizations of a passive scaler (e.g.,\ndye or smoke), transported in arbitrarily complex domains (e.g., in human\narteries or brain aneurysms). Our approach towards solving the aforementioned\ndata assimilation problem is unique as we design an algorithm that is agnostic\nto the geometry or the initial and boundary conditions. This makes HFM highly\nflexible in choosing the spatio-temporal domain of interest for data\nacquisition as well as subsequent training and predictions. Consequently, the\npredictions made by HFM are among those cases where a pure machine learning\nstrategy or a mere scientific computing approach simply cannot reproduce. The\nproposed algorithm achieves accurate predictions of the pressure and velocity\nfields in both two and three dimensional flows for several benchmark problems\nmotivated by real-world applications. Our results demonstrate that this\nrelatively simple methodology can be used in physical and biomedical problems\nto extract valuable quantitative information (e.g., lift and drag forces or\nwall shear stresses in arteries) for which direct measurements may not be\npossible.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 16:37:56 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Raissi", "Maziar", ""], ["Yazdani", "Alireza", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1808.04334", "submitter": "James O' Neill", "authors": "James O' Neill and Danushka Bollegala", "title": "Angular-Based Word Meta-Embedding Learning", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensembling word embeddings to improve distributed word representations has\nshown good success for natural language processing tasks in recent years. These\napproaches either carry out straightforward mathematical operations over a set\nof vectors or use unsupervised learning to find a lower-dimensional\nrepresentation. This work compares meta-embeddings trained for different\nlosses, namely loss functions that account for angular distance between the\nreconstructed embedding and the target and those that account normalized\ndistances based on the vector length. We argue that meta-embeddings are better\nto treat the ensemble set equally in unsupervised learning as the respective\nquality of each embedding is unknown for upstream tasks prior to\nmeta-embedding. We show that normalization methods that account for this such\nas cosine and KL-divergence objectives outperform meta-embedding trained on\nstandard $\\ell_1$ and $\\ell_2$ loss on \\textit{defacto} word similarity and\nrelatedness datasets and find it outperforms existing meta-learning strategies.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 17:20:20 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Neill", "James O'", ""], ["Bollegala", "Danushka", ""]]}, {"id": "1808.04355", "submitter": "Deepak Pathak", "authors": "Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor\n  Darrell, Alexei A. Efros", "title": "Large-Scale Study of Curiosity-Driven Learning", "comments": "First three authors contributed equally and ordered alphabetically.\n  Website at https://pathak22.github.io/large-scale-curiosity/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms rely on carefully engineering environment\nrewards that are extrinsic to the agent. However, annotating each environment\nwith hand-designed, dense rewards is not scalable, motivating the need for\ndeveloping reward functions that are intrinsic to the agent. Curiosity is a\ntype of intrinsic reward function which uses prediction error as reward signal.\nIn this paper: (a) We perform the first large-scale study of purely\ncuriosity-driven learning, i.e. without any extrinsic rewards, across 54\nstandard benchmark environments, including the Atari game suite. Our results\nshow surprisingly good performance, and a high degree of alignment between the\nintrinsic curiosity objective and the hand-designed extrinsic rewards of many\ngame environments. (b) We investigate the effect of using different feature\nspaces for computing prediction error and show that random features are\nsufficient for many popular RL game benchmarks, but learned features appear to\ngeneralize better (e.g. to novel game levels in Super Mario Bros.). (c) We\ndemonstrate limitations of the prediction-based rewards in stochastic setups.\nGame-play videos and code are at\nhttps://pathak22.github.io/large-scale-curiosity/\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 17:58:01 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Burda", "Yuri", ""], ["Edwards", "Harri", ""], ["Pathak", "Deepak", ""], ["Storkey", "Amos", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1808.04357", "submitter": "Jiarui Fang", "authors": "Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh", "title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "comments": "10 pages. Journal of Parallel and Distributed Computing, 2019", "journal-ref": null, "doi": "10.1016/j.jpdc.2019.05.016", "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallelism has become a dominant method to scale Deep Neural Network\n(DNN) training across multiple nodes. Since synchronizing a large number of\ngradients of the local model can be a bottleneck for large-scale distributed\ntraining, compressing communication data has gained widespread attention\nrecently. Among several recent proposed compression algorithms, Residual\nGradient Compression (RGC) is one of the most successful approaches---it can\nsignificantly compress the transmitting message size (0.1\\% of the gradient\nsize) of each node and still achieve correct accuracy and the same convergence\nspeed. However, the literature on compressing deep networks focuses almost\nexclusively on achieving good theoretical compression rate, while the\nefficiency of RGC in real distributed implementation has been less\ninvestigated. In this paper, we develop an RGC-based system that is able to\nreduce the end-to-end training time on real-world multi-GPU systems. Our\nproposed design called RedSync, which introduces a set of optimizations to\nreduce communication bandwidth requirement while introducing limited overhead.\nWe evaluate the performance of RedSync on two different multiple GPU platforms,\nincluding 128 GPUs of a supercomputer and an 8-GPU server. Our test cases\ninclude image classification tasks on Cifar10 and ImageNet, and language\nmodeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high\ncommunication to computation ratio, which have long been considered with poor\nscalability, RedSync brings significant performance improvements.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 19:02:47 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 03:25:36 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 09:48:26 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fang", "Jiarui", ""], ["Fu", "Haohuan", ""], ["Yang", "Guangwen", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1808.04362", "submitter": "Pascal Sturmfels", "authors": "Pascal Sturmfels, Saige Rutherford, Mike Angstadt, Mark Peterson,\n  Chandra Sripada, Jenna Wiens", "title": "A Domain Guided CNN Architecture for Predicting Age from Structural\n  Brain Images", "comments": "Machine Learning for Healthcare 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the wide success of convolutional neural networks (CNNs) applied to\nnatural images, researchers have begun to apply them to neuroimaging data. To\ndate, however, exploration of novel CNN architectures tailored to neuroimaging\ndata has been limited. Several recent works fail to leverage the 3D structure\nof the brain, instead treating the brain as a set of independent 2D slices.\nApproaches that do utilize 3D convolutions rely on architectures developed for\nobject recognition tasks in natural 2D images. Such architectures make\nassumptions about the input that may not hold for neuroimaging. For example,\nexisting architectures assume that patterns in the brain exhibit translation\ninvariance. However, a pattern in the brain may have different meaning\ndepending on where in the brain it is located. There is a need to explore novel\narchitectures that are tailored to brain images. We present two simple\nmodifications to existing CNN architectures based on brain image structure.\nApplied to the task of brain age prediction, our network achieves a mean\nabsolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline\nthat achieves a MAE of 1.6 years. Our results suggest that lessons learned from\ndeveloping models on natural images may not directly transfer to neuroimaging\ntasks. Instead, there remains a large space of unexplored questions regarding\nmodel development in this area, whose answers may differ from conventional\nwisdom.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 19:43:22 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Sturmfels", "Pascal", ""], ["Rutherford", "Saige", ""], ["Angstadt", "Mike", ""], ["Peterson", "Mark", ""], ["Sripada", "Chandra", ""], ["Wiens", "Jenna", ""]]}, {"id": "1808.04411", "submitter": "Shahnawaz Alam", "authors": "Shahnawaz Alam, Rohan Banerjee, Soma Bandyopadhyay", "title": "Murmur Detection Using Parallel Recurrent & Convolutional Neural\n  Networks", "comments": "4 pages, Machine Learning for Medicine and Healthcare Workshop, KDD\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel technique for classification of the\nMurmurs in heart sound. We introduce a novel deep neural network architecture\nusing parallel combination of the Recurrent Neural Network (RNN) based\nBidirectional Long Short-Term Memory (BiLSTM) & Convolutional Neural Network\n(CNN) to learn visual and time-dependent characteristics of Murmur in PCG\nwaveform. Set of acoustic features are presented to our proposed deep neural\nnetwork to discriminate between Normal and Murmur class. The proposed method\nwas evaluated on a large dataset using 5-fold cross-validation, resulting in a\nsensitivity and specificity of 96 +- 0.6 % , 100 +- 0 % respectively and F1\nScore of 98 +- 0.3 %.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 19:21:41 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Alam", "Shahnawaz", ""], ["Banerjee", "Rohan", ""], ["Bandyopadhyay", "Soma", ""]]}, {"id": "1808.04433", "submitter": "Nizar Ouarti Prof.", "authors": "Nizar Ouarti and David Carmona", "title": "Out of the Black Box: Properties of deep neural networks and their\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are powerful machine learning approaches that have\nexhibited excellent results on many classification tasks. However, they are\nconsidered as black boxes and some of their properties remain to be formalized.\nIn the context of image recognition, it is still an arduous task to understand\nwhy an image is recognized or not. In this study, we formalize some properties\nshared by eight state-of-the-art deep neural networks in order to grasp the\nprinciples allowing a given deep neural network to classify an image. Our\nresults, tested on these eight networks, show that an image can be sub-divided\ninto several regions (patches) responding at different degrees of probability\n(local property). With the same patch, some locations in the image can answer\ntwo (or three) orders of magnitude higher than other locations (spatial\nproperty). Some locations are activators and others inhibitors\n(activation-inhibition property). The repetition of the same patch can increase\n(or decrease) the probability of recognition of an object (cumulative\nproperty). Furthermore, we propose a new approach called Deepception that\nexploits these properties to deceive a deep neural network. We obtain for the\nVGG-VDD-19 neural network a fooling ratio of 88\\%. Thanks to our\n\"Psychophysics\" approach, no prior knowledge on the networks architectures is\nrequired.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 09:30:52 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Ouarti", "Nizar", ""], ["Carmona", "David", ""]]}, {"id": "1808.04439", "submitter": "Ayagoz Mussabayeva", "authors": "Ayagoz Mussabayeva, Alexey Kroshnin, Anvar Kurmukov, Yulia Dodonova,\n  Li Shen, Shan Cong, Lei Wang and Boris A. Gutman", "title": "Image Registration and Predictive Modeling: Learning the Metric on the\n  Space of Diffeomorphisms", "comments": "Accepted to ShapeMI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for metric optimization in the Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) framework, by treating the induced\nRiemannian metric on the space of diffeomorphisms as a kernel in a machine\nlearning context. For simplicity, we choose the kernel Fischer Linear\nDiscriminant Analysis (KLDA) as the framework. Optimizing the kernel parameters\nin an Expectation-Maximization framework, we define model fidelity via the\nhinge loss of the decision function. The resulting algorithm optimizes the\nparameters of the LDDMM norm-inducing differential operator as a solution to a\ngroup-wise registration and classification problem. In practice, this may lead\nto a biology-aware registration, focusing its attention on the predictive task\nat hand such as identifying the effects of disease. We first tested our\nalgorithm on a synthetic dataset, showing that our parameter selection improves\nregistration quality and classification accuracy. We then tested the algorithm\non 3D subcortical shapes from the Schizophrenia cohort Schizconnect. Our\nSchizpohrenia-Control predictive model showed significant improvement in ROC\nAUC compared to baseline parameters.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 15:25:10 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Mussabayeva", "Ayagoz", ""], ["Kroshnin", "Alexey", ""], ["Kurmukov", "Anvar", ""], ["Dodonova", "Yulia", ""], ["Shen", "Li", ""], ["Cong", "Shan", ""], ["Wang", "Lei", ""], ["Gutman", "Boris A.", ""]]}, {"id": "1808.04441", "submitter": "Aaron Pries", "authors": "Aaron Pries, Peter J. Schreier, Artur Lamm, Stefan Pede, J\\\"urgen\n  Schmidt", "title": "Deep Morphing: Detecting bone structures in fluoroscopic X-ray images\n  with prior knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose approaches based on deep learning to localize objects in images\nwhen only a small training dataset is available and the images have low\nquality. That applies to many problems in medical image processing, and in\nparticular to the analysis of fluoroscopic (low-dose) X-ray images, where the\nimages have low contrast. We solve the problem by incorporating high-level\ninformation about the objects, which could be a simple geometrical model, like\na circular outline, or a more complex statistical model. A simple geometrical\nrepresentation can sufficiently describe some objects and only requires minimal\nlabeling. Statistical shape models can be used to represent more complex\nobjects. We propose computationally efficient two-stage approaches, which we\ncall deep morphing, for both representations by fitting the representation to\nthe output of a deep segmentation network.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 17:55:31 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 17:02:27 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pries", "Aaron", ""], ["Schreier", "Peter J.", ""], ["Lamm", "Artur", ""], ["Pede", "Stefan", ""], ["Schmidt", "J\u00fcrgen", ""]]}, {"id": "1808.04444", "submitter": "Rami Al-Rfou", "authors": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones", "title": "Character-Level Language Modeling with Deeper Self-Attention", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTMs and other RNN variants have shown strong performance on character-level\nlanguage modeling. These models are typically trained using truncated\nbackpropagation through time, and it is common to assume that their success\nstems from their ability to remember long-term contexts. In this paper, we show\nthat a deep (64-layer) transformer model with fixed context outperforms RNN\nvariants by a large margin, achieving state of the art on two popular\nbenchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good\nresults at this depth, we show that it is important to add auxiliary losses,\nboth at intermediate network layers and intermediate sequence positions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 18:44:38 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 17:08:57 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Al-Rfou", "Rami", ""], ["Choe", "Dokook", ""], ["Constant", "Noah", ""], ["Guo", "Mandy", ""], ["Jones", "Llion", ""]]}, {"id": "1808.04446", "submitter": "Mathieu Seurin", "authors": "Florian Strub and Mathieu Seurin and Ethan Perez and Harm de Vries and\n  J\\'er\\'emie Mary and Philippe Preux and Aaron Courville and Olivier Pietquin", "title": "Visual Reasoning with Multi-hop Feature Modulation", "comments": "In Proc of ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in computer vision and natural language processing have\nspurred interest in challenging multi-modal tasks such as visual\nquestion-answering and visual dialogue. For such tasks, one successful approach\nis to condition image-based convolutional network computation on language via\nFeature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and\nshifting. We propose to generate the parameters of FiLM layers going up the\nhierarchy of a convolutional network in a multi-hop fashion rather than all at\nonce, as in prior work. By alternating between attending to the language input\nand generating FiLM layer parameters, this approach is better able to scale to\nsettings with longer input sequences such as dialogue. We demonstrate that\nmulti-hop FiLM generation achieves state-of-the-art for the short input\nsequence task ReferIt --- on-par with single-hop FiLM generation --- while also\nsignificantly outperforming prior state-of-the-art and single-hop FiLM\ngeneration on the GuessWhat?! visual dialogue task.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 14:32:02 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 11:36:42 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Strub", "Florian", ""], ["Seurin", "Mathieu", ""], ["Perez", "Ethan", ""], ["de Vries", "Harm", ""], ["Mary", "J\u00e9r\u00e9mie", ""], ["Preux", "Philippe", ""], ["Courville", "Aaron", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1808.04447", "submitter": "Akshay Chaudhari", "authors": "Akshay Chaudhari, Zhongnan Fang, Jin Hyung Lee, Garry Gold, Brian\n  Hargreaves", "title": "Deep Learning Super-Resolution Enables Rapid Simultaneous Morphological\n  and Quantitative Magnetic Resonance Imaging", "comments": "Accepted for the Machine Learning for Medical Image Reconstruction\n  Workshop at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining magnetic resonance images (MRI) with high resolution and generating\nquantitative image-based biomarkers for assessing tissue biochemistry is\ncrucial in clinical and research applications. How- ever, acquiring\nquantitative biomarkers requires high signal-to-noise ratio (SNR), which is at\nodds with high-resolution in MRI, especially in a single rapid sequence. In\nthis paper, we demonstrate how super-resolution can be utilized to maintain\nadequate SNR for accurate quantification of the T2 relaxation time biomarker,\nwhile simultaneously generating high- resolution images. We compare the\nefficacy of resolution enhancement using metrics such as peak SNR and\nstructural similarity. We assess accuracy of cartilage T2 relaxation times by\ncomparing against a standard reference method. Our evaluation suggests that SR\ncan successfully maintain high-resolution and generate accurate biomarkers for\naccelerating MRI scans and enhancing the value of clinical and research MRI.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 05:09:11 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Chaudhari", "Akshay", ""], ["Fang", "Zhongnan", ""], ["Lee", "Jin Hyung", ""], ["Gold", "Garry", ""], ["Hargreaves", "Brian", ""]]}, {"id": "1808.04456", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Khushmeen Sakloth, Charles Siegel, Abhinav Vishnu, Jim\n  Pfaendtner", "title": "Multimodal Deep Neural Networks using Both Engineered and Learned\n  Representations for Biodegradability Prediction", "comments": "Submitted to a peer-reviewed ML conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms excel at extracting patterns from raw data, and with\nlarge datasets, they have been very successful in computer vision and natural\nlanguage applications. However, in other domains, large datasets on which to\nlearn representations from may not exist. In this work, we develop a novel\nmultimodal CNN-MLP neural network architecture that utilizes both\ndomain-specific feature engineering as well as learned representations from raw\ndata. We illustrate the effectiveness of such network designs in the chemical\nsciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP\nnetwork is more accurate than either standalone network designs, and achieves\nan error classification rate of 0.125 that is 27% lower than the current\nstate-of-the-art. Thus, our work indicates that combining traditional feature\nengineering with representation learning can be effective, particularly in\nsituations where labeled data is limited.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 20:36:08 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 18:27:08 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Goh", "Garrett B.", ""], ["Sakloth", "Khushmeen", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""], ["Pfaendtner", "Jim", ""]]}, {"id": "1808.04468", "submitter": "Yinlam Chow", "authors": "Jonathan Lacotte and Mohammad Ghavamzadeh and Yinlam Chow and Marco\n  Pavone", "title": "Risk-Sensitive Generative Adversarial Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study risk-sensitive imitation learning where the agent's goal is to\nperform at least as well as the expert in terms of a risk profile. We first\nformulate our risk-sensitive imitation learning setting. We consider the\ngenerative adversarial approach to imitation learning (GAIL) and derive an\noptimization problem for our formulation, which we call it risk-sensitive GAIL\n(RS-GAIL). We then derive two different versions of our RS-GAIL optimization\nproblem that aim at matching the risk profiles of the agent and the expert\nw.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop\nrisk-sensitive generative adversarial imitation learning algorithms based on\nthese optimization problems. We evaluate the performance of our algorithms and\ncompare them with GAIL and the risk-averse imitation learning (RAIL) algorithms\nin two MuJoCo and two OpenAI classical control tasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 21:08:46 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 02:41:29 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Lacotte", "Jonathan", ""], ["Ghavamzadeh", "Mohammad", ""], ["Chow", "Yinlam", ""], ["Pavone", "Marco", ""]]}, {"id": "1808.04475", "submitter": "Houman Owhadi", "authors": "Houman Owhadi and Gene Ryan Yoo", "title": "Kernel Flows: from learning kernels from data into the abyss", "comments": "42 pages, 31 figures. See\n  https://www.youtube.com/watch?v=h9wB8FVH7YM&list=PLdWd7x7FVuLphAODzEvj2KRNws7z7Sv87\n  for animations of the flows. See\n  http://users.cms.caltech.edu/~owhadi/index_htm_files/kf-static.pdf for\n  slides. See http://users.cms.caltech.edu/~owhadi/KF/ for high resolution\n  videos of the flows in the setting of the Swiss roll cheesecake\n  classification problem", "journal-ref": null, "doi": "10.1016/j.jcp.2019.03.040", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning can be seen as approximating an unknown function by interpolating\nthe training data. Kriging offers a solution to this problem based on the prior\nspecification of a kernel. We explore a numerical approximation approach to\nkernel selection/construction based on the simple premise that a kernel must be\ngood if the number of interpolation points can be halved without significant\nloss in accuracy (measured using the intrinsic RKHS norm $\\|\\cdot\\|$ associated\nwith the kernel). We first test and motivate this idea on a simple problem of\nrecovering the Green's function of an elliptic PDE (with inhomogeneous\ncoefficients) from the sparse observation of one of its solutions. Next we\nconsider the problem of learning non-parametric families of deep kernels of the\nform $K_1(F_n(x),F_n(x'))$ with $F_{n+1}=(I_d+\\epsilon G_{n+1})\\circ F_n$ and\n$G_{n+1} \\in \\operatorname{Span}\\{K_1(F_n(x_i),\\cdot)\\}$. With the proposed\napproach constructing the kernel becomes equivalent to integrating a stochastic\ndata driven dynamical system, which allows for the training of very deep\n(bottomless) networks and the exploration of their properties. These networks\nlearn by constructing flow maps in the kernel and input spaces via incremental\ndata-dependent deformations/perturbations (appearing as the cooperative\ncounterpart of adversarial examples) and, at profound depths, they (1) can\nachieve accurate classification from only one data point per class (2) appear\nto learn archetypes of each class (3) expand distances between points that are\nin different classes and contract distances between points in the same class.\nFor kernels parameterized by the weights of Convolutional Neural Networks,\nminimizing approximation errors incurred by halving random subsets of\ninterpolation points, appears to outperform training (the same CNN\narchitecture) with relative entropy and dropout.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 21:43:20 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 06:19:09 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Owhadi", "Houman", ""], ["Yoo", "Gene Ryan", ""]]}, {"id": "1808.04521", "submitter": "Zhanxuan Hu", "authors": "Zhanxuan Hu, Feiping Nie, Rong Wang, Xuelong Li", "title": "Low Rank Regularization: A Review", "comments": "16 pages,4 figures,4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank regularization, in essence, involves introducing a low rank or\napproximately low rank assumption for matrix we aim to learn, which has\nachieved great success in many fields including machine learning, data mining\nand computer version. Over the last decade, much progress has been made in\ntheories and practical applications. Nevertheless, the intersection between\nthem is very slight. In order to construct a bridge between practical\napplications and theoretical research, in this paper we provide a comprehensive\nsurvey for low rank regularization. We first review several traditional machine\nlearning models using low rank regularization, and then show their (or their\nvariants) applications in solving practical issues, such as non-rigid structure\nfrom motion and image denoising. Subsequently, we summarize the regularizers\nand optimization methods that achieve great success in traditional machine\nlearning tasks but are rarely seen in solving practical issues. Finally, we\nprovide a discussion and comparison for some representative regularizers\nincluding convex and non-convex relaxations. Extensive experimental results\ndemonstrate that non-convex regularizers can provide a large advantage over the\nnuclear norm, the regularizer widely used in solving practical issues.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 04:38:58 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 14:45:07 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 03:05:30 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hu", "Zhanxuan", ""], ["Nie", "Feiping", ""], ["Wang", "Rong", ""], ["Li", "Xuelong", ""]]}, {"id": "1808.04523", "submitter": "Max Simchowitz", "authors": "Max Simchowitz and Kevin Jamieson and Jordan W. Suchow and Thomas L.\n  Griffiths", "title": "Adaptive Sampling for Convex Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the first principled adaptive-sampling procedure\nfor learning a convex function in the $L_\\infty$ norm, a problem that arises\noften in the behavioral and social sciences. We present a function-specific\nmeasure of complexity and use it to prove that, for each convex function\n$f_{\\star}$, our algorithm nearly attains the information-theoretically\noptimal, function-specific error rate. We also corroborate our theoretical\ncontributions with numerical experiments, finding that our method substantially\noutperforms passive, uniform sampling for favorable synthetic and data-derived\nfunctions in low-noise settings with large sampling budgets. Our results also\nsuggest an idealized \"oracle strategy\", which we use to gauge the potential\nadvance of any adaptive-sampling strategy over passive sampling, for any given\nconvex function.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 05:01:55 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 06:05:40 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2018 22:45:16 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Simchowitz", "Max", ""], ["Jamieson", "Kevin", ""], ["Suchow", "Jordan W.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1808.04538", "submitter": "Satya Krishna Gorti", "authors": "Satya Krishna Gorti and Jeremy Ma", "title": "Text-to-Image-to-Text Translation using Cycle Consistent Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-to-Image translation has been an active area of research in the recent\npast. The ability for a network to learn the meaning of a sentence and generate\nan accurate image that depicts the sentence shows ability of the model to think\nmore like humans. Popular methods on text to image translation make use of\nGenerative Adversarial Networks (GANs) to generate high quality images based on\ntext input, but the generated images don't always reflect the meaning of the\nsentence given to the model as input. We address this issue by using a\ncaptioning network to caption on generated images and exploit the distance\nbetween ground truth captions and generated captions to improve the network\nfurther. We show extensive comparisons between our method and existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 05:45:25 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Gorti", "Satya Krishna", ""], ["Ma", "Jeremy", ""]]}, {"id": "1808.04545", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan Sunkavalli, Eli\n  Shechtman, Sunil Hadap, Ersin Yumer, Honglak Lee", "title": "MT-VAE: Learning Motion Transformations to Generate Multimodal Human\n  Dynamics", "comments": "Published at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term human motion can be represented as a series of motion\nmodes---motion sequences that capture short-term temporal dynamics---with\ntransitions between them. We leverage this structure and present a novel Motion\nTransformation Variational Auto-Encoders (MT-VAE) for learning motion sequence\ngeneration. Our model jointly learns a feature embedding for motion modes (that\nthe motion sequence can be reconstructed from) and a feature transformation\nthat represents the transition of one motion mode to the next motion mode. Our\nmodel is able to generate multiple diverse and plausible motion sequences in\nthe future from the same input. We apply our approach to both facial and full\nbody motion, and demonstrate applications like analogy-based motion transfer\nand video synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 06:21:03 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Yan", "Xinchen", ""], ["Rastogi", "Akash", ""], ["Villegas", "Ruben", ""], ["Sunkavalli", "Kalyan", ""], ["Shechtman", "Eli", ""], ["Hadap", "Sunil", ""], ["Yumer", "Ersin", ""], ["Lee", "Honglak", ""]]}, {"id": "1808.04550", "submitter": "Clara Stegehuis", "authors": "Anatoliy Babic, Harshit Bansal, Gianluca Finocchio, Julian Golak, Mark\n  Peletier, Jim Portegies, Clara Stegehuis, Anuj Tyagi, Roland Vincze, William\n  Weimin Yoo", "title": "SciSports: Learning football kinematics through two-dimensional tracking\n  data", "comments": "This report was made for the Study Group Mathematics with Industry\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SciSports is a Dutch startup company specializing in football analytics. This\npaper describes a joint research effort with SciSports, during the Study Group\nMathematics with Industry 2018 at Eindhoven, the Netherlands. The main\nchallenge that we addressed was to automatically process empirical football\nplayers' trajectories, in order to extract useful information from them. The\ndata provided to us was two-dimensional positional data during entire matches.\nWe developed methods based on Newtonian mechanics and the Kalman filter,\nGenerative Adversarial Nets and Variational Autoencoders. In addition, we\ntrained a discriminator network to recognize and discern different movement\npatterns of players. The Kalman-filter approach yields an interpretable model,\nin which a small number of player-dependent parameters can be fit; in theory\nthis could be used to distinguish among players. The\nGenerative-Adversarial-Nets approach appears promising in theory, and some\ninitial tests showed an improvement with respect to the baseline, but the\nlimits in time and computational power meant that we could not fully explore\nit. We also trained a Discriminator network to distinguish between two players\nbased on their trajectories; after training, the network managed to distinguish\nbetween some pairs of players, but not between others. After training, the\nVariational Autoencoders generated trajectories that are difficult to\ndistinguish, visually, from the data. These experiments provide an indication\nthat deep generative models can learn the underlying structure and statistics\nof football players' trajectories. This can serve as a starting point for\ndetermining player qualities based on such trajectory data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 06:40:01 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Babic", "Anatoliy", ""], ["Bansal", "Harshit", ""], ["Finocchio", "Gianluca", ""], ["Golak", "Julian", ""], ["Peletier", "Mark", ""], ["Portegies", "Jim", ""], ["Stegehuis", "Clara", ""], ["Tyagi", "Anuj", ""], ["Vincze", "Roland", ""], ["Yoo", "William Weimin", ""]]}, {"id": "1808.04572", "submitter": "Jun Shu", "authors": "Jun Shu, Zongben Xu and Deyu Meng", "title": "Small Sample Learning in Big Data Era", "comments": "76 pages, 15 figures, survey of small sample learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a promising area in artificial intelligence, a new learning paradigm,\ncalled Small Sample Learning (SSL), has been attracting prominent research\nattention in the recent years. In this paper, we aim to present a survey to\ncomprehensively introduce the current techniques proposed on this topic.\nSpecifically, current SSL techniques can be mainly divided into two categories.\nThe first category of SSL approaches can be called \"concept learning\", which\nemphasizes learning new concepts from only few related observations. The\npurpose is mainly to simulate human learning behaviors like recognition,\ngeneration, imagination, synthesis and analysis. The second category is called\n\"experience learning\", which usually co-exists with the large sample learning\nmanner of conventional machine learning. This category mainly focuses on\nlearning with insufficient samples, and can also be called small data learning\nin some literatures. More extensive surveys on both categories of SSL\ntechniques are introduced and some neuroscience evidences are provided to\nclarify the rationality of the entire SSL regime, and the relationship with\nhuman learning process. Some discussions on the main challenges and possible\nfuture research directions along this line are also presented.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 08:01:07 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 04:36:14 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2018 14:48:43 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Shu", "Jun", ""], ["Xu", "Zongben", ""], ["Meng", "Deyu", ""]]}, {"id": "1808.04580", "submitter": "Toni Volkmer", "authors": "Dominik Alfke, Daniel Potts, Martin Stoll, Toni Volkmer", "title": "NFFT meets Krylov methods: Fast matrix-vector products for the graph\n  Laplacian of fully connected networks", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": "10.3389/fams.2018.00061", "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph Laplacian is a standard tool in data science, machine learning, and\nimage processing. The corresponding matrix inherits the complex structure of\nthe underlying network and is in certain applications densely populated. This\nmakes computations, in particular matrix-vector products, with the graph\nLaplacian a hard task. A typical application is the computation of a number of\nits eigenvalues and eigenvectors. Standard methods become infeasible as the\nnumber of nodes in the graph is too large. We propose the use of the fast\nsummation based on the nonequispaced fast Fourier transform (NFFT) to perform\nthe dense matrix-vector product with the graph Laplacian fast without ever\nforming the whole matrix. The enormous flexibility of the NFFT algorithm allows\nus to embed the accelerated multiplication into Lanczos-based eigenvalues\nroutines or iterative linear system solvers and even consider other than the\nstandard Gaussian kernels. We illustrate the feasibility of our approach on a\nnumber of test problems from image segmentation to semi-supervised learning\nbased on graph-based PDEs. In particular, we compare our approach with the\nNystr\\\"om method. Moreover, we present and test an enhanced, hybrid version of\nthe Nystr\\\"om method, which internally uses the NFFT.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 08:24:01 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 12:07:04 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Alfke", "Dominik", ""], ["Potts", "Daniel", ""], ["Stoll", "Martin", ""], ["Volkmer", "Toni", ""]]}, {"id": "1808.04670", "submitter": "Ariel Ekgren", "authors": "Ariel Ekgren, Amaru Cuba Gyllensten, Magnus Sahlgren", "title": "R-grams: Unsupervised Learning of Semantic Units in Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates data-driven segmentation using Re-Pair or Byte Pair\nEncoding-techniques. In contrast to previous work which has primarily been\nfocused on subword units for machine translation, we are interested in the\ngeneral properties of such segments above the word level. We call these\nsegments r-grams, and discuss their properties and the effect they have on the\ntoken frequency distribution. The proposed approach is evaluated by\ndemonstrating its viability in embedding techniques, both in monolingual and\nmultilingual test settings. We also provide a number of qualitative examples of\nthe proposed methodology, demonstrating its viability as a language-invariant\nsegmentation procedure.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 13:15:43 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 13:59:03 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ekgren", "Ariel", ""], ["Gyllensten", "Amaru Cuba", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "1808.04685", "submitter": "Gang Wang", "authors": "Gang Wang and Georgios B. Giannakis and Jie Chen", "title": "Learning ReLU Networks on Linearly Separable Data: Algorithm,\n  Optimality, and Generalization", "comments": "23 pages, 7 figures, work in progress", "journal-ref": null, "doi": "10.1109/TSP.2019.2904921", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks with REctified Linear Unit (ReLU) activation functions\n(a.k.a. ReLU networks) have achieved great empirical success in various\ndomains. Nonetheless, existing results for learning ReLU networks either pose\nassumptions on the underlying data distribution being e.g. Gaussian, or require\nthe network size and/or training size to be sufficiently large. In this\ncontext, the problem of learning a two-layer ReLU network is approached in a\nbinary classification setting, where the data are linearly separable and a\nhinge loss criterion is adopted. Leveraging the power of random noise\nperturbation, this paper presents a novel stochastic gradient descent (SGD)\nalgorithm, which can \\emph{provably} train any single-hidden-layer ReLU network\nto attain global optimality, despite the presence of infinitely many bad local\nminima, maxima, and saddle points in general. This result is the first of its\nkind, requiring no assumptions on the data distribution, training/network size,\nor initialization. Convergence of the resultant iterative algorithm to a global\nminimum is analyzed by establishing both an upper bound and a lower bound on\nthe number of non-zero updates to be performed. Moreover, generalization\nguarantees are developed for ReLU networks trained with the novel SGD\nleveraging classic compression bounds. These guarantees highlight a key\ndifference (at least in the worst case) between reliably learning a ReLU\nnetwork as well as a leaky ReLU network in terms of sample complexity.\nNumerical tests using both synthetic data and real images validate the\neffectiveness of the algorithm and the practical merits of the theory.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 13:45:34 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 19:20:42 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Wang", "Gang", ""], ["Giannakis", "Georgios B.", ""], ["Chen", "Jie", ""]]}, {"id": "1808.04699", "submitter": "Zhirong Wu", "authors": "Zhirong Wu, Alexei A. Efros, Stella X. Yu", "title": "Improving Generalization via Scalable Neighborhood Component Analysis", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current major approaches to visual recognition follow an end-to-end\nformulation that classifies an input image into one of the pre-determined set\nof semantic categories. Parametric softmax classifiers are a common choice for\nsuch a closed world with fixed categories, especially when big labeled data is\navailable during training. However, this becomes problematic for open-set\nscenarios where new categories are encountered with very few examples for\nlearning a generalizable parametric classifier. We adopt a non-parametric\napproach for visual recognition by optimizing feature embeddings instead of\nparametric classifiers. We use a deep neural network to learn the visual\nfeature that preserves the neighborhood structure in the semantic space, based\non the Neighborhood Component Analysis (NCA) criterion. Limited by its\ncomputational bottlenecks, we devise a mechanism to use augmented memory to\nscale NCA for large datasets and very deep networks. Our experiments deliver\nnot only remarkable performance on ImageNet classification for such a simple\nnon-parametric method, but most importantly a more generalizable feature\nrepresentation for sub-category discovery and few-shot recognition.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 14:03:47 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Wu", "Zhirong", ""], ["Efros", "Alexei A.", ""], ["Yu", "Stella X.", ""]]}, {"id": "1808.04728", "submitter": "Deborah Bard", "authors": "Amrita Mathuriya, Deborah Bard, Peter Mendygral, Lawrence Meadows,\n  James Arnemann, Lei Shao, Siyu He, Tuomas Karna, Daina Moise, Simon J.\n  Pennycook, Kristyn Maschoff, Jason Sewall, Nalini Kumar, Shirley Ho, Mike\n  Ringenburg, Prabhat and Victor Lee", "title": "CosmoFlow: Using Deep Learning to Learn the Universe at Scale", "comments": "11 pages, 6 pages, presented at SuperComputing 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO astro-ph.IM cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a promising tool to determine the physical model that\ndescribes our universe. To handle the considerable computational cost of this\nproblem, we present CosmoFlow: a highly scalable deep learning application\nbuilt on top of the TensorFlow framework. CosmoFlow uses efficient\nimplementations of 3D convolution and pooling primitives, together with\nimprovements in threading for many element-wise operations, to improve training\nperformance on Intel(C) Xeon Phi(TM) processors. We also utilize the Cray PE\nMachine Learning Plugin for efficient scaling to multiple nodes. We demonstrate\nfully synchronous data-parallel training on 8192 nodes of Cori with 77%\nparallel efficiency, achieving 3.5 Pflop/s sustained performance. To our\nknowledge, this is the first large-scale science application of the TensorFlow\nframework at supercomputer scale with fully-synchronous training. These\nenhancements enable us to process large 3D dark matter distribution and predict\nthe cosmological parameters $\\Omega_M$, $\\sigma_8$ and n$_s$ with unprecedented\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 14:54:37 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 20:15:11 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Mathuriya", "Amrita", ""], ["Bard", "Deborah", ""], ["Mendygral", "Peter", ""], ["Meadows", "Lawrence", ""], ["Arnemann", "James", ""], ["Shao", "Lei", ""], ["He", "Siyu", ""], ["Karna", "Tuomas", ""], ["Moise", "Daina", ""], ["Pennycook", "Simon J.", ""], ["Maschoff", "Kristyn", ""], ["Sewall", "Jason", ""], ["Kumar", "Nalini", ""], ["Ho", "Shirley", ""], ["Ringenburg", "Mike", ""], ["Prabhat", "", ""], ["Lee", "Victor", ""]]}, {"id": "1808.04730", "submitter": "Lynton Ardizzone", "authors": "Lynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric\n  W. Pellegrini, Ralf S. Klessen, Lena Maier-Hein, Carsten Rother, Ullrich\n  K\\\"othe", "title": "Analyzing Inverse Problems with Invertible Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many tasks, in particular in natural science, the goal is to determine\nhidden system parameters from a set of measurements. Often, the forward process\nfrom parameter- to measurement-space is a well-defined function, whereas the\ninverse problem is ambiguous: one measurement may map to multiple different\nsets of parameters. In this setting, the posterior parameter distribution,\nconditioned on an input measurement, has to be determined. We argue that a\nparticular class of neural networks is well suited for this task -- so-called\nInvertible Neural Networks (INNs). Although INNs are not new, they have, so\nfar, received little attention in literature. While classical neural networks\nattempt to solve the ambiguous inverse problem directly, INNs are able to learn\nit jointly with the well-defined forward process, using additional latent\noutput variables to capture the information otherwise lost. Given a specific\nmeasurement and sampled latent variables, the inverse pass of the INN provides\na full distribution over parameter space. We verify experimentally, on\nartificial data and real-world problems from astrophysics and medicine, that\nINNs are a powerful analysis tool to find multi-modalities in parameter space,\nto uncover parameter correlations, and to identify unrecoverable parameters.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 14:58:59 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 13:06:05 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 15:45:02 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Ardizzone", "Lynton", ""], ["Kruse", "Jakob", ""], ["Wirkert", "Sebastian", ""], ["Rahner", "Daniel", ""], ["Pellegrini", "Eric W.", ""], ["Klessen", "Ralf S.", ""], ["Maier-Hein", "Lena", ""], ["Rother", "Carsten", ""], ["K\u00f6the", "Ullrich", ""]]}, {"id": "1808.04750", "submitter": "Jens Schreiber", "authors": "Jens Schreiber and Bernhard Sick", "title": "Quantifying the Influences on Probabilistic Wind Power Forecasts", "comments": "5 pages; 1 table; 3 figures; This work has been submitted to the IEEE\n  for possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, probabilistic forecasts techniques were proposed in research\nas well as in applications to integrate volatile renewable energy resources\ninto the electrical grid. These techniques allow decision makers to take the\nuncertainty of the prediction into account and, therefore, to devise optimal\ndecisions, e.g., related to costs and risks in the electrical grid. However, it\nwas yet not studied how the input, such as numerical weather predictions,\naffects the model output of forecasting models in detail. Therefore, we examine\nthe potential influences with techniques from the field of sensitivity analysis\non three different black-box models to obtain insights into differences and\nsimilarities of these probabilistic models. The analysis shows a considerable\nnumber of potential influences in those models depending on, e.g., the\npredicted probability and the type of model. These effects motivate the need to\ntake various influences into account when models are tested, analyzed, or\ncompared. Nevertheless, results of the sensitivity analysis will allow us to\nselect a model with advantages in the practical application.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:27:22 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Schreiber", "Jens", ""], ["Sick", "Bernhard", ""]]}, {"id": "1808.04752", "submitter": "Yunhui Guo", "authors": "Yunhui Guo", "title": "A Survey on Methods and Theories of Quantized Neural Networks", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are the state-of-the-art methods for many real-world\ntasks, such as computer vision, natural language processing and speech\nrecognition. For all its popularity, deep neural networks are also criticized\nfor consuming a lot of memory and draining battery life of devices during\ntraining and inference. This makes it hard to deploy these models on mobile or\nembedded devices which have tight resource constraints. Quantization is\nrecognized as one of the most effective approaches to satisfy the extreme\nmemory requirements that deep neural network models demand. Instead of adopting\n32-bit floating point format to represent weights, quantized representations\nstore weights using more compact formats such as integers or even binary\nnumbers. Despite a possible degradation in predictive performance, quantization\nprovides a potential solution to greatly reduce the model size and the energy\nconsumption. In this survey, we give a thorough review of different aspects of\nquantized neural networks. Current challenges and trends of quantized neural\nnetworks are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 14:11:43 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 08:26:57 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Guo", "Yunhui", ""]]}, {"id": "1808.04759", "submitter": "Holger Trittenbach", "authors": "Holger Trittenbach, Adrian Englhardt, Klemens B\\\"ohm", "title": "An Overview and a Benchmark of Active Learning for Outlier Detection\n  with One-Class Classifiers", "comments": "Change history: update to more specific title; restructure of\n  experimental section: added additional data sets and heuristic to select\n  kernel parameter; add guidelines and decision rules (Section 4.4). Further\n  minor changes: additional references; discussion of split strategies now is\n  in Section 3.4; fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning methods increase classification quality by means of user\nfeedback. An important subcategory is active learning for outlier detection\nwith one-class classifiers. While various methods in this category exist,\nselecting one for a given application scenario is difficult. This is because\nexisting methods rely on different assumptions, have different objectives, and\noften are tailored to a specific use case. All this calls for a comprehensive\ncomparison, the topic of this article. This article starts with a\ncategorization of the various methods. We then propose ways to evaluate active\nlearning results. Next, we run extensive experiments to compare existing\nmethods, for a broad variety of scenarios. Based on our results, we formulate\nguidelines on how to select active learning methods for outlier detection with\none-class classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:45:48 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 16:34:15 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Trittenbach", "Holger", ""], ["Englhardt", "Adrian", ""], ["B\u00f6hm", "Klemens", ""]]}, {"id": "1808.04760", "submitter": "Yuri G. Gordienko", "authors": "Sergii Stirenko, Gang Peng, Wei Zeng, Yuri Gordienko, Oleg Alienin,\n  Oleksandr Rokovyi, Nikita Gordienko", "title": "Parallel Statistical and Machine Learning Methods for Estimation of\n  Physical Load", "comments": "15 pages, 8 figures, accepted for 18th International Conference on\n  Algorithms and Architectures for Parallel Processing (ICA3PP) 15-17 November,\n  2018 (Guangzhou, China)", "journal-ref": "In: Vaidya J., Li J. (eds) Algorithms and Architectures for\n  Parallel Processing. ICA3PP 2018. Lecture Notes in Computer Science, vol\n  11334, 483-497. Springer, Cham", "doi": "10.1007/978-3-030-05051-1_33", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several statistical and machine learning methods are proposed to estimate the\ntype and intensity of physical load and accumulated fatigue . They are based on\nthe statistical analysis of accumulated and moving window data subsets with\nconstruction of a kurtosis-skewness diagram. This approach was applied to the\ndata gathered by the wearable heart monitor for various types and levels of\nphysical activities, and for people with various physical conditions. The\ndifferent levels of physical activities, loads, and fitness can be\ndistinguished from the kurtosis-skewness diagram, and their evolution can be\nmonitored. Several metrics for estimation of the instant effect and accumulated\neffect (physical fatigue) of physical loads were proposed. The data and results\npresented allow to extend application of these methods for modeling and\ncharacterization of complex human activity patterns, for example, to estimate\nthe actual and accumulated physical load and fatigue, model the potential\ndangerous development, and give cautions and advice in real time.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:47:32 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Stirenko", "Sergii", ""], ["Peng", "Gang", ""], ["Zeng", "Wei", ""], ["Gordienko", "Yuri", ""], ["Alienin", "Oleg", ""], ["Rokovyi", "Oleksandr", ""], ["Gordienko", "Nikita", ""]]}, {"id": "1808.04761", "submitter": "Mengjia Yan", "authors": "Mengjia Yan, Christopher Fletcher, Josep Torrellas", "title": "Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are fast becoming ubiquitous for their ability to\nattain good accuracy in various machine learning tasks. A DNN's architecture\n(i.e., its hyper-parameters) broadly determines the DNN's accuracy and\nperformance, and is often confidential. Attacking a DNN in the cloud to obtain\nits architecture can potentially provide major commercial value. Further,\nattaining a DNN's architecture facilitates other, existing DNN attacks.\n  This paper presents Cache Telepathy: a fast and accurate mechanism to steal a\nDNN's architecture using the cache side channel. Our attack is based on the\ninsight that DNN inference relies heavily on tiled GEMM (Generalized Matrix\nMultiply), and that DNN architecture parameters determine the number of GEMM\ncalls and the dimensions of the matrices used in the GEMM functions. Such\ninformation can be leaked through the cache side channel.\n  This paper uses Prime+Probe and Flush+Reload to attack VGG and ResNet DNNs\nrunning OpenBLAS and Intel MKL libraries. Our attack is effective in helping\nobtain the architectures by very substantially reducing the search space of\ntarget DNN architectures. For example, for VGG using OpenBLAS, it reduces the\nsearch space from more than $10^{35}$ architectures to just 16.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:50:15 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Yan", "Mengjia", ""], ["Fletcher", "Christopher", ""], ["Torrellas", "Josep", ""]]}, {"id": "1808.04768", "submitter": "Alexander Neitz", "authors": "Alexander Neitz, Giambattista Parascandolo, Stefan Bauer, Bernhard\n  Sch\\\"olkopf", "title": "Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method which enables a recurrent dynamics model to be\ntemporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI),\nis based on the observation that in many sequential prediction tasks, the exact\ntime at which events occur is irrelevant to the underlying objective. Moreover,\nin many situations, there exist prediction intervals which result in\nparticularly easy-to-predict transitions. We show that there are prediction\ntasks for which we gain both computational efficiency and prediction accuracy\nby allowing the model to make predictions at a sampling rate which it can\nchoose itself.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 16:07:41 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 15:39:35 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 02:46:50 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Neitz", "Alexander", ""], ["Parascandolo", "Giambattista", ""], ["Bauer", "Stefan", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1808.04794", "submitter": "Maciej Swiechowski", "authors": "Maciej \\'Swiechowski, Tomasz Tajmajer and Andrzej Janusz", "title": "Improving Hearthstone AI by Combining MCTS and Supervised Learning\n  Algorithms", "comments": "Proceedings of the 2018 IEEE Conference on Computational Intelligence\n  and Games (CIG'18); pages 445-452; ISBN: 978-1-5386-4358-7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the impact of supervised prediction models on the strength and\nefficiency of artificial agents that use the Monte-Carlo Tree Search (MCTS)\nalgorithm to play a popular video game Hearthstone: Heroes of Warcraft. We\noverview our custom implementation of the MCTS that is well-suited for games\nwith partially hidden information and random effects. We also describe\nexperiments which we designed to quantify the performance of our Hearthstone\nagent's decision making. We show that even simple neural networks can be\ntrained and successfully used for the evaluation of game states. Moreover, we\ndemonstrate that by providing a guidance to the game state search heuristic, it\nis possible to substantially improve the win rate, and at the same time reduce\nthe required computations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 16:58:11 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["\u015awiechowski", "Maciej", ""], ["Tajmajer", "Tomasz", ""], ["Janusz", "Andrzej", ""]]}, {"id": "1808.04803", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "Hierarchical binary CNNs for landmark localization with limited\n  resources", "comments": "Accepted to IEEE TPAMI18: Best of ICCV 2017 SI. Previously portions\n  of this work appeared as arXiv:1703.00862, which was the conference version", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2866051", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to design architectures that retain the groundbreaking\nperformance of Convolutional Neural Networks (CNNs) for landmark localization\nand at the same time are lightweight, compact and suitable for applications\nwith limited computational resources. To this end, we make the following\ncontributions: (a) we are the first to study the effect of neural network\nbinarization on localization tasks, namely human pose estimation and face\nalignment. We exhaustively evaluate various design choices, identify\nperformance bottlenecks, and more importantly propose multiple orthogonal ways\nto boost performance. (b) Based on our analysis, we propose a novel\nhierarchical, parallel and multi-scale residual architecture that yields large\nperformance improvement over the standard bottleneck block while having the\nsame number of parameters, thus bridging the gap between the original network\nand its binarized counterpart. (c) We perform a large number of ablation\nstudies that shed light on the properties and the performance of the proposed\nblock. (d) We present results for experiments on the most challenging datasets\nfor human pose estimation and face alignment, reporting in many cases\nstate-of-the-art performance. (e) We further provide additional results for the\nproblem of facial part segmentation. Code can be downloaded from\nhttps://www.adrianbulat.com/binary-cnn-landmark\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 17:32:29 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1808.04819", "submitter": "Kevin Hu", "authors": "Kevin Z. Hu, Michiel A. Bakker, Stephen Li, Tim Kraska, C\\'esar A.\n  Hidalgo", "title": "VizML: A Machine Learning Approach to Visualization Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization should be accessible for all analysts with data, not just\nthe few with technical expertise. Visualization recommender systems aim to\nlower the barrier to exploring basic visualizations by automatically generating\nresults for analysts to search and select, rather than manually specify. Here,\nwe demonstrate a novel machine learning-based approach to visualization\nrecommendation that learns visualization design choices from a large corpus of\ndatasets and associated visualizations. First, we identify five key design\nchoices made by analysts while creating visualizations, such as selecting a\nvisualization type and choosing to encode a column along the X- or Y-axis. We\ntrain models to predict these design choices using one million\ndataset-visualization pairs collected from a popular online visualization\nplatform. Neural networks predict these design choices with high accuracy\ncompared to baseline models. We report and interpret feature importances from\none of these baseline models. To evaluate the generalizability and uncertainty\nof our approach, we benchmark with a crowdsourced test set, and show that the\nperformance of our model is comparable to human performance when predicting\nconsensus visualization type, and exceeds that of other ML-based systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 18:00:01 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Hu", "Kevin Z.", ""], ["Bakker", "Michiel A.", ""], ["Li", "Stephen", ""], ["Kraska", "Tim", ""], ["Hidalgo", "C\u00e9sar A.", ""]]}, {"id": "1808.04839", "submitter": "Y Cooper", "authors": "Y. Cooper", "title": "Gradient descent in some simple settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we observe the behavior of gradient flow and discrete and noisy\ngradient descent in some simple settings. It is commonly noted that addition of\nnoise to gradient descent can affect the trajectory of gradient descent. Here,\nwe run some computer experiments for gradient descent on some simple functions,\nand observe this principle in some concrete examples.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 18:06:58 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 14:14:34 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Cooper", "Y.", ""]]}, {"id": "1808.04866", "submitter": "Clement Fung", "authors": "Clement Fung, Chris J.M. Yoon, Ivan Beschastnikh", "title": "Mitigating Sybils in Federated Learning Poisoning", "comments": "16 pages, Extended technical version of conference paper \"The\n  Limitations of Federated Learning in Sybil Settings\" accepted at RAID 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) over distributed multi-party data is required for a\nvariety of domains. Existing approaches, such as federated learning, collect\nthe outputs computed by a group of devices at a central aggregator and run\niterative algorithms to train a globally shared model. Unfortunately, such\napproaches are susceptible to a variety of attacks, including model poisoning,\nwhich is made substantially worse in the presence of sybils.\n  In this paper we first evaluate the vulnerability of federated learning to\nsybil-based poisoning attacks. We then describe \\emph{FoolsGold}, a novel\ndefense to this problem that identifies poisoning sybils based on the diversity\nof client updates in the distributed learning process. Unlike prior work, our\nsystem does not bound the expected number of attackers, requires no auxiliary\ninformation outside of the learning process, and makes fewer assumptions about\nclients and their data.\n  In our evaluation we show that FoolsGold exceeds the capabilities of existing\nstate of the art approaches to countering sybil-based label-flipping and\nbackdoor poisoning attacks. Our results hold for different distributions of\nclient data, varying poisoning targets, and various sybil strategies.\n  Code can be found at: https://github.com/DistributedML/FoolsGold\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 19:20:35 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 19:32:10 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 00:14:25 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 17:54:48 GMT"}, {"version": "v5", "created": "Wed, 15 Jul 2020 15:35:17 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Fung", "Clement", ""], ["Yoon", "Chris J. M.", ""], ["Beschastnikh", "Ivan", ""]]}, {"id": "1808.04873", "submitter": "Benjamin Scellier", "authors": "Benjamin Scellier, Anirudh Goyal, Jonathan Binas, Thomas Mesnard,\n  Yoshua Bengio", "title": "Generalization of Equilibrium Propagation to Vector Field Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The biological plausibility of the backpropagation algorithm has long been\ndoubted by neuroscientists. Two major reasons are that neurons would need to\nsend two different types of signal in the forward and backward phases, and that\npairs of neurons would need to communicate through symmetric bidirectional\nconnections. We present a simple two-phase learning procedure for fixed point\nrecurrent networks that addresses both these issues. In our model, neurons\nperform leaky integration and synaptic weights are updated through a local\nmechanism. Our learning method generalizes Equilibrium Propagation to vector\nfield dynamics, relaxing the requirement of an energy function. As a\nconsequence of this generalization, the algorithm does not compute the true\ngradient of the objective function, but rather approximates it at a precision\nwhich is proven to be directly related to the degree of symmetry of the\nfeedforward and feedback weights. We show experimentally that our algorithm\noptimizes the objective function.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 19:41:12 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Scellier", "Benjamin", ""], ["Goyal", "Anirudh", ""], ["Binas", "Jonathan", ""], ["Mesnard", "Thomas", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1808.04875", "submitter": "Orly Avner", "authors": "Orly Avner and Shie Mannor", "title": "Multi-user Communication Networks: A Coordinated Multi-armed Bandit\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication networks shared by many users are a widespread challenge\nnowadays. In this paper we address several aspects of this challenge\nsimultaneously: learning unknown stochastic network characteristics, sharing\nresources with other users while keeping coordination overhead to a minimum.\nThe proposed solution combines Multi-Armed Bandit learning with a lightweight\nsignalling-based coordination scheme, and ensures convergence to a stable\nallocation of resources. Our work considers single-user level algorithms for\ntwo scenarios: an unknown fixed number of users, and a dynamic number of users.\nAnalytic performance guarantees, proving convergence to stable marriage\nconfigurations, are presented for both setups. The algorithms are designed\nbased on a system-wide perspective, rather than focusing on single user\nwelfare. Thus, maximal resource utilization is ensured. An extensive\nexperimental analysis covers convergence to a stable configuration as well as\nreward maximization. Experiments are carried out over a wide range of setups,\ndemonstrating the advantages of our approach over existing state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 19:55:38 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Avner", "Orly", ""], ["Mannor", "Shie", ""]]}, {"id": "1808.04880", "submitter": "Alexander New", "authors": "Alexander New and Kristin P. Bennett", "title": "A Precision Environment-Wide Association Study of Hypertension via\n  Supervised Cadre Models", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem in precision health of grouping people into\nsubpopulations based on their degree of vulnerability to a risk factor. These\nsubpopulations cannot be discovered with traditional clustering techniques\nbecause their quality is evaluated with a supervised metric: the ease of\nmodeling a response variable over observations within them. Instead, we apply\nthe supervised cadre model (SCM), which does use this metric. We extend the SCM\nformalism so that it may be applied to multivariate regression and binary\nclassification problems. We also develop a way to use conditional entropy to\nassess the confidence in the process by which a subject is assigned their\ncadre. Using the SCM, we generalize the environment-wide association study\n(EWAS) workflow to be able to model heterogeneity in population risk. In our\nEWAS, we consider more than two hundred environmental exposure factors and find\ntheir association with diastolic blood pressure, systolic blood pressure, and\nhypertension. This requires adapting the SCM to be applicable to data generated\nby a complex survey design. After correcting for false positives, we found 25\nexposure variables that had a significant association with at least one of our\nresponse variables. Eight of these were significant for a discovered\nsubpopulation but not for the overall population. Some of these associations\nhave been identified by previous researchers, while others appear to be novel.\nWe examine several discovered subpopulations in detail, and we find that they\nare interpretable and that they suggest further research questions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 20:08:33 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 20:25:47 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["New", "Alexander", ""], ["Bennett", "Kristin P.", ""]]}, {"id": "1808.04883", "submitter": "Lie He", "authors": "Lie He, An Bian and Martin Jaggi", "title": "COLA: Decentralized Linear Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized machine learning is a promising emerging paradigm in view of\nglobal challenges of data ownership and privacy. We consider learning of linear\nclassification and regression models, in the setting where the training data is\ndecentralized over many user devices, and the learning algorithm must run\non-device, on an arbitrary communication network, without a central\ncoordinator. We propose COLA, a new decentralized training algorithm with\nstrong theoretical guarantees and superior practical performance. Our framework\novercomes many limitations of existing methods, and achieves communication\nefficiency, scalability, elasticity as well as resilience to changes in data\nand participating devices.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 11:45:16 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 20:24:00 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2018 17:29:52 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 22:13:40 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["He", "Lie", ""], ["Bian", "An", ""], ["Jaggi", "Martin", ""]]}, {"id": "1808.04888", "submitter": "Catherine Olsson", "authors": "Catherine Olsson, Surya Bhupatiraju, Tom Brown, Augustus Odena, Ian\n  Goodfellow", "title": "Skill Rating for Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a new way to evaluate generative models using insights from\nevaluation of competitive games between human players. We show experimentally\nthat tournaments between generators and discriminators provide an effective way\nto evaluate generative models. We introduce two methods for summarizing\ntournament outcomes: tournament win rate and skill rating. Evaluations are\nuseful in different contexts, including monitoring the progress of a single\nmodel as it learns during the training process, and comparing the capabilities\nof two different fully trained models. We show that a tournament consisting of\na single model playing against past and future versions of itself produces a\nuseful measure of training progress. A tournament containing multiple separate\nmodels (using different seeds, hyperparameters, and architectures) provides a\nuseful relative comparison between different trained GANs. Tournament-based\nrating methods are conceptually distinct from numerous previous categories of\napproaches to evaluation of generative models, and have complementary\nadvantages and disadvantages.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 20:39:17 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Olsson", "Catherine", ""], ["Bhupatiraju", "Surya", ""], ["Brown", "Tom", ""], ["Odena", "Augustus", ""], ["Goodfellow", "Ian", ""]]}, {"id": "1808.04926", "submitter": "Divyansh Kaushik", "authors": "Divyansh Kaushik, Zachary C. Lipton", "title": "How Much Reading Does Reading Comprehension Require? A Critical\n  Investigation of Popular Benchmarks", "comments": "To appear in EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent papers address reading comprehension, where examples consist of\n(question, passage, answer) tuples. Presumably, a model must combine\ninformation from both questions and passages to predict corresponding answers.\nHowever, despite intense interest in the topic, with hundreds of published\npapers vying for leaderboard dominance, basic questions about the difficulty of\nmany popular benchmarks remain unanswered. In this paper, we establish sensible\nbaselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding\nthat question- and passage-only models often perform surprisingly well. On $14$\nout of $20$ bAbI tasks, passage-only models achieve greater than $50\\%$\naccuracy, sometimes matching the full model. Interestingly, while CBT provides\n$20$-sentence stories only the last is needed for comparably accurate\nprediction. By comparison, SQuAD and CNN appear better-constructed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 23:59:26 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 16:48:54 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Kaushik", "Divyansh", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1808.04928", "submitter": "Zachariah Zhang", "authors": "Jingshu Liu, Zachariah Zhang, Narges Razavian", "title": "Deep EHR: Chronic Disease Prediction Using Medical Notes", "comments": "Machine Learning for Health Care conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of preventable diseases is important for better disease\nmanagement, improved inter-ventions, and more efficient health-care resource\nallocation. Various machine learning approacheshave been developed to utilize\ninformation in Electronic Health Record (EHR) for this task. Majorityof\nprevious attempts, however, focus on structured fields and lose the vast amount\nof information inthe unstructured notes. In this work we propose a general\nmulti-task framework for disease onsetprediction that combines both free-text\nmedical notes and structured information. We compareperformance of different\ndeep learning architectures including CNN, LSTM and hierarchical models.In\ncontrast to traditional text-based prediction models, our approach does not\nrequire disease specificfeature engineering, and can handle negations and\nnumerical values that exist in the text. Ourresults on a cohort of about 1\nmillion patients show that models using text outperform modelsusing just\nstructured data, and that models capable of using numerical values and\nnegations in thetext, in addition to the raw text, further improve performance.\nAdditionally, we compare differentvisualization methods for medical\nprofessionals to interpret model predictions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 00:10:55 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Liu", "Jingshu", ""], ["Zhang", "Zachariah", ""], ["Razavian", "Narges", ""]]}, {"id": "1808.04929", "submitter": "Lucian Trestioreanu", "authors": "Lucian Trestioreanu", "title": "Holographic Visualisation of Radiology Data and Automated Machine\n  Learning-based Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within this thesis we propose a platform for combining Augmented Reality (AR)\nhardware with machine learning in a user-oriented pipeline, offering to the\nmedical staff an intuitive 3D visualization of volumetric Computed Tomography\n(CT) and Magnetic Resonance Imaging (MRI) medical image segmentations inside\nthe AR headset, that does not need human intervention for loading, processing\nand segmentation of medical images. The AR visualization, based on Microsoft\nHoloLens, employs a modular and thus scalable frontend-backend architecture for\nreal-time visualizations on multiple AR headsets. As Convolutional Neural\nNetworks (CNNs) have lastly demonstrated superior performance for the machine\nlearning task of image semantic segmentation, the pipeline also includes a\nfully automated CNN algorithm for the segmentation of the liver from CT scans.\nThe model is based on the Deep Retinal Image Understanding (DRIU) model which\nis a Fully Convolutional Network with side outputs from feature maps with\ndifferent resolution, extracted at different stages of the network. The\nalgorithm is 2.5D which means that the input is a set of consecutive scan\nslices. The experiments have been performed on the Liver Tumor Segmentation\nChallenge (LiTS) dataset for liver segmentation and demonstrated good results\nand flexibility. While multiple approaches exist in the domain, only few of\nthem have focused on overcoming the practical aspects which still largely hold\nthis technology away from the operating rooms. In line with this, we also are\nnext planning an evaluation from medical doctors and radiologists in a\nreal-world environment.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 00:20:35 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Trestioreanu", "Lucian", ""]]}, {"id": "1808.04947", "submitter": "Lu Lu", "authors": "Lu Lu, Yanhui Su, George Em Karniadakis", "title": "Collapse of Deep and Narrow Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical work has demonstrated that deep neural networks have\nsuperior performance over shallow networks, but their training is more\ndifficult, e.g., they suffer from the vanishing gradient problem. This problem\ncan be typically resolved by the rectified linear unit (ReLU) activation.\nHowever, here we show that even for such activation, deep and narrow neural\nnetworks (NNs) will converge to erroneous mean or median states of the target\nfunction depending on the loss with high probability. Deep and narrow NNs are\nencountered in solving partial differential equations with high-order\nderivatives. We demonstrate this collapse of such NNs both numerically and\ntheoretically, and provide estimates of the probability of collapse. We also\nconstruct a diagram of a safe region for designing NNs that avoid the collapse\nto erroneous states. Finally, we examine different ways of initialization and\nnormalization that may avoid the collapse problem. Asymmetric initializations\nmay reduce the probability of collapse but do not totally eliminate it.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 02:15:45 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 21:12:44 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Lu", "Lu", ""], ["Su", "Yanhui", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1808.04952", "submitter": "Hao Pan", "authors": "Yuqi Yang, Shilin Liu, Hao Pan, Yang Liu, Xin Tong", "title": "PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel\n  Frames", "comments": "15 pages, 18 figures. CVPR 2020. Project page:\n  https://haopan.github.io/surfacecnn.html", "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 13578-13587", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface meshes are widely used shape representations and capture finer\ngeometry data than point clouds or volumetric grids, but are challenging to\napply CNNs directly due to their non-Euclidean structure. We use parallel\nframes on surface to define PFCNNs that enable effective feature learning on\nsurface meshes by mimicking standard convolutions faithfully. In particular,\nthe convolution of PFCNN not only maps local surface patches onto flat tangent\nplanes, but also aligns the tangent planes such that they locally form a flat\nEuclidean structure, thus enabling recovery of standard convolutions. The\nalignment is achieved by the tool of locally flat connections borrowed from\ndiscrete differential geometry, which can be efficiently encoded and computed\nby parallel frame fields. In addition, the lack of canonical axis on surface is\nhandled by sampling with the frame directions. Experiments show that for tasks\nincluding classification, segmentation and registration on deformable geometric\ndomains, as well as semantic scene segmentation on rigid domains, PFCNNs\nachieve robust and superior performances without using sophisticated input\nfeatures than state-of-the-art surface based CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 02:39:35 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 05:58:56 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Yang", "Yuqi", ""], ["Liu", "Shilin", ""], ["Pan", "Hao", ""], ["Liu", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "1808.05032", "submitter": "Per-Arne Andersen", "authors": "Per-Arne Andersen, Morten Goodwin, Ole-Christoffer Granmo", "title": "Deep RTS: A Game Environment for Deep Reinforcement Learning in\n  Real-Time Strategy Games", "comments": "Proceedings of the IEEE International Conference on Computational\n  Intelligence and Games (CIG 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reinforcement learning (RL) is an area of research that has blossomed\ntremendously in recent years and has shown remarkable potential for artificial\nintelligence based opponents in computer games. This success is primarily due\nto the vast capabilities of convolutional neural networks, that can extract\nuseful features from noisy and complex data. Games are excellent tools to test\nand push the boundaries of novel RL algorithms because they give valuable\ninsight into how well an algorithm can perform in isolated environments without\nthe real-life consequences. Real-time strategy games (RTS) is a genre that has\ntremendous complexity and challenges the player in short and long-term\nplanning. There is much research that focuses on applied RL in RTS games, and\nnovel advances are therefore anticipated in the not too distant future.\nHowever, there are to date few environments for testing RTS AIs. Environments\nin the literature are often either overly simplistic, such as microRTS, or\ncomplex and without the possibility for accelerated learning on consumer\nhardware like StarCraft II. This paper introduces the Deep RTS game environment\nfor testing cutting-edge artificial intelligence algorithms for RTS games. Deep\nRTS is a high-performance RTS game made specifically for artificial\nintelligence research. It supports accelerated learning, meaning that it can\nlearn at a magnitude of 50 000 times faster compared to existing RTS games.\nDeep RTS has a flexible configuration, enabling research in several different\nRTS scenarios, including partially observable state-spaces and map complexity.\nWe show that Deep RTS lives up to our promises by comparing its performance\nwith microRTS, ELF, and StarCraft II on high-end consumer hardware. Using Deep\nRTS, we show that a Deep Q-Network agent beats random-play agents over 70% of\nthe time. Deep RTS is publicly available at https://github.com/cair/DeepRTS.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 10:30:41 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Andersen", "Per-Arne", ""], ["Goodwin", "Morten", ""], ["Granmo", "Ole-Christoffer", ""]]}, {"id": "1808.05054", "submitter": "Milo Honegger", "authors": "Milo Honegger", "title": "Shedding Light on Black Box Machine Learning Algorithms: Development of\n  an Axiomatic Framework to Assess the Quality of Methods that Explain\n  Individual Predictions", "comments": "Keywords: black box, machine learning, interpretability, explanation\n  methods, explanation quality, axiomatic explanation consistency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From self-driving vehicles and back-flipping robots to virtual assistants who\nbook our next appointment at the hair salon or at that restaurant for dinner -\nmachine learning systems are becoming increasingly ubiquitous. The main reason\nfor this is that these methods boast remarkable predictive capabilities.\nHowever, most of these models remain black boxes, meaning that it is very\nchallenging for humans to follow and understand their intricate inner workings.\nConsequently, interpretability has suffered under this ever-increasing\ncomplexity of machine learning models. Especially with regards to new\nregulations, such as the General Data Protection Regulation (GDPR), the\nnecessity for plausibility and verifiability of predictions made by these black\nboxes is indispensable. Driven by the needs of industry and practice, the\nresearch community has recognised this interpretability problem and focussed on\ndeveloping a growing number of so-called explanation methods over the past few\nyears. These methods explain individual predictions made by black box machine\nlearning models and help to recover some of the lost interpretability. With the\nproliferation of these explanation methods, it is, however, often unclear,\nwhich explanation method offers a higher explanation quality, or is generally\nbetter-suited for the situation at hand. In this thesis, we thus propose an\naxiomatic framework, which allows comparing the quality of different\nexplanation methods amongst each other. Through experimental validation, we\nfind that the developed framework is useful to assess the explanation quality\nof different explanation methods and reach conclusions that are consistent with\nindependent research.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 12:25:02 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Honegger", "Milo", ""]]}, {"id": "1808.05092", "submitter": "Hirokazu Kameoka", "authors": "Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, Nobukatsu Hojo", "title": "ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary\n  classifier variational autoencoder", "comments": "Publised in IEEE/ACM Trans. ASLP\n  https://ieeexplore.ieee.org/abstract/document/8718381 Please also refer to\n  our related articles: arXiv:1806.02169, arXiv:2008.12604", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a non-parallel many-to-many voice conversion (VC) method\nusing a variant of the conditional variational autoencoder (VAE) called an\nauxiliary classifier VAE (ACVAE). The proposed method has three key features.\nFirst, it adopts fully convolutional architectures to construct the encoder and\ndecoder networks so that the networks can learn conversion rules that capture\ntime dependencies in the acoustic feature sequences of source and target\nspeech. Second, it uses an information-theoretic regularization for the model\ntraining to ensure that the information in the attribute class label will not\nbe lost in the conversion process. With regular CVAEs, the encoder and decoder\nare free to ignore the attribute class label input. This can be problematic\nsince in such a situation, the attribute class label will have little effect on\ncontrolling the voice characteristics of input speech at test time. Such\nsituations can be avoided by introducing an auxiliary classifier and training\nthe encoder and decoder so that the attribute classes of the decoder outputs\nare correctly predicted by the classifier. Third, it avoids producing\nbuzzy-sounding speech at test time by simply transplanting the spectral details\nof the input speech into its converted version. Subjective evaluation\nexperiments revealed that this simple method worked reasonably well in a\nnon-parallel many-to-many speaker identity conversion task.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 23:31:01 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 07:34:07 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 19:41:54 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kameoka", "Hirokazu", ""], ["Kaneko", "Takuhiro", ""], ["Tanaka", "Kou", ""], ["Hojo", "Nobukatsu", ""]]}, {"id": "1808.05110", "submitter": "Xiaoxiang Zhu", "authors": "Danfeng Hong, Naoto Yokoya, Jian Xu, Xiaoxiang Zhu", "title": "Joint & Progressive Learning from High-Dimensional Data for Multi-Label\n  Classification", "comments": "accepted in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that nonlinear subspace learning techniques (e.g. manifold\nlearning) have successfully applied to data representation, there is still room\nfor improvement in explainability (explicit mapping), generalization\n(out-of-samples), and cost-effectiveness (linearization). To this end, a novel\nlinearized subspace learning technique is developed in a joint and progressive\nway, called \\textbf{j}oint and \\textbf{p}rogressive \\textbf{l}earning\nstr\\textbf{a}teg\\textbf{y} (J-Play), with its application to multi-label\nclassification. The J-Play learns high-level and semantically meaningful\nfeature representation from high-dimensional data by 1) jointly performing\nmultiple subspace learning and classification to find a latent subspace where\nsamples are expected to be better classified; 2) progressively learning\nmulti-coupled projections to linearly approach the optimal mapping bridging the\noriginal space with the most discriminative subspace; 3) locally embedding\nmanifold structure in each learnable latent subspace. Extensive experiments are\nperformed to demonstrate the superiority and effectiveness of the proposed\nmethod in comparison with previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 14:54:44 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Hong", "Danfeng", ""], ["Yokoya", "Naoto", ""], ["Xu", "Jian", ""], ["Zhu", "Xiaoxiang", ""]]}, {"id": "1808.05128", "submitter": "Abhijit Mahalunkar", "authors": "Abhijit Mahalunkar, John D. Kelleher", "title": "Using Regular Languages to Explore the Representational Capacity of\n  Recurrent Neural Architectures", "comments": "International Conference of Artificial Neural Networks (ICANN) 2018", "journal-ref": null, "doi": "10.1007/978-3-030-01424-7_19", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The presence of Long Distance Dependencies (LDDs) in sequential data poses\nsignificant challenges for computational models. Various recurrent neural\narchitectures have been designed to mitigate this issue. In order to test these\nstate-of-the-art architectures, there is growing need for rich benchmarking\ndatasets. However, one of the drawbacks of existing datasets is the lack of\nexperimental control with regards to the presence and/or degree of LDDs. This\nlack of control limits the analysis of model performance in relation to the\nspecific challenge posed by LDDs. One way to address this is to use synthetic\ndata having the properties of subregular languages. The degree of LDDs within\nthe generated data can be controlled through the k parameter, length of the\ngenerated strings, and by choosing appropriate forbidden strings. In this\npaper, we explore the capacity of different RNN extensions to model LDDs, by\nevaluating these models on a sequence of SPk synthesized datasets, where each\nsubsequent dataset exhibits a longer degree of LDD. Even though SPk are simple\nlanguages, the presence of LDDs does have significant impact on the performance\nof recurrent neural architectures, thus making them prime candidate in\nbenchmarking tasks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 15:20:49 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Mahalunkar", "Abhijit", ""], ["Kelleher", "John D.", ""]]}, {"id": "1808.05140", "submitter": "Faris B Mismar", "authors": "Faris B. Mismar, Jinseok Choi, and Brian L. Evans", "title": "A Framework for Automated Cellular Network Tuning with Reinforcement\n  Learning", "comments": "(c) 2019 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2019 IEEE Transactions on Communications", "doi": "10.1109/TCOMM.2019.2926715", "report-no": null, "categories": "cs.NI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning cellular network performance against always occurring wireless\nimpairments can dramatically improve reliability to end users. In this paper,\nwe formulate cellular network performance tuning as a reinforcement learning\n(RL) problem and provide a solution to improve the performance for indoor and\noutdoor environments. By leveraging the ability of Q-learning to estimate\nfuture performance improvement rewards, we propose two algorithms: (1) closed\nloop power control (PC) for downlink voice over LTE (VoLTE) and (2)\nself-organizing network (SON) fault management. The VoLTE PC algorithm uses RL\nto adjust the indoor base station transmit power so that the signal to\ninterference plus noise ratio (SINR) of a user equipment (UE) meets the target\nSINR. It does so without the UE having to send power control requests. The SON\nfault management algorithm uses RL to improve the performance of an outdoor\nbase station cluster by resolving faults in the network through configuration\nmanagement. Both algorithms exploit measurements from the connected users,\nwireless impairments, and relevant configuration parameters to solve a\nnon-convex performance optimization problem using RL. Simulation results show\nthat our proposed RL based algorithms outperform the industry standards today\nin realistic cellular communication environments.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 22:46:41 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 21:12:39 GMT"}, {"version": "v3", "created": "Sun, 5 May 2019 18:42:47 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 17:49:26 GMT"}, {"version": "v5", "created": "Thu, 18 Jul 2019 12:29:13 GMT"}, {"version": "v6", "created": "Sun, 29 Dec 2019 04:27:15 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Mismar", "Faris B.", ""], ["Choi", "Jinseok", ""], ["Evans", "Brian L.", ""]]}, {"id": "1808.05160", "submitter": "Tuyen Truong", "authors": "Tuyen Trung Truong and Tuan Hang Nguyen", "title": "Backtracking gradient descent method for general $C^1$ functions, with\n  applications to Deep Learning", "comments": "37 pages, 3 figures, 3 tables. Exposition improved, many new results\n  are added. Accompanying source codes will be available at the link:\n  https://github.com/hank-nguyen/MBT-optimizer", "journal-ref": "Applied Mathematics and Optimization 2020, Minimax Theory and its\n  Applications 2021", "doi": null, "report-no": "The paper, with additional experiments and in combination with\n  arXiv:2001.02005 and arXiv:2007.03618 - and a reference to a variant of\n  Backtracking GD by the first author (applicable to Lojasiewicz gradient\n  inequality), has been divided into 2 parts (titles changed) and accepted for\n  publications in 2 journals (see below)", "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Standard gradient descent is one very popular optimisation method, its\nconvergence cannot be proven beyond the class of functions whose gradient is\nglobally Lipschitz continuous. As such, it is not actually applicable to\nrealistic applications such as Deep Neural Networks. In this paper, we prove\nthat its backtracking variant behaves very nicely, in particular convergence\ncan be shown for all Morse functions. The main theoretical result of this paper\nis as follows.\n  Theorem. Let $f:\\mathbb{R}^k\\rightarrow \\mathbb{R}$ be a $C^1$ function, and\n$\\{z_n\\}$ a sequence constructed from the Backtracking gradient descent\nalgorithm. (1) Either $\\lim _{n\\rightarrow\\infty}||z_n||=\\infty$ or $\\lim\n_{n\\rightarrow\\infty}||z_{n+1}-z_n||=0$. (2) Assume that $f$ has at most\ncountably many critical points. Then either $\\lim\n_{n\\rightarrow\\infty}||z_n||=\\infty$ or $\\{z_n\\}$ converges to a critical point\nof $f$. (3) More generally, assume that all connected components of the set of\ncritical points of $f$ are compact. Then either $\\lim\n_{n\\rightarrow\\infty}||z_n||=\\infty$ or $\\{z_n\\}$ is bounded. Moreover, in the\nlatter case the set of cluster points of $\\{z_n\\}$ is connected.\n  Some generalised versions of this result, including an inexact version, are\nincluded. Another result in this paper concerns the problem of saddle points.\nWe then present a heuristic argument to explain why Standard gradient descent\nmethod works so well, and modifications of the backtracking versions of GD, MMT\nand NAG. Experiments with datasets CIFAR10 and CIFAR100 on various popular\narchitectures verify the heuristic argument also for the mini-batch practice\nand show that our new algorithms, while automatically fine tuning learning\nrates, perform better than current state-of-the-art methods such as MMT, NAG,\nAdagrad, Adadelta, RMSProp, Adam and Adamax.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 15:54:24 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 17:20:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Truong", "Tuyen Trung", ""], ["Nguyen", "Tuan Hang", ""]]}, {"id": "1808.05163", "submitter": "Fajie Yuan", "authors": "Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose,\n  Xiangnan He", "title": "A Simple Convolutional Generative Network for Next Item Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been recently introduced in the\ndomain of session-based next item recommendation. An ordered collection of past\nitems the user has interacted with in a session (or sequence) are embedded into\na 2-dimensional latent matrix, and treated as an image. The convolution and\npooling operations are then applied to the mapped item embeddings. In this\npaper, we first examine the typical session-based CNN recommender and show that\nboth the generative model and network architecture are suboptimal when modeling\nlong-range dependencies in the item sequence. To address the issues, we\nintroduce a simple, but very effective generative model that is capable of\nlearning high-level representation from both short- and long-range item\ndependencies. The network architecture of the proposed model is formed of a\nstack of \\emph{holed} convolutional layers, which can efficiently increase the\nreceptive fields without relying on the pooling operation. Another contribution\nis the effective use of residual block structure in recommender systems, which\ncan ease the optimization for much deeper networks. The proposed generative\nmodel attains state-of-the-art accuracy with less training time in the next\nitem recommendation task. It accordingly can be used as a powerful\nrecommendation baseline to beat in future, especially when there are long\nsequences of user feedback.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 16:04:28 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 13:13:02 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 17:48:04 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 03:18:35 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Yuan", "Fajie", ""], ["Karatzoglou", "Alexandros", ""], ["Arapakis", "Ioannis", ""], ["Jose", "Joemon M", ""], ["He", "Xiangnan", ""]]}, {"id": "1808.05174", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Shugao Ma, Deva Ramanan, Yaser Sheikh", "title": "Recycle-GAN: Unsupervised Video Retargeting", "comments": "ECCV 2018; Please refer to project webpage for videos -\n  http://www.cs.cmu.edu/~aayushb/Recycle-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-driven approach for unsupervised video retargeting that\ntranslates content from one domain to another while preserving the style native\nto a domain, i.e., if contents of John Oliver's speech were to be transferred\nto Stephen Colbert, then the generated content/speech should be in Stephen\nColbert's style. Our approach combines both spatial and temporal information\nalong with adversarial losses for content translation and style preservation.\nIn this work, we first study the advantages of using spatiotemporal constraints\nover spatial constraints for effective retargeting. We then demonstrate the\nproposed approach for the problems where information in both space and time\nmatters such as face-to-face translation, flower-to-flower, wind and cloud\nsynthesis, sunrise and sunset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 16:34:08 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Bansal", "Aayush", ""], ["Ma", "Shugao", ""], ["Ramanan", "Deva", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1808.05238", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Yufang Huang, Liang Zeng, Xuming Chen, Yong Liu, Zhen\n  Qian, Nan Du, Wei Fan, Xiaohui Xie", "title": "AnatomyNet: Deep Learning for Fast and Fully Automated Whole-volume\n  Segmentation of Head and Neck Anatomy", "comments": "6 figures, 4 videos in GitHub and YouTube. Accepted by Medical\n  Physics. Code and videos are available on GitHub. Video:\n  https://www.youtube.com/watch?v=_PpIUIm4XLU", "journal-ref": null, "doi": "10.1002/mp.13300", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Methods: Our deep learning model, called AnatomyNet, segments OARs from head\nand neck CT images in an end-to-end fashion, receiving whole-volume HaN CT\nimages as input and generating masks of all OARs of interest in one shot.\nAnatomyNet is built upon the popular 3D U-net architecture, but extends it in\nthree important ways: 1) a new encoding scheme to allow auto-segmentation on\nwhole-volume CT images instead of local patches or subsets of slices, 2)\nincorporating 3D squeeze-and-excitation residual blocks in encoding layers for\nbetter feature representation, and 3) a new loss function combining Dice scores\nand focal loss to facilitate the training of the neural model. These features\nare designed to address two main challenges in deep-learning-based HaN\nsegmentation: a) segmenting small anatomies (i.e., optic chiasm and optic\nnerves) occupying only a few slices, and b) training with inconsistent data\nannotations with missing ground truth for some anatomical structures.\n  Results: We collected 261 HaN CT images to train AnatomyNet, and used MICCAI\nHead and Neck Auto Segmentation Challenge 2015 as a benchmark dataset to\nevaluate the performance of AnatomyNet. The objective is to segment nine\nanatomies: brain stem, chiasm, mandible, optic nerve left, optic nerve right,\nparotid gland left, parotid gland right, submandibular gland left, and\nsubmandibular gland right. Compared to previous state-of-the-art results from\nthe MICCAI 2015 competition, AnatomyNet increases Dice similarity coefficient\nby 3.3% on average. AnatomyNet takes about 0.12 seconds to fully segment a head\nand neck CT image of dimension 178 x 302 x 225, significantly faster than\nprevious methods. In addition, the model is able to process whole-volume CT\nimages and delineate all OARs in one pass, requiring little pre- or\npost-processing.\nhttps://github.com/wentaozhu/AnatomyNet-for-anatomical-segmentation.git.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 18:03:12 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 00:23:48 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Zhu", "Wentao", ""], ["Huang", "Yufang", ""], ["Zeng", "Liang", ""], ["Chen", "Xuming", ""], ["Liu", "Yong", ""], ["Qian", "Zhen", ""], ["Du", "Nan", ""], ["Fan", "Wei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1808.05240", "submitter": "Penghang Yin", "authors": "Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi,\n  Jack Xin", "title": "Blended Coarse Gradient Descent for Full Quantization of Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantized deep neural networks (QDNNs) are attractive due to their much lower\nmemory storage and faster inference speed than their regular full precision\ncounterparts. To maintain the same performance level especially at low\nbit-widths, QDNNs must be retrained. Their training involves piecewise constant\nactivation functions and discrete weights, hence mathematical challenges arise.\nWe introduce the notion of coarse gradient and propose the blended coarse\ngradient descent (BCGD) algorithm, for training fully quantized neural\nnetworks. Coarse gradient is generally not a gradient of any function but an\nartificial ascent direction. The weight update of BCGD goes by coarse gradient\ncorrection of a weighted average of the full precision weights and their\nquantization (the so-called blending), which yields sufficient descent in the\nobjective value and thus accelerates the training. Our experiments demonstrate\nthat this simple blending technique is very effective for quantization at\nextremely low bit-width such as binarization. In full quantization of ResNet-18\nfor ImageNet classification task, BCGD gives 64.36\\% top-1 accuracy with binary\nweights across all layers and 4-bit adaptive activation. If the weights in the\nfirst and last layers are kept in full precision, this number increases to\n65.46\\%. As theoretical justification, we show convergence analysis of coarse\ngradient descent for a two-linear-layer neural network model with Gaussian\ninput data, and prove that the expected coarse gradient correlates positively\nwith the underlying true gradient.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 18:13:12 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 01:37:23 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2018 07:04:22 GMT"}, {"version": "v4", "created": "Sun, 6 Jan 2019 06:00:43 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Yin", "Penghang", ""], ["Zhang", "Shuai", ""], ["Lyu", "Jiancheng", ""], ["Osher", "Stanley", ""], ["Qi", "Yingyong", ""], ["Xin", "Jack", ""]]}, {"id": "1808.05264", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Eduardo R. Rodrigues, Igor Oliveira, Renato L. F. Cunha, Marco A. S.\n  Netto", "title": "DeepDownscale: a Deep Learning Strategy for High-Resolution Weather\n  Forecast", "comments": "8 pages, 6 figures, accepted for publication at 14th IEEE eScience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running high-resolution physical models is computationally expensive and\nessential for many disciplines. Agriculture, transportation, and energy are\nsectors that depend on high-resolution weather models, which typically consume\nmany hours of large High Performance Computing (HPC) systems to deliver timely\nresults. Many users cannot afford to run the desired resolution and are forced\nto use low resolution output. One simple solution is to interpolate results for\nvisualization. It is also possible to combine an ensemble of low resolution\nmodels to obtain a better prediction. However, these approaches fail to capture\nthe redundant information and patterns in the low-resolution input that could\nhelp improve the quality of prediction. In this paper, we propose and evaluate\na strategy based on a deep neural network to learn a high-resolution\nrepresentation from low-resolution predictions using weather forecast as a\npractical use case. We take a supervised learning approach, since obtaining\nlabeled data can be done automatically. Our results show significant\nimprovement when compared with standard practices and the strategy is still\nlightweight enough to run on modest computer systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 19:22:15 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Rodrigues", "Eduardo R.", ""], ["Oliveira", "Igor", ""], ["Cunha", "Renato L. F.", ""], ["Netto", "Marco A. S.", ""]]}, {"id": "1808.05293", "submitter": "Susan Athey", "authors": "Susan Athey and Guido Imbens", "title": "Design-based Analysis in Difference-In-Differences Settings with\n  Staggered Adoption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study estimation of and inference for average treatment\neffects in a setting with panel data. We focus on the setting where units,\ne.g., individuals, firms, or states, adopt the policy or treatment of interest\nat a particular point in time, and then remain exposed to this treatment at all\ntimes afterwards. We take a design perspective where we investigate the\nproperties of estimators and procedures given assumptions on the assignment\nprocess. We show that under random assignment of the adoption date the standard\nDifference-In-Differences estimator is is an unbiased estimator of a particular\nweighted average causal effect. We characterize the proeperties of this\nestimand, and show that the standard variance estimator is conservative.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 22:10:57 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 15:50:27 GMT"}, {"version": "v3", "created": "Sat, 1 Sep 2018 15:48:05 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Athey", "Susan", ""], ["Imbens", "Guido", ""]]}, {"id": "1808.05329", "submitter": "Ruinan Zhang", "authors": "Ruinan Zhang, Fanglan Zheng, Wei Min", "title": "Sequential Behavioral Data Processing Using Deep Learning and the Markov\n  Transition Field in Online Fraud Detection", "comments": "KDD2018 Data Science in Fintech Workshop Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the popularity of the Internet and smart mobile devices, more and more\nfinancial transactions and activities have been digitalized. Compared to\ntraditional financial fraud detection strategies using credit-related features,\ncustomers are generating a large amount of unstructured behavioral data every\nsecond. In this paper, we propose an Recurrent Neural Netword (RNN) based\ndeep-learning structure integrated with Markov Transition Field (MTF) for\npredicting online fraud behaviors using customer's interactions with websites\nor smart-phone apps as a series of states. In practice, we tested and proved\nthat the proposed network structure for processing sequential behavioral data\ncould significantly boost fraud predictive ability comparing with the\nmultilayer perceptron network and distance based classifier with Dynamic Time\nWarping(DTW) as distance metric.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 02:58:54 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Zhang", "Ruinan", ""], ["Zheng", "Fanglan", ""], ["Min", "Wei", ""]]}, {"id": "1808.05334", "submitter": "Samarth Gupta", "authors": "Samarth Gupta, Gauri Joshi, Osman Ya\\u{g}an", "title": "Active Distribution Learning from Indirect Samples", "comments": "Allerton Conference on Communication, Control and Computing, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of {\\em learning} the probability distribution\n$P_X$ of a discrete random variable $X$ using indirect and sequential samples.\nAt each time step, we choose one of the possible $K$ functions, $g_1, \\ldots,\ng_K$ and observe the corresponding sample $g_i(X)$. The goal is to estimate the\nprobability distribution of $X$ by using a minimum number of such sequential\nsamples. This problem has several real-world applications including inference\nunder non-precise information and privacy-preserving statistical estimation. We\nestablish necessary and sufficient conditions on the functions $g_1, \\ldots,\ng_K$ under which asymptotically consistent estimation is possible. We also\nderive lower bounds on the estimation error as a function of total samples and\nshow that it is order-wise achievable. Leveraging these results, we propose an\niterative algorithm that i) chooses the function to observe at each step based\non past observations; and ii) combines the obtained samples to estimate $p_X$.\nThe performance of this algorithm is investigated numerically under various\nscenarios, and shown to outperform baseline approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 03:25:09 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Gupta", "Samarth", ""], ["Joshi", "Gauri", ""], ["Ya\u011fan", "Osman", ""]]}, {"id": "1808.05335", "submitter": "Filip Korzeniowski", "authors": "Filip Korzeniowski and Gerhard Widmer", "title": "Improved Chord Recognition by Combining Duration and Harmonic Language\n  Models", "comments": "Published at 19th International Society for Music Information\n  Retrieval Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chord recognition systems typically comprise an acoustic model that predicts\nchords for each audio frame, and a temporal model that casts these predictions\ninto labelled chord segments. However, temporal models have been shown to only\nsmooth predictions, without being able to incorporate musical information about\nchord progressions. Recent research discovered that it might be the low\nhierarchical level such models have been applied to (directly on audio frames)\nwhich prevents learning musical relationships, even for expressive models such\nas recurrent neural networks (RNNs). However, if applied on the level of chord\nsequences, RNNs indeed can become powerful chord predictors. In this paper, we\ndisentangle temporal models into a harmonic language model---to be applied on\nchord sequences---and a chord duration model that connects the chord-level\npredictions of the language model to the frame-level predictions of the\nacoustic model. In our experiments, we explore the impact of each model on the\nchord recognition score, and show that using harmonic language and duration\nmodels improves the results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 03:34:27 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Korzeniowski", "Filip", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1808.05340", "submitter": "Filip Korzeniowski", "authors": "Filip Korzeniowski and Gerhard Widmer", "title": "Genre-Agnostic Key Classification With Convolutional Neural Networks", "comments": "Published at the 19th International Society for Music Information\n  Retrieval Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose modifications to the model structure and training procedure to a\nrecently introduced Convolutional Neural Network for musical key\nclassification. These modifications enable the network to learn a\ngenre-independent model that performs better than models trained for specific\nmusic styles, which has not been the case in existing work. We analyse this\ngeneralisation capability on three datasets comprising distinct genres. We then\nevaluate the model on a number of unseen data sets, and show its superior\nperformance compared to the state of the art. Finally, we investigate the\nmodel's performance on short excerpts of audio. From these experiments, we\nconclude that models need to consider the harmonic coherence of the whole piece\nwhen classifying the local key of short segments of audio.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 03:57:03 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Korzeniowski", "Filip", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1808.05341", "submitter": "Filip Korzeniowski", "authors": "Filip Korzeniowski and Gerhard Widmer", "title": "Automatic Chord Recognition with Higher-Order Harmonic Language\n  Modelling", "comments": "First published in the Proceedings of the 26th European Signal\n  Processing Conference (EUSIPCO-2018) in 2018, published by EURASIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common temporal models for automatic chord recognition model chord changes on\na frame-wise basis. Due to this fact, they are unable to capture musical\nknowledge about chord progressions. In this paper, we propose a temporal model\nthat enables explicit modelling of chord changes and durations. We then apply\nN-gram models and a neural-network-based acoustic model within this framework,\nand evaluate the effect of model overconfidence. Our results show that model\noverconfidence plays only a minor role (but target smoothing still improves the\nacoustic model), and that stronger chord language models do improve recognition\nresults, however their effects are small compared to other domains.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 04:10:02 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Korzeniowski", "Filip", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1808.05347", "submitter": "Junlin Zhou", "authors": "Guang Li, Xin Yang, Duanbing Chen, Anxing Song, Yuke Fang, Junlin Zhou", "title": "Tool Breakage Detection using Deep Learning", "comments": "6 pages,BCD2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In manufacture, steel and other metals are mainly cut and shaped during the\nfabrication process by computer numerical control (CNC) machines. To keep high\nproductivity and efficiency of the fabrication process, engineers need to\nmonitor the real-time process of CNC machines, and the lifetime management of\nmachine tools. In a real manufacturing process, breakage of machine tools\nusually happens without any indication, this problem seriously affects the\nfabrication process for many years. Previous studies suggested many different\napproaches for monitoring and detecting the breakage of machine tools. However,\nthere still exists a big gap between academic experiments and the complex real\nfabrication processes such as the high demands of real-time detections, the\ndifficulty in data acquisition and transmission. In this work, we use the\nspindle current approach to detect the breakage of machine tools, which has the\nhigh performance of real-time monitoring, low cost, and easy to install. We\nanalyze the features of the current of a milling machine spindle through tools\nwearing processes, and then we predict the status of tool breakage by a\nconvolutional neural network(CNN). In addition, we use a BP neural network to\nunderstand the reliability of the CNN. The results show that our CNN approach\ncan detect tool breakage with an accuracy of 93%, while the best performance of\nBP is 80%.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 04:45:15 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Li", "Guang", ""], ["Yang", "Xin", ""], ["Chen", "Duanbing", ""], ["Song", "Anxing", ""], ["Fang", "Yuke", ""], ["Zhou", "Junlin", ""]]}, {"id": "1808.05353", "submitter": "Anurag Dwarakanath", "authors": "Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M. Rao, R.\n  P. Jagadeesh Chandra Bose, Neville Dubash, and Sanjay Podder", "title": "Identifying Implementation Bugs in Machine Learning based Image\n  Classifiers using Metamorphic Testing", "comments": "Published at 27th ACM SIGSOFT International Symposium on Software\n  Testing and Analysis (ISSTA 2018)", "journal-ref": null, "doi": "10.1145/3213846.3213858", "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently witnessed tremendous success of Machine Learning (ML) in\npractical applications. Computer vision, speech recognition and language\ntranslation have all seen a near human level performance. We expect, in the\nnear future, most business applications will have some form of ML. However,\ntesting such applications is extremely challenging and would be very expensive\nif we follow today's methodologies. In this work, we present an articulation of\nthe challenges in testing ML based applications. We then present our solution\napproach, based on the concept of Metamorphic Testing, which aims to identify\nimplementation bugs in ML based image classifiers. We have developed\nmetamorphic relations for an application based on Support Vector Machine and a\nDeep Learning based application. Empirical validation showed that our approach\nwas able to catch 71% of the implementation bugs in the ML applications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 06:13:26 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Dwarakanath", "Anurag", ""], ["Ahuja", "Manish", ""], ["Sikand", "Samarth", ""], ["Rao", "Raghotham M.", ""], ["Bose", "R. P. Jagadeesh Chandra", ""], ["Dubash", "Neville", ""], ["Podder", "Sanjay", ""]]}, {"id": "1808.05355", "submitter": "Behrang Mehrparvar", "authors": "Behrang Mehrparvar and Ricardo Vilalta", "title": "Conceptual Domain Adaptation Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently been shown to be instrumental in the problem of\ndomain adaptation, where the goal is to learn a model on a target domain using\na similar --but not identical-- source domain. The rationale for coupling both\ntechniques is the possibility of extracting common concepts across domains.\nConsidering (strictly) local representations, traditional deep learning assumes\ncommon concepts must be captured in the same hidden units. We contend that\njointly training a model with source and target data using a single deep\nnetwork is prone to failure when there is inherently lower-level\nrepresentational discrepancy between the two domains; such discrepancy leads to\na misalignment of corresponding concepts in separate hidden units. We introduce\na search framework to correctly align high-level representations when training\ndeep networks; such framework leads to the notion of conceptual --as opposed to\nrepresentational-- domain adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 06:25:24 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Mehrparvar", "Behrang", ""], ["Vilalta", "Ricardo", ""]]}, {"id": "1808.05377", "submitter": "Thomas Elsken", "authors": "Thomas Elsken, Jan Hendrik Metzen, Frank Hutter", "title": "Neural Architecture Search: A Survey", "comments": null, "journal-ref": "Journal of Machine Learning Research 20 (2019) 1-21", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has enabled remarkable progress over the last years on a\nvariety of tasks, such as image recognition, speech recognition, and machine\ntranslation. One crucial aspect for this progress are novel neural\narchitectures. Currently employed architectures have mostly been developed\nmanually by human experts, which is a time-consuming and error-prone process.\nBecause of this, there is growing interest in automated neural architecture\nsearch methods. We provide an overview of existing work in this field of\nresearch and categorize them according to three dimensions: search space,\nsearch strategy, and performance estimation strategy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 08:45:01 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 13:49:26 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2019 09:50:47 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Elsken", "Thomas", ""], ["Metzen", "Jan Hendrik", ""], ["Hutter", "Frank", ""]]}, {"id": "1808.05385", "submitter": "Yu Li", "authors": "Yu Li, Lizhong Ding, Xin Gao", "title": "On the Decision Boundary of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning models and techniques have achieved great empirical\nsuccess, our understanding of the source of success in many aspects remains\nvery limited. In an attempt to bridge the gap, we investigate the decision\nboundary of a production deep learning architecture with weak assumptions on\nboth the training data and the model. We demonstrate, both theoretically and\nempirically, that the last weight layer of a neural network converges to a\nlinear SVM trained on the output of the last hidden layer, for both the binary\ncase and the multi-class case with the commonly used cross-entropy loss.\nFurthermore, we show empirically that training a neural network as a whole,\ninstead of only fine-tuning the last weight layer, may result in better bias\nconstant for the last weight layer, which is important for generalization. In\naddition to facilitating the understanding of deep learning, our result can be\nhelpful for solving a broad range of practical problems of deep learning, such\nas catastrophic forgetting and adversarial attacking. The experiment codes are\navailable at https://github.com/lykaust15/NN_decision_boundary\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 09:25:50 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 06:23:32 GMT"}, {"version": "v3", "created": "Tue, 1 Jan 2019 08:20:22 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Li", "Yu", ""], ["Ding", "Lizhong", ""], ["Gao", "Xin", ""]]}, {"id": "1808.05403", "submitter": "Fei Wen", "authors": "Fei Wen, Lei Chu, Peilin Liu, Robert C. Qiu", "title": "A Survey on Nonconvex Regularization Based Sparse and Low-Rank Recovery\n  in Signal Processing, Statistics, and Machine Learning", "comments": "22 pages", "journal-ref": "Published in IEEE Access 2018:\n  https://ieeexplore.ieee.org/abstract/document/8531588", "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, sparse and low-rank recovery have drawn much attention in\nmany areas such as signal/image processing, statistics, bioinformatics and\nmachine learning. To achieve sparsity and/or low-rankness inducing, the\n$\\ell_1$ norm and nuclear norm are of the most popular regularization penalties\ndue to their convexity. While the $\\ell_1$ and nuclear norm are convenient as\nthe related convex optimization problems are usually tractable, it has been\nshown in many applications that a nonconvex penalty can yield significantly\nbetter performance. In recent, nonconvex regularization based sparse and\nlow-rank recovery is of considerable interest and it in fact is a main driver\nof the recent progress in nonconvex and nonsmooth optimization. This paper\ngives an overview of this topic in various fields in signal processing,\nstatistics and machine learning, including compressive sensing (CS), sparse\nregression and variable selection, sparse signals separation, sparse principal\ncomponent analysis (PCA), large covariance and inverse covariance matrices\nestimation, matrix completion, and robust PCA. We present recent developments\nof nonconvex regularization based sparse and low-rank recovery in these fields,\naddressing the issues of penalty selection, applications and the convergence of\nnonconvex algorithms. Code is available at https://github.com/FWen/ncreg.git.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 10:25:48 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 08:04:17 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 04:20:45 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wen", "Fei", ""], ["Chu", "Lei", ""], ["Liu", "Peilin", ""], ["Qiu", "Robert C.", ""]]}, {"id": "1808.05443", "submitter": "Christofer Fellicious", "authors": "Christofer Fellicious", "title": "Transfer Learning and Organic Computing for Autonomous Vehicles", "comments": "5 pages, 2 figures, survey of papers and methods in transfer\n  learning, organic computing and online transfer learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous Vehicles(AV) are one of the brightest promises of the future which\nwould help cut down fatalities and improve travel time while working in\nharmony. Autonomous vehicles will face with challenging situations and\nexperiences not seen before. These experiences should be converted to knowledge\nand help the vehicle prepare better in the future. Online Transfer Learning\nwill help transferring prior knowledge to a new task and also keep the\nknowledge updated as the task evolves. This paper presents the different\nmethods of transfer learning, online transfer learning and organic computing\nthat could be adapted to the domain of autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 12:28:11 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Fellicious", "Christofer", ""]]}, {"id": "1808.05464", "submitter": "Dongrui Wu", "authors": "He He and Dongrui Wu", "title": "Transfer Learning for Brain-Computer Interfaces: A Euclidean Space Data\n  Alignment Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This paper targets a major challenge in developing practical\nEEG-based brain-computer interfaces (BCIs): how to cope with individual\ndifferences so that better learning performance can be obtained for a new\nsubject, with minimum or even no subject-specific data? Methods: We propose a\nnovel approach to align EEG trials from different subjects in the Euclidean\nspace to make them more similar, and hence improve the learning performance for\na new subject. Our approach has three desirable properties: 1) it aligns the\nEEG trials directly in the Euclidean space, and any signal processing, feature\nextraction and machine learning algorithms can then be applied to the aligned\ntrials; 2) its computational cost is very low; and, 3) it is unsupervised and\ndoes not need any label information from the new subject. Results: Both offline\nand simulated online experiments on motor imagery classification and\nevent-related potential classification verified that our proposed approach\noutperformed a state-of-the-art Riemannian space data alignment approach, and\nseveral approaches without data alignment. Conclusion: The proposed Euclidean\nspace EEG data alignment approach can greatly facilitate transfer learning in\nBCIs. Significance: Our proposed approach is effective, efficient, and easy to\nimplement. It could be an essential pre-processing step for EEG-based BCIs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 23:06:43 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 08:36:27 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["He", "He", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.05480", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey and Himanshu Jhamb", "title": "A novel Empirical Bayes with Reversible Jump Markov Chain in User-Movie\n  Recommendation system", "comments": "arXiv admin note: text overlap with arXiv:1707.02294", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we select the unknown dimension of the feature by re-\nversible jump MCMC inside a simulated annealing in bayesian set up of\ncollaborative filter. We implement the same in MovieLens small dataset. We also\ntune the hyper parameter by using a modified empirical bayes. It can also be\nused to guess an initial choice for hyper-parameters in grid search procedure\neven for the datasets where MCMC oscillates around the true value or takes long\ntime to converge.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 12:59:14 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Jhamb", "Himanshu", ""]]}, {"id": "1808.05500", "submitter": "Mostafa Mehdipour Ghazi", "authors": "Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, M. Jorge Cardoso,\n  Marc Modat, Sebastien Ourselin, Lauge S{\\o}rensen", "title": "Robust training of recurrent neural networks to handle missing data for\n  disease progression modeling", "comments": "9 pages, 1 figure, MIDL conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease progression modeling (DPM) using longitudinal data is a challenging\ntask in machine learning for healthcare that can provide clinicians with better\ntools for diagnosis and monitoring of disease. Existing DPM algorithms neglect\ntemporal dependencies among measurements and make parametric assumptions about\nbiomarker trajectories. In addition, they do not model multiple biomarkers\njointly and need to align subjects' trajectories. In this paper, recurrent\nneural networks (RNNs) are utilized to address these issues. However, in many\ncases, longitudinal cohorts contain incomplete data, which hinders the\napplication of standard RNNs and requires a pre-processing step such as\nimputation of the missing values. We, therefore, propose a generalized training\nrule for the most widely used RNN architecture, long short-term memory (LSTM)\nnetworks, that can handle missing values in both target and predictor\nvariables. This algorithm is applied for modeling the progression of\nAlzheimer's disease (AD) using magnetic resonance imaging (MRI) biomarkers. The\nresults show that the proposed LSTM algorithm achieves a lower mean absolute\nerror for prediction of measurements across all considered MRI biomarkers\ncompared to using standard LSTM networks with data imputation or using a\nregression-based DPM method. Moreover, applying linear discriminant analysis to\nthe biomarkers' values predicted by the proposed algorithm results in a larger\narea under the receiver operating characteristic curve (AUC) for clinical\ndiagnosis of AD compared to the same alternatives, and the AUC is comparable to\nstate-of-the-art AUCs from a recent cross-sectional medical image\nclassification challenge. This paper shows that built-in handling of missing\nvalues in LSTM network training paves the way for application of RNNs in\ndisease progression modeling.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:09:22 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["Nielsen", "Mads", ""], ["Pai", "Akshay", ""], ["Cardoso", "M. Jorge", ""], ["Modat", "Marc", ""], ["Ourselin", "Sebastien", ""], ["S\u00f8rensen", "Lauge", ""]]}, {"id": "1808.05517", "submitter": "Jianbo Guo", "authors": "Jianbo Guo, Yuxi Li, Weiyao Lin, Yurong Chen, Jianguo Li", "title": "Network Decoupling: From Regular to Depthwise Separable Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depthwise separable convolution has shown great efficiency in network design,\nbut requires time-consuming training procedure with full training-set\navailable. This paper first analyzes the mathematical relationship between\nregular convolutions and depthwise separable convolutions, and proves that the\nformer one could be approximated with the latter one in closed form. We show\ndepthwise separable convolutions are principal components of regular\nconvolutions. And then we propose network decoupling (ND), a training-free\nmethod to accelerate convolutional neural networks (CNNs) by transferring\npre-trained CNN models into the MobileNet-like depthwise separable convolution\nstructure, with a promising speedup yet negligible accuracy loss. We further\nverify through experiments that the proposed method is orthogonal to other\ntraining-free methods like channel decomposition, spatial decomposition, etc.\nCombining the proposed method with them will bring even larger CNN speedup. For\ninstance, ND itself achieves about 2X speedup for the widely used VGG16, and\ncombined with other methods, it reaches 3.7X speedup with graceful accuracy\ndegradation. We demonstrate that ND is widely applicable to classification\nnetworks like ResNet, and object detection network like SSD300.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:39:10 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Guo", "Jianbo", ""], ["Li", "Yuxi", ""], ["Lin", "Weiyao", ""], ["Chen", "Yurong", ""], ["Li", "Jianguo", ""]]}, {"id": "1808.05527", "submitter": "Vadim Sokolov", "authors": "Michael Polson and Vadim Sokolov", "title": "Deep Learning for Energy Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is applied to energy markets to predict extreme loads observed\nin energy grids. Forecasting energy loads and prices is challenging due to\nsharp peaks and troughs that arise due to supply and demand fluctuations from\nintraday system constraints. We propose deep spatio-temporal models and extreme\nvalue theory (EVT) to capture theses effects and in particular the tail\nbehavior of load spikes. Deep LSTM architectures with ReLU and $\\tanh$\nactivation functions can model trends and temporal dependencies while EVT\ncaptures highly volatile load spikes above a pre-specified threshold. To\nillustrate our methodology, we use hourly price and demand data from 4719 nodes\nof the PJM interconnection, and we construct a deep predictor. We show that\nDL-EVT outperforms traditional Fourier time series methods, both in-and\nout-of-sample, by capturing the observed nonlinearities in prices. Finally, we\nconclude with directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 15:01:01 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 12:50:41 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 14:50:14 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Polson", "Michael", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1808.05535", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues, Ioulia Markou, Francisco Pereira", "title": "Combining time-series and textual data for taxi demand prediction in\n  event areas: a deep learning approach", "comments": "20 pages, 6 figures", "journal-ref": "Rodrigues, F., Markou, I., Pereira, F. Combining time-series and\n  textual data for taxi demand prediction in event areas: a deep learning\n  approach. In Information Fusion, Elsevier, 2018", "doi": "10.1016/j.inffus.2018.07.007", "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate time-series forecasting is vital for numerous areas of application\nsuch as transportation, energy, finance, economics, etc. However, while modern\ntechniques are able to explore large sets of temporal data to build forecasting\nmodels, they typically neglect valuable information that is often available\nunder the form of unstructured text. Although this data is in a radically\ndifferent format, it often contains contextual explanations for many of the\npatterns that are observed in the temporal data. In this paper, we propose two\ndeep learning architectures that leverage word embeddings, convolutional layers\nand attention mechanisms for combining text information with time-series data.\nWe apply these approaches for the problem of taxi demand forecasting in event\nareas. Using publicly available taxi data from New York, we empirically show\nthat by fusing these two complementary cross-modal sources of information, the\nproposed models are able to significantly reduce the error in the forecasts.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 15:19:34 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Markou", "Ioulia", ""], ["Pereira", "Francisco", ""]]}, {"id": "1808.05537", "submitter": "Tianhang Zheng", "authors": "Tianhang Zheng, Changyou Chen, Kui Ren", "title": "Distributionally Adversarial Attack", "comments": "accepted to AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on adversarial attack has shown that Projected Gradient Descent\n(PGD) Adversary is a universal first-order adversary, and the classifier\nadversarially trained by PGD is robust against a wide range of first-order\nattacks. It is worth noting that the original objective of an attack/defense\nmodel relies on a data distribution $p(\\mathbf{x})$, typically in the form of\nrisk maximization/minimization, e.g.,\n$\\max/\\min\\mathbb{E}_{p(\\mathbf(x))}\\mathcal{L}(\\mathbf{x})$ with\n$p(\\mathbf{x})$ some unknown data distribution and $\\mathcal{L}(\\cdot)$ a loss\nfunction. However, since PGD generates attack samples independently for each\ndata sample based on $\\mathcal{L}(\\cdot)$, the procedure does not necessarily\nlead to good generalization in terms of risk optimization. In this paper, we\nachieve the goal by proposing distributionally adversarial attack (DAA), a\nframework to solve an optimal {\\em adversarial-data distribution}, a perturbed\ndistribution that satisfies the $L_\\infty$ constraint but deviates from the\noriginal data distribution to increase the generalization risk maximally.\nAlgorithmically, DAA performs optimization on the space of potential data\ndistributions, which introduces direct dependency between all data points when\ngenerating adversarial samples. DAA is evaluated by attacking state-of-the-art\ndefense models, including the adversarially-trained models provided by {\\em MIT\nMadryLab}. Notably, DAA ranks {\\em the first place} on MadryLab's white-box\nleaderboards, reducing the accuracy of their secret MNIST model to $88.79\\%$\n(with $l_\\infty$ perturbations of $\\epsilon = 0.3$) and the accuracy of their\nsecret CIFAR model to $44.71\\%$ (with $l_\\infty$ perturbations of $\\epsilon =\n8.0$). Code for the experiments is released on\n\\url{https://github.com/tianzheng4/Distributionally-Adversarial-Attack}.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 15:20:55 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 20:16:27 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 01:51:34 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Zheng", "Tianhang", ""], ["Chen", "Changyou", ""], ["Ren", "Kui", ""]]}, {"id": "1808.05563", "submitter": "Matthias Bauer", "authors": "Mark van der Wilk, Matthias Bauer, ST John, James Hensman", "title": "Learning Invariances using the Marginal Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalising well in supervised learning tasks relies on correctly\nextrapolating the training data to a large region of the input space. One way\nto achieve this is to constrain the predictions to be invariant to\ntransformations on the input that are known to be irrelevant (e.g.\ntranslation). Commonly, this is done through data augmentation, where the\ntraining set is enlarged by applying hand-crafted transformations to the\ninputs. We argue that invariances should instead be incorporated in the model\nstructure, and learned using the marginal likelihood, which correctly rewards\nthe reduced complexity of invariant models. We demonstrate this for Gaussian\nprocess models, due to the ease with which their marginal likelihood can be\nestimated. Our main contribution is a variational inference scheme for Gaussian\nprocesses containing invariances described by a sampling procedure. We learn\nthe sampling procedure by back-propagating through it to maximise the marginal\nlikelihood.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 16:12:39 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["van der Wilk", "Mark", ""], ["Bauer", "Matthias", ""], ["John", "ST", ""], ["Hensman", "James", ""]]}, {"id": "1808.05577", "submitter": "Stefano B. Blumberg", "authors": "Stefano B. Blumberg, Ryutaro Tanno, Iasonas Kokkinos, Daniel C.\n  Alexander", "title": "Deeper Image Quality Transfer: Training Low-Memory Neural Networks for\n  3D Images", "comments": "Accepted in: MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the memory demands that come with the processing of\n3-dimensional, high-resolution, multi-channeled medical images in deep\nlearning. We exploit memory-efficient backpropagation techniques, to reduce the\nmemory complexity of network training from being linear in the network's depth,\nto being roughly constant $ - $ permitting us to elongate deep architectures\nwith negligible memory increase. We evaluate our methodology in the paradigm of\nImage Quality Transfer, whilst noting its potential application to various\ntasks that use deep learning. We study the impact of depth on accuracy and show\nthat deeper models have more predictive power, which may exploit larger\ntraining sets. We obtain substantially better results than the previous\nstate-of-the-art model with a slight memory increase, reducing the\nroot-mean-squared-error by $ 13\\% $. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 16:42:10 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Blumberg", "Stefano B.", ""], ["Tanno", "Ryutaro", ""], ["Kokkinos", "Iasonas", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "1808.05578", "submitter": "Guillaume Chevalier", "authors": "Guillaume Chevalier", "title": "LARNN: Linear Attention Recurrent Neural Network", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Linear Attention Recurrent Neural Network (LARNN) is a recurrent\nattention module derived from the Long Short-Term Memory (LSTM) cell and ideas\nfrom the consciousness Recurrent Neural Network (RNN). Yes, it LARNNs. The\nLARNN uses attention on its past cell state values for a limited window size\n$k$. The formulas are also derived from the Batch Normalized LSTM (BN-LSTM)\ncell and the Transformer Network for its Multi-Head Attention Mechanism. The\nMulti-Head Attention Mechanism is used inside the cell such that it can query\nits own $k$ past values with the attention window. This has the effect of\naugmenting the rank of the tensor with the attention mechanism, such that the\ncell can perform complex queries to question its previous inner memories, which\nshould augment the long short-term effect of the memory. With a clever trick,\nthe LARNN cell with attention can be easily used inside a loop on the cell\nstate, just like how any other Recurrent Neural Network (RNN) cell can be\nlooped linearly through time series. This is due to the fact that its state,\nwhich is looped upon throughout time steps within time series, stores the inner\nstates in a \"first in, first out\" queue which contains the $k$ most recent\nstates and on which it is easily possible to add static positional encoding\nwhen the queue is represented as a tensor. This neural architecture yields\nbetter results than the vanilla LSTM cells. It can obtain results of 91.92% for\nthe test accuracy, compared to the previously attained 91.65% using vanilla\nLSTM cells. Note that this is not to compare to other research, where up to\n93.35% is obtained, but costly using 18 LSTM cells rather than with 2 to 3\ncells as analyzed here. Finally, an interesting discovery is made, such that\nadding activation within the multi-head attention mechanism's linear layers can\nyield better results in the context researched hereto.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 16:48:56 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Chevalier", "Guillaume", ""]]}, {"id": "1808.05584", "submitter": "Zhao Zhong", "authors": "Zhao Zhong, Zichen Yang, Boyang Deng, Junjie Yan, Wei Wu, Jing Shao,\n  Cheng-Lin Liu", "title": "BlockQNN: Efficient Block-wise Neural Network Architecture Generation", "comments": "14 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have gained a remarkable success in computer\nvision. However, most usable network architectures are hand-crafted and usually\nrequire expertise and elaborate design. In this paper, we provide a block-wise\nnetwork generation pipeline called BlockQNN which automatically builds\nhigh-performance networks using the Q-Learning paradigm with epsilon-greedy\nexploration strategy. The optimal network block is constructed by the learning\nagent which is trained to choose component layers sequentially. We stack the\nblock to construct the whole auto-generated network. To accelerate the\ngeneration process, we also propose a distributed asynchronous framework and an\nearly stop strategy. The block-wise generation brings unique advantages: (1) it\nyields state-of-the-art results in comparison to the hand-crafted networks on\nimage classification, particularly, the best network generated by BlockQNN\nachieves 2.35% top-1 error rate on CIFAR-10. (2) it offers tremendous reduction\nof the search space in designing networks, spending only 3 days with 32 GPUs. A\nfaster version can yield a comparable result with only 1 GPU in 20 hours. (3)\nit has strong generalizability in that the network built on CIFAR also performs\nwell on the larger-scale dataset. The best network achieves very competitive\naccuracy of 82.0% top-1 and 96.0% top-5 on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 17:02:24 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Zhong", "Zhao", ""], ["Yang", "Zichen", ""], ["Deng", "Boyang", ""], ["Yan", "Junjie", ""], ["Wu", "Wei", ""], ["Shao", "Jing", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1808.05587", "submitter": "Adri\\`a Garriga-Alonso", "authors": "Adri\\`a Garriga-Alonso, Carl Edward Rasmussen, Laurence Aitchison", "title": "Deep Convolutional Networks as shallow Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the output of a (residual) convolutional neural network (CNN)\nwith an appropriate prior over the weights and biases is a Gaussian process\n(GP) in the limit of infinitely many convolutional filters, extending similar\nresults for dense networks. For a CNN, the equivalent kernel can be computed\nexactly and, unlike \"deep kernels\", has very few parameters: only the\nhyperparameters of the original CNN. Further, we show that this kernel has two\nproperties that allow it to be computed efficiently; the cost of evaluating the\nkernel for a pair of images is similar to a single forward pass through the\noriginal CNN with only one filter per layer. The kernel equivalent to a\n32-layer ResNet obtains 0.84% classification error on MNIST, a new record for\nGPs with a comparable number of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 17:20:58 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 00:59:06 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Garriga-Alonso", "Adri\u00e0", ""], ["Rasmussen", "Carl Edward", ""], ["Aitchison", "Laurence", ""]]}, {"id": "1808.05671", "submitter": "Quanquan Gu", "authors": "Dongruo Zhou and Jinghui Chen and Yuan Cao and Yiqi Tang and Ziyan\n  Yang and Quanquan Gu", "title": "On the Convergence of Adaptive Gradient Methods for Nonconvex\n  Optimization", "comments": "30 pages. This version improves the presentation and adds high\n  probability convergence results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive gradient methods are workhorses in deep learning. However, the\nconvergence guarantees of adaptive gradient methods for nonconvex optimization\nhave not been thoroughly studied. In this paper, we provide a fine-grained\nconvergence analysis for a general class of adaptive gradient methods including\nAMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that\nadaptive gradient methods in expectation converge to a first-order stationary\npoint. Our convergence rate is better than existing results for adaptive\ngradient methods in terms of dimension, and is strictly faster than stochastic\ngradient decent (SGD) when the stochastic gradients are sparse. To the best of\nour knowledge, this is the first result showing the advantage of adaptive\ngradient methods over SGD in nonconvex setting. In addition, we also prove high\nprobability bounds on the convergence rates of AMSGrad, RMSProp as well as\nAdaGrad, which have not been established before. Our analyses shed light on\nbetter understanding the mechanism behind adaptive gradient methods in\noptimizing nonconvex objectives.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 20:25:28 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 05:03:46 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 17:52:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhou", "Dongruo", ""], ["Chen", "Jinghui", ""], ["Cao", "Yuan", ""], ["Tang", "Yiqi", ""], ["Yang", "Ziyan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1808.05689", "submitter": "Yunsheng Bai", "authors": "Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, Wei Wang", "title": "SimGNN: A Neural Network Approach to Fast Graph Similarity Computation", "comments": "WSDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph similarity search is among the most important graph-based applications,\ne.g. finding the chemical compounds that are most similar to a query compound.\nGraph similarity computation, such as Graph Edit Distance (GED) and Maximum\nCommon Subgraph (MCS), is the core operation of graph similarity search and\nmany other applications, but very costly to compute in practice. Inspired by\nthe recent success of neural network approaches to several graph applications,\nsuch as node or graph classification, we propose a novel neural network based\napproach to address this classic yet challenging graph problem, aiming to\nalleviate the computational burden while preserving a good performance.\n  The proposed approach, called SimGNN, combines two strategies. First, we\ndesign a learnable embedding function that maps every graph into a vector,\nwhich provides a global summary of a graph. A novel attention mechanism is\nproposed to emphasize the important nodes with respect to a specific similarity\nmetric. Second, we design a pairwise node comparison method to supplement the\ngraph-level embeddings with fine-grained node-level information. Our model\nachieves better generalization on unseen graphs, and in the worst case runs in\nquadratic time with respect to the number of nodes in two graphs. Taking GED\ncomputation as an example, experimental results on three real graph datasets\ndemonstrate the effectiveness and efficiency of our approach. Specifically, our\nmodel achieves smaller error rate and great time reduction compared against a\nseries of baselines, including several approximation algorithms on GED\ncomputation, and many existing graph neural network based models. To the best\nof our knowledge, we are among the first to adopt neural networks to explicitly\nmodel the similarity between two graphs, and provide a new direction for future\nresearch on graph similarity computation and graph similarity search.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 22:02:15 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 18:19:23 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 17:25:13 GMT"}, {"version": "v4", "created": "Sun, 1 Mar 2020 19:10:52 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Bai", "Yunsheng", ""], ["Ding", "Hao", ""], ["Bian", "Song", ""], ["Chen", "Ting", ""], ["Sun", "Yizhou", ""], ["Wang", "Wei", ""]]}, {"id": "1808.05697", "submitter": "Aditya Siddhant", "authors": "Aditya Siddhant, Zachary C. Lipton", "title": "Deep Bayesian Active Learning for Natural Language Processing: Results\n  of a Large-Scale Empirical Study", "comments": "To be presented at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent papers investigate Active Learning (AL) for mitigating the\ndata dependence of deep learning for natural language processing. However, the\napplicability of AL to real-world problems remains an open question. While in\nsupervised learning, practitioners can try many different methods, evaluating\neach against a validation set before selecting a model, AL affords no such\nluxury. Over the course of one AL run, an agent annotates its dataset\nexhausting its labeling budget. Thus, given a new task, an active learner has\nno opportunity to compare models and acquisition functions. This paper provides\na large scale empirical study of deep active learning, addressing multiple\ntasks and, for each, multiple datasets, multiple models, and a full suite of\nacquisition functions. We find that across all settings, Bayesian active\nlearning by disagreement, using uncertainty estimates provided either by\nDropout or Bayes-by Backprop significantly improves over i.i.d. baselines and\nusually outperforms classic uncertainty sampling.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 22:46:40 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 01:26:33 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 17:25:51 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Siddhant", "Aditya", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1808.05726", "submitter": "Rodrigo Collazo", "authors": "Rodrigo A. Collazo and Jim Q. Smith", "title": "An N Time-Slice Dynamic Chain Event Graph", "comments": "52 pages, 14 figures, revised Definition 10, added Lemmas 1 and 2,\n  corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dynamic Chain Event Graph (DCEG) is able to depict many classes of\ndiscrete random processes exhibiting asymmetries in their developments and\ncontext-specific conditional probabilities structures. However, paradoxically,\nthis very generality has so far frustrated its wide application. So in this\npaper we develop an object-oriented method to fully analyse a particularly\nuseful and feasibly implementable new subclass of these graphical models called\nthe N Time-Slice DCEG (NT-DCEG). After demonstrating a close relationship\nbetween an NT-DCEG and a specific class of Markov processes, we discuss how\ngraphical modellers can exploit this connection to gain a deep understanding of\ntheir processes. We also show how to read from the topology of this graph\ncontext-specific independence statements that can then be checked by domain\nexperts. Our methods are illustrated throughout using examples of dynamic\nmultivariate processes describing inmate radicalisation in a prison.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 01:59:57 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 13:41:07 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Collazo", "Rodrigo A.", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1808.05731", "submitter": "Ankur Moitra", "authors": "Allen Liu and Ankur Moitra", "title": "Efficiently Learning Mixtures of Mallows Models", "comments": "35 pages", "journal-ref": "FOCS 2018", "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of Mallows models are a popular generative model for ranking data\ncoming from a heterogeneous population. They have a variety of applications\nincluding social choice, recommendation systems and natural language\nprocessing. Here we give the first polynomial time algorithm for provably\nlearning the parameters of a mixture of Mallows models with any constant number\nof components. Prior to our work, only the two component case had been settled.\nOur analysis revolves around a determinantal identity of Zagier which was\nproven in the context of mathematical physics, which we use to show polynomial\nidentifiability and ultimately to construct test functions to peel off one\ncomponent at a time.\n  To complement our upper bounds, we show information-theoretic lower bounds on\nthe sample complexity as well as lower bounds against restricted families of\nalgorithms that make only local queries. Together, these results demonstrate\nvarious impediments to improving the dependence on the number of components.\nThey also motivate the study of learning mixtures of Mallows models from the\nperspective of beyond worst-case analysis. In this direction, we show that when\nthe scaling parameters of the Mallows models have separation, there are much\nfaster learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 02:24:23 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Liu", "Allen", ""], ["Moitra", "Ankur", ""]]}, {"id": "1808.05760", "submitter": "Yuzhe Ma", "authors": "Yuzhe Ma, Kwang-Sung Jun, Lihong Li, Xiaojin Zhu", "title": "Data Poisoning Attacks in Contextual Bandits", "comments": "GameSec 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study offline data poisoning attacks in contextual bandits, a class of\nreinforcement learning problems with important applications in online\nrecommendation and adaptive medical treatment, among others. We provide a\ngeneral attack framework based on convex optimization and show that by slightly\nmanipulating rewards in the data, an attacker can force the bandit algorithm to\npull a target arm for a target contextual vector. The target arm and target\ncontextual vector are both chosen by the attacker. That is, the attacker can\nhijack the behavior of a contextual bandit. We also investigate the feasibility\nand the side effects of such attacks, and identify future directions for\ndefense. Experiments on both synthetic and real-world data demonstrate the\nefficiency of the attack algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 05:25:29 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 03:26:42 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Ma", "Yuzhe", ""], ["Jun", "Kwang-Sung", ""], ["Li", "Lihong", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1808.05770", "submitter": "Yi Han", "authors": "Yi Han, Benjamin I.P. Rubinstein, Tamas Abraham, Tansu Alpcan, Olivier\n  De Vel, Sarah Erfani, David Hubczenko, Christopher Leckie, Paul Montague", "title": "Reinforcement Learning for Autonomous Defence in Software-Defined\n  Networking", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the successful application of machine learning (ML) in a wide range\nof domains, adaptability---the very property that makes machine learning\ndesirable---can be exploited by adversaries to contaminate training and evade\nclassification. In this paper, we investigate the feasibility of applying a\nspecific class of machine learning algorithms, namely, reinforcement learning\n(RL) algorithms, for autonomous cyber defence in software-defined networking\n(SDN). In particular, we focus on how an RL agent reacts towards different\nforms of causative attacks that poison its training process, including\nindiscriminate and targeted, white-box and black-box attacks. In addition, we\nalso study the impact of the attack timing, and explore potential\ncountermeasures such as adversarial training.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 06:34:53 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Han", "Yi", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Abraham", "Tamas", ""], ["Alpcan", "Tansu", ""], ["De Vel", "Olivier", ""], ["Erfani", "Sarah", ""], ["Hubczenko", "David", ""], ["Leckie", "Christopher", ""], ["Montague", "Paul", ""]]}, {"id": "1808.05777", "submitter": "Shayan Gharib", "authors": "Shayan Gharib, Konstantinos Drossos, Emre \\c{C}akir, Dmitriy Serdyuk,\n  and Tuomas Virtanen", "title": "Unsupervised adversarial domain adaptation for acoustic scene\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general problem in acoustic scene classification task is the mismatched\nconditions between training and testing data, which significantly reduces the\nperformance of the developed methods on classification accuracy. As a\ncountermeasure, we present the first method of unsupervised adversarial domain\nadaptation for acoustic scene classification. We employ a model pre-trained on\ndata from one set of conditions and by using data from other set of conditions,\nwe adapt the model in order that its output cannot be used for classifying the\nset of conditions that input data belong to. We use a freely available dataset\nfrom the DCASE 2018 challenge Task 1, subtask B, that contains data from\nmismatched recording devices. We consider the scenario where the annotations\nare available for the data recorded from one device, but not for the rest. Our\nresults show that with our model agnostic method we can achieve $\\sim 10\\%$\nincrease at the accuracy on an unseen and unlabeled dataset, while keeping\nalmost the same performance on the labeled dataset.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 07:25:31 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Gharib", "Shayan", ""], ["Drossos", "Konstantinos", ""], ["\u00c7akir", "Emre", ""], ["Serdyuk", "Dmitriy", ""], ["Virtanen", "Tuomas", ""]]}, {"id": "1808.05784", "submitter": "Anil Goyal", "authors": "Anil Goyal (AMA, LHC), Emilie Morvant (LHC), Pascal Germain (MODAL),\n  Massih-Reza Amini (AMA)", "title": "Multiview Boosting by Controlling the Diversity and the Accuracy of\n  View-specific Voters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a boosting based multiview learning algorithm,\nreferred to as PB-MVBoost, which iteratively learns i) weights over\nview-specific voters capturing view-specific information; and ii) weights over\nviews by optimizing a PAC-Bayes multiview C-Bound that takes into account the\naccuracy of view-specific classifiers and the diversity between the views. We\nderive a generalization bound for this strategy following the PAC-Bayes theory\nwhich is a suitable tool to deal with models expressed as weighted combination\nover a set of voters. Different experiments on three publicly available\ndatasets show the efficiency of the proposed approach with respect to\nstate-of-art models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 07:54:35 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 12:59:53 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Goyal", "Anil", "", "AMA, LHC"], ["Morvant", "Emilie", "", "LHC"], ["Germain", "Pascal", "", "MODAL"], ["Amini", "Massih-Reza", "", "AMA"]]}, {"id": "1808.05819", "submitter": "Nemanja Djuric", "authors": "Nemanja Djuric, Vladan Radosavljevic, Henggang Cui, Thi Nguyen,\n  Fang-Chieh Chou, Tsung-Han Lin, Nitin Singh, Jeff Schneider", "title": "Uncertainty-aware Short-term Motion Prediction of Traffic Actors for\n  Autonomous Driving", "comments": "Accepted for publication at IEEE Winter Conference on Applications of\n  Computer Vision (WACV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address one of the crucial aspects necessary for safe and efficient\noperations of autonomous vehicles, namely predicting future state of traffic\nactors in the autonomous vehicle's surroundings. We introduce a deep\nlearning-based approach that takes into account a current world state and\nproduces raster images of each actor's vicinity. The rasters are then used as\ninputs to deep convolutional models to infer future movement of actors while\nalso accounting for and capturing inherent uncertainty of the prediction task.\nExtensive experiments on real-world data strongly suggest benefits of the\nproposed approach. Moreover, following completion of the offline tests the\nsystem was successfully tested onboard self-driving vehicles.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 10:37:51 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 14:20:22 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 06:35:56 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Djuric", "Nemanja", ""], ["Radosavljevic", "Vladan", ""], ["Cui", "Henggang", ""], ["Nguyen", "Thi", ""], ["Chou", "Fang-Chieh", ""], ["Lin", "Tsung-Han", ""], ["Singh", "Nitin", ""], ["Schneider", "Jeff", ""]]}, {"id": "1808.05832", "submitter": "Olivier Sigaud", "authors": "Alo\\\"is Pourchot, Nicolas Perrin, Olivier Sigaud", "title": "Importance mixing: Improving sample reuse in evolutionary policy search\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neuroevolution, that is evolutionary policy search methods based on deep\nneural networks, have recently emerged as a competitor to deep reinforcement\nlearning algorithms due to their better parallelization capabilities. However,\nthese methods still suffer from a far worse sample efficiency. In this paper we\ninvestigate whether a mechanism known as \"importance mixing\" can significantly\nimprove their sample efficiency. We provide a didactic presentation of\nimportance mixing and we explain how it can be extended to reuse more samples.\nThen, from an empirical comparison based on a simple benchmark, we show that,\nthough it actually provides better sample efficiency, it is still far from the\nsample efficiency of deep reinforcement learning, though it is more stable.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 11:25:19 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Pourchot", "Alo\u00efs", ""], ["Perrin", "Nicolas", ""], ["Sigaud", "Olivier", ""]]}, {"id": "1808.05853", "submitter": "Dongrui Wu", "authors": "He He and Dongrui Wu", "title": "Transfer Learning Enhanced Common Spatial Pattern Filtering for Brain\n  Computer Interfaces (BCIs): Overview and a New Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electroencephalogram (EEG) is the most widely used input for brain\ncomputer interfaces (BCIs), and common spatial pattern (CSP) is frequently used\nto spatially filter it to increase its signal-to-noise ratio. However, CSP is a\nsupervised filter, which needs some subject-specific calibration data to\ndesign. This is time-consuming and not user-friendly. A promising approach for\nshortening or even completely eliminating this calibration session is transfer\nlearning, which leverages relevant data or knowledge from other subjects or\ntasks. This paper reviews three existing approaches for incorporating transfer\nlearning into CSP, and also proposes a new transfer learning enhanced CSP\napproach. Experiments on motor imagery classification demonstrate their\neffectiveness. Particularly, our proposed approach achieves the best\nperformance when the number of target domain calibration samples is small.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:52:12 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["He", "He", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.05854", "submitter": "Fahad Shamshad", "authors": "Fahad Shamshad, Ali Ahmed", "title": "Robust Compressive Phase Retrieval via Deep Generative Priors", "comments": "Preprint. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new framework to regularize the highly ill-posed and\nnon-linear phase retrieval problem through deep generative priors using simple\ngradient descent algorithm. We experimentally show effectiveness of proposed\nalgorithm for random Gaussian measurements (practically relevant in imaging\nthrough scattering media) and Fourier friendly measurements (relevant in\noptical set ups). We demonstrate that proposed approach achieves impressive\nresults when compared with traditional hand engineered priors including\nsparsity and denoising frameworks for number of measurements and robustness\nagainst noise. Finally, we show the effectiveness of the proposed approach on a\nreal transmission matrix dataset in an actual application of multiple\nscattering media imaging.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 13:31:30 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Shamshad", "Fahad", ""], ["Ahmed", "Ali", ""]]}, {"id": "1808.05902", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues, Mariana Louren\\c{c}o, Bernardete Ribeiro, Francisco\n  Pereira", "title": "Learning Supervised Topic Models for Classification and Regression from\n  Crowds", "comments": "14 pages", "journal-ref": "Rodrigues, F., Lourenco, M., Ribeiro, B. and Pereira, F.C., 2017.\n  Learning supervised topic models for classification and regression from\n  crowds. IEEE transactions on pattern analysis and machine intelligence,\n  39(12), pp.2409-2422", "doi": "10.1109/TPAMI.2017.2648786", "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing need to analyze large collections of documents has led to great\ndevelopments in topic modeling. Since documents are frequently associated with\nother related variables, such as labels or ratings, much interest has been\nplaced on supervised topic models. However, the nature of most annotation\ntasks, prone to ambiguity and noise, often with high volumes of documents, deem\nlearning under a single-annotator assumption unrealistic or unpractical for\nmost real-world applications. In this article, we propose two supervised topic\nmodels, one for classification and another for regression problems, which\naccount for the heterogeneity and biases among different annotators that are\nencountered in practice when learning from crowds. We develop an efficient\nstochastic variational inference algorithm that is able to scale to very large\ndatasets, and we empirically demonstrate the advantages of the proposed model\nover state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 15:32:24 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Louren\u00e7o", "Mariana", ""], ["Ribeiro", "Bernardete", ""], ["Pereira", "Francisco", ""]]}, {"id": "1808.05904", "submitter": "Samarth Gupta", "authors": "Samarth Gupta, Gauri Joshi, Osman Ya\\u{g}an", "title": "Correlated Multi-armed Bandits with a Latent Random Source", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel multi-armed bandit framework where the rewards obtained\nby pulling the arms are functions of a common latent random variable. The\ncorrelation between arms due to the common random source can be used to design\na generalized upper-confidence-bound (UCB) algorithm that identifies certain\narms as $non-competitive$, and avoids exploring them. As a result, we reduce a\n$K$-armed bandit problem to a $C+1$-armed problem, where $C+1$ includes the\nbest arm and $C$ $competitive$ arms. Our regret analysis shows that the\ncompetitive arms need to be pulled $\\mathcal{O}(\\log T)$ times, while the\nnon-competitive arms are pulled only $\\mathcal{O}(1)$ times. As a result, there\nare regimes where our algorithm achieves a $\\mathcal{O}(1)$ regret as opposed\nto the typical logarithmic regret scaling of multi-armed bandit algorithms. We\nalso evaluate lower bounds on the expected regret and prove that our\ncorrelated-UCB algorithm achieves $\\mathcal{O}(1)$ regret whenever possible.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 15:48:52 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 21:29:52 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Gupta", "Samarth", ""], ["Joshi", "Gauri", ""], ["Ya\u011fan", "Osman", ""]]}, {"id": "1808.05906", "submitter": "Bichen Shi", "authors": "Bichen Shi, Thanh-Binh Le, Neil Hurley and Georgiana Ifrim", "title": "Story Disambiguation: Tracking Evolving News Stories across News and\n  Social Streams", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following a particular news story online is an important but difficult task,\nas the relevant information is often scattered across different domains/sources\n(e.g., news articles, blogs, comments, tweets), presented in various formats\nand language styles, and may overlap with thousands of other stories. In this\nwork we join the areas of topic tracking and entity disambiguation, and propose\na framework named Story Disambiguation - a cross-domain story tracking approach\nthat builds on real-time entity disambiguation and a learning-to-rank framework\nto represent and update the rich semantic structure of news stories. Given a\ntarget news story, specified by a seed set of documents, the goal is to\neffectively select new story-relevant documents from an incoming document\nstream. We represent stories as entity graphs and we model the story tracking\nproblem as a learning-to-rank task. This enables us to track content with high\naccuracy, from multiple domains, in real-time. We study a range of text, entity\nand graph based features to understand which type of features are most\neffective for representing stories. We further propose new semi-supervised\nlearning techniques to automatically update the story representation over time.\nOur empirical study shows that we outperform the accuracy of state-of-the-art\nmethods for tracking mixed-domain document streams, while requiring fewer\nlabeled data to seed the tracked stories. This is particularly the case for\nlocal news stories that are easily over shadowed by other trending stories, and\nfor complex news stories with ambiguous content in noisy stream environments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 09:02:37 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Shi", "Bichen", ""], ["Le", "Thanh-Binh", ""], ["Hurley", "Neil", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "1808.05917", "submitter": "Roberto Barcenas", "authors": "R. B\\'arcenas, M. D. G\\'onzalez--Lima, and A. J. Quiroz", "title": "A bagging and importance sampling approach to Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An importance sampling and bagging approach to solving the support vector\nmachine (SVM) problem in the context of large databases is presented and\nevaluated. Our algorithm builds on the nearest neighbors ideas presented in\nCamelo at al. (2015). As in that reference, the goal of the present proposal is\nto achieve a faster solution of the SVM problem without a significance loss in\nthe prediction error. The performance of the methodology is evaluated in\nbenchmark examples and theoretical aspects of subsample methods are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 16:22:09 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["B\u00e1rcenas", "R.", ""], ["G\u00f3nzalez--Lima", "M. D.", ""], ["Quiroz", "A. J.", ""]]}, {"id": "1808.05924", "submitter": "Jocelyn Chi", "authors": "Jocelyn T. Chi and Ilse C. F. Ipsen", "title": "A Projector-Based Approach to Quantifying Total and Excess Uncertainties\n  for Sketched Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Linear regression is a classic method of data analysis. In recent years,\nsketching -- a method of dimension reduction using random sampling, random\nprojections, or both -- has gained popularity as an effective computational\napproximation when the number of observations greatly exceeds the number of\nvariables. In this paper, we address the following question: How does sketching\naffect the statistical properties of the solution and key quantities derived\nfrom it?\n  To answer this question, we present a projector-based approach to sketched\nlinear regression that is exact and that requires minimal assumptions on the\nsketching matrix. Therefore, downstream analyses hold exactly and generally for\nall sketching schemes. Additionally, a projector-based approach enables\nderivation of key quantities from classic linear regression that account for\nthe combined model- and algorithm-induced uncertainties. We demonstrate the\nusefulness of a projector-based approach in quantifying and enabling insight on\nexcess uncertainties and bias-variance decompositions for sketched linear\nregression. Finally, we demonstrate how the insights from our projector-based\nanalyses can be used to produce practical sketching diagnostics to aid the\ndesign of judicious sketching schemes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 16:42:09 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 04:13:26 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 14:34:07 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Chi", "Jocelyn T.", ""], ["Ipsen", "Ilse C. F.", ""]]}, {"id": "1808.05933", "submitter": "Amir Daneshmand", "authors": "Amir Daneshmand and Ying Sun and Gesualdo Scutari and Francisco\n  Facchinei and Brian M. Sadler", "title": "Decentralized Dictionary Learning Over Time-Varying Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies Dictionary Learning problems wherein the learning task is\ndistributed over a multi-agent network, modeled as a time-varying directed\ngraph. This formulation is relevant, for instance, in Big Data scenarios where\nmassive amounts of data are collected/stored in different locations (e.g.,\nsensors, clouds) and aggregating and/or processing all data in a fusion center\nmight be inefficient or unfeasible, due to resource limitations, communication\noverheads or privacy issues. We develop a unified decentralized algorithmic\nframework for this class of nonconvex problems, which is proved to converge to\nstationary solutions at a sublinear rate. The new method hinges on Successive\nConvex Approximation techniques, coupled with a decentralized tracking\nmechanism aiming at locally estimating the gradient of the smooth part of the\nsum-utility. To the best of our knowledge, this is the first provably\nconvergent decentralized algorithm for Dictionary Learning and, more generally,\nbi-convex problems over (time-varying) (di)graphs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 17:20:06 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 17:56:54 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Daneshmand", "Amir", ""], ["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""], ["Facchinei", "Francisco", ""], ["Sadler", "Brian M.", ""]]}, {"id": "1808.05965", "submitter": "Chun-Guang Li", "authors": "Chun-Guang Li, Chong You, and Ren\\'e Vidal", "title": "On Geometric Analysis of Affine Sparse Subspace Clustering", "comments": "15 pages, 6 figures, 2 tables. To appear on IEEE Journal of Selected\n  Topics in Signal Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2867446", "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse subspace clustering (SSC) is a state-of-the-art method for segmenting\na set of data points drawn from a union of subspaces into their respective\nsubspaces. It is now well understood that SSC produces subspace-preserving data\naffinity under broad geometric conditions but suffers from a connectivity\nissue. In this paper, we develop a novel geometric analysis for a variant of\nSSC, named affine SSC (ASSC), for the problem of clustering data from a union\nof affine subspaces. Our contributions include a new concept called affine\nindependence for capturing the arrangement of a collection of affine subspaces.\nUnder the affine independence assumption, we show that ASSC is guaranteed to\nproduce subspace-preserving affinity. Moreover, inspired by the phenomenon that\nthe $\\ell_1$ regularization no longer induces sparsity when the solution is\nnonnegative, we further show that subspace-preserving recovery can be achieved\nunder much weaker conditions for all data points other than the extreme points\nof samples from each subspace. In addition, we confirm a curious observation\nthat the affinity produced by ASSC may be subspace-dense---which could\nguarantee the subspace-preserving affinity of ASSC to produce correct\nclustering under rather weak conditions. We validate the theoretical findings\non carefully designed synthetic data and evaluate the performance of ASSC on\nseveral real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 18:11:37 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 15:38:25 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 04:07:34 GMT"}, {"version": "v4", "created": "Wed, 21 Nov 2018 07:39:36 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Li", "Chun-Guang", ""], ["You", "Chong", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1808.05979", "submitter": "Khalid Raza", "authors": "Tarun Kumar Gupta and Khalid Raza", "title": "Optimizing Deep Neural Network Architecture: A Tabu Search Based\n  Approach", "comments": "15 pages, 2 figures, 2 algorithms, 2 tables", "journal-ref": "Neural Processing Letters (2020), Springer", "doi": "10.1007/s11063-020-10234-7", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Feedforward neural network (FNN) fully de-pends upon the\nselection of architecture and training algorithm. FNN architecture can be\ntweaked using several parameters, such as the number of hidden layers, number\nof hidden neurons at each hidden layer and number of connections between\nlayers. There may be exponential combinations for these architectural\nattributes which may be unmanageable manually, so it requires an algorithm\nwhich can automatically design an optimal architecture with high generalization\nability. Numerous optimization algorithms have been utilized for FNN\narchitecture determination. This paper proposes a new methodology which can\nwork on the estimation of hidden layers and their respective neurons for FNN.\nThis work combines the advantages of Tabu search (TS) and Gradient descent with\nmomentum backpropagation (GDM) training algorithm to demonstrate how Tabu\nsearch can automatically select the best architecture from the populated\narchitectures based on minimum testing error criteria. The proposed approach\nhas been tested on four classification benchmark dataset of different size.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 20:12:26 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Gupta", "Tarun Kumar", ""], ["Raza", "Khalid", ""]]}, {"id": "1808.06079", "submitter": "Leto Peel", "authors": "Till Hoffmann, Leto Peel, Renaud Lambiotte, Nick S. Jones", "title": "Community detection in networks without observing edges", "comments": "16 pages, 7 figures", "journal-ref": "Science Advances 6(4) eaav1478, 2020", "doi": "10.1126/sciadv.aav1478", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian hierarchical model to identify communities in networks\nfor which we do not observe the edges directly, but instead observe a series of\ninterdependent signals for each of the nodes. Fitting the model provides an\nend-to-end community detection algorithm that does not extract information as a\nsequence of point estimates but propagates uncertainties from the raw data to\nthe community labels. Our approach naturally supports multiscale community\ndetection as well as the selection of an optimal scale using model comparison.\nWe study the properties of the algorithm using synthetic data and apply it to\ndaily returns of constituents of the S&P100 index as well as climate data from\nUS cities.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 12:34:17 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 18:13:28 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Hoffmann", "Till", ""], ["Peel", "Leto", ""], ["Lambiotte", "Renaud", ""], ["Jones", "Nick S.", ""]]}, {"id": "1808.06088", "submitter": "Jingfeng Wu", "authors": "Bing Yu, Jingfeng Wu, Jinwen Ma and Zhanxing Zhu", "title": "Tangent-Normal Adversarial Regularization for Semi-supervised Learning", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with standard supervised learning, the key difficulty in\nsemi-supervised learning is how to make full use of the unlabeled data. A\nrecently proposed method, virtual adversarial training (VAT), smartly performs\nadversarial training without label information to impose a local smoothness on\nthe classifier, which is especially beneficial to semi-supervised learning. In\nthis work, we propose tangent-normal adversarial regularization (TNAR) as an\nextension of VAT by taking the data manifold into consideration. The proposed\nTNAR is composed by two complementary parts, the tangent adversarial\nregularization (TAR) and the normal adversarial regularization (NAR). In TAR,\nVAT is applied along the tangent space of the data manifold, aiming to enforce\nlocal invariance of the classifier on the manifold, while in NAR, VAT is\nperformed on the normal space orthogonal to the tangent space, intending to\nimpose robustness on the classifier against the noise causing the observed data\ndeviating from the underlying data manifold. Demonstrated by experiments on\nboth artificial and practical datasets, our proposed TAR and NAR complement\nwith each other, and jointly outperforms other state-of-the-art methods for\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 14:30:57 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 13:44:47 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 14:57:07 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Yu", "Bing", ""], ["Wu", "Jingfeng", ""], ["Ma", "Jinwen", ""], ["Zhu", "Zhanxing", ""]]}, {"id": "1808.06107", "submitter": "Naresh Manwani", "authors": "Naresh Manwani, Mohit Chandra", "title": "Exact Passive-Aggressive Algorithms for Learning to Rank Using Interval\n  Labels", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2019.2939861", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose exact passive-aggressive (PA) online algorithms for\nlearning to rank. The proposed algorithms can be used even when we have\ninterval labels instead of actual labels for examples. The proposed algorithms\nsolve a convex optimization problem at every trial. We find exact solution to\nthose optimization problems to determine the updated parameters. We propose\nsupport class algorithm (SCA) which finds the active constraints using the KKT\nconditions of the optimization problems. These active constrains form support\nset which determines the set of thresholds that need to be updated. We derive\nupdate rules for PA, PA-I and PA-II. We show that the proposed algorithms\nmaintain the ordering of the thresholds after every trial. We provide the\nmistake bounds of the proposed algorithms in both ideal and general settings.\nWe also show experimentally that the proposed algorithms successfully learn\naccurate classifiers using interval labels as well as exact labels. Proposed\nalgorithms also do well compared to other approaches.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 17:49:36 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Manwani", "Naresh", ""], ["Chandra", "Mohit", ""]]}, {"id": "1808.06117", "submitter": "Piyush Madan", "authors": "Sharon Hensley Alford (1), Piyush Madan (2), Shilpa Mahatma (2), Italo\n  Buleje (2), Yanyan Han (2), Fang Lu (2) ((1) IBM Watson, (2) IBM Research)", "title": "Effect of secular trend in drug effectiveness study in real world data", "comments": "Presented at 7th Causal Inference Workshop at Uncertainty in\n  Artificial Intelligence (UAI) Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discovered secular trend bias in a drug effectiveness study for a recently\napproved drug. We compared treatment outcomes between patients who received the\nnewly approved drug and patients exposed to the standard treatment. All\npatients diagnosed after the new drug's approval date were considered. We built\na machine learning causal inference model to determine patient subpopulations\nlikely to respond better to the newly approved drug. After identifying the\npresence of secular trend bias in our data, we attempted to adjust for the bias\nin two different ways. First, we matched patients on the number of days from\nthe new drug's approval date that the patient's treatment (new or standard)\nbegan. Second, we included a covariate in the model for the number of days\nbetween the date of approval of the new drug and the treatment (new or\nstandard) start date. Neither approach completely mitigated the bias. Residual\nbias we attribute to differences in patient disease severity or other\nunmeasured patient characteristics. Had we not identified the secular trend\nbias in our data, the causal inference model would have been interpreted\nwithout consideration for this underlying bias. Being aware of, testing for,\nand handling potential bias in the data is essential to diminish the\nuncertainty in AI modeling.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 19:07:42 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Alford", "Sharon Hensley", "", "IBM Watson"], ["Madan", "Piyush", "", "IBM Research"], ["Mahatma", "Shilpa", "", "IBM Research"], ["Buleje", "Italo", "", "IBM Research"], ["Han", "Yanyan", "", "IBM Research"], ["Lu", "Fang", "", "IBM Research"]]}, {"id": "1808.06152", "submitter": "Jayant Gupchup A", "authors": "Jayant Gupchup, Ebrahim Beyrami, Martin Ellis, Yasaman Hosseinkashi,\n  Sam Johnson, Ross Cutler", "title": "On Design of Problem Token Questions in Quality of Experience Surveys", "comments": null, "journal-ref": null, "doi": "10.1109/QoMEX.2018.8463424", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User surveys for Quality of Experience (QoE) are a critical source of\ninformation. In addition to the common \"star rating\" used to estimate Mean\nOpinion Score (MOS), more detailed survey questions (problem tokens) about\nspecific areas provide valuable insight into the factors impacting QoE. This\npaper explores two aspects of the problem token questionnaire design. First, we\nstudy the bias introduced by fixed question order, and second, we study the\nchallenge of selecting a subset of questions to keep the token set small. Based\non 900,000 calls gathered using a randomized controlled experiment from a live\nsystem, we find that the order bias can be significantly reduced by randomizing\nthe display order of tokens. The difference in response rate varies based on\ntoken position and display design. It is worth noting that the users respond to\nthe randomized-order variant at levels that are comparable to the fixed-order\nvariant. The effective selection of a subset of token questions is achieved by\nextracting tokens that provide the highest information gain over user ratings.\nThis selection is known to be in the class of NP-hard problems. We apply a\nwell-known greedy submodular maximization method on our dataset to capture 94%\nof the information using just 30% of the questions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 02:16:40 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Gupchup", "Jayant", ""], ["Beyrami", "Ebrahim", ""], ["Ellis", "Martin", ""], ["Hosseinkashi", "Yasaman", ""], ["Johnson", "Sam", ""], ["Cutler", "Ross", ""]]}, {"id": "1808.06170", "submitter": "Zhiwei Wang", "authors": "Zhiwei Wang, Yao Ma, Dawei Yin, Jiliang Tang", "title": "Linked Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have been proven to be effective in modeling\nsequential data and they have been applied to boost a variety of tasks such as\ndocument classification, speech recognition and machine translation. Most of\nexisting RNN models have been designed for sequences assumed to be identically\nand independently distributed (i.i.d). However, in many real-world\napplications, sequences are naturally linked. For example, web documents are\nconnected by hyperlinks; and genes interact with each other. On the one hand,\nlinked sequences are inherently not i.i.d., which poses tremendous challenges\nto existing RNN models. On the other hand, linked sequences offer link\ninformation in addition to the sequential information, which enables\nunprecedented opportunities to build advanced RNN models. In this paper, we\nstudy the problem of RNN for linked sequences. In particular, we introduce a\nprincipled approach to capture link information and propose a linked Recurrent\nNeural Network (LinkedRNN), which models sequential and link information\ncoherently. We conduct experiments on real-world datasets from multiple domains\nand the experimental results validate the effectiveness of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 06:21:58 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Wang", "Zhiwei", ""], ["Ma", "Yao", ""], ["Yin", "Dawei", ""], ["Tang", "Jiliang", ""]]}, {"id": "1808.06191", "submitter": "Rustem Takhanov", "authors": "Rustem Takhanov", "title": "Fourier analysis perspective for sufficient dimension reduction problem", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theory of sufficient dimension reduction (SDR) is developed from an\noptimizational perspective. In our formulation of the problem, instead of\ndealing with raw data, we assume that our ground truth includes a mapping\n${\\mathbf f}: {\\mathbb R}^n\\rightarrow {\\mathbb R}^m$ and a probability\ndistribution function $p$ over ${\\mathbb R}^n$, both given analytically. We\nformulate SDR as a problem of finding a function ${\\mathbf g}: {\\mathbb\nR}^k\\rightarrow {\\mathbb R}^m$ and a matrix $P\\in {\\mathbb R}^{k\\times n}$ such\nthat ${\\mathbb E}_{{\\mathbf x}\\sim p({\\mathbf x})} \\left|{\\mathbf f}({\\mathbf\nx}) - {\\mathbf g}(P{\\mathbf x})\\right|^2$ is minimal. It turns out that the\nlatter problem allows a reformulation in the dual space, i.e. instead of\nsearching for ${\\mathbf g}(P{\\mathbf x})$ we suggest searching for its Fourier\ntransform. First, we characterize all tempered distributions that can serve as\nthe Fourier transform of such functions. The reformulation in the dual space\ncan be interpreted as a problem of finding a $k$-dimensional linear subspace\n$S$ and a tempered distribution ${\\mathbf t}$ supported in $S$ such that\n${\\mathbf t}$ is \"close\" in a certain sense to the Fourier transform of\n${\\mathbf f}$.\n  Instead of optimizing over generalized functions with a $k$-dimensional\nsupport, we suggest minimizing over ordinary functions but with an additional\nterm $R$ that penalizes a strong distortion of the support from any\n$k$-dimensional linear subspace. For a specific case of $R$, we develop an\nalgorithm that can be formulated for functions given in the initial form as\nwell as for their Fourier transforms. Eventually, we report results of\nnumerical experiments with a discretized version of the latter algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 09:41:21 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Takhanov", "Rustem", ""]]}, {"id": "1808.06206", "submitter": "Pan Xiao", "authors": "Pan Xiao, Bo Du, Jia Wu, Lefei Zhang, Ruimin Hu and Xuelong Li", "title": "TLR: Transfer Latent Representation for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation refers to the process of learning prediction models in a\ntarget domain by making use of data from a source domain. Many classic methods\nsolve the domain adaptation problem by establishing a common latent space,\nwhich may cause the loss of many important properties across both domains. In\nthis manuscript, we develop a novel method, transfer latent representation\n(TLR), to learn a better latent space. Specifically, we design an objective\nfunction based on a simple linear autoencoder to derive the latent\nrepresentations of both domains. The encoder in the autoencoder aims to project\nthe data of both domains into a robust latent space. Besides, the decoder\nimposes an additional constraint to reconstruct the original data, which can\npreserve the common properties of both domains and reduce the noise that causes\ndomain shift. Experiments on cross-domain tasks demonstrate the advantages of\nTLR over competing methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 13:14:01 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Xiao", "Pan", ""], ["Du", "Bo", ""], ["Wu", "Jia", ""], ["Zhang", "Lefei", ""], ["Hu", "Ruimin", ""], ["Li", "Xuelong", ""]]}, {"id": "1808.06275", "submitter": "Xupeng Chen", "authors": "Binbin Shi, Xupeng Chen", "title": "Applying Machine Learning To Maize Traits Prediction", "comments": "outdated work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterosis is the improved or increased function of any biological quality in\na hybrid offspring. We have studied yet the largest maize SNP dataset for\ntraits prediction. We develop linear and non-linear models which consider\nrelationships between different hybrids as well as other effect. Specially\ndesigned model proved to be efficient and robust in prediction maize's traits.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 00:24:41 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 04:05:33 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Shi", "Binbin", ""], ["Chen", "Xupeng", ""]]}, {"id": "1808.06314", "submitter": "Xianyi Wu", "authors": "Wenqing Bao, Xiaoqiang Cai, Xianyi Wu", "title": "A General Framework of Multi-Armed Bandit Processes by Arm Switch\n  Restrictions", "comments": "27 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general framework of multi-armed bandit (MAB) processes\nby introducing a type of restrictions on the switches among arms evolving in\ncontinuous time.\n  The Gittins index process is constructed for any single arm subject to the\nrestrictions on switches and then the optimality of the corresponding Gittins\nindex rule is established. The Gittins indices defined in this paper are\nconsistent with the ones for MAB processes in continuous time, integer time,\nsemi-Markovian setting as well as general discrete time setting, so that the\nnew theory covers the classical models as special cases and also applies to\nmany other situations that have not yet been touched in the literature. While\nthe proof of the optimality of Gittins index policies benefits from ideas in\nthe existing theory of MAB processes in continuous time, new techniques are\nintroduced which drastically simplify the proof.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 05:47:41 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 05:25:38 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Bao", "Wenqing", ""], ["Cai", "Xiaoqiang", ""], ["Wu", "Xianyi", ""]]}, {"id": "1808.06324", "submitter": "Sai Raam Venkatraman", "authors": "Sairaam Venkatraman, S Balasubramanian, R Raghunatha Sarma", "title": "PAC-learning is Undecidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of attempting to learn the mapping between data and labels is the\ncrux of any machine learning task. It is, therefore, of interest to the machine\nlearning community on practical as well as theoretical counts to consider the\nexistence of a test or criterion for deciding the feasibility of attempting to\nlearn. We investigate the existence of such a criterion in the setting of\nPAC-learning, basing the feasibility solely on whether the mapping to be learnt\nlends itself to approximation by a given class of hypothesis functions. We show\nthat no such criterion exists, exposing a fundamental limitation in the\ndecidability of learning. In other words, we prove that testing for\nPAC-learnability is undecidable in the Turing sense. We also briefly discuss\nsome of the probable implications of this result to the current practice of\nmachine learning.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 06:41:01 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 09:14:15 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Venkatraman", "Sairaam", ""], ["Balasubramanian", "S", ""], ["Sarma", "R Raghunatha", ""]]}, {"id": "1808.06347", "submitter": "Wenyi Wang Mr.", "authors": "Weirui Kong and Wenyi Wang", "title": "A Distribution Similarity Based Regularizer for Learning Bayesian\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models compactly represent joint distributions by\ndecomposing them into factors over subsets of random variables. In Bayesian\nnetworks, the factors are conditional probability distributions. For many\nproblems, common information exists among those factors. Adding similarity\nrestrictions can be viewed as imposing prior knowledge for model\nregularization. With proper restrictions, learned models usually generalize\nbetter. In this work, we study methods that exploit such high-level\nsimilarities to regularize the learning process and apply them to the task of\nmodeling the wave propagation in inhomogeneous media. We propose a novel\ndistribution-based penalization approach that encourages similar conditional\nprobability distribution rather than force the parameters to be similar\nexplicitly. We show in experiment that our proposed algorithm solves the\nmodeling wave propagation problem, which other baseline methods are not able to\nsolve.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 08:46:33 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Kong", "Weirui", ""], ["Wang", "Wenyi", ""]]}, {"id": "1808.06352", "submitter": "Sajad Saeedi", "authors": "Sajad Saeedi (1), Bruno Bodin (2), Harry Wagstaff (2), Andy Nisbet\n  (3), Luigi Nardi (4), John Mawer (3), Nicolas Melot (1), Oscar Palomar (3),\n  Emanuele Vespa (1), Tom Spink (2), Cosmin Gorgovan (3), Andrew Webb (3),\n  James Clarkson (3), Erik Tomusk (2), Thomas Debrunner (1), Kuba Kaszyk (2),\n  Pablo Gonzalez-de-Aledo (1), Andrey Rodchenko (3), Graham Riley (3), Christos\n  Kotselidis (3), Bj\\\"orn Franke (2), Michael F. P. O'Boyle (2), Andrew J.\n  Davison (1), Paul H. J. Kelly (1), Mikel Luj\\'an (3), Steve Furber (3) ((1)\n  Department of Computing, Imperial College London, UK, (2) School of\n  Informatics, University of Edinburgh, UK, (3) School of Computer Science,\n  University of Manchester, UK, (4) Electrical Engineering - Computer Systems,\n  Stanford University, USA)", "title": "Navigating the Landscape for Real-time Localisation and Mapping for\n  Robotics and Virtual and Augmented Reality", "comments": "Proceedings of the IEEE 2018", "journal-ref": null, "doi": "10.1109/JPROC.2018.2856739", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual understanding of 3D environments in real-time, at low power, is a huge\ncomputational challenge. Often referred to as SLAM (Simultaneous Localisation\nand Mapping), it is central to applications spanning domestic and industrial\nrobotics, autonomous vehicles, virtual and augmented reality. This paper\ndescribes the results of a major research effort to assemble the algorithms,\narchitectures, tools, and systems software needed to enable delivery of SLAM,\nby supporting applications specialists in selecting and configuring the\nappropriate algorithm and the appropriate hardware, and compilation pathway, to\nmeet their performance, accuracy, and energy consumption goals. The major\ncontributions we present are (1) tools and methodology for systematic\nquantitative evaluation of SLAM algorithms, (2) automated,\nmachine-learning-guided exploration of the algorithmic and implementation\ndesign space with respect to multiple objectives, (3) end-to-end simulation\ntools to enable optimisation of heterogeneous, accelerated architectures for\nthe specific algorithmic requirements of the various SLAM algorithmic\napproaches, and (4) tools for delivering, where appropriate, accelerated,\nadaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 09:06:21 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Saeedi", "Sajad", ""], ["Bodin", "Bruno", ""], ["Wagstaff", "Harry", ""], ["Nisbet", "Andy", ""], ["Nardi", "Luigi", ""], ["Mawer", "John", ""], ["Melot", "Nicolas", ""], ["Palomar", "Oscar", ""], ["Vespa", "Emanuele", ""], ["Spink", "Tom", ""], ["Gorgovan", "Cosmin", ""], ["Webb", "Andrew", ""], ["Clarkson", "James", ""], ["Tomusk", "Erik", ""], ["Debrunner", "Thomas", ""], ["Kaszyk", "Kuba", ""], ["Gonzalez-de-Aledo", "Pablo", ""], ["Rodchenko", "Andrey", ""], ["Riley", "Graham", ""], ["Kotselidis", "Christos", ""], ["Franke", "Bj\u00f6rn", ""], ["O'Boyle", "Michael F. P.", ""], ["Davison", "Andrew J.", ""], ["Kelly", "Paul H. J.", ""], ["Luj\u00e1n", "Mikel", ""], ["Furber", "Steve", ""]]}, {"id": "1808.06356", "submitter": "Alexander Marx", "authors": "Alexander Marx and Jilles Vreeken", "title": "Causal Discovery by Telling Apart Parents and Children", "comments": "11 pages, results section changed slightly", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inferring the directed, causal graph from\nobservational data, assuming no hidden confounders. We take an information\ntheoretic approach, and make three main contributions.\n  First, we show how through algorithmic information theory we can obtain SCI,\na highly robust, effective and computationally efficient test for conditional\nindependence---and show it outperforms the state of the art when applied in\nconstraint-based inference methods such as stable PC.\n  Second, building upon on SCI, we show how to tell apart the parents and\nchildren of a given node based on the algorithmic Markov condition. We give the\nClimb algorithm to efficiently discover the directed, causal Markov\nblanket---and show it is at least as accurate as inferring the global network,\nwhile being much more efficient.\n  Last, but not least, we detail how we can use the Climb score to direct those\nedges that state of the art causal discovery algorithms based on PC or GES\nleave undirected---and show this improves their precision, recall and F1 scores\nby up to 20%.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 09:28:24 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 13:38:03 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Marx", "Alexander", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1808.06362", "submitter": "Antonela Tommasel", "authors": "J. Andr\\'es D\\'iaz-Pace, Antonela Tommasel, Daniela Godoy", "title": "Towards Anticipation of Architectural Smells using Link Prediction\n  Techniques", "comments": "Accepted for publication at SCAM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software systems naturally evolve, and this evolution often brings design\nproblems that cause system degradation. Architectural smells are typical\nsymptoms of such problems, and several of these smells are related to undesired\ndependencies among modules. The early detection of these smells is important\nfor developers, because they can plan ahead for maintenance or refactoring\nefforts, thus preventing system degradation. Existing tools for identifying\narchitectural smells can detect the smells once they exist in the source code.\nThis means that their undesired dependencies are already created. In this work,\nwe explore a forward-looking approach that is able to infer groups of likely\nmodule dependencies that can anticipate architectural smells in a future system\nversion. Our approach considers the current module structure as a network,\nalong with information from previous versions, and applies link prediction\ntechniques (from the field of social network analysis). In particular, we focus\non dependency-related smells, such as Cyclic Dependency and Hublike Dependency,\nwhich fit well with the link prediction model. An initial evaluation with two\nopen-source projects shows that, under certain considerations, the predictions\nof our approach are satisfactory. Furthermore, the approach can be extended to\nother types of dependency-based smells or metrics.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 09:41:42 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["D\u00edaz-Pace", "J. Andr\u00e9s", ""], ["Tommasel", "Antonela", ""], ["Godoy", "Daniela", ""]]}, {"id": "1808.06394", "submitter": "Christian Schulz", "authors": "Sebastian Schlag, Matthias Schmitt, Christian Schulz", "title": "Faster Support Vector Machines", "comments": "Extended version of the ALENEX'19 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time complexity of support vector machines (SVMs) prohibits training on\nhuge data sets with millions of data points. Recently, multilevel approaches to\ntrain SVMs have been developed to allow for time-efficient training on huge\ndata sets. While regular SVMs perform the entire training in one -- time\nconsuming -- optimization step, multilevel SVMs first build a hierarchy of\nproblems decreasing in size that resemble the original problem and then train\nan SVM model for each hierarchy level, benefiting from the solved models of\nprevious levels. We present a faster multilevel support vector machine that\nuses a label propagation algorithm to construct the problem hierarchy.\nExtensive experiments indicate that our approach is up to orders of magnitude\nfaster than the previous fastest algorithm while having comparable\nclassification quality. For example, already one of our sequential solvers is\non average a factor 15 faster than the parallel ThunderSVM algorithm, while\nhaving similar classification quality.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 11:38:46 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 13:21:23 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 10:41:44 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Schlag", "Sebastian", ""], ["Schmitt", "Matthias", ""], ["Schulz", "Christian", ""]]}, {"id": "1808.06396", "submitter": "Adrian Popescu", "authors": "Eden Belouadah, Adrian Popescu", "title": "DeeSIL: Deep-Shallow Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental Learning (IL) is an interesting AI problem when the algorithm is\nassumed to work on a budget. This is especially true when IL is modeled using a\ndeep learning approach, where two com- plex challenges arise due to limited\nmemory, which induces catastrophic forgetting and delays related to the\nretraining needed in order to incorpo- rate new classes. Here we introduce\nDeeSIL, an adaptation of a known transfer learning scheme that combines a fixed\ndeep representation used as feature extractor and learning independent shallow\nclassifiers to in- crease recognition capacity. This scheme tackles the two\naforementioned challenges since it works well with a limited memory budget and\neach new concept can be added within a minute. Moreover, since no deep re-\ntraining is needed when the model is incremented, DeeSIL can integrate larger\namounts of initial data that provide more transferable features. Performance is\nevaluated on ImageNet LSVRC 2012 against three state of the art algorithms.\nResults show that, at scale, DeeSIL performance is 23 and 33 points higher than\nthe best baseline when using the same and more initial data respectively.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 11:39:09 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Belouadah", "Eden", ""], ["Popescu", "Adrian", ""]]}, {"id": "1808.06444", "submitter": "Ally Jr Salim", "authors": "Ally Salim Jr", "title": "Synthetic Patient Generation: A Deep Learning Approach Using Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial Intelligence in healthcare is a new and exciting frontier and the\npossibilities are endless. With deep learning approaches beating human\nperformances in many areas, the logical next step is to attempt their\napplication in the health space. For these and other Machine Learning\napproaches to produce good results and have their potential realized, the need\nfor, and importance of, large amounts of accurate data is second to none. This\nis a challenge faced by many industries and more so in the healthcare space. We\npresent an approach of using Variational Autoencoders (VAE's) as an approach to\ngenerating more data for training deeper networks, as well as uncovering\nunderlying patterns in diagnoses and the patients suffering from them. By\ntraining a VAE, on available data, it was able to learn the latent distribution\nof the patient features given the diagnosis. It is then possible, after\ntraining, to sample from the learnt latent distribution to generate new\naccurate patient records given the patient diagnosis.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 13:36:07 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Salim", "Ally", "Jr"]]}, {"id": "1808.06452", "submitter": "Jorge Samper-Gonz\\'alez", "authors": "Jorge Samper-Gonz\\'alez, Ninon Burgos, Simona Bottani, Sabrina\n  Fontanella, Pascal Lu, Arnaud Marcoux, Alexandre Routier, J\\'er\\'emy Guillon,\n  Michael Bacci, Junhao Wen, Anne Bertrand, Hugo Bertin, Marie-Odile Habert,\n  Stanley Durrleman, Theodoros Evgeniou, Olivier Colliot (for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing)", "title": "Reproducible evaluation of classification methods in Alzheimer's\n  disease: framework and application to MRI and PET data", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2018.08.042", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of papers have introduced novel machine learning and feature\nextraction methods for automatic classification of AD. However, they are\ndifficult to reproduce because key components of the validation are often not\nreadily available. These components include selected participants and input\ndata, image preprocessing and cross-validation procedures. The performance of\nthe different approaches is also difficult to compare objectively. In\nparticular, it is often difficult to assess which part of the method provides a\nreal improvement, if any. We propose a framework for reproducible and objective\nclassification experiments in AD using three publicly available datasets (ADNI,\nAIBL and OASIS). The framework comprises: i) automatic conversion of the three\ndatasets into BIDS format, ii) a modular set of preprocessing pipelines,\nfeature extraction and classification methods, together with an evaluation\nframework, that provide a baseline for benchmarking the different components.\nWe demonstrate the use of the framework for a large-scale evaluation on 1960\nparticipants using T1 MRI and FDG PET data. In this evaluation, we assess the\ninfluence of different modalities, preprocessing, feature types, classifiers,\ntraining set sizes and datasets. Performances were in line with the\nstate-of-the-art. FDG PET outperformed T1 MRI for all classification tasks. No\ndifference in performance was found for the use of different atlases, image\nsmoothing, partial volume correction of FDG PET images, or feature type. Linear\nSVM and L2-logistic regression resulted in similar performance and both\noutperformed random forests. The classification performance increased along\nwith the number of subjects used for training. Classifiers trained on ADNI\ngeneralized well to AIBL and OASIS. All the code of the framework and the\nexperiments is publicly available at:\nhttps://gitlab.icm-institute.org/aramislab/AD-ML.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 13:54:03 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Samper-Gonz\u00e1lez", "Jorge", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Burgos", "Ninon", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Bottani", "Simona", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Fontanella", "Sabrina", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Lu", "Pascal", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Marcoux", "Arnaud", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Routier", "Alexandre", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Guillon", "J\u00e9r\u00e9my", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Bacci", "Michael", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Wen", "Junhao", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Bertrand", "Anne", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Bertin", "Hugo", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Habert", "Marie-Odile", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Durrleman", "Stanley", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Evgeniou", "Theodoros", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"], ["Colliot", "Olivier", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative and the Australian Imaging Biomarkers and\n  Lifestyle flagship study of ageing"]]}, {"id": "1808.06453", "submitter": "Patrick Jahnke", "authors": "Patrick Jahnke, Emmanuel Stapf, Jonas Mieseler, Gerhard Neumann,\n  Patrick Eugster", "title": "Towards Fine Grained Network Flow Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One main challenge for the design of networks is that traffic load is not\ngenerally known in advance. This makes it hard to adequately devote resources\nsuch as to best prevent or mitigate bottlenecks. While several authors have\nshown how to predict traffic in a coarse grained manner by aggregating flows,\nfine grained prediction of traffic at the level of individual flows, including\nbursty traffic, is widely considered to be impossible. This paper shows, to the\nbest of our knowledge, the first approach to fine grained per flow traffic\nprediction. In short, we introduce the Frequency-based Kernel Kalman Filter\n(FKKF), which predicts individual flows' behavior based on measurements. Our\nFKKF relies on the well known Kalman Filter in combination with a kernel to\nsupport the prediction of non linear functions. Furthermore we change the\noperating space from time to frequency space. In this space, into which we\ntransform the input data via a Short-Time Fourier Transform (STFT), the peak\nstructures of flows can be predicted after gleaning their key characteristics,\nwith a Principal Component Analysis (PCA), from past and ongoing flows that\nstem from the same socket-to-socket connection. We demonstrate the\neffectiveness of our approach on popular benchmark traces from a university\ndata center. Our approach predicts traffic on average across 17 out of 20\ngroups of flows with an average prediction error of 6.43% around 0.49 (average)\nseconds in advance, whilst existing coarse grained approaches exhibit\nprediction errors of 77% at best.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 13:58:55 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Jahnke", "Patrick", ""], ["Stapf", "Emmanuel", ""], ["Mieseler", "Jonas", ""], ["Neumann", "Gerhard", ""], ["Eugster", "Patrick", ""]]}, {"id": "1808.06470", "submitter": "Mohamed Ibrahim", "authors": "Mohamed R. Ibrahim, Helena Titheridge, Tao Cheng and James Haworth", "title": "predictSLUMS: A new model for identifying and predicting informal\n  settlements and slums in cities from street intersections using machine\n  learning", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying current and future informal regions within cities remains a\ncrucial issue for policymakers and governments in developing countries. The\ndelineation process of identifying such regions in cities requires a lot of\nresources. While there are various studies that identify informal settlements\nbased on satellite image classification, relying on both supervised or\nunsupervised machine learning approaches, these models either require multiple\ninput data to function or need further development with regards to precision.\nIn this paper, we introduce a novel method for identifying and predicting\ninformal settlements using only street intersections data, regardless of the\nvariation of urban form, number of floors, materials used for construction or\nstreet width. With such minimal input data, we attempt to provide planners and\npolicy-makers with a pragmatic tool that can aid in identifying informal zones\nin cities. The algorithm of the model is based on spatial statistics and a\nmachine learning approach, using Multinomial Logistic Regression (MNL) and\nArtificial Neural Networks (ANN). The proposed model relies on defining\ninformal settlements based on two ubiquitous characteristics that these regions\ntend to be filled in with smaller subdivided lots of housing relative to the\nformal areas within the local context, and the paucity of services and\ninfrastructure within the boundary of these settlements that require relatively\nbigger lots. We applied the model in five major cities in Egypt and India that\nhave spatial structures in which informality is present. These cities are\nGreater Cairo, Alexandria, Hurghada and Minya in Egypt, and Mumbai in India.\nThe predictSLUMS model shows high validity and accuracy for identifying and\npredicting informality within the same city the model was trained on or in\ndifferent ones of a similar context.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 09:34:16 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Ibrahim", "Mohamed R.", ""], ["Titheridge", "Helena", ""], ["Cheng", "Tao", ""], ["Haworth", "James", ""]]}, {"id": "1808.06474", "submitter": "Yi-Te Hsu", "authors": "Yi-Te Hsu, Yu-Chen Lin, Szu-Wei Fu, Yu Tsao, Tei-Wei Kuo", "title": "A study on speech enhancement using exponent-only floating point\n  quantized neural network (EOFP-QNN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous studies have investigated the effectiveness of neural network\nquantization on pattern classification tasks. The present study, for the first\ntime, investigated the performance of speech enhancement (a regression task in\nspeech processing) using a novel exponent-only floating-point quantized neural\nnetwork (EOFP-QNN). The proposed EOFP-QNN consists of two stages:\nmantissa-quantization and exponent-quantization. In the mantissa-quantization\nstage, EOFP-QNN learns how to quantize the mantissa bits of the model\nparameters while preserving the regression accuracy using the least mantissa\nprecision. In the exponent-quantization stage, the exponent part of the\nparameters is further quantized without causing any additional performance\ndegradation. We evaluated the proposed EOFP quantization technique on two types\nof neural networks, namely, bidirectional long short-term memory (BLSTM) and\nfully convolutional neural network (FCN), on a speech enhancement task.\nExperimental results showed that the model sizes can be significantly reduced\n(the model sizes of the quantized BLSTM and FCN models were only 18.75% and\n21.89%, respectively, compared to those of the original models) while\nmaintaining satisfactory speech-enhancement performance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 11:44:34 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 13:04:58 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2018 16:28:20 GMT"}, {"version": "v4", "created": "Tue, 30 Oct 2018 23:49:21 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Hsu", "Yi-Te", ""], ["Lin", "Yu-Chen", ""], ["Fu", "Szu-Wei", ""], ["Tsao", "Yu", ""], ["Kuo", "Tei-Wei", ""]]}, {"id": "1808.06475", "submitter": "Jan Claes", "authors": "Francis Bru and Jan Claes", "title": "The perceived quality of process discovery tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Process discovery has seen a rise in popularity in the last decade for both\nresearchers and businesses. Recent developments mainly focused on the power and\nthe functionalities of the discovery algorithm. While continuous improvement of\nthese functional aspects is very important, non-functional aspects such as\nvisualization and usability are often overlooked. However, these aspects are\nconsidered valuable for end-users and play an important part in the experience\nof these end-users when working with a process discovery tool. A questionnaire\nhas been sent out to give end-users the opportunity to voice their opinion on\navailable process discovery tools and about the state of process discovery as a\ndomain in general. The results of 66 respondents are presented and compared\nwith the answers of 63 respondents that were contacted through one particular\nsoftware vendor's employee and customer base (i.e., Celonis).\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 18:43:11 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Bru", "Francis", ""], ["Claes", "Jan", ""]]}, {"id": "1808.06492", "submitter": "Adithya Balaji", "authors": "Adithya Balaji, Alexander Allen", "title": "Benchmarking Automatic Machine Learning Frameworks", "comments": "9 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  AutoML serves as the bridge between varying levels of expertise when\ndesigning machine learning systems and expedites the data science process. A\nwide range of techniques is taken to address this, however there does not exist\nan objective comparison of these techniques. We present a benchmark of current\nopen source AutoML solutions using open source datasets. We test auto-sklearn,\nTPOT, auto_ml, and H2O's AutoML solution against a compiled set of regression\nand classification datasets sourced from OpenML and find that auto-sklearn\nperforms the best across classification datasets and TPOT performs the best\nacross regression datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 02:15:39 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Balaji", "Adithya", ""], ["Allen", "Alexander", ""]]}, {"id": "1808.06503", "submitter": "Paul Fergus Dr", "authors": "Paul Fergus, Carl Chalmers, and David Tully", "title": "Collaborative Pressure Ulcer Prevention: An Automated Skin Damage and\n  Pressure Ulcer Assessment Tool for Nursing Professionals, Patients, Family\n  Members and Carers", "comments": "5 Pages, 7 figures, Position Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Pressure Ulcers Online Website, which is a first\nstep solution towards a new and innovative platform for helping people to\ndetect, understand and manage pressure ulcers. It outlines the reasons why the\nproject has been developed and provides a central point of contact for pressure\nulcer analysis and ongoing research. Using state-of-the-art technologies in\nconvolutional neural networks and transfer learning along with end-to-end web\ntechnologies, this platform allows pressure ulcers to be analysed and findings\nto be reported. As the system evolves through collaborative partnerships,\nfuture versions will provide decision support functions to describe the complex\ncharacteristics of pressure ulcers along with information on wound care across\nmultiple user boundaries. This project is therefore intended to raise awareness\nand support for people suffering with or providing care for pressure ulcers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 14:26:47 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Fergus", "Paul", ""], ["Chalmers", "Carl", ""], ["Tully", "David", ""]]}, {"id": "1808.06508", "submitter": "Irina Higgins", "authors": "Alessandro Achille and Tom Eccles and Loic Matthey and Christopher P.\n  Burgess and Nick Watters and Alexander Lerchner and Irina Higgins", "title": "Life-Long Disentangled Representation Learning with Cross-Domain Latent\n  Homologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent behaviour in the real-world requires the ability to acquire new\nknowledge from an ongoing sequence of experiences while preserving and reusing\npast knowledge. We propose a novel algorithm for unsupervised representation\nlearning from piece-wise stationary visual data: Variational Autoencoder with\nShared Embeddings (VASE). Based on the Minimum Description Length principle,\nVASE automatically detects shifts in the data distribution and allocates spare\nrepresentational capacity to new knowledge, while simultaneously protecting\npreviously learnt representations from catastrophic forgetting. Our approach\nencourages the learnt representations to be disentangled, which imparts a\nnumber of desirable properties: VASE can deal sensibly with ambiguous inputs,\nit can enhance its own representations through imagination-based exploration,\nand most importantly, it exhibits semantically meaningful sharing of latents\nbetween different datasets. Compared to baselines with entangled\nrepresentations, our approach is able to reason beyond surface-level statistics\nand perform semantically meaningful cross-domain inference.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 15:15:32 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Achille", "Alessandro", ""], ["Eccles", "Tom", ""], ["Matthey", "Loic", ""], ["Burgess", "Christopher P.", ""], ["Watters", "Nick", ""], ["Lerchner", "Alexander", ""], ["Higgins", "Irina", ""]]}, {"id": "1808.06533", "submitter": "Dongrui Wu", "authors": "He He and Dongrui Wu", "title": "Spatial Filtering for Brain Computer Interfaces: A Comparison between\n  the Common Spatial Pattern and Its Variant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electroencephalogram (EEG) is the most popular form of input for brain\ncomputer interfaces (BCIs). However, it can be easily contaminated by various\nartifacts and noise, e.g., eye blink, muscle activities, powerline noise, etc.\nTherefore, the EEG signals are often filtered both spatially and temporally to\nincrease the signal-to-noise ratio before they are fed into a machine learning\nalgorithm for recognition. This paper considers spatial filtering,\nparticularly, the common spatial pattern (CSP) filters for EEG classification.\nIn binary classification, CSP seeks a set of filters to maximize the variance\nfor one class while minimizing it for the other. We first introduce the\ntraditional solution, and then a new solution based on a slightly different\nobjective function. We performed comprehensive experiments on motor imagery to\ncompare the two approaches, and found that generally the traditional CSP\nsolution still gives better results. We also showed that adding regularization\nto the covariance matrices can improve the final classification performance, no\nmatter which objective function is used.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:39:13 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["He", "He", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.06536", "submitter": "Rodrigo de Lamare", "authors": "A. Flores and R. C. de Lamare", "title": "Study of Set-Membership Adaptive Kernel Algorithms", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, a considerable research effort has been devoted to\ndeveloping adaptive algorithms based on kernel functions. One of the main\nfeatures of these algorithms is that they form a family of universal\napproximation techniques, solving problems with nonlinearities elegantly. In\nthis paper, we present data-selective adaptive kernel normalized least-mean\nsquare (KNLMS) algorithms that can increase their learning rate and reduce\ntheir computational complexity. In fact, these methods deal with kernel\nexpansions, creating a growing structure also known as the dictionary, whose\nsize depends on the number of observations and their innovation. The algorithms\ndescribed herein use an adaptive step-size to accelerate the learning and can\noffer an excellent tradeoff between convergence speed and steady state, which\nallows them to solve nonlinear filtering and estimation problems with a large\nnumber of parameters without requiring a large computational cost. The\ndata-selective update scheme also limits the number of operations performed and\nthe size of the dictionary created by the kernel expansion, saving\ncomputational resources and dealing with one of the major problems of kernel\nadaptive algorithms. A statistical analysis is carried out along with a\ncomputational complexity analysis of the proposed algorithms. Simulations show\nthat the proposed KNLMS algorithms outperform existing algorithms in examples\nof nonlinear system identification and prediction of a time series originating\nfrom a nonlinear difference equation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 23:49:51 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Flores", "A.", ""], ["de Lamare", "R. C.", ""]]}, {"id": "1808.06537", "submitter": "Kun Wang", "authors": "Kun Wang", "title": "Ricean K-factor Estimation based on Channel Quality Indicator in OFDM\n  Systems using Neural Network", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ricean channel model is widely used in wireless communications to\ncharacterize the channels with a line-of-sight path. The Ricean K factor,\ndefined as the ratio of direct path and scattered paths, provides a good\nindication of the link quality. Most existing works estimate K factor based on\neither maximum-likelihood criterion or higher-order moments, and the existing\nworks are targeted at K-factor estimation at receiver side. In this work, a\nnovel approach is proposed. Cast as a classification problem, the estimation of\nK factor by neural network provides high accuracy. Moreover, the proposed\nK-factor estimation is done at transmitter side for transmit processing, thus\nsaving the limited feedback bandwidth.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 04:23:31 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Wang", "Kun", ""]]}, {"id": "1808.06556", "submitter": "Kai Xu", "authors": "Yawei Zhao, Kai Xu, Xinwang Liu, En Zhu, Xinzhong Zhu, Jianping Yin", "title": "Triangle Lasso for Simultaneous Clustering and Optimization in Graph\n  Datasets", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, network lasso has drawn many attentions due to its remarkable\nperformance on simultaneous clustering and optimization. However, it usually\nsuffers from the imperfect data (noise, missing values etc), and yields\nsub-optimal solutions. The reason is that it finds the similar instances\naccording to their features directly, which is usually impacted by the\nimperfect data, and thus returns sub-optimal results. In this paper, we propose\ntriangle lasso to avoid its disadvantage. Triangle lasso finds the similar\ninstances according to their neighbours. If two instances have many common\nneighbours, they tend to become similar. Although some instances are profiled\nby the imperfect data, it is still able to find the similar counterparts.\nFurthermore, we develop an efficient algorithm based on Alternating Direction\nMethod of Multipliers (ADMM) to obtain a moderately accurate solution. In\naddition, we present a dual method to obtain the accurate solution with the low\nadditional time consumption. We demonstrate through extensive numerical\nexperiments that triangle lasso is robust to the imperfect data. It usually\nyields a better performance than the state-of-the-art method when performing\ndata analysis tasks in practical scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 16:31:30 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Zhao", "Yawei", ""], ["Xu", "Kai", ""], ["Liu", "Xinwang", ""], ["Zhu", "En", ""], ["Zhu", "Xinzhong", ""], ["Yin", "Jianping", ""]]}, {"id": "1808.06560", "submitter": "Shuchin Aeron", "authors": "Anuththari Gamage, Brian Rappaport, Shuchin Aeron, Xiaozhe Hu", "title": "Multi-View Graph Embedding Using Randomized Shortest Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data sets often provide multiple types of information about the\nsame set of entities. This data is well represented by multi-view graphs, which\nconsist of several distinct sets of edges over the same nodes. These can be\nused to analyze how entities interact from different viewpoints. Combining\nmultiple views improves the quality of inferences drawn from the underlying\ndata, which has increased interest in developing efficient multi-view graph\nembedding methods. We propose an algorithm, C-RSP, that generates a common (C)\nembedding of a multi-view graph using Randomized Shortest Paths (RSP). This\nalgorithm generates a dissimilarity measure between nodes by minimizing the\nexpected cost of a random walk between any two nodes across all views of a\nmulti-view graph, in doing so encoding both the local and global structure of\nthe graph. We test C-RSP on both real and synthetic data and show that it\noutperforms benchmark algorithms at embedding and clustering tasks while\nremaining computationally efficient.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 16:40:35 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Gamage", "Anuththari", ""], ["Rappaport", "Brian", ""], ["Aeron", "Shuchin", ""], ["Hu", "Xiaozhe", ""]]}, {"id": "1808.06573", "submitter": "Xi Liu", "authors": "Xi Liu, Muhe Xie, Xidao Wen, Rui Chen, Yong Ge, Nick Duffield, Na Wang", "title": "A Semi-Supervised and Inductive Embedding Model for Churn Prediction of\n  Large-Scale Mobile Games", "comments": "to appear in ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile gaming has emerged as a promising market with billion-dollar revenues.\nA variety of mobile game platforms and services have been developed around the\nworld. One critical challenge for these platforms and services is to understand\nuser churn behavior in mobile games. Accurate churn prediction will benefit\nmany stakeholders such as game developers, advertisers, and platform operators.\nIn this paper, we present the first large-scale churn prediction solution for\nmobile games. In view of the common limitations of the state-of-the-art methods\nbuilt upon traditional machine learning models, we devise a novel\nsemi-supervised and inductive embedding model that jointly learns the\nprediction function and the embedding function for user-app relationships. We\nmodel these two functions by deep neural networks with a unique edge embedding\ntechnique that is able to capture both contextual information and relationship\ndynamics. We also design a novel attributed random walk technique that takes\ninto consideration both topological adjacency and attribute similarities. To\nevaluate the performance of our solution, we collect real-world data from the\nSamsung Game Launcher platform that includes tens of thousands of games and\nhundreds of millions of user-app interactions. The experimental results with\nthis data demonstrate the superiority of our proposed model against existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 17:18:25 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 06:51:56 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 20:05:17 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Liu", "Xi", ""], ["Xie", "Muhe", ""], ["Wen", "Xidao", ""], ["Chen", "Rui", ""], ["Ge", "Yong", ""], ["Duffield", "Nick", ""], ["Wang", "Na", ""]]}, {"id": "1808.06581", "submitter": "Yixin Wang", "authors": "Yixin Wang, Dawen Liang, Laurent Charlin, David M. Blei", "title": "The Deconfounded Recommender: A Causal Inference Approach to\n  Recommendation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of recommendation is to show users items that they will like. Though\nusually framed as a prediction, the spirit of recommendation is to answer an\ninterventional question---for each user and movie, what would the rating be if\nwe \"forced\" the user to watch the movie? To this end, we develop a causal\napproach to recommendation, one where watching a movie is a \"treatment\" and a\nuser's rating is an \"outcome.\" The problem is there may be unobserved\nconfounders, variables that affect both which movies the users watch and how\nthey rate them; unobserved confounders impede causal predictions with\nobservational data. To solve this problem, we develop the deconfounded\nrecommender, a way to use classical recommendation models for causal\nrecommendation. Following Wang & Blei [23], the deconfounded recommender\ninvolves two probabilistic models. The first models which movies the users\nwatch; it provides a substitute for the unobserved confounders. The second one\nmodels how each user rates each movie; it employs the substitute to help\naccount for confounders. This two-stage approach removes bias due to\nconfounding. It improves recommendation and enjoys stable performance against\ninterventions on test sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 17:41:39 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 10:17:09 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Wang", "Yixin", ""], ["Liang", "Dawen", ""], ["Charlin", "Laurent", ""], ["Blei", "David M.", ""]]}, {"id": "1808.06601", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan\n  Kautz, Bryan Catanzaro", "title": "Video-to-Video Synthesis", "comments": "In NeurIPS, 2018. Code, models, and more results are available at\n  https://github.com/NVIDIA/vid2vid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of video-to-video synthesis, whose goal is to learn a\nmapping function from an input source video (e.g., a sequence of semantic\nsegmentation masks) to an output photorealistic video that precisely depicts\nthe content of the source video. While its image counterpart, the\nimage-to-image synthesis problem, is a popular topic, the video-to-video\nsynthesis problem is less explored in the literature. Without understanding\ntemporal dynamics, directly applying existing image synthesis approaches to an\ninput video often results in temporally incoherent videos of low visual\nquality. In this paper, we propose a novel video-to-video synthesis approach\nunder the generative adversarial learning framework. Through carefully-designed\ngenerator and discriminator architectures, coupled with a spatio-temporal\nadversarial objective, we achieve high-resolution, photorealistic, temporally\ncoherent video results on a diverse set of input formats including segmentation\nmasks, sketches, and poses. Experiments on multiple benchmarks show the\nadvantage of our method compared to strong baselines. In particular, our model\nis capable of synthesizing 2K resolution videos of street scenes up to 30\nseconds long, which significantly advances the state-of-the-art of video\nsynthesis. Finally, we apply our approach to future video prediction,\noutperforming several state-of-the-art competing systems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 17:58:42 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 15:12:44 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Liu", "Ming-Yu", ""], ["Zhu", "Jun-Yan", ""], ["Liu", "Guilin", ""], ["Tao", "Andrew", ""], ["Kautz", "Jan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1808.06603", "submitter": "Ali Oskooei", "authors": "Ali Oskooei, Matteo Manica, Roland Mathis and Maria Rodriguez Martinez", "title": "Network-based Biased Tree Ensembles (NetBiTE) for Drug Sensitivity\n  Prediction and Drug Sensitivity Biomarker Identification in Cancer", "comments": "36 pages, 5 figures, 3 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Network-based Biased Tree Ensembles (NetBiTE) method for drug\nsensitivity prediction and drug sensitivity biomarker identification in cancer\nusing a combination of prior knowledge and gene expression data. Our devised\nmethod consists of a biased tree ensemble that is built according to a\nprobabilistic bias weight distribution. The bias weight distribution is\nobtained from the assignment of high weights to the drug targets and\npropagating the assigned weights over a protein-protein interaction network\nsuch as STRING. The propagation of weights, defines neighborhoods of influence\naround the drug targets and as such simulates the spread of perturbations\nwithin the cell, following drug administration. Using a synthetic dataset, we\nshowcase how application of biased tree ensembles (BiTE) results in significant\naccuracy gains at a much lower computational cost compared to the unbiased\nrandom forests (RF) algorithm. We then apply NetBiTE to the Genomics of Drug\nSensitivity in Cancer (GDSC) dataset and demonstrate that NetBiTE outperforms\nRF in predicting IC50 drug sensitivity, only for drugs that target membrane\nreceptor pathways (MRPs): RTK, EGFR and IGFR signaling pathways. We propose\nbased on the NetBiTE results, that for drugs that inhibit MRPs, the expression\nof target genes prior to drug administration is a biomarker for IC50 drug\nsensitivity following drug administration. We further verify and reinforce this\nproposition through control studies on, PI3K/MTOR signaling pathway inhibitors,\na drug category that does not target MRPs, and through assignment of dummy\ntargets to MRP inhibiting drugs and investigating the variation in NetBiTE\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 14:43:20 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 15:33:38 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Oskooei", "Ali", ""], ["Manica", "Matteo", ""], ["Mathis", "Roland", ""], ["Martinez", "Maria Rodriguez", ""]]}, {"id": "1808.06640", "submitter": "Yanai Elazar", "authors": "Yanai Elazar and Yoav Goldberg", "title": "Adversarial Removal of Demographic Attributes from Text Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Representation Learning and Adversarial Training seem to\nsucceed in removing unwanted features from the learned representation. We show\nthat demographic information of authors is encoded in -- and can be recovered\nfrom -- the intermediate representations learned by text-based neural\nclassifiers. The implication is that decisions of classifiers trained on\ntextual data are not agnostic to -- and likely condition on -- demographic\nattributes. When attempting to remove such demographic information using\nadversarial training, we find that while the adversarial component achieves\nchance-level development-set accuracy during training, a post-hoc classifier,\ntrained on the encoded sentences from the first part, still manages to reach\nsubstantially higher classification accuracies on the same data. This behavior\nis consistent across several tasks, demographic properties and datasets. We\nexplore several techniques to improve the effectiveness of the adversarial\ncomponent. Our main conclusion is a cautionary one: do not rely on the\nadversarial training to achieve invariant representation to sensitive features.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 18:20:01 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 10:29:44 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Elazar", "Yanai", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1808.06645", "submitter": "George Adam", "authors": "George A. Adam, Petr Smirnov, David Duvenaud, Benjamin Haibe-Kains,\n  Anna Goldenberg", "title": "Stochastic Combinatorial Ensembles for Defending Against Adversarial\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep learning algorithms can be easily fooled with simple adversarial\nexamples. To address the limitations of existing defenses, we devised a\nprobabilistic framework that can generate an exponentially large ensemble of\nmodels from a single model with just a linear cost. This framework takes\nadvantage of neural network depth and stochastically decides whether or not to\ninsert noise removal operators such as VAEs between layers. We show empirically\nthe important role that model gradients have when it comes to determining\ntransferability of adversarial examples, and take advantage of this result to\ndemonstrate that it is possible to train models with limited adversarial attack\ntransferability. Additionally, we propose a detection method based on metric\nlearning in order to detect adversarial examples that have no hope of being\ncleaned of maliciously engineered noise.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 18:39:04 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 16:12:53 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Adam", "George A.", ""], ["Smirnov", "Petr", ""], ["Duvenaud", "David", ""], ["Haibe-Kains", "Benjamin", ""], ["Goldenberg", "Anna", ""]]}, {"id": "1808.06649", "submitter": "Earl Bellinger", "authors": "Earl Patrick Bellinger", "title": "Inverse Problems in Asteroseismology", "comments": "256 pages, 68 figures, Ph.D. dissertation in Computer Science at the\n  Georg-August-University School of Science (GAUSS) G\\\"ottingen undertaken in\n  the context of the International Max Planck Research School for Solar System\n  Science under the supervision of Dr. ir. Saskia Hekker (Group Leader, SAGE\n  Group) and Prof. Dr. Sarbani Basu (Chair, Department of Astronomy, Yale\n  University)", "journal-ref": null, "doi": "10.5281/zenodo.2390979", "report-no": null, "categories": "astro-ph.SR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asteroseismology allows us to probe the internal structure of stars through\ntheir global modes of oscillation. Thanks to missions such as the NASA Kepler\nspace observatory, we now have high-quality asteroseismic data for nearly 100\nsolar-type stars. In this thesis, new techniques to measure the ages, masses,\nand radii of stars are presented, as well as a way to infer their internal\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 18:43:52 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Bellinger", "Earl Patrick", ""]]}, {"id": "1808.06651", "submitter": "Ilya Mironov", "authors": "Vitaly Feldman, Ilya Mironov, Kunal Talwar, Abhradeep Thakurta", "title": "Privacy Amplification by Iteration", "comments": "Extended abstract appears in Foundations of Computer Science (FOCS)\n  2018", "journal-ref": null, "doi": "10.1109/FOCS.2018.00056", "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many commonly used learning algorithms work by iteratively updating an\nintermediate solution using one or a few data points in each iteration.\nAnalysis of differential privacy for such algorithms often involves ensuring\nprivacy of each step and then reasoning about the cumulative privacy cost of\nthe algorithm. This is enabled by composition theorems for differential privacy\nthat allow releasing of all the intermediate results. In this work, we\ndemonstrate that for contractive iterations, not releasing the intermediate\nresults strongly amplifies the privacy guarantees.\n  We describe several applications of this new analysis technique to solving\nconvex optimization problems via noisy stochastic gradient descent. For\nexample, we demonstrate that a relatively small number of non-private data\npoints from the same distribution can be used to close the gap between private\nand non-private convex optimization. In addition, we demonstrate that we can\nachieve guarantees similar to those obtainable using the\nprivacy-amplification-by-sampling technique in several natural settings where\nthat technique cannot be applied.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 18:49:32 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 23:43:40 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Feldman", "Vitaly", ""], ["Mironov", "Ilya", ""], ["Talwar", "Kunal", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "1808.06661", "submitter": "Bin Wang", "authors": "Bin Wang and Yanan Sun and Bing Xue and Mengjie Zhang", "title": "A Hybrid Differential Evolution Approach to Designing Deep Convolutional\n  Neural Networks for Image Classification", "comments": "Accepted by The Australasian Joint Conference on Artificial\n  Intelligence 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have demonstrated their superiority in\nimage classification, and evolutionary computation (EC) methods have recently\nbeen surging to automatically design the architectures of CNNs to save the\ntedious work of manually designing CNNs. In this paper, a new hybrid\ndifferential evolution (DE) algorithm with a newly added crossover operator is\nproposed to evolve the architectures of CNNs of any lengths, which is named\nDECNN. There are three new ideas in the proposed DECNN method. Firstly, an\nexisting effective encoding scheme is refined to cater for variable-length CNN\narchitectures; Secondly, the new mutation and crossover operators are developed\nfor variable-length DE to optimise the hyperparameters of CNNs; Finally, the\nnew second crossover is introduced to evolve the depth of the CNN\narchitectures. The proposed algorithm is tested on six widely-used benchmark\ndatasets and the results are compared to 12 state-of-the-art methods, which\nshows the proposed method is vigorously competitive to the state-of-the-art\nalgorithms. Furthermore, the proposed method is also compared with a method\nusing particle swarm optimisation with a similar encoding strategy named IPPSO,\nand the proposed DECNN outperforms IPPSO in terms of the accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 19:24:45 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 01:32:59 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Wang", "Bin", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1808.06664", "submitter": "Yossi Adi", "authors": "Gabi Shalev, Yossi Adi and Joseph Keshet", "title": "Out-of-Distribution Detection using Multiple Semantic Label\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks are powerful models that attained remarkable results on\na variety of tasks. These models are shown to be extremely efficient when\ntraining and test data are drawn from the same distribution. However, it is not\nclear how a network will act when it is fed with an out-of-distribution\nexample. In this work, we consider the problem of out-of-distribution detection\nin neural networks. We propose to use multiple semantic dense representations\ninstead of sparse representation as the target label. Specifically, we propose\nto use several word representations obtained from different corpora or\narchitectures as target labels. We evaluated the proposed model on computer\nvision, and speech commands detection tasks and compared it to previous\nmethods. Results suggest that our method compares favorably with previous work.\nBesides, we present the efficiency of our approach for detecting wrongly\nclassified and adversarial examples.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 19:29:57 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 16:08:33 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 09:05:22 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Shalev", "Gabi", ""], ["Adi", "Yossi", ""], ["Keshet", "Joseph", ""]]}, {"id": "1808.06670", "submitter": "R Devon Hjelm", "authors": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal,\n  Phil Bachman, Adam Trischler, Yoshua Bengio", "title": "Learning deep representations by mutual information estimation and\n  maximization", "comments": "Accepted as an oral presentation at the International Conference for\n  Learning Representations (ICLR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we perform unsupervised learning of representations by\nmaximizing mutual information between an input and the output of a deep neural\nnetwork encoder. Importantly, we show that structure matters: incorporating\nknowledge about locality of the input to the objective can greatly influence a\nrepresentation's suitability for downstream tasks. We further control\ncharacteristics of the representation by matching to a prior distribution\nadversarially. Our method, which we call Deep InfoMax (DIM), outperforms a\nnumber of popular unsupervised learning methods and competes with\nfully-supervised learning on several classification tasks. DIM opens new\navenues for unsupervised learning of representations and is an important step\ntowards flexible formulations of representation-learning objectives for\nspecific end-goals.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 19:52:51 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 04:02:03 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 17:21:53 GMT"}, {"version": "v4", "created": "Sat, 26 Jan 2019 14:33:00 GMT"}, {"version": "v5", "created": "Fri, 22 Feb 2019 18:38:15 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Hjelm", "R Devon", ""], ["Fedorov", "Alex", ""], ["Lavoie-Marchildon", "Samuel", ""], ["Grewal", "Karan", ""], ["Bachman", "Phil", ""], ["Trischler", "Adam", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1808.06671", "submitter": "Christoph Mayer", "authors": "Christoph Mayer and Radu Timofte", "title": "Adversarial Sampling for Active Learning", "comments": "Accepted at WACV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes asal, a new GAN based active learning method that\ngenerates high entropy samples. Instead of directly annotating the synthetic\nsamples, ASAL searches similar samples from the pool and includes them for\ntraining. Hence, the quality of new samples is high and annotations are\nreliable. To the best of our knowledge, ASAL is the first GAN based AL method\napplicable to multi-class problems that outperforms random sample selection.\nAnother benefit of ASAL is its small run-time complexity (sub-linear) compared\nto traditional uncertainty sampling (linear). We present a comprehensive set of\nexperiments on multiple traditional data sets and show that ASAL outperforms\nsimilar methods and clearly exceeds the established baseline (random sampling).\nIn the discussion section we analyze in which situations ASAL performs best and\nwhy it is sometimes hard to outperform random sample selection.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 19:53:19 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 13:47:45 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mayer", "Christoph", ""], ["Timofte", "Radu", ""]]}, {"id": "1808.06675", "submitter": "Soham Saha", "authors": "Soham Saha, Girish Varma, C.V.Jawahar", "title": "Class2Str: End to End Latent Hierarchy Learning", "comments": "6 pages, ICPR 2018, Beijing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep neural networks for image classification typically consists of a\nconvolutional feature extractor followed by a fully connected classifier\nnetwork. The predicted and the ground truth labels are represented as one hot\nvectors. Such a representation assumes that all classes are equally dissimilar.\nHowever, classes have visual similarities and often form a hierarchy. Learning\nthis latent hierarchy explicitly in the architecture could provide invaluable\ninsights. We propose an alternate architecture to the classifier network called\nthe Latent Hierarchy (LH) Classifier and an end to end learned Class2Str\nmapping which discovers a latent hierarchy of the classes. We show that for\nsome of the best performing architectures on CIFAR and Imagenet datasets, the\nproposed replacement and training by LH classifier recovers the accuracy, with\na fraction of the number of parameters in the classifier part. Compared to the\nprevious work of HDCNN, which also learns a 2 level hierarchy, we are able to\nlearn a hierarchy at an arbitrary number of levels as well as obtain an\naccuracy improvement on the Imagenet classification task over them. We also\nverify that many visually similar classes are grouped together, under the\nlearnt hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 19:58:35 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Saha", "Soham", ""], ["Varma", "Girish", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1808.06684", "submitter": "Merlin Mpoudeu", "authors": "Merlin Mpoudeu", "title": "Use Of Vapnik-Chervonenkis Dimension in Model Selection", "comments": "131 pages, 34 figures, and 44 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation, I derive a new method to estimate the\nVapnik-Chervonenkis Dimension (VCD) for the class of linear functions. This\nmethod is inspired by the technique developed by Vapnik et al. Vapnik et al.\n(1994). My contribution rests on the approximation of the expected maximum\ndifference between two empirical Losses (EMDBTEL). In fact, I use a\ncross-validated form of the error to compute the EMDBTEL, and I make the bound\non the EMDBTEL tighter by minimizing a constant in of its right upper bound. I\nalso derive two bounds for the true unknown risk using the additive (ERM1) and\nthe multiplicative (ERM2) Chernoff bounds. These bounds depend on the estimated\nVCD and the empirical risk. These bounds can be used to perform model selection\nand to declare with high probability, the chosen model will perform better\nwithout making strong assumptions about the data generating process (DG).\n  I measure the accuracy of my technique on simulated datasets and also on\nthree real datasets. The model selection provided by VCD was always as good as\nif not better than the other methods under reasonable conditions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 20:39:24 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Mpoudeu", "Merlin", ""]]}, {"id": "1808.06686", "submitter": "Ekraam Sabir", "authors": "Ekraam Sabir, Wael AbdAlmageed, Yue Wu and Prem Natarajan", "title": "Deep Multimodal Image-Repurposing Detection", "comments": "To be published at ACM Multimeda 2018 (orals)", "journal-ref": null, "doi": "10.1145/3240508.3240707", "report-no": null, "categories": "cs.MM cs.AI cs.CV cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nefarious actors on social media and other platforms often spread rumors and\nfalsehoods through images whose metadata (e.g., captions) have been modified to\nprovide visual substantiation of the rumor/falsehood. This type of modification\nis referred to as image repurposing, in which often an unmanipulated image is\npublished along with incorrect or manipulated metadata to serve the actor's\nulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)\ndataset, a substantially challenging dataset over that which has been\npreviously available to support research into image repurposing detection. The\nnew dataset includes location, person, and organization manipulations on\nreal-world data sourced from Flickr. We also present a novel, end-to-end, deep\nmultimodal learning model for assessing the integrity of an image by combining\ninformation extracted from the image with related information from a knowledge\nbase. The proposed method is compared against state-of-the-art techniques on\nexisting datasets as well as MEIR, where it outperforms existing methods across\nthe board, with AUC improvement up to 0.23.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 20:47:56 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Sabir", "Ekraam", ""], ["AbdAlmageed", "Wael", ""], ["Wu", "Yue", ""], ["Natarajan", "Prem", ""]]}, {"id": "1808.06719", "submitter": "Sercan Arik", "authors": "Sercan O. Arik, Heewoo Jun, and Gregory Diamos", "title": "Fast Spectrogram Inversion using Multi-head Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2018.2880284", "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the multi-head convolutional neural network (MCNN) architecture\nfor waveform synthesis from spectrograms. Nonlinear interpolation in MCNN is\nemployed with transposed convolution layers in parallel heads. MCNN achieves\nmore than an order of magnitude higher compute intensity than commonly-used\niterative algorithms like Griffin-Lim, yielding efficient utilization for\nmodern multi-core processors, and very fast (more than 300x real-time) waveform\nsynthesis. For training of MCNN, we use a large-scale speech recognition\ndataset and losses defined on waveforms that are related to perceptual audio\nquality. We demonstrate that MCNN constitutes a very promising approach for\nhigh-quality speech synthesis, without any iterative algorithms or\nautoregression in computations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 23:19:48 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 04:53:41 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Arik", "Sercan O.", ""], ["Jun", "Heewoo", ""], ["Diamos", "Gregory", ""]]}, {"id": "1808.06725", "submitter": "Jeeheh Oh", "authors": "Jeeheh Oh, Jiaxuan Wang, Jenna Wiens", "title": "Learning to Exploit Invariances in Clinical Time-Series Data using\n  Sequence Transformer Networks", "comments": null, "journal-ref": "PMLR - Machine Learning for Healthcare 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers have started applying convolutional neural networks\n(CNNs) with one-dimensional convolutions to clinical tasks involving\ntime-series data. This is due, in part, to their computational efficiency,\nrelative to recurrent neural networks and their ability to efficiently exploit\ncertain temporal invariances, (e.g., phase invariance). However, it is\nwell-established that clinical data may exhibit many other types of invariances\n(e.g., scaling). While preprocessing techniques, (e.g., dynamic time warping)\nmay successfully transform and align inputs, their use often requires one to\nidentify the types of invariances in advance. In contrast, we propose the use\nof Sequence Transformer Networks, an end-to-end trainable architecture that\nlearns to identify and account for invariances in clinical time-series data.\nApplied to the task of predicting in-hospital mortality, our proposed approach\nachieves an improvement in the area under the receiver operating characteristic\ncurve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our\nresults suggest that a variety of valuable invariances can be learned directly\nfrom the data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 00:13:12 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Oh", "Jeeheh", ""], ["Wang", "Jiaxuan", ""], ["Wiens", "Jenna", ""]]}, {"id": "1808.06733", "submitter": "Chun-Ting Liu", "authors": "Chun Ting Liu, Ming Chuan Yang and Meng Chang Chen", "title": "Wrapped Loss Function for Regularizing Nonconforming Residual\n  Distributions", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output is essential in machine learning that it might suffer from\nnonconforming residual distributions, i.e., the multi-output residual\ndistributions are not conforming to the expected distribution. In this paper,\nwe propose \"Wrapped Loss Function\" to wrap the original loss function to\nalleviate the problem. This wrapped loss function acts just like the original\nloss function that its gradient can be used for backpropagation optimization.\nEmpirical evaluations show wrapped loss function has advanced properties of\nfaster convergence, better accuracy, and improving imbalanced data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 01:39:17 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 03:35:42 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Chun Ting", ""], ["Yang", "Ming Chuan", ""], ["Chen", "Meng Chang", ""]]}, {"id": "1808.06774", "submitter": "Daniel Lopez-Martinez", "authors": "Daniel Lopez-Martinez, Ke Peng, Sarah C. Steele, Arielle J. Lee, David\n  Borsook, Rosalind Picard", "title": "Multi-task multiple kernel machines for personalized pain recognition\n  from functional near-infrared spectroscopy brain signals", "comments": "International Conference on Pattern Recognition (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently there is no validated objective measure of pain. Recent\nneuroimaging studies have explored the feasibility of using functional\nnear-infrared spectroscopy (fNIRS) to measure alterations in brain function in\nevoked and ongoing pain. In this study, we applied multi-task machine learning\nmethods to derive a practical algorithm for pain detection derived from fNIRS\nsignals in healthy volunteers exposed to a painful stimulus. Especially, we\nemployed multi-task multiple kernel learning to account for the inter-subject\nvariability in pain response. Our results support the use of fNIRS and machine\nlearning techniques in developing objective pain detection, and also highlight\nthe importance of adopting personalized analysis in the process.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 05:50:33 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Lopez-Martinez", "Daniel", ""], ["Peng", "Ke", ""], ["Steele", "Sarah C.", ""], ["Lee", "Arielle J.", ""], ["Borsook", "David", ""], ["Picard", "Rosalind", ""]]}, {"id": "1808.06791", "submitter": "Cheng Wang", "authors": "Cheng Wang, Mathias Niepert, Hui Li", "title": "LRMM: Learning to Recommend with Missing Modalities", "comments": "11 pages, EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal learning has shown promising performance in content-based\nrecommendation due to the auxiliary user and item information of multiple\nmodalities such as text and images. However, the problem of incomplete and\nmissing modality is rarely explored and most existing methods fail in learning\na recommendation model with missing or corrupted modalities. In this paper, we\npropose LRMM, a novel framework that mitigates not only the problem of missing\nmodalities but also more generally the cold-start problem of recommender\nsystems. We propose modality dropout (m-drop) and a multimodal sequential\nautoencoder (m-auto) to learn multimodal representations for complementing and\nimputing missing modalities. Extensive experiments on real-world Amazon data\nshow that LRMM achieves state-of-the-art performance on rating prediction\ntasks. More importantly, LRMM is more robust to previous methods in alleviating\ndata-sparsity and the cold-start problem.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 07:45:10 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 12:33:22 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Wang", "Cheng", ""], ["Niepert", "Mathias", ""], ["Li", "Hui", ""]]}, {"id": "1808.06797", "submitter": "Erwan Le Merrer", "authors": "Adel Jaouen and Erwan Le Merrer", "title": "zoNNscan : a boundary-entropy index for zone inspection of neural models", "comments": null, "journal-ref": "Springer CCIS 1379, 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of deep neural network classifiers results in decision\nboundaries which geometry is still not well understood. This is in direct\nrelation with classification problems such as so called adversarial examples.\nWe introduce zoNNscan, an index that is intended to inform on the boundary\nuncertainty (in terms of the presence of other classes) around one given input\ndatapoint. It is based on confidence entropy, and is implemented through\nsampling in the multidimensional ball surrounding that input. We detail the\nzoNNscan index, give an algorithm for approximating it, and finally illustrate\nits benefits on four applications, including two important problems for the\nadoption of deep networks in critical systems: adversarial examples and corner\ncase inputs. We highlight that zoNNscan exhibits significantly higher values\nthan for standard inputs in those two problem classes.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 08:40:16 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Jaouen", "Adel", ""], ["Merrer", "Erwan Le", ""]]}, {"id": "1808.06809", "submitter": "Michele Alberti", "authors": "Michele Alberti, Vinaychandran Pondenkandath, Marcel W\\\"ursch, Manuel\n  Bouillon, Mathias Seuret, Rolf Ingold, Marcus Liwicki", "title": "Are You Tampering With My Data?", "comments": "18 pages", "journal-ref": "European Conference on Computer Vision (ECCV 2018), Workshop on\n  Objectionable Content and Misinformation", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach towards adversarial attacks on neural networks\n(NN), focusing on tampering the data used for training instead of generating\nattacks on trained models. Our network-agnostic method creates a backdoor\nduring training which can be exploited at test time to force a neural network\nto exhibit abnormal behaviour. We demonstrate on two widely used datasets\n(CIFAR-10 and SVHN) that a universal modification of just one pixel per image\nfor all the images of a class in the training set is enough to corrupt the\ntraining procedure of several state-of-the-art deep neural networks causing the\nnetworks to misclassify any images to which the modification is applied. Our\naim is to bring to the attention of the machine learning community, the\npossibility that even learning-based methods that are personally trained on\npublic datasets can be subject to attacks by a skillful adversary.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 09:08:56 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Alberti", "Michele", ""], ["Pondenkandath", "Vinaychandran", ""], ["W\u00fcrsch", "Marcel", ""], ["Bouillon", "Manuel", ""], ["Seuret", "Mathias", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1808.06846", "submitter": "Daiki Adachi", "authors": "Daiki Adachi, Naoto Tsujimoto, Ryosuke Akashi, Synge Todo, Shinji\n  Tsuneyuki", "title": "Search for Common Minima in Joint Optimization of Multiple Cost\n  Functions", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2019.02.004", "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel optimization method, named the Combined Optimization\nMethod (COM), for the joint optimization of two or more cost functions. Unlike\nthe conventional joint optimization schemes, which try to find minima in a\nweighted sum of cost functions, the COM explores search space for common minima\nshared by all the cost functions. Given a set of multiple cost functions that\nhave qualitatively different distributions of local minima with each other, the\nproposed method finds the common minima with a high success rate without the\nhelp of any metaheuristics. As a demonstration, we apply the COM to the crystal\nstructure prediction in materials science. By introducing the concept of data\nassimilation, i.e., adopting the theoretical potential energy of the crystal\nand the crystallinity, which characterizes the agreement with the theoretical\nand experimental X-ray diffraction patterns, as cost functions, we show that\nthe correct crystal structures of Si diamond, low quartz, and low cristobalite\ncan be predicted with significantly higher success rates than the previous\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 11:19:06 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Adachi", "Daiki", ""], ["Tsujimoto", "Naoto", ""], ["Akashi", "Ryosuke", ""], ["Todo", "Synge", ""], ["Tsuneyuki", "Shinji", ""]]}, {"id": "1808.06865", "submitter": "Xingjian Shi", "authors": "Xingjian Shi, Dit-Yan Yeung", "title": "Machine Learning for Spatiotemporal Sequence Forecasting: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal systems are common in the real-world. Forecasting the\nmulti-step future of these spatiotemporal systems based on the past\nobservations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant\nand challenging problem. Although lots of real-world problems can be viewed as\nSTSF and many research works have proposed machine learning based methods for\nthem, no existing work has summarized and compared these methods from a unified\nperspective. This survey aims to provide a systematic review of machine\nlearning for STSF. In this survey, we define the STSF problem and classify it\ninto three subcategories: Trajectory Forecasting of Moving Point Cloud\n(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).\nWe then introduce the two major challenges of STSF: 1) how to learn a model for\nmulti-step forecasting and 2) how to adequately model the spatial and temporal\nstructures. After that, we review the existing works for solving these\nchallenges, including the general learning strategies for multi-step\nforecasting, the classical machine learning based methods for STSF, and the\ndeep learning based methods for STSF. We also compare these methods and point\nout some potential research directions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 12:17:08 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Shi", "Xingjian", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1808.06910", "submitter": "Stanislav Borysov S", "authors": "Stanislav S. Borysov, Jeppe Rich, Francisco C. Pereira", "title": "Scalable Population Synthesis with Deep Generative Modeling", "comments": "27 pages, 15 figures, 4 tables", "journal-ref": "Transport. Res. Part C: Emerg. Technol., 106 (2019), pp. 73-97", "doi": "10.1016/j.trc.2019.07.006", "report-no": null, "categories": "stat.ML cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population synthesis is concerned with the generation of synthetic yet\nrealistic representations of populations. It is a fundamental problem in the\nmodeling of transport where the synthetic populations of micro-agents represent\na key input to most agent-based models. In this paper, a new methodological\nframework for how to 'grow' pools of micro-agents is presented. The model\nframework adopts a deep generative modeling approach from machine learning\nbased on a Variational Autoencoder (VAE). Compared to the previous population\nsynthesis approaches, including Iterative Proportional Fitting (IPF), Gibbs\nsampling and traditional generative models such as Bayesian Networks or Hidden\nMarkov Models, the proposed method allows fitting the full joint distribution\nfor high dimensions. The proposed methodology is compared with a conventional\nGibbs sampler and a Bayesian Network by using a large-scale Danish trip diary.\nIt is shown that, while these two methods outperform the VAE in the\nlow-dimensional case, they both suffer from scalability issues when the number\nof modeled attributes increases. It is also shown that the Gibbs sampler\nessentially replicates the agents from the original sample when the required\nconditional distributions are estimated as frequency tables. In contrast, the\nVAE allows addressing the problem of sampling zeros by generating agents that\nare virtually different from those in the original data but have similar\nstatistical properties. The presented approach can support agent-based modeling\nat all levels by enabling richer synthetic populations with smaller zones and\nmore detailed individual characteristics.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 14:08:44 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 17:11:35 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Borysov", "Stanislav S.", ""], ["Rich", "Jeppe", ""], ["Pereira", "Francisco C.", ""]]}, {"id": "1808.06914", "submitter": "Shivam Singh", "authors": "Shivam Singh and Stuti Pathak", "title": "Segmentation of Microscopy Data for finding Nuclei in Divergent Images", "comments": "7 pages, 7 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:1807.04459, arXiv:1802.10548, arXiv:1807.10165 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Every year millions of people die due to disease of Cancer. Due to its\ninvasive nature it is very complex to cure even in primary stages. Hence, only\nmethod to survive this disease completely is via forecasting by analyzing the\nearly mutation in cells of the patient biopsy. Cell Segmentation can be used to\nfind cell which have left their nuclei. This enables faster cure and high rate\nof survival. Cell counting is a hard, yet tedious task that would greatly\nbenefit from automation. To accomplish this task, segmentation of cells need to\nbe accurate. In this paper, we have improved the learning of training data by\nour network. It can annotate precise masks on test data. we examine the\nstrength of activation functions in medical image segmentation task by\nimproving learning rates by our proposed Carving Technique. Identifying the\ncells nuclei is the starting point for most analyses, identifying nuclei allows\nresearchers to identify each individual cell in a sample, and by measuring how\ncells react to various treatments, the researcher can understand the underlying\nbiological processes at work. Experimental results shows the efficiency of the\nproposed work.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 11:01:48 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 01:45:09 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Singh", "Shivam", ""], ["Pathak", "Stuti", ""]]}, {"id": "1808.06918", "submitter": "Umberto No\\`e", "authors": "Umberto No\\`e and Dirk Husmeier", "title": "On a New Improvement-Based Acquisition Function for Bayesian\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is a popular algorithm for solving challenging\noptimization tasks. It is designed for problems where the objective function is\nexpensive to evaluate, perhaps not available in exact form, without gradient\ninformation and possibly returning noisy values. Different versions of the\nalgorithm vary in the choice of the acquisition function, which recommends the\npoint to query the objective at next. Initially, researchers focused on\nimprovement-based acquisitions, while recently the attention has shifted to\nmore computationally expensive information-theoretical measures. In this paper\nwe present two major contributions to the literature. First, we propose a new\nimprovement-based acquisition function that recommends query points where the\nimprovement is expected to be high with high confidence. The proposed algorithm\nis evaluated on a large set of benchmark functions from the global optimization\nliterature, where it turns out to perform at least as well as current\nstate-of-the-art acquisition functions, and often better. This suggests that it\nis a powerful default choice for BO. The novel policy is then compared to\nwidely used global optimization solvers in order to confirm that BO methods\nreduce the computational costs of the optimization by keeping the number of\nfunction evaluations small. The second main contribution represents an\napplication to precision medicine, where the interest lies in the estimation of\nparameters of a partial differential equations model of the human pulmonary\nblood circulation system. Once inferred, these parameters can help clinicians\nin diagnosing a patient with pulmonary hypertension without going through the\nstandard invasive procedure of right heart catheterization, which can lead to\nside effects and complications (e.g. severe pain, internal bleeding,\nthrombosis).\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 14:25:09 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["No\u00e8", "Umberto", ""], ["Husmeier", "Dirk", ""]]}, {"id": "1808.06934", "submitter": "Giuseppe Marra", "authors": "Alessandro Betti and Marco Gori and Giuseppe Marra", "title": "Backpropagation and Biological Plausibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By and large, Backpropagation (BP) is regarded as one of the most important\nneural computation algorithms at the basis of the progress in machine learning,\nincluding the recent advances in deep learning. However, its computational\nstructure has been the source of many debates on its arguable biological\nplausibility. In this paper, it is shown that when framing supervised learning\nin the Lagrangian framework, while one can see a natural emergence of\nBackpropagation, biologically plausible local algorithms can also be devised\nthat are based on the search for saddle points in the learning adjoint space\ncomposed of weights, neural outputs, and Lagrangian multipliers. This might\nopen the doors to a truly novel class of learning algorithms where, because of\nthe introduction of the notion of support neurons, the optimization scheme also\nplays a fundamental role in the construction of the architecture.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 14:41:56 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Betti", "Alessandro", ""], ["Gori", "Marco", ""], ["Marra", "Giuseppe", ""]]}, {"id": "1808.06935", "submitter": "Luca Messina Dr.", "authors": "Luca Messina, Alessio Quaglino, Alexandra Goryaeva, Mihai-Cosmin\n  Marinica, Christophe Domain, Nicolas Castin, Giovanni Bonny, Rolf Krause", "title": "Smart energy models for atomistic simulations using a DFT-driven\n  multifidelity approach", "comments": "Proceedings of the COSIRES 2018 conference, submitted for peer-review\n  in Nuclear Instruments and Methods in Physics Research B: Beam Interactions\n  with Materials and Atoms", "journal-ref": "Nucl. Instrum. Methods Phys. Res. B 483, 15-21 (2020)", "doi": "10.1016/j.nimb.2020.09.011", "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliability of atomistic simulations depends on the quality of the\nunderlying energy models providing the source of physical information, for\ninstance for the calculation of migration barriers in atomistic Kinetic Monte\nCarlo simulations. Accurate (high-fidelity) methods are often available, but\nsince they are usually computationally expensive, they must be replaced by less\naccurate (low-fidelity) models that introduce some degrees of approximation.\nMachine-learning techniques such as artificial neural networks are usually\nemployed to work around this limitation and extract the needed parameters from\nlarge databases of high-fidelity data, but the latter are often computationally\nexpensive to produce. This work introduces an alternative method based on the\nmultifidelity approach, where correlations between high-fidelity and\nlow-fidelity outputs are exploited to make an educated guess of the\nhigh-fidelity outcome based only on quick low-fidelity estimations, hence\nwithout the need of running full expensive high-fidelity calculations. With\nrespect to neural networks, this approach is expected to require less training\ndata because of the lower amount of fitting parameters involved. The method is\ntested on the prediction of ab initio formation and migration energies of\nvacancy diffusion in iron-copper alloys, and compared with the neural networks\ntrained on the same database.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 14:43:13 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 10:44:14 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Messina", "Luca", ""], ["Quaglino", "Alessio", ""], ["Goryaeva", "Alexandra", ""], ["Marinica", "Mihai-Cosmin", ""], ["Domain", "Christophe", ""], ["Castin", "Nicolas", ""], ["Bonny", "Giovanni", ""], ["Krause", "Rolf", ""]]}, {"id": "1808.06940", "submitter": "Marin Toromanoff", "authors": "Marin Toromanoff, Emilie Wirbel, Fr\\'ed\\'eric Wilhelm, Camilo\n  Vejarano, Xavier Perrotton, Fabien Moutarde", "title": "End to End Vehicle Lateral Control Using a Single Fisheye Camera", "comments": "7 pages paper accepted at IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are commonly used to control the steering angle\nfor autonomous cars. Most of the time, multiple long range cameras are used to\ngenerate lateral failure cases. In this paper we present a novel model to\ngenerate this data and label augmentation using only one short range fisheye\ncamera. We present our simulator and how it can be used as a consistent metric\nfor lateral end-to-end control evaluation. Experiments are conducted on a\ncustom dataset corresponding to more than 10000 km and 200 hours of open road\ndriving. Finally we evaluate this model on real world driving scenarios, open\nroad and a custom test track with challenging obstacle avoidance and sharp\nturns. In our simulator based on real-world videos, the final model was capable\nof more than 99% autonomy on urban road\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 09:25:30 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Toromanoff", "Marin", ""], ["Wirbel", "Emilie", ""], ["Wilhelm", "Fr\u00e9d\u00e9ric", ""], ["Vejarano", "Camilo", ""], ["Perrotton", "Xavier", ""], ["Moutarde", "Fabien", ""]]}, {"id": "1808.06942", "submitter": "Ignacio Ramirez", "authors": "Ignacio Francisco Ram\\'irez Paulino", "title": "PACO: Global Signal Restoration via PAtch COnsensus", "comments": "Submitted to Siam SIIMS for peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many signal processing algorithms break the target signal into overlapping\nsegments (also called windows, or patches), process them separately, and then\nstitch them back into place to produce a unified output. At the overlaps, the\nfinal value of those samples that are estimated more than once needs to be\ndecided in some way. Averaging, the simplest approach, often leads to\nunsatisfactory results. Significant work has been devoted to this issue in\nrecent years. Several works explore the idea of a weighted average of the\noverlapped patches and/or pixels; others promote agreement (consensus) between\nthe patches at their intersections. Agreement can be either encouraged or\nimposed as a hard constraint. This work develops on the latter case. The result\nis a variational signal processing framework, named PACO, which features a\nnumber of appealing theoretical and practical properties. The PACO framework\nconsists of a variational formulation that fits a wide variety of problems, and\na general ADMMbased algorithm for minimizing the resulting energies. As a\nbyproduct, we show that the consensus step of the algorithm, which is the main\nbottleneck of similar methods, can be solved efficiently and easily for any\narbitrary patch decomposition scheme. We demonstrate the flexibility and power\nof PACO on three different problems: image inpainting (which we have already\ncovered in previous works), image denoising, and contrast enhancement, using\ndifferent cost functions including Laplacian and Gaussian Mixture Models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 15:20:46 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 20:48:24 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 18:02:53 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Paulino", "Ignacio Francisco Ram\u00edrez", ""]]}, {"id": "1808.06992", "submitter": "Mahesh Balasubramanian", "authors": "Mahesh Balasubramanian, Trevor Ruiz, Brandon Cook, Sharmodeep\n  Bhattacharyya, Prabhat, Aviral Shrivastava and Kristofer Bouchard", "title": "Optimizing the Union of Intersections LASSO ($UoI_{LASSO}$) and Vector\n  Autoregressive ($UoI_{VAR}$) Algorithms for Improved Statistical Estimation\n  at Scale", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of scientific data of increasing size and complexity requires\nstatistical machine learning methods that are both interpretable and\npredictive. Union of Intersections (UoI), a recently developed framework, is a\ntwo-step approach that separates model selection and model estimation. A linear\nregression algorithm based on UoI, $UoI_{LASSO}$, simultaneously achieves low\nfalse positives and low false negative feature selection as well as low bias\nand low variance estimates. Together, these qualities make the results both\npredictive and interpretable. In this paper, we optimize the $UoI_{LASSO}$\nalgorithm for single-node execution on NERSC's Cori Knights Landing, a Xeon Phi\nbased supercomputer. We then scale $UoI_{LASSO}$ to execute on cores ranging\nfrom 68-278,528 cores on a range of dataset sizes demonstrating the weak and\nstrong scaling of the implementation. We also implement a variant of\n$UoI_{LASSO}$, $UoI_{VAR}$ for vector autoregressive models, to analyze high\ndimensional time-series data. We perform single node optimization and\nmulti-node scaling experiments for $UoI_{VAR}$ to demonstrate the effectiveness\nof the algorithm for weak and strong scaling. Our implementations enable to use\nestimate the largest VAR model (1000 nodes) we are aware of, and apply it to\nlarge neurophysiology data 192 nodes).\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 16:14:55 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Balasubramanian", "Mahesh", ""], ["Ruiz", "Trevor", ""], ["Cook", "Brandon", ""], ["Bhattacharyya", "Sharmodeep", ""], ["Prabhat", "", ""], ["Shrivastava", "Aviral", ""], ["Bouchard", "Kristofer", ""]]}, {"id": "1808.06996", "submitter": "Zhuoran Yang", "authors": "Jianqing Fan, Han Liu, Zhaoran Wang, Zhuoran Yang", "title": "Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models\n  and Phase Retrieval", "comments": "75 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental tradeoffs between statistical accuracy and\ncomputational tractability in the analysis of high dimensional heterogeneous\ndata. As examples, we study sparse Gaussian mixture model, mixture of sparse\nlinear regressions, and sparse phase retrieval model. For these models, we\nexploit an oracle-based computational model to establish conjecture-free\ncomputationally feasible minimax lower bounds, which quantify the minimum\nsignal strength required for the existence of any algorithm that is both\ncomputationally tractable and statistically accurate. Our analysis shows that\nthere exist significant gaps between computationally feasible minimax risks and\nclassical ones. These gaps quantify the statistical price we must pay to\nachieve computational tractability in the presence of data heterogeneity. Our\nresults cover the problems of detection, estimation, support recovery, and\nclustering, and moreover, resolve several conjectures of Azizyan et al. (2013,\n2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our\nresults reveal a new but counter-intuitive phenomenon in heterogeneous data\nanalysis that more data might lead to less computation complexity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 16:16:46 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Fan", "Jianqing", ""], ["Liu", "Han", ""], ["Wang", "Zhaoran", ""], ["Yang", "Zhuoran", ""]]}, {"id": "1808.07018", "submitter": "Ivana Bala\\v{z}evi\\'c", "authors": "Ivana Bala\\v{z}evi\\'c, Carl Allen and Timothy M. Hospedales", "title": "Hypernetwork Knowledge Graph Embeddings", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30493-5_52", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs are graphical representations of large databases of facts,\nwhich typically suffer from incompleteness. Inferring missing relations (links)\nbetween entities (nodes) is the task of link prediction. A recent\nstate-of-the-art approach to link prediction, ConvE, implements a convolutional\nneural network to extract features from concatenated subject and relation\nvectors. Whilst results are impressive, the method is unintuitive and poorly\nunderstood. We propose a hypernetwork architecture that generates simplified\nrelation-specific convolutional filters that (i) outperforms ConvE and all\nprevious approaches across standard datasets; and (ii) can be framed as tensor\nfactorization and thus set within a well established family of factorization\nmodels for link prediction. We thus demonstrate that convolution simply offers\na convenient computational means of introducing sparsity and parameter tying to\nfind an effective trade-off between non-linear expressiveness and the number of\nparameters to learn.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 17:05:28 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 17:50:05 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 14:49:00 GMT"}, {"version": "v4", "created": "Sun, 30 Jun 2019 17:49:55 GMT"}, {"version": "v5", "created": "Mon, 15 Jul 2019 11:22:00 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Bala\u017eevi\u0107", "Ivana", ""], ["Allen", "Carl", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1808.07036", "submitter": "Mark Yatskar", "authors": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin\n  Choi, Percy Liang, Luke Zettlemoyer", "title": "QuAC : Question Answering in Context", "comments": "EMNLP Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present QuAC, a dataset for Question Answering in Context that contains\n14K information-seeking QA dialogs (100K questions in total). The dialogs\ninvolve two crowd workers: (1) a student who poses a sequence of freeform\nquestions to learn as much as possible about a hidden Wikipedia text, and (2) a\nteacher who answers the questions by providing short excerpts from the text.\nQuAC introduces challenges not found in existing machine comprehension\ndatasets: its questions are often more open-ended, unanswerable, or only\nmeaningful within the dialog context, as we show in a detailed qualitative\nevaluation. We also report results for a number of reference models, including\na recently state-of-the-art reading comprehension architecture extended to\nmodel dialog context. Our best model underperforms humans by 20 F1, suggesting\nthat there is significant room for future work on this data. Dataset, baseline,\nand leaderboard available at http://quac.ai.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 17:46:12 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 00:50:43 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 00:58:48 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Choi", "Eunsol", ""], ["He", "He", ""], ["Iyyer", "Mohit", ""], ["Yatskar", "Mark", ""], ["Yih", "Wen-tau", ""], ["Choi", "Yejin", ""], ["Liang", "Percy", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1808.07042", "submitter": "Siva Reddy", "authors": "Siva Reddy and Danqi Chen and Christopher D. Manning", "title": "CoQA: A Conversational Question Answering Challenge", "comments": "TACL (presented at NAACL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans gather information by engaging in conversations involving a series of\ninterconnected questions and answers. For machines to assist in information\ngathering, it is therefore essential to enable them to answer conversational\nquestions. We introduce CoQA, a novel dataset for building Conversational\nQuestion Answering systems. Our dataset contains 127k questions with answers,\nobtained from 8k conversations about text passages from seven diverse domains.\nThe questions are conversational, and the answers are free-form text with their\ncorresponding evidence highlighted in the passage. We analyze CoQA in depth and\nshow that conversational questions have challenging phenomena not present in\nexisting reading comprehension datasets, e.g., coreference and pragmatic\nreasoning. We evaluate strong conversational and reading comprehension models\non CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points\nbehind human performance (88.8%), indicating there is ample room for\nimprovement. We launch CoQA as a challenge to the community at\nhttp://stanfordnlp.github.io/coqa/\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 17:52:02 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 20:50:21 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Reddy", "Siva", ""], ["Chen", "Danqi", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1808.07049", "submitter": "Albert Ierusalem", "authors": "Albert Ierusalem", "title": "Catastrophic Importance of Catastrophic Forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes some of the possibilities of artificial neural networks\nthat open up after solving the problem of catastrophic forgetting. A simple\nmodel and reinforcement learning applications of existing methods are also\nproposed.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 21:49:13 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Ierusalem", "Albert", ""]]}, {"id": "1808.07089", "submitter": "Fernando De Aguiar Neto", "authors": "Fernando S. Aguiar Neto, Arthur F. da Costa and Marcelo G. Manzato", "title": "CoBaR: Confidence-Based Recommender", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighborhood-based collaborative filtering algorithms usually adopt a fixed\nneighborhood size for every user or item, although groups of users or items may\nhave different lengths depending on users' preferences. In this paper, we\npropose an extension to a non-personalized recommender based on confidence\nintervals and hierarchical clustering to generate groups of users with optimal\nsizes. The evaluation shows that the proposed technique outperformed the\ntraditional recommender algorithms in four publicly available datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 19:14:16 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Neto", "Fernando S. Aguiar", ""], ["da Costa", "Arthur F.", ""], ["Manzato", "Marcelo G.", ""]]}, {"id": "1808.07168", "submitter": "Stanimire Tomov", "authors": "Nathalie-Sofia Tomov and Stanimire Tomov", "title": "On Deep Neural Networks for Detecting Heart Disease", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart disease is the leading cause of death, and experts estimate that\napproximately half of all heart attacks and strokes occur in people who have\nnot been flagged as \"at risk.\" Thus, there is an urgent need to improve the\naccuracy of heart disease diagnosis. To this end, we investigate the potential\nof using data analysis, and in particular the design and use of deep neural\nnetworks (DNNs) for detecting heart disease based on routine clinical data. Our\nmain contribution is the design, evaluation, and optimization of DNN\narchitectures of increasing depth for heart disease diagnosis. This work led to\nthe discovery of a novel five layer DNN architecture - named Heart Evaluation\nfor Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields\nbest prediction accuracy. HEARO-5's design employs regularization optimization\nand automatically deals with missing data and/or data outliers. To evaluate and\ntune the architectures we use k-way cross-validation as well as Matthews\ncorrelation coefficient (MCC) to measure the quality of our classifications.\nThe study is performed on the publicly available Cleveland dataset of medical\ninformation, and we are making our developments open source, to further\nfacilitate openness and research on the use of DNNs in medicine. The HEARO-5\narchitecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms\ncurrently published research in the area.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 00:51:57 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Tomov", "Nathalie-Sofia", ""], ["Tomov", "Stanimire", ""]]}, {"id": "1808.07169", "submitter": "Ryo Karakida", "authors": "Shun-ichi Amari and Ryo Karakida and Masafumi Oizumi", "title": "Statistical Neurodynamics of Deep Networks: Geometry of Signal Spaces", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical neurodynamics studies macroscopic behaviors of randomly connected\nneural networks. We consider a deep layered feedforward network where input\nsignals are processed layer by layer. The manifold of input signals is embedded\nin a higher dimensional manifold of the next layer as a curved submanifold,\nprovided the number of neurons is larger than that of inputs. We show\ngeometrical features of the embedded manifold, proving that the manifold\nenlarges or shrinks locally isotropically so that it is always embedded\nconformally. We study the curvature of the embedded manifold. The scalar\ncurvature converges to a constant or diverges to infinity slowly. The distance\nbetween two signals also changes, converging eventually to a stable fixed\nvalue, provided both the number of neurons in a layer and the number of layers\ntend to infinity. This causes a problem, since when we consider a curve in the\ninput space, it is mapped as a continuous curve of fractal nature, but our\ntheory contradictorily suggests that the curve eventually converges to a\ndiscrete set of equally spaced points. In reality, the numbers of neurons and\nlayers are finite and thus, it is expected that the finite size effect causes\nthe discrepancies between our theory and reality. We need to further study the\ndiscrepancies to understand their implications on information processing.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 00:57:41 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Amari", "Shun-ichi", ""], ["Karakida", "Ryo", ""], ["Oizumi", "Masafumi", ""]]}, {"id": "1808.07172", "submitter": "Ryo Karakida", "authors": "Shun-ichi Amari and Ryo Karakida and Masafumi Oizumi", "title": "Fisher Information and Natural Gradient Learning of Random Deep Networks", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep neural network is a hierarchical nonlinear model transforming input\nsignals to output signals. Its input-output relation is considered to be\nstochastic, being described for a given input by a parameterized conditional\nprobability distribution of outputs. The space of parameters consisting of\nweights and biases is a Riemannian manifold, where the metric is defined by the\nFisher information matrix. The natural gradient method uses the steepest\ndescent direction in a Riemannian manifold, so it is effective in learning,\navoiding plateaus. It requires inversion of the Fisher information matrix,\nhowever, which is practically impossible when the matrix has a huge number of\ndimensions. Many methods for approximating the natural gradient have therefore\nbeen introduced. The present paper uses statistical neurodynamical method to\nreveal the properties of the Fisher information matrix in a net of random\nconnections under the mean field approximation. We prove that the Fisher\ninformation matrix is unit-wise block diagonal supplemented by small order\nterms of off-block-diagonal elements, which provides a justification for the\nquasi-diagonal natural gradient method by Y. Ollivier. A unitwise\nblock-diagonal Fisher metrix reduces to the tensor product of the Fisher\ninformation matrices of single units. We further prove that the Fisher\ninformation matrix of a single unit has a simple reduced form, a sum of a\ndiagonal matrix and a rank 2 matrix of weight-bias correlations. We obtain the\ninverse of Fisher information explicitly. We then have an explicit form of the\nnatural gradient, without relying on the numerical matrix inversion, which\ndrastically speeds up stochastic gradient learning.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 01:04:07 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Amari", "Shun-ichi", ""], ["Karakida", "Ryo", ""], ["Oizumi", "Masafumi", ""]]}, {"id": "1808.07187", "submitter": "Xingxing Zhang", "authors": "Xingxing Zhang, Mirella Lapata, Furu Wei and Ming Zhou", "title": "Neural Latent Extractive Document Summarization", "comments": "to appear in EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive summarization models require sentence-level labels, which are\nusually created heuristically (e.g., with rule-based methods) given that most\nsummarization datasets only have document-summary pairs. Since these labels\nmight be suboptimal, we propose a latent variable extractive model where\nsentences are viewed as latent variables and sentences with activated variables\nare used to infer gold summaries. During training the loss comes\n\\emph{directly} from gold summaries. Experiments on the CNN/Dailymail dataset\nshow that our model improves over a strong extractive baseline trained on\nheuristically approximated labels and also performs competitively to several\nrecent models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 02:18:40 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 06:27:09 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Zhang", "Xingxing", ""], ["Lapata", "Mirella", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "1808.07209", "submitter": "Yadan Luo", "authors": "Yadan Luo, Ziwei Wang, Zi Huang, Yang Yang, Cong Zhao", "title": "Coarse-to-Fine Annotation Enrichment for Semantic Segmentation Learning", "comments": "CIKM 2018 International Conference on Information and Knowledge\n  Management", "journal-ref": null, "doi": "10.1145/3269206.3271672", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Rich high-quality annotated data is critical for semantic segmentation\nlearning, yet acquiring dense and pixel-wise ground-truth is both labor- and\ntime-consuming. Coarse annotations (e.g., scribbles, coarse polygons) offer an\neconomical alternative, with which training phase could hardly generate\nsatisfactory performance unfortunately. In order to generate high-quality\nannotated data with a low time cost for accurate segmentation, in this paper,\nwe propose a novel annotation enrichment strategy, which expands existing\ncoarse annotations of training data to a finer scale. Extensive experiments on\nthe Cityscapes and PASCAL VOC 2012 benchmarks have shown that the neural\nnetworks trained with the enriched annotations from our framework yield a\nsignificant improvement over that trained with the original coarse labels. It\nis highly competitive to the performance obtained by using human annotated\ndense annotations. The proposed method also outperforms among other\nstate-of-the-art weakly-supervised segmentation methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 03:55:33 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Luo", "Yadan", ""], ["Wang", "Ziwei", ""], ["Huang", "Zi", ""], ["Yang", "Yang", ""], ["Zhao", "Cong", ""]]}, {"id": "1808.07216", "submitter": "Xiaoyu Liu", "authors": "Xiaoyu Liu, Jie Chen, Joel Vaughan, Vijayan Nair, Agus Sudjianto", "title": "Model Interpretation: A Unified Derivative-based Framework for\n  Nonparametric Regression and Supervised Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpreting a nonparametric regression model with many predictors is known\nto be a challenging problem. There has been renewed interest in this topic due\nto the extensive use of machine learning algorithms and the difficulty in\nunderstanding and explaining their input-output relationships. This paper\ndevelops a unified framework using a derivative-based approach for existing\ntools in the literature, including the partial-dependence plots, marginal plots\nand accumulated effects plots. It proposes a new interpretation technique\ncalled the accumulated total derivative effects plot and demonstrates how its\ncomponents can be used to develop extensive insights in complex regression\nmodels with correlated predictors. The techniques are illustrated through\nsimulation results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 04:24:30 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 16:17:09 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Liu", "Xiaoyu", ""], ["Chen", "Jie", ""], ["Vaughan", "Joel", ""], ["Nair", "Vijayan", ""], ["Sudjianto", "Agus", ""]]}, {"id": "1808.07217", "submitter": "Tao Lin", "authors": "Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, Martin Jaggi", "title": "Don't Use Large Mini-Batches, Use Local SGD", "comments": "To appear in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mini-batch stochastic gradient methods (SGD) are state of the art for\ndistributed training of deep neural networks. Drastic increases in the\nmini-batch sizes have lead to key efficiency and scalability gains in recent\nyears. However, progress faces a major roadblock, as models trained with large\nbatches often do not generalize well, i.e. they do not show good accuracy on\nnew data. As a remedy, we propose a \\emph{post-local} SGD and show that it\nsignificantly improves the generalization performance compared to large-batch\ntraining on standard benchmarks while enjoying the same efficiency\n(time-to-accuracy) and scalability. We further provide an extensive study of\nthe communication efficiency vs. performance trade-offs associated with a host\nof \\emph{local SGD} variants.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 04:50:55 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 13:47:52 GMT"}, {"version": "v3", "created": "Sun, 21 Oct 2018 14:23:33 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2019 07:30:54 GMT"}, {"version": "v5", "created": "Wed, 5 Jun 2019 11:39:13 GMT"}, {"version": "v6", "created": "Mon, 17 Feb 2020 11:42:10 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Lin", "Tao", ""], ["Stich", "Sebastian U.", ""], ["Patel", "Kumar Kshitij", ""], ["Jaggi", "Martin", ""]]}, {"id": "1808.07220", "submitter": "Brandon Da Silva", "authors": "Brandon Da Silva", "title": "Approximating Poker Probabilities with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many poker systems, whether created with heuristics or machine learning, rely\non the probability of winning as a key input. However calculating the precise\nprobability using combinatorics is an intractable problem, so instead we\napproximate it. Monte Carlo simulation is an effective technique that can be\nused to approximate the probability that a player will win and/or tie a hand.\nHowever, without the use of a memory-intensive lookup table or a supercomputer,\nit becomes infeasible to run millions of times when training an agent with\nself-play. To combat the space-time tradeoff, we use deep learning to\napproximate the probabilities obtained from the Monte Carlo simulation with\nhigh accuracy. The learned model proves to be a lightweight alternative to\nMonte Carlo simulation, which ultimately allows us to use the probabilities as\ninputs during self-play efficiently. The source code and optimized neural\nnetwork can be found at\nhttps://github.com/brandinho/Poker-Probability-Approximation\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 05:14:41 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 02:21:56 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Da Silva", "Brandon", ""]]}, {"id": "1808.07226", "submitter": "Vishesh Jain", "authors": "Vishesh Jain, Frederic Koehler, Andrej Risteski", "title": "Mean-field approximation, convex hierarchies, and the optimality of\n  correlation rounding: a unified perspective", "comments": "This version: minor formatting changes, added grant acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The free energy is a key quantity of interest in Ising models, but\nunfortunately, computing it in general is computationally intractable. Two\npopular (variational) approximation schemes for estimating the free energy of\ngeneral Ising models (in particular, even in regimes where correlation decay\ndoes not hold) are: (i) the mean-field approximation with roots in statistical\nphysics, which estimates the free energy from below, and (ii) hierarchies of\nconvex relaxations with roots in theoretical computer science, which estimate\nthe free energy from above. We show, surprisingly, that the tight regime for\nboth methods to compute the free energy to leading order is identical.\n  More precisely, we show that the mean-field approximation is within\n$O((n\\|J\\|_{F})^{2/3})$ of the free energy, where $\\|J\\|_F$ denotes the\nFrobenius norm of the interaction matrix of the Ising model. This\nsimultaneously subsumes both the breakthrough work of Basak and Mukherjee, who\nshowed the tight result that the mean-field approximation is within $o(n)$\nwhenever $\\|J\\|_{F} = o(\\sqrt{n})$, as well as the work of Jain, Koehler, and\nMossel, who gave the previously best known non-asymptotic bound of\n$O((n\\|J\\|_{F})^{2/3}\\log^{1/3}(n\\|J\\|_{F}))$. We give a simple, algorithmic\nproof of this result using a convex relaxation proposed by Risteski based on\nthe Sherali-Adams hierarchy, automatically giving sub-exponential time\napproximation schemes for the free energy in this entire regime. Our\nalgorithmic result is tight under Gap-ETH.\n  We furthermore combine our techniques with spin glass theory to prove (in a\nstrong sense) the optimality of correlation rounding, refuting a recent\nconjecture of Allen, O'Donnell, and Zhou. Finally, we give the tight\ngeneralization of all of these results to $k$-MRFs, capturing as a special case\nprevious work on approximating MAX-$k$-CSP.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 05:43:16 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 02:06:06 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Jain", "Vishesh", ""], ["Koehler", "Frederic", ""], ["Risteski", "Andrej", ""]]}, {"id": "1808.07233", "submitter": "Renqian Luo", "authors": "Renqian Luo and Fei Tian and Tao Qin and Enhong Chen and Tie-Yan Liu", "title": "Neural Architecture Optimization", "comments": "NeurIPS 2018. Code available at: https://github.com/renqianluo/NAO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic neural architecture design has shown its potential in discovering\npowerful neural network architectures. Existing methods, no matter based on\nreinforcement learning or evolutionary algorithms (EA), conduct architecture\nsearch in a discrete space, which is highly inefficient. In this paper, we\npropose a simple and efficient method to automatic neural architecture design\nbased on continuous optimization. We call this new approach neural architecture\noptimization (NAO). There are three key components in our proposed approach:\n(1) An encoder embeds/maps neural network architectures into a continuous\nspace. (2) A predictor takes the continuous representation of a network as\ninput and predicts its accuracy. (3) A decoder maps a continuous representation\nof a network back to its architecture. The performance predictor and the\nencoder enable us to perform gradient based optimization in the continuous\nspace to find the embedding of a new architecture with potentially better\naccuracy. Such a better embedding is then decoded to a network by the decoder.\nExperiments show that the architecture discovered by our method is very\ncompetitive for image classification task on CIFAR-10 and language modeling\ntask on PTB, outperforming or on par with the best results of previous\narchitecture search methods with a significantly reduction of computational\nresources. Specifically we obtain 1.93% test set error rate for CIFAR-10 image\nclassification task and 56.0 test set perplexity of PTB language modeling task.\nFurthermore, combined with the recent proposed weight sharing mechanism, we\ndiscover powerful architecture on CIFAR-10 (with error rate 2.93%) and on PTB\n(with test set perplexity 56.6), with very limited computational resources\n(less than 10 GPU hours) for both tasks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 06:07:03 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 12:02:37 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 14:10:46 GMT"}, {"version": "v4", "created": "Wed, 31 Oct 2018 04:20:39 GMT"}, {"version": "v5", "created": "Wed, 4 Sep 2019 12:53:35 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Luo", "Renqian", ""], ["Tian", "Fei", ""], ["Qin", "Tao", ""], ["Chen", "Enhong", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1808.07243", "submitter": "Oren Zeev Ben Mordehai", "authors": "Oren Zeev-Ben-Mordehai, Wouter Duivesteijn, Mykola Pechenizkiy", "title": "Controversy Rules - Discovering Regions Where Classifiers (Dis-)Agree\n  Exceptionally", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding regions for which there is higher controversy among different\nclassifiers is insightful with regards to the domain and our models. Such\nevaluation can falsify assumptions, assert some, or also, bring to the\nattention unknown phenomena. The present work describes an algorithm, which is\nbased on the Exceptional Model Mining framework, and enables that kind of\ninvestigations. We explore several public datasets and show the usefulness of\nthis approach in classification tasks. We show in this paper a few interesting\nobservations about those well explored datasets, some of which are general\nknowledge, and other that as far as we know, were not reported before.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 06:55:44 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Zeev-Ben-Mordehai", "Oren", ""], ["Duivesteijn", "Wouter", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1808.07249", "submitter": "Alexander Jung", "authors": "A. Jung and N. Vesselinova", "title": "Analysis of Network Lasso for Semi-Supervised Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply network Lasso to semi-supervised regression problems involving\nnetwork structured data. This approach lends quite naturally to highly scalable\nlearning algorithms in the form of message passing over an empirical graph\nwhich represents the network structure of the data. By using a simple\nnon-parametric regression model, which is motivated by a clustering hypothesis,\nwe provide an analysis of the estimation error incurred by network Lasso. This\nanalysis reveals conditions on the the network structure and the available\ntraining data which guarantee network Lasso to be accurate. Remarkably, the\naccuracy of network Lasso is related to the existence of sufficiently large\nnetwork flows over the empirical graph. Thus, our analysis reveals a connection\nbetween network Lasso and maximum flow problems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 07:20:04 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 18:02:24 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Jung", "A.", ""], ["Vesselinova", "N.", ""]]}, {"id": "1808.07251", "submitter": "Murat Ali Bayir", "authors": "Murat Ali Bayir, Mingsen Xu, Yaojia Zhu, Yifan Shi", "title": "Genie: An Open Box Counterfactual Policy Estimator for Optimizing\n  Sponsored Search Marketplace", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an offline counterfactual policy estimation\nframework called Genie to optimize Sponsored Search Marketplace. Genie employs\nan open box simulation engine with click calibration model to compute the KPI\nimpact of any modification to the system. From the experimental results on Bing\ntraffic, we showed that Genie performs better than existing observational\napproaches that employs randomized experiments for traffic slices that have\nfrequent policy updates. We also show that Genie can be used to tune completely\nnew policies efficiently without creating risky randomized experiments due to\ncold start problem. As time of today, Genie hosts more than 10000 optimization\njobs yearly which runs more than 30 Million processing node hours of big data\njobs for Bing Ads. For the last 3 years, Genie has been proven to be the one of\nthe major platforms to optimize Bing Ads Marketplace due to its reliability\nunder frequent policy changes and its efficiency to minimize risks in real\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 07:27:26 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Bayir", "Murat Ali", ""], ["Xu", "Mingsen", ""], ["Zhu", "Yaojia", ""], ["Shi", "Yifan", ""]]}, {"id": "1808.07258", "submitter": "Chieh Hubert Lin", "authors": "Chia-Che Chang, Chieh Hubert Lin, Che-Rung Lee, Da-Cheng Juan, Wei\n  Wei, Hwann-Tzong Chen", "title": "Escaping from Collapsing Modes in a Constrained Space", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) often suffer from unpredictable\nmode-collapsing during training. We study the issue of mode collapse of\nBoundary Equilibrium Generative Adversarial Network (BEGAN), which is one of\nthe state-of-the-art generative models. Despite its potential of generating\nhigh-quality images, we find that BEGAN tends to collapse at some modes after a\nperiod of training. We propose a new model, called \\emph{BEGAN with a\nConstrained Space} (BEGAN-CS), which includes a latent-space constraint in the\nloss function. We show that BEGAN-CS can significantly improve training\nstability and suppress mode collapse without either increasing the model\ncomplexity or degrading the image quality. Further, we visualize the\ndistribution of latent vectors to elucidate the effect of latent-space\nconstraint. The experimental results show that our method has additional\nadvantages of being able to train on small datasets and to generate images\nsimilar to a given real image yet with variations of designated attributes\non-the-fly.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 07:51:22 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Chang", "Chia-Che", ""], ["Lin", "Chieh Hubert", ""], ["Lee", "Che-Rung", ""], ["Juan", "Da-Cheng", ""], ["Wei", "Wei", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "1808.07260", "submitter": "Katsuyuki Hagiwara", "authors": "Katsuyuki Hagiwara", "title": "On an improvement of LASSO by scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sparse modeling is a major topic in machine learning and statistics. LASSO\n(Least Absolute Shrinkage and Selection Operator) is a popular sparse modeling\nmethod while it has been known to yield unexpected large bias especially at a\nsparse representation. There have been several studies for improving this\nproblem such as the introduction of non-convex regularization terms. The\nimportant point is that this bias problem directly affects model selection in\napplications since a sparse representation cannot be selected by a prediction\nerror based model selection even if it is a good representation. In this\narticle, we considered to improve this problem by introducing a scaling that\nexpands LASSO estimator to compensate excessive shrinkage, thus a large bias in\nLASSO estimator. We here gave an empirical value for the amount of scaling.\nThere are two advantages of this scaling method as follows. Since the proposed\nscaling value is calculated by using LASSO estimator, we only need LASSO\nestimator that is obtained by a fast and stable optimization procedure such as\nLARS (Least Angle Regression) under LASSO modification or coordinate descent.\nAnd, the simplicity of our scaling method enables us to derive SURE (Stein's\nUnbiased Risk Estimate) under the modified LASSO estimator with scaling. Our\nscaling method together with model selection based on SURE is fully empirical\nand do not need additional hyper-parameters. In a simple numerical example, we\nverified that our scaling method actually improves LASSO and the SURE based\nmodel selection criterion can stably choose an appropriate sparse model.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 07:55:29 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Hagiwara", "Katsuyuki", ""]]}, {"id": "1808.07270", "submitter": "Margarita Osadchy", "authors": "Jinchao Liu, Stuart J. Gibson, Margarita Osadchy", "title": "Learning to Support: Exploiting Structure Information in Support Sets\n  for One-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning shows very good performance when trained on large labeled data\nsets. The problem of training a deep net on a few or one sample per class\nrequires a different learning approach which can generalize to unseen classes\nusing only a few representatives of these classes. This problem has previously\nbeen approached by meta-learning. Here we propose a novel meta-learner which\nshows state-of-the-art performance on common benchmarks for one/few shot\nclassification. Our model features three novel components: First is a\nfeed-forward embedding that takes random class support samples (after a\ncustomary CNN embedding) and transfers them to a better class representation in\nterms of a classification problem. Second is a novel attention mechanism,\ninspired by competitive learning, which causes class representatives to compete\nwith each other to become a temporary class prototype with respect to the query\npoint. This mechanism allows switching between representatives depending on the\nposition of the query point. Once a prototype is chosen for each class, the\npredicated label is computed using a simple attention mechanism over prototypes\nof all considered classes. The third feature is the ability of our meta-learner\nto incorporate deeper CNN embedding, enabling larger capacity. Finally, to ease\nthe training procedure and reduce overfitting, we averages the top $t$ models\n(evaluated on the validation) over the optimization trajectory. We show that\nthis approach can be viewed as an approximation to an ensemble, which saves the\nfactor of $t$ in training and test times and the factor of of $t$ in the\nstorage of the final model.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:29:16 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Liu", "Jinchao", ""], ["Gibson", "Stuart J.", ""], ["Osadchy", "Margarita", ""]]}, {"id": "1808.07272", "submitter": "Sibo Song", "authors": "Sibo Song, Ngai-Man Cheung, Vijay Chandrasekhar, Bappaditya Mandal", "title": "Deep Adaptive Temporal Pooling for Activity Recognition", "comments": "Accepted by ACM Multimedia 2018", "journal-ref": null, "doi": "10.1145/3240508.3240713", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently achieved competitive accuracy for human\nactivity recognition. However, there is room for improvement, especially in\nmodeling long-term temporal importance and determining the activity relevance\nof different temporal segments in a video. To address this problem, we propose\na learnable and differentiable module: Deep Adaptive Temporal Pooling (DATP).\nDATP applies a self-attention mechanism to adaptively pool the classification\nscores of different video segments. Specifically, using frame-level features,\nDATP regresses importance of different temporal segments and generates weights\nfor them. Remarkably, DATP is trained using only the video-level label. There\nis no need of additional supervision except video-level activity class label.\nWe conduct extensive experiments to investigate various input features and\ndifferent weight models. Experimental results show that DATP can learn to\nassign large weights to key video segments. More importantly, DATP can improve\ntraining of frame-level feature extractor. This is because relevant temporal\nsegments are assigned large weights during back-propagation. Overall, we\nachieve state-of-the-art performance on UCF101, HMDB51 and Kinetics datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:29:38 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Song", "Sibo", ""], ["Cheung", "Ngai-Man", ""], ["Chandrasekhar", "Vijay", ""], ["Mandal", "Bappaditya", ""]]}, {"id": "1808.07285", "submitter": "Milad Nasr", "authors": "Milad Nasr, Alireza Bahramali, Amir Houmansadr", "title": "DeepCorr: Strong Flow Correlation Attacks on Tor Using Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1145/3243734.3243824", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow correlation is the core technique used in a multitude of deanonymization\nattacks on Tor. Despite the importance of flow correlation attacks on Tor,\nexisting flow correlation techniques are considered to be ineffective and\nunreliable in linking Tor flows when applied at a large scale, i.e., they\nimpose high rates of false positive error rates or require impractically long\nflow observations to be able to make reliable correlations. In this paper, we\nshow that, unfortunately, flow correlation attacks can be conducted on Tor\ntraffic with drastically higher accuracies than before by leveraging emerging\nlearning mechanisms. We particularly design a system, called DeepCorr, that\noutperforms the state-of-the-art by significant margins in correlating Tor\nconnections. DeepCorr leverages an advanced deep learning architecture to learn\na flow correlation function tailored to Tor's complex network this is in\ncontrast to previous works' use of generic statistical correlation metrics to\ncorrelated Tor flows. We show that with moderate learning, DeepCorr can\ncorrelate Tor connections (and therefore break its anonymity) with accuracies\nsignificantly higher than existing algorithms, and using substantially shorter\nlengths of flow observations. For instance, by collecting only about 900\npackets of each target Tor flow (roughly 900KB of Tor data), DeepCorr provides\na flow correlation accuracy of 96% compared to 4% by the state-of-the-art\nsystem of RAPTOR using the same exact setting.\n  We hope that our work demonstrates the escalating threat of flow correlation\nattacks on Tor given recent advances in learning algorithms, calling for the\ntimely deployment of effective countermeasures by the Tor community.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 09:02:53 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Nasr", "Milad", ""], ["Bahramali", "Alireza", ""], ["Houmansadr", "Amir", ""]]}, {"id": "1808.07288", "submitter": "Ahmad Alzahrani", "authors": "Ahmad Alzahrani and Samira Sadaoui", "title": "Clustering and Labelling Auction Fraud Data", "comments": null, "journal-ref": null, "doi": "10.6084/m9.figshare.6993308", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although shill bidding is a common auction fraud, it is however very tough to\ndetect. Due to the unavailability and lack of training data, in this study, we\nbuild a high-quality labeled shill bidding dataset based on recently collected\nauctions from eBay. Labeling shill biding instances with multidimensional\nfeatures is a critical phase for the fraud classification task. For this\npurpose, we introduce a new approach to systematically label the fraud data\nwith the help of the hierarchical clustering CURE that returns remarkable\nresults as illustrated in the experiments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 09:15:25 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Alzahrani", "Ahmad", ""], ["Sadaoui", "Samira", ""]]}, {"id": "1808.07292", "submitter": "Xi Peng", "authors": "Xi Peng and Ivor W. Tsang and Joey Tianyi Zhou and Hongyuan Zhu", "title": "k-meansNet: When k-means Meets Differentiable Programming", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two challenging problems. The first one is how to\nimplement \\textit{k}-means in the neural network, which enjoys efficient\ntraining based on the stochastic algorithm. The second one is how to enhance\nthe interpretability of network design for clustering. To solve the problems,\nwe propose a neural network which is a novel formulation of the vanilla\n$k$-means objective. Our contribution is in twofold. From the view of neural\nnetworks, the proposed \\textit{k}-meansNet is with explicit interpretability in\nneural processing. We could understand not only why the network structure is\npresented like itself but also why it could perform data clustering. Such an\ninterpretable neural network remarkably differs from the existing works that\nusually employ visualization technique to explain the result of the neural\nnetwork. From the view of \\textit{k}-means, three highly desired properties are\nachieved, i.e. robustness to initialization, the capability of handling new\ncoming data, and provable convergence. Extensive experimental studies show that\nour method achieves promising performance comparing with 12 clustering methods\non some challenging datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 09:28:43 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 13:44:42 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Peng", "Xi", ""], ["Tsang", "Ivor W.", ""], ["Zhou", "Joey Tianyi", ""], ["Zhu", "Hongyuan", ""]]}, {"id": "1808.07314", "submitter": "Bruce Ferwerda", "authors": "Bruce Ferwerda and Mark Graus", "title": "Predicting Musical Sophistication from Music Listening Behaviors: A\n  Preliminary Study", "comments": "The Late-Breaking Results track part of the Twelfth ACM Conference on\n  Recommender Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Psychological models are increasingly being used to explain online behavioral\ntraces. Aside from the commonly used personality traits as a general user\nmodel, more domain dependent models are gaining attention. The use of domain\ndependent psychological models allows for more fine-grained identification of\nbehaviors and provide a deeper understanding behind the occurrence of those\nbehaviors. Understanding behaviors based on psychological models can provide an\nadvantage over data-driven approaches. For example, relying on psychological\nmodels allow for ways to personalize when data is scarce. In this preliminary\nwork we look at the relation between users' musical sophistication and their\nonline music listening behaviors and to what extent we can successfully predict\nmusical sophistication. An analysis of data from a study with 61 participants\nshows that listening behaviors can successfully be used to infer users' musical\nsophistication.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 11:16:59 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Ferwerda", "Bruce", ""], ["Graus", "Mark", ""]]}, {"id": "1808.07325", "submitter": "Yang Liu", "authors": "Yang Liu, Lixin Ji, Ruiyang Huang, Tuosiyu Ming, Chao Gao, Jianpeng\n  Zhang", "title": "An Attention-Gated Convolutional Neural Network for Sentence\n  Classification", "comments": "Accepted for publication in the Intelligent Data Analysis journal, 19\n  pages, 4 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of sentences is very challenging, since sentences contain\nthe limited contextual information. In this paper, we proposed an\nAttention-Gated Convolutional Neural Network (AGCNN) for sentence\nclassification, which generates attention weights from the feature's context\nwindows of different sizes by using specialized convolution encoders. It makes\nfull use of limited contextual information to extract and enhance the influence\nof important features in predicting the sentence's category. Experimental\nresults demonstrated that our model can achieve up to 3.1% higher accuracy than\nstandard CNN models, and gain competitive results over the baselines on four\nout of the six tasks. Besides, we designed an activation function, namely,\nNatural Logarithm rescaled Rectified Linear Unit (NLReLU). Experiments showed\nthat NLReLU can outperform ReLU and is comparable to other well-known\nactivation functions on AGCNN.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 12:03:48 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 13:35:25 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 09:22:44 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Liu", "Yang", ""], ["Ji", "Lixin", ""], ["Huang", "Ruiyang", ""], ["Ming", "Tuosiyu", ""], ["Gao", "Chao", ""], ["Zhang", "Jianpeng", ""]]}, {"id": "1808.07379", "submitter": "Ming-Chang Lee", "authors": "Ming-Chang Lee, Jia-Chun Lin, Olaf Owe", "title": "Privacy Mining from IoT-based Smart Homes", "comments": "This paper, which has 11 pages and 7 figures, has been accepted BWCCA\n  2018 on 13th August 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a wide range of smart devices are deployed in a variety of\nenvironments to improve the quality of human life. One of the important\nIoT-based applications is smart homes for healthcare, especially for elders.\nIoT-based smart homes enable elders' health to be properly monitored and taken\ncare of. However, elders' privacy might be disclosed from smart homes due to\nnon-fully protected network communication or other reasons. To demonstrate how\nserious this issue is, we introduce in this paper a Privacy Mining Approach\n(PMA) to mine privacy from smart homes by conducting a series of deductions and\nanalyses on sensor datasets generated by smart homes. The experimental results\ndemonstrate that PMA is able to deduce a global sensor topology for a smart\nhome and disclose elders' privacy in terms of their house layouts.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 10:42:52 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 08:21:14 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Lee", "Ming-Chang", ""], ["Lin", "Jia-Chun", ""], ["Owe", "Olaf", ""]]}, {"id": "1808.07380", "submitter": "Tu  Nguyen", "authors": "Tu Ngoc Nguyen and Markus Rokicki", "title": "On the Predictability of non-CGM Diabetes Data for Personalized\n  Recommendation", "comments": "In Proceedings of ACM CIKM 2018 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With continuous glucose monitoring (CGM), data-driven models on blood glucose\nprediction have been shown to be effective in related work. However, such (CGM)\nsystems are not always available, e.g., for a patient at home. In this work, we\nconduct a study on 9 patients and examine the predictability of data-driven\n(aka. machine learning) based models on patient-level blood glucose prediction;\nwith measurements are taken only periodically (i.e., after several hours). To\nthis end, we propose several post-prediction methods to account for the noise\nnature of these data, that marginally improves the performance of the end\nsystem.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 02:53:33 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 18:57:35 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 00:28:59 GMT"}, {"version": "v4", "created": "Fri, 7 Sep 2018 00:41:55 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Nguyen", "Tu Ngoc", ""], ["Rokicki", "Markus", ""]]}, {"id": "1808.07382", "submitter": "Yi Zhou", "authors": "Yi Zhou and Zhe Wang and Yingbin Liang", "title": "Convergence of Cubic Regularization for Nonconvex Optimization under KL\n  Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cubic-regularized Newton's method (CR) is a popular algorithm that guarantees\nto produce a second-order stationary solution for solving nonconvex\noptimization problems. However, existing understandings of the convergence rate\nof CR are conditioned on special types of geometrical properties of the\nobjective function. In this paper, we explore the asymptotic convergence rate\nof CR by exploiting the ubiquitous Kurdyka-Lojasiewicz (KL) property of\nnonconvex objective functions. In specific, we characterize the asymptotic\nconvergence rate of various types of optimality measures for CR including\nfunction value gap, variable distance gap, gradient norm and least eigenvalue\nof the Hessian matrix. Our results fully characterize the diverse convergence\nbehaviors of these optimality measures in the full parameter regime of the KL\nproperty. Moreover, we show that the obtained asymptotic convergence rates of\nCR are order-wise faster than those of first-order gradient descent algorithms\nunder the KL property.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 14:28:57 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Zhou", "Yi", ""], ["Wang", "Zhe", ""], ["Liang", "Yingbin", ""]]}, {"id": "1808.07383", "submitter": "Deunsol Yoon", "authors": "Deunsol Yoon, Dongbok Lee, SangKeun Lee", "title": "Dynamic Self-Attention : Computing Attention over Words Dynamically for\n  Sentence Embedding", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Dynamic Self-Attention (DSA), a new self-attention\nmechanism for sentence embedding. We design DSA by modifying dynamic routing in\ncapsule network (Sabouretal.,2017) for natural language processing. DSA attends\nto informative words with a dynamic weight vector. We achieve new\nstate-of-the-art results among sentence encoding methods in Stanford Natural\nLanguage Inference (SNLI) dataset with the least number of parameters, while\nshowing comparative results in Stanford Sentiment Treebank (SST) dataset.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 14:30:03 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Yoon", "Deunsol", ""], ["Lee", "Dongbok", ""], ["Lee", "SangKeun", ""]]}, {"id": "1808.07384", "submitter": "Zhe Wang", "authors": "Zhe Wang, Yi Zhou, Yingbin Liang, Guanghui Lan", "title": "A Note on Inexact Condition for Cubic Regularized Newton's Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note considers the inexact cubic-regularized Newton's method (CR), which\nhas been shown in \\cite{Cartis2011a} to achieve the same order-level\nconvergence rate to a secondary stationary point as the exact CR\n\\citep{Nesterov2006}. However, the inexactness condition in \\cite{Cartis2011a}\nis not implementable due to its dependence on future iterates variable. This\nnote fixes such an issue by proving the same convergence rate for nonconvex\noptimization under an inexact adaptive condition that depends on only the\ncurrent iterate. Our proof controls the sufficient decrease of the function\nvalue over the total iterations rather than each iteration as used in the\nprevious studies, which can be of independent interest in other contexts.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 14:38:45 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Wang", "Zhe", ""], ["Zhou", "Yi", ""], ["Liang", "Yingbin", ""], ["Lan", "Guanghui", ""]]}, {"id": "1808.07390", "submitter": "Kailiang Wu", "authors": "Kailiang Wu, Dongbin Xiu", "title": "An Explicit Neural Network Construction for Piecewise Constant Function\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an explicit construction for feedforward neural network (FNN),\nwhich provides a piecewise constant approximation for multivariate functions.\nThe proposed FNN has two hidden layers, where the weights and thresholds are\nexplicitly defined and do not require numerical optimization for training.\nUnlike most of the existing work on explicit FNN construction, the proposed FNN\ndoes not rely on tensor structure in multiple dimensions. Instead, it\nautomatically creates Voronoi tessellation of the domain, based on the given\ndata of the target function, and piecewise constant approximation of the\nfunction. This makes the construction more practical for applications. We\npresent both theoretical analysis and numerical examples to demonstrate its\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 14:45:46 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Wu", "Kailiang", ""], ["Xiu", "Dongbin", ""]]}, {"id": "1808.07412", "submitter": "Charith Mendis", "authors": "Charith Mendis, Alex Renda, Saman Amarasinghe and Michael Carbin", "title": "Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation\n  using Deep Neural Networks", "comments": "Published at 36th International Conference on Machine Learning (ICML)\n  2019", "journal-ref": "Proceedings of Machine Learning Research - Volume 97 (ICML 2019)", "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the number of clock cycles a processor takes to execute a block of\nassembly instructions in steady state (the throughput) is important for both\ncompiler designers and performance engineers. Building an analytical model to\ndo so is especially complicated in modern x86-64 Complex Instruction Set\nComputer (CISC) machines with sophisticated processor microarchitectures in\nthat it is tedious, error prone, and must be performed from scratch for each\nprocessor generation. In this paper we present Ithemal, the first tool which\nlearns to predict the throughput of a set of instructions. Ithemal uses a\nhierarchical LSTM--based approach to predict throughput based on the opcodes\nand operands of instructions in a basic block. We show that Ithemal is more\naccurate than state-of-the-art hand-written tools currently used in compiler\nbackends and static machine code analyzers. In particular, our model has less\nthan half the error of state-of-the-art analytical models (LLVM's llvm-mca and\nIntel's IACA). Ithemal is also able to predict these throughput values just as\nfast as the aforementioned tools, and is easily ported across a variety of\nprocessor microarchitectures with minimal developer effort.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 03:40:21 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 13:32:47 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Mendis", "Charith", ""], ["Renda", "Alex", ""], ["Amarasinghe", "Saman", ""], ["Carbin", "Michael", ""]]}, {"id": "1808.07440", "submitter": "Levent Kara", "authors": "Saurabh Banga, Harsh Gehani, Sanket Bhilare, Sagar Patel, Levent Kara", "title": "3D Topology Optimization using Convolutional Neural Networks", "comments": "The paper is under review in 'Special issue on Computer-Aided Design\n  on Advances in Generative Design', 16 Pages, 7 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topology optimization is computationally demanding that requires the assembly\nand solution to a finite element problem for each material distribution\nhypothesis. As a complementary alternative to the traditional physics-based\ntopology optimization, we explore a data-driven approach that can quickly\ngenerate accurate solutions. To this end, we propose a deep learning approach\nbased on a 3D encoder-decoder Convolutional Neural Network architecture for\naccelerating 3D topology optimization and to determine the optimal\ncomputational strategy for its deployment. Analysis of iteration-wise progress\nof the Solid Isotropic Material with Penalization process is used as a\nguideline to study how the earlier steps of the conventional topology\noptimization can be used as input for our approach to predict the final\noptimized output structure directly from this input. We conduct a comparative\nstudy between multiple strategies for training the neural network and assess\nthe effect of using various input combinations for the CNN to finalize the\nstrategy with the highest accuracy in predictions for practical deployment. For\nthe best performing network, we achieved about 40% reduction in overall\ncomputation time while also attaining structural accuracies in the order of\n96%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 17:03:10 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Banga", "Saurabh", ""], ["Gehani", "Harsh", ""], ["Bhilare", "Sanket", ""], ["Patel", "Sagar", ""], ["Kara", "Levent", ""]]}, {"id": "1808.07452", "submitter": "Tamara Kolda", "authors": "David Hong, Tamara G. Kolda, Jed A. Duersch", "title": "Generalized Canonical Polyadic Tensor Decomposition", "comments": null, "journal-ref": "SIAM Review, Vol. 62, No. 1, pp. 133-163, 2020", "doi": "10.1137/18M1203626", "report-no": null, "categories": "math.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decomposition is a fundamental unsupervised machine learning method in\ndata science, with applications including network analysis and sensor data\nprocessing. This work develops a generalized canonical polyadic (GCP) low-rank\ntensor decomposition that allows other loss functions besides squared error.\nFor instance, we can use logistic loss or Kullback-Leibler divergence, enabling\ntensor decomposition for binary or count data. We present a variety\nstatistically-motivated loss functions for various scenarios. We provide a\ngeneralized framework for computing gradients and handling missing data that\nenables the use of standard optimization methods for fitting the model. We\ndemonstrate the flexibility of GCP on several real-world examples including\ninteractions in a social network, neural activity in a mouse, and monthly\nrainfall measurements in India.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 17:36:08 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 00:23:23 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Hong", "David", ""], ["Kolda", "Tamara G.", ""], ["Duersch", "Jed A.", ""]]}, {"id": "1808.07475", "submitter": "Wilson Rivera", "authors": "Dan Rosa de Jesus, Julian Cuevas, Wilson Rivera and Silvia Crivelli", "title": "Capsule Networks for Protein Structure Classification and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule Networks have great potential to tackle problems in structural\nbiology because of their attention to hierarchical relationships. This paper\ndescribes the implementation and application of a Capsule Network architecture\nto the classification of RAS protein family structures on GPU-based\ncomputational resources. The proposed Capsule Network trained on 2D and 3D\nstructural encodings can successfully classify HRAS and KRAS structures. The\nCapsule Network can also classify a protein-based dataset derived from a\nPSI-BLAST search on sequences of KRAS and HRAS mutations. Our results show an\naccuracy improvement compared to traditional convolutional networks, while\nimproving interpretability through visualization of activation vectors.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 13:38:19 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["de Jesus", "Dan Rosa", ""], ["Cuevas", "Julian", ""], ["Rivera", "Wilson", ""], ["Crivelli", "Silvia", ""]]}, {"id": "1808.07510", "submitter": "Tamara Kolda", "authors": "Clifford Anderson-Bergman, Tamara G. Kolda, Kina Kincher-Winoto", "title": "XPCA: Extending PCA for a Combination of Discrete and Continuous\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is arguably the most popular tool in\nmultivariate exploratory data analysis. In this paper, we consider the question\nof how to handle heterogeneous variables that include continuous, binary, and\nordinal. In the probabilistic interpretation of low-rank PCA, the data has a\nnormal multivariate distribution and, therefore, normal marginal distributions\nfor each column. If some marginals are continuous but not normal, the\nsemiparametric copula-based principal component analysis (COCA) method is an\nalternative to PCA that combines a Gaussian copula with nonparametric\nmarginals. If some marginals are discrete or semi-continuous, we propose a new\nextended PCA (XPCA) method that also uses a Gaussian copula and nonparametric\nmarginals and accounts for discrete variables in the likelihood calculation by\nintegrating over appropriate intervals. Like PCA, the factors produced by XPCA\ncan be used to find latent structure in data, build predictive models, and\nperform dimensionality reduction. We present the new model, its induced\nlikelihood function, and a fitting algorithm which can be applied in the\npresence of missing data. We demonstrate how to use XPCA to produce an\nestimated full conditional distribution for each data point, and use this to\nproduce to provide estimates for missing data that are automatically range\nrespecting. We compare the methods as applied to simulated and real-world data\nsets that have a mixture of discrete and continuous variables.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 18:25:53 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Anderson-Bergman", "Clifford", ""], ["Kolda", "Tamara G.", ""], ["Kincher-Winoto", "Kina", ""]]}, {"id": "1808.07561", "submitter": "Ankur Bapna", "authors": "Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, Yonghui Wu", "title": "Training Deeper Neural Machine Translation Models with Transparent\n  Attention", "comments": "To appear in EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current state-of-the-art NMT models, such as RNN seq2seq and\nTransformers, possess a large number of parameters, they are still shallow in\ncomparison to convolutional models used for both text and vision applications.\nIn this work we attempt to train significantly (2-3x) deeper Transformer and\nBi-RNN encoders for machine translation. We propose a simple modification to\nthe attention mechanism that eases the optimization of deeper models, and\nresults in consistent gains of 0.7-1.1 BLEU on the benchmark WMT'14\nEnglish-German and WMT'15 Czech-English tasks for both architectures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 20:53:37 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 20:10:24 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Bapna", "Ankur", ""], ["Chen", "Mia Xu", ""], ["Firat", "Orhan", ""], ["Cao", "Yuan", ""], ["Wu", "Yonghui", ""]]}, {"id": "1808.07569", "submitter": "Abhimanyu Mitra", "authors": "Abhimanyu Mitra, Kannan Achan and Sushant Kumar", "title": "Robust Counterfactual Inferences using Feature Learning and their\n  Applications", "comments": "15 pages,1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide variety of applications, including personalization, we want to\nmeasure the difference in outcome due to an intervention and thus have to deal\nwith counterfactual inference. The feedback from a customer in any of these\nsituations is only 'bandit feedback' - that is, a partial feedback based on\nwhether we chose to intervene or not. Typically randomized experiments are\ncarried out to understand whether an intervention is overall better than no\nintervention. Here we present a feature learning algorithm to learn from a\nrandomized experiment where the intervention in consideration is most effective\nand where it is least effective rather than only focusing on the overall\nimpact, thus adding a context to our learning mechanism and extract more\ninformation. From the randomized experiment, we learn the feature\nrepresentations which divide the population into subpopulations where we\nobserve statistically significant difference in average customer feedback\nbetween those who were subjected to the intervention and those who were not,\nwith a level of significance l, where l is a configurable parameter in our\nmodel. We use this information to derive the value of the intervention in\nconsideration for each instance in the population. With experiments, we show\nthat using this additional learning, in future interventions, the context for\neach instance could be leveraged to decide whether to intervene or not.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 21:26:06 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Mitra", "Abhimanyu", ""], ["Achan", "Kannan", ""], ["Kumar", "Sushant", ""]]}, {"id": "1808.07573", "submitter": "Giles Hooker", "authors": "Yichen Zhou, Zhengze Zhou and Giles Hooker", "title": "Approximation Trees: Statistical Stability in Model Distillation", "comments": "This paper supercedes arXiv:1610.09036", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the stability of learned explanations for black-box\npredictions via model distillation with decision trees. One approach to\nintelligibility in machine learning is to use an understandable `student' model\nto mimic the output of an accurate `teacher'. Here, we consider the use of\nregression trees as a student model, in which nodes of the tree can be used as\n`explanations' for particular predictions, and the whole structure of the tree\ncan be used as a global representation of the resulting function. However,\nindividual trees are sensitive to the particular data sets used to train them,\nand an interpretation of a student model may be suspect if small changes in the\ntraining data have a large effect on it. In this context, access to outcomes\nfrom a teacher helps to stabilize the greedy splitting strategy by generating a\nmuch larger corpus of training examples than was originally available. We\ndevelop tests to ensure that enough examples are generated at each split so\nthat the same splitting rule would be chosen with high probability were the\ntree to be re trained. Further, we develop a stopping rule to indicate how deep\nthe tree should be built based on recent results on the variability of Random\nForests when these are used as the teacher. We provide concrete examples of\nthese procedures on the CAD-MDD and COMPAS data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 21:50:56 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Zhou", "Yichen", ""], ["Zhou", "Zhengze", ""], ["Hooker", "Giles", ""]]}, {"id": "1808.07576", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Gauri Joshi", "title": "Cooperative SGD: A unified Framework for the Design and Analysis of\n  Communication-Efficient SGD Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication-efficient SGD algorithms, which allow nodes to perform local\nupdates and periodically synchronize local models, are highly effective in\nimproving the speed and scalability of distributed SGD. However, a rigorous\nconvergence analysis and comparative study of different communication-reduction\nstrategies remains a largely open problem. This paper presents a unified\nframework called Cooperative SGD that subsumes existing communication-efficient\nSGD algorithms such as periodic-averaging, elastic-averaging and decentralized\nSGD. By analyzing Cooperative SGD, we provide novel convergence guarantees for\nexisting algorithms. Moreover, this framework enables us to design new\ncommunication-efficient SGD algorithms that strike the best balance between\nreducing communication overhead and achieving fast error convergence with low\nerror floor.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 22:06:26 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 00:45:15 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 17:45:23 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Wang", "Jianyu", ""], ["Joshi", "Gauri", ""]]}, {"id": "1808.07593", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, Brendan D. Tracey, Steven Van Kuyk", "title": "Caveats for information bottleneck in deterministic scenarios", "comments": null, "journal-ref": "International Conference on Learning Representations (ICLR) 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information bottleneck (IB) is a method for extracting information from one\nrandom variable $X$ that is relevant for predicting another random variable\n$Y$. To do so, IB identifies an intermediate \"bottleneck\" variable $T$ that has\nlow mutual information $I(X;T)$ and high mutual information $I(Y;T)$. The \"IB\ncurve\" characterizes the set of bottleneck variables that achieve maximal\n$I(Y;T)$ for a given $I(X;T)$, and is typically explored by maximizing the \"IB\nLagrangian\", $I(Y;T) - \\beta I(X;T)$. In some cases, $Y$ is a deterministic\nfunction of $X$, including many classification problems in supervised learning\nwhere the output class $Y$ is a deterministic function of the input $X$. We\ndemonstrate three caveats when using IB in any situation where $Y$ is a\ndeterministic function of $X$: (1) the IB curve cannot be recovered by\nmaximizing the IB Lagrangian for different values of $\\beta$; (2) there are\n\"uninteresting\" trivial solutions at all points of the IB curve; and (3) for\nmulti-layer classifiers that achieve low prediction error, different layers\ncannot exhibit a strict trade-off between compression and prediction, contrary\nto a recent proposal. We also show that when $Y$ is a small perturbation away\nfrom being a deterministic function of $X$, these three caveats arise in an\napproximate way. To address problem (1), we propose a functional that, unlike\nthe IB Lagrangian, can recover the IB curve in all cases. We demonstrate the\nthree caveats on the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 00:13:18 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 07:58:05 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 20:56:54 GMT"}, {"version": "v4", "created": "Fri, 8 Feb 2019 19:22:40 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Tracey", "Brendan D.", ""], ["Van Kuyk", "Steven", ""]]}, {"id": "1808.07632", "submitter": "Yi Loo Mr.", "authors": "Swee Kiat Lim, Yi Loo, Ngoc-Trung Tran, Ngai-Man Cheung, Gemma Roig,\n  Yuval Elovici", "title": "DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection\n  with GAN", "comments": "Published as a conference paper at ICDM 2018 (IEEE International\n  Conference on Data Mining)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the introduction of the generative adversarial network (GAN) and\nits variants has enabled the generation of realistic synthetic samples, which\nhas been used for enlarging training sets. Previous work primarily focused on\ndata augmentation for semi-supervised and supervised tasks. In this paper, we\ninstead focus on unsupervised anomaly detection and propose a novel generative\ndata augmentation framework optimized for this task. In particular, we propose\nto oversample infrequent normal samples - normal samples that occur with small\nprobability, e.g., rare normal events. We show that these samples are\nresponsible for false positives in anomaly detection. However, oversampling of\ninfrequent normal samples is challenging for real-world high-dimensional data\nwith multimodal distributions. To address this challenge, we propose to use a\nGAN variant known as the adversarial autoencoder (AAE) to transform the\nhigh-dimensional multimodal data distributions into low-dimensional unimodal\nlatent distributions with well-defined tail probability. Then, we\nsystematically oversample at the `edge' of the latent distributions to increase\nthe density of infrequent normal samples. We show that our oversampling\npipeline is a unified one: it is generally applicable to datasets with\ndifferent complex data distributions. To the best of our knowledge, our method\nis the first data augmentation technique focused on improving performance in\nunsupervised anomaly detection. We validate our method by demonstrating\nconsistent improvements across several real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 04:44:25 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 02:26:23 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Lim", "Swee Kiat", ""], ["Loo", "Yi", ""], ["Tran", "Ngoc-Trung", ""], ["Cheung", "Ngai-Man", ""], ["Roig", "Gemma", ""], ["Elovici", "Yuval", ""]]}, {"id": "1808.07647", "submitter": "Michele Polese", "authors": "Michele Polese, Rittwik Jana, Velin Kounev, Ke Zhang, Supratim Deb,\n  Michele Zorzi", "title": "Machine Learning at the Edge: A Data-Driven Architecture with\n  Applications to 5G Cellular Networks", "comments": "15 pages, 10 figures, 5 tables. IEEE Transactions on Mobile Computing", "journal-ref": null, "doi": "10.1109/TMC.2020.2999852", "report-no": null, "categories": "cs.NI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fifth generation of cellular networks (5G) will rely on edge cloud\ndeployments to satisfy the ultra-low latency demand of future applications. In\nthis paper, we argue that such deployments can also be used to enable advanced\ndata-driven and Machine Learning (ML) applications in mobile networks. We\npropose an edge-controller-based architecture for cellular networks and\nevaluate its performance with real data from hundreds of base stations of a\nmajor U.S. operator. In this regard, we will provide insights on how to\ndynamically cluster and associate base stations and controllers, according to\nthe global mobility patterns of the users. Then, we will describe how the\ncontrollers can be used to run ML algorithms to predict the number of users in\neach base station, and a use case in which these predictions are exploited by a\nhigher-layer application to route vehicular traffic according to network Key\nPerformance Indicators (KPIs). We show that the prediction accuracy improves\nwhen based on machine learning algorithms that rely on the controllers' view\nand, consequently, on the spatial correlation introduced by the user mobility,\nwith respect to when the prediction is based only on the local data of each\nsingle base station.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 07:06:41 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 00:17:48 GMT"}, {"version": "v3", "created": "Sun, 7 Apr 2019 16:07:50 GMT"}, {"version": "v4", "created": "Thu, 4 Jun 2020 15:33:19 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Polese", "Michele", ""], ["Jana", "Rittwik", ""], ["Kounev", "Velin", ""], ["Zhang", "Ke", ""], ["Deb", "Supratim", ""], ["Zorzi", "Michele", ""]]}, {"id": "1808.07713", "submitter": "Meysam Sadeghi", "authors": "Meysam Sadeghi and Erik G. Larsson", "title": "Adversarial Attacks on Deep-Learning Based Radio Signal Classification", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.LG eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL), despite its enormous success in many computer vision and\nlanguage processing applications, is exceedingly vulnerable to adversarial\nattacks. We consider the use of DL for radio signal (modulation) classification\ntasks, and present practical methods for the crafting of white-box and\nuniversal black-box adversarial attacks in that application. We show that these\nattacks can considerably reduce the classification performance, with extremely\nsmall perturbations of the input. In particular, these attacks are\nsignificantly more powerful than classical jamming attacks, which raises\nsignificant security and robustness concerns in the use of DL-based algorithms\nfor the wireless physical layer.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 12:12:10 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Sadeghi", "Meysam", ""], ["Larsson", "Erik G.", ""]]}, {"id": "1808.07724", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis, Mohammad Taher Pilehvar, Nigel Collier", "title": "Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs", "comments": "Accepted for presentation at EMNLP 2018 (main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of mapping natural language text to\nknowledge base entities. The mapping process is approached as a composition of\na phrase or a sentence into a point in a multi-dimensional entity space\nobtained from a knowledge graph. The compositional model is an LSTM equipped\nwith a dynamic disambiguation mechanism on the input word embeddings (a\nMulti-Sense LSTM), addressing polysemy issues. Further, the knowledge base\nspace is prepared by collecting random walks from a graph enhanced with textual\nfeatures, which act as a set of semantic bridges between text and knowledge\nbase entities. The ideas of this work are demonstrated on large-scale\ntext-to-entity mapping and entity classification tasks, with state of the art\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 12:47:01 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Kartsaklis", "Dimitri", ""], ["Pilehvar", "Mohammad Taher", ""], ["Collier", "Nigel", ""]]}, {"id": "1808.07739", "submitter": "Fabien C. Y. Benureau", "authors": "Fabien C. Y. Benureau and Pierre-Yves Oudeyer", "title": "Diversity-Driven Selection of Exploration Strategies in Multi-Armed\n  Bandits", "comments": null, "journal-ref": "2015 Joint IEEE International Conference on Development and\n  Learning and Epigenetic Robotics (ICDL-EpiRob), pp. 135-142", "doi": "10.1109/devlrn.2015.7346130", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider a scenario where an agent has multiple available strategies to\nexplore an unknown environment. For each new interaction with the environment,\nthe agent must select which exploration strategy to use. We provide a new\nstrategy-agnostic method that treat the situation as a Multi-Armed Bandits\nproblem where the reward signal is the diversity of effects that each strategy\nproduces. We test the method empirically on a simulated planar robotic arm, and\nestablish that the method is both able discriminate between strategies of\ndissimilar quality, even when the differences are tenuous, and that the\nresulting performance is competitive with the best fixed mixture of strategies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 13:24:17 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Benureau", "Fabien C. Y.", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1808.07752", "submitter": "Jian-Xun Wang", "authors": "Jian-Xun Wang, Junji Huang, Lian Duan, Heng Xiao", "title": "Prediction of Reynolds Stresses in High-Mach-Number Turbulent Boundary\n  Layers using Physics-Informed Machine Learning", "comments": "28 pages, 12 figures", "journal-ref": "Theoretical Comput. Fluid Dyn. (2019) 33: 1", "doi": "10.1007/s00162-018-0480-2", "report-no": null, "categories": "physics.flu-dyn cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeled Reynolds stress is a major source of model-form uncertainties in\nReynolds-averaged Navier-Stokes (RANS) simulations. Recently, a\nphysics-informed machine-learning (PIML) approach has been proposed for\nreconstructing the discrepancies in RANS-modeled Reynolds stresses. The merits\nof the PIML framework has been demonstrated in several canonical incompressible\nflows. However, its performance on high-Mach-number flows is still not clear.\nIn this work we use the PIML approach to predict the discrepancies in RANS\nmodeled Reynolds stresses in high-Mach-number flat-plate turbulent boundary\nlayers by using an existing DNS database. Specifically, the discrepancy\nfunction is first constructed using a DNS training flow and then used to\ncorrect RANS-predicted Reynolds stresses under flow conditions different from\nthe DNS. The machine-learning technique is shown to significantly improve\nRANS-modeled turbulent normal stresses, the turbulent kinetic energy, and the\nReynolds-stress anisotropy. Improvements are consistently observed when\ndifferent training datasets are used. Moreover, a high-dimensional\nvisualization technique and distance metrics are used to provide a priori\nassessment of prediction confidence based only on RANS simulations. This study\ndemonstrates that the PIML approach is a computationally affordable technique\nfor improving the accuracy of RANS-modeled Reynolds stresses for\nhigh-Mach-number turbulent flows when there is a lack of experiments and\nhigh-fidelity simulations.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 03:19:40 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wang", "Jian-Xun", ""], ["Huang", "Junji", ""], ["Duan", "Lian", ""], ["Xiao", "Heng", ""]]}, {"id": "1808.07769", "submitter": "Matthew Baron", "authors": "Matthew Baron", "title": "Topology and Prediction Focused Research on Graph Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important advances have been made using convolutional neural network (CNN)\napproaches to solve complicated problems in areas that rely on grid structured\ndata such as image processing and object classification. Recently, research on\ngraph convolutional neural networks (GCNN) has increased dramatically as\nresearchers try to replicate the success of CNN for graph structured data.\nUnfortunately, traditional CNN methods are not readily transferable to GCNN,\ngiven the irregularity and geometric complexity of graphs. The emerging field\nof GCNN is further complicated by research papers that differ greatly in their\nscope, detail, and level of academic sophistication needed by the reader.\n  The present paper provides a review of some basic properties of GCNN. As a\nguide to the interested reader, recent examples of GCNN research are then\ngrouped according to techniques that attempt to uncover the underlying topology\nof the graph model and those that seek to generalize traditional CNN methods on\ngraph data to improve prediction of class membership. Discrete Signal\nProcessing on Graphs (DSPg) is used as a theoretical framework to better\nunderstand some of the performance gains and limitations of these recent GCNN\napproaches. A brief discussion of Topology Adaptive Graph Convolutional\nNetworks (TAGCN) is presented as an approach motivated by DSPg and future\nresearch directions using this approach are briefly discussed.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 14:01:05 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Baron", "Matthew", ""]]}, {"id": "1808.07784", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman, Frederik Ebert, Alexei A. Efros, Sergey Levine", "title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "comments": "8 pages, plus appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction is arguably one of the most basic functions of an intelligent\nsystem. In general, the problem of predicting events in the future or between\ntwo waypoints is exceedingly difficult. However, most phenomena naturally pass\nthrough relatively predictable bottlenecks---while we cannot predict the\nprecise trajectory of a robot arm between being at rest and holding an object\nup, we can be certain that it must have picked the object up. To exploit this,\nwe decouple visual prediction from a rigid notion of time. While conventional\napproaches predict frames at regularly spaced temporal intervals, our\ntime-agnostic predictors (TAP) are not tied to specific times so that they may\ninstead discover predictable \"bottleneck\" frames no matter when they occur. We\nevaluate our approach for future and intermediate frame prediction across three\nrobotic manipulation tasks. Our predictions are not only of higher visual\nquality, but also correspond to coherent semantic subgoals in temporally\nextended tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 14:52:40 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 22:46:41 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 19:22:25 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Ebert", "Frederik", ""], ["Efros", "Alexei A.", ""], ["Levine", "Sergey", ""]]}, {"id": "1808.07801", "submitter": "Youngser Park", "authors": "Carey E. Priebe, Youngser Park, Joshua T. Vogelstein, John M. Conroy,\n  Vince Lyzinski, Minh Tang, Avanti Athreya, Joshua Cape, Eric Bridgeford", "title": "On a 'Two Truths' Phenomenon in Spectral Graph Clustering", "comments": null, "journal-ref": "PNAS 116 (2019) 5995-6000", "doi": "10.1073/pnas.1814462116", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is concerned with coherently grouping observations without any\nexplicit concept of true groupings. Spectral graph clustering - clustering the\nvertices of a graph based on their spectral embedding - is commonly approached\nvia K-means (or, more generally, Gaussian mixture model) clustering composed\nwith either Laplacian or Adjacency spectral embedding (LSE or ASE). Recent\ntheoretical results provide new understanding of the problem and solutions, and\nlead us to a 'Two Truths' LSE vs. ASE spectral graph clustering phenomenon\nconvincingly illustrated here via a diffusion MRI connectome data set: the\ndifferent embedding methods yield different clustering results, with LSE\ncapturing left hemisphere/right hemisphere affinity structure and ASE capturing\ngray matter/white matter core-periphery structure.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 15:22:57 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 13:43:46 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 15:20:41 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Priebe", "Carey E.", ""], ["Park", "Youngser", ""], ["Vogelstein", "Joshua T.", ""], ["Conroy", "John M.", ""], ["Lyzinski", "Vince", ""], ["Tang", "Minh", ""], ["Athreya", "Avanti", ""], ["Cape", "Joshua", ""], ["Bridgeford", "Eric", ""]]}, {"id": "1808.07804", "submitter": "Bradly Stadie", "authors": "S\\\"oren R. K\\\"unzel, Bradly C. Stadie, Nikita Vemuri, Varsha\n  Ramakrishnan, Jasjeet S. Sekhon, Pieter Abbeel", "title": "Transfer Learning for Estimating Causal Effects using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new algorithms for estimating heterogeneous treatment effects,\ncombining recent developments in transfer learning for neural networks with\ninsights from the causal inference literature. By taking advantage of transfer\nlearning, we are able to efficiently use different data sources that are\nrelated to the same underlying causal mechanisms. We compare our algorithms\nwith those in the extant literature using extensive simulation studies based on\nlarge-scale voter persuasion experiments and the MNIST database. Our methods\ncan perform an order of magnitude better than existing benchmarks while using a\nfraction of the data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 15:27:14 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["K\u00fcnzel", "S\u00f6ren R.", ""], ["Stadie", "Bradly C.", ""], ["Vemuri", "Nikita", ""], ["Ramakrishnan", "Varsha", ""], ["Sekhon", "Jasjeet S.", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1808.07840", "submitter": "Quan Zheng", "authors": "Quan Zheng and Matthias Zwicker", "title": "Learning to Importance Sample in Primary Sample Space", "comments": "Submitted to SIGGRAPH ASIA'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is one of the most widely used variance reduction\nstrategies in Monte Carlo rendering. In this paper, we propose a novel\nimportance sampling technique that uses a neural network to learn how to sample\nfrom a desired density represented by a set of samples. Our approach considers\nan existing Monte Carlo rendering algorithm as a black box. During a\nscene-dependent training phase, we learn to generate samples with a desired\ndensity in the primary sample space of the rendering algorithm using maximum\nlikelihood estimation. We leverage a recent neural network architecture that\nwas designed to represent real-valued non-volume preserving ('Real NVP')\ntransformations in high dimensional spaces. We use Real NVP to non-linearly\nwarp primary sample space and obtain desired densities. In addition, Real NVP\nefficiently computes the determinant of the Jacobian of the warp, which is\nrequired to implement the change of integration variables implied by the warp.\nA main advantage of our approach is that it is agnostic of underlying light\ntransport effects, and can be combined with many existing rendering techniques\nby treating them as a black box. We show that our approach leads to effective\nvariance reduction in several practical scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 16:55:53 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Zheng", "Quan", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1808.07903", "submitter": "Michael Schaarschmidt", "authors": "Michael Schaarschmidt, Alexander Kuhnle, Ben Ellis, Kai Fricke, Felix\n  Gessert, Eiko Yoneki", "title": "LIFT: Reinforcement Learning in Computer Systems by Learning From\n  Demonstrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning approaches have long appealed to the data management\ncommunity due to their ability to learn to control dynamic behavior from raw\nsystem performance. Recent successes in combining deep neural networks with\nreinforcement learning have sparked significant new interest in this domain.\nHowever, practical solutions remain elusive due to large training data\nrequirements, algorithmic instability, and lack of standard tools. In this\nwork, we introduce LIFT, an end-to-end software stack for applying deep\nreinforcement learning to data management tasks. While prior work has\nfrequently explored applications in simulations, LIFT centers on utilizing\nhuman expertise to learn from demonstrations, thus lowering online training\ntimes. We further introduce TensorForce, a TensorFlow library for applied deep\nreinforcement learning exposing a unified declarative interface to common RL\nalgorithms, thus providing a backend to LIFT. We demonstrate the utility of\nLIFT in two case studies in database compound indexing and resource management\nin stream processing. Results show LIFT controllers initialized from\ndemonstrations can outperform human baselines and heuristics across latency\nmetrics and space usage by up to 70%.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 18:50:49 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Schaarschmidt", "Michael", ""], ["Kuhnle", "Alexander", ""], ["Ellis", "Ben", ""], ["Fricke", "Kai", ""], ["Gessert", "Felix", ""], ["Yoneki", "Eiko", ""]]}, {"id": "1808.07910", "submitter": "Nicolas Ford", "authors": "Nicolas Ford, Daniel Duckworth, Mohammad Norouzi, and George E. Dahl", "title": "The Importance of Generation Order in Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models are a critical component of state-of-the-art systems\nfor machine translation, summarization, audio transcription, and other tasks.\nThese language models are almost universally autoregressive in nature,\ngenerating sentences one token at a time from left to right. This paper studies\nthe influence of token generation order on model quality via a novel two-pass\nlanguage model that produces partially-filled sentence \"templates\" and then\nfills in missing tokens. We compare various strategies for structuring these\ntwo passes and observe a surprisingly large variation in model quality. We find\nthe most effective strategy generates function words in the first pass followed\nby content words in the second. We believe these experimental results justify a\nmore extensive investigation of generation order for neural language models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 19:17:24 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Ford", "Nicolas", ""], ["Duckworth", "Daniel", ""], ["Norouzi", "Mohammad", ""], ["Dahl", "George E.", ""]]}, {"id": "1808.07912", "submitter": "Shujian Yu", "authors": "Shujian Yu, Luis Gonzalo Sanchez Giraldo, Robert Jenssen, Jose C.\n  Principe", "title": "Multivariate Extension of Matrix-based Renyi's \\alpha-order Entropy\n  Functional", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. Matlab code is available from Google drive at\n  https://drive.google.com/open?id=1SlxzEOX8RbnLwCgRyqGwMOL7vuT90Gje or Baidu\n  Cloud at https://pan.baidu.com/s/1xupfXCmIV20gXPr0TicGkg (access code: d1sa)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix-based Renyi's \\alpha-order entropy functional was recently\nintroduced using the normalized eigenspectrum of a Hermitian matrix of the\nprojected data in a reproducing kernel Hilbert space (RKHS). However, the\ncurrent theory in the matrix-based Renyi's \\alpha-order entropy functional only\ndefines the entropy of a single variable or mutual information between two\nrandom variables. In information theory and machine learning communities, one\nis also frequently interested in multivariate information quantities, such as\nthe multivariate joint entropy and different interactive quantities among\nmultiple variables. In this paper, we first define the matrix-based Renyi's\n\\alpha-order joint entropy among multiple variables. We then show how this\ndefinition can ease the estimation of various information quantities that\nmeasure the interactions among multiple variables, such as interactive\ninformation and total correlation. We finally present an application to feature\nselection to show how our definition provides a simple yet powerful way to\nestimate a widely-acknowledged intractable quantity from data. A real example\non hyperspectral image (HSI) band selection is also provided.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 19:19:16 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 01:30:03 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 08:30:44 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Yu", "Shujian", ""], ["Giraldo", "Luis Gonzalo Sanchez", ""], ["Jenssen", "Robert", ""], ["Principe", "Jose C.", ""]]}, {"id": "1808.07945", "submitter": "Anqi Xu", "authors": "Rey Wiyatno and Anqi Xu", "title": "Maximal Jacobian-based Saliency Map Attack", "comments": "Extended version of extended abstract for MAIS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jacobian-based Saliency Map Attack is a family of adversarial attack\nmethods for fooling classification models, such as deep neural networks for\nimage classification tasks. By saturating a few pixels in a given image to\ntheir maximum or minimum values, JSMA can cause the model to misclassify the\nresulting adversarial image as a specified erroneous target class. We propose\ntwo variants of JSMA, one which removes the requirement to specify a target\nclass, and another that additionally does not need to specify whether to only\nincrease or decrease pixel intensities. Our experiments highlight the\ncompetitive speeds and qualities of these variants when applied to datasets of\nhand-written digits and natural scenes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 20:58:08 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Wiyatno", "Rey", ""], ["Xu", "Anqi", ""]]}, {"id": "1808.07982", "submitter": "Yi-Lin Tuan", "authors": "Yi-Lin Tuan, Jinzhi Zhang, Yujia Li, Hung-yi Lee", "title": "Proximal Policy Optimization and its Dynamic Version for Sequence\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sequence generation task, many works use policy gradient for model\noptimization to tackle the intractable backpropagation issue when maximizing\nthe non-differentiable evaluation metrics or fooling the discriminator in\nadversarial learning. In this paper, we replace policy gradient with proximal\npolicy optimization (PPO), which is a proved more efficient reinforcement\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 02:14:43 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Tuan", "Yi-Lin", ""], ["Zhang", "Jinzhi", ""], ["Li", "Yujia", ""], ["Lee", "Hung-yi", ""]]}, {"id": "1808.07983", "submitter": "Masatoshi Uehara", "authors": "Masatoshi Uehara and Takeru Matsuda and Fumiyasu Komaki", "title": "Analysis of Noise Contrastive Estimation from the Perspective of\n  Asymptotic Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many models, often called unnormalized models, whose normalizing\nconstants are not calculated in closed form. Maximum likelihood estimation is\nnot directly applicable to unnormalized models. Score matching, contrastive\ndivergence method, pseudo-likelihood, Monte Carlo maximum likelihood, and noise\ncontrastive estimation (NCE) are popular methods for estimating parameters of\nsuch models. In this paper, we focus on NCE. The estimator derived from NCE is\nconsistent and asymptotically normal because it is an M-estimator. NCE\ncharacteristically uses an auxiliary distribution to calculate the normalizing\nconstant in the same spirit of the importance sampling. In addition, there are\nseveral candidates as objective functions of NCE.\n  We focus on how to reduce asymptotic variance. First, we propose a method for\nreducing asymptotic variance by estimating the parameters of the auxiliary\ndistribution. Then, we determine the form of the objective functions, where the\nasymptotic variance takes the smallest values in the original estimator class\nand the proposed estimator classes. We further analyze the robustness of the\nestimator.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 02:14:45 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Uehara", "Masatoshi", ""], ["Matsuda", "Takeru", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1808.07991", "submitter": "Charles Onu", "authors": "Charles C. Onu, Lara J. Kanbar, Wissam Shalish, Karen A. Brown,\n  Guilherme M. Sant'Anna, Robert E. Kearney, Doina Precup", "title": "Predicting Extubation Readiness in Extreme Preterm Infants based on\n  Patterns of Breathing", "comments": "Published in: 2017 IEEE Symposium Series on Computational\n  Intelligence (SSCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extremely preterm infants commonly require intubation and invasive mechanical\nventilation after birth. While the duration of mechanical ventilation should be\nminimized in order to avoid complications, extubation failure is associated\nwith increases in morbidities and mortality. As part of a prospective\nobservational study aimed at developing an accurate predictor of extubation\nreadiness, Markov and semi-Markov chain models were applied to gain insight\ninto the respiratory patterns of these infants, with more robust time-series\nmodeling using semi-Markov models. This model revealed interesting similarities\nand differences between newborns who succeeded extubation and those who failed.\nThe parameters of the model were further applied to predict extubation\nreadiness via generative (joint likelihood) and discriminative (support vector\nmachine) approaches. Results showed that up to 84\\% of infants who failed\nextubation could have been accurately identified prior to extubation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 03:10:48 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Onu", "Charles C.", ""], ["Kanbar", "Lara J.", ""], ["Shalish", "Wissam", ""], ["Brown", "Karen A.", ""], ["Sant'Anna", "Guilherme M.", ""], ["Kearney", "Robert E.", ""], ["Precup", "Doina", ""]]}, {"id": "1808.07992", "submitter": "Charles Onu", "authors": "Lara J. Kanbar, Charles C. Onu, Wissam Shalish, Karen A. Brown,\n  Guilherme M. Sant'Anna, Robert E. Kearney, Doina Precup", "title": "Undersampling and Bagging of Decision Trees in the Analysis of\n  Cardiorespiratory Behavior for the Prediction of Extubation Readiness in\n  Extremely Preterm Infants", "comments": "Published in: 2018 40th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extremely preterm infants often require endotracheal intubation and\nmechanical ventilation during the first days of life. Due to the detrimental\neffects of prolonged invasive mechanical ventilation (IMV), clinicians aim to\nextubate infants as soon as they deem them ready. Unfortunately, existing\nstrategies for prediction of extubation readiness vary across clinicians and\ninstitutions, and lead to high reintubation rates. We present an approach using\nRandom Forest classifiers for the analysis of cardiorespiratory variability to\npredict extubation readiness. We address the issue of data imbalance by\nemploying random undersampling of examples from the majority class before\ntraining each Decision Tree in a bag. By incorporating clinical domain\nknowledge, we further demonstrate that our classifier could have identified 71%\nof infants who failed extubation, while maintaining a success detection rate of\n78%.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 03:15:15 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Kanbar", "Lara J.", ""], ["Onu", "Charles C.", ""], ["Shalish", "Wissam", ""], ["Brown", "Karen A.", ""], ["Sant'Anna", "Guilherme M.", ""], ["Kearney", "Robert E.", ""], ["Precup", "Doina", ""]]}, {"id": "1808.08013", "submitter": "Jun Feng", "authors": "Jun Feng, Minlie Huang, Li Zhao, Yang Yang and Xiaoyan Zhu", "title": "Reinforcement Learning for Relation Classification from Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing relation classification methods that rely on distant supervision\nassume that a bag of sentences mentioning an entity pair are all describing a\nrelation for the entity pair. Such methods, performing classification at the\nbag level, cannot identify the mapping between a relation and a sentence, and\nlargely suffers from the noisy labeling problem. In this paper, we propose a\nnovel model for relation classification at the sentence level from noisy data.\nThe model has two modules: an instance selector and a relation classifier. The\ninstance selector chooses high-quality sentences with reinforcement learning\nand feeds the selected sentences into the relation classifier, and the relation\nclassifier makes sentence level prediction and provides rewards to the instance\nselector. The two modules are trained jointly to optimize the instance\nselection and relation classification processes. Experiment results show that\nour model can deal with the noise of data effectively and obtains better\nperformance for relation classification at the sentence level.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 06:08:56 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Feng", "Jun", ""], ["Huang", "Minlie", ""], ["Zhao", "Li", ""], ["Yang", "Yang", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1808.08023", "submitter": "Jiayuan He", "authors": "Jiayuan He, Jianzhong Qi, Kotagiri Ramamohanarao", "title": "A Jointly Learned Context-Aware Place of Interest Embedding for Trip\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trip recommendation is an important location-based service that helps relieve\nusers from the time and efforts for trip planning. It aims to recommend a\nsequence of places of interest (POIs) for a user to visit that maximizes the\nuser's satisfaction. When adding a POI to a recommended trip, it is essential\nto understand the context of the recommendation, including the POI popularity,\nother POIs co-occurring in the trip, and the preferences of the user. These\ncontextual factors are learned separately in existing studies, while in\nreality, they impact jointly on a user's choice of a POI to visit. In this\nstudy, we propose a POI embedding model to jointly learn the impact of these\ncontextual factors. We call the learned POI embedding a context-aware POI\nembedding. To showcase the effectiveness of this embedding, we apply it to\ngenerate trip recommendations given a user and a time budget. We propose two\ntrip recommendation algorithms based on our context-aware POI embedding. The\nfirst algorithm finds the exact optimal trip by transforming and solving the\ntrip recommendation problem as an integer linear programming problem. To\nachieve a high computation efficiency, the second algorithm finds a\nheuristically optimal trip based on adaptive large neighborhood search. We\nperform extensive experiments on real datasets. The results show that our\nproposed algorithms consistently outperform state-of-the-art algorithms in trip\nrecommendation quality, with an advantage of up to 43% in F1-score.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 06:52:04 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["He", "Jiayuan", ""], ["Qi", "Jianzhong", ""], ["Ramamohanarao", "Kotagiri", ""]]}, {"id": "1808.08065", "submitter": "Christian Sieber", "authors": "Christian Sieber, Korbinian Hagn, Christian Moldovan, Tobias\n  Ho{\\ss}feld, Wolfgang Kellerer", "title": "Towards Machine Learning-Based Optimal HAS", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile video consumption is increasing and sophisticated video quality\nadaptation strategies are required to deal with mobile throughput fluctuations.\nThese adaptation strategies have to keep the switching frequency low, the\naverage quality high and prevent stalling occurrences to ensure customer\nsatisfaction. This paper proposes a novel methodology for the design of machine\nlearning-based adaptation logics named HASBRAIN. Furthermore, the performance\nof a trained neural network against two algorithms from the literature is\nevaluated. We first use a modified existing optimization formulation to\ncalculate optimal adaptation paths with a minimum number of quality switches\nfor a wide range of videos and for challenging mobile throughput patterns.\nAfterwards we use the resulting optimal adaptation paths to train and compare\ndifferent machine learning models. The evaluation shows that an artificial\nneural network-based model can reach a high average quality with a low number\nof switches in the mobile scenario. The proposed methodology is general enough\nto be extended for further designs of machine learning-based algorithms and the\nprovided model can be deployed in on-demand streaming scenarios or be further\nrefined using reward-based mechanisms such as reinforcement learning. All\ntools, models and datasets created during the work are provided as open-source\nsoftware.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 09:41:26 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Sieber", "Christian", ""], ["Hagn", "Korbinian", ""], ["Moldovan", "Christian", ""], ["Ho\u00dffeld", "Tobias", ""], ["Kellerer", "Wolfgang", ""]]}, {"id": "1808.08068", "submitter": "Yazhou Ren", "authors": "Yazhou Ren, Xiaofan Que, Dezhong Yao, and Zenglin Xu", "title": "Self-Paced Multi-Task Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task clustering (MTC) has attracted a lot of research attentions in\nmachine learning due to its ability in utilizing the relationship among\ndifferent tasks. Despite the success of traditional MTC models, they are either\neasy to stuck into local optima, or sensitive to outliers and noisy data. To\nalleviate these problems, we propose a novel self-paced multi-task clustering\n(SPMTC) paradigm. In detail, SPMTC progressively selects data examples to train\na series of MTC models with increasing complexity, thus highly decreases the\nrisk of trapping into poor local optima. Furthermore, to reduce the negative\ninfluence of outliers and noisy data, we design a soft version of SPMTC to\nfurther improve the clustering performance. The corresponding SPMTC framework\ncan be easily solved by an alternating optimization method. The proposed model\nis guaranteed to converge and experiments on real data sets have demonstrated\nits promising results compared with state-of-the-art multi-task clustering\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 09:55:41 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Ren", "Yazhou", ""], ["Que", "Xiaofan", ""], ["Yao", "Dezhong", ""], ["Xu", "Zenglin", ""]]}, {"id": "1808.08095", "submitter": "Jeroen Zegers", "authors": "Jeroen Zegers, Hugo Van hamme", "title": "Multi-scenario deep learning for multi-speaker source separation", "comments": null, "journal-ref": "ICASSP 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in deep learning for multi-speaker source separation has received a\nboost in the last years. However, most studies are restricted to mixtures of a\nspecific number of speakers, called a specific scenario. While some works\nincluded experiments for different scenarios, research towards combining data\nof different scenarios or creating a single model for multiple scenarios have\nbeen very rare. In this work it is shown that data of a specific scenario is\nrelevant for solving another scenario. Furthermore, it is concluded that a\nsingle model, trained on different scenarios is capable of matching performance\nof scenario specific models.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 11:29:07 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Zegers", "Jeroen", ""], ["Van hamme", "Hugo", ""]]}, {"id": "1808.08097", "submitter": "Jeroen Zegers", "authors": "Jeroen Zegers, Hugo Van hamme", "title": "Memory Time Span in LSTMs for Multi-Speaker Source Separation", "comments": null, "journal-ref": "Interspeech 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With deep learning approaches becoming state-of-the-art in many speech (as\nwell as non-speech) related machine learning tasks, efforts are being taken to\ndelve into the neural networks which are often considered as a black box. In\nthis paper it is analyzed how recurrent neural network (RNNs) cope with\ntemporal dependencies by determining the relevant memory time span in a long\nshort-term memory (LSTM) cell. This is done by leaking the state variable with\na controlled lifetime and evaluating the task performance. This technique can\nbe used for any task to estimate the time span the LSTM exploits in that\nspecific scenario. The focus in this paper is on the task of separating\nspeakers from overlapping speech. We discern two effects: A long term effect,\nprobably due to speaker characterization and a short term effect, probably\nexploiting phone-size formant tracks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 11:36:15 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Zegers", "Jeroen", ""], ["Van hamme", "Hugo", ""]]}, {"id": "1808.08111", "submitter": "Sauptik Dhar", "authors": "Sauptik Dhar, Vladimir Cherkassky, Mohak Shah", "title": "Multiclass Universum SVM", "comments": "33 pages. arXiv admin note: text overlap with arXiv:1609.09162", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Universum learning for multiclass problems and propose a novel\nformulation for multiclass universum SVM (MU-SVM). We also propose an analytic\nspan bound for model selection with almost 2-4x faster computation times than\nstandard resampling techniques. We empirically demonstrate the efficacy of the\nproposed MUSVM formulation on several real world datasets achieving > 20%\nimprovement in test accuracies compared to multi-class SVM.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 06:51:09 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Dhar", "Sauptik", ""], ["Cherkassky", "Vladimir", ""], ["Shah", "Mohak", ""]]}, {"id": "1808.08121", "submitter": "Mojtaba Heidarysafa", "authors": "Mojtaba Heidarysafa, Kamran Kowsari, Donald E. Brown, Kiana Jafari\n  Meimandi, Laura E. Barnes", "title": "An Improvement of Data Classification Using Random Multimodel Deep\n  Learning (RMDL)", "comments": "published in International Journal of Machine Learning and Computing\n  (IJMLC). arXiv admin note: substantial text overlap with arXiv:1805.01890", "journal-ref": null, "doi": "10.18178/ijmlc.2018.8.4.703", "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth in the number of complex datasets every year requires\nmore enhancement in machine learning methods to provide robust and accurate\ndata classification. Lately, deep learning approaches have achieved surpassing\nresults in comparison to previous machine learning algorithms. However, finding\nthe suitable structure for these models has been a challenge for researchers.\nThis paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble,\ndeep learning approach for classification. RMDL solves the problem of finding\nthe best deep learning structure and architecture while simultaneously\nimproving robustness and accuracy through ensembles of deep learning\narchitectures. In short, RMDL trains multiple randomly generated models of Deep\nNeural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural\nNetwork (RNN) in parallel and combines their results to produce better result\nof any of those models individually. In this paper, we describe RMDL model and\ncompare the results for image and text classification as well as face\nrecognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for\nimage classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text\nclassification. Lastly, we used ORL dataset to compare the model performance on\nface recognition task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 00:38:14 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Brown", "Donald E.", ""], ["Meimandi", "Kiana Jafari", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1808.08124", "submitter": "Charles Delahunt", "authors": "Charles B Delahunt, J Nathan Kutz", "title": "Insect cyborgs: Bio-mimetic feature generators improve machine learning\n  accuracy on limited data", "comments": "14 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) classifiers always benefit from more informative input\nfeatures. We seek to auto-generate stronger feature sets in order to address\nthe difficulty that ML methods often experience given limited training data. A\nwide range of biological neural nets (BNNs) excel at fast learning, implying\nthat they are adept at extracting informative features. We can thus look to\nBNNs for tools to improve ML performance in this low-data regime. The insect\nolfactory network learns new odors very rapidly, by means of three key\nelements: A competitive inhibition layer; a high-dimensional sparse plastic\nlayer; and Hebbian updates of synaptic weights.\n  In this work, we deployed MothNet, a computational model of the insect\nolfactory network, as an automatic feature generator: Attached as a front-end\npre-processor, its Readout Neurons provided new features, derived from the\noriginal features, for use by standard ML classifiers. We found that these\n\"insect cyborgs\", i.e. classifiers that are part-insect model and part-ML\nmethod, had significantly better performance than baseline ML methods alone on\na vectorized MNIST dataset. The MothNet feature generator also substantially\nout-performed other feature generating methods such as PCA, PLS, and NNs, as\nwell as pre-training to initialize NN weights. Cyborgs improved relative test\nset accuracy by an average of 6% to 33% depending on baseline ML accuracy,\nwhile relative reduction in test set error exceeded 50% for higher baseline\naccuracy ML models. These results indicate the potential value of BNN-inspired\nfeature generators in the ML context.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 05:01:05 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 22:11:48 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 18:21:02 GMT"}, {"version": "v4", "created": "Fri, 6 Dec 2019 00:49:17 GMT"}, {"version": "v5", "created": "Fri, 11 Sep 2020 22:19:39 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Delahunt", "Charles B", ""], ["Kutz", "J Nathan", ""]]}, {"id": "1808.08149", "submitter": "Shen Li", "authors": "Hengru Xu, Shen Li, Renfen Hu, Si Li, Sheng Gao", "title": "From Random to Supervised: A Novel Dropout Mechanism Integrated with\n  Global Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is used to avoid overfitting by randomly dropping units from the\nneural networks during training. Inspired by dropout, this paper presents\nGI-Dropout, a novel dropout method integrating with global information to\nimprove neural networks for text classification. Unlike the traditional dropout\nmethod in which the units are dropped randomly according to the same\nprobability, we aim to use explicit instructions based on global information of\nthe dataset to guide the training process. With GI-Dropout, the model is\nsupposed to pay more attention to inapparent features or patterns. Experiments\ndemonstrate the effectiveness of the dropout with global information on seven\ntext classification tasks, including sentiment analysis and topic\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 14:17:01 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 02:04:51 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 07:54:51 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Xu", "Hengru", ""], ["Li", "Shen", ""], ["Hu", "Renfen", ""], ["Li", "Si", ""], ["Gao", "Sheng", ""]]}, {"id": "1808.08166", "submitter": "Zhiwei Steven Wu", "authors": "Michael Kearns, Seth Neel, Aaron Roth, Zhiwei Steven Wu", "title": "An Empirical Study of Rich Subgroup Fairness for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kearns et al. [2018] recently proposed a notion of rich subgroup fairness\nintended to bridge the gap between statistical and individual notions of\nfairness. Rich subgroup fairness picks a statistical fairness constraint (say,\nequalizing false positive rates across protected groups), but then asks that\nthis constraint hold over an exponentially or infinitely large collection of\nsubgroups defined by a class of functions with bounded VC dimension. They give\nan algorithm guaranteed to learn subject to this constraint, under the\ncondition that it has access to oracles for perfectly learning absent a\nfairness constraint. In this paper, we undertake an extensive empirical\nevaluation of the algorithm of Kearns et al. On four real datasets for which\nfairness is a concern, we investigate the basic convergence of the algorithm\nwhen instantiated with fast heuristics in place of learning oracles, measure\nthe tradeoffs between fairness and accuracy, and compare this approach with the\nrecent algorithm of Agarwal et al. [2018], which implements weaker and more\ntraditional marginal fairness constraints defined by individual protected\nattributes. We find that in general, the Kearns et al. algorithm converges\nquickly, large gains in fairness can be obtained with mild costs to accuracy,\nand that optimizing accuracy subject only to marginal fairness leads to\nclassifiers with substantial subgroup unfairness. We also provide a number of\nanalyses and visualizations of the dynamics and behavior of the Kearns et al.\nalgorithm. Overall we find this algorithm to be effective on real data, and\nrich subgroup fairness to be a viable notion in practice.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 15:08:33 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Kearns", "Michael", ""], ["Neel", "Seth", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1808.08195", "submitter": "David Aparicio", "authors": "David Apar\\'icio, Pedro Ribeiro, Tijana Milenkovi\\'c and Fernando\n  Silva", "title": "GoT-WAVE: Temporal network alignment using graphlet-orbit transitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global pairwise network alignment (GPNA) aims to find a one-to-one node\nmapping between two networks that identifies conserved network regions. GPNA\nalgorithms optimize node conservation (NC) and edge conservation (EC). NC\nquantifies topological similarity between nodes. Graphlet-based degree vectors\n(GDVs) are a state-of-the-art topological NC measure. Dynamic GDVs (DGDVs) were\nused as a dynamic NC measure within the first-ever algorithms for GPNA of\ntemporal networks: DynaMAGNA++ and DynaWAVE. The latter is superior for larger\nnetworks. We recently developed a different graphlet-based measure of temporal\nnode similarity, graphlet-orbit transitions (GoTs). Here, we use GoTs instead\nof DGDVs as a new dynamic NC measure within DynaWAVE, resulting in a new\napproach, GoT-WAVE.\n  On synthetic networks, GoT-WAVE improves DynaWAVE's accuracy by 25% and speed\nby 64%. On real networks, when optimizing only dynamic NC, each method is\nsuperior ~50% of the time. While DynaWAVE benefits more from also optimizing\ndynamic EC, only GoT-WAVE can support directed edges. Hence, GoT-WAVE is a\npromising new temporal GPNA algorithm, which efficiently optimizes dynamic NC.\nFuture work on better incorporating dynamic EC may yield further improvements.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 16:12:21 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Apar\u00edcio", "David", ""], ["Ribeiro", "Pedro", ""], ["Milenkovi\u0107", "Tijana", ""], ["Silva", "Fernando", ""]]}, {"id": "1808.08230", "submitter": "Andrew Borkowski M.D.", "authors": "Andrew A. Borkowski, Catherine P. Wilson, Steven A. Borkowski, Lauren\n  A. Deland, Stephen M. Mastorides", "title": "Using Apple Machine Learning Algorithms to Detect and Subclassify\n  Non-Small Cell Lung Cancer", "comments": "12 pages, 2 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer continues to be a major healthcare challenge with high morbidity\nand mortality rates among both men and women worldwide. The majority of lung\ncancer cases are of non-small cell lung cancer type. With the advent of\ntargeted cancer therapy, it is imperative not only to properly diagnose but\nalso sub-classify non-small cell lung cancer. In our study, we evaluated the\nutility of using Apple Create ML module to detect and sub-classify non-small\ncell carcinomas based on histopathological images. After module optimization,\nthe program detected 100% of non-small cell lung cancer images and successfully\nsubclassified the majority of the images. Trained modules, such as ours, can be\nutilized in diagnostic smartphone-based applications, augmenting diagnostic\nservices in understaffed areas of the world.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 13:57:40 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 20:09:48 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Borkowski", "Andrew A.", ""], ["Wilson", "Catherine P.", ""], ["Borkowski", "Steven A.", ""], ["Deland", "Lauren A.", ""], ["Mastorides", "Stephen M.", ""]]}, {"id": "1808.08268", "submitter": "Alexander Broad", "authors": "Alexander Broad, Todd Murphey, Brenna Argall", "title": "Learning Models for Shared Control of Human-Machine Systems with Unknown\n  Dynamics", "comments": "Robotics: Science and Systems Proceedings, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to shared control of human-machine systems. Our\nmethod assumes no a priori knowledge of the system dynamics. Instead, we learn\nboth the dynamics and information about the user's interaction from observation\nthrough the use of the Koopman operator. Using the learned model, we define an\noptimization problem to compute the optimal policy for a given task, and\ncompare the user input to the optimal input. We demonstrate the efficacy of our\napproach with a user study. We also analyze the individual nature of the\nlearned models by comparing the effectiveness of our approach when the\ndemonstration data comes from a user's own interactions, from the interactions\nof a group of users and from a domain expert. Positive results include\nstatistically significant improvements on task metrics when comparing a\nuser-only control paradigm with our shared control paradigm. Surprising results\ninclude findings that suggest that individualizing the model based on a user's\nown data does not effect the ability to learn a useful dynamic system. We\nexplore this tension as it relates to developing human-in-the-loop systems\nfurther in the discussion.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 19:07:10 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Broad", "Alexander", ""], ["Murphey", "Todd", ""], ["Argall", "Brenna", ""]]}, {"id": "1808.08270", "submitter": "Md Rizwan Parvez", "authors": "Md Rizwan Parvez, Tolga Bolukbasi, Kai-Wei Chang, Venkatesh Saligrama", "title": "Robust Text Classifier on Test-Time Budgets", "comments": "To appear at EMNLP-IJCAI 2019, 6 pages + 2 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic and interpretable learning framework for building robust\ntext classification model that achieves accuracy comparable to full models\nunder test-time budget constraints. Our approach learns a selector to identify\nwords that are relevant to the prediction tasks and passes them to the\nclassifier for processing. The selector is trained jointly with the classifier\nand directly learns to incorporate with the classifier. We further propose a\ndata aggregation scheme to improve the robustness of the classifier. Our\nlearning framework is general and can be incorporated with any type of text\nclassification model. On real-world data, we show that the proposed approach\nimproves the performance of a given classifier and speeds up the model with a\nmere loss in accuracy performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 19:22:12 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 04:29:26 GMT"}, {"version": "v3", "created": "Sun, 1 Sep 2019 22:26:48 GMT"}, {"version": "v4", "created": "Wed, 11 Sep 2019 01:21:25 GMT"}, {"version": "v5", "created": "Fri, 13 Sep 2019 19:46:04 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Parvez", "Md Rizwan", ""], ["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1808.08271", "submitter": "Frank Nielsen", "authors": "Frank Nielsen", "title": "An elementary introduction to information geometry", "comments": "56 pages, 16 figures", "journal-ref": "Entropy 2020, 22(10), 1100", "doi": "10.3390/e22101100", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey, we describe the fundamental differential-geometric structures\nof information manifolds, state the fundamental theorem of information\ngeometry, and illustrate some use cases of these information manifolds in\ninformation sciences. The exposition is self-contained by concisely introducing\nthe necessary concepts of differential geometry, but proofs are omitted for\nbrevity.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 02:33:55 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 09:54:34 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Nielsen", "Frank", ""]]}, {"id": "1808.08272", "submitter": "Fangqi Li", "authors": "Fang-Qi Li, Xu-Die Ren, Hao-Nan Guo", "title": "Probabilistic Model of Object Detection Based on Convolutional Neural\n  Network", "comments": "8 pages, 8 figures, International Conference on Communication, Signal\n  Processing and Systems (CSPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of a CNN detector and a search framework forms the basis for\nlocal object/pattern detection. To handle the waste of regional information and\nthe defective compromise between efficiency and accuracy, this paper proposes a\nprobabilistic model with a powerful search framework. By mapping an image into\na probabilistic distribution of objects, this new model gives more informative\noutputs with less computation. The setting and analytic traits are elaborated\nin this paper, followed by a series of experiments carried out on FDDB, which\nshow that the proposed model is sound, efficient and analytic.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 18:11:56 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Fang-Qi", ""], ["Ren", "Xu-Die", ""], ["Guo", "Hao-Nan", ""]]}, {"id": "1808.08280", "submitter": "Suman Sedai", "authors": "Suman Sedai, Dwarikanath Mahapatra, Zongyuan Ge, Rajib Chakravorty and\n  Rahil Garnavi", "title": "Deep multiscale convolutional feature learning for weakly supervised\n  localization of chest pathologies in X-ray images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization of chest pathologies in chest X-ray images is a challenging task\nbecause of their varying sizes and appearances. We propose a novel weakly\nsupervised method to localize chest pathologies using class aware deep\nmultiscale feature learning. Our method leverages intermediate feature maps\nfrom CNN layers at different stages of a deep network during the training of a\nclassification model using image level annotations of pathologies. During the\ntraining phase, a set of \\emph{layer relevance weights} are learned for each\npathology class and the CNN is optimized to perform pathology classification by\nconvex combination of feature maps from both shallow and deep layers using the\nlearned weights. During the test phase, to localize the predicted pathology,\nthe multiscale attention map is obtained by convex combination of class\nactivation maps from each stage using the \\emph{layer relevance weights}\nlearned during the training phase. We have validated our method using 112000\nX-ray images and compared with the state-of-the-art localization methods. We\nexperimentally demonstrate that the proposed weakly supervised method can\nimprove the localization performance of small pathologies such as nodule and\nmass while giving comparable performance for bigger pathologies e.g.,\nCardiomegaly\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 06:08:45 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Sedai", "Suman", ""], ["Mahapatra", "Dwarikanath", ""], ["Ge", "Zongyuan", ""], ["Chakravorty", "Rajib", ""], ["Garnavi", "Rahil", ""]]}, {"id": "1808.08294", "submitter": "Yeounoh Chung", "authors": "Yeounoh Chung, Peter J. Haas, Eli Upfal and Tim Kraska", "title": "Unknown Examples & Machine Learning Model Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decades, researchers and ML practitioners have come up with\nbetter and better ways to build, understand and improve the quality of ML\nmodels, but mostly under the key assumption that the training data is\ndistributed identically to the testing data. In many real-world applications,\nhowever, some potential training examples are unknown to the modeler, due to\nsample selection bias or, more generally, covariate shift, i.e., a distribution\nshift between the training and deployment stage. The resulting discrepancy\nbetween training and testing distributions leads to poor generalization\nperformance of the ML model and hence biased predictions. We provide novel\nalgorithms that estimate the number and properties of these unknown training\nexamples---unknown unknowns. This information can then be used to correct the\ntraining set, prior to seeing any test data. The key idea is to combine\nspecies-estimation techniques with data-driven methods for estimating the\nfeature values for the unknown unknowns. Experiments on a variety of ML models\nand datasets indicate that taking the unknown examples into account can yield a\nmore robust ML model that generalizes better.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 20:13:49 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 04:54:10 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Chung", "Yeounoh", ""], ["Haas", "Peter J.", ""], ["Upfal", "Eli", ""], ["Kraska", "Tim", ""]]}, {"id": "1808.08311", "submitter": "Vivek Kumar", "authors": "Cong Zhou, Michael Horgan, Vivek Kumar, Cristina Vasco, Dan Darcy", "title": "Voice Conversion with Conditional SampleRNN", "comments": "Accepted at Interspeech 2018, Hyderabad, India. This version matches\n  the final version submitted to the conference", "journal-ref": null, "doi": "10.21437/Interspeech.2018-1121", "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present a novel approach to conditioning the SampleRNN generative\nmodel for voice conversion (VC). Conventional methods for VC modify the\nperceived speaker identity by converting between source and target acoustic\nfeatures. Our approach focuses on preserving voice content and depends on the\ngenerative network to learn voice style. We first train a multi-speaker\nSampleRNN model conditioned on linguistic features, pitch contour, and speaker\nidentity using a multi-speaker speech corpus. Voice-converted speech is\ngenerated using linguistic features and pitch contour extracted from the source\nspeaker, and the target speaker identity. We demonstrate that our system is\ncapable of many-to-many voice conversion without requiring parallel data,\nenabling broad applications. Subjective evaluation demonstrates that our\napproach outperforms conventional VC methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 21:14:40 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Zhou", "Cong", ""], ["Horgan", "Michael", ""], ["Kumar", "Vivek", ""], ["Vasco", "Cristina", ""], ["Darcy", "Dan", ""]]}, {"id": "1808.08315", "submitter": "Wenbin Zhang", "authors": "Wenbin Zhang, Jianwu Wang, Daeho Jin, Lazaros Oreopoulos, Zhibo Zhang", "title": "A Deterministic Self-Organizing Map Approach and its Application on\n  Satellite Data based Cloud Type Classification", "comments": "Accepted to IEEE Big Data 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A self-organizing map (SOM) is a type of competitive artificial neural\nnetwork, which projects the high-dimensional input space of the training\nsamples into a low-dimensional space with the topology relations preserved.\nThis makes SOMs supportive of organizing and visualizing complex data sets and\nhave been pervasively used among numerous disciplines with different\napplications. Notwithstanding its wide applications, the self-organizing map is\nperplexed by its inherent randomness, which produces dissimilar SOM patterns\neven when being trained on identical training samples with the same parameters\nevery time, and thus causes usability concerns for other domain practitioners\nand precludes more potential users from exploring SOM based applications in a\nbroader spectrum. Motivated by this practical concern, we propose a\ndeterministic approach as a supplement to the standard self-organizing map. In\naccordance with the theoretical design, the experimental results with satellite\ncloud data demonstrate the effective and efficient organization as well as\nsimplification capabilities of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 21:28:36 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 19:02:57 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Zhang", "Wenbin", ""], ["Wang", "Jianwu", ""], ["Jin", "Daeho", ""], ["Oreopoulos", "Lazaros", ""], ["Zhang", "Zhibo", ""]]}, {"id": "1808.08316", "submitter": "Tu  Nguyen", "authors": "Tu Ngoc Nguyen, Tuan Tran and Wolfgang Nejdl", "title": "A Trio Neural Model for Dynamic Entity Relatedness Ranking", "comments": "In Proceedings of CoNLL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring entity relatedness is a fundamental task for many natural language\nprocessing and information retrieval applications. Prior work often studies\nentity relatedness in static settings and an unsupervised manner. However,\nentities in real-world are often involved in many different relationships,\nconsequently entity-relations are very dynamic over time. In this work, we\npropose a neural networkbased approach for dynamic entity relatedness,\nleveraging the collective attention as supervision. Our model is capable of\nlearning rich and different entity representations in a joint framework.\nThrough extensive experiments on large-scale datasets, we demonstrate that our\nmethod achieves better results than competitive baselines.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 21:29:53 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 22:00:03 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 00:38:54 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Nguyen", "Tu Ngoc", ""], ["Tran", "Tuan", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1808.08317", "submitter": "Naomi Brownstein", "authors": "A. Adolfsson, M. Ackerman, and N. C. Brownstein", "title": "To Cluster, or Not to Cluster: An Analysis of Clusterability Methods", "comments": "30 pages, 3 figures, 10 tables", "journal-ref": null, "doi": "10.1016/j.patcog.2018.10.026", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Clustering is an essential data mining tool that aims to discover inherent\ncluster structure in data. For most applications, applying clustering is only\nappropriate when cluster structure is present. As such, the study of\nclusterability, which evaluates whether data possesses such structure, is an\nintegral part of cluster analysis. However, methods for evaluating\nclusterability vary radically, making it challenging to select a suitable\nmeasure. In this paper, we perform an extensive comparison of measures of\nclusterability and provide guidelines that clustering users can reference to\nselect suitable measures for their applications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 21:36:05 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Adolfsson", "A.", ""], ["Ackerman", "M.", ""], ["Brownstein", "N. C.", ""]]}, {"id": "1808.08344", "submitter": "Liang He", "authors": "Liang He, Xianhong Chen, Can Xu and Jia Liu", "title": "Multiobjective Optimization Training of PLDA for Speaker Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current state-of-the-art text-independent speaker verification systems\ntake probabilistic linear discriminant analysis (PLDA) as their backend\nclassifiers. The parameters of PLDA are often estimated by maximizing the\nobjective function, which focuses on increasing the value of log-likelihood\nfunction, but ignoring the distinction between speakers. In order to better\ndistinguish speakers, we propose a multi-objective optimization training for\nPLDA. Experiment results show that the proposed method has more than 10%\nrelative performance improvement in both EER and MinDCF on the NIST SRE14\ni-vector challenge dataset, and about 20% relative performance improvement in\nEER on the MCE18 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 01:48:05 GMT"}, {"version": "v2", "created": "Sun, 11 Nov 2018 14:41:45 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["He", "Liang", ""], ["Chen", "Xianhong", ""], ["Xu", "Can", ""], ["Liu", "Jia", ""]]}, {"id": "1808.08361", "submitter": "Hitoshi Manabe", "authors": "Hitoshi Manabe, Katsuhiko Hayashi, Masashi Shimbo", "title": "Data-dependent Learning of Symmetric/Antisymmetric Relations for\n  Knowledge Base Completion", "comments": "In AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding-based methods for knowledge base completion (KBC) learn\nrepresentations of entities and relations in a vector space, along with the\nscoring function to estimate the likelihood of relations between entities. The\nlearnable class of scoring functions is designed to be expressive enough to\ncover a variety of real-world relations, but this expressive comes at the cost\nof an increased number of parameters. In particular, parameters in these\nmethods are superfluous for relations that are either symmetric or\nantisymmetric. To mitigate this problem, we propose a new L1 regularizer for\nComplex Embeddings, which is one of the state-of-the-art embedding-based\nmethods for KBC. This regularizer promotes symmetry or antisymmetry of the\nscoring function on a relation-by-relation basis, in accordance with the\nobserved data. Our empirical evaluation shows that the proposed method\noutperforms the original Complex Embeddings and other baseline methods on the\nFB15k dataset.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 06:00:44 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Manabe", "Hitoshi", ""], ["Hayashi", "Katsuhiko", ""], ["Shimbo", "Masashi", ""]]}, {"id": "1808.08366", "submitter": "Michael Gallaugher", "authors": "M.P.B. Gallaugher, C. Biernacki, and P.D. McNicholas", "title": "Parameter-wise co-clustering for high-dimensional data", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, data dimensionality has increasingly become a concern,\nleading to many parameter and dimension reduction techniques being proposed in\nthe literature. A parameter-wise co-clustering model, for data modelled via\ncontinuous random variables, is presented. The proposed model, although\nallowing more flexibility, still maintains the very high degree of parsimony\nachieved by traditional co-clustering. A stochastic expectation-maximization\n(SEM) algorithm along with a Gibbs sampler is used for parameter estimation and\nan integrated complete log-likelihood criterion is used for model selection.\nSimulated and real datasets are used for illustration and comparison with\ntraditional co-clustering.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 07:07:42 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:02:54 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Gallaugher", "M. P. B.", ""], ["Biernacki", "C.", ""], ["McNicholas", "P. D.", ""]]}, {"id": "1808.08414", "submitter": "Ammar Gilani", "authors": "Ammar Gilani, Maryam Amirmazlaghani", "title": "Unsupervised Hypergraph Feature Selection via a Novel Point-Weighting\n  Framework and Low-Rank Representation", "comments": "33 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection methods are widely used in order to solve the 'curse of\ndimensionality' problem. Many proposed feature selection frameworks, treat all\ndata points equally; neglecting their different representation power and\nimportance. In this paper, we propose an unsupervised hypergraph feature\nselection method via a novel point-weighting framework and low-rank\nrepresentation that captures the importance of different data points. We\nintroduce a novel soft hypergraph with low complexity to model data. Then, we\nformulate the feature selection as an optimization problem to preserve local\nrelationships and also global structure of data. Our approach for global\nstructure preservation helps the framework overcome the problem of\nunavailability of data labels in unsupervised learning. The proposed feature\nselection method treats with different data points based on their importance in\ndefining data structure and representation power. Moreover, since the\nrobustness of feature selection methods against noise and outlier is of great\nimportance, we adopt low-rank representation in our model. Also, we provide an\nefficient algorithm to solve the proposed optimization problem. The\ncomputational cost of the proposed algorithm is lower than many\nstate-of-the-art methods which is of high importance in feature selection\ntasks. We conducted comprehensive experiments with various evaluation methods\non different benchmark data sets. These experiments indicate significant\nimprovement, compared with state-of-the-art feature selection methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 12:02:41 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 02:47:16 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Gilani", "Ammar", ""], ["Amirmazlaghani", "Maryam", ""]]}, {"id": "1808.08416", "submitter": "Abbas Mehrabian", "authors": "Gabor Lugosi and Abbas Mehrabian", "title": "Multiplayer bandits without observing collision information", "comments": "To appear in Mathematics of Operations Research. 34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study multiplayer stochastic multi-armed bandit problems in which the\nplayers cannot communicate and if two or more players pull the same arm, a\ncollision occurs and the involved players receive zero reward. We consider two\nfeedback models: a model in which the players can observe whether a collision\nhas occurred and a more difficult setup when no collision information is\navailable. We give the first theoretical guarantees for the second model: an\nalgorithm with a logarithmic regret, and an algorithm with a square-root regret\ntype that does not depend on the gaps between the means. For the first model,\nwe give the first square-root regret bounds that do not depend on the gaps.\nBuilding on these ideas, we also give an algorithm for reaching approximate\nNash equilibria quickly in stochastic anti-coordination games.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 12:06:17 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 22:32:27 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lugosi", "Gabor", ""], ["Mehrabian", "Abbas", ""]]}, {"id": "1808.08437", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho and Victor O.K. Li", "title": "Meta-Learning for Low-Resource Neural Machine Translation", "comments": "Accepted as a full paper at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to extend the recently introduced model-agnostic\nmeta-learning algorithm (MAML) for low-resource neural machine translation\n(NMT). We frame low-resource translation as a meta-learning problem, and we\nlearn to adapt to low-resource languages based on multilingual high-resource\nlanguage tasks. We use the universal lexical\nrepresentation~\\citep{gu2018universal} to overcome the input-output mismatch\nacross different languages. We evaluate the proposed meta-learning strategy\nusing eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt,\nNl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro,\nLv, Fi, Tr and Ko) as target tasks. We show that the proposed approach\nsignificantly outperforms the multilingual, transfer learning based\napproach~\\citep{zoph2016transfer} and enables us to train a competitive NMT\nsystem with only a fraction of training examples. For instance, the proposed\napproach can achieve as high as 22.04 BLEU on Romanian-English WMT'16 by seeing\nonly 16,000 translated words (~600 parallel sentences).\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 15:10:59 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Gu", "Jiatao", ""], ["Wang", "Yong", ""], ["Chen", "Yun", ""], ["Cho", "Kyunghyun", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1808.08460", "submitter": "Smitha Milli", "authors": "Smitha Milli, John Miller, Anca D. Dragan, Moritz Hardt", "title": "The Social Cost of Strategic Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consequential decision-making typically incentivizes individuals to behave\nstrategically, tailoring their behavior to the specifics of the decision rule.\nA long line of work has therefore sought to counteract strategic behavior by\ndesigning more conservative decision boundaries in an effort to increase\nrobustness to the effects of strategic covariate shift. We show that these\nefforts benefit the institutional decision maker at the expense of the\nindividuals being classified. Introducing a notion of social burden, we prove\nthat any increase in institutional utility necessarily leads to a corresponding\nincrease in social burden. Moreover, we show that the negative externalities of\nstrategic classification can disproportionately harm disadvantaged groups in\nthe population. Our results highlight that strategy-robustness must be weighed\nagainst considerations of social welfare and fairness.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 18:31:52 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 13:51:18 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Milli", "Smitha", ""], ["Miller", "John", ""], ["Dragan", "Anca D.", ""], ["Hardt", "Moritz", ""]]}, {"id": "1808.08469", "submitter": "Jinchi Lv", "authors": "Emre Demirkaya, Yingying Fan, Lan Gao, Jinchi Lv, Patrick Vossler and\n  Jingbo Wang", "title": "Nonparametric Inference of Heterogeneous Treatment Effects with\n  Two-Scale Distributional Nearest Neighbors", "comments": "65 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding heterogeneous treatment effects (HTE) plays a key role in many\ncontemporary causal inference applications arising from different areas. Most\nof the existing works have focused on the estimation of HTE. Yet the\nstatistical inference aspect of the problem remains relatively undeveloped. In\nthis paper we investigate the inference of HTE in a nonparametric setting for\nrandomized experiments. We formulate the problem as two separate nonparametric\nmean regressions, one for control group and the other for treatment group. For\neach mean regression, we extend the tool of $k$-nearest neighbors to the\nframework of distributional nearest neighbors (DNN). We show that the DNN\nestimator has two equivalent representations of L-statistic and U-statistic,\nwhere the former endorses easy and fast implementation, and the latter enables\nus to obtain higher-order asymptotic expansion of bias and establish the\nasymptotic normality. To reduce the finite sample bias of DNN, we further\nsuggest a new method of two-scale distributional nearest neighbors (TDNN).\nUnder some regularity conditions, we show through delicate higher-order\nasymptotic expansions that the TDNN heterogeneous treatment effect estimator is\nasymptotically normal. We further establish the consistency of the variance\nestimates of the TDNN estimator with both jackknife and bootstrap, enabling\nuser-friendly inference tools for heterogeneous treatment effects. The\ntheoretical results and appealing finite-sample performance of the suggested\nTDNN method are illustrated with several simulation examples and a children's\nbirth weight application.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 21:02:46 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 22:49:58 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Demirkaya", "Emre", ""], ["Fan", "Yingying", ""], ["Gao", "Lan", ""], ["Lv", "Jinchi", ""], ["Vossler", "Patrick", ""], ["Wang", "Jingbo", ""]]}, {"id": "1808.08478", "submitter": "Yunpeng Zhao", "authors": "Yunpeng Zhao", "title": "Network Inference from Temporal-Dependent Grouped Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social network analysis, the observed data is usually some social\nbehavior, such as the formation of groups, rather than an explicit network\nstructure. Zhao and Weko (2017) propose a model-based approach called the hub\nmodel to infer implicit networks from grouped observations. The hub model\nassumes independence between groups, which sometimes is not valid in practice.\nIn this article, we generalize the idea of the hub model into the case of\ngrouped observations with temporal dependence. As in the hub model, we assume\nthat the group at each time point is gathered by one leader. Unlike in the hub\nmodel, the group leaders are not sampled independently but follow a Markov\nchain, and other members in adjacent groups can also be correlated.\n  An expectation-maximization (EM) algorithm is developed for this model and a\npolynomial-time algorithm is proposed for the E-step. The performance of the\nnew model is evaluated under different simulation settings. We apply this model\nto a data set of the Kibale Chimpanzee Project.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 22:42:46 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Zhao", "Yunpeng", ""]]}, {"id": "1808.08493", "submitter": "Emmanouil Antonios Platanios", "authors": "Emmanouil Antonios Platanios and Mrinmaya Sachan and Graham Neubig and\n  Tom Mitchell", "title": "Contextual Parameter Generation for Universal Neural Machine Translation", "comments": "Published in the proceedings of Empirical Methods in Natural Language\n  Processing (EMNLP), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple modification to existing neural machine translation (NMT)\nmodels that enables using a single universal model to translate between\nmultiple languages while allowing for language specific parameterization, and\nthat can also be used for domain adaptation. Our approach requires no changes\nto the model architecture of a standard NMT system, but instead introduces a\nnew component, the contextual parameter generator (CPG), that generates the\nparameters of the system (e.g., weights in a neural network). This parameter\ngenerator accepts source and target language embeddings as input, and generates\nthe parameters for the encoder and the decoder, respectively. The rest of the\nmodel remains unchanged and is shared across all languages. We show how this\nsimple modification enables the system to use monolingual data for training and\nalso perform zero-shot translation. We further show it is able to surpass\nstate-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and\nthat the learned language embeddings are able to uncover interesting\nrelationships between languages.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 01:17:50 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Platanios", "Emmanouil Antonios", ""], ["Sachan", "Mrinmaya", ""], ["Neubig", "Graham", ""], ["Mitchell", "Tom", ""]]}, {"id": "1808.08531", "submitter": "Dongyu Liu", "authors": "Dongyu Liu, Weiwei Cui, Kai Jin, Yuxiao Guo, Huamin Qu", "title": "DeepTracker: Visualizing the Training Process of Convolutional Neural\n  Networks", "comments": "Published at ACM Transactions on Intelligent Systems and Technology\n  (in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have achieved remarkable success in\nvarious fields. However, training an excellent CNN is practically a\ntrial-and-error process that consumes a tremendous amount of time and computer\nresources. To accelerate the training process and reduce the number of trials,\nexperts need to understand what has occurred in the training process and why\nthe resulting CNN behaves as such. However, current popular training platforms,\nsuch as TensorFlow, only provide very little and general information, such as\ntraining/validation errors, which is far from enough to serve this purpose. To\nbridge this gap and help domain experts with their training tasks in a\npractical environment, we propose a visual analytics system, DeepTracker, to\nfacilitate the exploration of the rich dynamics of CNN training processes and\nto identify the unusual patterns that are hidden behind the huge amount of\ntraining log. Specifically,we combine a hierarchical index mechanism and a set\nof hierarchical small multiples to help experts explore the entire training log\nfrom different levels of detail. We also introduce a novel cube-style\nvisualization to reveal the complex correlations among multiple types of\nheterogeneous training data including neuron weights, validation images, and\ntraining iterations. Three case studies are conducted to demonstrate how\nDeepTracker provides its users with valuable knowledge in an industry-level CNN\ntraining process, namely in our case, training ResNet-50 on the ImageNet\ndataset. We show that our method can be easily applied to other\nstate-of-the-art \"very deep\" CNN models.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 11:09:44 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Liu", "Dongyu", ""], ["Cui", "Weiwei", ""], ["Jin", "Kai", ""], ["Guo", "Yuxiao", ""], ["Qu", "Huamin", ""]]}, {"id": "1808.08558", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki and Hiroshi Abe and Tomoya Murata and Shingo Horiuchi and\n  Kotaro Ito and Tokuma Wachi and So Hirai and Masatoshi Yukishima and Tomoaki\n  Nishimura", "title": "Spectral Pruning: Compressing Deep Neural Networks via Spectral Analysis\n  and its Generalization Error", "comments": "17 pages, 4 figures. Accepted in IJCAI-PRICAI 2020. Proceedings of\n  the Twenty-Ninth International Joint Conference on Artificial Intelligence,\n  pages 2839--2846", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression techniques for deep neural network models are becoming very\nimportant for the efficient execution of high-performance deep learning systems\non edge-computing devices. The concept of model compression is also important\nfor analyzing the generalization error of deep learning, known as the\ncompression-based error bound. However, there is still huge gap between a\npractically effective compression method and its rigorous background of\nstatistical learning theory. To resolve this issue, we develop a new\ntheoretical framework for model compression and propose a new pruning method\ncalled {\\it spectral pruning} based on this framework. We define the ``degrees\nof freedom'' to quantify the intrinsic dimensionality of a model by using the\neigenvalue distribution of the covariance matrix across the internal nodes and\nshow that the compression ability is essentially controlled by this quantity.\nMoreover, we present a sharp generalization error bound of the compressed model\nand characterize the bias--variance tradeoff induced by the compression\nprocedure. We apply our method to several datasets to justify our theoretical\nanalyses and show the superiority of the the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 14:25:52 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 06:09:34 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Suzuki", "Taiji", ""], ["Abe", "Hiroshi", ""], ["Murata", "Tomoya", ""], ["Horiuchi", "Shingo", ""], ["Ito", "Kotaro", ""], ["Wachi", "Tokuma", ""], ["Hirai", "So", ""], ["Yukishima", "Masatoshi", ""], ["Nishimura", "Tomoaki", ""]]}, {"id": "1808.08609", "submitter": "Pasquale Minervini", "authors": "Pasquale Minervini, Sebastian Riedel", "title": "Adversarially Regularising Neural NLI Models to Integrate Logical\n  Background Knowledge", "comments": "Accepted at the SIGNLL Conference on Computational Natural Language\n  Learning (CoNLL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are inputs to machine learning models designed to cause\nthe model to make a mistake. They are useful for understanding the shortcomings\nof machine learning models, interpreting their results, and for regularisation.\nIn NLP, however, most example generation strategies produce input text by using\nknown, pre-specified semantic transformations, requiring significant manual\neffort and in-depth understanding of the problem and domain. In this paper, we\ninvestigate the problem of automatically generating adversarial examples that\nviolate a set of given First-Order Logic constraints in Natural Language\nInference (NLI). We reduce the problem of identifying such adversarial examples\nto a combinatorial optimisation problem, by maximising a quantity measuring the\ndegree of violation of such constraints and by using a language model for\ngenerating linguistically-plausible examples. Furthermore, we propose a method\nfor adversarially regularising neural NLI models for incorporating background\nknowledge. Our results show that, while the proposed method does not always\nimprove results on the SNLI and MultiNLI datasets, it significantly and\nconsistently increases the predictive accuracy on adversarially-crafted\ndatasets -- up to a 79.6% relative improvement -- while drastically reducing\nthe number of background knowledge violations. Furthermore, we show that\nadversarial examples transfer among model architectures, and that the proposed\nadversarial training procedure improves the robustness of NLI models to\nadversarial examples.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 18:36:20 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Minervini", "Pasquale", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1808.08613", "submitter": "Dewan Fayzur", "authors": "Dewan Fayzur", "title": "Ensemble Learning Applied to Classify GPS Trajectories of Birds into\n  Male or Female", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our first-place solution to the Animal Behavior Challenge (ABC\n2018) on predicting gender of bird from its GPS trajectory. The task consisted\nin predicting the gender of shearwater based on how they navigate themselves\nacross a big ocean. The trajectories are collected from GPS loggers attached on\nshearwaters' body, and represented as a variable-length sequence of GPS points\n(latitude and longitude), and associated meta-information, such as the sun\nazimuth, the sun elevation, the daytime, the elapsed time on each GPS location\nafter starting the trip, the local time (date is trimmed), and the indicator of\nthe day starting the from the trip. We used ensemble of several variants of\nGradient Boosting Classifier along with Gaussian Process Classifier and Support\nVector Classifier after extensive feature engineering and we ranked first out\nof 74 registered teams. The variants of Gradient Boosting Classifier we tried\nare CatBoost (Developed by Yandex), LightGBM (Developed by Microsoft), XGBoost\n(Developed by Distributed Machine Learning Community). Our approach could\neasily be adapted to other applications in which the goal is to predict a\nclassification output from a variable-length sequence.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 19:19:24 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Fayzur", "Dewan", ""]]}, {"id": "1808.08618", "submitter": "Vadim Sokolov", "authors": "Nicholas Polson and Vadim Sokolov", "title": "Deep Learning: Computational Aspects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we review computational aspects of Deep Learning (DL). Deep\nlearning uses network architectures consisting of hierarchical layers of latent\nvariables to construct predictors for high-dimensional input-output models.\nTraining a deep learning architecture is computationally intensive, and\nefficient linear algebra libraries is the key for training and inference.\nStochastic gradient descent (SGD) optimization and batch sampling are used to\nlearn from massive data sets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 20:26:11 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 21:54:41 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Polson", "Nicholas", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1808.08619", "submitter": "Samuel Yeom", "authors": "Samuel Yeom, Michael Carl Tschantz", "title": "Avoiding Disparity Amplification under Different Worldviews", "comments": "This is a draft version. For the published version, please go to\n  https://dl.acm.org/doi/10.1145/3442188.3445892", "journal-ref": null, "doi": "10.1145/3442188.3445892", "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We mathematically compare four competing definitions of group-level\nnondiscrimination: demographic parity, equalized odds, predictive parity, and\ncalibration. Using the theoretical framework of Friedler et al., we study the\nproperties of each definition under various worldviews, which are assumptions\nabout how, if at all, the observed data is biased. We argue that different\nworldviews call for different definitions of fairness, and we specify the\nworldviews that, when combined with the desire to avoid a criterion for\ndiscrimination that we call disparity amplification, motivate demographic\nparity and equalized odds. We also argue that predictive parity and calibration\nare insufficient for avoiding disparity amplification because predictive parity\nallows an arbitrarily large inter-group disparity and calibration is not robust\nto post-processing. Finally, we define a worldview that is more realistic than\nthe previously considered ones, and we introduce a new notion of fairness that\ncorresponds to this worldview.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 20:36:58 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 22:03:32 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 18:08:07 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 22:09:21 GMT"}, {"version": "v5", "created": "Thu, 2 Jul 2020 17:49:29 GMT"}, {"version": "v6", "created": "Sat, 10 Oct 2020 03:23:48 GMT"}, {"version": "v7", "created": "Tue, 9 Mar 2021 18:22:46 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yeom", "Samuel", ""], ["Tschantz", "Michael Carl", ""]]}, {"id": "1808.08627", "submitter": "Jundong Li", "authors": "Jundong Li, Liang Wu, Huan Liu", "title": "Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As opposed to manual feature engineering which is tedious and difficult to\nscale, network representation learning has attracted a surge of research\ninterests as it automates the process of feature learning on graphs. The\nlearned low-dimensional node vector representation is generalizable and eases\nthe knowledge discovery process on graphs by enabling various off-the-shelf\nmachine learning tools to be directly applied. Recent research has shown that\nthe past decade of network embedding approaches either explicitly factorize a\ncarefully designed matrix to obtain the low-dimensional node vector\nrepresentation or are closely related to implicit matrix factorization, with\nthe fundamental assumption that the factorized node connectivity matrix is\nlow-rank. Nonetheless, the global low-rank assumption does not necessarily hold\nespecially when the factorized matrix encodes complex node interactions, and\nthe resultant single low-rank embedding matrix is insufficient to capture all\nthe observed connectivity patterns. In this regard, we propose a novel\nmulti-level network embedding framework BoostNE, which can learn multiple\nnetwork embedding representations of different granularity from coarse to fine\nwithout imposing the prevalent global low-rank assumption. The proposed BoostNE\nmethod is also in line with the successful gradient boosting method in ensemble\nlearning as multiple weak embeddings lead to a stronger and more effective one.\nWe assess the effectiveness of the proposed BoostNE framework by comparing it\nwith existing state-of-the-art network embedding methods on various datasets,\nand the experimental results corroborate the superiority of the proposed\nBoostNE network embedding framework.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 21:34:36 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Jundong", ""], ["Wu", "Liang", ""], ["Liu", "Huan", ""]]}, {"id": "1808.08640", "submitter": "Yu-Hsuan Kuo", "authors": "Yu-Hsuan Kuo, Zhenhui Li, Daniel Kifer", "title": "Detecting Outliers in Data with Correlated Measures", "comments": "10 pages", "journal-ref": null, "doi": "10.1145/3269206.3271798", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in sensor technology have enabled the collection of large-scale\ndatasets. Such datasets can be extremely noisy and often contain a significant\namount of outliers that result from sensor malfunction or human operation\nfaults. In order to utilize such data for real-world applications, it is\ncritical to detect outliers so that models built from these datasets will not\nbe skewed by outliers.\n  In this paper, we propose a new outlier detection method that utilizes the\ncorrelations in the data (e.g., taxi trip distance vs. trip time). Different\nfrom existing outlier detection methods, we build a robust regression model\nthat explicitly models the outliers and detects outliers simultaneously with\nthe model fitting.\n  We validate our approach on real-world datasets against methods specifically\ndesigned for each dataset as well as the state of the art outlier detectors.\nOur outlier detection method achieves better performances, demonstrating the\nrobustness and generality of our method. Last, we report interesting case\nstudies on some outliers that result from atypical events.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 23:02:54 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Kuo", "Yu-Hsuan", ""], ["Li", "Zhenhui", ""], ["Kifer", "Daniel", ""]]}, {"id": "1808.08646", "submitter": "Lily Hu", "authors": "Lily Hu, Nicole Immorlica, Jennifer Wortman Vaughan", "title": "The Disparate Effects of Strategic Manipulation", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3287560.3287597", "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When consequential decisions are informed by algorithmic input, individuals\nmay feel compelled to alter their behavior in order to gain a system's\napproval. Models of agent responsiveness, termed \"strategic manipulation,\"\nanalyze the interaction between a learner and agents in a world where all\nagents are equally able to manipulate their features in an attempt to \"trick\" a\npublished classifier. In cases of real world classification, however, an\nagent's ability to adapt to an algorithm is not simply a function of her\npersonal interest in receiving a positive classification, but is bound up in a\ncomplex web of social factors that affect her ability to pursue certain action\nresponses. In this paper, we adapt models of strategic manipulation to capture\ndynamics that may arise in a setting of social inequality wherein candidate\ngroups face different costs to manipulation. We find that whenever one group's\ncosts are higher than the other's, the learner's equilibrium strategy exhibits\nan inequality-reinforcing phenomenon wherein the learner erroneously admits\nsome members of the advantaged group, while erroneously excluding some members\nof the disadvantaged group. We also consider the effects of interventions in\nwhich a learner subsidizes members of the disadvantaged group, lowering their\ncosts in order to improve her own classification performance. Here we encounter\na paradoxical result: there exist cases in which providing a subsidy improves\nonly the learner's utility while actually making both candidate groups\nworse-off--even the group receiving the subsidy. Our results reveal the\npotentially adverse social ramifications of deploying tools that attempt to\nevaluate an individual's \"quality\" when agents' capacities to adaptively\nrespond differ.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 00:54:02 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 13:30:10 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 18:54:29 GMT"}, {"version": "v4", "created": "Fri, 10 May 2019 15:14:57 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Hu", "Lily", ""], ["Immorlica", "Nicole", ""], ["Vaughan", "Jennifer Wortman", ""]]}, {"id": "1808.08703", "submitter": "Afroz Ahamad Siddiqui", "authors": "Afroz Ahamad", "title": "Generating Text through Adversarial Training using Skip-Thought Vectors", "comments": "NAACL 2019: https://www.aclweb.org/anthology/N19-3008", "journal-ref": "\"Proceedings of the 2019 Conference of the North {A}merican\n  Chapter of the Association for Computational Linguistics: Student Research\n  Workshop, Jun 2019, Pages 53-60\"", "doi": "10.18653/v1/N19-3008", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GANs have been shown to perform exceedingly well on tasks pertaining to image\ngeneration and style transfer. In the field of language modelling, word\nembeddings such as GLoVe and word2vec are state-of-the-art methods for applying\nneural network models on textual data. Attempts have been made to utilize GANs\nwith word embeddings for text generation. This study presents an approach to\ntext generation using Skip-Thought sentence embeddings with GANs based on\ngradient penalty functions and f-measures. The proposed architecture aims to\nreproduce writing style in the generated text by modelling the way of\nexpression at a sentence level across all the works of an author. Extensive\nexperiments were run in different embedding settings on a variety of tasks\nincluding conditional text generation and language generation. The model\noutperforms baseline text generation networks across several automated\nevaluation metrics like BLEU-n, METEOR and ROUGE. Further, wide applicability\nand effectiveness in real life tasks are demonstrated through human judgement\nscores.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 06:51:07 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 21:37:36 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 10:18:53 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ahamad", "Afroz", ""]]}, {"id": "1808.08720", "submitter": "Thomas Demeester", "authors": "Thomas Demeester, Johannes Deleu, Fr\\'ederic Godin, Chris Develder", "title": "Predefined Sparseness in Recurrent Sequence Models", "comments": "the SIGNLL Conference on Computational Natural Language Learning\n  (CoNLL, 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inducing sparseness while training neural networks has been shown to yield\nmodels with a lower memory footprint but similar effectiveness to dense models.\nHowever, sparseness is typically induced starting from a dense model, and thus\nthis advantage does not hold during training. We propose techniques to enforce\nsparseness upfront in recurrent sequence models for NLP applications, to also\nbenefit training. First, in language modeling, we show how to increase hidden\nstate sizes in recurrent layers without increasing the number of parameters,\nleading to more expressive models. Second, for sequence labeling, we show that\nword embeddings with predefined sparseness lead to similar performance as dense\nembeddings, at a fraction of the number of trainable parameters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 07:55:41 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Demeester", "Thomas", ""], ["Deleu", "Johannes", ""], ["Godin", "Fr\u00e9deric", ""], ["Develder", "Chris", ""]]}, {"id": "1808.08750", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Carlos R. Medina Temme, Jonas Rauber, Heiko H.\n  Sch\\\"utt, Matthias Bethge, Felix A. Wichmann", "title": "Generalisation in humans and deep neural networks", "comments": "Added optimal probability aggregation method to appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the robustness of humans and current convolutional deep neural\nnetworks (DNNs) on object recognition under twelve different types of image\ndegradations. First, using three well known DNNs (ResNet-152, VGG-19,\nGoogLeNet) we find the human visual system to be more robust to nearly all of\nthe tested image manipulations, and we observe progressively diverging\nclassification error-patterns between humans and DNNs when the signal gets\nweaker. Secondly, we show that DNNs trained directly on distorted images\nconsistently surpass human performance on the exact distortion types they were\ntrained on, yet they display extremely poor generalisation abilities when\ntested on other distortion types. For example, training on salt-and-pepper\nnoise does not imply robustness on uniform white noise and vice versa. Thus,\nchanges in the noise distribution between training and testing constitutes a\ncrucial challenge to deep learning vision systems that can be systematically\naddressed in a lifelong machine learning approach. Our new dataset consisting\nof 83K carefully measured human psychophysical trials provide a useful\nreference for lifelong robustness against image degradations set by the human\nvisual system.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 09:17:57 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 16:26:58 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 09:05:30 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Geirhos", "Robert", ""], ["Temme", "Carlos R. Medina", ""], ["Rauber", "Jonas", ""], ["Sch\u00fctt", "Heiko H.", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""]]}, {"id": "1808.08755", "submitter": "Jessa Bekker", "authors": "Jessa Bekker and Jesse Davis", "title": "Learning from Positive and Unlabeled Data under the Selected At Random\n  Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many interesting tasks, such as medical diagnosis and web page\nclassification, a learner only has access to some positively labeled examples\nand many unlabeled examples. Learning from this type of data requires making\nassumptions about the true distribution of the classes and/or the mechanism\nthat was used to select the positive examples to be labeled. The commonly made\nassumptions, separability of the classes and positive examples being selected\ncompletely at random, are very strong. This paper proposes a weaker assumption\nthat assumes the positive examples to be selected at random, conditioned on\nsome of the attributes. To learn under this assumption, an EM method is\nproposed. Experiments show that our method is not only very capable of learning\nunder this assumption, but it also outperforms the state of the art for\nlearning under the selected completely at random assumption.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 09:39:52 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bekker", "Jessa", ""], ["Davis", "Jesse", ""]]}, {"id": "1808.08762", "submitter": "Aarne Talman", "authors": "Aarne Talman, Anssi Yli-Jyr\\\"a and J\\\"org Tiedemann", "title": "Sentence Embeddings in NLI with Iterative Refinement Encoders", "comments": "To appear in JNLE", "journal-ref": "Nat. Lang. Eng. 25 (2019) 467-482", "doi": "10.1017/S1351324919000202", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence-level representations are necessary for various NLP tasks. Recurrent\nneural networks have proven to be very effective in learning distributed\nrepresentations and can be trained efficiently on natural language inference\ntasks. We build on top of one such model and propose a hierarchy of BiLSTM and\nmax pooling layers that implements an iterative refinement strategy and yields\nstate of the art results on the SciTail dataset as well as strong results for\nSNLI and MultiNLI. We can show that the sentence embeddings learned in this way\ncan be utilized in a wide variety of transfer learning tasks, outperforming\nInferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence\nembedding evaluation tasks. Furthermore, our model beats the InferSent model in\n8 out of 10 recently published SentEval probing tasks designed to evaluate\nsentence embeddings' ability to capture some of the important linguistic\nproperties of sentences.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 09:50:56 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 19:50:52 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Talman", "Aarne", ""], ["Yli-Jyr\u00e4", "Anssi", ""], ["Tiedemann", "J\u00f6rg", ""]]}, {"id": "1808.08763", "submitter": "Yuanlong Chen", "authors": "Yuanlong Chen", "title": "On the convergence of optimistic policy iteration for stochastic\n  shortest path problem", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove some convergence results of a special case of\noptimistic policy iteration algorithm for stochastic shortest path problem. We\nconsider both Monte Carlo and $TD(\\lambda)$ methods for the policy evaluation\nstep under the condition that the termination state will eventually be reached\nalmost surely.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 09:51:03 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 00:51:38 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Chen", "Yuanlong", ""]]}, {"id": "1808.08765", "submitter": "Nicolas Gillis", "authors": "J\\'er\\'emy E. Cohen, Nicolas Gillis", "title": "Identifiability of Complete Dictionary Learning", "comments": "19 pages, 2 figures, new title, added references and discussions", "journal-ref": "SIAM Journal on Mathematics of Data Science 1 (3), pp. 518-536,\n  2019", "doi": "10.1137/18M1233339", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse component analysis (SCA), also known as complete dictionary learning,\nis the following problem: Given an input matrix $M$ and an integer $r$, find a\ndictionary $D$ with $r$ columns and a matrix $B$ with $k$-sparse columns (that\nis, each column of $B$ has at most $k$ non-zero entries) such that $M \\approx\nDB$. A key issue in SCA is identifiability, that is, characterizing the\nconditions under which $D$ and $B$ are essentially unique (that is, they are\nunique up to permutation and scaling of the columns of $D$ and rows of $B$).\nAlthough SCA has been vastly investigated in the last two decades, only a few\nworks have tackled this issue in the deterministic scenario, and no work\nprovides reasonable bounds in the minimum number of samples (that is, columns\nof $M$) that leads to identifiability. In this work, we provide new results in\nthe deterministic scenario when the data has a low-rank structure, that is,\nwhen $D$ is (under)complete. While previous bounds feature a combinatorial term\n$r \\choose k$, we exhibit a sufficient condition involving\n$\\mathcal{O}(r^3/(r-k)^2)$ samples that yields an essentially unique\ndecomposition, as long as these data points are well spread among the subspaces\nspanned by $r-1$ columns of $D$. We also exhibit a necessary lower bound on the\nnumber of samples that contradicts previous results in the literature when $k$\nequals $r-1$. Our bounds provide a drastic improvement compared to the state of\nthe art, and imply for example that for a fixed proportion of zeros (constant\nand independent of $r$, e.g., 10\\% of zero entries in $B$), one only requires\n$\\mathcal{O}(r)$ data points to guarantee identifiability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 10:04:36 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 08:22:30 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Cohen", "J\u00e9r\u00e9my E.", ""], ["Gillis", "Nicolas", ""]]}, {"id": "1808.08766", "submitter": "Aaqib Saeed", "authors": "Aaqib Saeed, Tanir Ozcelebi, Stojan Trajanovski, Johan Lukkien", "title": "Learning behavioral context recognition with multi-stream temporal\n  convolutional networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart devices of everyday use (such as smartphones and wearables) are\nincreasingly integrated with sensors that provide immense amounts of\ninformation about a person's daily life such as behavior and context. The\nautomatic and unobtrusive sensing of behavioral context can help develop\nsolutions for assisted living, fitness tracking, sleep monitoring, and several\nother fields. Towards addressing this issue, we raise the question: can a\nmachine learn to recognize a diverse set of contexts and activities in a\nreal-life through joint learning from raw multi-modal signals (e.g.\naccelerometer, gyroscope and audio etc.)? In this paper, we propose a\nmulti-stream temporal convolutional network to address the problem of\nmulti-label behavioral context recognition. A four-stream network architecture\nhandles learning from each modality with a contextualization module which\nincorporates extracted representations to infer a user's context. Our empirical\nevaluation suggests that a deep convolutional network trained end-to-end\nachieves an optimal recognition rate. Furthermore, the presented architecture\ncan be extended to include similar sensors for performance improvements and\nhandles missing modalities through multi-task learning without any manual\nfeature engineering on highly imbalanced and sparsely labeled dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 10:06:01 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Saeed", "Aaqib", ""], ["Ozcelebi", "Tanir", ""], ["Trajanovski", "Stojan", ""], ["Lukkien", "Johan", ""]]}, {"id": "1808.08773", "submitter": "Pratik Jawanpuria", "authors": "Pratik Jawanpuria, Arjun Balgovind, Anoop Kunchukuttan, Bamdev Mishra", "title": "Learning Multilingual Word Embeddings in Latent Metric Space: A\n  Geometric Approach", "comments": "Accepted in Transactions of the Association for Computational\n  Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel geometric approach for learning bilingual mappings given\nmonolingual embeddings and a bilingual dictionary. Our approach decouples\nlearning the transformation from the source language to the target language\ninto (a) learning rotations for language-specific embeddings to align them to a\ncommon space, and (b) learning a similarity metric in the common space to model\nsimilarities between the embeddings. We model the bilingual mapping problem as\nan optimization problem on smooth Riemannian manifolds. We show that our\napproach outperforms previous approaches on the bilingual lexicon induction and\ncross-lingual word similarity tasks. We also generalize our framework to\nrepresent multiple languages in a common latent space. In particular, the\nlatent space representations for several languages are learned jointly, given\nbilingual dictionaries for multiple language pairs. We illustrate the\neffectiveness of joint learning for multiple languages in zero-shot word\ntranslation setting. Our implementation is available at\nhttps://github.com/anoopkunchukuttan/geomm .\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 10:37:16 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 17:30:39 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 09:48:26 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Jawanpuria", "Pratik", ""], ["Balgovind", "Arjun", ""], ["Kunchukuttan", "Anoop", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1808.08784", "submitter": "Dominik Marek Loroch", "authors": "Dominik Marek Loroch, Franz-Josef Pfreundt, Norbert Wehn, Janis Keuper", "title": "Sparsity in Deep Neural Networks - An Empirical Investigation with\n  TensorQuant", "comments": "ECML18, Decentralized Machine Learning at the Edge workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is finding its way into the embedded world with applications\nsuch as autonomous driving, smart sensors and aug- mented reality. However, the\ncomputation of deep neural networks is demanding in energy, compute power and\nmemory. Various approaches have been investigated to reduce the necessary\nresources, one of which is to leverage the sparsity occurring in deep neural\nnetworks due to the high levels of redundancy in the network parameters. It has\nbeen shown that sparsity can be promoted specifically and the achieved sparsity\ncan be very high. But in many cases the methods are evaluated on rather small\ntopologies. It is not clear if the results transfer onto deeper topologies. In\nthis paper, the TensorQuant toolbox has been extended to offer a platform to\ninvestigate sparsity, especially in deeper models. Several practical relevant\ntopologies for varying classification problem sizes are investigated to show\nthe differences in sparsity for activations, weights and gradients.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 11:12:14 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Loroch", "Dominik Marek", ""], ["Pfreundt", "Franz-Josef", ""], ["Wehn", "Norbert", ""], ["Keuper", "Janis", ""]]}, {"id": "1808.08798", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues and Francisco C. Pereira", "title": "Beyond expectation: Deep joint mean and quantile regression for\n  spatio-temporal problems", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal problems are ubiquitous and of vital importance in many\nresearch fields. Despite the potential already demonstrated by deep learning\nmethods in modeling spatio-temporal data, typical approaches tend to focus\nsolely on conditional expectations of the output variables being modeled. In\nthis paper, we propose a multi-output multi-quantile deep learning approach for\njointly modeling several conditional quantiles together with the conditional\nexpectation as a way to provide a more complete \"picture\" of the predictive\ndensity in spatio-temporal problems. Using two large-scale datasets from the\ntransportation domain, we empirically demonstrate that, by approaching the\nquantile regression problem from a multi-task learning perspective, it is\npossible to solve the embarrassing quantile crossings problem, while\nsimultaneously significantly outperforming state-of-the-art quantile regression\nmethods. Moreover, we show that jointly modeling the mean and several\nconditional quantiles not only provides a rich description about the predictive\ndensity that can capture heteroscedastic properties at a neglectable\ncomputational overhead, but also leads to improved predictions of the\nconditional expectation due to the extra information and a regularization\neffect induced by the added quantiles.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 11:49:44 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Pereira", "Francisco C.", ""]]}, {"id": "1808.08811", "submitter": "Pierre Alquier", "authors": "Pierre Alquier, Paul Doukhan, Xiequan Fan", "title": "Exponential inequalities for nonstationary Markov Chains", "comments": null, "journal-ref": "Dependence Modeling, 2019, vol. 7, pp. 150-168", "doi": "10.1515/demo-2019-0007", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential inequalities are main tools in machine learning theory. To prove\nexponential inequalities for non i.i.d random variables allows to extend many\nlearning techniques to these variables. Indeed, much work has been done both on\ninequalities and learning theory for time series, in the past 15 years.\nHowever, for the non independent case, almost all the results concern\nstationary time series. This excludes many important applications: for example\nany series with a periodic behavior is non-stationary. In this paper, we extend\nthe basic tools of Dedecker and Fan (2015) to nonstationary Markov chains. As\nan application, we provide a Bernstein-type inequality, and we deduce risk\nbounds for the prediction of periodic autoregressive processes with an unknown\nperiod.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 12:15:14 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 12:05:46 GMT"}, {"version": "v3", "created": "Sat, 4 May 2019 08:32:37 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Alquier", "Pierre", ""], ["Doukhan", "Paul", ""], ["Fan", "Xiequan", ""]]}, {"id": "1808.08833", "submitter": "Merlin Sch\\\"uler", "authors": "Merlin Sch\\\"uler, Hlynur Dav\\'i{\\dh} Hlynsson, Laurenz Wiskott", "title": "Gradient-based Training of Slow Feature Analysis by Differentiable\n  Approximate Whitening", "comments": "replaced second experiment; added corresponding figures and an\n  additional explanatory figure; some major restructuring, reformulation, and\n  changes in mathematical notation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Power Slow Feature Analysis, a gradient-based method to extract\ntemporally slow features from a high-dimensional input stream that varies on a\nfaster time-scale, as a variant of Slow Feature Analysis (SFA) that allows\nend-to-end training of arbitrary differentiable architectures and thereby\nsignificantly extends the class of models that can effectively be used for slow\nfeature extraction. We provide experimental evidence that PowerSFA is able to\nextract meaningful and informative low-dimensional features in the case of (a)\nsynthetic low-dimensional data, (b) ego-visual data, and also for (c) a general\ndataset for which symmetric non-temporal similarities between points can be\ndefined.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 13:08:43 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 14:18:11 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 11:57:16 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Sch\u00fcler", "Merlin", ""], ["Hlynsson", "Hlynur Dav\u00ed\u00f0", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1808.08836", "submitter": "Ana Valeria Gonzalez", "authors": "Ana V. Gonz\\'alez-Gardu\\~no, Isabelle Augenstein, Anders S{\\o}gaard", "title": "A strong baseline for question relevancy ranking", "comments": "To appear at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The best systems at the SemEval-16 and SemEval-17 community question\nanswering shared tasks -- a task that amounts to question relevancy ranking --\ninvolve complex pipelines and manual feature engineering. Despite this, many of\nthese still fail at beating the IR baseline, i.e., the rankings provided by\nGoogle's search engine. We present a strong baseline for question relevancy\nranking by training a simple multi-task feed forward network on a bag of 14\ndistance measures for the input question pair. This baseline model, which is\nfast to train and uses only language-independent features, outperforms the best\nshared task systems on the task of retrieving relevant previously asked\nquestions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 13:19:49 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Gonz\u00e1lez-Gardu\u00f1o", "Ana V.", ""], ["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1808.08858", "submitter": "Stefanos Angelidis", "authors": "Stefanos Angelidis, Mirella Lapata", "title": "Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and\n  They Are Both Weakly Supervised", "comments": "In EMNLP 2018 (long paper). For supplementary material, see\n  http://stangelid.github.io/supplemental.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural framework for opinion summarization from online product\nreviews which is knowledge-lean and only requires light supervision (e.g., in\nthe form of product domain labels and user-provided ratings). Our method\ncombines two weakly supervised components to identify salient opinions and form\nextractive summaries from multiple reviews: an aspect extractor trained under a\nmulti-task objective, and a sentiment predictor based on multiple instance\nlearning. We introduce an opinion summarization dataset that includes a\ntraining set of product reviews from six diverse domains and human-annotated\ndevelopment and test sets with gold standard aspect annotations, salience\nlabels, and opinion summaries. Automatic evaluation shows significant\nimprovements over baselines, and a large-scale study indicates that our opinion\nsummaries are preferred by human judges according to multiple criteria.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 14:17:08 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Angelidis", "Stefanos", ""], ["Lapata", "Mirella", ""]]}, {"id": "1808.08866", "submitter": "Lijun Wu", "authors": "Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai and Tie-Yan Liu", "title": "A Study of Reinforcement Learning for Neural Machine Translation", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that reinforcement learning (RL) is an effective\napproach for improving the performance of neural machine translation (NMT)\nsystem. However, due to its instability, successfully RL training is\nchallenging, especially in real-world systems where deep models and large\ndatasets are leveraged. In this paper, taking several large-scale translation\ntasks as testbeds, we conduct a systematic study on how to train better NMT\nmodels using reinforcement learning. We provide a comprehensive comparison of\nseveral important factors (e.g., baseline reward, reward shaping) in RL\ntraining. Furthermore, to fill in the gap that it remains unclear whether RL is\nstill beneficial when monolingual data is used, we propose a new method to\nleverage RL to further boost the performance of NMT systems trained with\nsource/target monolingual data. By integrating all our findings, we obtain\ncompetitive results on WMT14 English- German, WMT17 English-Chinese, and WMT17\nChinese-English translation tasks, especially setting a state-of-the-art\nperformance on WMT17 Chinese-English translation task.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 14:43:38 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Wu", "Lijun", ""], ["Tian", "Fei", ""], ["Qin", "Tao", ""], ["Lai", "Jianhuang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1808.08871", "submitter": "Wei Chen", "authors": "Wei Chen and Mark Fuge", "title": "B\\'ezierGAN: Automatic Generation of Smooth Curves from Interpretable\n  Low-Dimensional Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world objects are designed by smooth curves, especially in the\ndomain of aerospace and ship, where aerodynamic shapes (e.g., airfoils) and\nhydrodynamic shapes (e.g., hulls) are designed. To facilitate the design\nprocess of those objects, we propose a deep learning based generative model\nthat can synthesize smooth curves. The model maps a low-dimensional latent\nrepresentation to a sequence of discrete points sampled from a rational\nB\\'ezier curve. We demonstrate the performance of our method in completing both\nsynthetic and real-world generative tasks. Results show that our method can\ngenerate diverse and realistic curves, while preserving consistent shape\nvariation in the latent space, which is favorable for latent space design\noptimization or design space exploration.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 14:57:17 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 03:36:06 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Chen", "Wei", ""], ["Fuge", "Mark", ""]]}, {"id": "1808.08914", "submitter": "Zhenguo Nie", "authors": "Zhenguo Nie, Haoliang Jiang, Levent Burak Kara", "title": "Stress Field Prediction in Cantilevered Structures Using Convolutional\n  Neural Networks", "comments": "Submitted to Journal of Computing and Information Science in\n  Engineering", "journal-ref": null, "doi": "10.1115/1.4044097", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for fast and accurate structural analysis is becoming increasingly\nmore prevalent with the advance of generative design and topology optimization\ntechnologies. As one step toward accelerating structural analysis, this work\nexplores a deep learning based approach for predicting the stress fields in 2D\nlinear elastic cantilevered structures subjected to external static loads at\nits free end using convolutional neural networks (CNN). Two different\narchitectures are implemented that take as input the structure geometry,\nexternal loads, and displacement boundary conditions, and output the predicted\nvon Mises stress field. The first is a single input channel network called\nSCSNet as the baseline architecture, and the second is the multi-channel input\nnetwork called StressNet. Accuracy analysis shows that StressNet results in\nsignificantly lower prediction errors than SCSNet on three loss functions, with\na mean relative error of 2.04% for testing. These results suggest that deep\nlearning models may offer a promising alternative to classical methods in\nstructural design and topology optimization. Code and dataset are available at\nhttps://github.com/zhenguonie/stress_net\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 16:34:51 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 22:01:29 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 00:45:33 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Nie", "Zhenguo", ""], ["Jiang", "Haoliang", ""], ["Kara", "Levent Burak", ""]]}, {"id": "1808.08931", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang and Shuiwang Ji", "title": "Smoothed Dilated Convolutions for Improved Dense Prediction", "comments": "The original version was accepted by KDD2018. Code is publicly\n  available at https://github.com/divelab/dilated", "journal-ref": "In Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining (pp. 2486-2495). 2018", "doi": "10.1145/3219819.3219944", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dilated convolutions, also known as atrous convolutions, have been widely\nexplored in deep convolutional neural networks (DCNNs) for various dense\nprediction tasks. However, dilated convolutions suffer from the gridding\nartifacts, which hampers the performance. In this work, we propose two simple\nyet effective degridding methods by studying a decomposition of dilated\nconvolutions. Unlike existing models, which explore solutions by focusing on a\nblock of cascaded dilated convolutional layers, our methods address the\ngridding artifacts by smoothing the dilated convolution itself. In addition, we\npoint out that the two degridding approaches are intrinsically related and\ndefine separable and shared (SS) operations, which generalize the proposed\nmethods. We further explore SS operations in view of operations on graphs and\npropose the SS output layer, which is able to smooth the entire DCNNs by only\nreplacing the output layer. We evaluate our degridding methods and the SS\noutput layer thoroughly, and visualize the smoothing effect through effective\nreceptive field analysis. Results show that our methods degridding yield\nconsistent improvements on the performance of dense prediction tasks, while\nadding negligible amounts of extra training parameters. And the SS output layer\nimproves the performance significantly and is very efficient in terms of number\nof training parameters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 17:13:38 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 23:04:40 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1808.08951", "submitter": "Xuchao Zhang", "authors": "Bingsheng Wang, Xuchao Zhang, Chang-Tien Lu, Feng Chen", "title": "Water Disaggregation via Shape Features based Bayesian Discriminative\n  Sparse Coding", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the issue of freshwater shortage is increasing daily, it is critical to\ntake effective measures for water conservation. According to previous studies,\ndevice level consumption could lead to significant freshwater conservation.\nExisting water disaggregation methods focus on learning the signatures for\nappliances; however, they are lack of the mechanism to accurately discriminate\nparallel appliances' consumption. In this paper, we propose a Bayesian\nDiscriminative Sparse Coding model using Laplace Prior (BDSC-LP) to extensively\nenhance the disaggregation performance. To derive discriminative basis\nfunctions, shape features are presented to describe the low-sampling-rate water\nconsumption patterns. A Gibbs sampling based inference method is designed to\nextend the discriminative capability of the disaggregation dictionaries.\nExtensive experiments were performed to validate the effectiveness of the\nproposed model using both real-world and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 16:01:11 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Wang", "Bingsheng", ""], ["Zhang", "Xuchao", ""], ["Lu", "Chang-Tien", ""], ["Chen", "Feng", ""]]}, {"id": "1808.08952", "submitter": "Maziar Raissi", "authors": "Maziar Raissi, Zhicheng Wang, Michael S. Triantafyllou, and George Em\n  Karniadakis", "title": "Deep Learning of Vortex Induced Vibrations", "comments": "arXiv admin note: text overlap with arXiv:1808.04327", "journal-ref": null, "doi": "10.1017/jfm.2018.872", "report-no": null, "categories": "physics.flu-dyn cs.CE cs.LG math.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vortex induced vibrations of bluff bodies occur when the vortex shedding\nfrequency is close to the natural frequency of the structure. Of interest is\nthe prediction of the lift and drag forces on the structure given some limited\nand scattered information on the velocity field. This is an inverse problem\nthat is not straightforward to solve using standard computational fluid\ndynamics (CFD) methods, especially since no information is provided for the\npressure. An even greater challenge is to infer the lift and drag forces given\nsome dye or smoke visualizations of the flow field. Here we employ deep neural\nnetworks that are extended to encode the incompressible Navier-Stokes equations\ncoupled with the structure's dynamic motion equation. In the first case, given\nscattered data in space-time on the velocity field and the structure's motion,\nwe use four coupled deep neural networks to infer very accurately the\nstructural parameters, the entire time-dependent pressure field (with no prior\ntraining data), and reconstruct the velocity vector field and the structure's\ndynamic motion. In the second case, given scattered data in space-time on a\nconcentration field only, we use five coupled deep neural networks to infer\nvery accurately the vector velocity field and all other quantities of interest\nas before. This new paradigm of inference in fluid mechanics for coupled\nmulti-physics problems enables velocity and pressure quantification from flow\nsnapshots in small subdomains and can be exploited for flow control\napplications and also for system identification.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 20:54:33 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Raissi", "Maziar", ""], ["Wang", "Zhicheng", ""], ["Triantafyllou", "Michael S.", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1808.08994", "submitter": "Yizhen Wang", "authors": "Yizhen Wang and Kamalika Chaudhuri", "title": "Data Poisoning Attacks against Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider data poisoning attacks, a class of adversarial attacks on machine\nlearning where an adversary has the power to alter a small fraction of the\ntraining data in order to make the trained classifier satisfy certain\nobjectives. While there has been much prior work on data poisoning, most of it\nis in the offline setting, and attacks for online learning, where training data\narrives in a streaming manner, are not well understood.\n  In this work, we initiate a systematic investigation of data poisoning\nattacks for online learning. We formalize the problem into two settings, and we\npropose a general attack strategy, formulated as an optimization problem, that\napplies to both with some modifications. We propose three solution strategies,\nand perform extensive experimental evaluation. Finally, we discuss the\nimplications of our findings for building successful defenses.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 18:59:43 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Wang", "Yizhen", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1808.09004", "submitter": "Aaron Roth", "authors": "Sampath Kannan and Aaron Roth and Juba Ziani", "title": "Downstream Effects of Affirmative Action", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a two-stage model, in which students are 1) admitted to college on\nthe basis of an entrance exam which is a noisy signal about their\nqualifications (type), and then 2) those students who were admitted to college\ncan be hired by an employer as a function of their college grades, which are an\nindependently drawn noisy signal of their type. Students are drawn from one of\ntwo populations, which might have different type distributions. We assume that\nthe employer at the end of the pipeline is rational, in the sense that it\ncomputes a posterior distribution on student type conditional on all\ninformation that it has available (college admissions, grades, and group\nmembership), and makes a decision based on posterior expectation. We then study\nwhat kinds of fairness goals can be achieved by the college by setting its\nadmissions rule and grading policy. For example, the college might have the\ngoal of guaranteeing equal opportunity across populations: that the probability\nof passing through the pipeline and being hired by the employer should be\nindependent of group membership, conditioned on type. Alternately, the college\nmight have the goal of incentivizing the employer to have a group blind hiring\nrule. We show that both goals can be achieved when the college does not report\ngrades. On the other hand, we show that under reasonable conditions, these\ngoals are impossible to achieve even in isolation when the college uses an\n(even minimally) informative grading policy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 19:15:15 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Kannan", "Sampath", ""], ["Roth", "Aaron", ""], ["Ziani", "Juba", ""]]}, {"id": "1808.09034", "submitter": "Justin Domke", "authors": "Justin Domke, Daniel Sheldon", "title": "Importance Weighting and Variational Inference", "comments": "Neural Information Processing Systems (NIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work used importance sampling ideas for better variational bounds on\nlikelihoods. We clarify the applicability of these ideas to pure probabilistic\ninference, by showing the resulting Importance Weighted Variational Inference\n(IWVI) technique is an instance of augmented variational inference, thus\nidentifying the looseness in previous work. Experiments confirm IWVI's\npracticality for probabilistic inference. As a second contribution, we\ninvestigate inference with elliptical distributions, which improves accuracy in\nlow dimensions, and convergence in high dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 21:12:47 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 20:13:22 GMT"}, {"version": "v3", "created": "Sat, 27 Oct 2018 03:05:32 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Domke", "Justin", ""], ["Sheldon", "Daniel", ""]]}, {"id": "1808.09050", "submitter": "Xin Shi", "authors": "Xin Shi, Robert Qiu, Tiebin Mi, Xing He, Yongli Zhu", "title": "Adversarial Feature Learning of Online Monitoring Data for Operational\n  Risk Assessment in Distribution Networks", "comments": "10 pages, IEEE Trans on Power Systems, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the deployment of online monitoring systems in distribution networks,\nmassive amounts of data collected through them contains rich information on the\noperating states of the networks. By leveraging the data, an unsupervised\napproach based on bidirectional generative adversarial networks (BiGANs) is\nproposed for operational risk assessment in distribution networks in this\npaper. The approach includes two stages: (1) adversarial feature learning. The\nmost representative features are extracted from the online monitoring data and\na statistical index $\\mathcal{N}_{\\phi}$ is calculated for the features, during\nwhich we make no assumptions or simplifications on the real data. (2)\noperational risk assessment. The confidence level $1-\\alpha$ for the population\nmean of the standardized $\\mathcal{N}_{\\phi}$ is combined with the operational\nrisk levels which are divided into emergency, high risk, preventive and normal,\nand the p value for each data point is calculated and compared with\n$\\frac{\\alpha}{2}$ to determine the risk levels. The proposed approach is\ncapable of discovering the latent structure of the real data and providing more\naccurate assessment result. The synthetic data is employed to illustrate the\nselection of parameters involved in the proposed approach. Case studies on the\nreal-world online monitoring data validate the effectiveness and advantages of\nthe proposed approach in risk assessment.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 22:26:00 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 04:08:47 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Shi", "Xin", ""], ["Qiu", "Robert", ""], ["Mi", "Tiebin", ""], ["He", "Xing", ""], ["Zhu", "Yongli", ""]]}, {"id": "1808.09057", "submitter": "Ritesh Noothigattu", "authors": "Ritesh Noothigattu, Nihar B. Shah, Ariel D. Procaccia", "title": "Loss Functions, Axioms, and Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to see a handful of reviewers reject a highly novel paper,\nbecause they view, say, extensive experiments as far more important than\nnovelty, whereas the community as a whole would have embraced the paper. More\ngenerally, the disparate mapping of criteria scores to final recommendations by\ndifferent reviewers is a major source of inconsistency in peer review. In this\npaper we present a framework inspired by empirical risk minimization (ERM) for\nlearning the community's aggregate mapping. The key challenge that arises is\nthe specification of a loss function for ERM. We consider the class of $L(p,q)$\nloss functions, which is a matrix-extension of the standard class of $L_p$\nlosses on vectors; here the choice of the loss function amounts to choosing the\nhyperparameters $p, q \\in [1,\\infty]$. To deal with the absence of ground truth\nin our problem, we instead draw on computational social choice to identify\ndesirable values of the hyperparameters $p$ and $q$. Specifically, we\ncharacterize $p=q=1$ as the only choice of these hyperparameters that satisfies\nthree natural axiomatic properties. Finally, we implement and apply our\napproach to reviews from IJCAI 2017.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 23:02:18 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 08:15:48 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Noothigattu", "Ritesh", ""], ["Shah", "Nihar B.", ""], ["Procaccia", "Ariel D.", ""]]}, {"id": "1808.09062", "submitter": "Huayu Li", "authors": "Huayu Li", "title": "Cognitive Consistency Routing Algorithm of Capsule-network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) are computational models inspired by the\ncentral nervous system (especially the brain) of animals and are used to\nestimate or generate unknown approximation functions relied on large amounts of\ninputs. Capsule Neural Network (Sabour S, et al.[2017]) is a novel structure of\nConvolutional Neural Networks which simulates the visual processing system of\nhuman brain. In this paper, we introduce psychological theories which called\nCognitive Consistency to optimize the routing algorithm of Capsnet to make it\nmore close to the work pattern of human brain. It has been shown in the\nexperiment that a progress had been made compared with the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 23:26:08 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 15:18:14 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 20:01:10 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Li", "Huayu", ""]]}, {"id": "1808.09105", "submitter": "Marvin Zhang", "authors": "Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J.\n  Johnson, Sergey Levine", "title": "SOLAR: Deep Structured Representations for Model-Based Reinforcement\n  Learning", "comments": "ICML 2019. Project website: https://sites.google.com/view/icml19solar", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based reinforcement learning (RL) has proven to be a data efficient\napproach for learning control tasks but is difficult to utilize in domains with\ncomplex observations such as images. In this paper, we present a method for\nlearning representations that are suitable for iterative model-based policy\nimprovement, even when the underlying dynamical system has complex dynamics and\nimage observations, in that these representations are optimized for inferring\nsimple dynamics and cost models given data from the current policy. This\nenables a model-based RL method based on the linear-quadratic regulator (LQR)\nto be used for systems with image observations. We evaluate our approach on a\nrange of robotics tasks, including manipulation with a real-world robotic arm\ndirectly from images. We find that our method produces substantially better\nfinal performance than other model-based RL methods while being significantly\nmore efficient than model-free RL.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 03:48:25 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 22:09:11 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 02:46:32 GMT"}, {"version": "v4", "created": "Sat, 22 Jun 2019 23:01:00 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Zhang", "Marvin", ""], ["Vikram", "Sharad", ""], ["Smith", "Laura", ""], ["Abbeel", "Pieter", ""], ["Johnson", "Matthew J.", ""], ["Levine", "Sergey", ""]]}, {"id": "1808.09111", "submitter": "Junxian He", "authors": "Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick", "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural\n  Projections", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of syntactic structure is typically performed using\ngenerative models with discrete latent variables and multinomial parameters. In\nmost cases, these models have not leveraged continuous word representations. In\nthis work, we propose a novel generative model that jointly learns discrete\nsyntactic structure and continuous word representations in an unsupervised\nfashion by cascading an invertible neural network with a structured generative\nprior. We show that the invertibility condition allows for efficient exact\ninference and marginal likelihood computation in our model so long as the prior\nis well-behaved. In experiments we instantiate our approach with both Markov\nand tree-structured priors, evaluating on two tasks: part-of-speech (POS)\ninduction, and unsupervised dependency parsing without gold POS annotation. On\nthe Penn Treebank, our Markov-structured model surpasses state-of-the-art\nresults on POS induction. Similarly, we find that our tree-structured model\nachieves state-of-the-art performance on unsupervised dependency parsing for\nthe difficult training condition where neither gold POS annotation nor\npunctuation-based constraints are available.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 04:33:25 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["He", "Junxian", ""], ["Neubig", "Graham", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1808.09123", "submitter": "Sarah Tan", "authors": "Sarah Tan and Julius Adebayo and Kori Inkpen and Ece Kamar", "title": "Investigating Human + Machine Complementarity for Recidivism Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When might human input help (or not) when assessing risk in fairness domains?\nDressel and Farid (2018) asked Mechanical Turk workers to evaluate a subset of\ndefendants in the ProPublica COMPAS data for risk of recidivism, and concluded\nthat COMPAS predictions were no more accurate or fair than predictions made by\nhumans. We delve deeper into this claim to explore differences in human and\nalgorithmic decision making. We construct a Human Risk Score based on the\npredictions made by multiple Turk workers, characterize the features that\ndetermine agreement and disagreement between COMPAS and Human Scores, and\nconstruct hybrid Human+Machine models to predict recidivism. Our key finding is\nthat on this data set, Human and COMPAS decision making differed, but not in\nways that could be leveraged to significantly improve ground-truth prediction.\nWe present the results of our analyses and suggestions for data collection best\npractices to leverage complementary strengths of human and machines in the\nfairness domain.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 05:28:35 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 07:11:32 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Tan", "Sarah", ""], ["Adebayo", "Julius", ""], ["Inkpen", "Kori", ""], ["Kamar", "Ece", ""]]}, {"id": "1808.09127", "submitter": "Touqir Sajed", "authors": "Touqir Sajed, Wesley Chung, Martha White", "title": "High-confidence error estimates for learned value functions", "comments": "Presented at (UAI) Uncertainty in Artificial Intelligence 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the value function for a fixed policy is a fundamental problem in\nreinforcement learning. Policy evaluation algorithms---to estimate value\nfunctions---continue to be developed, to improve convergence rates, improve\nstability and handle variability, particularly for off-policy learning. To\nunderstand the properties of these algorithms, the experimenter needs\nhigh-confidence estimates of the accuracy of the learned value functions. For\nenvironments with small, finite state-spaces, like chains, the true value\nfunction can be easily computed, to compute accuracy. For large, or continuous\nstate-spaces, however, this is no longer feasible. In this paper, we address\nthe largely open problem of how to obtain these high-confidence estimates, for\ngeneral state-spaces. We provide a high-confidence bound on an empirical\nestimate of the value error to the true value error. We use this bound to\ndesign an offline sampling algorithm, which stores the required quantities to\nrepeatedly compute value error estimates for any learned value function. We\nprovide experiments investigating the number of samples required by this\noffline algorithm in simple benchmark reinforcement learning domains, and\nhighlight that there are still many open questions to be solved for this\nimportant problem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 05:39:48 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Sajed", "Touqir", ""], ["Chung", "Wesley", ""], ["White", "Martha", ""]]}, {"id": "1808.09144", "submitter": "Guodong Xu", "authors": "Guodong Xu, Yu Xia, Hui Ji", "title": "Weighted total variation based convex clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a fundamental problem with a wide range of applications.\nStandard methods, eg the $k$-means method, usually require solving a non-convex\noptimization problem. Recently, total variation based convex relaxation to the\n$k$-means model has emerged as an attractive alternative for data clustering.\nHowever, the existing results on its exact clustering property, ie, the\ncondition imposed on data so that the method can provably give correct\nidentification of all cluster memberships, is only applicable to very specific\ndata and is also much more restrictive than that of some other methods. This\npaper aims at the revisit of total variation based convex clustering, by\nproposing a weighted sum-of-$\\ell_1$-norm relating convex model. Its exact\nclustering property established in this paper, in both deterministic and\nprobabilistic context, is applicable to general data and is much sharper than\nthe existing results. These results provided good insights to advance the\nresearch on convex clustering. Moreover, the experiments also demonstrated that\nthe proposed convex model has better empirical performance when be compared to\nstandard clustering methods, and thus it can see its potential in practice.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 07:19:43 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Xu", "Guodong", ""], ["Xia", "Yu", ""], ["Ji", "Hui", ""]]}, {"id": "1808.09222", "submitter": "Babatunde Ayeni", "authors": "Babatunde M. Ayeni", "title": "Making \\emph{ordinary least squares} linear classfiers more robust", "comments": "9 pages with 6 figures. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of statistics and machine learning, the sums-of-squares,\ncommonly referred to as \\emph{ordinary least squares}, can be used as a\nconvenient choice of cost function because of its many nice analytical\nproperties, though not always the best choice. However, it has been long known\nthat \\emph{ordinary least squares} is not robust to outliers. Several attempts\nto resolve this problem led to the creation of alternative methods that, either\ndid not fully resolved the \\emph{outlier problem} or were computationally\ndifficult. In this paper, we provide a very simple solution that can make\n\\emph{ordinary least squares} less sensitive to outliers in data\nclassification, by \\emph{scaling the augmented input vector by its length}. We\nshow some mathematical expositions of the \\emph{outlier problem} using some\napproximations and geometrical techniques. We present numerical results to\nsupport the efficacy of our method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 11:09:02 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Ayeni", "Babatunde M.", ""]]}, {"id": "1808.09270", "submitter": "Benjamin Horne", "authors": "Benjamin D. Horne, William Dron, and Sibel Adali", "title": "Models for Predicting Community-Specific Interest in News Articles", "comments": "Published at IEEE MILCOM 2018 in Los Angeles, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we ask two questions: 1. Can we predict the type of community\ninterested in a news article using only features from the article content? and\n2. How well do these models generalize over time? To answer these questions, we\ncompute well-studied content-based features on over 60K news articles from 4\ncommunities on reddit.com. We train and test models over three different time\nperiods between 2015 and 2017 to demonstrate which features degrade in\nperformance the most due to concept drift. Our models can classify news\narticles into communities with high accuracy, ranging from 0.81 ROC AUC to 1.0\nROC AUC. However, while we can predict the community-specific popularity of\nnews articles with high accuracy, practitioners should approach these models\ncarefully. Predictions are both community-pair dependent and feature group\ndependent. Moreover, these feature groups generalize over time differently,\nwith some only degrading slightly over time, but others degrading greatly.\nTherefore, we recommend that community-interest predictions are done in a\nhierarchical structure, where multiple binary classifiers can be used to\nseparate community pairs, rather than a traditional multi-class model. Second,\nthese models should be retrained over time based on accuracy goals and the\navailability of training data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 17:42:14 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Horne", "Benjamin D.", ""], ["Dron", "William", ""], ["Adali", "Sibel", ""]]}, {"id": "1808.09271", "submitter": "Lex Razoux Schultz", "authors": "Lex Razoux Schultz, Marco Loog, Peyman Mohajerin Esfahani", "title": "Distance Based Source Domain Selection for Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated sentiment classification (SC) on short text fragments has received\nincreasing attention in recent years. Performing SC on unseen domains with few\nor no labeled samples can significantly affect the classification performance\ndue to different expression of sentiment in source and target domain. In this\nstudy, we aim to mitigate this undesired impact by proposing a methodology\nbased on a predictive measure, which allows us to select an optimal source\ndomain from a set of candidates. The proposed measure is a linear combination\nof well-known distance functions between probability distributions supported on\nthe source and target domains (e.g. Earth Mover's distance and Kullback-Leibler\ndivergence). The performance of the proposed methodology is validated through\nan SC case study in which our numerical experiments suggest a significant\nimprovement in the cross domain classification error in comparison with a\nrandom selected source domain for both a naive and adaptive learning setting.\nIn the case of more heterogeneous datasets, the predictability feature of the\nproposed model can be utilized to further select a subset of candidate domains,\nwhere the corresponding classifier outperforms the one trained on all available\nsource domains. This observation reinforces a hypothesis that our proposed\nmodel may also be deployed as a means to filter out redundant information\nduring a training phase of SC.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 13:14:33 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Schultz", "Lex Razoux", ""], ["Loog", "Marco", ""], ["Esfahani", "Peyman Mohajerin", ""]]}, {"id": "1808.09334", "submitter": "Ryan Cotterell Ryan D Cotterell", "authors": "Sebastian Ruder, Ryan Cotterell, Yova Kementchedjhieva, Anders\n  S{\\o}gaard", "title": "A Discriminative Latent-Variable Model for Bilingual Lexicon Induction", "comments": "Proceedings of the 2018 Conference on Empirical Methods in Natural\n  Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel discriminative latent-variable model for the task of\nbilingual lexicon induction. Our model combines the bipartite matching\ndictionary prior of Haghighi et al. (2008) with a state-of-the-art\nembedding-based approach. To train the model, we derive an efficient Viterbi EM\nalgorithm. We provide empirical improvements on six language pairs under two\nmetrics and show that the prior theoretically and empirically helps to mitigate\nthe hubness problem. We also demonstrate how previous work may be viewed as a\nsimilarly fashioned latent-variable model, albeit with a different prior.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 14:47:33 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 13:43:19 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Ruder", "Sebastian", ""], ["Cotterell", "Ryan", ""], ["Kementchedjhieva", "Yova", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1808.09347", "submitter": "Chen Chao", "authors": "Chao Chen and Zhihong Chen and Boyuan Jiang and Xinyu Jin", "title": "Joint Domain Alignment and Discriminative Feature Learning for\n  Unsupervised Deep Domain Adaptation", "comments": "This paper has been accepted by AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, considerable effort has been devoted to deep domain adaptation in\ncomputer vision and machine learning communities. However, most of existing\nwork only concentrates on learning shared feature representation by minimizing\nthe distribution discrepancy across different domains. Due to the fact that all\nthe domain alignment approaches can only reduce, but not remove the domain\nshift. Target domain samples distributed near the edge of the clusters, or far\nfrom their corresponding class centers are easily to be misclassified by the\nhyperplane learned from the source domain. To alleviate this issue, we propose\nto joint domain alignment and discriminative feature learning, which could\nbenefit both domain alignment and final classification. Specifically, an\ninstance-based discriminative feature learning method and a center-based\ndiscriminative feature learning method are proposed, both of which guarantee\nthe domain invariant features with better intra-class compactness and\ninter-class separability. Extensive experiments show that learning the\ndiscriminative features in the shared feature space can significantly boost the\nperformance of deep domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 15:04:32 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 07:19:39 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Chen", "Chao", ""], ["Chen", "Zhihong", ""], ["Jiang", "Boyuan", ""], ["Jin", "Xinyu", ""]]}, {"id": "1808.09371", "submitter": "Farhan Khawar", "authors": "Farhan Khawar, Nevin L. Zhang", "title": "Matrix Factorization Equals Efficient Co-occurrence Representation", "comments": "RecSys 2018 LBRS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is a simple and effective solution to the recommendation\nproblem. It has been extensively employed in the industry and has attracted\nmuch attention from the academia. However, it is unclear what the\nlow-dimensional matrices represent. We show that matrix factorization can\nactually be seen as simultaneously calculating the eigenvectors of the\nuser-user and item-item sample co-occurrence matrices. We then use insights\nfrom random matrix theory (RMT) to show that picking the top eigenvectors\ncorresponds to removing sampling noise from user/item co-occurrence matrices.\nTherefore, the low-dimension matrices represent a reduced noise user and item\nco-occurrence space. We also analyze the structure of the top eigenvector and\nshow that it corresponds to global effects and removing it results in less\npopular items being recommended. This increases the diversity of the items\nrecommended without affecting the accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 15:40:47 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Khawar", "Farhan", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1808.09442", "submitter": "Yun-Nung Chen", "authors": "Shang-Yu Su and Xiujun Li and Jianfeng Gao and Jingjing Liu and\n  Yun-Nung Chen", "title": "Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning", "comments": "11 pages, 10 figures, EMNLP 2018 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving\nthe effectiveness and robustness of Deep Dyna-Q (DDQ), a recently proposed\nframework that extends the Dyna-Q algorithm to integrate planning for\ntask-completion dialogue policy learning. To obviate DDQ's high dependency on\nthe quality of simulated experiences, we incorporate an RNN-based discriminator\nin D3Q to differentiate simulated experience from real user experience in order\nto control the quality of training data. Experiments show that D3Q\nsignificantly outperforms DDQ by controlling the quality of simulated\nexperience used for planning. The effectiveness and robustness of D3Q is\nfurther demonstrated in a domain extension setting, where the agent's\ncapability of adapting to a changing environment is tested.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 17:59:08 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 17:50:22 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Su", "Shang-Yu", ""], ["Li", "Xiujun", ""], ["Gao", "Jianfeng", ""], ["Liu", "Jingjing", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1808.09446", "submitter": "Bin Liu", "authors": "Bin Liu, Yaochu Jin", "title": "A Particle Filter based Multi-Objective Optimization Algorithm: PFOPS", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a recently developed paradigm for\npopulation-based optimization, termed particle filter optimization (PFO). This\nparadigm is attractive in terms of coherence in theory and easiness in\nmathematical analysis and interpretation. Current PFO algorithms only work for\nsingle-objective optimization cases, while many real-life problems involve\nmultiple objectives to be optimized simultaneously. To this end, we make an\neffort to extend the scope of application of the PFO paradigm to\nmulti-objective optimization (MOO) cases. An idea called path sampling is\nadopted within the PFO scheme to balance the different objectives to be\noptimized. The resulting algorithm is thus termed PFO with Path Sampling\n(PFOPS). The validity of the presented algorithm is assessed based on three\nbenchmark MOO experiments, in which the shapes of the Pareto fronts are convex,\nconcave and discontinuous, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 08:30:12 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 08:33:54 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 03:37:19 GMT"}, {"version": "v4", "created": "Fri, 23 Nov 2018 03:08:30 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Liu", "Bin", ""], ["Jin", "Yaochu", ""]]}, {"id": "1808.09489", "submitter": "Jiangning Chen", "authors": "Jiangning Chen", "title": "Convergence Rate of Krasulina Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is one of the most commonly used\nstatistical procedures with a wide range of applications. Consider the points\n$X_1, X_2,..., X_n$ are vectors drawn i.i.d. from a distribution with mean zero\nand covariance $\\Sigma$, where $\\Sigma$ is unknown. Let $A_n = X_nX_n^T$, then\n$E[A_n] = \\Sigma$. This paper consider the problem of finding the least\neigenvalue and eigenvector of matrix $\\Sigma$. A classical such estimator are\ndue to Krasulina\\cite{krasulina_method_1969}. We are going to state the\nconvergence proof of Krasulina for the least eigenvalue and corresponding\neigenvector, and then find their convergence rate.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 18:47:20 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 22:12:53 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 18:09:15 GMT"}, {"version": "v4", "created": "Tue, 23 Jul 2019 04:05:08 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chen", "Jiangning", ""]]}, {"id": "1808.09501", "submitter": "Jaewoo Lee", "authors": "Jaewoo Lee and Daniel Kifer", "title": "Concentrated Differentially Private Gradient Descent with Adaptive\n  per-Iteration Privacy Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative algorithms, like gradient descent, are common tools for solving a\nvariety of problems, such as model fitting. For this reason, there is interest\nin creating differentially private versions of them. However, their conversion\nto differentially private algorithms is often naive. For instance, a fixed\nnumber of iterations are chosen, the privacy budget is split evenly among them,\nand at each iteration, parameters are updated with a noisy gradient. In this\npaper, we show that gradient-based algorithms can be improved by a more careful\nallocation of privacy budget per iteration. Intuitively, at the beginning of\nthe optimization, gradients are expected to be large, so that they do not need\nto be measured as accurately. However, as the parameters approach their optimal\nvalues, the gradients decrease and hence need to be measured more accurately.\nWe add a basic line-search capability that helps the algorithm decide when more\naccurate gradient measurements are necessary. Our gradient descent algorithm\nworks with the recently introduced zCDP version of differential privacy. It\noutperforms prior algorithms for model fitting and is competitive with the\nstate-of-the-art for $(\\epsilon,\\delta)$-differential privacy, a strictly\nweaker definition than zCDP.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 19:15:12 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Lee", "Jaewoo", ""], ["Kifer", "Daniel", ""]]}, {"id": "1808.09517", "submitter": "Basma Abdulaimma", "authors": "Basma Abdulaimma, Paul Fergus, Carl Chalmers", "title": "Extracting Epistatic Interactions in Type 2 Diabetes Genome-Wide Data\n  Using Stacked Autoencoder", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  2 Diabetes is a leading worldwide public health concern, and its increasing\nprevalence has significant health and economic importance in all nations. The\ncondition is a multifactorial disorder with a complex aetiology. The genetic\ndeterminants remain largely elusive, with only a handful of identified\ncandidate genes. Genome wide association studies (GWAS) promised to\nsignificantly enhance our understanding of genetic based determinants of common\ncomplex diseases. To date, 83 single nucleotide polymorphisms (SNPs) for type 2\ndiabetes have been identified using GWAS. Standard statistical tests for single\nand multi-locus analysis such as logistic regression, have demonstrated little\neffect in understanding the genetic architecture of complex human diseases.\nLogistic regression is modelled to capture linear interactions but neglects the\nnon-linear epistatic interactions present within genetic data. There is an\nurgent need to detect epistatic interactions in complex diseases as this may\nexplain the remaining missing heritability in such diseases. In this paper, we\npresent a novel framework based on deep learning algorithms that deal with\nnon-linear epistatic interactions that exist in genome wide association data.\nLogistic association analysis under an additive genetic model, adjusted for\ngenomic control inflation factor, is conducted to remove statistically\nimprobable SNPs to minimize computational overheads.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 20:01:11 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Abdulaimma", "Basma", ""], ["Fergus", "Paul", ""], ["Chalmers", "Carl", ""]]}, {"id": "1808.09540", "submitter": "Jeff Calder", "authors": "Chris Finlay, Jeff Calder, Bilal Abbasi, and Adam Oberman", "title": "Lipschitz regularized Deep Neural Networks generalize and are\n  adversarially robust", "comments": "18 pages, 4 figures (merged with arXiv:1810.00953)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study input gradient regularization of deep neural networks,\nand demonstrate that such regularization leads to generalization proofs and\nimproved adversarial robustness. The proof of generalization does not overcome\nthe curse of dimensionality, but it is independent of the number of layers in\nthe networks. The adversarial robustness regularization combines adversarial\ntraining, which we show to be equivalent to Total Variation regularization,\nwith Lipschitz regularization. We demonstrate empirically that the regularized\nmodels are more robust, and that gradient norms of images can be used for\nattack detection.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 20:53:01 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 13:49:52 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 14:08:55 GMT"}, {"version": "v4", "created": "Thu, 12 Sep 2019 02:40:39 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Finlay", "Chris", ""], ["Calder", "Jeff", ""], ["Abbasi", "Bilal", ""], ["Oberman", "Adam", ""]]}, {"id": "1808.09551", "submitter": "Fr\\'ederic Godin", "authors": "Fr\\'ederic Godin, Kris Demuynck, Joni Dambre, Wesley De Neve and\n  Thomas Demeester", "title": "Explaining Character-Aware Neural Networks for Word-Level Prediction: Do\n  They Discover Linguistic Rules?", "comments": "Accepted at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Character-level features are currently used in different neural network-based\nnatural language processing algorithms. However, little is known about the\ncharacter-level patterns those models learn. Moreover, models are often\ncompared only quantitatively while a qualitative analysis is missing. In this\npaper, we investigate which character-level patterns neural networks learn and\nif those patterns coincide with manually-defined word segmentations and\nannotations. To that end, we extend the contextual decomposition technique\n(Murdoch et al. 2018) to convolutional neural networks which allows us to\ncompare convolutional neural networks and bidirectional long short-term memory\nnetworks. We evaluate and compare these models for the task of morphological\ntagging on three morphologically different languages and show that these models\nimplicitly discover understandable linguistic rules. Our implementation can be\nfound at https://github.com/FredericGodin/ContextualDecomposition-NLP .\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 21:44:26 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Godin", "Fr\u00e9deric", ""], ["Demuynck", "Kris", ""], ["Dambre", "Joni", ""], ["De Neve", "Wesley", ""], ["Demeester", "Thomas", ""]]}, {"id": "1808.09574", "submitter": "Maryam Jaberi", "authors": "Maryam Jaberi, Marianna Pensky, Hassan Foroosh", "title": "Probabilistic Sparse Subspace Clustering Using Delayed Association", "comments": null, "journal-ref": "ICPR 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering and clustering subspaces in high-dimensional data is a\nfundamental problem of machine learning with a wide range of applications in\ndata mining, computer vision, and pattern recognition. Earlier methods divided\nthe problem into two separate stages of finding the similarity matrix and\nfinding clusters. Similar to some recent works, we integrate these two steps\nusing a joint optimization approach. We make the following contributions: (i)\nwe estimate the reliability of the cluster assignment for each point before\nassigning a point to a subspace. We group the data points into two groups of\n\"certain\" and \"uncertain\", with the assignment of latter group delayed until\ntheir subspace association certainty improves. (ii) We demonstrate that delayed\nassociation is better suited for clustering subspaces that have ambiguities,\ni.e. when subspaces intersect or data are contaminated with outliers/noise.\n(iii) We demonstrate experimentally that such delayed probabilistic association\nleads to a more accurate self-representation and final clusters. The proposed\nmethod has higher accuracy both for points that exclusively lie in one\nsubspace, and those that are on the intersection of subspaces. (iv) We show\nthat delayed association leads to huge reduction of computational cost, since\nit allows for incremental spectral clustering.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 23:03:55 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Jaberi", "Maryam", ""], ["Pensky", "Marianna", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1808.09607", "submitter": "Dan-Bo Zhang Dr.", "authors": "Dan-Bo Zhang, Shi-Liang Zhu, and Z. D. Wang", "title": "Nonlinear regression based on a hybrid quantum computer", "comments": "6 pages, comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating nonlinearity into quantum machine learning is essential for\nlearning a complicated input-output mapping. We here propose quantum algorithms\nfor nonlinear regression, where nonlinearity is introduced with feature maps\nwhen loading classical data into quantum states. Our implementation is based on\na hybrid quantum computer, exploiting both discrete and continuous variables,\nfor their capacity to encode novel features and efficiency of processing\ninformation. We propose encoding schemes that can realize well-known polynomial\nand Gaussian kernel ridge regressions, with exponentially speed-up regarding to\nthe number of samples.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 02:24:50 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Zhang", "Dan-Bo", ""], ["Zhu", "Shi-Liang", ""], ["Wang", "Z. D.", ""]]}, {"id": "1808.09617", "submitter": "Chang Wei Tan", "authors": "Chang Wei Tan, Francois Petitjean, Geoffrey I. Webb", "title": "Elastic bands across the path: A new framework and methods to lower\n  bound DTW", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been renewed recent interest in developing effective lower bounds\nfor Dynamic Time Warping (DTW) distance between time series. These have many\napplications in time series indexing, clustering, forecasting, regression and\nclassification. One of the key time series classification algorithms, the\nnearest neighbor algorithm with DTW distance (NN-DTW) is very expensive to\ncompute, due to the quadratic complexity of DTW. Lower bound search can speed\nup NN-DTW substantially. An effective and tight lower bound quickly prunes off\nunpromising nearest neighbor candidates from the search space and minimises the\nnumber of the costly DTW computations. The speed up provided by lower bound\nsearch becomes increasingly critical as training set size increases. Different\nlower bounds provide different trade-offs between computation time and\ntightness. Most existing lower bounds interact with DTW warping window sizes.\nThey are very tight and effective at smaller warping window sizes, but become\nlooser as the warping window increases, thus reducing the pruning effectiveness\nfor NN-DTW. In this work, we present a new class of lower bounds that are\ntighter than the popular Keogh lower bound, while requiring similar computation\ntime. Our new lower bounds take advantage of the DTW boundary condition,\nmonotonicity and continuity constraints to create a tighter lower bound. Of\nparticular significance, they remain relatively tight even for large windows. A\nsingle parameter to these new lower bounds controls the speed-tightness\ntrade-off. We demonstrate that these new lower bounds provide an exceptional\nbalance between computation time and tightness for the NN-DTW time series\nclassification task, resulting in greatly improved efficiency for NN-DTW lower\nbound search.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 03:22:47 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 00:02:42 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 06:48:57 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Tan", "Chang Wei", ""], ["Petitjean", "Francois", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1808.09633", "submitter": "Dinghan Shen", "authors": "Dinghan Shen, Xinyuan Zhang, Ricardo Henao, Lawrence Carin", "title": "Improved Semantic-Aware Network Embedding with Fine-Grained Word\n  Alignment", "comments": "To appear at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embeddings, which learn low-dimensional representations for each\nvertex in a large-scale network, have received considerable attention in recent\nyears. For a wide range of applications, vertices in a network are typically\naccompanied by rich textual information such as user profiles, paper abstracts,\netc. We propose to incorporate semantic features into network embeddings by\nmatching important words between text sequences for all pairs of vertices. We\nintroduce a word-by-word alignment framework that measures the compatibility of\nembeddings between word pairs, and then adaptively accumulates these alignment\nfeatures with a simple yet effective aggregation function. In experiments, we\nevaluate the proposed framework on three real-world benchmarks for downstream\ntasks, including link prediction and multi-label vertex classification. Results\ndemonstrate that our model outperforms state-of-the-art network embedding\nmethods by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 04:23:02 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Shen", "Dinghan", ""], ["Zhang", "Xinyuan", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1808.09634", "submitter": "Wen-Chin Huang", "authors": "Wen-Chin Huang, Hsin-Te Hwang, Yu-Huai Peng, Yu Tsao, Hsin-Min Wang", "title": "Voice Conversion Based on Cross-Domain Features Using Variational Auto\n  Encoders", "comments": "Accepted to ISCSLP 2018", "journal-ref": null, "doi": "10.1109/ISCSLP.2018.8706604", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective approach to non-parallel voice conversion (VC) is to utilize\ndeep neural networks (DNNs), specifically variational auto encoders (VAEs), to\nmodel the latent structure of speech in an unsupervised manner. A previous\nstudy has confirmed the ef- fectiveness of VAE using the STRAIGHT spectra for\nVC. How- ever, VAE using other types of spectral features such as mel- cepstral\ncoefficients (MCCs), which are related to human per- ception and have been\nwidely used in VC, have not been prop- erly investigated. Instead of using one\nspecific type of spectral feature, it is expected that VAE may benefit from\nusing multi- ple types of spectral features simultaneously, thereby improving\nthe capability of VAE for VC. To this end, we propose a novel VAE framework\n(called cross-domain VAE, CDVAE) for VC. Specifically, the proposed framework\nutilizes both STRAIGHT spectra and MCCs by explicitly regularizing multiple\nobjectives in order to constrain the behavior of the learned encoder and de-\ncoder. Experimental results demonstrate that the proposed CD- VAE framework\noutperforms the conventional VAE framework in terms of subjective tests.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 04:32:42 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Huang", "Wen-Chin", ""], ["Hwang", "Hsin-Te", ""], ["Peng", "Yu-Huai", ""], ["Tsao", "Yu", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1808.09638", "submitter": "Hye-jin Shim", "authors": "Hye-Jin Shim, Jee-weon Jung, Hee-Soo Heo, Sunghyun Yoon, Ha-Jin Yu", "title": "Replay spoofing detection system for automatic speaker verification\n  using multi-task learning of noise classes", "comments": "5 pages, accepted by Technologies and Applications of Artificial\n  Intelligence(TAAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a replay attack spoofing detection system for\nautomatic speaker verification using multitask learning of noise classes. We\ndefine the noise that is caused by the replay attack as replay noise. We\nexplore the effectiveness of training a deep neural network simultaneously for\nreplay attack spoofing detection and replay noise classification. The\nmulti-task learning includes classifying the noise of playback devices,\nrecording environments, and recording devices as well as the spoofing\ndetection. Each of the three types of the noise classes also includes a genuine\nclass. The experiment results on the ASVspoof2017 datasets demonstrate that the\nperformance of our proposed system is improved by 30% relatively on the\nevaluation set.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 05:01:00 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 16:57:25 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 11:38:57 GMT"}, {"version": "v4", "created": "Thu, 25 Oct 2018 11:59:36 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Shim", "Hye-Jin", ""], ["Jung", "Jee-weon", ""], ["Heo", "Hee-Soo", ""], ["Yoon", "Sunghyun", ""], ["Yu", "Ha-Jin", ""]]}, {"id": "1808.09642", "submitter": "Junchi Li", "authors": "Chris Junchi Li, Zhaoran Wang, Han Liu", "title": "Online ICA: Understanding Global Dynamics of Nonconvex Optimization via\n  Diffusion Processes", "comments": "Appeared in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving statistical learning problems often involves nonconvex optimization.\nDespite the empirical success of nonconvex statistical optimization methods,\ntheir global dynamics, especially convergence to the desirable local minima,\nremain less well understood in theory. In this paper, we propose a new analytic\nparadigm based on diffusion processes to characterize the global dynamics of\nnonconvex statistical optimization. As a concrete example, we study stochastic\ngradient descent (SGD) for the tensor decomposition formulation of independent\ncomponent analysis. In particular, we cast different phases of SGD into\ndiffusion processes, i.e., solutions to stochastic differential equations.\nInitialized from an unstable equilibrium, the global dynamics of SGD transit\nover three consecutive phases: (i) an unstable Ornstein-Uhlenbeck process\nslowly departing from the initialization, (ii) the solution to an ordinary\ndifferential equation, which quickly evolves towards the desirable local\nminimum, and (iii) a stable Ornstein-Uhlenbeck process oscillating around the\ndesirable local minimum. Our proof techniques are based upon Stroock and\nVaradhan's weak convergence of Markov chains to diffusion processes, which are\nof independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 05:30:21 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Li", "Chris Junchi", ""], ["Wang", "Zhaoran", ""], ["Liu", "Han", ""]]}, {"id": "1808.09645", "submitter": "Junchi Li", "authors": "Chris Junchi Li, Mengdi Wang, Han Liu, Tong Zhang", "title": "Diffusion Approximations for Online Principal Component Estimation and\n  Global Convergence", "comments": "Appeared in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to adopt the diffusion approximation tools to study\nthe dynamics of Oja's iteration which is an online stochastic gradient descent\nmethod for the principal component analysis. Oja's iteration maintains a\nrunning estimate of the true principal component from streaming data and enjoys\nless temporal and spatial complexities. We show that the Oja's iteration for\nthe top eigenvector generates a continuous-state discrete-time Markov chain\nover the unit sphere. We characterize the Oja's iteration in three phases using\ndiffusion approximation and weak convergence tools. Our three-phase analysis\nfurther provides a finite-sample error bound for the running estimate, which\nmatches the minimax information lower bound for principal component analysis\nunder the additional assumption of bounded samples.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 05:36:07 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Li", "Chris Junchi", ""], ["Wang", "Mengdi", ""], ["Liu", "Han", ""], ["Zhang", "Tong", ""]]}, {"id": "1808.09658", "submitter": "Yang Gao", "authors": "Yang Gao, Christian M. Meyer, Iryna Gurevych", "title": "APRIL: Interactively Learning to Summarise by Combining Active\n  Preference Learning and Reinforcement Learning", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to perform automatic document summarisation without using\nreference summaries. Instead, our method interactively learns from users'\npreferences. The merit of preference-based interactive summarisation is that\npreferences are easier for users to provide than reference summaries. Existing\npreference-based interactive learning methods suffer from high sample\ncomplexity, i.e. they need to interact with the oracle for many rounds in order\nto converge. In this work, we propose a new objective function, which enables\nus to leverage active learning, preference learning and reinforcement learning\ntechniques in order to reduce the sample complexity. Both simulation and\nreal-user experiments suggest that our method significantly advances the state\nof the art. Our source code is freely available at\nhttps://github.com/UKPLab/emnlp2018-april.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 06:49:49 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Gao", "Yang", ""], ["Meyer", "Christian M.", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1808.09663", "submitter": "Sidak Pal Singh", "authors": "Sidak Pal Singh, Andreas Hug, Aymeric Dieuleveut, Martin Jaggi", "title": "Context Mover's Distance & Barycenters: Optimal Transport of Contexts\n  for Building Representations", "comments": "AISTATS 2020. Also, accepted previously at ICLR 2019 DeepGenStruct\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a framework for building unsupervised representations of entities\nand their compositions, where each entity is viewed as a probability\ndistribution rather than a vector embedding. In particular, this distribution\nis supported over the contexts which co-occur with the entity and are embedded\nin a suitable low-dimensional space. This enables us to consider representation\nlearning from the perspective of Optimal Transport and take advantage of its\ntools such as Wasserstein distance and barycenters. We elaborate how the method\ncan be applied for obtaining unsupervised representations of text and\nillustrate the performance (quantitatively as well as qualitatively) on tasks\nsuch as measuring sentence similarity, word entailment and similarity, where we\nempirically observe significant gains (e.g., 4.1% relative improvement over\nSent2vec, GenSen).\n  The key benefits of the proposed approach include: (a) capturing uncertainty\nand polysemy via modeling the entities as distributions, (b) utilizing the\nunderlying geometry of the particular task (with the ground cost), (c)\nsimultaneously providing interpretability with the notion of optimal transport\nbetween contexts and (d) easy applicability on top of existing point embedding\nmethods. The code, as well as prebuilt histograms, are available under\nhttps://github.com/context-mover/.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 07:18:29 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 10:02:56 GMT"}, {"version": "v3", "created": "Thu, 20 Dec 2018 21:18:56 GMT"}, {"version": "v4", "created": "Sat, 23 Mar 2019 01:18:50 GMT"}, {"version": "v5", "created": "Tue, 13 Aug 2019 15:45:32 GMT"}, {"version": "v6", "created": "Sat, 29 Feb 2020 18:04:45 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Singh", "Sidak Pal", ""], ["Hug", "Andreas", ""], ["Dieuleveut", "Aymeric", ""], ["Jaggi", "Martin", ""]]}, {"id": "1808.09670", "submitter": "Maxime Sangnier", "authors": "Erwan Fouillen, Claire Boyer, Maxime Sangnier", "title": "Proximal boosting and variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting is a prediction method that iteratively combines weak\nlearners to produce a complex and accurate model. From an optimization point of\nview, the learning procedure of gradient boosting mimics a gradient descent on\na functional variable. This paper proposes to build upon the proximal point\nalgorithm, when the empirical risk to minimize is not differentiable, in order\nto introduce a novel boosting approach, called proximal boosting. Besides being\nmotivated by non-differentiable optimization, the proposed algorithm benefits\nfrom algorithmic improvements such as controlling the approximation error and\nNesterov's acceleration, in the same way as gradient boosting [Grubb and\nBagnell, 2011, Biau et al., 2018]. This leads to two variants, respectively\ncalled residual proximal boosting and accelerated proximal boosting.\nTheoretical convergence is proved for the first two procedures under different\nhypotheses on the empirical risk and advantages of leveraging proximal methods\nfor boosting are illustrated by numerical experiments on simulated and\nreal-world data. In particular, we exhibit a favorable comparison over gradient\nboosting regarding convergence rate and prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 07:58:28 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 09:42:24 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 21:04:21 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Fouillen", "Erwan", ""], ["Boyer", "Claire", ""], ["Sangnier", "Maxime", ""]]}, {"id": "1808.09744", "submitter": "Madhumita Sushil", "authors": "Madhumita Sushil and Simon \\v{S}uster and Walter Daelemans", "title": "Rule induction for global explanation of trained models", "comments": "Accepted at the Workshop on 'Analyzing and interpreting neural\n  networks for NLP' (BlackboxNLP), EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behavior of a trained network and finding explanations for\nits outputs is important for improving the network's performance and\ngeneralization ability, and for ensuring trust in automated systems. Several\napproaches have previously been proposed to identify and visualize the most\nimportant features by analyzing a trained network. However, the relations\nbetween different features and classes are lost in most cases. We propose a\ntechnique to induce sets of if-then-else rules that capture these relations to\nglobally explain the predictions of a network. We first calculate the\nimportance of the features in the trained network. We then weigh the original\ninputs with these feature importance scores, simplify the transformed input\nspace, and finally fit a rule induction model to explain the model predictions.\nWe find that the output rule-sets can explain the predictions of a neural\nnetwork trained for 4-class text classification from the 20 newsgroups dataset\nto a macro-averaged F-score of 0.80. We make the code available at\nhttps://github.com/clips/interpret_with_rules.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 12:02:11 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Sushil", "Madhumita", ""], ["\u0160uster", "Simon", ""], ["Daelemans", "Walter", ""]]}, {"id": "1808.09781", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang and Julian McAuley", "title": "Self-Attentive Sequential Recommendation", "comments": "Accepted by ICDM'18 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential dynamics are a key feature of many modern recommender systems,\nwhich seek to capture the `context' of users' activities on the basis of\nactions they have performed recently. To capture such patterns, two approaches\nhave proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs).\nMarkov Chains assume that a user's next action can be predicted on the basis of\njust their last (or last few) actions, while RNNs in principle allow for\nlonger-term semantics to be uncovered. Generally speaking, MC-based methods\nperform best in extremely sparse datasets, where model parsimony is critical,\nwhile RNNs perform better in denser datasets where higher model complexity is\naffordable. The goal of our work is to balance these two goals, by proposing a\nself-attention based sequential model (SASRec) that allows us to capture\nlong-term semantics (like an RNN), but, using an attention mechanism, makes its\npredictions based on relatively few actions (like an MC). At each time step,\nSASRec seeks to identify which items are `relevant' from a user's action\nhistory, and use them to predict the next item. Extensive empirical studies\nshow that our method outperforms various state-of-the-art sequential models\n(including MC/CNN/RNN-based approaches) on both sparse and dense datasets.\nMoreover, the model is an order of magnitude more efficient than comparable\nCNN/RNN-based models. Visualizations on attention weights also show how our\nmodel adaptively handles datasets with various density, and uncovers meaningful\npatterns in activity sequences.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 04:28:05 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["McAuley", "Julian", ""]]}, {"id": "1808.09784", "submitter": "Ting-Hsiang Wang", "authors": "Kwei-Herng Lai, Ting-Hsiang Wang, Heng-Yu Chi, Yian Chen, Ming-Feng\n  Tsai, and Chuan-Ju Wang", "title": "Superhighway: Bypass Data Sparsity in Cross-Domain CF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain collaborative filtering (CF) aims to alleviate data sparsity in\nsingle-domain CF by leveraging knowledge transferred from related domains. Many\ntraditional methods focus on enriching compared neighborhood relations in CF\ndirectly to address the sparsity problem. In this paper, we propose\nsuperhighway construction, an alternative explicit relation-enrichment\nprocedure, to improve recommendations by enhancing cross-domain connectivity.\nSpecifically, assuming partially overlapped items (users), superhighway\nbypasses multi-hop inter-domain paths between cross-domain users (items,\nrespectively) with direct paths to enrich the cross-domain connectivity. The\nexperiments conducted on a real-world cross-region music dataset and a\ncross-platform movie dataset show that the proposed superhighway construction\nsignificantly improves recommendation performance in both target and source\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 13:07:34 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Lai", "Kwei-Herng", ""], ["Wang", "Ting-Hsiang", ""], ["Chi", "Heng-Yu", ""], ["Chen", "Yian", ""], ["Tsai", "Ming-Feng", ""], ["Wang", "Chuan-Ju", ""]]}, {"id": "1808.09785", "submitter": "Farhan Khawar", "authors": "Farhan Khawar and Nevin L. Zhang", "title": "Using Taste Groups for Collaborative Filtering", "comments": "RecSys 2018 LBRS. arXiv admin note: substantial text overlap with\n  arXiv:1704.01889", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit feedback is the simplest form of user feedback that can be used for\nitem recommendation. It is easy to collect and domain independent. However,\nthere is a lack of negative examples. Existing works circumvent this problem by\nmaking various assumptions regarding the unconsumed items, which fail to hold\nwhen the user did not consume an item because she was unaware of it. In this\npaper, we propose as a novel method for addressing the lack of negative\nexamples in implicit feedback. The motivation is that if there is a large group\nof users who share the same taste and none of them consumed an item, then it is\nhighly likely that the item is irrelevant to this taste. We use Hierarchical\nLatent Tree Analysis(HLTA) to identify taste-based user groups and make\nrecommendations for a user based on her memberships in the groups.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 15:53:14 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Khawar", "Farhan", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1808.09794", "submitter": "Bin Yang", "authors": "Razvan-Gabriel Cirstea, Darius-Valer Micu, Gabriel-Marcel Muresan,\n  Chenjuan Guo, Bin Yang", "title": "Correlated Time Series Forecasting using Deep Neural Networks: A Summary\n  of Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems often consist of entities that interact with each\nother over time. Meanwhile, as part of the continued digitization of industrial\nprocesses, various sensor technologies are deployed that enable us to record\ntime-varying attributes (a.k.a., time series) of such entities, thus producing\ncorrelated time series. To enable accurate forecasting on such correlated time\nseries, this paper proposes two models that combine convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs). The first model employs a\nCNN on each individual time series, combines the convoluted features, and then\napplies an RNN on top of the convoluted features in the end to enable\nforecasting. The second model adds additional auto-encoders into the individual\nCNNs, making the second model a multi-task learning model, which provides\naccurate and robust forecasting. Experiments on two real-world correlated time\nseries data set suggest that the proposed two models are effective and\noutperform baselines in most settings.\n  This report extends the paper \"Correlated Time Series Forecasting using\nMulti-Task Deep Neural Networks,\" to appear in ACM CIKM 2018, by providing\nadditional experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 13:25:11 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 19:40:56 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Cirstea", "Razvan-Gabriel", ""], ["Micu", "Darius-Valer", ""], ["Muresan", "Gabriel-Marcel", ""], ["Guo", "Chenjuan", ""], ["Yang", "Bin", ""]]}, {"id": "1808.09802", "submitter": "Di Zhu", "authors": "Di Zhu and Yu Liu", "title": "Modelling Irregular Spatial Patterns using Graph Convolutional Neural\n  Networks", "comments": "10 pages, 8 figures, preprint for arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The understanding of geographical reality is a process of data representation\nand pattern discovery. Former studies mainly adopted continuous-field models to\nrepresent spatial variables and to investigate the underlying spatial\ncontinuity/heterogeneity in the regular spatial domain. In this article, we\nintroduce a more generalized model based on graph convolutional neural networks\n(GCNs) that can capture the complex parameters of spatial patterns underlying\ngraph-structured spatial data, which generally contain both Euclidean spatial\ninformation and non-Euclidean feature information. A trainable semi-supervised\nprediction framework is proposed to model the spatial distribution patterns of\nintra-urban points of interest(POI) check-ins. This work demonstrates the\nfeasibility of GCNs in complex geographic decision problems and provides a\npromising tool to analyze irregular spatial data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 07:36:01 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Zhu", "Di", ""], ["Liu", "Yu", ""]]}, {"id": "1808.09819", "submitter": "Adrien Ali Taiga", "authors": "Adrien Ali Ta\\\"iga, Aaron Courville and Marc G. Bellemare", "title": "Approximate Exploration through State Abstraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although exploration in reinforcement learning is well understood from a\ntheoretical point of view, provably correct methods remain impractical. In this\npaper we study the interplay between exploration and approximation, what we\ncall approximate exploration. Our main goal is to further our theoretical\nunderstanding of pseudo-count based exploration bonuses (Bellemare et al.,\n2016), a practical exploration scheme based on density modelling. As a warm-up,\nwe quantify the performance of an exploration algorithm, MBIE-EB (Strehl and\nLittman, 2008), when explicitly combined with state aggregation. This allows us\nto confirm that, as might be expected, approximation allows the agent to trade\noff between learning speed and quality of the learned policy. Next, we show how\na given density model can be related to an abstraction and that the\ncorresponding pseudo-count bonus can act as a substitute in MBIE-EB combined\nwith this abstraction, but may lead to either under- or over-exploration. Then,\nwe show that a given density model also defines an implicit abstraction, and\nfind a surprising mismatch between pseudo-counts derived either implicitly or\nexplicitly. Finally we derive a new pseudo-count bonus alleviating this issue.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 13:41:33 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 17:18:53 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Ta\u00efga", "Adrien Ali", ""], ["Courville", "Aaron", ""], ["Bellemare", "Marc G.", ""]]}, {"id": "1808.09830", "submitter": "AnChieh Cheng", "authors": "An-Chieh Cheng, Jin-Dong Dong, Chi-Hung Hsu, Shu-Huan Chang, Min Sun,\n  Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan", "title": "Searching Toward Pareto-Optimal Device-Aware Neural Architectures", "comments": "ICCAD'18 Invited Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in Neural Architectural Search (NAS) have achieved\nstate-of-the-art performance in many tasks such as image classification and\nlanguage understanding. However, most existing works only optimize for model\naccuracy and largely ignore other important factors imposed by the underlying\nhardware and devices, such as latency and energy, when making inference. In\nthis paper, we first introduce the problem of NAS and provide a survey on\nrecent works. Then we deep dive into two recent advancements on extending NAS\ninto multiple-objective frameworks: MONAS and DPP-Net. Both MONAS and DPP-Net\nare capable of optimizing accuracy and other objectives imposed by devices,\nsearching for neural architectures that can be best deployed on a wide spectrum\nof devices: from embedded systems and mobile devices to workstations.\nExperimental results are poised to show that architectures found by MONAS and\nDPP-Net achieves Pareto optimality w.r.t the given objectives for various\ndevices.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 13:52:40 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 00:35:52 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Cheng", "An-Chieh", ""], ["Dong", "Jin-Dong", ""], ["Hsu", "Chi-Hung", ""], ["Chang", "Shu-Huan", ""], ["Sun", "Min", ""], ["Chang", "Shih-Chieh", ""], ["Pan", "Jia-Yu", ""], ["Chen", "Yu-Ting", ""], ["Wei", "Wei", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "1808.09856", "submitter": "Yu Zeng", "authors": "Jie Chen, Yu Zeng (Corresponding author)", "title": "Application of Machine Learning in Rock Facies Classification with\n  Physics-Motivated Feature Augmentation", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent progress in algorithms and the availability of massive amounts of\ncomputation power, application of machine learning techniques is becoming a hot\ntopic in the oil and gas industry. One of the most promising aspects to apply\nmachine learning to the upstream field is the rock facies classification in\nreservoir characterization, which is crucial in determining the net pay\nthickness of reservoirs, thus a definitive factor in drilling decision making\nprocess. For complex machine learning tasks like facies classification, feature\nengineering is often critical. This paper shows the inclusion of\nphysics-motivated feature interaction in feature augmentation can further\nimprove the capability of machine learning in rock facies classification. We\ndemonstrate this approach with the SEG 2016 machine learning contest dataset\nand the top winning algorithms. The improvement is roboust and can be $\\sim5\\%$\nbetter than current existing best F-1 score, where F-1 is an evaluation metric\nused to quantify average prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 14:43:12 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Chen", "Jie", "", "Corresponding author"], ["Zeng", "Yu", "", "Corresponding author"]]}, {"id": "1808.09889", "submitter": "Javid Dadashkarimi", "authors": "Javid Dadashkarimi and Alexander Fabbri and Sekhar Tatikonda and\n  Dragomir R. Radev", "title": "Zero-shot Transfer Learning for Semantic Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural networks have shown impressive performance on large datasets,\napplying these models to tasks where little data is available remains a\nchallenging problem.\n  In this paper we propose to use feature transfer in a zero-shot experimental\nsetting on the task of semantic parsing.\n  We first introduce a new method for learning the shared space between\nmultiple domains based on the prediction of the domain label for each example.\n  Our experiments support the superiority of this method in a zero-shot\nexperimental setting in terms of accuracy metrics compared to state-of-the-art\ntechniques.\n  In the second part of this paper we study the impact of individual domains\nand examples on semantic parsing performance.\n  We use influence functions to this aim and investigate the sensitivity of\ndomain-label classification loss on each example.\n  Our findings reveal that cross-domain adversarial attacks identify useful\nexamples for training even from the domains the least similar to the target\ndomain. Augmenting our training data with these influential examples further\nboosts our accuracy at both the token and the sequence level.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 16:12:36 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Dadashkarimi", "Javid", ""], ["Fabbri", "Alexander", ""], ["Tatikonda", "Sekhar", ""], ["Radev", "Dragomir R.", ""]]}, {"id": "1808.09897", "submitter": "Nathan VanHoudnos", "authors": "Carson D. Sestili and William S. Snavely and Nathan M. VanHoudnos", "title": "Towards security defect prediction with AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate the limits of the current state of the art AI\nsystem for detecting buffer overflows and compare it with current static\nanalysis tools. To do so, we developed a code generator, s-bAbI, capable of\nproducing an arbitrarily large number of code samples of controlled complexity.\nWe found that the static analysis engines we examined have good precision, but\npoor recall on this dataset, except for a sound static analyzer that has good\nprecision and recall. We found that the state of the art AI system, a memory\nnetwork modeled after Choi et al. [1], can achieve similar performance to the\nstatic analysis engines, but requires an exhaustive amount of training data in\norder to do so. Our work points towards future approaches that may solve these\nproblems; namely, using representations of code that can capture appropriate\nscope information and using deep learning methods that are able to perform\narithmetic operations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 15:57:27 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 16:40:29 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Sestili", "Carson D.", ""], ["Snavely", "William S.", ""], ["VanHoudnos", "Nathan M.", ""]]}, {"id": "1808.09902", "submitter": "Edoardo Vignotto", "authors": "Edoardo Vignotto, Sebastian Engelke", "title": "Extreme Value Theory for Open Set Classification -- GPD and GEV\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification tasks usually assume that all possible classes are present\nduring the training phase. This is restrictive if the algorithm is used over a\nlong time and possibly encounters samples from unknown classes. The recently\nintroduced extreme value machine, a classifier motivated by extreme value\ntheory, addresses this problem and achieves competitive performance in specific\ncases. We show that this algorithm can fail when the geometries of known and\nunknown classes differ. To overcome this problem, we propose two new algorithms\nrelying on approximations from extreme value theory. We show the effectiveness\nof our classifiers in simulations and on the LETTER and MNIST data sets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 16:07:26 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 07:51:57 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 19:32:13 GMT"}, {"version": "v4", "created": "Tue, 16 Jul 2019 20:03:55 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Vignotto", "Edoardo", ""], ["Engelke", "Sebastian", ""]]}, {"id": "1808.09907", "submitter": "Zongjie Ma", "authors": "Zongjie Ma, Abdul Sattar, Jun Zhou, Qingliang Chen, Kaile Su", "title": "Dropout with Tabu Strategy for Regularizing Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout has proven to be an effective technique for regularization and\npreventing the co-adaptation of neurons in deep neural networks (DNN). It\nrandomly drops units with a probability $p$ during the training stage of DNN.\nDropout also provides a way of approximately combining exponentially many\ndifferent neural network architectures efficiently. In this work, we add a\ndiversification strategy into dropout, which aims at generating more different\nneural network architectures in a proper times of iterations. The dropped units\nin last forward propagation will be marked. Then the selected units for\ndropping in the current FP will be kept if they have been marked in the last\nforward propagation. We only mark the units from the last forward propagation.\nWe call this new technique Tabu Dropout. Tabu Dropout has no extra parameters\ncompared with the standard Dropout and also it is computationally cheap. The\nexperiments conducted on MNIST, Fashion-MNIST datasets show that Tabu Dropout\nimproves the performance of the standard dropout.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 16:18:02 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Ma", "Zongjie", ""], ["Sattar", "Abdul", ""], ["Zhou", "Jun", ""], ["Chen", "Qingliang", ""], ["Su", "Kaile", ""]]}, {"id": "1808.09933", "submitter": "Mikael Vejdemo-Johansson", "authors": "Mikael Vejdemo-Johansson and Alisa Leshchenko", "title": "Certified Mapper: Repeated testing for acyclicity and obstructions to\n  the nerve lemma", "comments": "16 pages, submitted to the proceedings of the Abel symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mapper algorithm does not include a check for whether the cover produced\nconforms to the requirements of the nerve lemma. To perform a check for\nobstructions to the nerve lemma, statistical considerations of multiple testing\nquickly arise.\n  In this paper, we propose several statistical approaches to finding\nobstructions: through a persistent nerve lemma, through simulation testing, and\nusing a parametric refinement of simulation tests.\n  We suggest Certified Mapper -- a method built from these approaches to\ngenerate certificates of non-obstruction, or identify specific obstructions to\nthe nerve lemma -- and we give recommendations for which statistical approaches\nare most appropriate for the task.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 17:21:17 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Vejdemo-Johansson", "Mikael", ""], ["Leshchenko", "Alisa", ""]]}, {"id": "1808.09935", "submitter": "Pinkesh Badjatiya", "authors": "Pinkesh Badjatiya, Litton J Kurisinkel, Manish Gupta, and Vasudeva\n  Varma", "title": "Attention-based Neural Text Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text segmentation plays an important role in various Natural Language\nProcessing (NLP) tasks like summarization, context understanding, document\nindexing and document noise removal. Previous methods for this task require\nmanual feature engineering, huge memory requirements and large execution times.\nTo the best of our knowledge, this paper is the first one to present a novel\nsupervised neural approach for text segmentation. Specifically, we propose an\nattention-based bidirectional LSTM model where sentence embeddings are learned\nusing CNNs and the segments are predicted based on contextual information. This\nmodel can automatically handle variable sized context information. Compared to\nthe existing competitive baselines, the proposed model shows a performance\nimprovement of ~7% in WinDiff score on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 17:29:12 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Badjatiya", "Pinkesh", ""], ["Kurisinkel", "Litton J", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1808.09940", "submitter": "Zhipeng Liang", "authors": "Zhipeng Liang, Hao Chen, Junhao Zhu, Kangkang Jiang, Yanran Li", "title": "Adversarial Deep Reinforcement Learning in Portfolio Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we implement three state-of-art continuous reinforcement\nlearning algorithms, Deep Deterministic Policy Gradient (DDPG), Proximal Policy\nOptimization (PPO) and Policy Gradient (PG)in portfolio management. All of them\nare widely-used in game playing and robot control. What's more, PPO has\nappealing theoretical propeties which is hopefully potential in portfolio\nmanagement. We present the performances of them under different settings,\nincluding different learning rates, objective functions, feature combinations,\nin order to provide insights for parameters tuning, features selection and data\npreparation. We also conduct intensive experiments in China Stock market and\nshow that PG is more desirable in financial market than DDPG and PPO, although\nboth of them are more advanced. What's more, we propose a so called Adversarial\nTraining method and show that it can greatly improve the training efficiency\nand significantly promote average daily return and sharpe ratio in back test.\nBased on this new modification, our experiments results show that our agent\nbased on Policy Gradient can outperform UCRP.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 17:39:08 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 05:07:58 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 01:26:41 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Liang", "Zhipeng", ""], ["Chen", "Hao", ""], ["Zhu", "Junhao", ""], ["Jiang", "Kangkang", ""], ["Li", "Yanran", ""]]}, {"id": "1808.09942", "submitter": "Nitish Gupta", "authors": "Nitish Gupta and Mike Lewis", "title": "Neural Compositional Denotational Semantics for Question Answering", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering compositional questions requiring multi-step reasoning is\nchallenging. We introduce an end-to-end differentiable model for interpreting\nquestions about a knowledge graph (KG), which is inspired by formal approaches\nto semantics. Each span of text is represented by a denotation in a KG and a\nvector that captures ungrounded aspects of meaning. Learned composition modules\nrecursively combine constituent spans, culminating in a grounding for the\ncomplete sentence which answers the question. For example, to interpret \"not\ngreen\", the model represents \"green\" as a set of KG entities and \"not\" as a\ntrainable ungrounded vector---and then uses this vector to parameterize a\ncomposition function that performs a complement operation. For each sentence,\nwe build a parse chart subsuming all possible parses, allowing the model to\njointly learn both the composition operators and output structure by gradient\ndescent from end-task supervision. The model learns a variety of challenging\nsemantic operators, such as quantifiers, disjunctions and composed relations,\nand infers latent syntactic structure. It also generalizes well to longer\nquestions than seen in its training data, in contrast to RNN, its tree-based\nvariants, and semantic parsing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 17:43:11 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Gupta", "Nitish", ""], ["Lewis", "Mike", ""]]}, {"id": "1808.09955", "submitter": "Nicolas Busca", "authors": "Nicolas Busca, Christophe Balland", "title": "QuasarNET: Human-level spectral classification and redshifting with Deep\n  Neural Networks", "comments": "Submitted to MMRAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.CO astro-ph.GA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce QuasarNET, a deep convolutional neural network that performs\nclassification and redshift estimation of astrophysical spectra with\nhuman-expert accuracy. We pose these two tasks as a \\emph{feature detection}\nproblem: presence or absence of spectral features determines the class, and\ntheir wavelength determines the redshift, very much like human-experts proceed.\nWhen ran on BOSS data to identify quasars through their emission lines,\nQuasarNET defines a sample $99.51\\pm0.03$\\% pure and $99.52\\pm0.03$\\% complete,\nwell above the requirements of many analyses using these data. QuasarNET\nsignificantly reduces the problem of line-confusion that induces catastrophic\nredshift failures to below 0.2\\%. We also extend QuasarNET to classify spectra\nwith broad absorption line (BAL) features, achieving an accuracy of\n$98.0\\pm0.4$\\% for recognizing BAL and $97.0\\pm0.2$\\% for rejecting non-BAL\nquasars. QuasarNET is trained on data of low signal-to-noise and medium\nresolution, typical of current and future astrophysical surveys, and could be\neasily applied to classify spectra from current and upcoming surveys such as\neBOSS, DESI and 4MOST.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 07:52:42 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Busca", "Nicolas", ""], ["Balland", "Christophe", ""]]}, {"id": "1808.09964", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Semi-Metrification of the Dynamic Time Warping Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic time warping (dtw) distance fails to satisfy the triangle\ninequality and the identity of indiscernibles. As a consequence, the\ndtw-distance is not warping-invariant, which in turn results in peculiarities\nin data mining applications. This article converts the dtw-distance to a\nsemi-metric and shows that its canonical extension is warping-invariant.\nEmpirical results indicate that the nearest-neighbor classifier in the proposed\nsemi-metric space performs comparably to the same classifier in the standard\ndtw-space. To overcome the undesirable peculiarities of dtw-spaces, this result\nsuggests to further explore the semi-metric space for data mining applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 14:46:35 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 06:17:56 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1808.10009", "submitter": "Aishwarya Padmakumar", "authors": "Aishwarya Padmakumar and Peter Stone and Raymond J. Mooney", "title": "Learning a Policy for Opportunistic Active Learning", "comments": "EMNLP 2018 Camera Ready", "journal-ref": "EMNLP 2018", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning identifies data points to label that are expected to be the\nmost useful in improving a supervised model. Opportunistic active learning\nincorporates active learning into interactive tasks that constrain possible\nqueries during interactions. Prior work has shown that opportunistic active\nlearning can be used to improve grounding of natural language descriptions in\nan interactive object retrieval task. In this work, we use reinforcement\nlearning for such an object retrieval task, to learn a policy that effectively\ntrades off task completion with model improvement that would benefit future\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 18:40:26 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Padmakumar", "Aishwarya", ""], ["Stone", "Peter", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1808.10013", "submitter": "Lydia T. Liu", "authors": "Lydia T. Liu, Max Simchowitz, Moritz Hardt", "title": "The implicit fairness criterion of unconstrained learning", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We clarify what fairness guarantees we can and cannot expect to follow from\nunconstrained machine learning. Specifically, we characterize when\nunconstrained learning on its own implies group calibration, that is, the\noutcome variable is conditionally independent of group membership given the\nscore. We show that under reasonable conditions, the deviation from satisfying\ngroup calibration is upper bounded by the excess risk of the learned score\nrelative to the Bayes optimal score function. A lower bound confirms the\noptimality of our upper bound. Moreover, we prove that as the excess risk of\nthe learned score decreases, it strongly violates separation and independence,\ntwo other standard fairness criteria.\n  Our results show that group calibration is the fairness criterion that\nunconstrained learning implicitly favors. On the one hand, this means that\ncalibration is often satisfied on its own without the need for active\nintervention, albeit at the cost of violating other criteria that are at odds\nwith calibration. On the other hand, it suggests that we should be satisfied\nwith calibration as a fairness criterion only if we are at ease with the use of\nunconstrained machine learning in a given application.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 18:55:22 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 07:47:36 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Liu", "Lydia T.", ""], ["Simchowitz", "Max", ""], ["Hardt", "Moritz", ""]]}, {"id": "1808.10026", "submitter": "Andr\\'es Felipe L\\'opez-Lopera", "authors": "Andr\\'es F. L\\'opez-Lopera, Nicolas Durrande and Mauricio A. Alvarez", "title": "Physically-Inspired Gaussian Process Models for Post-Transcriptional\n  Regulation in Drosophila", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regulatory process of Drosophila is thoroughly studied for understanding\na great variety of biological principles. While pattern-forming gene networks\nare analysed in the transcription step, post-transcriptional events (e.g.\ntranslation, protein processing) play an important role in establishing protein\nexpression patterns and levels. Since the post-transcriptional regulation of\nDrosophila depends on spatiotemporal interactions between mRNAs and gap\nproteins, proper physically-inspired stochastic models are required to study\nthe link between both quantities. Previous research attempts have shown that\nusing Gaussian processes (GPs) and differential equations lead to promising\npredictions when analysing regulatory networks. Here we aim at further\ninvestigating two types of physically-inspired GP models based on a\nreaction-diffusion equation where the main difference lies in where the prior\nis placed. While one of them has been studied previously using protein data\nonly, the other is novel and yields a simple approach requiring only the\ndifferentiation of kernel functions. In contrast to other stochastic\nframeworks, discretising the spatial space is not required here. Both GP models\nare tested under different conditions depending on the availability of gap gene\nmRNA expression data. Finally, their performances are assessed on a\nhigh-resolution dataset describing the blastoderm stage of the early embryo of\nDrosophila melanogaster\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 20:02:41 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 07:49:26 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 07:29:15 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["L\u00f3pez-Lopera", "Andr\u00e9s F.", ""], ["Durrande", "Nicolas", ""], ["Alvarez", "Mauricio A.", ""]]}, {"id": "1808.10038", "submitter": "Xiaohan Chen", "authors": "Xiaohan Chen, Jialin Liu, Zhangyang Wang, Wotao Yin", "title": "Theoretical Linear Convergence of Unfolded ISTA and its Practical\n  Weights and Thresholds", "comments": "18 pages, 6 figures, 1 table. Accepted as spotlight oral in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, unfolding iterative algorithms as neural networks has become\nan empirical success in solving sparse recovery problems. However, its\ntheoretical understanding is still immature, which prevents us from fully\nutilizing the power of neural networks. In this work, we study unfolded ISTA\n(Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We\nintroduce a weight structure that is necessary for asymptotic convergence to\nthe true sparse signal. With this structure, unfolded ISTA can attain a linear\nconvergence, which is better than the sublinear convergence of ISTA/FISTA in\ngeneral cases. Furthermore, we propose to incorporate thresholding in the\nnetwork to perform support selection, which is easy to implement and able to\nboost the convergence rate both theoretically and empirically. Extensive\nsimulations, including sparse vector recovery and a compressive sensing\nexperiment on real image data, corroborate our theoretical results and\ndemonstrate their practical usefulness. We have made our codes publicly\navailable: https://github.com/xchen-tamu/linear-lista-cpss.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 20:45:54 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 03:54:13 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Chen", "Xiaohan", ""], ["Liu", "Jialin", ""], ["Wang", "Zhangyang", ""], ["Yin", "Wotao", ""]]}, {"id": "1808.10056", "submitter": "Rachel Cummings", "authors": "Rachel Cummings, Sara Krehbiel, Yajun Mei, Rui Tuo, Wanrong Zhang", "title": "Differentially Private Change-Point Detection", "comments": null, "journal-ref": "Proceedings of the 32nd International Conference on Neural\n  Information Processing Systems (2018) Pages 10848-10857", "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The change-point detection problem seeks to identify distributional changes\nat an unknown change-point k* in a stream of data. This problem appears in many\nimportant practical settings involving personal data, including\nbiosurveillance, fault detection, finance, signal detection, and security\nsystems. The field of differential privacy offers data analysis tools that\nprovide powerful worst-case privacy guarantees. We study the statistical\nproblem of change-point detection through the lens of differential privacy. We\ngive private algorithms for both online and offline change-point detection,\nanalyze these algorithms theoretically, and provide empirical validation of our\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 22:17:24 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Cummings", "Rachel", ""], ["Krehbiel", "Sara", ""], ["Mei", "Yajun", ""], ["Tuo", "Rui", ""], ["Zhang", "Wanrong", ""]]}, {"id": "1808.10073", "submitter": "Zhiqian Chen", "authors": "Zhiqian Chen, Feng Chen, Rongjie Lai, Xuchao Zhang and Chang-Tien Lu", "title": "Rational Neural Networks for Approximating Jump Discontinuities of Graph\n  Convolution Operator", "comments": "ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For node level graph encoding, a recent important state-of-art method is the\ngraph convolutional networks (GCN), which nicely integrate local vertex\nfeatures and graph topology in the spectral domain. However, current studies\nsuffer from several drawbacks: (1) graph CNNs relies on Chebyshev polynomial\napproximation which results in oscillatory approximation at jump\ndiscontinuities; (2) Increasing the order of Chebyshev polynomial can reduce\nthe oscillations issue, but also incurs unaffordable computational cost; (3)\nChebyshev polynomials require degree $\\Omega$(poly(1/$\\epsilon$)) to\napproximate a jump signal such as $|x|$, while rational function only needs\n$\\mathcal{O}$(poly log(1/$\\epsilon$))\\cite{liang2016deep,telgarsky2017neural}.\nHowever, it's non-trivial to apply rational approximation without increasing\ncomputational complexity due to the denominator. In this paper, the superiority\nof rational approximation is exploited for graph signal recovering. RatioanlNet\nis proposed to integrate rational function and neural networks. We show that\nrational function of eigenvalues can be rewritten as a function of graph\nLaplacian, which can avoid multiplication by the eigenvector matrix. Focusing\non the analysis of approximation on graph convolution operation, a graph signal\nregression task is formulated. Under graph signal regression task, its time\ncomplexity can be significantly reduced by graph Fourier transform. To overcome\nthe local minimum problem of neural networks model, a relaxed Remez algorithm\nis utilized to initialize the weight parameters. Convergence rate of\nRatioanlNet and polynomial based methods on jump signal is analyzed for a\ntheoretical guarantee. The extensive experimental results demonstrated that our\napproach could effectively characterize the jump discontinuities, outperforming\ncompeting methods by a substantial margin on both synthetic and real-world\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 00:33:28 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Chen", "Zhiqian", ""], ["Chen", "Feng", ""], ["Lai", "Rongjie", ""], ["Zhang", "Xuchao", ""], ["Lu", "Chang-Tien", ""]]}, {"id": "1808.10078", "submitter": "Shubhendu Trivedi", "authors": "Shubhendu Trivedi", "title": "Discriminative Learning of Similarity and Group Equivariant\n  Representations", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental problems in machine learning is to compare\nexamples: Given a pair of objects we want to return a value which indicates\ndegree of (dis)similarity. Similarity is often task specific, and pre-defined\ndistances can perform poorly, leading to work in metric learning. However,\nbeing able to learn a similarity-sensitive distance function also presupposes\naccess to a rich, discriminative representation for the objects at hand. In\nthis dissertation we present contributions towards both ends. In the first part\nof the thesis, assuming good representations for the data, we present a\nformulation for metric learning that makes a more direct attempt to optimize\nfor the k-NN accuracy as compared to prior work. We also present extensions of\nthis formulation to metric learning for kNN regression, asymmetric similarity\nlearning and discriminative learning of Hamming distance. In the second part,\nwe consider a situation where we are on a limited computational budget i.e.\noptimizing over a space of possible metrics would be infeasible, but access to\na label aware distance metric is still desirable. We present a simple, and\ncomputationally inexpensive approach for estimating a well motivated metric\nthat relies only on gradient estimates, discussing theoretical and experimental\nresults. In the final part, we address representational issues, considering\ngroup equivariant convolutional neural networks (GCNNs). Equivariance to\nsymmetry transformations is explicitly encoded in GCNNs; a classical CNN being\nthe simplest example. In particular, we present a SO(3)-equivariant neural\nnetwork architecture for spherical data, that operates entirely in Fourier\nspace, while also providing a formalism for the design of fully Fourier neural\nnetworks that are equivariant to the action of any continuous compact group.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 01:05:40 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Trivedi", "Shubhendu", ""]]}, {"id": "1808.10101", "submitter": "Zonghao Huang", "authors": "Zonghao Huang, Rui Hu, Yuanxiong Guo, Eric Chan-Tin, and Yanmin Gong", "title": "DP-ADMM: ADMM-based Distributed Learning with Differential Privacy", "comments": "Accepted for publication in IEEE Transactions on Information\n  Forensics and Security (TIFS)", "journal-ref": "IEEE Transactions on Information Forensics and Security, vol. 15,\n  pp. 1002-1012, 2020", "doi": "10.1109/TIFS.2019.2931068", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating Direction Method of Multipliers (ADMM) is a widely used tool for\nmachine learning in distributed settings, where a machine learning model is\ntrained over distributed data sources through an interactive process of local\ncomputation and message passing. Such an iterative process could cause privacy\nconcerns of data owners. The goal of this paper is to provide differential\nprivacy for ADMM-based distributed machine learning. Prior approaches on\ndifferentially private ADMM exhibit low utility under high privacy guarantee\nand often assume the objective functions of the learning problems to be smooth\nand strongly convex. To address these concerns, we propose a novel\ndifferentially private ADMM-based distributed learning algorithm called\nDP-ADMM, which combines an approximate augmented Lagrangian function with\ntime-varying Gaussian noise addition in the iterative process to achieve higher\nutility for general objective functions under the same differential privacy\nguarantee. We also apply the moments accountant method to bound the end-to-end\nprivacy loss. The theoretical analysis shows that DP-ADMM can be applied to a\nwider class of distributed learning problems, is provably convergent, and\noffers an explicit utility-privacy tradeoff. To our knowledge, this is the\nfirst paper to provide explicit convergence and utility properties for\ndifferentially private ADMM-based distributed learning algorithms. The\nevaluation results demonstrate that our approach can achieve good convergence\nand model accuracy under high end-to-end differential privacy guarantee.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 03:35:50 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 03:12:03 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 18:22:30 GMT"}, {"version": "v4", "created": "Tue, 25 Dec 2018 07:43:39 GMT"}, {"version": "v5", "created": "Mon, 25 Mar 2019 04:56:48 GMT"}, {"version": "v6", "created": "Fri, 19 Jul 2019 19:28:35 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Huang", "Zonghao", ""], ["Hu", "Rui", ""], ["Guo", "Yuanxiong", ""], ["Chan-Tin", "Eric", ""], ["Gong", "Yanmin", ""]]}, {"id": "1808.10120", "submitter": "Andy Kitchen BSc", "authors": "Andy Kitchen and Michela Benedetti", "title": "ExIt-OOS: Towards Learning from Planning in Imperfect Information Games", "comments": "8 pages. 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The current state of the art in playing many important perfect information\ngames, including Chess and Go, combines planning and deep reinforcement\nlearning with self-play. We extend this approach to imperfect information games\nand present ExIt-OOS, a novel approach to playing imperfect information games\nwithin the Expert Iteration framework and inspired by AlphaZero. We use Online\nOutcome Sampling, an online search algorithm for imperfect information games in\nplace of MCTS. While training online, our neural strategy is used to improve\nthe accuracy of playouts in OOS, allowing a learning and planning feedback loop\nfor imperfect information games.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 05:04:44 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 00:51:42 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Kitchen", "Andy", ""], ["Benedetti", "Michela", ""]]}, {"id": "1808.10122", "submitter": "Sam Wiseman", "authors": "Sam Wiseman, Stuart M. Shieber, Alexander M. Rush", "title": "Learning Neural Templates for Text Generation", "comments": "EMNLP 2018; purity calculations updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural, encoder-decoder models have had significant empirical success\nin text generation, there remain several unaddressed problems with this style\nof generation. Encoder-decoder models are largely (a) uninterpretable, and (b)\ndifficult to control in terms of their phrasing or content. This work proposes\na neural generation system using a hidden semi-markov model (HSMM) decoder,\nwhich learns latent, discrete templates jointly with learning to generate. We\nshow that this model learns useful templates, and that these templates make\ngeneration both more interpretable and controllable. Furthermore, we show that\nthis approach scales to real data sets and achieves strong performance nearing\nthat of encoder-decoder text generation models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 05:15:42 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 22:45:09 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 17:42:49 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Wiseman", "Sam", ""], ["Shieber", "Stuart M.", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1808.10128", "submitter": "Yu-An Chung", "authors": "Yu-An Chung, Yuxuan Wang, Wei-Ning Hsu, Yu Zhang, RJ Skerry-Ryan", "title": "Semi-Supervised Training for Improving Data Efficiency in End-to-End\n  Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although end-to-end text-to-speech (TTS) models such as Tacotron have shown\nexcellent results, they typically require a sizable set of high-quality <text,\naudio> pairs for training, which are expensive to collect. In this paper, we\npropose a semi-supervised training framework to improve the data efficiency of\nTacotron. The idea is to allow Tacotron to utilize textual and acoustic\nknowledge contained in large, publicly-available text and speech corpora.\nImportantly, these external data are unpaired and potentially noisy.\nSpecifically, first we embed each word in the input text into word vectors and\ncondition the Tacotron encoder on them. We then use an unpaired speech corpus\nto pre-train the Tacotron decoder in the acoustic domain. Finally, we fine-tune\nthe model using available paired data. We demonstrate that the proposed\nframework enables Tacotron to generate intelligible speech using less than half\nan hour of paired training data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 05:51:30 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Chung", "Yu-An", ""], ["Wang", "Yuxuan", ""], ["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Skerry-Ryan", "RJ", ""]]}, {"id": "1808.10134", "submitter": "Fan Zhu", "authors": "Fan Zhu, Lin Ma, Xin Xu, Dingfeng Guo, Xiao Cui, and Qi Kong", "title": "Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and\n  Learning based Vehicle Longitude Dynamic Calibrating Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any autonomous driving vehicle, control module determines its road\nperformance and safety, i.e. its precision and stability should stay within a\ncarefully-designed range. Nonetheless, control algorithms require vehicle\ndynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are\nobscure to calibrate in real time. As a result, to achieve reasonable\nperformance, most, if not all, research-oriented autonomous vehicles do manual\ncalibrations in a one-by-one fashion. Since manual calibration is not\nsustainable once entering into mass production stage for industrial purposes,\nwe here introduce a machine-learning based auto-calibration system for\nautonomous driving vehicles. In this paper, we will show how we build a\ndata-driven longitudinal calibration procedure using machine learning\ntechniques. We first generated offline calibration tables from human driving\ndata. The offline table serves as an initial guess for later uses and it only\nneeds twenty-minutes data collection and process. We then used an\nonline-learning algorithm to appropriately update the initial table (the\noffline table) based on real-time performance analysis. This longitudinal\nauto-calibration system has been deployed to more than one hundred Baidu Apollo\nself-driving vehicles (including hybrid family vehicles and electronic\ndelivery-only vehicles) since April 2018. By August 27, 2018, it had been\ntested for more than two thousands hours, ten thousands kilometers (6,213\nmiles) and yet proven to be effective.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 06:29:10 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Zhu", "Fan", ""], ["Ma", "Lin", ""], ["Xu", "Xin", ""], ["Guo", "Dingfeng", ""], ["Cui", "Xiao", ""], ["Kong", "Qi", ""]]}, {"id": "1808.10151", "submitter": "Rafael Dowsley", "authors": "Sisi Wang, Wing-Sea Poon, Golnoosh Farnadi, Caleb Horst, Kebra\n  Thompson, Michael Nickels, Rafael Dowsley, Anderson C. A. Nascimento and\n  Martine De Cock", "title": "VirtualIdentity: Privacy-Preserving User Profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User profiling from user generated content (UGC) is a common practice that\nsupports the business models of many social media companies. Existing systems\nrequire that the UGC is fully exposed to the module that constructs the user\nprofiles. In this paper we show that it is possible to build user profiles\nwithout ever accessing the user's original data, and without exposing the\ntrained machine learning models for user profiling -- which are the\nintellectual property of the company -- to the users of the social media site.\nWe present VirtualIdentity, an application that uses secure multi-party\ncryptographic protocols to detect the age, gender and personality traits of\nusers by classifying their user-generated text and personal pictures with\ntrained support vector machine models in a privacy-preserving manner.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 07:21:16 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Wang", "Sisi", ""], ["Poon", "Wing-Sea", ""], ["Farnadi", "Golnoosh", ""], ["Horst", "Caleb", ""], ["Thompson", "Kebra", ""], ["Nickels", "Michael", ""], ["Dowsley", "Rafael", ""], ["Nascimento", "Anderson C. A.", ""], ["De Cock", "Martine", ""]]}, {"id": "1808.10259", "submitter": "Aboubakr Aqle", "authors": "Aboubakr Aqle, Dena Al-Thani, Ali Jaoua", "title": "Analyze Unstructured Data Patterns for Conceptual Representation", "comments": "4 pages, 6 Figures, 4th Annual Conference on Computational Science &\n  Computational Intelligence (CSCI'17)", "journal-ref": null, "doi": "10.1109/CSCI.2017.46", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online news media provides aggregated news and stories from different sources\nall over the world and up-to-date news coverage. The main goal of this study is\nto have a solution that considered as a homogeneous source for the news and to\nrepresent the news in a new conceptual framework. Furthermore, the user can\neasily find different updated news in a fast way through the designed\ninterface. The Mobile App implementation is based on modeling the multi-level\nconceptual analysis discipline. Discovering main concepts of any domain is\ncaptured from the hidden unstructured data that are analyzed by the proposed\nsolution. Concepts are discovered through analyzing data patterns to be\nstructured into a tree-based interface for easy navigation for the end user,\nthrough the discovered news concepts. Our final experiment results showing that\nanalyzing the news before displaying to the end-user and restructuring the\nfinal output in a conceptual multilevel structure, that producing new display\nframe for the end user to find the related information to his interest.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 14:06:31 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Aqle", "Aboubakr", ""], ["Al-Thani", "Dena", ""], ["Jaoua", "Ali", ""]]}, {"id": "1808.10260", "submitter": "Benedikt Loepp", "authors": "Johannes Kunkel, Benedikt Loepp, J\\\"urgen Ziegler", "title": "Understanding Latent Factors Using a GWAP", "comments": "Proceedings of the Late-Breaking Results track part of the Twelfth\n  ACM Conference on Recommender Systems (RecSys '18), Vancouver, BC, Canada,\n  October 2-7, 2018, 2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems relying on latent factor models often appear as black\nboxes to their users. Semantic descriptions for the factors might help to\nmitigate this problem. Achieving this automatically is, however, a\nnon-straightforward task due to the models' statistical nature. We present an\noutput-agreement game that represents factors by means of sample items and\nmotivates players to create such descriptions. A user study shows that the\ncollected output actually reflects real-world characteristics of the factors.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 11:04:07 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Kunkel", "Johannes", ""], ["Loepp", "Benedikt", ""], ["Ziegler", "J\u00fcrgen", ""]]}, {"id": "1808.10261", "submitter": "Jiangning Chen", "authors": "Jiangning Chen, Heinrich Matzinger, Haoyan Zhai, Mi Zhou", "title": "Centroid estimation based on symmetric KL divergence for Multinomial\n  text classification problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new method to estimate centroid for text classification based on\nthe symmetric KL-divergence between the distribution of words in training\ndocuments and their class centroids. Experiments on several standard data sets\nindicate that the new method achieves substantial improvements over the\ntraditional classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 15:24:33 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 18:40:54 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chen", "Jiangning", ""], ["Matzinger", "Heinrich", ""], ["Zhai", "Haoyan", ""], ["Zhou", "Mi", ""]]}, {"id": "1808.10307", "submitter": "Cong Liao", "authors": "Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, David Miller", "title": "Backdoor Embedding in Convolutional Neural Network Models via Invisible\n  Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have consistently outperformed traditional machine\nlearning models in various classification tasks, including image\nclassification. As such, they have become increasingly prevalent in many real\nworld applications including those where security is of great concern. Such\npopularity, however, may attract attackers to exploit the vulnerabilities of\nthe deployed deep learning models and launch attacks against security-sensitive\napplications. In this paper, we focus on a specific type of data poisoning\nattack, which we refer to as a {\\em backdoor injection attack}. The main goal\nof the adversary performing such attack is to generate and inject a backdoor\ninto a deep learning model that can be triggered to recognize certain embedded\npatterns with a target label of the attacker's choice. Additionally, a backdoor\ninjection attack should occur in a stealthy manner, without undermining the\nefficacy of the victim model. Specifically, we propose two approaches for\ngenerating a backdoor that is hardly perceptible yet effective in poisoning the\nmodel. We consider two attack settings, with backdoor injection carried out\neither before model training or during model updating. We carry out extensive\nexperimental evaluations under various assumptions on the adversary model, and\ndemonstrate that such attacks can be effective and achieve a high attack\nsuccess rate (above $90\\%$) at a small cost of model accuracy loss (below\n$1\\%$) with a small injection rate (around $1\\%$), even under the weakest\nassumption wherein the adversary has no knowledge either of the original\ntraining data or the classifier model.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 14:13:39 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Liao", "Cong", ""], ["Zhong", "Haoti", ""], ["Squicciarini", "Anna", ""], ["Zhu", "Sencun", ""], ["Miller", "David", ""]]}, {"id": "1808.10322", "submitter": "Tolga Birdal", "authors": "Haowen Deng, Tolga Birdal, Slobodan Ilic", "title": "PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local\n  Descriptors", "comments": "Accepted for publication at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PPF-FoldNet for unsupervised learning of 3D local descriptors on\npure point cloud geometry. Based on the folding-based auto-encoding of well\nknown point pair features, PPF-FoldNet offers many desirable properties: it\nnecessitates neither supervision, nor a sensitive local reference frame,\nbenefits from point-set sparsity, is end-to-end, fast, and can extract powerful\nrotation invariant descriptors. Thanks to a novel feature visualization, its\nevolution can be monitored to provide interpretable insights. Our extensive\nexperiments demonstrate that despite having six degree-of-freedom invariance\nand lack of training labels, our network achieves state of the art results in\nstandard benchmark datasets and outperforms its competitors when rotations and\nvarying point densities are present. PPF-FoldNet achieves $9\\%$ higher recall\non standard benchmarks, $23\\%$ higher recall when rotations are introduced into\nthe same datasets and finally, a margin of $>35\\%$ is attained when point\ndensity is significantly decreased.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 14:45:26 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Deng", "Haowen", ""], ["Birdal", "Tolga", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1808.10340", "submitter": "Kevin Luk", "authors": "Kevin Luk, Roger Grosse", "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most neural networks are trained using first-order optimization methods,\nwhich are sensitive to the parameterization of the model. Natural gradient\ndescent is invariant to smooth reparameterizations because it is defined in a\ncoordinate-free way, but tractable approximations are typically defined in\nterms of coordinate systems, and hence may lose the invariance properties. We\nanalyze the invariance properties of the Kronecker-Factored Approximate\nCurvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free\nway. We explicitly construct a Riemannian metric under which the natural\ngradient matches the K-FAC update; invariance to affine transformations of the\nactivations follows immediately. We extend our framework to analyze the\ninvariance properties of K-FAC applied to convolutional networks and recurrent\nneural networks, as well as metrics other than the usual Fisher metric.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:06:03 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Luk", "Kevin", ""], ["Grosse", "Roger", ""]]}, {"id": "1808.10350", "submitter": "Abduallah Mohamed", "authors": "Abduallah Mohamed, Xinrui Hua, Xianda Zhou and Christian Claudel", "title": "IEA: Inner Ensemble Average within a convolutional neural network", "comments": "Updating results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning is a method of combining multiple trained models to improve\nmodel accuracy. We propose the usage of such methods, specifically ensemble\naverage, inside Convolutional Neural Network (CNN) architectures by replacing\nthe single convolutional layers with Inner Average Ensembles (IEA) of multiple\nconvolutional layers. Empirical results on different benchmarking datasets show\nthat CNN models using IEA outperform those with regular convolutional layers. A\nvisual and a similarity score analysis of the features generated from IEA\nexplains why it boosts the model performance.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:24:20 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 01:36:31 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 02:50:55 GMT"}, {"version": "v4", "created": "Sun, 2 Dec 2018 05:34:10 GMT"}, {"version": "v5", "created": "Tue, 6 Aug 2019 21:35:34 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Mohamed", "Abduallah", ""], ["Hua", "Xinrui", ""], ["Zhou", "Xianda", ""], ["Claudel", "Christian", ""]]}, {"id": "1808.10356", "submitter": "Matan Ben-Yosef", "authors": "Matan Ben-Yosef and Daphna Weinshall", "title": "Gaussian Mixture Generative Adversarial Networks for Diverse Datasets,\n  and the Unsupervised Clustering of Images", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have been shown to produce\nrealistically looking synthetic images with remarkable success, yet their\nperformance seems less impressive when the training set is highly diverse. In\norder to provide a better fit to the target data distribution when the dataset\nincludes many different classes, we propose a variant of the basic GAN model,\ncalled Gaussian Mixture GAN (GM-GAN), where the probability distribution over\nthe latent space is a mixture of Gaussians. We also propose a supervised\nvariant which is capable of conditional sample synthesis. In order to evaluate\nthe model's performance, we propose a new scoring method which separately takes\ninto account two (typically conflicting) measures - diversity vs. quality of\nthe generated data. Through a series of empirical experiments, using both\nsynthetic and real-world datasets, we quantitatively show that GM-GANs\noutperform baselines, both when evaluated using the commonly used Inception\nScore, and when evaluated using our own alternative scoring method. In\naddition, we qualitatively demonstrate how the \\textit{unsupervised} variant of\nGM-GAN tends to map latent vectors sampled from different Gaussians in the\nlatent space to samples of different classes in the data space. We show how\nthis phenomenon can be exploited for the task of unsupervised clustering, and\nprovide quantitative evaluation showing the superiority of our method for the\nunsupervised clustering of image datasets. Finally, we demonstrate a feature\nwhich further sets our model apart from other GAN models: the option to control\nthe quality-diversity trade-off by altering, post-training, the probability\ndistribution of the latent space. This allows one to sample higher quality and\nlower diversity samples, or vice versa, according to one's needs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:32:40 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Ben-Yosef", "Matan", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1808.10369", "submitter": "Risto Kojcev", "authors": "V\\'ictor Mayoral Vilches, Alejandro Hern\\'andez Cordero, Asier Bilbao\n  Calvo, Irati Zamalloa Ugarte, Risto Kojcev", "title": "Robot_gym: accelerated robot training through simulation in the cloud\n  with ROS and Gazebo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rather than programming, training allows robots to achieve behaviors that\ngeneralize better and are capable to respond to real-world needs. However, such\ntraining requires a big amount of experimentation which is not always feasible\nfor a physical robot. In this work, we present robot_gym, a framework to\naccelerate robot training through simulation in the cloud that makes use of\nroboticists' tools, simplifying the development and deployment processes on\nreal robots. We unveil that, for simple tasks, simple 3DoF robots require more\nthan 140 attempts to learn. For more complex, 6DoF robots, the number of\nattempts increases to more than 900 for the same task. We demonstrate that our\nframework, for simple tasks, accelerates the robot training time by more than\n33% while maintaining similar levels of accuracy and repeatability.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:55:42 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Vilches", "V\u00edctor Mayoral", ""], ["Cordero", "Alejandro Hern\u00e1ndez", ""], ["Calvo", "Asier Bilbao", ""], ["Ugarte", "Irati Zamalloa", ""], ["Kojcev", "Risto", ""]]}, {"id": "1808.10393", "submitter": "Ashish Mehta", "authors": "Ashish Mehta, Adithya Subramanian, Anbumani Subramanian", "title": "Learning End-to-end Autonomous Driving using Guided Auxiliary\n  Supervision", "comments": "12 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Learning to drive faithfully in highly stochastic urban settings remains an\nopen problem. To that end, we propose a Multi-task Learning from Demonstration\n(MT-LfD) framework which uses supervised auxiliary task prediction to guide the\nmain task of predicting the driving commands. Our framework involves an\nend-to-end trainable network for imitating the expert demonstrator's driving\ncommands. The network intermediately predicts visual affordances and action\nprimitives through direct supervision which provide the aforementioned\nauxiliary supervised guidance. We demonstrate that such joint learning and\nsupervised guidance facilitates hierarchical task decomposition, assisting the\nagent to learn faster, achieve better driving performance and increases\ntransparency of the otherwise black-box end-to-end network. We run our\nexperiments to validate the MT-LfD framework in CARLA, an open-source urban\ndriving simulator. We introduce multiple non-player agents in CARLA and induce\ntemporal noise in them for realistic stochasticity.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 16:46:22 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Mehta", "Ashish", ""], ["Subramanian", "Adithya", ""], ["Subramanian", "Anbumani", ""]]}, {"id": "1808.10396", "submitter": "Yan Yan", "authors": "Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, Yi Yang", "title": "A Unified Analysis of Stochastic Momentum Methods for Deep Learning", "comments": "Previous Technical Report: arXiv:1604.03257", "journal-ref": "In IJCAI, pp. 2955-2961. 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic momentum methods have been widely adopted in training deep neural\nnetworks. However, their theoretical analysis of convergence of the training\nobjective and the generalization error for prediction is still under-explored.\nThis paper aims to bridge the gap between practice and theory by analyzing the\nstochastic gradient (SG) method, and the stochastic momentum methods including\ntwo famous variants, i.e., the stochastic heavy-ball (SHB) method and the\nstochastic variant of Nesterov's accelerated gradient (SNAG) method. We propose\na framework that unifies the three variants. We then derive the convergence\nrates of the norm of gradient for the non-convex optimization problem, and\nanalyze the generalization performance through the uniform stability approach.\nParticularly, the convergence analysis of the training objective exhibits that\nSHB and SNAG have no advantage over SG. However, the stability analysis shows\nthat the momentum term can improve the stability of the learned model and hence\nimprove the generalization performance. These theoretical insights verify the\ncommon wisdom and are also corroborated by our empirical analysis on deep\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 17:00:03 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Yan", "Yan", ""], ["Yang", "Tianbao", ""], ["Li", "Zhe", ""], ["Lin", "Qihang", ""], ["Yang", "Yi", ""]]}, {"id": "1808.10406", "submitter": "Adriano Rivolli", "authors": "Adriano Rivolli, Lu\\'is P. F. Garcia, Carlos Soares, Joaquin\n  Vanschoren, Andr\\'e C. P. L. F. de Carvalho", "title": "Characterizing classification datasets: a study of meta-features for\n  meta-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning is increasingly used to support the recommendation of machine\nlearning algorithms and their configurations. Such recommendations are made\nbased on meta-data, consisting of performance evaluations of algorithms on\nprior datasets, as well as characterizations of these datasets. These\ncharacterizations, also called meta-features, describe properties of the data\nwhich are predictive for the performance of machine learning algorithms trained\non them. Unfortunately, despite being used in a large number of studies,\nmeta-features are not uniformly described, organized and computed, making many\nempirical studies irreproducible and hard to compare. This paper aims to deal\nwith this by systematizing and standardizing data characterization measures for\nclassification datasets used in meta-learning. Moreover, it presents MFE, a new\ntool for extracting meta-features from datasets and identifying more subtle\nreproducibility issues in the literature, proposing guidelines for data\ncharacterization that strengthen reproducible empirical research in\nmeta-learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 17:25:48 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 17:09:25 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Rivolli", "Adriano", ""], ["Garcia", "Lu\u00eds P. F.", ""], ["Soares", "Carlos", ""], ["Vanschoren", "Joaquin", ""], ["de Carvalho", "Andr\u00e9 C. P. L. F.", ""]]}, {"id": "1808.10430", "submitter": "Alexander Stec", "authors": "Alexander Stec and Diego Klabjan and Jean Utke", "title": "Nested multi-instance classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are classification tasks that take as inputs groups of images rather\nthan single images. In order to address such situations, we introduce a nested\nmulti-instance deep network. The approach is generic in that it is applicable\nto general data instances, not just images. The network has several\nconvolutional neural networks grouped together at different stages. This\nprimarily differs from other previous works in that we organize instances into\nrelevant groups that are treated differently. We also introduce a method to\nreplace instances that are missing which successfully creates neutral input\ninstances and consistently outperforms standard fill-in methods in real world\nuse cases. In addition, we propose a method for manual dropout when a whole\ngroup of instances is missing that allows us to use richer training data and\nobtain higher accuracy at the end of training. With specific pretraining, we\nfind that the model works to great effect on our real world and pub-lic\ndatasets in comparison to baseline methods, justifying the different treatment\namong groups of instances.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 17:48:23 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Stec", "Alexander", ""], ["Klabjan", "Diego", ""], ["Utke", "Jean", ""]]}, {"id": "1808.10442", "submitter": "Henry Charlesworth", "authors": "Henry Charlesworth", "title": "Application of Self-Play Reinforcement Learning to a Four-Player Game of\n  Imperfect Information", "comments": "6 pages + 7 pages SI, 5 figures in total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new virtual environment for simulating a card game known as\n\"Big 2\". This is a four-player game of imperfect information with a relatively\ncomplicated action space (being allowed to play 1,2,3,4 or 5 card combinations\nfrom an initial starting hand of 13 cards). As such it poses a challenge for\nmany current reinforcement learning methods. We then use the recently proposed\n\"Proximal Policy Optimization\" algorithm to train a deep neural network to play\nthe game, purely learning via self-play, and find that it is able to reach a\nlevel which outperforms amateur human players after only a relatively short\namount of training time and without needing to search a tree of future game\nstates.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 11:26:59 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Charlesworth", "Henry", ""]]}, {"id": "1808.10502", "submitter": "Saeid Tizpaz-Niari", "authors": "Saeid Tizpaz-Niari, Pavol Cerny, Ashutosh Trivedi", "title": "Data-Driven Debugging for Functional Side Channels", "comments": "To Appear in NDSS'20 (17 pages, 11 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information leaks through side channels are a pervasive problem, even in\nsecurity-critical applications. Functional side channels arise when an attacker\nknows that a secret value of a server stays fixed for a certain time. Then, the\nattacker can observe the server executions on a sequence of different public\ninputs, each paired with the same secret input. Thus for each secret, the\nattacker observes a function from public inputs to execution time, for\ninstance, and she can compare these functions for different secrets. First, we\nintroduce a notion of noninterference for functional side channels. We focus on\nthe case of noisy observations, where we demonstrate with examples that there\nis a practical functional side channel in programs that would be deemed\ninformation-leak-free or be underestimated using the standard definition.\nSecond, we develop a framework and techniques for debugging programs for\nfunctional side channels. We extend evolutionary fuzzing techniques to generate\ninputs that exploit functional dependencies of response times on public inputs.\nWe adapt existing results and algorithms in functional data analysis to model\nthe functions and discover the existence of side channels. We use a functional\nextension of standard decision tree learning to pinpoint the code fragments\ncausing a side channel if there is one. We empirically evaluate the performance\nof our tool FUCHSIA on a series of micro-benchmarks and realistic Java\nprograms. On the set of benchmarks, we show that FUCHSIA outperforms the\nstate-of-the-art techniques in detecting side channel classes. On the realistic\nprograms, we show the scalability of FUCHSIA in analyzing functional side\nchannels in Java programs with thousands of methods. Also, we show the\nusefulness of FUCHSIA in finding side channels including a zero-day\nvulnerability in OpenJDK and another vulnerability in Jetty that was since\nfixed by the developers.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 20:17:21 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 04:45:15 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Tizpaz-Niari", "Saeid", ""], ["Cerny", "Pavol", ""], ["Trivedi", "Ashutosh", ""]]}, {"id": "1808.10511", "submitter": "MD Zadid Khan", "authors": "MD Zadid Khan, Sakib Mahmud Khan, Mashrur Chowdhury, Kakan Dey", "title": "Development and Evaluation of Recurrent Neural Network based Models for\n  Hourly Traffic Volume and AADT Prediction", "comments": "26 pages, 17 figures, 8275 words, Submitted to Transportation\n  Research Record (TRR) Journal, Currently under review", "journal-ref": "Transportation Research Record, Vol 2673, Issue 7, 2019", "doi": "10.1177/0361198119849059", "report-no": "0361198119849059", "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of high-resolution hourly traffic volumes of a given roadway\nis essential for transportation planning. Traditionally, Automatic Traffic\nRecorders (ATR) are used to collect this hourly volume data. These large\ndatasets are time series data characterized by long-term temporal dependencies\nand missing values. Regarding the temporal dependencies, all roadways are\ncharacterized by seasonal variations that can be weekly, monthly or yearly,\ndepending on the cause of the variation. Regarding the missing data in a\ntime-series sequence, traditional time series forecasting models perform poorly\nunder the influence of seasonal variations. To address this limitation, robust,\nRecurrent Neural Network (RNN) based, multi-step ahead forecasting models are\ndeveloped for time-series in this study. The simple RNN, the Gated Recurrent\nUnit (GRU) and the Long Short-Term Memory (LSTM) units are used to develop the\nmodel and evaluate its performance. Two approaches are used to address the\nmissing value issue: masking and imputation, in conjunction with the RNN\nmodels. Six different imputation algorithms are then used to identify the best\nmodel. The analysis indicates that the LSTM model performs better than simple\nRNN and GRU models, and imputation performs better than masking to predict\nfuture traffic volume. Based on analysis using 92 ATRs, the LSTM-Median model\nis deemed the best model in all scenarios for hourly traffic volume and AADT\nprediction, with an average RMSE of 274 and MAPE of 18.91% for hourly traffic\nvolume prediction and average RMSE of 824 and MAPE of 2.10% for AADT\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 16:07:33 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 18:12:57 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 04:36:04 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Khan", "MD Zadid", ""], ["Khan", "Sakib Mahmud", ""], ["Chowdhury", "Mashrur", ""], ["Dey", "Kakan", ""]]}, {"id": "1808.10532", "submitter": "Jannis K\\\"uck", "authors": "Sven Klaassen, Jannis K\\\"uck, Martin Spindler, Victor Chernozhukov", "title": "Uniform Inference in High-Dimensional Gaussian Graphical Models", "comments": "59 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models have become a very popular tool for representing\ndependencies within a large set of variables and are key for representing\ncausal structures. We provide results for uniform inference on high-dimensional\ngraphical models with the number of target parameters $d$ being possible much\nlarger than sample size. This is in particular important when certain features\nor structures of a causal model should be recovered. Our results highlight how\nin high-dimensional settings graphical models can be estimated and recovered\nwith modern machine learning methods in complex data sets. To construct\nsimultaneous confidence regions on many target parameters, sufficiently fast\nestimation rates of the nuisance functions are crucial. In this context, we\nestablish uniform estimation rates and sparsity guarantees of the square-root\nestimator in a random design under approximate sparsity conditions that might\nbe of independent interest for related problems in high-dimensions. We also\ndemonstrate in a comprehensive simulation study that our procedure has good\nsmall sample properties.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 21:53:06 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 14:40:37 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Klaassen", "Sven", ""], ["K\u00fcck", "Jannis", ""], ["Spindler", "Martin", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "1808.10543", "submitter": "Martin Spindler", "authors": "Leander L\\\"ow, Martin Spindler, Eike Brechmann", "title": "A Self-Attention Network for Hierarchical Data Structures with an\n  Application to Claims Management", "comments": "7 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insurance companies must manage millions of claims per year. While most of\nthese claims are non-fraudulent, fraud detection is core for insurance\ncompanies. The ultimate goal is a predictive model to single out the fraudulent\nclaims and pay out the non-fraudulent ones immediately. Modern machine learning\nmethods are well suited for this kind of problem. Health care claims often have\na data structure that is hierarchical and of variable length. We propose one\nmodel based on piecewise feed forward neural networks (deep learning) and\nanother model based on self-attention neural networks for the task of claim\nmanagement. We show that the proposed methods outperform bag-of-words based\nmodels, hand designed features, and models based on convolutional neural\nnetworks, on a data set of two million health care claims. The proposed\nself-attention method performs the best.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 22:56:46 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["L\u00f6w", "Leander", ""], ["Spindler", "Martin", ""], ["Brechmann", "Eike", ""]]}, {"id": "1808.10549", "submitter": "Shahin Jabbari", "authors": "Hadi Elzayn, Shahin Jabbari, Christopher Jung, Michael Kearns, Seth\n  Neel, Aaron Roth, Zachary Schutzman", "title": "Fair Algorithms for Learning in Allocation Problems", "comments": "The short version of this paper appears in the proceedings of ACM\n  FAT*-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Settings such as lending and policing can be modeled by a centralized agent\nallocating a resource (loans or police officers) amongst several groups, in\norder to maximize some objective (loans given that are repaid or criminals that\nare apprehended). Often in such problems fairness is also a concern. A natural\nnotion of fairness, based on general principles of equality of opportunity,\nasks that conditional on an individual being a candidate for the resource, the\nprobability of actually receiving it is approximately independent of the\nindividual's group. In lending this means that equally creditworthy individuals\nin different racial groups have roughly equal chances of receiving a loan. In\npolicing it means that two individuals committing the same crime in different\ndistricts would have roughly equal chances of being arrested.\n  We formalize this fairness notion for allocation problems and investigate its\nalgorithmic consequences. Our main technical results include an efficient\nlearning algorithm that converges to an optimal fair allocation even when the\nfrequency of candidates (creditworthy individuals or criminals) in each group\nis unknown. The algorithm operates in a censored feedback model in which only\nthe number of candidates who received the resource in a given allocation can be\nobserved, rather than the true number of candidates. This models the fact that\nwe do not learn the creditworthiness of individuals we do not give loans to nor\nlearn about crimes committed if the police presence in a district is low.\n  As an application of our framework, we consider the predictive policing\nproblem. The learning algorithm is trained on arrest data gathered from its own\ndeployments on previous days, resulting in a potential feedback loop that our\nalgorithm provably overcomes. We empirically investigate the performance of our\nalgorithm on the Philadelphia Crime Incidents dataset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 23:48:33 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 20:49:07 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Elzayn", "Hadi", ""], ["Jabbari", "Shahin", ""], ["Jung", "Christopher", ""], ["Kearns", "Michael", ""], ["Neel", "Seth", ""], ["Roth", "Aaron", ""], ["Schutzman", "Zachary", ""]]}, {"id": "1808.10551", "submitter": "Keisuke Fujii", "authors": "Keisuke Fujii, Yoshinobu Kawahara", "title": "Dynamic mode decomposition in vector-valued reproducing kernel Hilbert\n  spaces for extracting dynamical structure among observables", "comments": "34 pages with 4 figures, Published in Neural Networks, 2019", "journal-ref": null, "doi": "10.1016/j.neunet.2019.04.020", "report-no": null, "categories": "stat.ML cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding nonlinear dynamical systems (NLDSs) is challenging in a variety\nof engineering and scientific fields. Dynamic mode decomposition (DMD), which\nis a numerical algorithm for the spectral analysis of Koopman operators, has\nbeen attracting attention as a way of obtaining global modal descriptions of\nNLDSs without requiring explicit prior knowledge. However, since existing DMD\nalgorithms are in principle formulated based on the concatenation of scalar\nobservables, it is not directly applicable to data with dependent structures\namong observables, which take, for example, the form of a sequence of graphs.\nIn this paper, we formulate Koopman spectral analysis for NLDSs with structures\namong observables and propose an estimation algorithm for this problem. This\nmethod can extract and visualize the underlying low-dimensional global dynamics\nof NLDSs with structures among observables from data, which can be useful in\nunderstanding the underlying dynamics of such NLDSs. To this end, we first\nformulate the problem of estimating spectra of the Koopman operator defined in\nvector-valued reproducing kernel Hilbert spaces, and then develop an estimation\nprocedure for this problem by reformulating tensor-based DMD. As a special case\nof our method, we propose the method named as Graph DMD, which is a numerical\nalgorithm for Koopman spectral analysis of graph dynamical systems, using a\nsequence of adjacency matrices. We investigate the empirical performance of our\nmethod by using synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 23:57:19 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 03:05:12 GMT"}, {"version": "v3", "created": "Sat, 26 Jan 2019 11:43:08 GMT"}, {"version": "v4", "created": "Fri, 17 May 2019 02:47:56 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Fujii", "Keisuke", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1808.10552", "submitter": "Min-Hwan Oh", "authors": "Min-hwan Oh, Garud Iyengar", "title": "Directed Exploration in PAC Model-Free Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study an exploration method for model-free RL that generalizes the\ncounter-based exploration bonus methods and takes into account long term\nexploratory value of actions rather than a single step look-ahead. We propose a\nmodel-free RL method that modifies Delayed Q-learning and utilizes the\nlong-term exploration bonus with provable efficiency. We show that our proposed\nmethod finds a near-optimal policy in polynomial time (PAC-MDP), and also\nprovide experimental evidence that our proposed algorithm is an efficient\nexploration method.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 00:00:22 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Oh", "Min-hwan", ""], ["Iyengar", "Garud", ""]]}, {"id": "1808.10556", "submitter": "Alan Preciado-Grijalva Mr.", "authors": "Alan Preciado-Grijalva and Ramon F. Brena", "title": "Speaker Fluency Level Classification Using Machine Learning Techniques", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Level assessment for foreign language students is necessary for putting them\nin the right level group, furthermore, interviewing students is a very\ntime-consuming task, so we propose to automate the evaluation of speaker\nfluency level by implementing machine learning techniques. This work presents\nan audio processing system capable of classifying the level of fluency of\nnon-native English speakers using five different machine learning models. As a\nfirst step, we have built our own dataset, which consists of labeled audio\nconversations in English between people ranging in different fluency\ndomains/classes (low, intermediate, high). We segment the audio conversations\ninto 5s non-overlapped audio clips to perform feature extraction on them. We\nstart by extracting Mel cepstral coefficients from the audios, selecting 20\ncoefficients is an appropriate quantity for our data. We thereafter extracted\nzero-crossing rate, root mean square energy and spectral flux features, proving\nthat this improves model performance. Out of a total of 1424 audio segments,\nwith 70% training data and 30% test data, one of our trained models (support\nvector machine) achieved a classification accuracy of 94.39%, whereas the other\nfour models passed an 89% classification accuracy threshold.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 00:15:53 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Preciado-Grijalva", "Alan", ""], ["Brena", "Ramon F.", ""]]}, {"id": "1808.10568", "submitter": "Xi Victoria Lin", "authors": "Xi Victoria Lin and Richard Socher and Caiming Xiong", "title": "Multi-Hop Knowledge Graph Reasoning with Reward Shaping", "comments": "Accepted to EMNLP 2018, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop reasoning is an effective approach for query answering (QA) over\nincomplete knowledge graphs (KGs). The problem can be formulated in a\nreinforcement learning (RL) setup, where a policy-based agent sequentially\nextends its inference path until it reaches a target. However, in an incomplete\nKG environment, the agent receives low-quality rewards corrupted by false\nnegatives in the training data, which harms generalization at test time.\nFurthermore, since no golden action sequence is used for training, the agent\ncan be misled by spurious search trajectories that incidentally lead to the\ncorrect answer. We propose two modeling advances to address both issues: (1) we\nreduce the impact of false negative supervision by adopting a pretrained\none-hop embedding model to estimate the reward of unobserved facts; (2) we\ncounter the sensitivity to spurious paths of on-policy RL by forcing the agent\nto explore a diverse set of paths using randomly generated edge masks. Our\napproach significantly improves over existing path-based KGQA models on several\nbenchmark datasets and is comparable or better than embedding-based models.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 01:55:09 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 22:00:49 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Lin", "Xi Victoria", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "1808.10585", "submitter": "Nan Lu", "authors": "Nan Lu, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama", "title": "On the Minimal Supervision for Training Any Binary Classifier from Only\n  Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization (ERM), with proper loss function and\nregularization, is the common practice of supervised classification. In this\npaper, we study training arbitrary (from linear to deep) binary classifier from\nonly unlabeled (U) data by ERM. We prove that it is impossible to estimate the\nrisk of an arbitrary binary classifier in an unbiased manner given a single set\nof U data, but it becomes possible given two sets of U data with different\nclass priors. These two facts answer a fundamental question---what the minimal\nsupervision is for training any binary classifier from only U data. Following\nthese findings, we propose an ERM-based learning method from two sets of U\ndata, and then prove it is consistent. Experiments demonstrate the proposed\nmethod could train deep models and outperform state-of-the-art methods for\nlearning from two sets of U data.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 03:18:00 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 07:39:07 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 06:52:03 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2019 13:21:24 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Lu", "Nan", ""], ["Niu", "Gang", ""], ["Menon", "Aditya Krishna", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1808.10594", "submitter": "Benjamin Lucas", "authors": "Benjamin Lucas, Ahmed Shifaz, Charlotte Pelletier, Lachlan O'Neill,\n  Nayyar Zaidi, Bart Goethals, Francois Petitjean, and Geoffrey I. Webb", "title": "Proximity Forest: An effective and scalable distance-based classifier\n  for time series", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": "10.1007/s10618-019-00617-3", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into the classification of time series has made enormous progress in\nthe last decade. The UCR time series archive has played a significant role in\nchallenging and guiding the development of new learners for time series\nclassification. The largest dataset in the UCR archive holds 10 thousand time\nseries only; which may explain why the primary research focus has been in\ncreating algorithms that have high accuracy on relatively small datasets.\n  This paper introduces Proximity Forest, an algorithm that learns accurate\nmodels from datasets with millions of time series, and classifies a time series\nin milliseconds. The models are ensembles of highly randomized Proximity Trees.\nWhereas conventional decision trees branch on attribute values (and usually\nperform poorly on time series), Proximity Trees branch on the proximity of time\nseries to one exemplar time series or another; allowing us to leverage the\ndecades of work into developing relevant measures for time series. Proximity\nForest gains both efficiency and accuracy by stochastic selection of both\nexemplars and similarity measures.\n  Our work is motivated by recent time series applications that provide orders\nof magnitude more time series than the UCR benchmarks. Our experiments\ndemonstrate that Proximity Forest is highly competitive on the UCR archive: it\nranks among the most accurate classifiers while being significantly faster. We\ndemonstrate on a 1M time series Earth observation dataset that Proximity Forest\nretains this accuracy on datasets that are many orders of magnitude greater\nthan those in the UCR repository, while learning its models at least 100,000\ntimes faster than current state of the art models Elastic Ensemble and COTE.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 04:17:44 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 03:04:59 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Lucas", "Benjamin", ""], ["Shifaz", "Ahmed", ""], ["Pelletier", "Charlotte", ""], ["O'Neill", "Lachlan", ""], ["Zaidi", "Nayyar", ""], ["Goethals", "Bart", ""], ["Petitjean", "Francois", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1808.10632", "submitter": "Mansoor Rezghi", "authors": "Soheil Ahmadi and Mansoor Rezghi", "title": "A novel extension of Generalized Low-Rank Approximation of Matrices\n  based on multiple-pairs of transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is a main step in the learning process which plays\nan essential role in many applications. The most popular methods in this field\nlike SVD, PCA, and LDA, only can be applied to data with vector format. This\nmeans that for higher order data like matrices or more generally tensors, data\nshould be fold to the vector format. So, in this approach, the spatial\nrelations of features are not considered and also the probability of\nover-fitting is increased. Due to these issues, in recent years some methods\nlike Generalized low-rank approximation of matrices (GLRAM) and Multilinear PCA\n(MPCA) are proposed which deal with the data in their own format. So, in these\nmethods, the spatial relationships of features are preserved and the\nprobability of overfitting could be fallen. Also, their time and space\ncomplexities are less than vector-based ones. However, because of the fewer\nparameters, the search space in a multilinear approach is much smaller than the\nsearch space of the vector-based approach. To overcome this drawback of\nmultilinear methods like GLRAM, we proposed a new method which is a general\nform of GLRAM and by preserving the merits of it have a larger search space.\nExperimental results confirm the quality of the proposed method. Also, applying\nthis approach to the other multilinear dimensionality reduction methods like\nMPCA and MLDA is straightforward.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 08:34:14 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 17:27:11 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 23:52:22 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Ahmadi", "Soheil", ""], ["Rezghi", "Mansoor", ""]]}, {"id": "1808.10648", "submitter": "Sebastian Gomez-Gonzalez", "authors": "Sebastian Gomez-Gonzalez, Gerhard Neumann, Bernhard Sch\\\"olkopf, Jan\n  Peters", "title": "Adaptation and Robust Learning of Probabilistic Movement Primitives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic representations of movement primitives open important new\npossibilities for machine learning in robotics. These representations are able\nto capture the variability of the demonstrations from a teacher as a\nprobability distribution over trajectories, providing a sensible region of\nexploration and the ability to adapt to changes in the robot environment.\nHowever, to be able to capture variability and correlations between different\njoints, a probabilistic movement primitive requires the estimation of a larger\nnumber of parameters compared to their deterministic counterparts, that focus\non modeling only the mean behavior. In this paper, we make use of prior\ndistributions over the parameters of a probabilistic movement primitive to make\nrobust estimates of the parameters with few training instances. In addition, we\nintroduce general purpose operators to adapt movement primitives in joint and\ntask space. The proposed training method and adaptation operators are tested in\na coffee preparation and in robot table tennis task. In the coffee preparation\ntask we evaluate the generalization performance to changes in the location of\nthe coffee grinder and brewing chamber in a target area, achieving the desired\nbehavior after only two demonstrations. In the table tennis task we evaluate\nthe hit and return rates, outperforming previous approaches while using fewer\ntask specific heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 09:46:07 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 08:59:37 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Gomez-Gonzalez", "Sebastian", ""], ["Neumann", "Gerhard", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Peters", "Jan", ""]]}, {"id": "1808.10654", "submitter": "Amir Zamir", "authors": "Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik,\n  Silvio Savarese", "title": "Gibson Env: Real-World Perception for Embodied Agents", "comments": "Access the code, dataset, and project website at\n  http://gibsonenv.vision/ . CVPR 2018", "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing visual perception models for active agents and sensorimotor\ncontrol are cumbersome to be done in the physical world, as existing algorithms\nare too slow to efficiently learn in real-time and robots are fragile and\ncostly. This has given rise to learning-in-simulation which consequently casts\na question on whether the results transfer to real-world. In this paper, we are\nconcerned with the problem of developing real-world perception for active\nagents, propose Gibson Virtual Environment for this purpose, and showcase\nsample perceptual tasks learned therein. Gibson is based on virtualizing real\nspaces, rather than using artificially designed ones, and currently includes\nover 1400 floor spaces from 572 full buildings. The main characteristics of\nGibson are: I. being from the real-world and reflecting its semantic\ncomplexity, II. having an internal synthesis mechanism, \"Goggles\", enabling\ndeploying the trained models in real-world without needing further domain\nadaptation, III. embodiment of agents and making them subject to constraints of\nphysics and space.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 09:56:43 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Xia", "Fei", ""], ["Zamir", "Amir", ""], ["He", "Zhi-Yang", ""], ["Sax", "Alexander", ""], ["Malik", "Jitendra", ""], ["Savarese", "Silvio", ""]]}, {"id": "1808.10663", "submitter": "Satoshi Endo", "authors": "Muriel Lang, Franz M.J. Pfister, Jakob Fr\\\"ohner, Kian Abedinpour,\n  Daniel Pichler, Urban Fietzek, Terry T. Um, Dana Kuli\\'c, Satoshi Endo,\n  Sandra Hirche", "title": "A Multi-layer Gaussian Process for Motor Symptom Estimation in People\n  with Parkinson's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of Parkinson's disease (PD) poses a significant challenge as\nit is influenced by various factors which lead to a complex and fluctuating\nsymptom manifestation. Thus, a frequent and objective PD assessment is highly\nvaluable for effective health management of people with Parkinson's disease\n(PwP). Here, we propose a method for monitoring PwP by stochastically modeling\nthe relationships between their wrist movements during unscripted daily\nactivities and corresponding annotations about clinical displays of movement\nabnormalities. We approach the estimation of PD motor signs by independently\nmodeling and hierarchically stacking Gaussian process models for three classes\nof commonly observed movement abnormalities in PwP including tremor,\n(non-tremulous) bradykinesia, and (non-tremulous) dyskinesia. We use clinically\nadopted severity measures as annotations for training the models, thus allowing\nour multi-layer Gaussian process prediction models to estimate not only their\npresence but also their severities. The experimental validation of our approach\ndemonstrates strong agreement of the model predictions with these PD\nannotations. Our results show the proposed method produces promising results in\nobjective monitoring of movement abnormalities of PD in the presence of\narbitrary and unknown voluntary motions, and makes an important step towards\ncontinuous monitoring of PD in the home environment.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 10:20:43 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 09:01:37 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Lang", "Muriel", ""], ["Pfister", "Franz M. J.", ""], ["Fr\u00f6hner", "Jakob", ""], ["Abedinpour", "Kian", ""], ["Pichler", "Daniel", ""], ["Fietzek", "Urban", ""], ["Um", "Terry T.", ""], ["Kuli\u0107", "Dana", ""], ["Endo", "Satoshi", ""], ["Hirche", "Sandra", ""]]}, {"id": "1808.10664", "submitter": "Maurizio Ferrari Dacrema", "authors": "Cesare Bernardis, Maurizio Ferrari Dacrema and Paolo Cremonesi", "title": "A novel graph-based model for hybrid recommendations in cold-start\n  scenarios", "comments": null, "journal-ref": "Proceedings of the Late-Breaking Results track part of the Twelfth\n  ACM Conference on Recommender Systems (RecSys 2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cold-start is a very common and still open problem in the Recommender Systems\nliterature. Since cold start items do not have any interaction, collaborative\nalgorithms are not applicable. One of the main strategies is to use pure or\nhybrid content-based approaches, which usually yield to lower recommendation\nquality than collaborative ones. Some techniques to optimize performance of\nthis type of approaches have been studied in recent past. One of them is called\nfeature weighting, which assigns to every feature a real value, called weight,\nthat estimates its importance. Statistical techniques for feature weighting\ncommonly used in Information Retrieval, like TF-IDF, have been adapted for\nRecommender Systems, but they often do not provide sufficient quality\nimprovements. More recent approaches, FBSM and LFW, estimate weights by\nleveraging collaborative information via machine learning, in order to learn\nthe importance of a feature based on other users opinions. This type of models\nhave shown promising results compared to classic statistical analyzes cited\npreviously. We propose a novel graph, feature-based machine learning model to\nface the cold-start item scenario, learning the relevance of features from\nprobabilities of item-based collaborative filtering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 10:22:32 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bernardis", "Cesare", ""], ["Dacrema", "Maurizio Ferrari", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1808.10678", "submitter": "Santiago Pascual de la Puente", "authors": "Santiago Pascual, Antonio Bonafonte, Joan Serr\\`a", "title": "Self-Attention Linguistic-Acoustic Decoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conversion from text to speech relies on the accurate mapping from\nlinguistic to acoustic symbol sequences, for which current practice employs\nrecurrent statistical models like recurrent neural networks. Despite the good\nperformance of such models (in terms of low distortion in the generated\nspeech), their recursive structure tends to make them slow to train and to\nsample from. In this work, we try to overcome the limitations of recursive\nstructure by using a module based on the transformer decoder network, designed\nwithout recurrent connections but emulating them with attention and positioning\ncodes. Our results show that the proposed decoder network is competitive in\nterms of distortion when compared to a recurrent baseline, whilst being\nsignificantly faster in terms of CPU inference time. On average, it increases\nMel cepstral distortion between 0.1 and 0.3 dB, but it is over an order of\nmagnitude faster on average. Fast inference is important for the deployment of\nspeech synthesis systems on devices with restricted resources, like mobile\nphones or embedded systems, where speaking virtual assistants are gaining\nimportance.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 11:08:41 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 16:43:15 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Pascual", "Santiago", ""], ["Bonafonte", "Antonio", ""], ["Serr\u00e0", "Joan", ""]]}, {"id": "1808.10687", "submitter": "Santiago Pascual de la Puente", "authors": "Santiago Pascual, Antonio Bonafonte, Joan Serr\\`a, Jose A. Gonzalez", "title": "Whispered-to-voiced Alaryngeal Speech Conversion with Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods of voice restoration for patients suffering from aphonia either\nproduce whispered or monotone speech. Apart from intelligibility, this type of\nspeech lacks expressiveness and naturalness due to the absence of pitch\n(whispered speech) or artificial generation of it (monotone speech). Existing\ntechniques to restore prosodic information typically combine a vocoder, which\nparameterises the speech signal, with machine learning techniques that predict\nprosodic information. In contrast, this paper describes an end-to-end neural\napproach for estimating a fully-voiced speech waveform from whispered\nalaryngeal speech. By adapting our previous work in speech enhancement with\ngenerative adversarial networks, we develop a speaker-dependent model to\nperform whispered-to-voiced speech conversion. Preliminary qualitative results\nshow effectiveness in re-generating voiced speech, with the creation of\nrealistic pitch contours.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 11:23:56 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 16:34:49 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Pascual", "Santiago", ""], ["Bonafonte", "Antonio", ""], ["Serr\u00e0", "Joan", ""], ["Gonzalez", "Jose A.", ""]]}, {"id": "1808.10692", "submitter": "Aqeel Labash", "authors": "Aqeel Labash, Ardi Tampuu, Tambet Matiisen, Jaan Aru, Raul Vicente", "title": "APES: a Python toolbox for simulating reinforcement learning\n  environments", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assisted by neural networks, reinforcement learning agents have been able to\nsolve increasingly complex tasks over the last years. The simulation\nenvironment in which the agents interact is an essential component in any\nreinforcement learning problem. The environment simulates the dynamics of the\nagents' world and hence provides feedback to their actions in terms of state\nobservations and external rewards. To ease the design and simulation of such\nenvironments this work introduces $\\texttt{APES}$, a highly customizable and\nopen source package in Python to create 2D grid-world environments for\nreinforcement learning problems. $\\texttt{APES}$ equips agents with algorithms\nto simulate any field of vision, it allows the creation and positioning of\nitems and rewards according to user-defined rules, and supports the interaction\nof multiple agents.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 11:47:10 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Labash", "Aqeel", ""], ["Tampuu", "Ardi", ""], ["Matiisen", "Tambet", ""], ["Aru", "Jaan", ""], ["Vicente", "Raul", ""]]}, {"id": "1808.10696", "submitter": "Diane Bouchacourt", "authors": "Diane Bouchacourt and Marco Baroni", "title": "How agents see things: On visual representations in an emergent language\n  game", "comments": "2018 Conference on Empirical Methods in Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in the language developed by agents interacting in\nemergent-communication settings. Earlier studies have focused on the agents'\nsymbol usage, rather than on their representation of visual input. In this\npaper, we consider the referential games of Lazaridou et al. (2017) and\ninvestigate the representations the agents develop during their evolving\ninteraction. We find that the agents establish successful communication by\ninducing visual representations that almost perfectly align with each other,\nbut, surprisingly, do not capture the conceptual properties of the objects\ndepicted in the input images. We conclude that, if we are interested in\ndeveloping language-like communication systems, we must pay more attention to\nthe visual semantics agents associate to the symbols they use.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 11:56:10 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 14:11:46 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Bouchacourt", "Diane", ""], ["Baroni", "Marco", ""]]}, {"id": "1808.10705", "submitter": "Jonathan P. Epperlein PhD", "authors": "Jonathan P. Epperlein and Julien Monteil and Mingming Liu and Yingqi\n  Gu and Sergiy Zhuk and Robert Shorten", "title": "Bayesian Classifier for Route Prediction with Markov Chains", "comments": "Accepted at The 21st IEEE International Conference on Intelligent\n  Transportation Systems (ITSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here a general framework and a specific algorithm for predicting\nthe destination, route, or more generally a pattern, of an ongoing journey,\nbuilding on the recent work of [Y. Lassoued, J. Monteil, Y. Gu, G. Russo, R.\nShorten, and M. Mevissen, \"Hidden Markov model for route and destination\nprediction,\" in IEEE International Conference on Intelligent Transportation\nSystems, 2017]. In the presented framework, known journey patterns are modelled\nas stochastic processes, emitting the road segments visited during the journey,\nand the ongoing journey is predicted by updating the posterior probability of\neach journey pattern given the road segments visited so far. In this\ncontribution, we use Markov chains as models for the journey patterns, and\nconsider the prediction as final, once one of the posterior probabilities\ncrosses a predefined threshold. Despite the simplicity of both, examples run on\na synthetic dataset demonstrate high accuracy of the made predictions.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 12:24:01 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Epperlein", "Jonathan P.", ""], ["Monteil", "Julien", ""], ["Liu", "Mingming", ""], ["Gu", "Yingqi", ""], ["Zhuk", "Sergiy", ""], ["Shorten", "Robert", ""]]}, {"id": "1808.10724", "submitter": "Fanghui Liu", "authors": "Fanghui Liu, Xiaolin Huang, Chen Gong, Jie Yang, and Li Li", "title": "Learning Data-adaptive Nonparametric Kernels", "comments": "This paper was accepted by Journal of Machine Learning Research\n  (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a data-adaptive non-parametric kernel learning\nframework in margin based kernel methods. In model formulation, given an\ninitial kernel matrix, a data-adaptive matrix with two constraints is imposed\nin an entry-wise scheme. Learning this data-adaptive matrix in a\nformulation-free strategy enlarges the margin between classes and thus improves\nthe model flexibility. The introduced two constraints are imposed either\nexactly (on small data sets) or approximately (on large data sets) in our\nmodel, which provides a controllable trade-off between model flexibility and\ncomplexity with theoretical demonstration. In algorithm optimization, the\nobjective function of our learning framework is proven to be gradient-Lipschitz\ncontinuous. Thereby, kernel and classifier/regressor learning can be\nefficiently optimized in a unified framework via Nesterov's acceleration. For\nthe scalability issue, we study a decomposition-based approach to our model in\nthe large sample case. The effectiveness of this approximation is illustrated\nby both empirical studies and theoretical guarantees. Experimental results on\nvarious classification and regression benchmark data sets demonstrate that our\nnon-parametric kernel learning framework achieves good performance when\ncompared with other representative kernel learning based algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 13:16:31 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 09:58:32 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 19:59:55 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Liu", "Fanghui", ""], ["Huang", "Xiaolin", ""], ["Gong", "Chen", ""], ["Yang", "Jie", ""], ["Li", "Li", ""]]}, {"id": "1808.10776", "submitter": "Jaroslaw Zola", "authors": "Frank Schoeneman, Jaroslaw Zola", "title": "Scalable Manifold Learning for Big Data with Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear spectral dimensionality reduction methods, such as Isomap, remain\nimportant technique for learning manifolds. However, due to computational\ncomplexity, exact manifold learning using Isomap is currently impossible from\nlarge-scale data. In this paper, we propose a distributed memory framework\nimplementing end-to-end exact Isomap under Apache Spark model. We show how each\ncritical step of the Isomap algorithm can be efficiently realized using basic\nSpark model, without the need to provision data in the secondary storage. We\nshow how the entire method can be implemented using PySpark, offloading compute\nintensive linear algebra routines to BLAS. Through experimental results, we\ndemonstrate excellent scalability of our method, and we show that it can\nprocess datasets orders of magnitude larger than what is currently possible,\nusing a 25-node parallel~cluster.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:32:10 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Schoeneman", "Frank", ""], ["Zola", "Jaroslaw", ""]]}, {"id": "1808.10788", "submitter": "Jens Berg", "authors": "Jens Berg and Kaj Nystr\\\"om", "title": "Data-driven discovery of PDEs in complex datasets", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.01.036", "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many processes in science and engineering can be described by partial\ndifferential equations (PDEs). Traditionally, PDEs are derived by considering\nfirst principles of physics to derive the relations between the involved\nphysical quantities of interest. A different approach is to measure the\nquantities of interest and use deep learning to reverse engineer the PDEs which\nare describing the physical process.\n  In this paper we use machine learning, and deep learning in particular, to\ndiscover PDEs hidden in complex data sets from measurement data. We include\nexamples of data from a known model problem, and real data from weather station\nmeasurements. We show how necessary transformations of the input data amounts\nto coordinate transformations in the discovered PDE, and we elaborate on\nfeature and model selection. It is shown that the dynamics of a non-linear,\nsecond order PDE can be accurately described by an ordinary differential\nequation which is automatically discovered by our deep learning algorithm. Even\nmore interestingly, we show that similar results apply in the context of more\ncomplex simulations of the Swedish temperature distribution.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:40:03 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Berg", "Jens", ""], ["Nystr\u00f6m", "Kaj", ""]]}, {"id": "1808.10792", "submitter": "Sebastian Gehrmann", "authors": "Sebastian Gehrmann, Yuntian Deng, Alexander M. Rush", "title": "Bottom-Up Abstractive Summarization", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based methods for abstractive summarization produce outputs\nthat are more fluent than other techniques, but which can be poor at content\nselection. This work proposes a simple technique for addressing this issue: use\na data-efficient content selector to over-determine phrases in a source\ndocument that should be part of the summary. We use this selector as a\nbottom-up attention step to constrain the model to likely phrases. We show that\nthis approach improves the ability to compress text, while still generating\nfluent summaries. This two-step process is both simpler and higher performing\nthan other end-to-end content selection models, leading to significant\nimprovements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the\ncontent selector can be trained with as little as 1,000 sentences, making it\neasy to transfer a trained summarizer to a new domain.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:55:52 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 02:04:07 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Gehrmann", "Sebastian", ""], ["Deng", "Yuntian", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1808.10862", "submitter": "Yuri G. Gordienko", "authors": "Nikita Gordienko, Peng Gang, Yuri Gordienko, Wei Zeng, Oleg Alienin,\n  Oleksandr Rokovyi, and Sergii Stirenko", "title": "Open Source Dataset and Machine Learning Techniques for Automatic\n  Recognition of Historical Graffiti", "comments": "11 pages, 9 figures, accepted for 25th International Conference on\n  Neural Information Processing (ICONIP 2018), 14-16 December, 2018 (Siem Reap,\n  Cambodia)", "journal-ref": "In: Cheng L., Leung A., Ozawa S. (eds) Neural Information\n  Processing. ICONIP 2018. Lecture Notes in Computer Science, vol. 11305, pp.\n  414-424. Springer, Cham", "doi": "10.1007/978-3-030-04221-9_37", "report-no": null, "categories": "cs.LG cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques are presented for automatic recognition of the\nhistorical letters (XI-XVIII centuries) carved on the stoned walls of St.Sophia\ncathedral in Kyiv (Ukraine). A new image dataset of these carved Glagolitic and\nCyrillic letters (CGCL) was assembled and pre-processed for recognition and\nprediction by machine learning methods. The dataset consists of more than 4000\nimages for 34 types of letters. The explanatory data analysis of CGCL and\nnotMNIST datasets shown that the carved letters can hardly be differentiated by\ndimensionality reduction methods, for example, by t-distributed stochastic\nneighbor embedding (tSNE) due to the worse letter representation by stone\ncarving in comparison to hand writing. The multinomial logistic regression\n(MLR) and a 2D convolutional neural network (CNN) models were applied. The MLR\nmodel demonstrated the area under curve (AUC) values for receiver operating\ncharacteristic (ROC) are not lower than 0.92 and 0.60 for notMNIST and CGCL,\nrespectively. The CNN model gave AUC values close to 0.99 for both notMNIST and\nCGCL (despite the much smaller size and quality of CGCL in comparison to\nnotMNIST) under condition of the high lossy data augmentation. CGCL dataset was\npublished to be available for the data science community as an open source\nresource.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:43:21 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Gordienko", "Nikita", ""], ["Gang", "Peng", ""], ["Gordienko", "Yuri", ""], ["Zeng", "Wei", ""], ["Alienin", "Oleg", ""], ["Rokovyi", "Oleksandr", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1808.10867", "submitter": "Homa Hosseinmardi", "authors": "Homa Hosseinmardi and Amir Ghasemian and Shrikanth Narayanan and\n  Kristina Lerman and Emilio Ferrara", "title": "Tensor Embedding: A Supervised Framework for Human Behavioral Data\n  Mining and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's densely instrumented world offers tremendous opportunities for\ncontinuous acquisition and analysis of multimodal sensor data providing\ntemporal characterization of an individual's behaviors. Is it possible to\nefficiently couple such rich sensor data with predictive modeling techniques to\nprovide contextual, and insightful assessments of individual performance and\nwellbeing? Prediction of different aspects of human behavior from these noisy,\nincomplete, and heterogeneous bio-behavioral temporal data is a challenging\nproblem, beyond unsupervised discovery of latent structures. We propose a\nSupervised Tensor Embedding (STE) algorithm for high dimension multimodal data\nwith join decomposition of input and target variable. Furthermore, we show that\nfeatures selection will help to reduce the contamination in the prediction and\nincrease the performance. The efficiently of the methods was tested via two\ndifferent real world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:51:54 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Hosseinmardi", "Homa", ""], ["Ghasemian", "Amir", ""], ["Narayanan", "Shrikanth", ""], ["Lerman", "Kristina", ""], ["Ferrara", "Emilio", ""]]}]