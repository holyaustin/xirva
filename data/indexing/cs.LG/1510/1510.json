[{"id": "1510.00012", "submitter": "Jianbo Ye", "authors": "Jianbo Ye, Panruo Wu, James Z. Wang and Jia Li", "title": "Fast Discrete Distribution Clustering Using Wasserstein Barycenter with\n  Sparse Support", "comments": "double-column, 17 pages, 3 figures, 5 tables. English usage improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of research areas, the weighted bag of vectors and the histogram\nare widely used descriptors for complex objects. Both can be expressed as\ndiscrete distributions. D2-clustering pursues the minimum total within-cluster\nvariation for a set of discrete distributions subject to the\nKantorovich-Wasserstein metric. D2-clustering has a severe scalability issue,\nthe bottleneck being the computation of a centroid distribution, called\nWasserstein barycenter, that minimizes its sum of squared distances to the\ncluster members. In this paper, we develop a modified Bregman ADMM approach for\ncomputing the approximate discrete Wasserstein barycenter of large clusters. In\nthe case when the support points of the barycenters are unknown and have low\ncardinality, our method achieves high accuracy empirically at a much reduced\ncomputational cost. The strengths and weaknesses of our method and its\nalternatives are examined through experiments, and we recommend scenarios for\ntheir respective usage. Moreover, we develop both serial and parallelized\nversions of the algorithm. By experimenting with large-scale data, we\ndemonstrate the computational efficiency of the new methods and investigate\ntheir convergence properties and numerical stability. The clustering results\nobtained on several datasets in different domains are highly competitive in\ncomparison with some widely used methods in the corresponding areas.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 20:10:59 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 22:40:26 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 23:41:22 GMT"}, {"version": "v4", "created": "Mon, 9 Jan 2017 18:14:20 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Ye", "Jianbo", ""], ["Wu", "Panruo", ""], ["Wang", "James Z.", ""], ["Li", "Jia", ""]]}, {"id": "1510.00087", "submitter": "Adrian Weller", "authors": "Adrian Weller and Justin Domke", "title": "Clamping Improves TRW and Mean Field Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the effect of clamping variables for approximate inference in\nundirected graphical models with pairwise relationships and discrete variables.\nFor any number of variable labels, we demonstrate that clamping and summing\napproximate sub-partition functions can lead only to a decrease in the\npartition function estimate for TRW, and an increase for the naive mean field\nmethod, in each case guaranteeing an improvement in the approximation and\nbound. We next focus on binary variables, add the Bethe approximation to\nconsideration and examine ways to choose good variables to clamp, introducing\nnew methods. We show the importance of identifying highly frustrated cycles,\nand of checking the singleton entropy of a variable. We explore the value of\nour methods by empirical analysis and draw lessons to guide practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 01:49:04 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Weller", "Adrian", ""], ["Domke", "Justin", ""]]}, {"id": "1510.00095", "submitter": "Wei Xie", "authors": "Wenfa Li, Hongzhe Liu, Peng Yang, Wei Xie", "title": "Supporting Regularized Logistic Regression Privately and Efficiently", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0156479", "report-no": null, "categories": "cs.LG cs.CR q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most popular statistical and machine learning models, logistic\nregression with regularization has found wide adoption in biomedicine, social\nsciences, information technology, and so on. These domains often involve data\nof human subjects that are contingent upon strict privacy regulations.\nIncreasing concerns over data privacy make it more and more difficult to\ncoordinate and conduct large-scale collaborative studies, which typically rely\non cross-institution data sharing and joint analysis. Our work here focuses on\nsafeguarding regularized logistic regression, a widely-used machine learning\nmodel in various disciplines while at the same time has not been investigated\nfrom a data security and privacy perspective. We consider a common use scenario\nof multi-institution collaborative studies, such as in the form of research\nconsortia or networks as widely seen in genetics, epidemiology, social\nsciences, etc. To make our privacy-enhancing solution practical, we demonstrate\na non-conventional and computationally efficient method leveraging distributing\ncomputing and strong cryptography to provide comprehensive protection over\nindividual-level and summary data. Extensive empirical evaluation on several\nstudies validated the privacy guarantees, efficiency and scalability of our\nproposal. We also discuss the practical implications of our solution for\nlarge-scale studies and applications from various disciplines, including\ngenetic and biomedical studies, smart grid, network analysis, etc.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 02:44:09 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Li", "Wenfa", ""], ["Liu", "Hongzhe", ""], ["Yang", "Peng", ""], ["Xie", "Wei", ""]]}, {"id": "1510.00132", "submitter": "MIkhail Hushchyn", "authors": "Mikhail Hushchyn, Philippe Charpentier, Andrey Ustyuzhanin", "title": "Disk storage management for LHCb based on Data Popularity estimator", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/664/4/042026", "report-no": null, "categories": "cs.DC cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm providing recommendations for optimizing the\nLHCb data storage. The LHCb data storage system is a hybrid system. All\ndatasets are kept as archives on magnetic tapes. The most popular datasets are\nkept on disks. The algorithm takes the dataset usage history and metadata\n(size, type, configuration etc.) to generate a recommendation report. This\narticle presents how we use machine learning algorithms to predict future data\npopularity. Using these predictions it is possible to estimate which datasets\nshould be removed from disk. We use regression algorithms and time series\nanalysis to find the optimal number of replicas for datasets that are kept on\ndisk. Based on the data popularity and the number of replicas optimization, the\nalgorithm minimizes a loss function to find the optimal data distribution. The\nloss function represents all requirements for data distribution in the data\nstorage system. We demonstrate how our algorithm helps to save disk space and\nto reduce waiting times for jobs using this data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 07:40:37 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Hushchyn", "Mikhail", ""], ["Charpentier", "Philippe", ""], ["Ustyuzhanin", "Andrey", ""]]}, {"id": "1510.00259", "submitter": "Stephanie L. Hyland", "authors": "Stephanie L. Hyland, Theofanis Karaletsos, Gunnar R\\\"atsch", "title": "A Generative Model of Words and Relationships from Multiple Sources", "comments": "8 pages, 5 figures; incorporated feedback from reviewers; to appear\n  in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models are a powerful tool to embed words into semantic\nvector spaces. However, learning such models generally relies on the\navailability of abundant and diverse training examples. In highly specialised\ndomains this requirement may not be met due to difficulties in obtaining a\nlarge corpus, or the limited range of expression in average use. Such domains\nmay encode prior knowledge about entities in a knowledge base or ontology. We\npropose a generative model which integrates evidence from diverse data sources,\nenabling the sharing of semantic information. We achieve this by generalising\nthe concept of co-occurrence from distributional semantics to include other\nrelationships between entities or words, which we model as affine\ntransformations on the embedding space. We demonstrate the effectiveness of\nthis approach by outperforming recent models on a link prediction task and\ndemonstrating its ability to profit from partially or fully unobserved data\ntraining labels. We further demonstrate the usefulness of learning from\ndifferent data sources with overlapping vocabularies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 14:42:19 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 17:08:28 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Hyland", "Stephanie L.", ""], ["Karaletsos", "Theofanis", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1510.00452", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Yoav Freund", "title": "Optimal Binary Classifier Aggregation for General Losses", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of aggregating an ensemble of predictors with known\nloss bounds in a semi-supervised binary classification setting, to minimize\nprediction loss incurred on the unlabeled data. We find the minimax optimal\npredictions for a very general class of loss functions including all convex and\nmany non-convex losses, extending a recent analysis of the problem for\nmisclassification error. The result is a family of semi-supervised ensemble\naggregation algorithms which are as efficient as linear learning by convex\noptimization, but are minimax optimal without any relaxations. Their decision\nrules take a form familiar in decision theory -- applying sigmoid functions to\na notion of ensemble margin -- without the assumptions typically made in\nmargin-based learning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 23:58:46 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 06:05:15 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2015 20:28:54 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2016 02:04:48 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 10:28:36 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Freund", "Yoav", ""]]}, {"id": "1510.00627", "submitter": "Setareh Maghsudi", "authors": "Setareh Maghsudi and Ekram Hossain", "title": "Multi-armed Bandits with Application to 5G Small Cells", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/MWC.2016.7498076", "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the pervasive demand for mobile services, next generation wireless\nnetworks are expected to be able to deliver high date rates while wireless\nresources become more and more scarce. This requires the next generation\nwireless networks to move towards new networking paradigms that are able to\nefficiently support resource-demanding applications such as personalized mobile\nservices. Examples of such paradigms foreseen for the emerging fifth generation\n(5G) cellular networks include very densely deployed small cells and\ndevice-to-device communications. For 5G networks, it will be imperative to\nsearch for spectrum and energy-efficient solutions to the resource allocation\nproblems that i) are amenable to distributed implementation, ii) are capable of\ndealing with uncertainty and lack of information, and iii) can cope with users'\nselfishness. The core objective of this article is to investigate and to\nestablish the potential of multi-armed bandit (MAB) framework to address this\nchallenge. In particular, we provide a brief tutorial on bandit problems,\nincluding different variations and solution approaches. Furthermore, we discuss\nrecent applications as well as future research directions. In addition, we\nprovide a detailed example of using an MAB model for energy-efficient small\ncell planning in 5G networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 15:49:59 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Maghsudi", "Setareh", ""], ["Hossain", "Ekram", ""]]}, {"id": "1510.00633", "submitter": "Jialei Wang", "authors": "Jialei Wang, Mladen Kolar, Nathan Srebro", "title": "Distributed Multitask Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed multi-task learning, where each\nmachine learns a separate, but related, task. Specifically, each machine learns\na linear predictor in high-dimensional space,where all tasks share the same\nsmall support. We present a communication-efficient estimator based on the\ndebiased lasso and show that it is comparable with the optimal centralized\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 16:15:30 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Wang", "Jialei", ""], ["Kolar", "Mladen", ""], ["Srebro", "Nathan", ""]]}, {"id": "1510.00756", "submitter": "Christopher De Sa", "authors": "Christopher De Sa, Ce Zhang, Kunle Olukotun, Christopher R\\'e", "title": "Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using\n  Hierarchy Width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling on factor graphs is a widely used inference technique, which\noften produces good empirical results. Theoretical guarantees for its\nperformance are weak: even for tree structured graphs, the mixing time of Gibbs\nmay be exponential in the number of variables. To help understand the behavior\nof Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy\nwidth. We show that under suitable conditions on the weights, bounded hierarchy\nwidth ensures polynomial mixing time. Our study of hierarchy width is in part\nmotivated by a class of factor graph templates, hierarchical templates, which\nhave bounded hierarchy width---regardless of the data used to instantiate them.\nWe demonstrate a rich application from natural language processing in which\nGibbs sampling provably mixes rapidly and achieves accuracy that exceeds human\nvolunteers.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 23:14:05 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["De Sa", "Christopher", ""], ["Zhang", "Ce", ""], ["Olukotun", "Kunle", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1510.00757", "submitter": "Giuseppe Burtini", "authors": "Giuseppe Burtini, Jason Loeppky, Ramon Lawrence", "title": "A Survey of Online Experiment Design with the Stochastic Multi-Armed\n  Bandit", "comments": "49 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive and sequential experiment design is a well-studied area in numerous\ndomains. We survey and synthesize the work of the online statistical learning\nparadigm referred to as multi-armed bandits integrating the existing research\nas a resource for a certain class of online experiments. We first explore the\ntraditional stochastic model of a multi-armed bandit, then explore a taxonomic\nscheme of complications to that model, for each complication relating it to a\nspecific requirement or consideration of the experiment design context.\nFinally, at the end of the paper, we present a table of known upper-bounds of\nregret for all studied algorithms providing both perspectives for future\ntheoretical work and a decision-making tool for practitioners looking for\ntheoretical guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 23:28:11 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 18:44:44 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2015 17:38:26 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2015 18:37:43 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Burtini", "Giuseppe", ""], ["Loeppky", "Jason", ""], ["Lawrence", "Ramon", ""]]}, {"id": "1510.00772", "submitter": "Sou-Cheng Choi", "authors": "Sou-Cheng T. Choi", "title": "Machine Learning for Machine Data from a CATI Network", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a machine learning application paper involving big data. We present\nhigh-accuracy prediction methods of rare events in semi-structured machine log\nfiles, which are produced at high velocity and high volume by NORC's\ncomputer-assisted telephone interviewing (CATI) network for conducting surveys.\nWe judiciously apply natural language processing (NLP) techniques and\ndata-mining strategies to train effective learning and prediction models for\nclassifying uncommon error messages in the log---without access to source code,\nupdated documentation or dictionaries. In particular, our simple but effective\napproach of features preallocation for learning from imbalanced data coupled\nwith naive Bayes classifiers can be conceivably generalized to supervised or\nsemi-supervised learning and prediction methods for other critical events such\nas cyberattack detection.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 02:57:47 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Choi", "Sou-Cheng T.", ""]]}, {"id": "1510.00817", "submitter": "Qi Li", "authors": "Qi Li", "title": "Distributed Parameter Map-Reduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how to convert a machine learning problem into a series\nof map-reduce tasks. We study logistic regression algorithm. In logistic\nregression algorithm, it is assumed that samples are independent and each\nsample is assigned a probability. Parameters are obtained by maxmizing the\nproduct of all sample probabilities. Rapid expansion of training samples brings\nchallenges to machine learning method. Training samples are so many that they\ncan be only stored in distributed file system and driven by map-reduce style\nprograms. The main step of logistic regression is inference. According to\nmap-reduce spirit, each sample makes inference through a separate map\nprocedure. But the premise of inference is that the map procedure holds\nparameters for all features in the sample. In this paper, we propose\nDistributed Parameter Map-Reduce, in which not only samples, but also\nparameters are distributed in nodes of distributed filesystem. Through a series\nof map-reduce tasks, we assign each sample parameters for its features, make\ninference for the sample and update paramters of the model. The above processes\nare excuted looply until convergence. We test the proposed algorithm in actual\nhadoop production environment. Experiments show that the acceleration of the\nalgorithm is in linear relationship with the number of cluster nodes.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 13:12:28 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Li", "Qi", ""]]}, {"id": "1510.00857", "submitter": "Ramazan Gokberk Cinbis", "authors": "Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid", "title": "Approximate Fisher Kernels of non-iid Image Models for Image\n  Categorization", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence, in\n  press, 2015", "journal-ref": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 38,\n  no. 6, pp. 1084-1098, June 1 2016", "doi": "10.1109/TPAMI.2015.2484342", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bag-of-words (BoW) model treats images as sets of local descriptors and\nrepresents them by visual word histograms. The Fisher vector (FV)\nrepresentation extends BoW, by considering the first and second order\nstatistics of local descriptors. In both representations local descriptors are\nassumed to be identically and independently distributed (iid), which is a poor\nassumption from a modeling perspective. It has been experimentally observed\nthat the performance of BoW and FV representations can be improved by employing\ndiscounting transformations such as power normalization. In this paper, we\nintroduce non-iid models by treating the model parameters as latent variables\nwhich are integrated out, rendering all local regions dependent. Using the\nFisher kernel principle we encode an image by the gradient of the data\nlog-likelihood w.r.t. the model hyper-parameters. Our models naturally generate\ndiscounting effects in the representations; suggesting that such\ntransformations have proven successful because they closely correspond to the\nrepresentations obtained for non-iid models. To enable tractable computation,\nwe rely on variational free-energy bounds to learn the hyper-parameters and to\ncompute approximate Fisher kernels. Our experimental evaluation results\nvalidate that our models lead to performance improvements comparable to using\npower normalization, as employed in state-of-the-art feature aggregation\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 19:35:38 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Cinbis", "Ramazan Gokberk", ""], ["Verbeek", "Jakob", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1510.00878", "submitter": "Claudio Alexandre", "authors": "Claudio Alexandre and Jo\\~ao Balsa", "title": "Client Profiling for an Anti-Money Laundering System", "comments": "7 pages, 15 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a data mining approach for profiling bank clients in order to\nsupport the process of detection of anti-money laundering operations. We first\npresent the overall system architecture, and then focus on the relevant\ncomponent for this paper. We detail the experiments performed on real world\ndata from a financial institution, which allowed us to group clients in\nclusters and then generate a set of classification rules. We discuss the\nrelevance of the founded client profiles and of the generated classification\nrules. According to the defined overall agent-based architecture, these rules\nwill be incorporated in the knowledge base of the intelligent agents\nresponsible for the signaling of suspicious transactions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 22:31:58 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 16:57:57 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Alexandre", "Claudio", ""], ["Balsa", "Jo\u00e3o", ""]]}, {"id": "1510.01025", "submitter": "Anthony Man-Cho So", "authors": "Huikang Liu and Weijie Wu and Anthony Man-Cho So", "title": "Quadratic Optimization with Orthogonality Constraints: Explicit\n  Lojasiewicz Exponent and Linear Convergence of Line-Search Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental class of matrix optimization problems that arise in many areas\nof science and engineering is that of quadratic optimization with orthogonality\nconstraints. Such problems can be solved using line-search methods on the\nStiefel manifold, which are known to converge globally under mild conditions.\nTo determine the convergence rate of these methods, we give an explicit\nestimate of the exponent in a Lojasiewicz inequality for the (non-convex) set\nof critical points of the aforementioned class of problems. By combining such\nan estimate with known arguments, we are able to establish the linear\nconvergence of a large class of line-search methods. A key step in our proof is\nto establish a local error bound for the set of critical points, which may be\nof independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 04:14:22 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Liu", "Huikang", ""], ["Wu", "Weijie", ""], ["So", "Anthony Man-Cho", ""]]}, {"id": "1510.01027", "submitter": "Xinggang Wang", "authors": "Xinggang Wang, Zhuotun Zhu, Cong Yao, Xiang Bai", "title": "Relaxed Multiple-Instance SVM with Application to Object Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-instance learning (MIL) has served as an important tool for a wide\nrange of vision applications, for instance, image classification, object\ndetection, and visual tracking. In this paper, we propose a novel method to\nsolve the classical MIL problem, named relaxed multiple-instance SVM (RMI-SVM).\nWe treat the positiveness of instance as a continuous variable, use Noisy-OR\nmodel to enforce the MIL constraints, and jointly optimize the bag label and\ninstance label in a unified framework. The optimization problem can be\nefficiently solved using stochastic gradient decent. The extensive experiments\ndemonstrate that RMI-SVM consistently achieves superior performance on various\nbenchmarks for MIL. Moreover, we simply applied RMI-SVM to a challenging vision\ntask, common object discovery. The state-of-the-art results of object discovery\non Pascal VOC datasets further confirm the advantages of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 04:18:18 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Wang", "Xinggang", ""], ["Zhu", "Zhuotun", ""], ["Yao", "Cong", ""], ["Bai", "Xiang", ""]]}, {"id": "1510.01064", "submitter": "Jelena Bradic", "authors": "Alexander Hanbo Li and Jelena Bradic", "title": "Boosting in the presence of outliers: adaptive classification with\n  non-convex loss functions", "comments": null, "journal-ref": "Journal of the American Statistical Association: theory and\n  methods, 2017", "doi": "10.1080/01621459.2016.1273116", "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the role and efficiency of the non-convex loss functions\nfor binary classification problems. In particular, we investigate how to design\na simple and effective boosting algorithm that is robust to the outliers in the\ndata. The analysis of the role of a particular non-convex loss for prediction\naccuracy varies depending on the diminishing tail properties of the gradient of\nthe loss -- the ability of the loss to efficiently adapt to the outlying data,\nthe local convex properties of the loss and the proportion of the contaminated\ndata. In order to use these properties efficiently, we propose a new family of\nnon-convex losses named $\\gamma$-robust losses. Moreover, we present a new\nboosting framework, {\\it Arch Boost}, designed for augmenting the existing work\nsuch that its corresponding classification algorithm is significantly more\nadaptable to the unknown data contamination. Along with the Arch Boosting\nframework, the non-convex losses lead to the new class of boosting algorithms,\nnamed adaptive, robust, boosting (ARB). Furthermore, we present theoretical\nexamples that demonstrate the robustness properties of the proposed algorithms.\nIn particular, we develop a new breakdown point analysis and a new influence\nfunction analysis that demonstrate gains in robustness. Moreover, we present\nnew theoretical results, based only on local curvatures, which may be used to\nestablish statistical and optimization properties of the proposed Arch boosting\nalgorithms with highly non-convex loss functions. Extensive numerical\ncalculations are used to illustrate these theoretical properties and reveal\nadvantages over the existing boosting methods when data exhibits a number of\noutliers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 08:50:56 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Li", "Alexander Hanbo", ""], ["Bradic", "Jelena", ""]]}, {"id": "1510.01171", "submitter": "Hoi-To Wai", "authors": "Jean Lafond, Hoi-To Wai, Eric Moulines", "title": "On the Online Frank-Wolfe Algorithms for Convex and Non-convex\n  Optimizations", "comments": "28 pages, 4 figures. Incorporated new results on the away-step\n  algorithms and non-convex losses. Expanded the numerical experiments section", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, the online variants of the classical Frank-Wolfe algorithm are\nconsidered. We consider minimizing the regret with a stochastic cost. The\nonline algorithms only require simple iterative updates and a non-adaptive step\nsize rule, in contrast to the hybrid schemes commonly considered in the\nliterature. Several new results are derived for convex and non-convex losses.\nWith a strongly convex stochastic cost and when the optimal solution lies in\nthe interior of the constraint set or the constraint set is a polytope, the\nregret bound and anytime optimality are shown to be ${\\cal O}( \\log^3 T / T )$\nand ${\\cal O}( \\log^2 T / T)$, respectively, where $T$ is the number of rounds\nplayed. These results are based on an improved analysis on the stochastic\nFrank-Wolfe algorithms. Moreover, the online algorithms are shown to converge\neven when the loss is non-convex, i.e., the algorithms find a stationary point\nto the time-varying/stochastic loss at a rate of ${\\cal O}(\\sqrt{1/T})$.\nNumerical experiments on realistic data sets are presented to support our\ntheoretical claims.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 14:42:36 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 17:11:30 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Lafond", "Jean", ""], ["Wai", "Hoi-To", ""], ["Moulines", "Eric", ""]]}, {"id": "1510.01175", "submitter": "Roberto Diaz Morales", "authors": "Roberto D\\'iaz-Morales", "title": "Cross-Device Tracking: Matching Devices and Cookies", "comments": null, "journal-ref": null, "doi": "10.1109/ICDMW.2015.244", "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of computers, tablets and smartphones is increasing rapidly, which\nentails the ownership and use of multiple devices to perform online tasks. As\npeople move across devices to complete these tasks, their identities becomes\nfragmented. Understanding the usage and transition between those devices is\nessential to develop efficient applications in a multi-device world. In this\npaper we present a solution to deal with the cross-device identification of\nusers based on semi-supervised machine learning methods to identify which\ncookies belong to an individual using a device. The method proposed in this\npaper scored third in the ICDM 2015 Drawbridge Cross-Device Connections\nchallenge proving its good performance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 14:56:24 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["D\u00edaz-Morales", "Roberto", ""]]}, {"id": "1510.01225", "submitter": "Tohid Ardeshiri", "authors": "Tohid Ardeshiri, Umut Orguner, and Fredrik Gustafsson", "title": "Bayesian Inference via Approximation of Log-likelihood for Priors in\n  Exponential Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Bayesian inference technique based on Taylor series\napproximation of the logarithm of the likelihood function is presented. The\nproposed approximation is devised for the case, where the prior distribution\nbelongs to the exponential family of distributions. The logarithm of the\nlikelihood function is linearized with respect to the sufficient statistic of\nthe prior distribution in exponential family such that the posterior obtains\nthe same exponential family form as the prior. Similarities between the\nproposed method and the extended Kalman filter for nonlinear filtering are\nillustrated. Furthermore, an extended target measurement update for target\nmodels where the target extent is represented by a random matrix having an\ninverse Wishart distribution is derived. The approximate update covers the\nimportant case where the spread of measurement is due to the target extent as\nwell as the measurement noise in the sensor.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 16:47:54 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Ardeshiri", "Tohid", ""], ["Orguner", "Umut", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1510.01270", "submitter": "Rados{\\l}aw Michalski", "authors": "Tomasz Kajdanowicz, Rados{\\l}aw Michalski, Katarzyna Musia{\\l},\n  Przemys{\\l}aw Kazienko", "title": "Learning in Unlabeled Networks - An Active Learning and Inference\n  Approach", "comments": null, "journal-ref": "AI Communications, Vol. 29, No. 1, 2016, IOS Press", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of determining labels of all network nodes based on the knowledge\nabout network structure and labels of some training subset of nodes is called\nthe within-network classification. It may happen that none of the labels of the\nnodes is known and additionally there is no information about number of classes\nto which nodes can be assigned. In such a case a subset of nodes has to be\nselected for initial label acquisition. The question that arises is: \"labels of\nwhich nodes should be collected and used for learning in order to provide the\nbest classification accuracy for the whole network?\". Active learning and\ninference is a practical framework to study this problem.\n  A set of methods for active learning and inference for within network\nclassification is proposed and validated. The utility score calculation for\neach node based on network structure is the first step in the process. The\nscores enable to rank the nodes. Based on the ranking, a set of nodes, for\nwhich the labels are acquired, is selected (e.g. by taking top or bottom N from\nthe ranking). The new measure-neighbour methods proposed in the paper suggest\nnot obtaining labels of nodes from the ranking but rather acquiring labels of\ntheir neighbours. The paper examines 29 distinct formulations of utility score\nand selection methods reporting their impact on the results of two collective\nclassification algorithms: Iterative Classification Algorithm and Loopy Belief\nPropagation.\n  We advocate that the accuracy of presented methods depends on the structural\nproperties of the examined network. We claim that measure-neighbour methods\nwill work better than the regular methods for networks with higher clustering\ncoefficient and worse than regular methods for networks with low clustering\ncoefficient. According to our hypothesis, based on clustering coefficient we\nare able to recommend appropriate active learning and inference method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 18:25:19 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Kajdanowicz", "Tomasz", ""], ["Michalski", "Rados\u0142aw", ""], ["Musia\u0142", "Katarzyna", ""], ["Kazienko", "Przemys\u0142aw", ""]]}, {"id": "1510.01308", "submitter": "Tudor Achim", "authors": "Lun-Kai Hsu, Tudor Achim, Stefano Ermon", "title": "Tight Variational Bounds via Random Projections and I-Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information projections are the key building block of variational inference\nalgorithms and are used to approximate a target probabilistic model by\nprojecting it onto a family of tractable distributions. In general, there is no\nguarantee on the quality of the approximation obtained. To overcome this issue,\nwe introduce a new class of random projections to reduce the dimensionality and\nhence the complexity of the original model. In the spirit of random\nprojections, the projection preserves (with high probability) key properties of\nthe target distribution. We show that information projections can be combined\nwith random projections to obtain provable guarantees on the quality of the\napproximation obtained, regardless of the complexity of the original model. We\ndemonstrate empirically that augmenting mean field with a random projection\nstep dramatically improves partition function and marginal probability\nestimates, both on synthetic and real world data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 19:53:22 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Hsu", "Lun-Kai", ""], ["Achim", "Tudor", ""], ["Ermon", "Stefano", ""]]}, {"id": "1510.01378", "submitter": "Phil\\'emon Brakel", "authors": "C\\'esar Laurent, Gabriel Pereyra, Phil\\'emon Brakel, Ying Zhang and\n  Yoshua Bengio", "title": "Batch Normalized Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are powerful models for sequential data that\nhave the potential to learn long-term dependencies. However, they are\ncomputationally expensive to train and difficult to parallelize. Recent work\nhas shown that normalizing intermediate representations of neural networks can\nsignificantly improve convergence rates in feedforward neural networks . In\nparticular, batch normalization, which uses mini-batch statistics to\nstandardize features, was shown to significantly reduce training time. In this\npaper, we show that applying batch normalization to the hidden-to-hidden\ntransitions of our RNNs doesn't help the training procedure. We also show that\nwhen applied to the input-to-hidden transitions, batch normalization can lead\nto a faster convergence of the training criterion but doesn't seem to improve\nthe generalization performance on both our language modelling and speech\nrecognition tasks. All in all, applying batch normalization to RNNs turns out\nto be more challenging than applying it to feedforward networks, but certain\nvariants of it can still be beneficial.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 21:45:31 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Laurent", "C\u00e9sar", ""], ["Pereyra", "Gabriel", ""], ["Brakel", "Phil\u00e9mon", ""], ["Zhang", "Ying", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1510.01422", "submitter": "Norm Matloff PhD", "authors": "Norman Matloff", "title": "Improved Estimation of Class Prior Probabilities through Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work in the classification literature has shown that in computing a\nclassification function, one need not know the class membership of all\nobservations in the training set; the unlabeled observations still provide\ninformation on the marginal distribution of the feature set, and can thus\ncontribute to increased classification accuracy for future observations. The\npresent paper will show that this scheme can also be used for the estimation of\nclass prior probabilities, which would be very useful in applications in which\nit is difficult or expensive to determine class membership. Both parametric and\nnonparametric estimators are developed. Asymptotic distributions of the\nestimators are derived, and it is proven that the use of the unlabeled\nobservations does reduce asymptotic variance. This methodology is also extended\nto the estimation of subclass probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 03:58:52 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Matloff", "Norman", ""]]}, {"id": "1510.01443", "submitter": "Siu Wa Lee", "authors": "Bo Fan, Siu Wa Lee, Xiaohai Tian, Lei Xie and Minghui Dong", "title": "A Waveform Representation Framework for High-quality Statistical\n  Parametric Speech Synthesis", "comments": "accepted and will appear in APSIPA2015; keywords: speech synthesis,\n  LSTM-RNN, vocoder, phase, waveform, modeling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  State-of-the-art statistical parametric speech synthesis (SPSS) generally\nuses a vocoder to represent speech signals and parameterize them into features\nfor subsequent modeling. Magnitude spectrum has been a dominant feature over\nthe years. Although perceptual studies have shown that phase spectrum is\nessential to the quality of synthesized speech, it is often ignored by using a\nminimum phase filter during synthesis and the speech quality suffers. To bypass\nthis bottleneck in vocoded speech, this paper proposes a phase-embedded\nwaveform representation framework and establishes a magnitude-phase joint\nmodeling platform for high-quality SPSS. Our experiments on waveform\nreconstruction show that the performance is better than that of the widely-used\nSTRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms\na leading-edge, vocoded, deep bidirectional long short-term memory recurrent\nneural network (DBLSTM-RNN)-based baseline system in various objective\nevaluation metrics conducted.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 06:12:31 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Fan", "Bo", ""], ["Lee", "Siu Wa", ""], ["Tian", "Xiaohai", ""], ["Xie", "Lei", ""], ["Dong", "Minghui", ""]]}, {"id": "1510.01444", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Qihang Lin", "title": "Stochastic subGradient Methods with Linear Convergence for Polyhedral\n  Convex Optimization", "comments": "This paper has been withdrawn by the author due to that it has been\n  merged into arXiv manuscript arXiv:1512.03107", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that simple {Stochastic} subGradient Decent methods\nwith multiple Restarting, named {\\bf RSGD}, can achieve a \\textit{linear\nconvergence rate} for a class of non-smooth and non-strongly convex\noptimization problems where the epigraph of the objective function is a\npolyhedron, to which we refer as {\\bf polyhedral convex optimization}. Its\napplications in machine learning include $\\ell_1$ constrained or regularized\npiecewise linear loss minimization and submodular function minimization. To the\nbest of our knowledge, this is the first result on the linear convergence rate\nof stochastic subgradient methods for non-smooth and non-strongly convex\noptimization problems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 06:17:56 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 16:41:07 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 21:22:59 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2015 03:21:12 GMT"}, {"version": "v5", "created": "Thu, 31 Mar 2016 16:46:45 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Yang", "Tianbao", ""], ["Lin", "Qihang", ""]]}, {"id": "1510.01463", "submitter": "Yunwen Lei", "authors": "Yunwen Lei, Lixin Ding and Yingzhou Bi", "title": "Local Rademacher Complexity Bounds based on Covering Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a general result on controlling local Rademacher\ncomplexities, which captures in an elegant form to relate the complexities with\nconstraint on the expected norm to the corresponding ones with constraint on\nthe empirical norm. This result is convenient to apply in real applications and\ncould yield refined local Rademacher complexity bounds for function classes\nsatisfying general entropy conditions. We demonstrate the power of our\ncomplexity bounds by applying them to derive effective generalization error\nbounds.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 07:44:08 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Lei", "Yunwen", ""], ["Ding", "Lixin", ""], ["Bi", "Yingzhou", ""]]}, {"id": "1510.01485", "submitter": "Dinu Kaufmann", "authors": "Dinu Kaufmann, Sonali Parbhoo, Aleksander Wieczorek, Sebastian Keller,\n  David Adametz, Volker Roth", "title": "Bayesian Markov Blanket Estimation", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a Bayesian view for estimating a sub-network in a Markov\nrandom field. The sub-network corresponds to the Markov blanket of a set of\nquery variables, where the set of potential neighbours here is big. We\nfactorize the posterior such that the Markov blanket is conditionally\nindependent of the network of the potential neighbours. By exploiting this\nblockwise decoupling, we derive analytic expressions for posterior\nconditionals. Subsequently, we develop an inference scheme which makes use of\nthe factorization. As a result, estimation of a sub-network is possible without\ninferring an entire network. Since the resulting Gibbs sampler scales linearly\nwith the number of variables, it can handle relatively large neighbourhoods.\nThe proposed scheme results in faster convergence and superior mixing of the\nMarkov chain than existing Bayesian network estimation techniques.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 09:06:11 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Kaufmann", "Dinu", ""], ["Parbhoo", "Sonali", ""], ["Wieczorek", "Aleksander", ""], ["Keller", "Sebastian", ""], ["Adametz", "David", ""], ["Roth", "Volker", ""]]}, {"id": "1510.01495", "submitter": "Georg Martius", "authors": "Georg Martius and Eckehard Olbrich", "title": "Quantifying Emergent Behavior of Autonomous Robots", "comments": "24 pages, 10 figures, submitted Entropy Journal", "journal-ref": "Entropy 2015, 17(10), 7266-7297", "doi": "10.3390/e17107266", "report-no": null, "categories": "cs.IT cs.LG cs.RO math.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying behaviors of robots which were generated autonomously from\ntask-independent objective functions is an important prerequisite for objective\ncomparisons of algorithms and movements of animals. The temporal sequence of\nsuch a behavior can be considered as a time series and hence complexity\nmeasures developed for time series are natural candidates for its\nquantification. The predictive information and the excess entropy are such\ncomplexity measures. They measure the amount of information the past contains\nabout the future and thus quantify the nonrandom structure in the temporal\nsequence. However, when using these measures for systems with continuous states\none has to deal with the fact that their values will depend on the resolution\nwith which the systems states are observed. For deterministic systems both\nmeasures will diverge with increasing resolution. We therefore propose a new\ndecomposition of the excess entropy in resolution dependent and resolution\nindependent parts and discuss how they depend on the dimensionality of the\ndynamics, correlations and the noise level. For the practical estimation we\npropose to use estimates based on the correlation integral instead of the\ndirect estimation of the mutual information using the algorithm by Kraskov et\nal. (2004) which is based on next neighbor statistics because the latter allows\nless control of the scale dependencies. Using our algorithm we are able to show\nhow autonomous learning generates behavior of increasing complexity with\nincreasing learning duration.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 09:18:41 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Martius", "Georg", ""], ["Olbrich", "Eckehard", ""]]}, {"id": "1510.01624", "submitter": "Christian Igel", "authors": "Oswin Krause, Asja Fischer, Christian Igel", "title": "Population-Contrastive-Divergence: Does Consistency help with RBM\n  training?", "comments": "An updated version is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the log-likelihood gradient with respect to the parameters of a\nRestricted Boltzmann Machine (RBM) typically requires sampling using Markov\nChain Monte Carlo (MCMC) techniques. To save computation time, the Markov\nchains are only run for a small number of steps, which leads to a biased\nestimate. This bias can cause RBM training algorithms such as Contrastive\nDivergence (CD) learning to deteriorate. We adopt the idea behind Population\nMonte Carlo (PMC) methods to devise a new RBM training algorithm termed\nPopulation-Contrastive-Divergence (pop-CD). Compared to CD, it leads to a\nconsistent estimate and may have a significantly lower bias. Its computational\noverhead is negligible compared to CD. However, the variance of the gradient\nestimate increases. We experimentally show that pop-CD can significantly\noutperform CD. In many cases, we observed a smaller bias and achieved higher\nlog-likelihood values. However, when the RBM distribution has many hidden\nneurons, the consistent estimate of pop-CD may still have a considerable bias\nand the variance of the gradient estimate requires a smaller learning rate.\nThus, despite its superior theoretical properties, it is not advisable to use\npop-CD in its current form on large problems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 15:29:04 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 12:30:35 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 17:37:27 GMT"}, {"version": "v4", "created": "Wed, 28 Jun 2017 09:35:12 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Krause", "Oswin", ""], ["Fischer", "Asja", ""], ["Igel", "Christian", ""]]}, {"id": "1510.01628", "submitter": "Panagiotis Traganitis", "authors": "Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis", "title": "Large-scale subspace clustering using sketching and validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nowadays massive amounts of generated and communicated data present major\nchallenges in their processing. While capable of successfully classifying\nnonlinearly separable objects in various settings, subspace clustering (SC)\nmethods incur prohibitively high computational complexity when processing\nlarge-scale data. Inspired by the random sampling and consensus (RANSAC)\napproach to robust regression, the present paper introduces a randomized scheme\nfor SC, termed sketching and validation (SkeVa-)SC, tailored for large-scale\ndata. At the heart of SkeVa-SC lies a randomized scheme for approximating the\nunderlying probability density function of the observed data by kernel\nsmoothing arguments. Sparsity in data representations is also exploited to\nreduce the computational burden of SC, while achieving high clustering\naccuracy. Performance analysis as well as extensive numerical tests on\nsynthetic and real data corroborate the potential of SkeVa-SC and its\ncompetitive performance relative to state-of-the-art scalable SC approaches.\nKeywords: Subspace clustering, big data, kernel smoothing, randomization,\nsketching, validation, sparsity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 15:34:32 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Traganitis", "Panagiotis A.", ""], ["Slavakis", "Konstantinos", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1510.01722", "submitter": "Vikas Sindhwani", "authors": "Vikas Sindhwani and Tara N. Sainath and Sanjiv Kumar", "title": "Structured Transforms for Small-Footprint Deep Learning", "comments": "To appear in NIPS 2015; 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of building compact deep learning pipelines suitable for\ndeployment on storage and power constrained mobile devices. We propose a\nunified framework to learn a broad family of structured parameter matrices that\nare characterized by the notion of low displacement rank. Our structured\ntransforms admit fast function and gradient evaluation, and span a rich range\nof parameter sharing configurations whose statistical modeling capacity can be\nexplicitly tuned along a continuum from structured to unstructured.\nExperimental results show that these transforms can significantly accelerate\ninference and forward/backward passes during training, and offer superior\naccuracy-compactness-speed tradeoffs in comparison to a number of existing\ntechniques. In keyword spotting applications in mobile speech recognition, our\nmethods are much more effective than standard linear low-rank bottleneck layers\nand nearly retain the performance of state of the art models, while providing\nmore than 3.5-fold compression.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 19:42:22 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Sindhwani", "Vikas", ""], ["Sainath", "Tara N.", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "1510.01799", "submitter": "Ian Goodfellow", "authors": "Ian Goodfellow", "title": "Efficient Per-Example Gradient Computations", "comments": "This revision fixed some typos. Many thanks to Hugo Larochelle for\n  reporting them!", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes an efficient technique for computing the norm\nof the gradient of the loss function for a neural network with respect to its\nparameters. This gradient norm can be computed efficiently for every example.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 01:42:23 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 23:59:02 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Goodfellow", "Ian", ""]]}, {"id": "1510.02054", "submitter": "Weiran Wang", "authors": "Weiran Wang, Raman Arora, Karen Livescu, Nathan Srebro", "title": "Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations", "comments": "in 2015 Annual Allerton Conference on Communication, Control and\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep CCA is a recently proposed deep neural network extension to the\ntraditional canonical correlation analysis (CCA), and has been successful for\nmulti-view representation learning in several domains. However, stochastic\noptimization of the deep CCA objective is not straightforward, because it does\nnot decouple over training examples. Previous optimizers for deep CCA are\neither batch-based algorithms or stochastic optimization using large\nminibatches, which can have high memory consumption. In this paper, we tackle\nthe problem of stochastic optimization for deep CCA with small minibatches,\nbased on an iterative solution to the CCA objective, and show that we can\nachieve as good performance as previous optimizers and thus alleviate the\nmemory requirement.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 18:32:41 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Wang", "Weiran", ""], ["Arora", "Raman", ""], ["Livescu", "Karen", ""], ["Srebro", "Nathan", ""]]}, {"id": "1510.02173", "submitter": "John-Alexander Assael", "authors": "John-Alexander M. Assael, Niklas Wahlstr\\\"om, Thomas B. Sch\\\"on, Marc\n  Peter Deisenroth", "title": "Data-Efficient Learning of Feedback Policies from Image Pixels using\n  Deep Dynamical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-efficient reinforcement learning (RL) in continuous state-action spaces\nusing very high-dimensional observations remains a key challenge in developing\nfully autonomous systems. We consider a particularly important instance of this\nchallenge, the pixels-to-torques problem, where an RL agent learns a\nclosed-loop control policy (\"torques\") from pixel information only. We\nintroduce a data-efficient, model-based reinforcement learning algorithm that\nlearns such a closed-loop policy directly from pixel information. The key\ningredient is a deep dynamical model for learning a low-dimensional feature\nembedding of images jointly with a predictive model in this low-dimensional\nfeature space. Joint learning is crucial for long-term predictions, which lie\nat the core of the adaptive nonlinear model predictive control strategy that we\nuse for closed-loop control. Compared to state-of-the-art RL methods for\ncontinuous states and actions, our approach learns quickly, scales to\nhigh-dimensional state spaces, is lightweight and an important step toward\nfully autonomous end-to-end learning from pixels to torques.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 00:20:42 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 15:21:01 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Assael", "John-Alexander M.", ""], ["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1510.02255", "submitter": "Vidyadhar Upadhya", "authors": "Vidyadhar Upadhya, P.S. Sastry", "title": "Empirical Analysis of Sampling Based Estimators for Evaluating RBMs", "comments": "edited version of this manuscript will appear in proceedings of\n  International Conference on Neural Information Processing (ICONIP) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Restricted Boltzmann Machines (RBM) can be used either as classifiers or\nas generative models. The quality of the generative RBM is measured through the\naverage log-likelihood on test data. Due to the high computational complexity\nof evaluating the partition function, exact calculation of test log-likelihood\nis very difficult. In recent years some estimation methods are suggested for\napproximate computation of test log-likelihood. In this paper we present an\nempirical comparison of the main estimation methods, namely, the AIS algorithm\nfor estimating the partition function, the CSL method for directly estimating\nthe log-likelihood, and the RAISE algorithm that combines these two ideas. We\nuse the MNIST data set to learn the RBM and then compare these methods for\nestimating the test log-likelihood.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 09:44:15 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Upadhya", "Vidyadhar", ""], ["Sastry", "P. S.", ""]]}, {"id": "1510.02364", "submitter": "Ralph Versteegen", "authors": "Ralph Versteegen, Georgy Gimel'farb, Patricia Riddle", "title": "Texture Modelling with Nested High-order Markov-Gibbs Random Fields", "comments": "Submitted to Computer Vision and Image Understanding", "journal-ref": null, "doi": "10.1016/j.cviu.2015.11.003", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, Markov-Gibbs random field (MGRF) image models which include\nhigh-order interactions are almost always built by modelling responses of a\nstack of local linear filters. Actual interaction structure is specified\nimplicitly by the filter coefficients. In contrast, we learn an explicit\nhigh-order MGRF structure by considering the learning process in terms of\ngeneral exponential family distributions nested over base models, so that\npotentials added later can build on previous ones. We relatively rapidly add\nnew features by skipping over the costly optimisation of parameters.\n  We introduce the use of local binary patterns as features in MGRF texture\nmodels, and generalise them by learning offsets to the surrounding pixels.\nThese prove effective as high-order features, and are fast to compute. Several\nschemes for selecting high-order features by composition or search of a small\nsubclass are compared. Additionally we present a simple modification of the\nmaximum likelihood as a texture modelling-specific objective function which\naims to improve generalisation by local windowing of statistics.\n  The proposed method was experimentally evaluated by learning high-order MGRF\nmodels for a broad selection of complex textures and then performing texture\nsynthesis, and succeeded on much of the continuum from stochastic through\nirregularly structured to near-regular textures. Learning interaction structure\nis very beneficial for textures with large-scale structure, although those with\ncomplex irregular structure still provide difficulties. The texture models were\nalso quantitatively evaluated on two tasks and found to be competitive with\nother works: grading of synthesised textures by a panel of observers; and\ncomparison against several recent MGRF models by evaluation on a constrained\ninpainting task.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 15:22:21 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Versteegen", "Ralph", ""], ["Gimel'farb", "Georgy", ""], ["Riddle", "Patricia", ""]]}, {"id": "1510.02387", "submitter": "Pranava Swaroop Madhyastha", "authors": "Pranava Swaroop Madhyastha, Mohit Bansal, Kevin Gimpel and Karen\n  Livescu", "title": "Mapping Unseen Words to Task-Trained Embedding Spaces", "comments": "8 + 3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the supervised training setting in which we learn task-specific\nword embeddings. We assume that we start with initial embeddings learned from\nunlabelled data and update them to learn task-specific embeddings for words in\nthe supervised training data. However, for new words in the test set, we must\nuse either their initial embeddings or a single unknown embedding, which often\nleads to errors. We address this by learning a neural network to map from\ninitial embeddings to the task-specific embedding space, via a multi-loss\nobjective function. The technique is general, but here we demonstrate its use\nfor improved dependency parsing (especially for sentences with\nout-of-vocabulary words), as well as for downstream improvements on sentiment\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 16:17:47 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 06:24:18 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Madhyastha", "Pranava Swaroop", ""], ["Bansal", "Mohit", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1510.02437", "submitter": "George Papamakarios", "authors": "George Papamakarios", "title": "Distilling Model Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-performing machine learning systems, such as deep neural networks, large\nensembles and complex probabilistic graphical models, can be expensive to\nstore, slow to evaluate and hard to integrate into larger systems. Ideally, we\nwould like to replace such cumbersome models with simpler models that perform\nequally well.\n  In this thesis, we study knowledge distillation, the idea of extracting the\nknowledge contained in a complex model and injecting it into a more convenient\nmodel. We present a general framework for knowledge distillation, whereby a\nconvenient model of our choosing learns how to mimic a complex model, by\nobserving the latter's behaviour and being penalized whenever it fails to\nreproduce it.\n  We develop our framework within the context of three distinct machine\nlearning applications: (a) model compression, where we compress large\ndiscriminative models, such as ensembles of neural networks, into models of\nmuch smaller size; (b) compact predictive distributions for Bayesian inference,\nwhere we distil large bags of MCMC samples into compact predictive\ndistributions in closed form; (c) intractable generative models, where we\ndistil unnormalizable models such as RBMs into tractable models such as NADEs.\n  We contribute to the state of the art with novel techniques and ideas. In\nmodel compression, we describe and implement derivative matching, which allows\nfor better distillation when data is scarce. In compact predictive\ndistributions, we introduce online distillation, which allows for significant\nsavings in memory. Finally, in intractable generative models, we show how to\nuse distilled models to robustly estimate intractable quantities of the\noriginal model, such as its intractable partition function.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 18:40:50 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Papamakarios", "George", ""]]}, {"id": "1510.02442", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Uniform Learning in a Deep Neural Network via \"Oddball\" Stochastic\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training deep neural networks, it is typically assumed that the training\nexamples are uniformly difficult to learn. Or, to restate, it is assumed that\nthe training error will be uniformly distributed across the training examples.\nBased on these assumptions, each training example is used an equal number of\ntimes. However, this assumption may not be valid in many cases. \"Oddball SGD\"\n(novelty-driven stochastic gradient descent) was recently introduced to drive\ntraining probabilistically according to the error distribution - training\nfrequency is proportional to training error magnitude. In this article, using a\ndeep neural network to encode a video, we show that oddball SGD can be used to\nenforce uniform error across the training set.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 18:55:22 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1510.02533", "submitter": "Aaron Defazio Dr", "authors": "Aaron Defazio", "title": "New Optimisation Methods for Machine Learning", "comments": "PhD thesis, 205 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A thesis submitted for the degree of Doctor of Philosophy of The Australian\nNational University.\n  In this work we introduce several new optimisation methods for problems in\nmachine learning. Our algorithms broadly fall into two categories: optimisation\nof finite sums and of graph structured objectives. The finite sum problem is\nsimply the minimisation of objective functions that are naturally expressed as\na summation over a large number of terms, where each term has a similar or\nidentical weight. Such objectives most often appear in machine learning in the\nempirical risk minimisation framework in the non-online learning setting. The\nsecond category, that of graph structured objectives, consists of objectives\nthat result from applying maximum likelihood to Markov random field models.\nUnlike the finite sum case, all the non-linearity is contained within a\npartition function term, which does not readily decompose into a summation.\n  For the finite sum problem, we introduce the Finito and SAGA algorithms, as\nwell as variants of each.\n  For graph-structured problems, we take three complementary approaches. We\nlook at learning the parameters for a fixed structure, learning the structure\nindependently, and learning both simultaneously. Specifically, for the combined\napproach, we introduce a new method for encouraging graph structures with the\n\"scale-free\" property. For the structure learning problem, we establish\nSHORTCUT, a O(n^{2.5}) expected time approximate structure learning method for\nGaussian graphical models. For problems where the structure is known but the\nparameters unknown, we introduce an approximate maximum likelihood learning\nalgorithm that is capable of learning a useful subclass of Gaussian graphical\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 00:34:12 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 02:00:45 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Defazio", "Aaron", ""]]}, {"id": "1510.02558", "submitter": "Chu Wang", "authors": "Chu Wang and Yingfei Wang and Weinan E and Robert Schapire", "title": "Functional Frank-Wolfe Boosting for General Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is a generic learning method for classification and regression. Yet,\nas the number of base hypotheses becomes larger, boosting can lead to a\ndeterioration of test performance. Overfitting is an important and ubiquitous\nphenomenon, especially in regression settings. To avoid overfitting, we\nconsider using $l_1$ regularization. We propose a novel Frank-Wolfe type\nboosting algorithm (FWBoost) applied to general loss functions. By using\nexponential loss, the FWBoost algorithm can be rewritten as a variant of\nAdaBoost for binary classification. FWBoost algorithms have exactly the same\nform as existing boosting methods, in terms of making calls to a base learning\nalgorithm with different weights update. This direct connection between\nboosting and Frank-Wolfe yields a new algorithm that is as practical as\nexisting boosting methods but with new guarantees and rates of convergence.\nExperimental results show that the test performance of FWBoost is not degraded\nwith larger rounds in boosting, which is consistent with the theoretical\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 03:16:07 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Wang", "Chu", ""], ["Wang", "Yingfei", ""], ["E", "Weinan", ""], ["Schapire", "Robert", ""]]}, {"id": "1510.02674", "submitter": "Syed Raza Ahmad", "authors": "S. Raza Ahmad", "title": "Technical Report of Participation in Higgs Boson Machine Learning\n  Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report entails the detailed description of the approach and\nmethodologies taken as part of competing in the Higgs Boson Machine Learning\nCompetition hosted by Kaggle Inc. and organized by CERN et al. It briefly\ndescribes the theoretical background of the problem and the motivation for\ntaking part in the competition. Furthermore, the various machine learning\nmodels and algorithms analyzed and implemented during the 4 month period of\nparticipation are discussed and compared. Special attention is paid to the Deep\nLearning techniques and architectures implemented from scratch using Python and\nNumPy for this competition.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 14:00:48 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Ahmad", "S. Raza", ""]]}, {"id": "1510.02676", "submitter": "Eric Bax", "authors": "Eric Bax, Ya Le", "title": "Some Theory For Practical Classifier Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare and contrast two approaches to validating a trained classifier\nwhile using all in-sample data for training. One is simultaneous validation\nover an organized set of hypotheses (SVOOSH), the well-known method that began\nwith VC theory. The other is withhold and gap (WAG). WAG withholds a validation\nset, trains a holdout classifier on the remaining data, uses the validation\ndata to validate that classifier, then adds the rate of disagreement between\nthe holdout classifier and one trained using all in-sample data, which is an\nupper bound on the difference in error rates. We show that complex hypothesis\nclasses and limited training data can make WAG a favorable alternative.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 14:04:01 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Bax", "Eric", ""], ["Le", "Ya", ""]]}, {"id": "1510.02693", "submitter": "ShiLiang Zhang", "authors": "ShiLiang Zhang, Hui Jiang, Si Wei, LiRong Dai", "title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback", "comments": "4 pages, 1figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new structure for memory neural networks, called feedforward\nsequential memory networks (FSMN), which can learn long-term dependency without\nusing recurrent feedback. The proposed FSMN is a standard feedforward neural\nnetworks equipped with learnable sequential memory blocks in the hidden layers.\nIn this work, we have applied FSMN to several language modeling (LM) tasks.\nExperimental results have shown that the memory blocks in FSMN can learn\neffective representations of long history. Experiments have shown that FSMN\nbased language models can significantly outperform not only feedforward neural\nnetwork (FNN) based LMs but also the popular recurrent neural network (RNN)\nLMs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:04:11 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Zhang", "ShiLiang", ""], ["Jiang", "Hui", ""], ["Wei", "Si", ""], ["Dai", "LiRong", ""]]}, {"id": "1510.02706", "submitter": "Alexander Zimin", "authors": "Alexander Zimin, Christoph H. Lampert", "title": "Conditional Risk Minimization for Stochastic Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of learning from non-i.i.d. data. In particular, we aim at\nlearning predictors that minimize the conditional risk for a stochastic\nprocess, i.e. the expected loss of the predictor on the next point conditioned\non the set of training samples observed so far. For non-i.i.d. data, the\ntraining set contains information about the upcoming samples, so learning with\nrespect to the conditional distribution can be expected to yield better\npredictors than one obtains from the classical setting of minimizing the\nmarginal risk. Our main contribution is a practical estimator for the\nconditional risk based on the theory of non-parametric time-series prediction,\nand a finite sample concentration bound that establishes uniform convergence of\nthe estimator to the true conditional risk under certain regularity assumptions\non the process.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:31:36 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2016 12:54:04 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Zimin", "Alexander", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1510.02709", "submitter": "Kairan Sun", "authors": "Kairan Sun, Xu Wei, Gengtao Jia, Risheng Wang, Ruizhi Li", "title": "Large-scale Artificial Neural Network: MapReduce-based Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faced with continuously increasing scale of data, original back-propagation\nneural network based machine learning algorithm presents two non-trivial\nchallenges: huge amount of data makes it difficult to maintain both efficiency\nand accuracy; redundant data aggravates the system workload. This project is\nmainly focused on the solution to the issues above, combining deep learning\nalgorithm with cloud computing platform to deal with large-scale data. A\nMapReduce-based handwriting character recognizer will be designed in this\nproject to verify the efficiency improvement this mechanism will achieve on\ntraining and practical large-scale data. Careful discussion and experiment will\nbe developed to illustrate how deep learning algorithm works to train\nhandwritten digits data, how MapReduce is implemented on deep learning neural\nnetwork, and why this combination accelerates computation. Besides performance,\nthe scalability and robustness will be mentioned in this report as well. Our\nsystem comes with two demonstration software that visually illustrates our\nhandwritten digit recognition/encoding application.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:45:44 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Sun", "Kairan", ""], ["Wei", "Xu", ""], ["Jia", "Gengtao", ""], ["Wang", "Risheng", ""], ["Li", "Ruizhi", ""]]}, {"id": "1510.02777", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio and Asja Fischer", "title": "Early Inference in Energy-Based Models Approximates Back-Propagation", "comments": "arXiv admin note: text overlap with arXiv:1509.05936", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Langevin MCMC inference in an energy-based model with latent\nvariables has the property that the early steps of inference, starting from a\nstationary point, correspond to propagating error gradients into internal\nlayers, similarly to back-propagation. The error that is back-propagated is\nwith respect to visible units that have received an outside driving force\npushing them away from the stationary point. Back-propagated error gradients\ncorrespond to temporal derivatives of the activation of hidden units. This\nobservation could be an element of a theory for explaining how brains perform\ncredit assignment in deep hierarchies as efficiently as back-propagation does.\nIn this theory, the continuous-valued latent variables correspond to averaged\nvoltage potential (across time, spikes, and possibly neurons in the same\nminicolumn), and neural computation corresponds to approximate inference and\nerror back-propagation at the same time.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 19:21:32 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 20:24:38 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Bengio", "Yoshua", ""], ["Fischer", "Asja", ""]]}, {"id": "1510.02824", "submitter": "Thomas Dybdahl Ahle", "authors": "Thomas D. Ahle and Rasmus Pagh and Ilya Razenshteyn and Francesco\n  Silvestri", "title": "On the Complexity of Inner Product Similarity Join", "comments": "in Proc. 35th ACM Symposium on Principles of Database Systems, 2016", "journal-ref": null, "doi": "10.1145/2902251.2902285", "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of tasks in classification, information retrieval, recommendation\nsystems, and record linkage reduce to the core problem of inner product\nsimilarity join (IPS join): identifying pairs of vectors in a collection that\nhave a sufficiently large inner product. IPS join is well understood when\nvectors are normalized and some approximation of inner products is allowed.\nHowever, the general case where vectors may have any length appears much more\nchallenging. Recently, new upper bounds based on asymmetric locality-sensitive\nhashing (ALSH) and asymmetric embeddings have emerged, but little has been\nknown on the lower bound side. In this paper we initiate a systematic study of\ninner product similarity join, showing new lower and upper bounds. Our main\nresults are:\n  * Approximation hardness of IPS join in subquadratic time, assuming the\nstrong exponential time hypothesis.\n  * New upper and lower bounds for (A)LSH-based algorithms. In particular, we\nshow that asymmetry can be avoided by relaxing the LSH definition to only\nconsider the collision probability of distinct elements.\n  * A new indexing method for IPS based on linear sketches, implying that our\nhardness results are not far from being tight.\n  Our technical contributions include new asymmetric embeddings that may be of\nindependent interest. At the conceptual level we strive to provide greater\nclarity, for example by distinguishing among signed and unsigned variants of\nIPS join and shedding new light on the effect of asymmetry.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 21:10:12 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 09:45:18 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2016 11:36:25 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Ahle", "Thomas D.", ""], ["Pagh", "Rasmus", ""], ["Razenshteyn", "Ilya", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1510.02833", "submitter": "Andrew Gardner", "authors": "Andrew Gardner, Christian A. Duncan, Jinko Kanno, and Rastko R. Selmic", "title": "On the Definiteness of Earth Mover's Distance and Its Relation to Set\n  Intersection", "comments": "Major revision based on referee comments. Includes significant\n  reorganization of content, new title, new propositions, revised proofs of\n  previous propositions, and additional experiments with new data, kernels, and\n  indefinite kernel techniques", "journal-ref": null, "doi": "10.1109/TCYB.2017.2761798", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive definite kernels are an important tool in machine learning that\nenable efficient solutions to otherwise difficult or intractable problems by\nimplicitly linearizing the problem geometry. In this paper we develop a\nset-theoretic interpretation of the Earth Mover's Distance (EMD) and propose\nEarth Mover's Intersection (EMI), a positive definite analog to EMD for sets of\ndifferent sizes. We provide conditions under which EMD or certain\napproximations to EMD are negative definite. We also present a\npositive-definite-preserving transformation that can be applied to any kernel\nand can also be used to derive positive definite EMD-based kernels and show\nthat the Jaccard index is simply the result of this transformation. Finally, we\nevaluate kernels based on EMI and the proposed transformation versus EMD in\nvarious computer vision tasks and show that EMD is generally inferior even with\nindefinite kernel techniques.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 21:51:09 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 21:16:08 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 14:31:18 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Gardner", "Andrew", ""], ["Duncan", "Christian A.", ""], ["Kanno", "Jinko", ""], ["Selmic", "Rastko R.", ""]]}, {"id": "1510.02847", "submitter": "Chicheng Zhang", "authors": "Chicheng Zhang, Kamalika Chaudhuri", "title": "Active Learning from Weak and Strong Labelers", "comments": "To appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An active learner is given a hypothesis class, a large set of unlabeled\nexamples and the ability to interactively query labels to an oracle of a subset\nof these examples; the goal of the learner is to learn a hypothesis in the\nclass that fits the data well by making as few label queries as possible.\n  This work addresses active learning with labels obtained from strong and weak\nlabelers, where in addition to the standard active learning setting, we have an\nextra weak labeler which may occasionally provide incorrect labels. An example\nis learning to classify medical images where either expensive labels may be\nobtained from a physician (oracle or strong labeler), or cheaper but\noccasionally incorrect labels may be obtained from a medical resident (weak\nlabeler). Our goal is to learn a classifier with low error on data labeled by\nthe oracle, while using the weak labeler to reduce the number of label queries\nmade to this labeler. We provide an active learning algorithm for this setting,\nestablish its statistical consistency, and analyze its label complexity to\ncharacterize when it can provide label savings over using the strong labeler\nalone.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 23:15:40 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 01:06:27 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Zhang", "Chicheng", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1510.02855", "submitter": "Abraham Heifets", "authors": "Izhar Wallach and Michael Dzamba and Abraham Heifets", "title": "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction\n  in Structure-based Drug Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks comprise a subclass of deep neural\nnetworks (DNN) with a constrained architecture that leverages the spatial and\ntemporal structure of the domain they model. Convolutional networks achieve the\nbest predictive performance in areas such as speech and image recognition by\nhierarchically composing simple local features into complex models. Although\nDNNs have been used in drug discovery for QSAR and ligand-based bioactivity\npredictions, none of these models have benefited from this powerful\nconvolutional architecture. This paper introduces AtomNet, the first\nstructure-based, deep convolutional neural network designed to predict the\nbioactivity of small molecules for drug discovery applications. We demonstrate\nhow to apply the convolutional concepts of feature locality and hierarchical\ncomposition to the modeling of bioactivity and chemical interactions. In\nfurther contrast to existing DNN techniques, we show that AtomNet's application\nof local convolutional filters to structural target information successfully\npredicts new active molecules for targets with no previously known modulators.\nFinally, we show that AtomNet outperforms previous docking approaches on a\ndiverse set of benchmarks by a large margin, achieving an AUC greater than 0.9\non 57.8% of the targets in the DUDE benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 00:09:00 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Wallach", "Izhar", ""], ["Dzamba", "Michael", ""], ["Heifets", "Abraham", ""]]}, {"id": "1510.02874", "submitter": "Sarath Chandar", "authors": "P. Prasanna, Sarath Chandar, Balaraman Ravindran", "title": "TSEB: More Efficient Thompson Sampling for Policy Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model-based solution approaches to the problem of learning in an unknown\nenvironment, exploring to learn the model parameters takes a toll on the\nregret. The optimal performance with respect to regret or PAC bounds is\nachievable, if the algorithm exploits with respect to reward or explores with\nrespect to the model parameters, respectively. In this paper, we propose TSEB,\na Thompson Sampling based algorithm with adaptive exploration bonus that aims\nto solve the problem with tighter PAC guarantees, while being cautious on the\nregret as well. The proposed approach maintains distributions over the model\nparameters which are successively refined with more experience. At any given\ntime, the agent solves a model sampled from this distribution, and the sampled\nreward distribution is skewed by an exploration bonus in order to generate more\ninformative exploration. The policy by solving is then used for generating more\nexperience that helps in updating the posterior over the model parameters. We\nprovide a detailed analysis of the PAC guarantees, and convergence of the\nproposed approach. We show that our adaptive exploration bonus encourages the\nadditional exploration required for better PAC bounds on the algorithm. We\nprovide empirical analysis on two different simulated domains.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 04:16:08 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Prasanna", "P.", ""], ["Chandar", "Sarath", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1510.02879", "submitter": "Aravind Srinivas", "authors": "Janarthanan Rajendran, Aravind Srinivas, Mitesh M. Khapra, P Prasanna,\n  Balaraman Ravindran", "title": "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive\n  Transfer from multiple sources in the same domain", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge from prior source tasks in solving a new target task\ncan be useful in several learning applications. The application of transfer\nposes two serious challenges which have not been adequately addressed. First,\nthe agent should be able to avoid negative transfer, which happens when the\ntransfer hampers or slows down the learning instead of helping it. Second, the\nagent should be able to selectively transfer, which is the ability to select\nand transfer from different and multiple source tasks for different parts of\nthe state space of the target task. We propose A2T (Attend, Adapt and\nTransfer), an attentive deep architecture which adapts and transfers from these\nsource tasks. Our model is generic enough to effect transfer of either policies\nor value functions. Empirical evaluations on different learning algorithms show\nthat A2T is an effective architecture for transfer by being able to avoid\nnegative transfer while transferring selectively from multiple source tasks in\nthe same domain.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 05:32:24 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2015 07:07:51 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2016 20:55:43 GMT"}, {"version": "v4", "created": "Tue, 27 Dec 2016 22:59:55 GMT"}, {"version": "v5", "created": "Tue, 18 Apr 2017 01:05:04 GMT"}, {"version": "v6", "created": "Mon, 21 Sep 2020 22:16:47 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Rajendran", "Janarthanan", ""], ["Srinivas", "Aravind", ""], ["Khapra", "Mitesh M.", ""], ["Prasanna", "P", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1510.02892", "submitter": "Tarek Amr", "authors": "Tarek Amr Abdallah, Beatriz de La Iglesia", "title": "Survey on Feature Selection", "comments": "Report, for Data Mining class during KDD Masters Degree at University\n  of East Anglia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Feature selection plays an important role in the data mining process. It is\nneeded to deal with the excessive number of features, which can become a\ncomputational burden on the learning algorithms. It is also necessary, even\nwhen computational resources are not scarce, since it improves the accuracy of\nthe machine learning tasks, as we will see in the upcoming sections. In this\nreview, we discuss the different feature selection approaches, and the relation\nbetween them and the various machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 08:54:48 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Abdallah", "Tarek Amr", ""], ["de La Iglesia", "Beatriz", ""]]}, {"id": "1510.02942", "submitter": "Baris Gecer", "authors": "Baris Gecer, Ozge Yalcinkaya, Onur Tasar and Selim Aksoy", "title": "Evaluation of Joint Multi-Instance Multi-Label Learning For Breast\n  Cancer Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance multi-label (MIML) learning is a challenging problem in many\naspects. Such learning approaches might be useful for many medical diagnosis\napplications including breast cancer detection and classification. In this\nstudy subset of digiPATH dataset (whole slide digital breast cancer\nhistopathology images) are used for training and evaluation of six\nstate-of-the-art MIML methods.\n  At the end, performance comparison of these approaches are given by means of\neffective evaluation metrics. It is shown that MIML-kNN achieve the best\nperformance that is %65.3 average precision, where most of other methods attain\nacceptable results as well.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 14:30:25 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Gecer", "Baris", ""], ["Yalcinkaya", "Ozge", ""], ["Tasar", "Onur", ""], ["Aksoy", "Selim", ""]]}, {"id": "1510.02969", "submitter": "Pooya Khorrami", "authors": "Pooya Khorrami, Tom Le Paine, Thomas S. Huang", "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression\n  Recognition?", "comments": "Accepted at ICCV 2015 CV4AC Workshop. Corrected numbers in Tables 2\n  and 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being the appearance-based classifier of choice in recent years,\nrelatively few works have examined how much convolutional neural networks\n(CNNs) can improve performance on accepted expression recognition benchmarks\nand, more importantly, examine what it is they actually learn. In this work,\nnot only do we show that CNNs can achieve strong performance, but we also\nintroduce an approach to decipher which portions of the face influence the\nCNN's predictions. First, we train a zero-bias CNN on facial expression data\nand achieve, to our knowledge, state-of-the-art performance on two expression\nrecognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto\nFace Dataset (TFD). We then qualitatively analyze the network by visualizing\nthe spatial patterns that maximally excite different neurons in the\nconvolutional layers and show how they resemble Facial Action Units (FAUs).\nFinally, we use the FAU labels provided in the CK+ dataset to verify that the\nFAUs observed in our filter visualizations indeed align with the subject's\nfacial movements.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 18:53:21 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 06:12:07 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 03:07:21 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Khorrami", "Pooya", ""], ["Paine", "Tom Le", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1510.02983", "submitter": "Becky Passonneau", "authors": "Boyi Xie and Rebecca J. Passonneau", "title": "OmniGraph: Rich Representation and Graph Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OmniGraph, a novel representation to support a range of NLP classification\ntasks, integrates lexical items, syntactic dependencies and frame semantic\nparses into graphs. Feature engineering is folded into the learning through\nconvolution graph kernel learning to explore different extents of the graph. A\nhigh-dimensional space of features includes individual nodes as well as complex\nsubgraphs. In experiments on a text-forecasting problem that predicts stock\nprice change from news for company mentions, OmniGraph beats several benchmarks\nbased on bag-of-words, syntactic dependencies, and semantic trees. The highly\nexpressive features OmniGraph discovers provide insights into the semantics\nacross distinct market sectors. To demonstrate the method's generality, we also\nreport its high performance results on a fine-grained sentiment corpus.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 21:22:00 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Xie", "Boyi", ""], ["Passonneau", "Rebecca J.", ""]]}, {"id": "1510.03009", "submitter": "Zhouhan Lin", "authors": "Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, Yoshua Bengio", "title": "Neural Networks with Few Multiplications", "comments": "Published as a conference paper at ICLR 2016. 9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most deep learning algorithms training is notoriously time consuming.\nSince most of the computation in training neural networks is typically spent on\nfloating point multiplications, we investigate an approach to training that\neliminates the need for most of these. Our method consists of two parts: First\nwe stochastically binarize weights to convert multiplications involved in\ncomputing hidden states to sign changes. Second, while back-propagating error\nderivatives, in addition to binarizing the weights, we quantize the\nrepresentations at each layer to convert the remaining multiplications into\nbinary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10,\nSVHN) show that this approach not only does not hurt classification performance\nbut can result in even better performance than standard stochastic gradient\ndescent training, paving the way to fast, hardware-friendly training of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 04:32:39 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 20:16:10 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2016 05:24:30 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Lin", "Zhouhan", ""], ["Courbariaux", "Matthieu", ""], ["Memisevic", "Roland", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1510.03130", "submitter": "Manish Purohit", "authors": "Hal Daum\\'e III, Samir Khuller, Manish Purohit, and Gregory Sanders", "title": "On Correcting Inputs: Inverse Optimization for Online Structured\n  Prediction", "comments": "Conference version to appear in FSTTCS, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm designers typically assume that the input data is correct, and then\nproceed to find \"optimal\" or \"sub-optimal\" solutions using this input data.\nHowever this assumption of correct data does not always hold in practice,\nespecially in the context of online learning systems where the objective is to\nlearn appropriate feature weights given some training samples. Such scenarios\nnecessitate the study of inverse optimization problems where one is given an\ninput instance as well as a desired output and the task is to adjust the input\ndata so that the given output is indeed optimal. Motivated by learning\nstructured prediction models, in this paper we consider inverse optimization\nwith a margin, i.e., we require the given output to be better than all other\nfeasible outputs by a desired margin. We consider such inverse optimization\nproblems for maximum weight matroid basis, matroid intersection, perfect\nmatchings, minimum cost maximum flows, and shortest paths and derive the first\nknown results for such problems with a non-zero margin. The effectiveness of\nthese algorithmic approaches to online learning for structured prediction is\nalso discussed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 03:33:47 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Khuller", "Samir", ""], ["Purohit", "Manish", ""], ["Sanders", "Gregory", ""]]}, {"id": "1510.03164", "submitter": "Purushottam Kar", "authors": "Shuai Li and Purushottam Kar", "title": "Context-Aware Bandits", "comments": "The paper has been withdrawn as the work has been superseded", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient Context-Aware clustering of Bandits (CAB) algorithm,\nwhich can capture collaborative effects. CAB can be easily deployed in a\nreal-world recommendation system, where multi-armed bandits have been shown to\nperform well in particular with respect to the cold-start problem. CAB utilizes\na context-aware clustering augmented by exploration-exploitation strategies.\nCAB dynamically clusters the users based on the content universe under\nconsideration. We give a theoretical analysis in the standard stochastic\nmulti-armed bandits setting. We show the efficiency of our approach on\nproduction and real-world datasets, demonstrate the scalability, and, more\nimportantly, the significant increased prediction performance against several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 07:04:16 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 05:47:32 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 16:18:43 GMT"}, {"version": "v4", "created": "Fri, 10 Jun 2016 20:51:08 GMT"}, {"version": "v5", "created": "Sun, 26 Feb 2017 15:53:30 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Li", "Shuai", ""], ["Kar", "Purushottam", ""]]}, {"id": "1510.03203", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer", "title": "VB calibration to improve the interface between phone recognizer and\n  i-vector extractor", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM training algorithm of the classical i-vector extractor is often\nincorrectly described as a maximum-likelihood method. The i-vector model is\nhowever intractable: the likelihood itself and the hidden-variable posteriors\nneeded for the EM algorithm cannot be computed in closed form. We show here\nthat the classical i-vector extractor recipe is actually a mean-field\nvariational Bayes (VB) recipe.\n  This theoretical VB interpretation turns out to be of further use, because it\nalso offers an interpretation of the newer phonetic i-vector extractor recipe,\nthereby unifying the two flavours of extractor.\n  More importantly, the VB interpretation is also practically useful: it\nsuggests ways of modifying existing i-vector extractors to make them more\naccurate. In particular, in existing methods, the approximate VB posterior for\nthe GMM states is fixed, while only the parameters of the generative model are\nadapted. Here we explore the possibility of also mildly adjusting (calibrating)\nthose posteriors, so that they better fit the generative model.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 09:48:43 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 16:22:23 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Br\u00fcmmer", "Niko", ""]]}, {"id": "1510.03317", "submitter": "Anastasia Paparrizou Ms", "authors": "Christian Bessiere, Luc De Raedt, Tias Guns, Lars Kotthoff, Mirco\n  Nanni, Siegfried Nijssen, Barry O'Sullivan, Anastasia Paparrizou, Dino\n  Pedreschi, Helmut Simonis", "title": "The Inductive Constraint Programming Loop", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint programming is used for a variety of real-world optimisation\nproblems, such as planning, scheduling and resource allocation problems. At the\nsame time, one continuously gathers vast amounts of data about these problems.\nCurrent constraint programming software does not exploit such data to update\nschedules, resources and plans. We propose a new framework, that we call the\nInductive Constraint Programming loop. In this approach data is gathered and\nanalyzed systematically, in order to dynamically revise and adapt constraints\nand optimization criteria. Inductive Constraint Programming aims at bridging\nthe gap between the areas of data mining and machine learning on the one hand,\nand constraint programming on the other hand.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 14:51:03 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Bessiere", "Christian", ""], ["De Raedt", "Luc", ""], ["Guns", "Tias", ""], ["Kotthoff", "Lars", ""], ["Nanni", "Mirco", ""], ["Nijssen", "Siegfried", ""], ["O'Sullivan", "Barry", ""], ["Paparrizou", "Anastasia", ""], ["Pedreschi", "Dino", ""], ["Simonis", "Helmut", ""]]}, {"id": "1510.03336", "submitter": "Subutai Ahmad", "authors": "Alexander Lavin, Subutai Ahmad", "title": "Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly\n  Benchmark", "comments": "14th International Conference on Machine Learning and Applications\n  (IEEE ICMLA), 2015. Fixed typo in equation and formatting", "journal-ref": null, "doi": "10.1109/ICMLA.2015.141", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of the world's data is streaming, time-series data, where anomalies give\nsignificant information in critical situations; examples abound in domains such\nas finance, IT, security, medical, and energy. Yet detecting anomalies in\nstreaming data is a difficult task, requiring detectors to process data in\nreal-time, not batches, and learn while simultaneously making predictions.\nThere are no benchmarks to adequately test and score the efficacy of real-time\nanomaly detectors. Here we propose the Numenta Anomaly Benchmark (NAB), which\nattempts to provide a controlled and repeatable environment of open-source\ntools to test and measure anomaly detection algorithms on streaming data. The\nperfect detector would detect all anomalies as soon as possible, trigger no\nfalse alarms, work with real-world time-series data across a variety of\ndomains, and automatically adapt to changing statistics. Rewarding these\ncharacteristics is formalized in NAB, using a scoring algorithm designed for\nstreaming data. NAB evaluates detectors on a benchmark dataset with labeled,\nreal-world time-series data. We present these components, and give results and\nanalyses for several open source, commercially-used algorithms. The goal for\nNAB is to provide a standard, open source framework with which the research\ncommunity can compare and evaluate different algorithms for detecting anomalies\nin streaming data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 15:30:34 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 23:09:58 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 20:52:44 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2015 17:17:06 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lavin", "Alexander", ""], ["Ahmad", "Subutai", ""]]}, {"id": "1510.03349", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng", "title": "Toward a Better Understanding of Leaderboard", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leaderboard in machine learning competitions is a tool to show the\nperformance of various participants and to compare them. However, the\nleaderboard quickly becomes no longer accurate, due to hack or overfitting.\nThis article gives two pieces of advice to prevent easy hack or overfitting. By\nfollowing these advice, we reach the conclusion that something like the Ladder\nleaderboard introduced in [blum2015ladder] is inevitable. With this\nunderstanding, we naturally simplify Ladder by eliminating its redundant\ncomputation and explain how to choose the parameter and interpret it. We also\nprove that the sample complexity is cubic to the desired precision of the\nleaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 16:12:49 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 13:12:56 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Zheng", "Wenjie", ""]]}, {"id": "1510.03370", "submitter": "Scott Garrabrant", "authors": "Scott Garrabrant, Siddharth Bhaskar, Abram Demski, Joanna Garrabrant,\n  George Koleszarik, Evan Lloyd", "title": "Asymptotic Logical Uncertainty and The Benford Test", "comments": null, "journal-ref": null, "doi": null, "report-no": "2015--11", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm A which assigns probabilities to logical sentences. For\nany simple infinite sequence of sentences whose truth-values appear\nindistinguishable from a biased coin that outputs \"true\" with probability p, we\nhave that the sequence of probabilities that A assigns to these sentences\nconverges to p.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 17:14:44 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Garrabrant", "Scott", ""], ["Bhaskar", "Siddharth", ""], ["Demski", "Abram", ""], ["Garrabrant", "Joanna", ""], ["Koleszarik", "George", ""], ["Lloyd", "Evan", ""]]}, {"id": "1510.03507", "submitter": "Kevin Moon", "authors": "Stephen V. Gliske, Kevin R. Moon, William C. Stacey, Alfred O. Hero\n  III", "title": "The intrinsic value of HFO features as a biomarker of epileptic activity", "comments": "5 pages, 5 figures", "journal-ref": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP), pp. 6290-6294, Mar. 2016", "doi": "10.1109/ICASSP.2016.7472887", "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High frequency oscillations (HFOs) are a promising biomarker of epileptic\nbrain tissue and activity. HFOs additionally serve as a prototypical example of\nchallenges in the analysis of discrete events in high-temporal resolution,\nintracranial EEG data. Two primary challenges are 1) dimensionality reduction,\nand 2) assessing feasibility of classification. Dimensionality reduction\nassumes that the data lie on a manifold with dimension less than that of the\nfeature space. However, previous HFO analyses have assumed a linear manifold,\nglobal across time, space (i.e. recording electrode/channel), and individual\npatients. Instead, we assess both a) whether linear methods are appropriate and\nb) the consistency of the manifold across time, space, and patients. We also\nestimate bounds on the Bayes classification error to quantify the distinction\nbetween two classes of HFOs (those occurring during seizures and those\noccurring due to other processes). This analysis provides the foundation for\nfuture clinical use of HFO features and buides the analysis for other discrete\nevents, such as individual action potentials or multi-unit activity.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 01:57:12 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Gliske", "Stephen V.", ""], ["Moon", "Kevin R.", ""], ["Stacey", "William C.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1510.03528", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang, Jason D. Lee, Michael I. Jordan", "title": "$\\ell_1$-regularized Neural Networks are Improperly Learnable in\n  Polynomial Time", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the improper learning of multi-layer neural networks. Suppose that\nthe neural network to be learned has $k$ hidden layers and that the\n$\\ell_1$-norm of the incoming weights of any neuron is bounded by $L$. We\npresent a kernel-based method, such that with probability at least $1 -\n\\delta$, it learns a predictor whose generalization error is at most $\\epsilon$\nworse than that of the neural network. The sample complexity and the time\ncomplexity of the presented method are polynomial in the input dimension and in\n$(1/\\epsilon,\\log(1/\\delta),F(k,L))$, where $F(k,L)$ is a function depending on\n$(k,L)$ and on the activation function, independent of the number of neurons.\nThe algorithm applies to both sigmoid-like activation functions and ReLU-like\nactivation functions. It implies that any sufficiently sparse neural network is\nlearnable in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 04:36:09 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Zhang", "Yuchen", ""], ["Lee", "Jason D.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1510.03623", "submitter": "Sai Zhang", "authors": "Sai Zhang", "title": "Elastic regularization in restricted Boltzmann machines: Dealing with\n  $p\\gg N$", "comments": "This paper has been withdrawn by the author due to a critical error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines (RBMs) are endowed with the universal power of\nmodeling (binary) joint distributions. Meanwhile, as a result of their\nconfining network structure, training RBMs confronts less difficulties\n(compared with more complicated models, e.g., Boltzmann machines) when dealing\nwith approximation and inference issues. However, in certain computational\nbiology scenarios, such as the cancer data analysis, employing RBMs to model\ndata features may lose its efficacy due to the \"$p\\gg N$\" problem, in which the\nnumber of features/predictors is much larger than the sample size. The \"$p\\gg\nN$\" problem puts the bias-variance trade-off in a more crucial place when\ndesigning statistical learning methods. In this manuscript, we try to address\nthis problem by proposing a novel RBM model, called elastic restricted\nBoltzmann machine (eRBM), which incorporates the elastic regularization term\ninto the likelihood/cost function. We provide several theoretical analysis on\nthe superiority of our model. Furthermore, attributed to the classic\ncontrastive divergence (CD) algorithm, eRBMs can be trained efficiently. Our\nnovel model is a promising method for future cancer data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 11:14:03 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 12:19:13 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Zhang", "Sai", ""]]}, {"id": "1510.03820", "submitter": "Ye Zhang", "authors": "Ye Zhang and Byron Wallace", "title": "A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional\n  Neural Networks for Sentence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have recently achieved remarkably strong\nperformance on the practically important task of sentence classification (kim\n2014, kalchbrenner 2014, johnson 2014). However, these models require\npractitioners to specify an exact model architecture and set accompanying\nhyperparameters, including the filter region size, regularization parameters,\nand so on. It is currently unknown how sensitive model performance is to\nchanges in these configurations for the task of sentence classification. We\nthus conduct a sensitivity analysis of one-layer CNNs to explore the effect of\narchitecture components on model performance; our aim is to distinguish between\nimportant and comparatively inconsequential design decisions for sentence\nclassification. We focus on one-layer CNNs (to the exclusion of more complex\nmodels) due to their comparative simplicity and strong empirical performance,\nwhich makes it a modern standard baseline method akin to Support Vector Machine\n(SVMs) and logistic regression. We derive practical advice from our extensive\nempirical results for those interested in getting the most out of CNNs for\nsentence classification in real world settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 19:00:57 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 19:26:44 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2016 07:01:52 GMT"}, {"version": "v4", "created": "Wed, 6 Apr 2016 23:20:27 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Zhang", "Ye", ""], ["Wallace", "Byron", ""]]}, {"id": "1510.03826", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang, Tim Oates, James Lo", "title": "Adopting Robustness and Optimality in Fitting and Learning", "comments": "This paper has been withdrawn by the authors due to some errors and\n  confusions in terminology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalized a modified exponentialized estimator by pushing the\nrobust-optimal (RO) index $\\lambda$ to $-\\infty$ for achieving robustness to\noutliers by optimizing a quasi-Minimin function. The robustness is realized and\ncontrolled adaptively by the RO index without any predefined threshold.\nOptimality is guaranteed by expansion of the convexity region in the Hessian\nmatrix to largely avoid local optima. Detailed quantitative analysis on both\nrobustness and optimality are provided. The results of proposed experiments on\nfitting tasks for three noisy non-convex functions and the digits recognition\ntask on the MNIST dataset consolidate the conclusions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 19:14:43 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2015 04:52:35 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 21:38:52 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Wang", "Zhiguang", ""], ["Oates", "Tim", ""], ["Lo", "James", ""]]}, {"id": "1510.03925", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan", "title": "On Equivalence of Martingale Tail Bounds and Deterministic Regret\n  Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an equivalence of (i) deterministic pathwise statements appearing in\nthe online learning literature (termed \\emph{regret bounds}), (ii)\nhigh-probability tail bounds for the supremum of a collection of martingales\n(of a specific form arising from uniform laws of large numbers for\nmartingales), and (iii) in-expectation bounds for the supremum. By virtue of\nthe equivalence, we prove exponential tail bounds for norms of Banach space\nvalued martingales via deterministic regret bounds for the online mirror\ndescent algorithm with an adaptive step size. We extend these results beyond\nthe linear structure of the Banach space: we define a notion of\n\\emph{martingale type} for general classes of real-valued functions and show\nits equivalence (up to a logarithmic factor) to various sequential complexities\nof the class (in particular, the sequential Rademacher complexity and its\noffset version). For classes with the general martingale type 2, we exhibit a\nfiner notion of variation that allows partial adaptation to the function\nindexing the martingale. Our proof technique rests on sequential symmetrization\nand on certifying the \\emph{existence} of regret minimization strategies for\ncertain online prediction problems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 23:23:58 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1510.04130", "submitter": "Jaroslav Fowkes", "authors": "Jaroslav Fowkes and Charles Sutton", "title": "A Bayesian Network Model for Interesting Itemsets", "comments": "Supplementary material attached as Ancillary File; in PKDD 2016:\n  European Conference on Machine Learning and Knowledge Discovery in Databases", "journal-ref": null, "doi": "10.1007/978-3-319-46227-1_26", "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining itemsets that are the most interesting under a statistical model of\nthe underlying data is a commonly used and well-studied technique for\nexploratory data analysis, with the most recent interestingness models\nexhibiting state of the art performance. Continuing this highly promising line\nof work, we propose the first, to the best of our knowledge, generative model\nover itemsets, in the form of a Bayesian network, and an associated novel\nmeasure of interestingness. Our model is able to efficiently infer interesting\nitemsets directly from the transaction database using structural EM, in which\nthe E-step employs the greedy approximation to weighted set cover. Our approach\nis theoretically simple, straightforward to implement, trivially parallelizable\nand retrieves itemsets whose quality is comparable to, if not better than,\nexisting state of the art algorithms as we demonstrate on several real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 14:55:17 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 11:15:30 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Fowkes", "Jaroslav", ""], ["Sutton", "Charles", ""]]}, {"id": "1510.04163", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Chong Wang, Eric Xing", "title": "Embarrassingly Parallel Variational Inference in Nonconjugate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a parallel variational inference (VI) procedure for use in\ndata-distributed settings, where each machine only has access to a subset of\ndata and runs VI independently, without communicating with other machines. This\ntype of \"embarrassingly parallel\" procedure has recently been developed for\nMCMC inference algorithms; however, in many cases it is not possible to\ndirectly extend this procedure to VI methods without requiring certain\nrestrictive exponential family conditions on the form of the model.\nFurthermore, most existing (nonparallel) VI methods are restricted to use on\nconditionally conjugate models, which limits their applicability. To combat\nthese issues, we make use of the recently proposed nonparametric VI to\nfacilitate an embarrassingly parallel VI procedure that can be applied to a\nwider scope of models, including to nonconjugate models. We derive our\nembarrassingly parallel VI algorithm, analyze our method theoretically, and\ndemonstrate our method empirically on a few nonconjugate models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 15:48:19 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wang", "Chong", ""], ["Xing", "Eric", ""]]}, {"id": "1510.04189", "submitter": "Arild N{\\o}kland", "authors": "Arild N{\\o}kland", "title": "Improving Back-Propagation by Adding an Adversarial Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The back-propagation algorithm is widely used for learning in artificial\nneural networks. A challenge in machine learning is to create models that\ngeneralize to new data samples not seen in the training data. Recently, a\ncommon flaw in several machine learning algorithms was discovered: small\nperturbations added to the input data lead to consistent misclassification of\ndata samples. Samples that easily mislead the model are called adversarial\nexamples. Training a \"maxout\" network on adversarial examples has shown to\ndecrease this vulnerability, but also increase classification performance. This\npaper shows that adversarial training has a regularizing effect also in\nnetworks with logistic, hyperbolic tangent and rectified linear units. A simple\nextension to the back-propagation method is proposed, that adds an adversarial\ngradient to the training. The extension requires an additional forward and\nbackward pass to calculate a modified input sample, or mini batch, used as\ninput for standard back-propagation learning. The first experimental results on\nMNIST show that the \"adversarial back-propagation\" method increases the\nresistance to adversarial examples and boosts the classification performance.\nThe extension reduces the classification error on the permutation invariant\nMNIST from 1.60% to 0.95% in a logistic network, and from 1.40% to 0.78% in a\nnetwork with rectified linear units. Results on CIFAR-10 indicate that the\nmethod has a regularizing effect similar to dropout in fully connected\nnetworks. Based on these promising results, adversarial back-propagation is\nproposed as a stand-alone regularizing method that should be further\ninvestigated.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 16:27:28 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 16:35:13 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["N\u00f8kland", "Arild", ""]]}, {"id": "1510.04356", "submitter": "Shuchin Aeron", "authors": "Shuchin Aeron and Eric Kernfeld", "title": "Group-Invariant Subspace Clustering", "comments": "Proceedings of Allerton 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of group invariant subspace clustering\nwhere the data is assumed to come from a union of group-invariant subspaces of\na vector space, i.e. subspaces which are invariant with respect to action of a\ngiven group. Algebraically, such group-invariant subspaces are also referred to\nas submodules. Similar to the well known Sparse Subspace Clustering approach\nwhere the data is assumed to come from a union of subspaces, we analyze an\nalgorithm which, following a recent work [1], we refer to as Sparse Sub-module\nClustering (SSmC). The method is based on finding group-sparse\nself-representation of data points. In this paper we primarily derive general\nconditions under which such a group-invariant subspace identification is\npossible. In particular we extend the geometric analysis in [2] and in the\nprocess we identify a related problem in geometric functional analysis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 00:05:21 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Aeron", "Shuchin", ""], ["Kernfeld", "Eric", ""]]}, {"id": "1510.04373", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and David Balduzzi and W. Bastiaan Kleijn and Mengjie\n  Zhang", "title": "Scatter Component Analysis: A Unified Framework for Domain Adaptation\n  and Domain Generalization", "comments": "to appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses classification tasks on a particular target domain in\nwhich labeled training data are only available from source domains different\nfrom (but related to) the target. Two closely related frameworks, domain\nadaptation and domain generalization, are concerned with such tasks, where the\nonly difference between those frameworks is the availability of the unlabeled\ntarget data: domain adaptation can leverage unlabeled target information, while\ndomain generalization cannot. We propose Scatter Component Analyis (SCA), a\nfast representation learning algorithm that can be applied to both domain\nadaptation and domain generalization. SCA is based on a simple geometrical\nmeasure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA\nfinds a representation that trades between maximizing the separability of\nclasses, minimizing the mismatch between domains, and maximizing the\nseparability of data; each of which is quantified through scatter. The\noptimization problem of SCA can be reduced to a generalized eigenvalue problem,\nwhich results in a fast and exact solution. Comprehensive experiments on\nbenchmark cross-domain object recognition datasets verify that SCA performs\nmuch faster than several state-of-the-art algorithms and also provides\nstate-of-the-art classification accuracy in both domain adaptation and domain\ngeneralization. We also show that scatter can be used to establish a\ntheoretical generalization bound in the case of domain adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 01:41:12 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 21:35:08 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Balduzzi", "David", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1510.04390", "submitter": "Manolis Tsakiris", "authors": "Manolis C. Tsakiris and Rene Vidal", "title": "Dual Principal Component Pursuit", "comments": "fixed two typos in section 7.3", "journal-ref": "Journal of Machine Learning Research 19 (2018) 1-49", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a linear subspace from data corrupted by\noutliers. Classical approaches are typically designed for the case in which the\nsubspace dimension is small relative to the ambient dimension. Our approach\nworks with a dual representation of the subspace and hence aims to find its\northogonal complement; as such, it is particularly suitable for subspaces whose\ndimension is close to the ambient dimension (subspaces of high relative\ndimension). We pose the problem of computing normal vectors to the inlier\nsubspace as a non-convex $\\ell_1$ minimization problem on the sphere, which we\ncall Dual Principal Component Pursuit (DPCP) problem. We provide theoretical\nguarantees under which every global solution to DPCP is a vector in the\northogonal complement of the inlier subspace. Moreover, we relax the non-convex\nDPCP problem to a recursion of linear programs whose solutions are shown to\nconverge in a finite number of steps to a vector orthogonal to the subspace. In\nparticular, when the inlier subspace is a hyperplane, the solutions to the\nrecursion of linear programs converge to the global minimum of the non-convex\nDPCP problem in a finite number of steps. We also propose algorithms based on\nalternating minimization and iteratively re-weighted least squares, which are\nsuitable for dealing with large-scale data. Experiments on synthetic data show\nthat the proposed methods are able to handle more outliers and higher relative\ndimensions than current state-of-the-art methods, while experiments in the\ncontext of the three-view geometry problem in computer vision suggest that the\nproposed methods can be a useful or even superior alternative to traditional\nRANSAC-based approaches for computer vision and other applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 03:50:01 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 21:42:05 GMT"}, {"version": "v3", "created": "Sat, 22 Jul 2017 16:24:27 GMT"}, {"version": "v4", "created": "Sat, 4 Aug 2018 18:53:49 GMT"}, {"version": "v5", "created": "Thu, 7 Nov 2019 11:11:23 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Tsakiris", "Manolis C.", ""], ["Vidal", "Rene", ""]]}, {"id": "1510.04396", "submitter": "Manolis Tsakiris", "authors": "Manolis C. Tsakiris and Rene Vidal", "title": "Filtrated Spectral Algebraic Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic Subspace Clustering (ASC) is a simple and elegant method based on\npolynomial fitting and differentiation for clustering noiseless data drawn from\nan arbitrary union of subspaces. In practice, however, ASC is limited to\nequi-dimensional subspaces because the estimation of the subspace dimension via\nalgebraic methods is sensitive to noise. This paper proposes a new ASC\nalgorithm that can handle noisy data drawn from subspaces of arbitrary\ndimensions. The key ideas are (1) to construct, at each point, a decreasing\nsequence of subspaces containing the subspace passing through that point; (2)\nto use the distances from any other point to each subspace in the sequence to\nconstruct a subspace clustering affinity, which is superior to alternative\naffinities both in theory and in practice. Experiments on the Hopkins 155\ndataset demonstrate the superiority of the proposed method with respect to\nsparse and low rank subspace clustering methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 04:12:37 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Tsakiris", "Manolis C.", ""], ["Vidal", "Rene", ""]]}, {"id": "1510.04454", "submitter": "Yao Ma", "authors": "Yao Ma, Hao Zhang, Masashi Sugiyama", "title": "Online Markov decision processes with policy iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online Markov decision process (MDP) is a generalization of the classical\nMarkov decision process that incorporates changing reward functions. In this\npaper, we propose practical online MDP algorithms with policy iteration and\ntheoretically establish a sublinear regret bound. A notable advantage of the\nproposed algorithm is that it can be easily combined with function\napproximation, and thus large and possibly continuous state spaces can be\nefficiently handled. Through experiments, we demonstrate the usefulness of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 09:19:49 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Ma", "Yao", ""], ["Zhang", "Hao", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1510.04609", "submitter": "Bharat Singh", "authors": "Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, and Gavin\n  Taylor", "title": "Layer-Specific Adaptive Learning Rates for Deep Networks", "comments": "ICMLA 2015, deep learning, adaptive learning rates for training,\n  layer specific learning rate", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of deep learning architectures is resulting in\ntraining time requiring weeks or even months. This slow training is due in part\nto vanishing gradients, in which the gradients used by back-propagation are\nextremely large for weights connecting deep layers (layers near the output\nlayer), and extremely small for shallow layers (near the input layer); this\nresults in slow learning in the shallow layers. Additionally, it has also been\nshown that in highly non-convex problems, such as deep neural networks, there\nis a proliferation of high-error low curvature saddle points, which slows down\nlearning dramatically. In this paper, we attempt to overcome the two above\nproblems by proposing an optimization method for training deep neural networks\nwhich uses learning rates which are both specific to each layer in the network\nand adaptive to the curvature of the function, increasing the learning rate at\nlow curvature points. This enables us to speed up learning in the shallow\nlayers of the network and quickly escape high-error low curvature saddle\npoints. We test our method on standard image classification datasets such as\nMNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy\nas well as reduces the required training time over standard algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 16:31:46 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Singh", "Bharat", ""], ["De", "Soham", ""], ["Zhang", "Yangmuzi", ""], ["Goldstein", "Thomas", ""], ["Taylor", "Gavin", ""]]}, {"id": "1510.04709", "submitter": "Desmond Elliott", "authors": "Desmond Elliott, Stella Frank, Eva Hasler", "title": "Multilingual Image Description with Neural Sequence Models", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach to multi-language image description\nbringing together insights from neural machine translation and neural image\ndescription. To create a description of an image for a given target language,\nour sequence generation models condition on feature vectors from the image, the\ndescription from the source language, and/or a multimodal vector computed over\nthe image and a description in the source language. In image description\nexperiments on the IAPR-TC12 dataset of images aligned with English and German\nsentences, we find significant and substantial improvements in BLEU4 and Meteor\nscores for models trained over multiple languages, compared to a monolingual\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 20:29:21 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 17:04:35 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Elliott", "Desmond", ""], ["Frank", "Stella", ""], ["Hasler", "Eva", ""]]}, {"id": "1510.04747", "submitter": "Yang Shi", "authors": "Animashree Anandkumar, Prateek Jain, Yang Shi, U. N. Niranjan", "title": "Tensor vs Matrix Methods: Robust Tensor Decomposition under Block Sparse\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust tensor CP decomposition involves decomposing a tensor into low rank\nand sparse components. We propose a novel non-convex iterative algorithm with\nguaranteed recovery. It alternates between low-rank CP decomposition through\ngradient ascent (a variant of the tensor power method), and hard thresholding\nof the residual. We prove convergence to the globally optimal solution under\nnatural incoherence conditions on the low rank component, and bounded level of\nsparse perturbations. We compare our method with natural baselines which apply\nrobust matrix PCA either to the {\\em flattened} tensor, or to the matrix slices\nof the tensor. Our method can provably handle a far greater level of\nperturbation when the sparse tensor is block-structured. This naturally occurs\nin many applications such as the activity detection task in videos. Our\nexperiments validate these findings. Thus, we establish that tensor methods can\ntolerate a higher level of gross corruptions compared to matrix methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 23:40:13 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 00:53:13 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2015 05:02:03 GMT"}, {"version": "v4", "created": "Sat, 14 Nov 2015 21:54:08 GMT"}, {"version": "v5", "created": "Sun, 27 Dec 2015 03:06:51 GMT"}, {"version": "v6", "created": "Fri, 22 Jan 2016 22:41:21 GMT"}, {"version": "v7", "created": "Wed, 27 Apr 2016 05:19:21 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Jain", "Prateek", ""], ["Shi", "Yang", ""], ["Niranjan", "U. N.", ""]]}, {"id": "1510.04781", "submitter": "Haohan Wang", "authors": "Haohan Wang and Bhiksha Raj", "title": "A Survey: Time Travel in Deep Learning Space: An Introduction to Deep\n  Learning Models and How Deep Learning Models Evolved from the Initial Ideas", "comments": "43 pages, 31 figures. Fix typos in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report will show the history of deep learning evolves. It will trace\nback as far as the initial belief of connectionism modelling of brain, and come\nback to look at its early stage realization: neural networks. With the\nbackground of neural network, we will gradually introduce how convolutional\nneural network, as a representative of deep discriminative models, is developed\nfrom neural networks, together with many practical techniques that can help in\noptimization of neural networks. On the other hand, we will also trace back to\nsee the evolution history of deep generative models, to see how researchers\nbalance the representation power and computation complexity to reach Restricted\nBoltzmann Machine and eventually reach Deep Belief Nets. Further, we will also\nlook into the development history of modelling time series data with neural\nnetworks. We start with Time Delay Neural Networks and move further to\ncurrently famous model named Recurrent Neural Network and its extension Long\nShort Term Memory. We will also briefly look into how to construct deep\nrecurrent neural networks. Finally, we will conclude this report with some\ninteresting open-ended questions of deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 05:37:06 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 19:41:13 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Wang", "Haohan", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1510.04811", "submitter": "Oscar Beijbom Dr", "authors": "Oscar Beijbom and Judy Hoffman and Evan Yao and Trevor Darrell and\n  Alberto Rodriguez-Ramirez and Manuel Gonzalez-Rivero and Ove Hoegh - Guldberg", "title": "Quantification in-the-wild: data-sets and baselines", "comments": "This report was prsented at the NIPS 2015 workshop on Transfer and\n  Multi-Task Learning: Trends and New Perspectives. It is 4 pages + 1 page of\n  references followed by a 6 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification is the task of estimating the class-distribution of a\ndata-set. While typically considered as a parameter estimation problem with\nstrict assumptions on the data-set shift, we consider quantification\nin-the-wild, on two large scale data-sets from marine ecology: a survey of\nCaribbean coral reefs, and a plankton time series from Martha's Vineyard\nCoastal Observatory. We investigate several quantification methods from the\nliterature and indicate opportunities for future work. In particular, we show\nthat a deep neural network can be fine-tuned on a very limited amount of data\n(25 - 100 samples) to outperform alternative methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 08:06:26 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 07:46:08 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Beijbom", "Oscar", ""], ["Hoffman", "Judy", ""], ["Yao", "Evan", ""], ["Darrell", "Trevor", ""], ["Rodriguez-Ramirez", "Alberto", ""], ["Gonzalez-Rivero", "Manuel", ""], ["Guldberg", "Ove Hoegh -", ""]]}, {"id": "1510.04815", "submitter": "Wenzhe Li", "authors": "Wenzhe Li, Sungjin Ahn, Max Welling", "title": "Scalable MCMC for Mixed Membership Stochastic Blockmodels", "comments": "9 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stochastic gradient Markov chain Monte Carlo (SG-MCMC) algorithm\nfor scalable inference in mixed-membership stochastic blockmodels (MMSB). Our\nalgorithm is based on the stochastic gradient Riemannian Langevin sampler and\nachieves both faster speed and higher accuracy at every iteration than the\ncurrent state-of-the-art algorithm based on stochastic variational inference.\nIn addition we develop an approximation that can handle models that entertain a\nvery large number of communities. The experimental results show that SG-MCMC\nstrictly dominates competing algorithms in all cases.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 08:32:29 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2015 02:14:13 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Li", "Wenzhe", ""], ["Ahn", "Sungjin", ""], ["Welling", "Max", ""]]}, {"id": "1510.04822", "submitter": "Massil Achab", "authors": "Massil Achab (CMAP), Agathe Guilloux (LSTA), St\\'ephane Ga\\\"iffas\n  (CMAP) and Emmanuel Bacry (CMAP)", "title": "SGD with Variance Reduction beyond Empirical Risk Minimization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a doubly stochastic proximal gradient algorithm for optimizing a\nfinite average of smooth convex functions, whose gradients depend on\nnumerically expensive expectations. Our main motivation is the acceleration of\nthe optimization of the regularized Cox partial-likelihood (the core model used\nin survival analysis), but our algorithm can be used in different settings as\nwell. The proposed algorithm is doubly stochastic in the sense that gradient\nsteps are done using stochastic gradient descent (SGD) with variance reduction,\nwhere the inner expectations are approximated by a Monte-Carlo Markov-Chain\n(MCMC) algorithm. We derive conditions on the MCMC number of iterations\nguaranteeing convergence, and obtain a linear rate of convergence under strong\nconvexity and a sublinear rate without this assumption. We illustrate the fact\nthat our algorithm improves the state-of-the-art solver for regularized Cox\npartial-likelihood on several datasets from survival analysis.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 09:32:24 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 19:45:58 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 09:43:23 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Achab", "Massil", "", "CMAP"], ["Guilloux", "Agathe", "", "LSTA"], ["Ga\u00efffas", "St\u00e9phane", "", "CMAP"], ["Bacry", "Emmanuel", "", "CMAP"]]}, {"id": "1510.04905", "submitter": "Marek Petrik", "authors": "Stephen Becker, Ban Kawas, Marek Petrik, Karthikeyan N. Ramamurthy", "title": "Robust Partially-Compressed Least-Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized matrix compression techniques, such as the Johnson-Lindenstrauss\ntransform, have emerged as an effective and practical way for solving\nlarge-scale problems efficiently. With a focus on computational efficiency,\nhowever, forsaking solutions quality and accuracy becomes the trade-off. In\nthis paper, we investigate compressed least-squares problems and propose new\nmodels and algorithms that address the issue of error and noise introduced by\ncompression. While maintaining computational efficiency, our models provide\nrobust solutions that are more accurate--relative to solutions of uncompressed\nleast-squares--than those of classical compressed variants. We introduce tools\nfrom robust optimization together with a form of partial compression to improve\nthe error-time trade-offs of compressed least-squares solvers. We develop an\nefficient solution algorithm for our Robust Partially-Compressed (RPC) model\nbased on a reduction to a one-dimensional search. We also derive the first\napproximation error bounds for Partially-Compressed least-squares solutions.\nEmpirical results comparing numerous alternatives suggest that robust and\npartially compressed solutions are effectively insulated against aggressive\nrandomized transforms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 14:59:04 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Becker", "Stephen", ""], ["Kawas", "Ban", ""], ["Petrik", "Marek", ""], ["Ramamurthy", "Karthikeyan N.", ""]]}, {"id": "1510.04931", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "Bad Universal Priors and Notions of Optimality", "comments": "COLT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A big open question of algorithmic information theory is the choice of the\nuniversal Turing machine (UTM). For Kolmogorov complexity and Solomonoff\ninduction we have invariance theorems: the choice of the UTM changes bounds\nonly by a constant. For the universally intelligent agent AIXI (Hutter, 2005)\nno invariance theorem is known. Our results are entirely negative: we discuss\ncases in which unlucky or adversarial choices of the UTM cause AIXI to\nmisbehave drastically. We show that Legg-Hutter intelligence and thus balanced\nPareto optimality is entirely subjective, and that every policy is Pareto\noptimal in the class of all computable environments. This undermines all\nexisting optimality properties for AIXI. While it may still serve as a gold\nstandard for AI, our results imply that AIXI is a relative theory, dependent on\nthe choice of the UTM.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 16:22:23 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1510.04935", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Lorenzo Rosasco, Tomaso Poggio", "title": "Holographic Embeddings of Knowledge Graphs", "comments": "To appear in AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning embeddings of entities and relations is an efficient and versatile\nmethod to perform machine learning on relational data such as knowledge graphs.\nIn this work, we propose holographic embeddings (HolE) to learn compositional\nvector space representations of entire knowledge graphs. The proposed method is\nrelated to holographic models of associative memory in that it employs circular\ncorrelation to create compositional representations. By using correlation as\nthe compositional operator HolE can capture rich interactions but\nsimultaneously remains efficient to compute, easy to train, and scalable to\nvery large datasets. In extensive experiments we show that holographic\nembeddings are able to outperform state-of-the-art methods for link prediction\nin knowledge graphs and relational learning benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 16:29:07 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 18:05:52 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Nickel", "Maximilian", ""], ["Rosasco", "Lorenzo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1510.04953", "submitter": "Benjamin Krause", "authors": "Ben Krause", "title": "Optimizing and Contrasting Recurrent Neural Network Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have long been recognized for their\npotential to model complex time series. However, it remains to be determined\nwhat optimization techniques and recurrent architectures can be used to best\nrealize this potential. The experiments presented take a deep look into Hessian\nfree optimization, a powerful second order optimization method that has shown\npromising results, but still does not enjoy widespread use. This algorithm was\nused to train to a number of RNN architectures including standard RNNs, long\nshort-term memory, multiplicative RNNs, and stacked RNNs on the task of\ncharacter prediction. The insights from these experiments led to the creation\nof a new multiplicative LSTM hybrid architecture that outperformed both LSTM\nand multiplicative RNNs. When tested on a larger scale, multiplicative LSTM\nachieved character level modelling results competitive with the state of the\nart for RNNs using very different methodology.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 17:16:14 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Krause", "Ben", ""]]}, {"id": "1510.05034", "submitter": "Yu Wang", "authors": "Kumpati S. Narendra, Snehasis Mukhopadyhay, and Yu Wang", "title": "Improving the Speed of Response of Learning Algorithms Using Multiple\n  Models", "comments": "7 Pages, submitted to American Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the first of a series of papers that the authors propose to write on\nthe subject of improving the speed of response of learning systems using\nmultiple models. During the past two decades, the first author has worked on\nnumerous methods for improving the stability, robustness, and performance of\nadaptive systems using multiple models and the other authors have collaborated\nwith him on some of them. Independently, they have also worked on several\nlearning methods, and have considerable experience with their advantages and\nlimitations. In particular, they are well aware that it is common knowledge\nthat machine learning is in general very slow. Numerous attempts have been made\nby researchers to improve the speed of convergence of algorithms in different\ncontexts. In view of the success of multiple model based methods in improving\nthe speed of convergence in adaptive systems, the authors believe that the same\napproach will also prove fruitful in the domain of learning. In this paper, a\nfirst attempt is made to use multiple models for improving the speed of\nresponse of the simplest learning schemes that have been studied. i.e. Learning\nAutomata.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 21:49:30 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 01:00:33 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Narendra", "Kumpati S.", ""], ["Mukhopadyhay", "Snehasis", ""], ["Wang", "Yu", ""]]}, {"id": "1510.05043", "submitter": "Sanjoy Dasgupta", "authors": "Sanjoy Dasgupta", "title": "A cost function for similarity-based hierarchical clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of algorithms for hierarchical clustering has been hampered\nby a shortage of precise objective functions. To help address this situation,\nwe introduce a simple cost function on hierarchies over a set of points, given\npairwise similarities between those points. We show that this criterion behaves\nsensibly in canonical instances and that it admits a top-down construction\nprocedure with a provably good approximation ratio.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 22:48:28 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Dasgupta", "Sanjoy", ""]]}, {"id": "1510.05067", "submitter": "Qianli Liao", "authors": "Qianli Liao, Joel Z. Leibo, Tomaso Poggio", "title": "How Important is Weight Symmetry in Backpropagation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient backpropagation (BP) requires symmetric feedforward and feedback\nconnections -- the same weights must be used for forward and backward passes.\nThis \"weight transport problem\" (Grossberg 1987) is thought to be one of the\nmain reasons to doubt BP's biologically plausibility. Using 15 different\nclassification datasets, we systematically investigate to what extent BP really\ndepends on weight symmetry. In a study that turned out to be surprisingly\nsimilar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014)\nbut orthogonal in its results, our experiments indicate that: (1) the\nmagnitudes of feedback weights do not matter to performance (2) the signs of\nfeedback weights do matter -- the more concordant signs between feedforward and\ntheir corresponding feedback connections, the better (3) with feedback weights\nhaving random magnitudes and 100% concordant signs, we were able to achieve the\nsame or even better performance than SGD. (4) some\nnormalizations/stabilizations are indispensable for such asymmetric BP to work,\nnamely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a \"Batch\nManhattan\" (BM) update rule.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 03:49:05 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2015 16:55:06 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2015 01:49:38 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2016 08:35:58 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Liao", "Qianli", ""], ["Leibo", "Joel Z.", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1510.05214", "submitter": "Or Zuk", "authors": "Tom Hope, Avishai Wagner and Or Zuk", "title": "Clustering Noisy Signals with Structured Sparsity Using Time-Frequency\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and efficient time-series clustering framework\nparticularly suited for low Signal-to-Noise Ratio (SNR), by simultaneous\nsmoothing and dimensionality reduction aimed at preserving clustering\ninformation. We extend the sparse K-means algorithm by incorporating structured\nsparsity, and use it to exploit the multi-scale property of wavelets and group\nstructure in multivariate signals. Finally, we extract features invariant to\ntranslation and scaling with the scattering transform, which corresponds to a\nconvolutional network with filters given by a wavelet operator, and use the\nnetwork's structure in sparse clustering. By promoting sparsity, this transform\ncan yield a low-dimensional representation of signals that gives improved\nclustering results on several real datasets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 09:41:50 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Hope", "Tom", ""], ["Wagner", "Avishai", ""], ["Zuk", "Or", ""]]}, {"id": "1510.05237", "submitter": "Vijay Gadepally", "authors": "Brendan Gavin and Vijay Gadepally and Jeremy Kepner", "title": "Large Enforced Sparse Non-Negative Matrix Factorization", "comments": "9 pages", "journal-ref": null, "doi": "10.1109/IPDPSW.2016.58", "report-no": null, "categories": "cs.LG cs.NA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is a common method for generating\ntopic models from text data. NMF is widely accepted for producing good results\ndespite its relative simplicity of implementation and ease of computation. One\nchallenge with applying NMF to large datasets is that intermediate matrix\nproducts often become dense, stressing the memory and compute elements of a\nsystem. In this article, we investigate a simple but powerful modification of a\ncommon NMF algorithm that enforces the generation of sparse intermediate and\noutput matrices. This method enables the application of NMF to large datasets\nthrough improved memory and compute performance. Further, we demonstrate\nempirically that this method of enforcing sparsity in the NMF either preserves\nor improves both the accuracy of the resulting topic model and the convergence\nrate of the underlying algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 12:53:38 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Gavin", "Brendan", ""], ["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1510.05318", "submitter": "Emilio Ferrara", "authors": "Yoon-Sik Cho, Greg Ver Steeg, Emilio Ferrara, Aram Galstyan", "title": "Latent Space Model for Multi-Modal Social Data", "comments": "12 pages, 7 figures, 2 tables", "journal-ref": "Proceedings of the 25th International Conference on World Wide Web\n  (pp. 447-458). 2016", "doi": "10.1145/2872427.2883031", "report-no": null, "categories": "cs.SI cs.LG physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of social networking services, researchers enjoy the\nincreasing availability of large-scale heterogenous datasets capturing online\nuser interactions and behaviors. Traditional analysis of techno-social systems\ndata has focused mainly on describing either the dynamics of social\ninteractions, or the attributes and behaviors of the users. However,\noverwhelming empirical evidence suggests that the two dimensions affect one\nanother, and therefore they should be jointly modeled and analyzed in a\nmulti-modal framework. The benefits of such an approach include the ability to\nbuild better predictive models, leveraging social network information as well\nas user behavioral signals. To this purpose, here we propose the Constrained\nLatent Space Model (CLSM), a generalized framework that combines Mixed\nMembership Stochastic Blockmodels (MMSB) and Latent Dirichlet Allocation (LDA)\nincorporating a constraint that forces the latent space to concurrently\ndescribe the multiple data modalities. We derive an efficient inference\nalgorithm based on Variational Expectation Maximization that has a\ncomputational cost linear in the size of the network, thus making it feasible\nto analyze massive social datasets. We validate the proposed framework on two\nproblems: prediction of social interactions from user attributes and behaviors,\nand behavior prediction exploiting network information. We perform experiments\nwith a variety of multi-modal social systems, spanning location-based social\nnetworks (Gowalla), social media services (Instagram, Orkut), e-commerce and\nreview sites (Amazon, Ciao), and finally citation networks (Cora). The results\nindicate significant improvement in prediction accuracy over state of the art\nmethods, and demonstrate the flexibility of the proposed approach for\naddressing a variety of different learning problems commonly occurring with\nmulti-modal social data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 22:16:38 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Cho", "Yoon-Sik", ""], ["Steeg", "Greg Ver", ""], ["Ferrara", "Emilio", ""], ["Galstyan", "Aram", ""]]}, {"id": "1510.05336", "submitter": "Shai Ben-David", "authors": "Shai Ben-David", "title": "Clustering is Easy When ....What?", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that most of the common clustering objectives are NP-hard to\noptimize. In practice, however, clustering is being routinely carried out. One\napproach for providing theoretical understanding of this seeming discrepancy is\nto come up with notions of clusterability that distinguish realistically\ninteresting input data from worst-case data sets. The hope is that there will\nbe clustering algorithms that are provably efficient on such \"clusterable\"\ninstances. This paper addresses the thesis that the computational hardness of\nclustering tasks goes away for inputs that one really cares about. In other\nwords, that \"Clustering is difficult only when it does not matter\" (the\n\\emph{CDNM thesis} for short).\n  I wish to present a a critical bird's eye overview of the results published\non this issue so far and to call attention to the gap between available and\ndesirable results on this issue. A longer, more detailed version of this note\nis available as arXiv:1507.05307.\n  I discuss which requirements should be met in order to provide formal support\nto the the CDNM thesis and then examine existing results in view of these\nrequirements and list some significant unsolved research challenges in that\ndirection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 02:40:33 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ben-David", "Shai", ""]]}, {"id": "1510.05417", "submitter": "Toshiki Sato", "authors": "Toshiki Sato, Yuichi Takano, Ryuhei Miyashiro", "title": "Piecewise-Linear Approximation for Feature Subset Selection in a\n  Sequential Logit Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns a method of selecting a subset of features for a\nsequential logit model. Tanaka and Nakagawa (2014) proposed a mixed integer\nquadratic optimization formulation for solving the problem based on a quadratic\napproximation of the logistic loss function. However, since there is a\nsignificant gap between the logistic loss function and its quadratic\napproximation, their formulation may fail to find a good subset of features. To\novercome this drawback, we apply a piecewise-linear approximation to the\nlogistic loss function. Accordingly, we frame the feature subset selection\nproblem of minimizing an information criterion as a mixed integer linear\noptimization problem. The computational results demonstrate that our\npiecewise-linear approximation approach found a better subset of features than\nthe quadratic approximation approach.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 10:44:53 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Sato", "Toshiki", ""], ["Takano", "Yuichi", ""], ["Miyashiro", "Ryuhei", ""]]}, {"id": "1510.05477", "submitter": "Mehmet Basbug", "authors": "Mehmet Emin Basbug, Koray Ozcan and Senem Velipasalar", "title": "Accelerometer based Activity Classification with Variational Inference\n  on Sticky HDP-SLDS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As part of daily monitoring of human activities, wearable sensors and devices\nare becoming increasingly popular sources of data. With the advent of\nsmartphones equipped with acceloremeter, gyroscope and camera; it is now\npossible to develop activity classification platforms everyone can use\nconveniently. In this paper, we propose a fast inference method for an\nunsupervised non-parametric time series model namely variational inference for\nsticky HDP-SLDS(Hierarchical Dirichlet Process Switching Linear Dynamical\nSystem). We show that the proposed algorithm can differentiate various indoor\nactivities such as sitting, walking, turning, going up/down the stairs and\ntaking the elevator using only the acceloremeter of an Android smartphone\nSamsung Galaxy S4. We used the front camera of the smartphone to annotate\nactivity types precisely. We compared the proposed method with Hidden Markov\nModels with Gaussian emission probabilities on a dataset of 10 subjects. We\nshowed that the efficacy of the stickiness property. We further compared the\nvariational inference to the Gibbs sampler on the same model and show that\nvariational inference is faster in one order of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 13:58:37 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Basbug", "Mehmet Emin", ""], ["Ozcan", "Koray", ""], ["Velipasalar", "Senem", ""]]}, {"id": "1510.05491", "submitter": "Mehmet Basbug", "authors": "Mehmet Emin Basbug and Barbara Engelhardt", "title": "AdaCluster : Adaptive Clustering for Heterogeneous Data", "comments": "Submitted to JMLR on January 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Clustering algorithms start with a fixed divergence, which captures the\npossibly asymmetric distance between a sample and a centroid. In the mixture\nmodel setting, the sample distribution plays the same role. When all attributes\nhave the same topology and dispersion, the data are said to be homogeneous. If\nthe prior knowledge of the distribution is inaccurate or the set of plausible\ndistributions is large, an adaptive approach is essential. The motivation is\nmore compelling for heterogeneous data, where the dispersion or the topology\ndiffers among attributes. We propose an adaptive approach to clustering using\nclasses of parametrized Bregman divergences. We first show that the density of\na steep exponential dispersion model (EDM) can be represented with a Bregman\ndivergence. We then propose AdaCluster, an expectation-maximization (EM)\nalgorithm to cluster heterogeneous data using classes of steep EDMs. We compare\nAdaCluster with EM for a Gaussian mixture model on synthetic data and nine UCI\ndata sets. We also propose an adaptive hard clustering algorithm based on\nGeneralized Method of Moments. We compare the hard clustering algorithm with\nk-means on the UCI data sets. We empirically verified that adaptively learning\nthe underlying topology yields better clustering of heterogeneous data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 14:25:27 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 23:11:39 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Basbug", "Mehmet Emin", ""], ["Engelhardt", "Barbara", ""]]}, {"id": "1510.05577", "submitter": "Tanya Jha", "authors": "Jitenkumar Babubhai Rana, Rashmi Shetty, Tanya Jha", "title": "Application of Machine Learning Techniques in Human Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity detection has seen a tremendous growth in the last decade\nplaying a major role in the field of pervasive computing. This emerging\npopularity can be attributed to its myriad of real-life applications primarily\ndealing with human-centric problems like healthcare and elder care. Many\nresearch attempts with data mining and machine learning techniques have been\nundergoing to accurately detect human activities for e-health systems. This\npaper reviews some of the predictive data mining algorithms and compares the\naccuracy and performances of these models. A discussion on the future research\ndirections is subsequently offered.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 16:43:02 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Rana", "Jitenkumar Babubhai", ""], ["Shetty", "Rashmi", ""], ["Jha", "Tanya", ""]]}, {"id": "1510.05610", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Adityanand Guntuboyina and\n  Martin J. Wainwright", "title": "Stochastically Transitive Models for Pairwise Comparisons: Statistical\n  and Computational Issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are various parametric models for analyzing pairwise comparison data,\nincluding the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance\non strong parametric assumptions is limiting. In this work, we study a flexible\nmodel for pairwise comparisons, under which the probabilities of outcomes are\nrequired only to satisfy a natural form of stochastic transitivity. This class\nincludes parametric models including the BTL and Thurstone models as special\ncases, but is considerably more general. We provide various examples of models\nin this broader stochastically transitive class for which classical parametric\nmodels provide poor fits. Despite this greater flexibility, we show that the\nmatrix of probabilities can be estimated at the same rate as in standard\nparametric models. On the other hand, unlike in the BTL and Thurstone models,\ncomputing the minimax-optimal estimator in the stochastically transitive model\nis non-trivial, and we explore various computationally tractable alternatives.\nWe show that a simple singular value thresholding algorithm is statistically\nconsistent but does not achieve the minimax rate. We then propose and study\nalgorithms that achieve the minimax rate over interesting sub-classes of the\nfull stochastically transitive class. We complement our theoretical results\nwith thorough numerical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 18:19:16 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 17:54:49 GMT"}, {"version": "v3", "created": "Wed, 25 May 2016 18:41:40 GMT"}, {"version": "v4", "created": "Wed, 28 Sep 2016 02:14:25 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Guntuboyina", "Adityanand", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1510.05682", "submitter": "Jianzhu Ma", "authors": "Jianzhu Ma", "title": "Protein Structure Prediction by Protein Alignments", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.4420 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proteins are the basic building blocks of life. They usually perform\nfunctions by folding to a particular structure. Understanding the folding\nprocess could help the researchers to understand the functions of proteins and\ncould also help to develop supplemental proteins for people with deficiencies\nand gain more insight into diseases associated with troublesome folding\nproteins. Experimental methods are both expensive and time consuming. In this\nthesis I introduce a new machine learning based method to predict the protein\nstructure. The new method improves the performance from two directions:\ncreating accurate protein alignments and predicting accurate protein contacts.\nFirst, I present an alignment framework MRFalign which goes beyond\nstate-of-the-art methods and uses Markov Random Fields to model a protein\nfamily and align two proteins by aligning two MRFs together. Compared to other\nmethods, that can only model local-range residue correlation, MRFs can model\nlong-range residue interactions and thus, encodes global information in a\nprotein. Secondly, I present a Group Graphical Lasso method for contact\nprediction that integrates joint multi-family Evolutionary Coupling analysis\nand supervised learning to improve accuracy on proteins without many sequence\nhomologs. Different from single-family EC analysis that uses residue\nco-evolution information in only the target protein family, our joint EC\nanalysis uses residue co-evolution in both the target family and its related\nfamilies, which may have divergent sequences but similar folds. Our method can\nalso integrate supervised learning methods to further improve accuracy. We\nevaluate the performance of both methods including each of its components on\nlarge public benchmarks. Experiments show that our methods can achieve better\naccuracy than existing state-of-the-art methods under all the measurements on\nmost of the protein classes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 20:46:04 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Ma", "Jianzhu", ""]]}, {"id": "1510.05711", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Qualitative Projection Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) abstract by demodulating the output of linear\nfilters. In this article, we refine this definition of abstraction to show that\nthe inputs of a DNN are abstracted with respect to the filters. Or, to restate,\nthe abstraction is qualified by the filters. This leads us to introduce the\nnotion of qualitative projection. We use qualitative projection to abstract\nMNIST hand-written digits with respect to the various dogs, horses, planes and\ncars of the CIFAR dataset. We then classify the MNIST digits according to the\nmagnitude of their dogness, horseness, planeness and carness qualities,\nillustrating the generality of qualitative projection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 22:38:09 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 08:42:54 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1510.05830", "submitter": "Ariel Jaffe", "authors": "Ariel Jaffe, Ethan Fetaya, Boaz Nadler, Tingting Jiang, Yuval Kluger", "title": "Unsupervised Ensemble Learning with Dependent Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised ensemble learning, one obtains predictions from multiple\nsources or classifiers, yet without knowing the reliability and expertise of\neach source, and with no labeled data to assess it. The task is to combine\nthese possibly conflicting predictions into an accurate meta-learner. Most\nworks to date assumed perfect diversity between the different sources, a\nproperty known as conditional independence. In realistic scenarios, however,\nthis assumption is often violated, and ensemble learners based on it can be\nseverely sub-optimal. The key challenges we address in this paper are:\\ (i) how\nto detect, in an unsupervised manner, strong violations of conditional\nindependence; and (ii) construct a suitable meta-learner. To this end we\nintroduce a statistical model that allows for dependencies between classifiers.\nOur main contributions are the development of novel unsupervised methods to\ndetect strongly dependent classifiers, better estimate their accuracies, and\nconstruct an improved meta-learner. Using both artificial and real datasets, we\nshowcase the importance of taking classifier dependencies into account and the\ncompetitive performance of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 10:48:40 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 20:50:55 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Jaffe", "Ariel", ""], ["Fetaya", "Ethan", ""], ["Nadler", "Boaz", ""], ["Jiang", "Tingting", ""], ["Kluger", "Yuval", ""]]}, {"id": "1510.05937", "submitter": "Lantian Li Mr.", "authors": "Lantian Li and Dong Wang and Chao Xing and Kaimin Yu and Thomas Fang\n  Zheng", "title": "Binary Speaker Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular i-vector model represents speakers as low-dimensional continuous\nvectors (i-vectors), and hence it is a way of continuous speaker embedding. In\nthis paper, we investigate binary speaker embedding, which transforms i-vectors\nto binary vectors (codes) by a hash function. We start from locality sensitive\nhashing (LSH), a simple binarization approach where binary codes are derived\nfrom a set of random hash functions. A potential problem of LSH is that the\nrandomly sampled hash functions might be suboptimal. We therefore propose an\nimproved Hamming distance learning approach, where the hash function is learned\nby a variable-sized block training that projects each dimension of the original\ni-vectors to variable-sized binary codes independently. Our experiments show\nthat binary speaker embedding can deliver competitive or even better results on\nboth speaker verification and identification tasks, while the memory usage and\nthe computation cost are significantly reduced.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 15:49:59 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 05:33:49 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Li", "Lantian", ""], ["Wang", "Dong", ""], ["Xing", "Chao", ""], ["Yu", "Kaimin", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1510.05940", "submitter": "Lantian Li Mr.", "authors": "Lantian Li and Dong Wang and Chao Xing and Thomas Fang Zheng", "title": "Max-margin Metric Learning for Speaker Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic linear discriminant analysis (PLDA) is a popular normalization\napproach for the i-vector model, and has delivered state-of-the-art performance\nin speaker recognition. A potential problem of the PLDA model, however, is that\nit essentially assumes Gaussian distributions over speaker vectors, which is\nnot always true in practice. Additionally, the objective function is not\ndirectly related to the goal of the task, e.g., discriminating true speakers\nand imposters. In this paper, we propose a max-margin metric learning approach\nto solve the problems. It learns a linear transform with a criterion that the\nmargin between target and imposter trials are maximized. Experiments conducted\non the SRE08 core test show that compared to PLDA, the new approach can obtain\ncomparable or even better performance, though the scoring is simply a cosine\ncomputation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 16:01:05 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 05:27:17 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Li", "Lantian", ""], ["Wang", "Dong", ""], ["Xing", "Chao", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1510.05956", "submitter": "Seyoung Yun", "authors": "Se-Young Yun and Alexandre Proutiere", "title": "Optimal Cluster Recovery in the Labeled Stochastic Block Model", "comments": "arXiv admin note: text overlap with arXiv:1412.7335", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of community detection or clustering in the labeled\nStochastic Block Model (LSBM) with a finite number $K$ of clusters of sizes\nlinearly growing with the global population of items $n$. Every pair of items\nis labeled independently at random, and label $\\ell$ appears with probability\n$p(i,j,\\ell)$ between two items in clusters indexed by $i$ and $j$,\nrespectively. The objective is to reconstruct the clusters from the observation\nof these random labels.\n  Clustering under the SBM and their extensions has attracted much attention\nrecently. Most existing work aimed at characterizing the set of parameters such\nthat it is possible to infer clusters either positively correlated with the\ntrue clusters, or with a vanishing proportion of misclassified items, or\nexactly matching the true clusters. We find the set of parameters such that\nthere exists a clustering algorithm with at most $s$ misclassified items in\naverage under the general LSBM and for any $s=o(n)$, which solves one open\nproblem raised in \\cite{abbe2015community}. We further develop an algorithm,\nbased on simple spectral methods, that achieves this fundamental performance\nlimit within $O(n \\mbox{polylog}(n))$ computations and without the a-priori\nknowledge of the model parameters.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 16:47:27 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 01:18:59 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 23:50:11 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2015 14:03:41 GMT"}, {"version": "v5", "created": "Mon, 21 Dec 2015 01:23:31 GMT"}, {"version": "v6", "created": "Sat, 21 May 2016 19:41:08 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Yun", "Se-Young", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1510.05970", "submitter": "Jure \\v{Z}bontar", "authors": "Jure \\v{Z}bontar and Yann LeCun", "title": "Stereo Matching by Training a Convolutional Neural Network to Compare\n  Image Patches", "comments": null, "journal-ref": "JMLR 17(65):1-32, 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for extracting depth information from a rectified image\npair. Our approach focuses on the first stage of many stereo algorithms: the\nmatching cost computation. We approach the problem by learning a similarity\nmeasure on small image patches using a convolutional neural network. Training\nis carried out in a supervised manner by constructing a binary classification\ndata set with examples of similar and dissimilar pairs of patches. We examine\ntwo network architectures for this task: one tuned for speed, the other for\naccuracy. The output of the convolutional neural network is used to initialize\nthe stereo matching cost. A series of post-processing steps follow: cross-based\ncost aggregation, semiglobal matching, a left-right consistency check, subpixel\nenhancement, a median filter, and a bilateral filter. We evaluate our method on\nthe KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it\noutperforms other approaches on all three data sets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 17:15:05 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 19:53:41 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["\u017dbontar", "Jure", ""], ["LeCun", "Yann", ""]]}, {"id": "1510.05976", "submitter": "Liping Liu", "authors": "Li-Ping Liu and Thomas G. Dietterich and Nan Li and Zhi-Hua Zhou", "title": "Transductive Optimization of Top k Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a binary classification problem in which the learner is given a\nlabeled training set, an unlabeled test set, and is restricted to choosing\nexactly $k$ test points to output as positive predictions. Problems of this\nkind---{\\it transductive precision@$k$}---arise in information retrieval,\ndigital advertising, and reserve design for endangered species. Previous\nmethods separate the training of the model from its use in scoring the test\npoints. This paper introduces a new approach, Transductive Top K (TTK), that\nseeks to minimize the hinge loss over all training instances under the\nconstraint that exactly $k$ test instances are predicted as positive. The paper\npresents two optimization methods for this challenging problem. Experiments and\nanalysis confirm the importance of incorporating the knowledge of $k$ into the\nlearning process. Experimental evaluations of the TTK approach show that the\nperformance of TTK matches or exceeds existing state-of-the-art methods on 7\nUCI datasets and 3 reserve design problem instances.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 17:27:12 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Liu", "Li-Ping", ""], ["Dietterich", "Thomas G.", ""], ["Li", "Nan", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1510.06002", "submitter": "Heejin Choi", "authors": "Heejin Choi, Ofer Meshi, Nathan Srebro", "title": "Fast and Scalable Structural SVM with Slack Rescaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient method for training slack-rescaled structural SVM.\nAlthough finding the most violating label in a margin-rescaled formulation is\noften easy since the target function decomposes with respect to the structure,\nthis is not the case for a slack-rescaled formulation, and finding the most\nviolated label might be very difficult. Our core contribution is an efficient\nmethod for finding the most-violating-label in a slack-rescaled formulation,\ngiven an oracle that returns the most-violating-label in a (slightly modified)\nmargin-rescaled formulation. We show that our method enables accurate and\nscalable training for slack-rescaled SVMs, reducing runtime by an order of\nmagnitude compared to previous approaches to slack-rescaled SVMs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 18:25:45 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 22:25:33 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Choi", "Heejin", ""], ["Meshi", "Ofer", ""], ["Srebro", "Nathan", ""]]}, {"id": "1510.06024", "submitter": "Leman Akoglu", "authors": "Junting Ye, Leman Akoglu", "title": "Robust Semi-Supervised Classification for Multi-Relational Graphs", "comments": "14 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-regularized semi-supervised learning has been used effectively for\nclassification when (i) instances are connected through a graph, and (ii)\nlabeled data is scarce. If available, using multiple relations (or graphs)\nbetween the instances can improve the prediction performance. On the other\nhand, when these relations have varying levels of veracity and exhibit varying\nrelevance for the task, very noisy and/or irrelevant relations may deteriorate\nthe performance. As a result, an effective weighing scheme needs to be put in\nplace. In this work, we propose a robust and scalable approach for\nmulti-relational graph-regularized semi-supervised classification. Under a\nconvex optimization scheme, we simultaneously infer weights for the multiple\ngraphs as well as a solution. We provide a careful analysis of the inferred\nweights, based on which we devise an algorithm that filters out irrelevant and\nnoisy graphs and produces weights proportional to the informativeness of the\nremaining graphs. Moreover, the proposed method is linearly scalable w.r.t. the\nnumber of edges in the union of the multiple graphs. Through extensive\nexperiments we show that our method yields superior results under different\nnoise models, and under increasing number of noisy graphs and intensity of\nnoise, as compared to a list of baselines and state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 22:33:30 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Ye", "Junting", ""], ["Akoglu", "Leman", ""]]}, {"id": "1510.06083", "submitter": "Hongbo Dong", "authors": "Hongbo Dong and Kun Chen and Jeff Linderoth", "title": "Regularization vs. Relaxation: A conic optimization perspective of\n  statistical variable selection", "comments": "Also available on optimization online\n  {http://www.optimization-online.org/DB_HTML/2015/05/4932.html}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is a fundamental task in statistical data analysis.\nSparsity-inducing regularization methods are a popular class of methods that\nsimultaneously perform variable selection and model estimation. The central\nproblem is a quadratic optimization problem with an l0-norm penalty. Exactly\nenforcing the l0-norm penalty is computationally intractable for larger scale\nproblems, so dif- ferent sparsity-inducing penalty functions that approximate\nthe l0-norm have been introduced. In this paper, we show that viewing the\nproblem from a convex relaxation perspective offers new insights. In\nparticular, we show that a popular sparsity-inducing concave penalty function\nknown as the Minimax Concave Penalty (MCP), and the reverse Huber penalty\nderived in a recent work by Pilanci, Wainwright and Ghaoui, can both be derived\nas special cases of a lifted convex relaxation called the perspective\nrelaxation. The optimal perspective relaxation is a related minimax problem\nthat balances the overall convexity and tightness of approximation to the l0\nnorm. We show it can be solved by a semidefinite relaxation. Moreover, a\nprobabilistic interpretation of the semidefinite relaxation reveals connections\nwith the boolean quadric polytope in combinatorial optimization. Finally by\nreformulating the l0-norm pe- nalized problem as a two-level problem, with the\ninner level being a Max-Cut problem, our proposed semidefinite relaxation can\nbe realized by replacing the inner level problem with its semidefinite\nrelaxation studied by Goemans and Williamson. This interpretation suggests\nusing the Goemans-Williamson rounding procedure to find approximate solutions\nto the l0-norm penalized problem. Numerical experiments demonstrate the\ntightness of our proposed semidefinite relaxation, and the effectiveness of\nfinding approximate solutions by Goemans-Williamson rounding.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 22:55:48 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Dong", "Hongbo", ""], ["Chen", "Kun", ""], ["Linderoth", "Jeff", ""]]}, {"id": "1510.06143", "submitter": "arXiv Admin", "authors": "Aaron Q. Li, Amr Ahmed, Mu Li, Vanja Josifovski", "title": "High Performance Latent Variable Models", "comments": "arXiv admin note: This paper has been withdrawn due to an\n  irreconcilable author dispute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models have accumulated a considerable amount of interest\nfrom the industry and academia for their versatility in a wide range of\napplications. A large amount of effort has been made to develop systems that is\nable to extend the systems to a large scale, in the hope to make use of them on\nindustry scale data. In this paper, we describe a system that operates at a\nscale orders of magnitude higher than previous works, and an order of magnitude\nfaster than state-of-the-art system at the same scale, at the same time showing\nmore robustness and more accurate results.\n  Our system uses a number of advances in distributed inference: high\nperformance in synchronization of sufficient statistics with relaxed\nconsistency model; fast sampling, using the Metropolis-Hastings-Walker method\nto overcome dense generative models; statistical modeling, moving beyond Latent\nDirichlet Allocation (LDA) to Pitman-Yor distributions (PDP) and Hierarchical\nDirichlet Process (HDP) models; sophisticated parameter projection schemes, to\nresolve the conflicts within the constraint between parameters arising from the\nrelaxed consistency model.\n  This work significantly extends the domain of applicability of what is\ncommonly known as the Parameter Server. We obtain results with up to hundreds\nbillion oftokens, thousands of topics, and a vocabulary of a few million\ntoken-types, using up to 60,000 processor cores operating on a production\ncluster of a large Internet company. This demonstrates the feasibility to scale\nto problems orders of magnitude larger than any previously published work.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 06:23:55 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 22:39:06 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2015 03:37:21 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2015 05:16:06 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Li", "Aaron Q.", ""], ["Ahmed", "Amr", ""], ["Li", "Mu", ""], ["Josifovski", "Vanja", ""]]}, {"id": "1510.06188", "submitter": "Jonathan Scarlett", "authors": "Luca Baldassarre and Yen-Huan Li and Jonathan Scarlett and Baran\n  G\\\"ozc\\\"u and Ilija Bogunovic and Volkan Cevher", "title": "Learning-based Compressive Subsampling", "comments": "Submitted to IEEE Journal on Selected Topics in Signal Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2016.2548442", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of recovering a structured signal $\\mathbf{x} \\in \\mathbb{C}^p$\nfrom a set of dimensionality-reduced linear measurements $\\mathbf{b} = \\mathbf\n{A}\\mathbf {x}$ arises in a variety of applications, such as medical imaging,\nspectroscopy, Fourier optics, and computerized tomography. Due to computational\nand storage complexity or physical constraints imposed by the problem, the\nmeasurement matrix $\\mathbf{A} \\in \\mathbb{C}^{n \\times p}$ is often of the\nform $\\mathbf{A} = \\mathbf{P}_{\\Omega}\\boldsymbol{\\Psi}$ for some orthonormal\nbasis matrix $\\boldsymbol{\\Psi}\\in \\mathbb{C}^{p \\times p}$ and subsampling\noperator $\\mathbf{P}_{\\Omega}: \\mathbb{C}^{p} \\rightarrow \\mathbb{C}^{n}$ that\nselects the rows indexed by $\\Omega$. This raises the fundamental question of\nhow best to choose the index set $\\Omega$ in order to optimize the recovery\nperformance. Previous approaches to addressing this question rely on\nnon-uniform \\emph{random} subsampling using application-specific knowledge of\nthe structure of $\\mathbf{x}$. In this paper, we instead take a principled\nlearning-based approach in which a \\emph{fixed} index set is chosen based on a\nset of training signals $\\mathbf{x}_1,\\dotsc,\\mathbf{x}_m$. We formulate\ncombinatorial optimization problems seeking to maximize the energy captured in\nthese signals in an average-case or worst-case sense, and we show that these\ncan be efficiently solved either exactly or approximately via the\nidentification of modularity and submodularity structures. We provide both\ndeterministic and statistical theoretical guarantees showing how the resulting\nmeasurement matrices perform on signals differing from the training signals,\nand we provide numerical examples showing our approach to be effective on a\nvariety of data sets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 10:03:45 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2016 09:56:20 GMT"}, {"version": "v3", "created": "Mon, 28 Mar 2016 15:32:26 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Baldassarre", "Luca", ""], ["Li", "Yen-Huan", ""], ["Scarlett", "Jonathan", ""], ["G\u00f6zc\u00fc", "Baran", ""], ["Bogunovic", "Ilija", ""], ["Cevher", "Volkan", ""]]}, {"id": "1510.06335", "submitter": "Matteo Venanzi", "authors": "Matteo Venanzi, John Guiver, Pushmeet Kohli, Nick Jennings", "title": "Time-Sensitive Bayesian Information Aggregation for Crowdsourcing\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing systems commonly face the problem of aggregating multiple\njudgments provided by potentially unreliable workers. In addition, several\naspects of the design of efficient crowdsourcing processes, such as defining\nworker's bonuses, fair prices and time limits of the tasks, involve knowledge\nof the likely duration of the task at hand. Bringing this together, in this\nwork we introduce a new time--sensitive Bayesian aggregation method that\nsimultaneously estimates a task's duration and obtains reliable aggregations of\ncrowdsourced judgments. Our method, called BCCTime, builds on the key insight\nthat the time taken by a worker to perform a task is an important indicator of\nthe likely quality of the produced judgment. To capture this, BCCTime uses\nlatent variables to represent the uncertainty about the workers' completion\ntime, the tasks' duration and the workers' accuracy. To relate the quality of a\njudgment to the time a worker spends on a task, our model assumes that each\ntask is completed within a latent time window within which all workers with a\npropensity to genuinely attempt the labelling task (i.e., no spammers) are\nexpected to submit their judgments. In contrast, workers with a lower\npropensity to valid labeling, such as spammers, bots or lazy labelers, are\nassumed to perform tasks considerably faster or slower than the time required\nby normal workers. Specifically, we use efficient message-passing Bayesian\ninference to learn approximate posterior probabilities of (i) the confusion\nmatrix of each worker, (ii) the propensity to valid labeling of each worker,\n(iii) the unbiased duration of each task and (iv) the true label of each task.\nUsing two real-world public datasets for entity linking tasks, we show that\nBCCTime produces up to 11% more accurate classifications and up to 100% more\ninformative estimates of a task's duration compared to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 16:42:55 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 21:09:58 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Venanzi", "Matteo", ""], ["Guiver", "John", ""], ["Kohli", "Pushmeet", ""], ["Jennings", "Nick", ""]]}, {"id": "1510.06356", "submitter": "Steven Adachi", "authors": "Steven H. Adachi and Maxwell P. Henderson", "title": "Application of Quantum Annealing to Training of Deep Neural Networks", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": "DIS201510002", "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Deep Learning, a well-known approach for training a Deep Neural Network\nstarts by training a generative Deep Belief Network model, typically using\nContrastive Divergence (CD), then fine-tuning the weights using backpropagation\nor other discriminative techniques. However, the generative training can be\ntime-consuming due to the slow mixing of Gibbs sampling. We investigated an\nalternative approach that estimates model expectations of Restricted Boltzmann\nMachines using samples from a D-Wave quantum annealing machine. We tested this\nmethod on a coarse-grained version of the MNIST data set. In our tests we found\nthat the quantum sampling-based training approach achieves comparable or better\naccuracy with significantly fewer iterations of generative training than\nconventional CD-based training. Further investigation is needed to determine\nwhether similar improvements can be achieved for other data sets, and to what\nextent these improvements can be attributed to quantum effects.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 18:21:39 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Adachi", "Steven H.", ""], ["Henderson", "Maxwell P.", ""]]}, {"id": "1510.06423", "submitter": "Zi Wang", "authors": "Zi Wang, Bolei Zhou, Stefanie Jegelka", "title": "Optimization as Estimation with Gaussian Processes in Bandit Settings", "comments": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been rising interest in Bayesian optimization -- the\noptimization of an unknown function with assumptions usually expressed by a\nGaussian Process (GP) prior. We study an optimization strategy that directly\nuses an estimate of the argmax of the function. This strategy offers both\npractical and theoretical advantages: no tradeoff parameter needs to be\nselected, and, moreover, we establish close connections to the popular GP-UCB\nand GP-PI strategies. Our approach can be understood as automatically and\nadaptively trading off exploration and exploitation in GP-UCB and GP-PI. We\nillustrate the effects of this adaptive tuning via bounds on the regret as well\nas an extensive empirical evaluation on robotics and vision tasks,\ndemonstrating the robustness of this strategy for a range of performance\ncriteria.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 20:35:13 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2015 18:22:28 GMT"}, {"version": "v3", "created": "Sun, 24 Apr 2016 16:56:38 GMT"}, {"version": "v4", "created": "Sun, 12 Aug 2018 16:03:00 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wang", "Zi", ""], ["Zhou", "Bolei", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1510.06492", "submitter": "Linus Hermansson", "authors": "Linus Hermansson, Fredrik D. Johansson, and Osamu Watanabe", "title": "Generalized Shortest Path Kernel on Graphs", "comments": "Short version presented at Discovery Science 2015 in Banff", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classifying graphs using graph kernels. We define\na new graph kernel, called the generalized shortest path kernel, based on the\nnumber and length of shortest paths between nodes. For our example\nclassification problem, we consider the task of classifying random graphs from\ntwo well-known families, by the number of clusters they contain. We verify\nempirically that the generalized shortest path kernel outperforms the original\nshortest path kernel on a number of datasets. We give a theoretical analysis\nfor explaining our experimental results. In particular, we estimate\ndistributions of the expected feature vectors for the shortest path kernel and\nthe generalized shortest path kernel, and we show some evidence explaining why\nour graph kernel outperforms the shortest path kernel for our graph\nclassification problem.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 05:49:31 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Hermansson", "Linus", ""], ["Johansson", "Fredrik D.", ""], ["Watanabe", "Osamu", ""]]}, {"id": "1510.06549", "submitter": "Aaron Li", "authors": "Aaron Q Li", "title": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an explosion of data, documents, and other content, and people\nrequire tools to analyze and interpret these, tools to turn the content into\ninformation and knowledge. Topic modeling have been developed to solve these\nproblems. Topic models such as LDA [Blei et. al. 2003] allow salient patterns\nin data to be extracted automatically. When analyzing texts, these patterns are\ncalled topics. Among numerous extensions of LDA, few of them can reliably\nanalyze multiple groups of documents and extract topic similarities. Recently,\nthe introduction of differential topic modeling (SPDP) [Chen et. al. 2012]\nperforms uniformly better than many topic models in a discriminative setting.\n  There is also a need to improve the sampling speed for topic models. While\nsome effort has been made for distributed algorithms, there is no work\ncurrently done using graphical processing units (GPU). Note the GPU framework\nhas already become the most cost-efficient platform for many problems.\n  In this thesis, I propose and implement a scalable multi-GPU distributed\nparallel framework which approximates SPDP. Through experiments, I have shown\nmy algorithms have a gain in speed of about 50 times while being almost as\naccurate, with only one single cheap laptop GPU. Furthermore, I have shown the\nspeed improvement is sublinearly scalable when multiple GPUs are used, while\nfairly maintaining the accuracy. Therefore on a medium-sized GPU cluster, the\nspeed improvement could potentially reach a factor of a thousand.\n  Note SPDP is just a representative of other extensions of LDA. Although my\nalgorithm is implemented to work with SPDP, it is designed to be a general\nenough to work with other topic models. The speed-up on smaller collections\n(i.e., 1000s of documents), means that these more complex LDA extensions could\nnow be done in real-time, thus opening up a new way of using these LDA models\nin industry.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 09:40:54 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Li", "Aaron Q", ""]]}, {"id": "1510.06567", "submitter": "Remi Flamary", "authors": "Alain Rakotomamonjy (LITIS), R\\'emi Flamary (LAGRANGE, OCA), Nicolas\n  Courty (OBELIX)", "title": "Generalized conditional gradient: analysis of convergence and\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objectives of this technical report is to provide additional results on\nthe generalized conditional gradient methods introduced by Bredies et al.\n[BLM05]. Indeed , when the objective function is smooth, we provide a novel\ncertificate of optimality and we show that the algorithm has a linear\nconvergence rate. Applications of this algorithm are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 10:19:52 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Rakotomamonjy", "Alain", "", "LITIS"], ["Flamary", "R\u00e9mi", "", "LAGRANGE, OCA"], ["Courty", "Nicolas", "", "OBELIX"]]}, {"id": "1510.06582", "submitter": "Bartosz Hawelka", "authors": "Bartosz Hawelka, Izabela Sitko, Pavlos Kazakopoulos and Euro Beinat", "title": "Collective Prediction of Individual Mobility Traces with Exponential\n  Weights", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and test a sequential learning algorithm for the short-term\nprediction of human mobility. This novel approach pairs the Exponential Weights\nforecaster with a very large ensemble of experts. The experts are individual\nsequence prediction algorithms constructed from the mobility traces of 10\nmillion roaming mobile phone users in a European country. Average prediction\naccuracy is significantly higher than that of individual sequence prediction\nalgorithms, namely constant order Markov models derived from the user's own\ndata, that have been shown to achieve high accuracy in previous studies of\nhuman mobility prediction. The algorithm uses only time stamped location data,\nand accuracy depends on the completeness of the expert ensemble, which should\ncontain redundant records of typical mobility patterns. The proposed algorithm\nis applicable to the prediction of any sufficiently large dataset of sequences.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 11:27:03 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Hawelka", "Bartosz", ""], ["Sitko", "Izabela", ""], ["Kazakopoulos", "Pavlos", ""], ["Beinat", "Euro", ""]]}, {"id": "1510.06646", "submitter": "Osama Khalifa", "authors": "Osama Khalifa, David Wolfe Corne, Mike Chantler", "title": "A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya\n  Parameters and Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper-parameters play a major role in the learning and inference process of\nlatent Dirichlet allocation (LDA). In order to begin the LDA latent variables\nlearning process, these hyper-parameters values need to be pre-determined. We\npropose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs\nNewton' (LDA-GN), which places non-informative priors over these\nhyper-parameters and uses Gibbs sampling to learn appropriate values for them.\nAt the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new\ntechnique for learning the parameters of multivariate Polya distributions. We\nreport Gibbs-Newton performance results compared with two prominent existing\napproaches to the latter task: Minka's fixed-point iteration method and the\nMoments method. We then evaluate LDA-GN in two ways: (i) by comparing it with\nstandard LDA in terms of the ability of the resulting topic models to\ngeneralize to unseen documents; (ii) by comparing it with standard LDA in its\nperformance on a binary classification task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 14:39:58 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 12:29:43 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Khalifa", "Osama", ""], ["Corne", "David Wolfe", ""], ["Chantler", "Mike", ""]]}, {"id": "1510.06664", "submitter": "Alaa Saade", "authors": "Alaa Saade, Francesco Caltagirone, Igor Carron, Laurent Daudet,\n  Ang\\'elique Dr\\'emeau, Sylvain Gigan and Florent Krzakala", "title": "Random Projections through multiple optical scattering: Approximating\n  kernels at the speed of light", "comments": null, "journal-ref": "Proceedings of the 2016 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP) pages: 6215 - 6219", "doi": "10.1109/ICASSP.2016.7472872", "report-no": null, "categories": "cs.ET cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projections have proven extremely useful in many signal processing and\nmachine learning applications. However, they often require either to store a\nvery large random matrix, or to use a different, structured matrix to reduce\nthe computational and memory costs. Here, we overcome this difficulty by\nproposing an analog, optical device, that performs the random projections\nliterally at the speed of light without having to store any matrix in memory.\nThis is achieved using the physical properties of multiple coherent scattering\nof coherent light in random media. We use this device on a simple task of\nclassification with a kernel machine, and we show that, on the MNIST database,\nthe experimental results closely match the theoretical performance of the\ncorresponding kernel. This framework can help make kernel methods practical for\napplications that have large training sets and/or require real-time prediction.\nWe discuss possible extensions of the method in terms of a class of kernels,\nspeed, memory consumption and different problems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 15:54:30 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2015 11:19:23 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Saade", "Alaa", ""], ["Caltagirone", "Francesco", ""], ["Carron", "Igor", ""], ["Daudet", "Laurent", ""], ["Dr\u00e9meau", "Ang\u00e9lique", ""], ["Gigan", "Sylvain", ""], ["Krzakala", "Florent", ""]]}, {"id": "1510.06684", "submitter": "Xi He", "authors": "Xi He and Martin Tak\\'a\\v{c}", "title": "Dual Free Adaptive Mini-batch SDCA for Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop dual free mini-batch SDCA with adaptive\nprobabilities for regularized empirical risk minimization. This work is\nmotivated by recent work of Shai Shalev-Shwartz on dual free SDCA method,\nhowever, we allow a non-uniform selection of \"dual\" coordinates in SDCA.\nMoreover, the probability can change over time, making it more efficient than\nfix uniform or non-uniform selection. We also propose an efficient procedure to\ngenerate a random non-uniform mini-batch through iterative process. The work is\nconcluded with multiple numerical experiments to show the efficiency of\nproposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 16:50:56 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 03:24:17 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 00:43:53 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["He", "Xi", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1510.06688", "submitter": "Martin Tak\\'a\\v{c}", "authors": "Chenxin Ma and Martin Tak\\'a\\v{c}", "title": "Partitioning Data on Features or Samples in Communication-Efficient\n  Distributed Optimization?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the effect of the way that the data is partitioned in\ndistributed optimization. The original DiSCO algorithm [Communication-Efficient\nDistributed Optimization of Self-Concordant Empirical Loss, Yuchen Zhang and\nLin Xiao, 2015] partitions the input data based on samples. We describe how the\noriginal algorithm has to be modified to allow partitioning on features and\nshow its efficiency both in theory and also in practice.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 17:03:04 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Ma", "Chenxin", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1510.06706", "submitter": "Aleksandar Zlateski", "authors": "Aleksandar Zlateski, Kisuk Lee and H. Sebastian Seung", "title": "ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional\n  Networks on Multi-Core and Many-Core Shared Memory Machines", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS.2016.119", "report-no": null, "categories": "cs.NE cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks (ConvNets) have become a popular approach to computer\nvision. It is important to accelerate ConvNet training, which is\ncomputationally costly. We propose a novel parallel algorithm based on\ndecomposition into a set of tasks, most of which are convolutions or FFTs.\nApplying Brent's theorem to the task dependency graph implies that linear\nspeedup with the number of processors is attainable within the PRAM model of\nparallel computation, for wide network architectures. To attain such\nperformance on real shared-memory machines, our algorithm computes convolutions\nconverging on the same node of the network with temporal locality to reduce\ncache misses, and sums the convergent convolution outputs via an almost\nwait-free concurrent method to reduce time spent in critical sections. We\nimplement the algorithm with a publicly available software package called ZNN.\nBenchmarking with multi-core CPUs shows that ZNN can attain speedup roughly\nequal to the number of physical cores. We also show that ZNN can attain over\n90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are\nachieved for network architectures with widths that are in common use. The task\nparallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism\nof previous algorithms is compatible with GPUs. Through examples, we show that\nZNN can be either faster or slower than certain GPU implementations depending\non specifics of the network architecture, kernel sizes, and density and size of\nthe output patch. ZNN may be less costly to develop and maintain, due to the\nrelative ease of general-purpose CPU programming.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 18:14:42 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zlateski", "Aleksandar", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1510.06786", "submitter": "Vivek Kulkarni", "authors": "Vivek Kulkarni, Bryan Perozzi, Steven Skiena", "title": "Freshman or Fresher? Quantifying the Geographic Variation of Internet\n  Language", "comments": "11 pages (updated submission)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new computational technique to detect and analyze statistically\nsignificant geographic variation in language. Our meta-analysis approach\ncaptures statistical properties of word usage across geographical regions and\nuses statistical methods to identify significant changes specific to regions.\nWhile previous approaches have primarily focused on lexical variation between\nregions, our method identifies words that demonstrate semantic and syntactic\nvariation as well.\n  We extend recently developed techniques for neural language models to learn\nword representations which capture differing semantics across geographical\nregions. In order to quantify this variation and ensure robust detection of\ntrue regional differences, we formulate a null model to determine whether\nobserved changes are statistically significant. Our method is the first such\napproach to explicitly account for random variation due to chance while\ndetecting regional variation in word meaning.\n  To validate our model, we study and analyze two different massive online data\nsets: millions of tweets from Twitter spanning not only four different\ncountries but also fifty states, as well as millions of phrases contained in\nthe Google Book Ngrams. Our analysis reveals interesting facets of language\nchange at multiple scales of geographic resolution -- from neighboring states\nto distant continents.\n  Finally, using our model, we propose a measure of semantic distance between\nlanguages. Our analysis of British and American English over a period of 100\nyears reveals that semantic variation between these dialects is shrinking.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 22:53:10 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 14:40:36 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1510.06895", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin", "title": "Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted\n  Nuclear Norm", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2511584", "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nuclear norm is widely used as a convex surrogate of the rank function in\ncompressive sensing for low rank matrix recovery with its applications in image\nrecovery and signal processing. However, solving the nuclear norm based relaxed\nconvex problem usually leads to a suboptimal solution of the original rank\nminimization problem. In this paper, we propose to perform a family of\nnonconvex surrogates of $L_0$-norm on the singular values of a matrix to\napproximate the rank function. This leads to a nonconvex nonsmooth minimization\nproblem. Then we propose to solve the problem by Iteratively Reweighted Nuclear\nNorm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value\nThresholding (WSVT) problem, which has a closed form solution due to the\nspecial properties of the nonconvex surrogate functions. We also extend IRNN to\nsolve the nonconvex problem with two or more blocks of variables. In theory, we\nprove that IRNN decreases the objective function value monotonically, and any\nlimit point is a stationary point. Extensive experiments on both synthesized\ndata and real images demonstrate that IRNN enhances the low-rank matrix\nrecovery compared with state-of-the-art convex algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 11:28:06 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Lu", "Canyi", ""], ["Tang", "Jinhui", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1510.06920", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (ABC)", "title": "On the complexity of switching linear regression", "comments": "Automatica, Elsevier, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical note extends recent results on the computational complexity of\nglobally minimizing the error of piecewise-affine models to the related problem\nof minimizing the error of switching linear regression models. In particular,\nwe show that, on the one hand the problem is NP-hard, but on the other hand, it\nadmits a polynomial-time algorithm with respect to the number of data points\nfor any fixed data dimension and number of modes.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 12:45:29 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 12:32:11 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Lauer", "Fabien", "", "ABC"]]}, {"id": "1510.07025", "submitter": "Dawen Liang", "authors": "Dawen Liang, Laurent Charlin, James McInerney, David M. Blei", "title": "Modeling User Exposure in Recommendation", "comments": "11 pages, 4 figures. WWW'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering analyzes user preferences for items (e.g., books,\nmovies, restaurants, academic papers) by exploiting the similarity patterns\nacross users. In implicit feedback settings, all the items, including the ones\nthat a user did not consume, are taken into consideration. But this assumption\ndoes not accord with the common sense understanding that users have a limited\nscope and awareness of items. For example, a user might not have heard of a\ncertain paper, or might live too far away from a restaurant to experience it.\nIn the language of causal analysis, the assignment mechanism (i.e., the items\nthat a user is exposed to) is a latent variable that may change for various\nuser/item combinations. In this paper, we propose a new probabilistic approach\nthat directly incorporates user exposure to items into collaborative filtering.\nThe exposure is modeled as a latent variable and the model infers its value\nfrom data. In doing so, we recover one of the most successful state-of-the-art\napproaches as a special case of our model, and provide a plug-in method for\nconditioning exposure on various forms of exposure covariates (e.g., topics in\ntext, venue locations). We show that our scalable inference algorithm\noutperforms existing benchmarks in four different domains both with and without\nexposure covariates.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 19:39:38 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 18:58:44 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Liang", "Dawen", ""], ["Charlin", "Laurent", ""], ["McInerney", "James", ""], ["Blei", "David M.", ""]]}, {"id": "1510.07035", "submitter": "Aaron Li", "authors": "Joseph W Robinson, Aaron Q Li", "title": "Fast Latent Variable Models for Inference and Visualization on Mobile\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we outline Vedalia, a high performance distributed network\nfor performing inference on latent variable models in the context of Amazon\nreview visualization. We introduce a new model, RLDA, which extends Latent\nDirichlet Allocation (LDA) [Blei et al., 2003] for the review space by\nincorporating auxiliary data available in online reviews to improve modeling\nwhile simultaneously remaining compatible with pre-existing fast sampling\ntechniques such as [Yao et al., 2009; Li et al., 2014a] to achieve high\nperformance. The network is designed such that computation is efficiently\noffloaded to the client devices using the Chital system [Robinson & Li, 2015],\nimproving response times and reducing server costs. The resulting system is\nable to rapidly compute a large number of specialized latent variable models\nwhile requiring minimal server resources.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 05:26:09 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Robinson", "Joseph W", ""], ["Li", "Aaron Q", ""]]}, {"id": "1510.07146", "submitter": "Lorenzo Livi", "authors": "Enrico Maiorino, Filippo Maria Bianchi, Lorenzo Livi, Antonello Rizzi,\n  Alireza Sadeghian", "title": "Data-driven detrending of nonstationary fractal time series with echo\n  state networks", "comments": "Revised version", "journal-ref": null, "doi": "10.1016/j.ins.2016.12.015", "report-no": null, "categories": "physics.data-an cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel data-driven approach for removing trends\n(detrending) from nonstationary, fractal and multifractal time series. We\nconsider real-valued time series relative to measurements of an underlying\ndynamical system that evolves through time. We assume that such a dynamical\nprocess is predictable to a certain degree by means of a class of recurrent\nnetworks called Echo State Network (ESN), which are capable to model a generic\ndynamical process. In order to isolate the superimposed (multi)fractal\ncomponent of interest, we define a data-driven filter by leveraging on the ESN\nprediction capability to identify the trend component of a given input time\nseries. Specifically, the (estimated) trend is removed from the original time\nseries and the residual signal is analyzed with the multifractal detrended\nfluctuation analysis procedure to verify the correctness of the detrending\nprocedure. In order to demonstrate the effectiveness of the proposed technique,\nwe consider several synthetic time series consisting of different types of\ntrends and fractal noise components with known characteristics. We also process\na real-world dataset, the sunspot time series, which is well-known for its\nmultifractal features and has recently gained attention in the complex systems\nfield. Results demonstrate the validity and generality of the proposed\ndetrending method based on ESNs.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 13:38:13 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 18:19:30 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Maiorino", "Enrico", ""], ["Bianchi", "Filippo Maria", ""], ["Livi", "Lorenzo", ""], ["Rizzi", "Antonello", ""], ["Sadeghian", "Alireza", ""]]}, {"id": "1510.07169", "submitter": "Emanuele Frandi", "authors": "Emanuele Frandi, Ricardo Nanculef, Stefano Lodi, Claudio Sartori,\n  Johan A. K. Suykens", "title": "Fast and Scalable Lasso via Stochastic Frank-Wolfe Methods with a\n  Convergence Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": "Internal Report 15-93, ESAT-STADIUS, KU Leuven, 2015", "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frank-Wolfe (FW) algorithms have been often proposed over the last few years\nas efficient solvers for a variety of optimization problems arising in the\nfield of Machine Learning. The ability to work with cheap projection-free\niterations and the incremental nature of the method make FW a very effective\nchoice for many large-scale problems where computing a sparse model is\ndesirable.\n  In this paper, we present a high-performance implementation of the FW method\ntailored to solve large-scale Lasso regression problems, based on a randomized\niteration, and prove that the convergence guarantees of the standard FW method\nare preserved in the stochastic setting. We show experimentally that our\nalgorithm outperforms several existing state of the art methods, including the\nCoordinate Descent algorithm by Friedman et al. (one of the fastest known Lasso\nsolvers), on several benchmark datasets with a very large number of features,\nwithout sacrificing the accuracy of the model. Our results illustrate that the\nalgorithm is able to generate the complete regularization path on problems of\nsize up to four million variables in less than one minute.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 17:56:27 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Lodi", "Stefano", ""], ["Sartori", "Claudio", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1510.07208", "submitter": "Yuan Ma", "authors": "Joe Lemieux, Yuan Ma", "title": "Vehicle Speed Prediction using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Global optimization of the energy consumption of dual power source vehicles\nsuch as hybrid electric vehicles, plug-in hybrid electric vehicles, and plug in\nfuel cell electric vehicles requires knowledge of the complete route\ncharacteristics at the beginning of the trip. One of the main characteristics\nis the vehicle speed profile across the route. The profile will translate\ndirectly into energy requirements for a given vehicle. However, the vehicle\nspeed that a given driver chooses will vary from driver to driver and from time\nto time, and may be slower, equal to, or faster than the average traffic flow.\nIf the specific driver speed profile can be predicted, the energy usage can be\noptimized across the route chosen. The purpose of this paper is to research the\napplication of Deep Learning techniques to this problem to identify at the\nbeginning of a drive cycle the driver specific vehicle speed profile for an\nindividual driver repeated drive cycle, which can be used in an optimization\nalgorithm to minimize the amount of fossil fuel energy used during the trip.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 05:52:59 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Lemieux", "Joe", ""], ["Ma", "Yuan", ""]]}, {"id": "1510.07211", "submitter": "Lili Mou", "authors": "Lili Mou, Rui Men, Ge Li, Lu Zhang, Zhi Jin", "title": "On End-to-End Program Generation from User Intention by Deep Neural\n  Networks", "comments": "Submitted to 2016 International Conference of Software Engineering\n  \"Vision of 2025 and Beyond\" track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper envisions an end-to-end program generation scenario using\nrecurrent neural networks (RNNs): Users can express their intention in natural\nlanguage; an RNN then automatically generates corresponding code in a\ncharacterby-by-character fashion. We demonstrate its feasibility through a case\nstudy and empirical analysis. To fully make such technique useful in practice,\nwe also point out several cross-disciplinary challenges, including modeling\nuser intention, providing datasets, improving model architectures, etc.\nAlthough much long-term research shall be addressed in this new field, we\nbelieve end-to-end program generation would become a reality in future decades,\nand we are looking forward to its practice.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 06:52:45 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Mou", "Lili", ""], ["Men", "Rui", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1510.07303", "submitter": "Clay McLeod", "authors": "Clay McLeod", "title": "A Framework for Distributed Deep Learning Layer Design in Python", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a framework for testing Deep Neural Network (DNN) design in\nPython is presented. First, big data, machine learning (ML), and Artificial\nNeural Networks (ANNs) are discussed to familiarize the reader with the\nimportance of such a system. Next, the benefits and detriments of implementing\nsuch a system in Python are presented. Lastly, the specifics of the system are\nexplained, and some experimental results are presented to prove the\neffectiveness of the system.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 21:04:12 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["McLeod", "Clay", ""]]}, {"id": "1510.07389", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Christoph Dann, Christopher G. Lucas, Eric P.\n  Xing", "title": "The Human Kernel", "comments": "11 pages, 5 figures. To appear in Neural Information Processing\n  Systems (NIPS) 2015. Version 2: Figure 2 (i)-(n) now displays the second set\n  of progressive function learning experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametric models, such as Gaussian processes, provide a\ncompelling framework for automatic statistical modelling: these models have a\nhigh degree of flexibility, and automatically calibrated complexity. However,\nautomating human expertise remains elusive; for example, Gaussian processes\nwith standard kernels struggle on function extrapolation problems that are\ntrivial for human learners. In this paper, we create function extrapolation\nproblems and acquire human responses, and then design a kernel learning\nframework to reverse engineer the inductive biases of human learners across a\nset of behavioral experiments. We use the learned kernels to gain psychological\ninsights and to extrapolate in human-like ways that go beyond traditional\nstationary and polynomial kernels. Finally, we investigate Occam's razor in\nhuman and Gaussian process based function learning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 07:39:47 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 18:21:11 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 18:07:35 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Dann", "Christoph", ""], ["Lucas", "Christopher G.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1510.07471", "submitter": "Cheng Chen", "authors": "Cheng Chen, Shuang Liu, Zhihua Zhang, Wu-Jun Li", "title": "A Parallel algorithm for $\\mathcal{X}$-Armed bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The target of $\\mathcal{X}$-armed bandit problem is to find the global\nmaximum of an unknown stochastic function $f$, given a finite budget of $n$\nevaluations. Recently, $\\mathcal{X}$-armed bandits have been widely used in\nmany situations. Many of these applications need to deal with large-scale data\nsets. To deal with these large-scale data sets, we study a distributed setting\nof $\\mathcal{X}$-armed bandits, where $m$ players collaborate to find the\nmaximum of the unknown function. We develop a novel anytime distributed\n$\\mathcal{X}$-armed bandit algorithm. Compared with prior work on\n$\\mathcal{X}$-armed bandits, our algorithm uses a quite different searching\nstrategy so as to fit distributed learning scenarios. Our theoretical analysis\nshows that our distributed algorithm is $m$ times faster than the classical\nsingle-player algorithm. Moreover, the number of communication rounds of our\nalgorithm is only logarithmic in $mn$. The numerical results show that our\nmethod can make effective use of every players to minimize the loss. Thus, our\ndistributed approach is attractive and useful.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 13:23:48 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Chen", "Cheng", ""], ["Liu", "Shuang", ""], ["Zhang", "Zhihua", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1510.07526", "submitter": "Yang Yu", "authors": "Yang Yu, Wei Zhang, Chung-Wei Hang, Bing Xiang and Bowen Zhou", "title": "Empirical Study on Deep Learning Models for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore deep learning models with memory component or\nattention mechanism for question answering task. We combine and compare three\nmodels, Neural Machine Translation, Neural Turing Machine, and Memory Networks\nfor a simulated QA data set. This paper is the first one that uses Neural\nMachine Translation and Neural Turing Machines for solving QA tasks. Our\nresults suggest that the combination of attention and memory have potential to\nsolve certain QA problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 16:03:27 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 16:56:48 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 15:36:56 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Yu", "Yang", ""], ["Zhang", "Wei", ""], ["Hang", "Chung-Wei", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1510.07545", "submitter": "Tobias Schnabel", "authors": "Tobias Schnabel, Paul N. Bennett, Susan T. Dumais and Thorsten\n  Joachims", "title": "Using Shortlists to Support Decision Making and Improve Recommender\n  System Performance", "comments": "11 pages in WWW 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study shortlists as an interface component for recommender\nsystems with the dual goal of supporting the user's decision process, as well\nas improving implicit feedback elicitation for increased recommendation\nquality. A shortlist is a temporary list of candidates that the user is\ncurrently considering, e.g., a list of a few movies the user is currently\nconsidering for viewing. From a cognitive perspective, shortlists serve as\ndigital short-term memory where users can off-load the items under\nconsideration -- thereby decreasing their cognitive load. From a machine\nlearning perspective, adding items to the shortlist generates a new implicit\nfeedback signal as a by-product of exploration and decision making which can\nimprove recommendation quality. Shortlisting therefore provides additional data\nfor training recommendation systems without the increases in cognitive load\nthat requesting explicit feedback would incur.\n  We perform an user study with a movie recommendation setup to compare\ninterfaces that offer shortlist support with those that do not. From the user\nstudies we conclude: (i) users make better decisions with a shortlist; (ii)\nusers prefer an interface with shortlist support; and (iii) the additional\nimplicit feedback from sessions with a shortlist improves the quality of\nrecommendations by nearly a factor of two.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 16:49:07 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 12:16:23 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Schnabel", "Tobias", ""], ["Bennett", "Paul N.", ""], ["Dumais", "Susan T.", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1510.07609", "submitter": "Joseph Wang", "authors": "Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama", "title": "Efficient Learning by Directed Acyclic Graph For Resource Constrained\n  Prediction", "comments": "To appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of reducing test-time acquisition costs in\nclassification systems. Our goal is to learn decision rules that adaptively\nselect sensors for each example as necessary to make a confident prediction. We\nmodel our system as a directed acyclic graph (DAG) where internal nodes\ncorrespond to sensor subsets and decision functions at each node choose whether\nto acquire a new sensor or classify using the available measurements. This\nproblem can be naturally posed as an empirical risk minimization over training\ndata. Rather than jointly optimizing such a highly coupled and non-convex\nproblem over all decision nodes, we propose an efficient algorithm motivated by\ndynamic programming. We learn node policies in the DAG by reducing the global\nobjective to a series of cost sensitive learning problems. Our approach is\ncomputationally efficient and has proven guarantees of convergence to the\noptimal system for a fixed architecture. In addition, we present an extension\nto map other budgeted learning problems with large number of sensors to our DAG\narchitecture and demonstrate empirical performance exceeding state-of-the-art\nalgorithms for data composed of both few and many sensors.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 19:40:10 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Wang", "Joseph", ""], ["Trapeznikov", "Kirill", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1510.07641", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, David C. Kale, Randall C. Wetzel", "title": "Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel application of LSTM recurrent neural networks to\nmultilabel classification of diagnoses given variable-length time series of\nclinical measurements. Our method outperforms a strong baseline on a variety of\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 20:18:56 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 21:28:55 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Kale", "David C.", ""], ["Wetzel", "Randall C.", ""]]}, {"id": "1510.07727", "submitter": "Art Owen", "authors": "Art B. Owen", "title": "Statistically efficient thinning of a Markov chain sampler", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to subsample Markov chain output to reduce the storage burden.\nGeyer (1992) shows that discarding $k-1$ out of every $k$ observations will not\nimprove statistical efficiency, as quantified through variance in a given\ncomputational budget. That observation is often taken to mean that thinning\nMCMC output cannot improve statistical efficiency. Here we suppose that it\ncosts one unit of time to advance a Markov chain and then $\\theta>0$ units of\ntime to compute a sampled quantity of interest. For a thinned process, that\ncost $\\theta$ is incurred less often, so it can be advanced through more\nstages. Here we provide examples to show that thinning will improve statistical\nefficiency if $\\theta$ is large and the sample autocorrelations decay slowly\nenough. If the lag $\\ell\\ge1$ autocorrelations of a scalar measurement satisfy\n$\\rho_\\ell\\ge\\rho_{\\ell+1}\\ge0$, then there is always a $\\theta<\\infty$ at\nwhich thinning becomes more efficient for averages of that scalar. Many sample\nautocorrelation functions resemble first order AR(1) processes with $\\rho_\\ell\n=\\rho^{|\\ell|}$ for some $-1<\\rho<1$. For an AR(1) process it is possible to\ncompute the most efficient subsampling frequency $k$. The optimal $k$ grows\nrapidly as $\\rho$ increases towards $1$. The resulting efficiency gain depends\nprimarily on $\\theta$, not $\\rho$. Taking $k=1$ (no thinning) is optimal when\n$\\rho\\le0$. For $\\rho>0$ it is optimal if and only if $\\theta \\le\n(1-\\rho)^2/(2\\rho)$. This efficiency gain never exceeds $1+\\theta$. This paper\nalso gives efficiency bounds for autocorrelations bounded between those of two\nAR(1) processes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 00:00:54 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 21:55:29 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2015 22:06:58 GMT"}, {"version": "v4", "created": "Sun, 31 Jan 2016 23:47:55 GMT"}, {"version": "v5", "created": "Wed, 21 Sep 2016 01:03:26 GMT"}, {"version": "v6", "created": "Tue, 14 Mar 2017 22:51:02 GMT"}, {"version": "v7", "created": "Tue, 11 Apr 2017 02:23:00 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Owen", "Art B.", ""]]}, {"id": "1510.07740", "submitter": "Saeed Saremi", "authors": "Saeed Saremi, Terrence J. Sejnowski", "title": "The Wilson Machine for Image Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the distribution of natural images is one of the hardest and most\nimportant problems in machine learning. The problem remains open, because the\nenormous complexity of the structures in natural images spans all length\nscales. We break down the complexity of the problem and show that the hierarchy\nof structures in natural images fuels a new class of learning algorithms based\non the theory of critical phenomena and stochastic processes. We approach this\nproblem from the perspective of the theory of critical phenomena, which was\ndeveloped in condensed matter physics to address problems with infinite\nlength-scale fluctuations, and build a framework to integrate the criticality\nof natural images into a learning algorithm. The problem is broken down by\nmapping images into a hierarchy of binary images, called bitplanes. In this\nrepresentation, the top bitplane is critical, having fluctuations in structures\nover a vast range of scales. The bitplanes below go through a gradual\nstochastic heating process to disorder. We turn this representation into a\ndirected probabilistic graphical model, transforming the learning problem into\nthe unsupervised learning of the distribution of the critical bitplane and the\nsupervised learning of the conditional distributions for the remaining\nbitplanes. We learnt the conditional distributions by logistic regression in a\nconvolutional architecture. Conditioned on the critical binary image, this\nsimple architecture can generate large, natural-looking images, with many\nshades of gray, without the use of hidden units, unprecedented in the studies\nof natural images. The framework presented here is a major step in bringing\ncriticality and stochastic processes to machine learning and in studying\nnatural image statistics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 01:04:05 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 22:28:55 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Saremi", "Saeed", ""], ["Sejnowski", "Terrence J.", ""]]}, {"id": "1510.07925", "submitter": "Ji Liu", "authors": "Yijun Huang and Ji Liu", "title": "Exclusive Sparsity Norm Minimization with Random Groups via Cone\n  Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Many practical applications such as gene expression analysis, multi-task\nlearning, image recognition, signal processing, and medical data analysis\npursue a sparse solution for the feature selection purpose and particularly\nfavor the nonzeros \\emph{evenly} distributed in different groups. The exclusive\nsparsity norm has been widely used to serve to this purpose. However, it still\nlacks systematical studies for exclusive sparsity norm optimization. This paper\noffers two main contributions from the optimization perspective: 1) We provide\nseveral efficient algorithms to solve exclusive sparsity norm minimization with\neither smooth loss or hinge loss (non-smooth loss). All algorithms achieve the\noptimal convergence rate $O(1/k^2)$ ($k$ is the iteration number). To the best\nof our knowledge, this is the first time to guarantee such convergence rate for\nthe general exclusive sparsity norm minimization; 2) When the group information\nis unavailable to define the exclusive sparsity norm, we propose to use the\nrandom grouping scheme to construct groups and prove that if the number of\ngroups is appropriately chosen, the nonzeros (true features) would be grouped\nin the ideal way with high probability. Empirical studies validate the\nefficiency of proposed algorithms, and the effectiveness of random grouping\nscheme on the proposed exclusive SVM formulation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 14:58:17 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Huang", "Yijun", ""], ["Liu", "Ji", ""]]}, {"id": "1510.08108", "submitter": "Yifan Wu", "authors": "Yifan Wu, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari", "title": "Online Learning with Gaussian Payoffs and Side Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sequential learning problem with Gaussian payoffs and side\ninformation: after selecting an action $i$, the learner receives information\nabout the payoff of every action $j$ in the form of Gaussian observations whose\nmean is the same as the mean payoff, but the variance depends on the pair\n$(i,j)$ (and may be infinite). The setup allows a more refined information\ntransfer from one action to another than previous partial monitoring setups,\nincluding the recently introduced graph-structured feedback case. For the first\ntime in the literature, we provide non-asymptotic problem-dependent lower\nbounds on the regret of any algorithm, which recover existing asymptotic\nproblem-dependent lower bounds and finite-time minimax lower bounds available\nin the literature. We also provide algorithms that achieve the\nproblem-dependent lower bound (up to some universal constant factor) or the\nminimax lower bounds (up to logarithmic factors).\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 21:59:33 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Wu", "Yifan", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1510.08231", "submitter": "Hachem Kadri", "authors": "Hachem Kadri (LIF), Emmanuel Duflos (CRIStAL), Philippe Preux\n  (CRIStAL, SEQUEL), St\\'ephane Canu (LITIS), Alain Rakotomamonjy (LITIS),\n  Julien Audiffren (CMLA)", "title": "Operator-valued Kernels for Learning from Functional Response Data", "comments": "in Journal of Machine Learning Research (JMLR), 2016", "journal-ref": "Journal of Machine Learning Research 17 (2016) 1-54", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problems of supervised classification and\nregression in the case where attributes and labels are functions: a data is\nrepresented by a set of functions, and the label is also a function. We focus\non the use of reproducing kernel Hilbert space theory to learn from such\nfunctional data. Basic concepts and properties of kernel-based learning are\nextended to include the estimation of function-valued functions. In this\nsetting, the representer theorem is restated, a set of rigorously defined\ninfinite-dimensional operator-valued kernels that can be valuably applied when\nthe data are functions is described, and a learning algorithm for nonlinear\nfunctional data analysis is introduced. The methodology is illustrated through\nspeech and audio signal processing experiments.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 09:18:50 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 14:10:25 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 14:29:29 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Kadri", "Hachem", "", "LIF"], ["Duflos", "Emmanuel", "", "CRIStAL"], ["Preux", "Philippe", "", "CRIStAL, SEQUEL"], ["Canu", "St\u00e9phane", "", "LITIS"], ["Rakotomamonjy", "Alain", "", "LITIS"], ["Audiffren", "Julien", "", "CMLA"]]}, {"id": "1510.08370", "submitter": "Hoang Vu Nguyen", "authors": "Hoang-Vu Nguyen and Jilles Vreeken", "title": "Canonical Divergence Analysis", "comments": "Submission to AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to analyze the relation between two random vectors that may\npotentially have both different number of attributes as well as realizations,\nand which may even not have a joint distribution. This problem arises in many\npractical domains, including biology and architecture. Existing techniques\nassume the vectors to have the same domain or to be jointly distributed, and\nhence are not applicable. To address this, we propose Canonical Divergence\nAnalysis (CDA). We introduce three instantiations, each of which permits\npractical implementation. Extensive empirical evaluation shows the potential of\nour method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 16:31:57 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Nguyen", "Hoang-Vu", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1510.08382", "submitter": "Hoang Vu Nguyen", "authors": "Hoang-Vu Nguyen and Jilles Vreeken", "title": "Flexibly Mining Better Subgroups", "comments": "Submission to SDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In subgroup discovery, also known as supervised pattern mining, discovering\nhigh quality one-dimensional subgroups and refinements of these is a crucial\ntask. For nominal attributes, this is relatively straightforward, as we can\nconsider individual attribute values as binary features. For numerical\nattributes, the task is more challenging as individual numeric values are not\nreliable statistics. Instead, we can consider combinations of adjacent values,\ni.e. bins. Existing binning strategies, however, are not tailored for subgroup\ndiscovery. That is, they do not directly optimize for the quality of subgroups,\ntherewith potentially degrading the mining result.\n  To address this issue, we propose FLEXI. In short, with FLEXI we propose to\nuse optimal binning to find high quality binary features for both numeric and\nordinal attributes. We instantiate FLEXI with various quality measures and show\nhow to achieve efficiency accordingly. Experiments on both synthetic and\nreal-world data sets show that FLEXI outperforms state of the art with up to 25\ntimes improvement in subgroup quality.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 17:18:46 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Nguyen", "Hoang-Vu", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1510.08385", "submitter": "Hoang Vu Nguyen", "authors": "Hoang-Vu Nguyen and Jilles Vreeken", "title": "Linear-time Detection of Non-linear Changes in Massively High\n  Dimensional Time Series", "comments": "Submission to SDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection in multivariate time series has applications in many\ndomains, including health care and network monitoring. A common approach to\ndetect changes is to compare the divergence between the distributions of a\nreference window and a test window. When the number of dimensions is very\nlarge, however, the naive approach has both quality and efficiency issues: to\nensure robustness the window size needs to be large, which not only leads to\nmissed alarms but also increases runtime.\n  To this end, we propose LIGHT, a linear-time algorithm for robustly detecting\nnon-linear changes in massively high dimensional time series. Importantly,\nLIGHT provides high flexibility in choosing the window size, allowing the\ndomain expert to fit the level of details required. To do such, we 1) perform\nscalable PCA to reduce dimensionality, 2) perform scalable factorization of the\njoint distribution, and 3) scalably compute divergences between these lower\ndimensional distributions. Extensive empirical evaluation on both synthetic and\nreal-world data show that LIGHT outperforms state of the art with up to 100%\nimprovement in both quality and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 17:28:38 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Nguyen", "Hoang-Vu", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1510.08389", "submitter": "Hoang Vu Nguyen", "authors": "Hoang-Vu Nguyen and Jilles Vreeken", "title": "Universal Dependency Analysis", "comments": "Submission to SDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most data is multi-dimensional. Discovering whether any subset of dimensions,\nor subspaces, of such data is significantly correlated is a core task in data\nmining. To do so, we require a measure that quantifies how correlated a\nsubspace is. For practical use, such a measure should be universal in the sense\nthat it captures correlation in subspaces of any dimensionality and allows to\nmeaningfully compare correlation scores across different subspaces, regardless\nhow many dimensions they have and what specific statistical properties their\ndimensions possess. Further, it would be nice if the measure can\nnon-parametrically and efficiently capture both linear and non-linear\ncorrelations.\n  In this paper, we propose UDS, a multivariate correlation measure that\nfulfills all of these desiderata. In short, we define \\uds based on cumulative\nentropy and propose a principled normalization scheme to bring its scores\nacross different subspaces to the same domain, enabling universal correlation\nassessment. UDS is purely non-parametric as we make no assumption on data\ndistributions nor types of correlation. To compute it on empirical data, we\nintroduce an efficient and non-parametric method. Extensive experiments show\nthat UDS outperforms state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 17:40:18 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Nguyen", "Hoang-Vu", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1510.08520", "submitter": "Yingzhen Yang", "authors": "Yingzhen Yang, Jiashi Feng, Jianchao Yang, Thomas S. Huang", "title": "Learning with $\\ell^{0}$-Graph: $\\ell^{0}$-Induced Sparse Subspace\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse subspace clustering methods, such as Sparse Subspace Clustering (SSC)\n\\cite{ElhamifarV13} and $\\ell^{1}$-graph \\cite{YanW09,ChengYYFH10}, are\neffective in partitioning the data that lie in a union of subspaces. Most of\nthose methods use $\\ell^{1}$-norm or $\\ell^{2}$-norm with thresholding to\nimpose the sparsity of the constructed sparse similarity graph, and certain\nassumptions, e.g. independence or disjointness, on the subspaces are required\nto obtain the subspace-sparse representation, which is the key to their\nsuccess. Such assumptions are not guaranteed to hold in practice and they limit\nthe application of sparse subspace clustering on subspaces with general\nlocation. In this paper, we propose a new sparse subspace clustering method\nnamed $\\ell^{0}$-graph. In contrast to the required assumptions on subspaces\nfor most existing sparse subspace clustering methods, it is proved that\nsubspace-sparse representation can be obtained by $\\ell^{0}$-graph for\narbitrary distinct underlying subspaces almost surely under the mild i.i.d.\nassumption on the data generation. We develop a proximal method to obtain the\nsub-optimal solution to the optimization problem of $\\ell^{0}$-graph with\nproved guarantee of convergence. Moreover, we propose a regularized\n$\\ell^{0}$-graph that encourages nearby data to have similar neighbors so that\nthe similarity graph is more aligned within each cluster and the graph\nconnectivity issue is alleviated. Extensive experimental results on various\ndata sets demonstrate the superiority of $\\ell^{0}$-graph compared to other\ncompeting clustering methods, as well as the effectiveness of regularized\n$\\ell^{0}$-graph.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 22:48:09 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 07:11:42 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Yang", "Yingzhen", ""], ["Feng", "Jiashi", ""], ["Yang", "Jianchao", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1510.08532", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang", "title": "The Singular Value Decomposition, Applications and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The singular value decomposition (SVD) is not only a classical theory in\nmatrix computation and analysis, but also is a powerful tool in machine\nlearning and modern data analysis. In this tutorial we first study the basic\nnotion of SVD and then show the central role of SVD in matrices. Using\nmajorization theory, we consider variational principles of singular values and\neigenvalues. Built on SVD and a theory of symmetric gauge functions, we discuss\nunitarily invariant norms, which are then used to formulate general results for\nmatrix low rank approximation. We study the subdifferentials of unitarily\ninvariant norms. These results would be potentially useful in many machine\nlearning problems such as matrix completion and matrix data classification.\nFinally, we discuss matrix low rank approximation and its recent developments\nsuch as randomized SVD, approximate matrix multiplication, CUR decomposition,\nand Nystrom approximation. Randomized algorithms are important approaches to\nlarge scale SVD as well as fast matrix computations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 00:59:53 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Zhang", "Zhihua", ""]]}, {"id": "1510.08565", "submitter": "Kaisheng Yao", "authors": "Kaisheng Yao and Geoffrey Zweig and Baolin Peng", "title": "Attention with Intention for a Neural Network Conversation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a conversation or a dialogue process, attention and intention play\nintrinsic roles. This paper proposes a neural network based approach that\nmodels the attention and intention processes. It essentially consists of three\nrecurrent networks. The encoder network is a word-level model representing\nsource side sentences. The intention network is a recurrent network that models\nthe dynamics of the intention process. The decoder network is a recurrent\nnetwork produces responses to the input from the source side. It is a language\nmodel that is dependent on the intention and has an attention mechanism to\nattend to particular source side words, when predicting a symbol in the\nresponse. The model is trained end-to-end without labeling data. Experiments\nshow that this model generates natural responses to user inputs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 05:31:28 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 02:17:15 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2015 07:26:01 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Yao", "Kaisheng", ""], ["Zweig", "Geoffrey", ""], ["Peng", "Baolin", ""]]}, {"id": "1510.08628", "submitter": "Jianfei Chen", "authors": "Jianfei Chen, Kaiwei Li, Jun Zhu, Wenguang Chen", "title": "WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet\n  Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient and scalable algorithms for Latent Dirichlet Allocation\n(LDA) is of wide interest for many applications. Previous work has developed an\nO(1) Metropolis-Hastings sampling method for each token. However, the\nperformance is far from being optimal due to random accesses to the parameter\nmatrices and frequent cache misses.\n  In this paper, we first carefully analyze the memory access efficiency of\nexisting algorithms for LDA by the scope of random access, which is the size of\nthe memory region in which random accesses fall, within a short period of time.\nWe then develop WarpLDA, an LDA sampler which achieves both the best O(1) time\ncomplexity per token and the best O(K) scope of random access. Our empirical\nresults in a wide range of testing conditions demonstrate that WarpLDA is\nconsistently 5-15x faster than the state-of-the-art Metropolis-Hastings based\nLightLDA, and is comparable or faster than the sparsity aware F+LDA. With\nWarpLDA, users can learn up to one million topics from hundreds of millions of\ndocuments in a few hours, at an unprecedentedly throughput of 11G tokens per\nsecond.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 10:33:20 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 06:29:30 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Chen", "Jianfei", ""], ["Li", "Kaiwei", ""], ["Zhu", "Jun", ""], ["Chen", "Wenguang", ""]]}, {"id": "1510.08660", "submitter": "Vincent Michalski", "authors": "Samira Ebrahimi Kahou, Vincent Michalski, Roland Memisevic", "title": "RATM: Recurrent Attentive Tracking Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an attention-based modular neural framework for computer vision.\nThe framework uses a soft attention mechanism allowing models to be trained\nwith gradient descent. It consists of three modules: a recurrent attention\nmodule controlling where to look in an image or video frame, a\nfeature-extraction module providing a representation of what is seen, and an\nobjective module formalizing why the model learns its attentive behavior. The\nattention module allows the model to focus computation on task-related\ninformation in the input. We apply the framework to several object tracking\ntasks and explore various design choices. We experiment with three data sets,\nbouncing ball, moving digits and the real-world KTH data set. The proposed\nRecurrent Attentive Tracking Model performs well on all three tasks and can\ngeneralize to related but previously unseen sequences from a challenging\ntracking data set.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 12:06:56 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 21:04:30 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2015 13:47:19 GMT"}, {"version": "v4", "created": "Thu, 28 Apr 2016 07:32:03 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Kahou", "Samira Ebrahimi", ""], ["Michalski", "Vincent", ""], ["Memisevic", "Roland", ""]]}, {"id": "1510.08692", "submitter": "Xiaocheng Shang", "authors": "Xiaocheng Shang, Zhanxing Zhu, Benedict Leimkuhler, Amos J. Storkey", "title": "Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale\n  Bayesian Sampling", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, 28, 37-45,\n  (2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo sampling for Bayesian posterior inference is a common approach\nused in machine learning. The Markov Chain Monte Carlo procedures that are used\nare often discrete-time analogues of associated stochastic differential\nequations (SDEs). These SDEs are guaranteed to leave invariant the required\nposterior distribution. An area of current research addresses the computational\nbenefits of stochastic gradient methods in this setting. Existing techniques\nrely on estimating the variance or covariance of the subsampling error, and\ntypically assume constant variance. In this article, we propose a\ncovariance-controlled adaptive Langevin thermostat that can effectively\ndissipate parameter-dependent noise while maintaining a desired target\ndistribution. The proposed method achieves a substantial speedup over popular\nalternative schemes for large-scale machine learning applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 13:57:11 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 21:23:53 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Shang", "Xiaocheng", ""], ["Zhu", "Zhanxing", ""], ["Leimkuhler", "Benedict", ""], ["Storkey", "Amos J.", ""]]}, {"id": "1510.08713", "submitter": "Nipun Batra", "authors": "Nipun Batra and Rishi Baijal and Amarjeet Singh and Kamin Whitehouse", "title": "How good is good enough? Re-evaluating the bar for energy disaggregation", "comments": "Under submission to Percom 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the early 1980s, the research community has developed ever more\nsophisticated algorithms for the problem of energy disaggregation, but despite\ndecades of research, there is still a dearth of applications with demonstrated\nvalue. In this work, we explore a question that is highly pertinent to this\nresearch community: how good does energy disaggregation need to be in order to\ninfer characteristics of a household? We present novel techniques that use\nunsupervised energy disaggregation to predict both household occupancy and\nstatic properties of the household such as size of the home and number of\noccupants. Results show that basic disaggregation approaches performs up to 30%\nbetter at occupancy estimation than using aggregate power data alone, and are\nup to 10% better at estimating static household characteristics. These results\nshow that even rudimentary energy disaggregation techniques are sufficient for\nimproved inference of household characteristics. To conclude, we re-evaluate\nthe bar set by the community for energy disaggregation accuracy and try to\nanswer the question \"how good is good enough?\"\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 08:34:07 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Batra", "Nipun", ""], ["Baijal", "Rishi", ""], ["Singh", "Amarjeet", ""], ["Whitehouse", "Kamin", ""]]}, {"id": "1510.08829", "submitter": "Eric Hunsberger", "authors": "Eric Hunsberger and Chris Eliasmith", "title": "Spiking Deep Networks with LIF Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train spiking deep networks using leaky integrate-and-fire (LIF) neurons,\nand achieve state-of-the-art results for spiking networks on the CIFAR-10 and\nMNIST datasets. This demonstrates that biologically-plausible spiking LIF\nneurons can be integrated into deep networks can perform as well as other\nspiking models (e.g. integrate-and-fire). We achieved this result by softening\nthe LIF response function, such that its derivative remains bounded, and by\ntraining the network with noise to provide robustness against the variability\nintroduced by spikes. Our method is general and could be applied to other\nneuron types, including those used on modern neuromorphic hardware. Our work\nbrings more biological realism into modern image classification models, with\nthe hope that these models can inform how the brain performs this difficult\ntask. It also provides new methods for training deep networks to run on\nneuromorphic hardware, with the aim of fast, power-efficient image\nclassification for robotics applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 19:24:03 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Hunsberger", "Eric", ""], ["Eliasmith", "Chris", ""]]}, {"id": "1510.08865", "submitter": "Kai Wei", "authors": "Kai Wei, Rishabh Iyer, Shengjie Wang, Wenruo Bai, Jeff Bilmes", "title": "Mixed Robust/Average Submodular Partitioning: Fast Algorithms,\n  Guarantees, and Applications to Parallel Machine Learning and Multi-Label\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two mixed robust/average-case submodular partitioning problems that\nwe collectively call Submodular Partitioning. These problems generalize both\npurely robust instances of the problem (namely max-min submodular fair\nallocation (SFA) and min-max submodular load balancing (SLB) and also\ngeneralize average-case instances (that is the submodular welfare problem (SWP)\nand submodular multiway partition (SMP). While the robust versions have been\nstudied in the theory community, existing work has focused on tight\napproximation guarantees, and the resultant algorithms are not, in general,\nscalable to very large real-world applications. This is in contrast to the\naverage case, where most of the algorithms are scalable. In the present paper,\nwe bridge this gap, by proposing several new algorithms (including those based\non greedy, majorization-minimization, minorization-maximization, and relaxation\nalgorithms) that not only scale to large sizes but that also achieve\ntheoretical approximation guarantees close to the state-of-the-art, and in some\ncases achieve new tight bounds. We also provide new scalable algorithms that\napply to additive combinations of the robust and average-case extreme\nobjectives. We show that these problems have many applications in machine\nlearning (ML). This includes: 1) data partitioning and load balancing for\ndistributed machine algorithms on parallel machines; 2) data clustering; and 3)\nmulti-label image segmentation with (only) Boolean submodular functions via\npixel partitioning. We empirically demonstrate the efficacy of our algorithms\non real-world problems involving data partitioning for distributed optimization\nof standard machine learning objectives (including both convex and deep neural\nnetwork objectives), and also on purely unsupervised (i.e., no supervised or\nsemi-supervised learning, and no interactive segmentation) image segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:07:32 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 04:00:41 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Wei", "Kai", ""], ["Iyer", "Rishabh", ""], ["Wang", "Shengjie", ""], ["Bai", "Wenruo", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1510.08896", "submitter": "Cameron Musco", "authors": "Chi Jin and Sham M. Kakade and Cameron Musco and Praneeth Netrapalli\n  and Aaron Sidford", "title": "Robust Shift-and-Invert Preconditioning: Faster and More Sample\n  Efficient Algorithms for Eigenvector Computation", "comments": "Manuscript outdated. Updated version at arxiv:1605.08754", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide faster algorithms and improved sample complexities for\napproximating the top eigenvector of a matrix.\n  Offline Setting: Given an $n \\times d$ matrix $A$, we show how to compute an\n$\\epsilon$ approximate top eigenvector in time $\\tilde O ( [nnz(A) + \\frac{d\n\\cdot sr(A)}{gap^2}]\\cdot \\log 1/\\epsilon )$ and $\\tilde O([\\frac{nnz(A)^{3/4}\n(d \\cdot sr(A))^{1/4}}{\\sqrt{gap}}]\\cdot \\log1/\\epsilon )$. Here $sr(A)$ is the\nstable rank and $gap$ is the multiplicative eigenvalue gap. By separating the\n$gap$ dependence from $nnz(A)$ we improve on the classic power and Lanczos\nmethods. We also improve prior work using fast subspace embeddings and\nstochastic optimization, giving significantly improved dependencies on $sr(A)$\nand $\\epsilon$. Our second running time improves this further when $nnz(A) \\le\n\\frac{d\\cdot sr(A)}{gap^2}$.\n  Online Setting: Given a distribution $D$ with covariance matrix $\\Sigma$ and\na vector $x_0$ which is an $O(gap)$ approximate top eigenvector for $\\Sigma$,\nwe show how to refine to an $\\epsilon$ approximation using $\\tilde\nO(\\frac{v(D)}{gap^2} + \\frac{v(D)}{gap \\cdot \\epsilon})$ samples from $D$. Here\n$v(D)$ is a natural variance measure. Combining our algorithm with previous\nwork to initialize $x_0$, we obtain a number of improved sample complexity and\nruntime results. For general distributions, we achieve asymptotically optimal\naccuracy as a function of sample size as the number of samples grows large.\n  Our results center around a robust analysis of the classic method of\nshift-and-invert preconditioning to reduce eigenvector computation to\napproximately solving a sequence of linear systems. We then apply fast SVRG\nbased approximate system solvers to achieve our claims. We believe our results\nsuggest the general effectiveness of shift-and-invert based approaches and\nimply that further computational gains may be reaped in practice.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:47:27 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 02:40:00 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Musco", "Cameron", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1510.08906", "submitter": "Christoph Dann", "authors": "Christoph Dann, Emma Brunskill", "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning", "comments": "28 pages, appeared in Neural Information Processing Systems (NIPS)\n  2015, updated version with fixed typos and modified Lemma 1 and Lemma C.5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been significant progress in understanding reinforcement\nlearning in discounted infinite-horizon Markov decision processes (MDPs) by\nderiving tight sample complexity bounds. However, in many real-world\napplications, an interactive learning agent operates for a fixed or bounded\nperiod of time, for example tutoring students for exams or handling customer\nservice requests. Such scenarios can often be better treated as episodic\nfixed-horizon MDPs, for which only looser bounds on the sample complexity\nexist. A natural notion of sample complexity in this setting is the number of\nepisodes required to guarantee a certain performance with high probability (PAC\nguarantee). In this paper, we derive an upper PAC bound $\\tilde\nO(\\frac{|\\mathcal S|^2 |\\mathcal A| H^2}{\\epsilon^2} \\ln\\frac 1 \\delta)$ and a\nlower PAC bound $\\tilde \\Omega(\\frac{|\\mathcal S| |\\mathcal A| H^2}{\\epsilon^2}\n\\ln \\frac 1 {\\delta + c})$ that match up to log-terms and an additional linear\ndependency on the number of states $|\\mathcal S|$. The lower bound is the first\nof its kind for this setting. Our upper bound leverages Bernstein's inequality\nto improve on previous bounds for episodic finite-horizon MDPs which have a\ntime-horizon dependency of at least $H^3$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 21:14:42 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 16:36:03 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 15:27:28 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Dann", "Christoph", ""], ["Brunskill", "Emma", ""]]}, {"id": "1510.08949", "submitter": "Philip Bachman", "authors": "Philip Bachman and David Krueger and Doina Precup", "title": "Testing Visual Attention in Dynamic Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate attention as the active pursuit of useful information. This\ncontrasts with attention as a mechanism for the attenuation of irrelevant\ninformation. We also consider the role of short-term memory, whose use is\ncritical to any model incapable of simultaneously perceiving all information on\nwhich its output depends. We present several simple synthetic tasks, which\nbecome considerably more interesting when we impose strong constraints on how a\nmodel can interact with its input, and on how long it can take to produce its\noutput. We develop a model with a different structure from those seen in\nprevious work, and we train it using stochastic variational inference with a\nlearned proposal distribution.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 01:39:54 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Bachman", "Philip", ""], ["Krueger", "David", ""], ["Precup", "Doina", ""]]}, {"id": "1510.08956", "submitter": "Jonas Mueller", "authors": "Jonas Mueller and Tommi Jaakkola", "title": "Principal Differences Analysis: Interpretable Characterization of\n  Differences between Distributions", "comments": "Advances in Neural Information Processing Systems 28 (NIPS 2015)", "journal-ref": "Advances in Neural Information Processing Systems 28: 1702-1710,\n  2015", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce principal differences analysis (PDA) for analyzing differences\nbetween high-dimensional distributions. The method operates by finding the\nprojection that maximizes the Wasserstein divergence between the resulting\nunivariate populations. Relying on the Cramer-Wold device, it requires no\nassumptions about the form of the underlying distributions, nor the nature of\ntheir inter-class differences. A sparse variant of the method is introduced to\nidentify features responsible for the differences. We provide algorithms for\nboth the original minimax formulation as well as its semidefinite relaxation.\nIn addition to deriving some convergence results, we illustrate how the\napproach may be applied to identify differences between cell populations in the\nsomatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our\nbroader framework extends beyond the specific choice of Wasserstein divergence.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 03:06:00 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Mueller", "Jonas", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1510.08971", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Robust Subspace Clustering via Tighter Rank Approximation", "comments": "ACM CIKM 2015", "journal-ref": null, "doi": "10.1145/2806416.2806506", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix rank minimization problem is in general NP-hard. The nuclear norm is\nused to substitute the rank function in many recent studies. Nevertheless, the\nnuclear norm approximation adds all singular values together and the\napproximation error may depend heavily on the magnitudes of singular values.\nThis might restrict its capability in dealing with many practical problems. In\nthis paper, an arctangent function is used as a tighter approximation to the\nrank function. We use it on the challenging subspace clustering problem. For\nthis nonconvex minimization problem, we develop an effective optimization\nprocedure based on a type of augmented Lagrange multipliers (ALM) method.\nExtensive experiments on face clustering and motion segmentation show that the\nproposed method is effective for rank approximation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 05:34:49 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1510.08974", "submitter": "Koby Crammer", "authors": "Daniel Barsky and Koby Crammer", "title": "CONQUER: Confusion Queried Online Bandit Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new recommendation setting for picking out two items from a\ngiven set to be highlighted to a user, based on contextual input. These two\nitems are presented to a user who chooses one of them, possibly stochastically,\nwith a bias that favours the item with the higher value. We propose a\nsecond-order algorithm framework that members of it use uses relative\nupper-confidence bounds to trade off exploration and exploitation, and some\nexplore via sampling. We analyze one algorithm in this framework in an\nadversarial setting with only mild assumption on the data, and prove a regret\nbound of $O(Q_T + \\sqrt{TQ_T\\log T} + \\sqrt{T}\\log T)$, where $T$ is the number\nof rounds and $Q_T$ is the cumulative approximation error of item values using\na linear model. Experiments with product reviews from 33 domains show the\nadvantage of our methods over algorithms designed for related settings, and\nthat UCB based algorithms are inferior to greed or sampling based algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 05:46:23 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Barsky", "Daniel", ""], ["Crammer", "Koby", ""]]}, {"id": "1510.08983", "submitter": "Yu Zhang", "authors": "Yu Zhang and Guoguo Chen and Dong Yu and Kaisheng Yao and Sanjeev\n  Khudanpur and James Glass", "title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the deep long short-term memory (DLSTM) recurrent\nneural networks by introducing gated direct connections between memory cells in\nadjacent layers. These direct links, called highway connections, enable\nunimpeded information flow across different layers and thus alleviate the\ngradient vanishing problem when building deeper LSTMs. We further introduce the\nlatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole\nhistory while keeping the latency under control. Efficient algorithms are\nproposed to train these novel networks using both frame and sequence\ndiscriminative criteria. Experiments on the AMI distant speech recognition\n(DSR) task indicate that we can train deeper LSTMs and achieve better\nimprovement from sequence training with highway LSTMs (HLSTMs). Our novel model\nobtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all\nprevious works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and\n$5.3\\%$ relative improvement respectively.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 06:40:14 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 09:48:01 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Yu", ""], ["Chen", "Guoguo", ""], ["Yu", "Dong", ""], ["Yao", "Kaisheng", ""], ["Khudanpur", "Sanjeev", ""], ["Glass", "James", ""]]}, {"id": "1510.08985", "submitter": "Yu Zhang", "authors": "Yu Zhang, Ekapol Chuangsuwanich, James Glass, Dong Yu", "title": "Prediction-Adaptation-Correction Recurrent Neural Networks for\n  Low-Resource Language Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the use of prediction-adaptation-correction\nrecurrent neural networks (PAC-RNNs) for low-resource speech recognition. A\nPAC-RNN is comprised of a pair of neural networks in which a {\\it correction}\nnetwork uses auxiliary information given by a {\\it prediction} network to help\nestimate the state probability. The information from the correction network is\nalso used by the prediction network in a recurrent loop. Our model outperforms\nother state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.\nMoreover, transfer learning from a language that is similar to the target\nlanguage can help improve performance further.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 06:42:03 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Yu", ""], ["Chuangsuwanich", "Ekapol", ""], ["Glass", "James", ""], ["Yu", "Dong", ""]]}, {"id": "1510.09123", "submitter": "Yan Zheng", "authors": "Jeff M. Phillips and Yan Zheng", "title": "Subsampling in Smoothed Range Spaces", "comments": "This is the full version of the paper which appeared in ALT 2015. 16\n  pages, 3 figures. In Algorithmic Learning Theory, pp. 224-238. Springer\n  International Publishing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider smoothed versions of geometric range spaces, so an element of the\nground set (e.g. a point) can be contained in a range with a non-binary value\nin $[0,1]$. Similar notions have been considered for kernels; we extend them to\nmore general types of ranges. We then consider approximations of these range\nspaces through $\\varepsilon $-nets and $\\varepsilon $-samples (aka\n$\\varepsilon$-approximations). We characterize when size bounds for\n$\\varepsilon $-samples on kernels can be extended to these more general\nsmoothed range spaces. We also describe new generalizations for $\\varepsilon\n$-nets to these range spaces and show when results from binary range spaces can\ncarry over to these smoothed ones.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 15:27:31 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Phillips", "Jeff M.", ""], ["Zheng", "Yan", ""]]}, {"id": "1510.09142", "submitter": "Greg Wayne", "authors": "Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval\n  Tassa, and Tom Erez", "title": "Learning Continuous Control Policies by Stochastic Value Gradients", "comments": "13 pages, NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for learning continuous control policies using\nbackpropagation. It supports stochastic control by treating stochasticity in\nthe Bellman equation as a deterministic function of exogenous noise. The\nproduct is a spectrum of general policy gradient algorithms that range from\nmodel-free methods with value functions to model-based methods without value\nfunctions. We use learned models but only require observations from the\nenvironment in- stead of observations from model-predicted trajectories,\nminimizing the impact of compounded model errors. We apply these algorithms\nfirst to a toy stochastic control problem and then to several physics-based\ncontrol problems in simulation. One of these variants, SVG(1), shows the\neffectiveness of learning models, value functions, and policies simultaneously\nin continuous domains.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 16:07:51 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Heess", "Nicolas", ""], ["Wayne", "Greg", ""], ["Silver", "David", ""], ["Lillicrap", "Timothy", ""], ["Tassa", "Yuval", ""], ["Erez", "Tom", ""]]}, {"id": "1510.09161", "submitter": "Trevor Campbell", "authors": "Trevor Campbell, Julian Straub, John W. Fisher III, Jonathan P. How", "title": "Streaming, Distributed Variational Inference for Bayesian Nonparametrics", "comments": "This paper was presented at NIPS 2015. Please use the following\n  BibTeX citation: @inproceedings{Campbell15_NIPS, Author = {Trevor Campbell\n  and Julian Straub and John W. {Fisher III} and Jonathan P. How}, Title =\n  {Streaming, Distributed Variational Inference for Bayesian Nonparametrics},\n  Booktitle = {Advances in Neural Information Processing Systems (NIPS)}, Year\n  = {2015}}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a methodology for creating streaming, distributed\ninference algorithms for Bayesian nonparametric (BNP) models. In the proposed\nframework, processing nodes receive a sequence of data minibatches, compute a\nvariational posterior for each, and make asynchronous streaming updates to a\ncentral model. In contrast to previous algorithms, the proposed framework is\ntruly streaming, distributed, asynchronous, learning-rate-free, and\ntruncation-free. The key challenge in developing the framework, arising from\nthe fact that BNP models do not impose an inherent ordering on their\ncomponents, is finding the correspondence between minibatch and central BNP\nposterior components before performing each update. To address this, the paper\ndevelops a combinatorial optimization problem over component correspondences,\nand provides an efficient solution technique. The paper concludes with an\napplication of the methodology to the DP mixture model, with experimental\nresults demonstrating its practical scalability and performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 17:04:33 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Campbell", "Trevor", ""], ["Straub", "Julian", ""], ["Fisher", "John W.", "III"], ["How", "Jonathan P.", ""]]}, {"id": "1510.09202", "submitter": "Hongyu Guo Ph.D", "authors": "Hongyu Guo", "title": "Generating Text with Deep Reinforcement Learning", "comments": "Accepted to the NIPS2015 Deep Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel schema for sequence to sequence learning with a Deep\nQ-Network (DQN), which decodes the output sequence iteratively. The aim here is\nto enable the decoder to first tackle easier portions of the sequences, and\nthen turn to cope with difficult parts. Specifically, in each iteration, an\nencoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the\ninput sequence, automatically create features to represent the internal states\nof and formulate a list of potential actions for the DQN. Take rephrasing a\nnatural sentence as an example. This list can contain ranked potential words.\nNext, the DQN learns to make decision on which action (e.g., word) will be\nselected from the list to modify the current decoded sequence. The newly\nmodified output sequence is subsequently used as the input to the DQN for the\nnext decoding iteration. In each iteration, we also bias the reinforcement\nlearning's attention to explore sequence portions which are previously\ndifficult to be decoded. For evaluation, the proposed strategy was trained to\ndecode ten thousands natural sentences. Our experiments indicate that, when\ncompared to a left-to-right greedy beam search LSTM decoder, the proposed\nmethod performed competitively well when decoding sentences from the training\nset, but significantly outperformed the baseline when decoding unseen\nsentences, in terms of BLEU score obtained.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 19:02:53 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Guo", "Hongyu", ""]]}]