[{"id": "1407.0013", "submitter": "Martin Sundin", "authors": "Martin Sundin, Saikat Chatterjee, Magnus Jansson and Cristian R. Rojas", "title": "Relevance Singular Vector Machine for low-rank matrix sensing", "comments": "International Conference on Signal Processing and Communications\n  (SPCOM), 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a new Bayesian inference method for low rank matrix\nreconstruction. We call the new method the Relevance Singular Vector Machine\n(RSVM) where appropriate priors are defined on the singular vectors of the\nunderlying matrix to promote low rank. To accelerate computations, a\nnumerically efficient approximation is developed. The proposed algorithms are\napplied to matrix completion and matrix reconstruction problems and their\nperformance is studied numerically.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 12:19:17 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Sundin", "Martin", ""], ["Chatterjee", "Saikat", ""], ["Jansson", "Magnus", ""], ["Rojas", "Cristian R.", ""]]}, {"id": "1407.0067", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri and Sanjoy Dasgupta", "title": "Rates of Convergence for Nearest Neighbor Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor methods are a popular class of nonparametric estimators with\nseveral desirable properties, such as adaptivity to different distance scales\nin different regions of space. Prior work on convergence rates for nearest\nneighbor classification has not fully reflected these subtle properties. We\nanalyze the behavior of these estimators in metric spaces and provide\nfinite-sample, distribution-dependent rates of convergence under minimal\nassumptions. As a by-product, we are able to establish the universal\nconsistency of nearest neighbor in a broader range of data spaces than was\npreviously known. We illustrate our upper and lower bounds by introducing\nsmoothness classes that are customized for nearest neighbor classification.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 22:00:57 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 00:44:29 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Dasgupta", "Sanjoy", ""]]}, {"id": "1407.0107", "submitter": "Huahua Wang", "authors": "Huahua Wang and Arindam Banerjee", "title": "Randomized Block Coordinate Descent for Online and Stochastic\n  Optimization", "comments": "The errors in the proof of ORBCD with variance reduction have been\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two types of low cost-per-iteration gradient descent methods have been\nextensively studied in parallel. One is online or stochastic gradient descent\n(OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this\npaper, we combine the two types of methods together and propose online\nrandomized block coordinate descent (ORBCD). At each iteration, ORBCD only\ncomputes the partial gradient of one block coordinate of one mini-batch\nsamples. ORBCD is well suited for the composite minimization problem where one\nfunction is the average of the losses of a large number of samples and the\nother is a simple regularizer defined on high dimensional variables. We show\nthat the iteration complexity of ORBCD has the same order as OGD or SGD. For\nstrongly convex functions, by reducing the variance of stochastic gradients, we\nshow that ORBCD can converge at a geometric rate in expectation, matching the\nconvergence rate of SGD with variance reduction and RBCD.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 05:57:43 GMT"}, {"version": "v2", "created": "Sat, 12 Jul 2014 21:03:06 GMT"}, {"version": "v3", "created": "Sat, 26 Jul 2014 19:16:39 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Wang", "Huahua", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1407.0179", "submitter": "Novi Quadrianto", "authors": "Daniel Hern\\'andez-Lobato, Viktoriia Sharmanska, Kristian Kersting,\n  Christoph H. Lampert, Novi Quadrianto", "title": "Mind the Nuisance: Gaussian Process Classification using Privileged\n  Noise", "comments": "14 pages with figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning with privileged information setting has recently attracted a lot\nof attention within the machine learning community, as it allows the\nintegration of additional knowledge into the training process of a classifier,\neven when this comes in the form of a data modality that is not available at\ntest time. Here, we show that privileged information can naturally be treated\nas noise in the latent function of a Gaussian Process classifier (GPC). That\nis, in contrast to the standard GPC setting, the latent function is not just a\nnuisance but a feature: it becomes a natural measure of confidence about the\ntraining data by modulating the slope of the GPC sigmoid likelihood function.\nExtensive experiments on public datasets show that the proposed GPC method\nusing privileged noise, called GPC+, improves over a standard GPC without\nprivileged knowledge, and also over the current state-of-the-art SVM-based\nmethod, SVM+. Moreover, we show that advanced neural networks and deep learning\nmethods can be compressed as privileged information.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 10:44:49 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Sharmanska", "Viktoriia", ""], ["Kersting", "Kristian", ""], ["Lampert", "Christoph H.", ""], ["Quadrianto", "Novi", ""]]}, {"id": "1407.0202", "submitter": "Francis Bach", "authors": "Aaron Defazio, Francis Bach (INRIA Paris - Rocquencourt, LIENS, MSR -\n  INRIA), Simon Lacoste-Julien (INRIA Paris - Rocquencourt, LIENS, MSR - INRIA)", "title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly\n  Convex Composite Objectives", "comments": "Advances In Neural Information Processing Systems, Nov 2014,\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a new optimisation method called SAGA in the spirit\nof SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient\nalgorithms with fast linear convergence rates. SAGA improves on the theory\nbehind SAG and SVRG, with better theoretical convergence rates, and has support\nfor composite objectives where a proximal operator is used on the regulariser.\nUnlike SDCA, SAGA supports non-strongly convex problems directly, and is\nadaptive to any inherent strong convexity of the problem. We give experimental\nresults showing the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 11:47:56 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 06:57:50 GMT"}, {"version": "v3", "created": "Tue, 16 Dec 2014 08:44:27 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Defazio", "Aaron", "", "INRIA Paris - Rocquencourt, LIENS, MSR -\n  INRIA"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS, MSR -\n  INRIA"], ["Lacoste-Julien", "Simon", "", "INRIA Paris - Rocquencourt, LIENS, MSR - INRIA"]]}, {"id": "1407.0208", "submitter": "Roi Weiss", "authors": "Aryeh Kontorovich and Roi Weiss", "title": "A Bayes consistent 1-NN classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a simple modification of the 1-nearest neighbor classifier\nyields a strongly Bayes consistent learner. Prior to this work, the only\nstrongly Bayes consistent proximity-based method was the k-nearest neighbor\nclassifier, for k growing appropriately with sample size. We will argue that a\nmargin-regularized 1-NN enjoys considerable statistical and algorithmic\nadvantages over the k-NN classifier. These include user-friendly finite-sample\nerror bounds, as well as time- and memory-efficient learning and test-point\nevaluation algorithms with a principled speed-accuracy tradeoff. Encouraging\nempirical results are reported.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 12:08:10 GMT"}, {"version": "v2", "created": "Sun, 26 Oct 2014 10:35:28 GMT"}, {"version": "v3", "created": "Sat, 1 Apr 2017 17:10:50 GMT"}, {"version": "v4", "created": "Fri, 17 Aug 2018 09:09:37 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Kontorovich", "Aryeh", ""], ["Weiss", "Roi", ""]]}, {"id": "1407.0286", "submitter": "Hoai An Le Thi", "authors": "Hoai An Le Thi, Tao Pham Dinh, Hoai Minh Le, Xuan Thanh Vo", "title": "DC approximation approaches for sparse optimization", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse optimization refers to an optimization problem involving the zero-norm\nin objective or constraints. In this paper, nonconvex approximation approaches\nfor sparse optimization have been studied with a unifying point of view in DC\n(Difference of Convex functions) programming framework. Considering a common DC\napproximation of the zero-norm including all standard sparse inducing penalty\nfunctions, we studied the consistency between global minimums (resp. local\nminimums) of approximate and original problems. We showed that, in several\ncases, some global minimizers (resp. local minimizers) of the approximate\nproblem are also those of the original problem. Using exact penalty techniques\nin DC programming, we proved stronger results for some particular\napproximations, namely, the approximate problem, with suitable parameters, is\nequivalent to the original problem. The efficiency of several sparse inducing\npenalty functions have been fully analyzed. Four DCA (DC Algorithm) schemes\nwere developed that cover all standard algorithms in nonconvex sparse\napproximation approaches as special versions. They can be viewed as, an $\\ell\n_{1}$-perturbed algorithm / reweighted-$\\ell _{1}$ algorithm / reweighted-$\\ell\n_{1}$ algorithm. We offer a unifying nonconvex approximation approach, with\nsolid theoretical tools as well as efficient algorithms based on DC programming\nand DCA, to tackle the zero-norm and sparse optimization. As an application, we\nimplemented our methods for the feature selection in SVM (Support Vector\nMachine) problem and performed empirical comparative numerical experiments on\nthe proposed algorithms with various approximation functions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 15:45:05 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 08:28:33 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Thi", "Hoai An Le", ""], ["Dinh", "Tao Pham", ""], ["Le", "Hoai Minh", ""], ["Vo", "Xuan Thanh", ""]]}, {"id": "1407.0312", "submitter": "Xingguo Li", "authors": "Xingguo Li and Jarvis Haupt", "title": "Identifying Outliers in Large Matrices via Randomized Adaptive\n  Compressive Sampling", "comments": "16 pages, 7 figures, 2 tables, IEEE Transactions on Signal Processing\n  (submitted)", "journal-ref": null, "doi": "10.1109/TSP.2015.2401536", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the problem of locating outlier columns in a large,\notherwise low-rank, matrix. We propose a simple two-step adaptive sensing and\ninference approach and establish theoretical guarantees for its performance;\nour results show that accurate outlier identification is achievable using very\nfew linear summaries of the original data matrix -- as few as the squared rank\nof the low-rank component plus the number of outliers, times constant and\nlogarithmic factors. We demonstrate the performance of our approach\nexperimentally in two stylized applications, one motivated by robust\ncollaborative filtering tasks, and the other by saliency map estimation tasks\narising in computer vision and automated surveillance, and also investigate\nextensions to settings where the data are noisy, or possibly incomplete.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 16:37:22 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 20:51:44 GMT"}, {"version": "v3", "created": "Wed, 19 Nov 2014 02:08:49 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Li", "Xingguo", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1407.0316", "submitter": "Mahito Sugiyama", "authors": "Mahito Sugiyama, Felipe Llinares L\\'opez, Niklas Kasenburg, Karsten M.\n  Borgwardt", "title": "Significant Subgraph Mining with Multiple Testing Correction", "comments": "18 pages, 5 figure, accepted to the 2015 SIAM International\n  Conference on Data Mining (SDM15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding itemsets that are statistically significantly enriched\nin a class of transactions is complicated by the need to correct for multiple\nhypothesis testing. Pruning untestable hypotheses was recently proposed as a\nstrategy for this task of significant itemset mining. It was shown to lead to\ngreater statistical power, the discovery of more truly significant itemsets,\nthan the standard Bonferroni correction on real-world datasets. An open\nquestion, however, is whether this strategy of excluding untestable hypotheses\nalso leads to greater statistical power in subgraph mining, in which the number\nof hypotheses is much larger than in itemset mining. Here we answer this\nquestion by an empirical investigation on eight popular graph benchmark\ndatasets. We propose a new efficient search strategy, which always returns the\nsame solution as the state-of-the-art approach and is approximately two orders\nof magnitude faster. Moreover, we exploit the dependence between subgraphs by\nconsidering the effective number of tests and thereby further increase the\nstatistical power.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 16:53:51 GMT"}, {"version": "v2", "created": "Sun, 6 Jul 2014 13:39:21 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 16:11:17 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Sugiyama", "Mahito", ""], ["L\u00f3pez", "Felipe Llinares", ""], ["Kasenburg", "Niklas", ""], ["Borgwardt", "Karsten M.", ""]]}, {"id": "1407.0380", "submitter": "Imen Trabelsi", "authors": "Imen Trabelsi and Dorra Ben Ayed", "title": "A Multi Level Data Fusion Approach for Speaker Identification on\n  Telephone Speech", "comments": "10 pages, 4 figures, International Journal of Signal Processing,\n  Image Processing and Pattern Recognition Vol. 6, No. 2, April, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Several speaker identification systems are giving good performance with clean\nspeech but are affected by the degradations introduced by noisy audio\nconditions. To deal with this problem, we investigate the use of complementary\ninformation at different levels for computing a combined match score for the\nunknown speaker. In this work, we observe the effect of two supervised machine\nlearning approaches including support vectors machines (SVM) and na\\\"ive bayes\n(NB). We define two feature vector sets based on mel frequency cepstral\ncoefficients (MFCC) and relative spectral perceptual linear predictive\ncoefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture\nModel (GMM). Several ways of combining these information sources give\nsignificant improvements in a text-independent speaker identification task\nusing a very large telephone degraded NTIMIT database.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 20:34:05 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Trabelsi", "Imen", ""], ["Ayed", "Dorra Ben", ""]]}, {"id": "1407.0439", "submitter": "Haixia Liu", "authors": "Haixia Liu, Raymond H. Chan, and Yuan Yao", "title": "Geometric Tight Frame based Stylometry for Art Authentication of van\n  Gogh Paintings", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about authenticating genuine van Gogh paintings from forgeries.\nThe authentication process depends on two key steps: feature extraction and\noutlier detection. In this paper, a geometric tight frame and some simple\nstatistics of the tight frame coefficients are used to extract features from\nthe paintings. Then a forward stage-wise rank boosting is used to select a\nsmall set of features for more accurate classification so that van Gogh\npaintings are highly concentrated towards some center point while forgeries are\nspread out as outliers. Numerical results show that our method can achieve\n86.08% classification accuracy under the leave-one-out cross-validation\nprocedure. Our method also identifies five features that are much more\npredominant than other features. Using just these five features for\nclassification, our method can give 88.61% classification accuracy which is the\nhighest so far reported in literature. Evaluation of the five features is also\nperformed on two hundred datasets generated by bootstrap sampling with\nreplacement. The median and the mean are 88.61% and 87.77% respectively. Our\nresults show that a small set of statistics of the tight frame coefficients\nalong certain orientations can serve as discriminative features for van Gogh\npaintings. It is more important to look at the tail distributions of such\ndirectional coefficients than mean values and standard deviations. It reflects\na highly consistent style in van Gogh's brushstroke movements, where many\nforgeries demonstrate a more diverse spread in these features.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 01:55:37 GMT"}, {"version": "v2", "created": "Sat, 13 Sep 2014 00:53:16 GMT"}, {"version": "v3", "created": "Tue, 13 Jan 2015 07:20:12 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Liu", "Haixia", ""], ["Chan", "Raymond H.", ""], ["Yao", "Yuan", ""]]}, {"id": "1407.0449", "submitter": "Amir-massoud Farahmand", "authors": "Amir-massoud Farahmand, Doina Precup, Andr\\'e M.S. Barreto, Mohammad\n  Ghavamzadeh", "title": "Classification-based Approximate Policy Iteration: Experiments and\n  Extended Discussions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling large approximate dynamic programming or reinforcement learning\nproblems requires methods that can exploit regularities, or intrinsic\nstructure, of the problem in hand. Most current methods are geared towards\nexploiting the regularities of either the value function or the policy. We\nintroduce a general classification-based approximate policy iteration (CAPI)\nframework, which encompasses a large class of algorithms that can exploit\nregularities of both the value function and the policy space, depending on what\nis advantageous. This framework has two main components: a generic value\nfunction estimator and a classifier that learns a policy based on the estimated\nvalue function. We establish theoretical guarantees for the sample complexity\nof CAPI-style algorithms, which allow the policy evaluation step to be\nperformed by a wide variety of algorithms (including temporal-difference-style\nmethods), and can handle nonparametric representations of policies. Our bounds\non the estimation error of the performance loss are tighter than existing\nresults. We also illustrate this approach empirically on several problems,\nincluding a large HIV control task.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 03:19:43 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Farahmand", "Amir-massoud", ""], ["Precup", "Doina", ""], ["Barreto", "Andr\u00e9 M. S.", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1407.0611", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi (SAMM)", "title": "How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need?", "comments": null, "journal-ref": "10th International Workshop on Self Organizing Maps, WSSOM 2014,\n  Mittweida : Germany (2014)", "doi": "10.1007/978-3-319-07695-9_1", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In numerous applicative contexts, data are too rich and too complex to be\nrepresented by numerical vectors. A general approach to extend machine learning\nand data mining techniques to such data is to really on a dissimilarity or on a\nkernel that measures how different or similar two objects are. This approach\nhas been used to define several variants of the Self Organizing Map (SOM). This\npaper reviews those variants in using a common set of notations in order to\noutline differences and similarities between them. It discusses the advantages\nand drawbacks of the variants, as well as the actual relevance of the\ndissimilarity/kernel SOM for practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 15:31:20 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1407.0612", "submitter": "Fabrice Rossi", "authors": "Marc Boull\\'e, Romain Guigour\\`es (SAMM), Fabrice Rossi (SAMM)", "title": "Nonparametric Hierarchical Clustering of Functional Data", "comments": null, "journal-ref": "Advances in Knowledge Discovery and Management, Guillet, Fabrice\n  and Pinaud, Bruno and Venturini, Gilles and Zighed, Djamel Abdelkader (Ed.)\n  (2014) 15-35", "doi": "10.1007/978-3-319-02999-3_2", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with the problem of curves clustering. We propose a\nnonparametric method which partitions the curves into clusters and discretizes\nthe dimensions of the curve points into intervals. The cross-product of these\npartitions forms a data-grid which is obtained using a Bayesian model selection\napproach while making no assumptions regarding the curves. Finally, a\npost-processing technique, aiming at reducing the number of clusters in order\nto improve the interpretability of the clustering, is proposed. It consists in\noptimally merging the clusters step by step, which corresponds to an\nagglomerative hierarchical classification whose dissimilarity measure is the\nvariation of the criterion. Interestingly this measure is none other than the\nsum of the Kullback-Leibler divergences between clusters distributions before\nand after the merges. The practical interest of the approach for functional\ndata exploratory analysis is presented and compared with an alternative\napproach on an artificial and a real world data set.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 15:32:10 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Boull\u00e9", "Marc", "", "SAMM"], ["Guigour\u00e8s", "Romain", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1407.0726", "submitter": "Yao Xie", "authors": "Yang Cao and Yao Xie", "title": "Fast Algorithm for Low-rank matrix recovery in Poisson noise", "comments": "Presented at IEEE GLOBALSIP2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a fast algorithm for recovering low-rank matrices from\ntheir linear measurements contaminated with Poisson noise: the Poisson noise\nMaximum Likelihood Singular Value thresholding (PMLSV) algorithm. We propose a\nconvex optimization formulation with a cost function consisting of the sum of a\nlikelihood function and a regularization function which the nuclear norm of the\nmatrix. Instead of solving the optimization problem directly by semi-definite\nprogram (SDP), we derive an iterative singular value thresholding algorithm by\nexpanding the likelihood function. We demonstrate the good performance of the\nproposed algorithm on recovery of solar flare images with Poisson noise: the\nalgorithm is more efficient than solving SDP using the interior-point algorithm\nand it generates a good approximate solution compared to that solved from SDP.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 21:27:23 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 20:11:13 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1407.0749", "submitter": "Justin Domke", "authors": "Justin Domke and Xianghang Liu", "title": "Projecting Ising Model Parameters for Fast Mixing", "comments": "Advances in Neural Information Processing Systems 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in general Ising models is difficult, due to high treewidth making\ntree-based algorithms intractable. Moreover, when interactions are strong,\nGibbs sampling may take exponential time to converge to the stationary\ndistribution. We present an algorithm to project Ising model parameters onto a\nparameter set that is guaranteed to be fast mixing, under several divergences.\nWe find that Gibbs sampling using the projected parameters is more accurate\nthan with the original parameters when interaction strengths are strong and\nwhen limited time is available for sampling.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 00:19:08 GMT"}, {"version": "v2", "created": "Wed, 8 Oct 2014 06:30:20 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Domke", "Justin", ""], ["Liu", "Xianghang", ""]]}, {"id": "1407.0753", "submitter": "Ting Kei Pong", "authors": "Guoyin Li, Ting Kei Pong", "title": "Global convergence of splitting methods for nonconvex composite\n  optimization", "comments": "To appear in SIOPT", "journal-ref": null, "doi": "10.1137/140998135", "report-no": null, "categories": "math.OC cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the sum of a smooth function $h$ with a\nbounded Hessian, and a nonsmooth function. We assume that the latter function\nis a composition of a proper closed function $P$ and a surjective linear map\n$\\cal M$, with the proximal mappings of $\\tau P$, $\\tau > 0$, simple to\ncompute. This problem is nonconvex in general and encompasses many important\napplications in engineering and machine learning. In this paper, we examined\ntwo types of splitting methods for solving this nonconvex optimization problem:\nalternating direction method of multipliers and proximal gradient algorithm.\nFor the direct adaptation of the alternating direction method of multipliers,\nwe show that, if the penalty parameter is chosen sufficiently large and the\nsequence generated has a cluster point, then it gives a stationary point of the\nnonconvex problem. We also establish convergence of the whole sequence under an\nadditional assumption that the functions $h$ and $P$ are semi-algebraic.\nFurthermore, we give simple sufficient conditions to guarantee boundedness of\nthe sequence generated. These conditions can be satisfied for a wide range of\napplications including the least squares problem with the $\\ell_{1/2}$\nregularization. Finally, when $\\cal M$ is the identity so that the proximal\ngradient algorithm can be efficiently applied, we show that any cluster point\nis stationary under a slightly more flexible constant step-size rule than what\nis known in the literature for a nonconvex $h$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 00:29:25 GMT"}, {"version": "v2", "created": "Sat, 2 Aug 2014 13:09:55 GMT"}, {"version": "v3", "created": "Wed, 3 Dec 2014 00:59:38 GMT"}, {"version": "v4", "created": "Mon, 4 May 2015 11:51:54 GMT"}, {"version": "v5", "created": "Wed, 24 Jun 2015 11:32:31 GMT"}, {"version": "v6", "created": "Wed, 4 Nov 2015 02:27:46 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Li", "Guoyin", ""], ["Pong", "Ting Kei", ""]]}, {"id": "1407.0754", "submitter": "Justin Domke", "authors": "Justin Domke", "title": "Structured Learning via Logistic Regression", "comments": "Advances in Neural Information Processing Systems 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A successful approach to structured learning is to write the learning\nobjective as a joint function of linear parameters and inference messages, and\niterate between updates to each. This paper observes that if the inference\nproblem is \"smoothed\" through the addition of entropy terms, for fixed\nmessages, the learning objective reduces to a traditional (non-structured)\nlogistic regression problem with respect to parameters. In these logistic\nregression problems, each training example has a bias term determined by the\ncurrent set of messages. Based on this insight, the structured energy function\ncan be extended from linear factors to any function class where an \"oracle\"\nexists to minimize a logistic loss.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 00:48:34 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Domke", "Justin", ""]]}, {"id": "1407.0822", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM), B\\'en\\'edicte Le Grand (CRI), Boris\n  Golden (Viadeo), Fabrice Rossi (SAMM)", "title": "Reducing Offline Evaluation Bias in Recommendation Systems", "comments": "23rd annual Belgian-Dutch Conference on Machine Learning (Benelearn\n  2014), Bruxelles : Belgium (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have been integrated into the majority of large online\nsystems. They tailor those systems to individual users by filtering and ranking\ninformation according to user profiles. This adaptation process influences the\nway users interact with the system and, as a consequence, increases the\ndifficulty of evaluating a recommendation algorithm with historical data (via\noffline evaluation). This paper analyses this evaluation bias and proposes a\nsimple item weighting solution that reduces its impact. The efficiency of the\nproposed solution is evaluated on real world data extracted from Viadeo\nprofessional social network.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 09:05:33 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Golden", "Boris", "", "Viadeo"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1407.0880", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Anomaly Detection Based on Aggregation of Indicators", "comments": "23rd annual Belgian-Dutch Conference on Machine Learning (Benelearn\n  2014), Bruxelles : Belgium (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic anomaly detection is a major issue in various areas. Beyond mere\ndetection, the identification of the origin of the problem that produced the\nanomaly is also essential. This paper introduces a general methodology that can\nassist human operators who aim at classifying monitoring signals. The main idea\nis to leverage expert knowledge by generating a very large number of\nindicators. A feature selection method is used to keep only the most\ndiscriminant indicators which are used as inputs of a Naive Bayes classifier.\nThe parameters of the classifier have been optimized indirectly by the\nselection process. Simulated data designed to reproduce some of the anomaly\ntypes observed in real world engines.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 12:16:50 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 19:43:54 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1407.1082", "submitter": "Daniel Golovin", "authors": "Daniel Golovin, Andreas Krause, Matthew Streeter", "title": "Online Submodular Maximization under a Matroid Constraint with\n  Application to Learning Assignments", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which ads should we display in sponsored search in order to maximize our\nrevenue? How should we dynamically rank information sources to maximize the\nvalue of the ranking? These applications exhibit strong diminishing returns:\nRedundancy decreases the marginal utility of each ad or information source. We\nshow that these and other problems can be formalized as repeatedly selecting an\nassignment of items to positions to maximize a sequence of monotone submodular\nfunctions that arrive one by one. We present an efficient algorithm for this\ngeneral problem and analyze it in the no-regret model. Our algorithm possesses\nstrong theoretical guarantees, such as a performance ratio that converges to\nthe optimal constant of 1 - 1/e. We empirically evaluate our algorithm on two\nreal-world online optimization problems on the web: ad allocation with\nsubmodular utilities, and dynamically ranking blogs to detect information\ncascades. Finally, we present a second algorithm that handles the more general\ncase in which the feasible sets are given by a matroid constraint, while still\nmaintaining a 1 - 1/e asymptotic performance ratio.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 23:06:10 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Golovin", "Daniel", ""], ["Krause", "Andreas", ""], ["Streeter", "Matthew", ""]]}, {"id": "1407.1097", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula, Cynthia Rudin", "title": "Robust Optimization using Machine Learning for Uncertainty Sets", "comments": "28 pages, 2 figures; a shorter preliminary version appeared in ISAIM\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to build robust optimization problems for making decisions based\non complex data from the past. In robust optimization (RO) generally, the goal\nis to create a policy for decision-making that is robust to our uncertainty\nabout the future. In particular, we want our policy to best handle the the\nworst possible situation that could arise, out of an uncertainty set of\npossible situations. Classically, the uncertainty set is simply chosen by the\nuser, or it might be estimated in overly simplistic ways with strong\nassumptions; whereas in this work, we learn the uncertainty set from data\ncollected in the past. The past data are drawn randomly from an (unknown)\npossibly complicated high-dimensional distribution. We propose a new\nuncertainty set design and show how tools from statistical learning theory can\nbe employed to provide probabilistic guarantees on the robustness of the\npolicy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 00:39:00 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1407.1123", "submitter": "Mehrtash Harandi", "authors": "Mehrtash T. Harandi and Mathieu Salzmann and Sadeep Jayasumana and\n  Richard Hartley and Hongdong Li", "title": "Expanding the Family of Grassmannian Kernels: An Embedding Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling videos and image-sets as linear subspaces has proven beneficial for\nmany visual recognition tasks. However, it also incurs challenges arising from\nthe fact that linear subspaces do not obey Euclidean geometry, but lie on a\nspecial type of Riemannian manifolds known as Grassmannian. To leverage the\ntechniques developed for Euclidean spaces (e.g, support vector machines) with\nsubspaces, several recent studies have proposed to embed the Grassmannian into\na Hilbert space by making use of a positive definite kernel. Unfortunately,\nonly two Grassmannian kernels are known, none of which -as we will show- is\nuniversal, which limits their ability to approximate a target function\narbitrarily well. Here, we introduce several positive definite Grassmannian\nkernels, including universal ones, and demonstrate their superiority over\npreviously-known kernels in various tasks, such as classification, clustering,\nsparse coding and hashing.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 05:34:38 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Harandi", "Mehrtash T.", ""], ["Salzmann", "Mathieu", ""], ["Jayasumana", "Sadeep", ""], ["Hartley", "Richard", ""], ["Li", "Hongdong", ""]]}, {"id": "1407.1151", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Jianxin Wu", "title": "Optimizing Ranking Measures for Compact Binary Code Learning", "comments": "Appearing in Proc. European Conference on Computer Vision 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has proven a valuable tool for large-scale information retrieval.\nDespite much success, existing hashing methods optimize over simple objectives\nsuch as the reconstruction error or graph Laplacian related loss functions,\ninstead of the performance evaluation criteria of interest---multivariate\nperformance measures such as the AUC and NDCG. Here we present a general\nframework (termed StructHash) that allows one to directly optimize multivariate\nperformance measures. The resulting optimization problem can involve\nexponentially or infinitely many variables and constraints, which is more\nchallenging than standard structured output learning. To solve the StructHash\noptimization problem, we use a combination of column generation and\ncutting-plane techniques. We demonstrate the generality of StructHash by\napplying it to ranking prediction and image retrieval, and show that it\noutperforms a few state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 08:18:45 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Wu", "Jianxin", ""]]}, {"id": "1407.1176", "submitter": "Felipe Llinares", "authors": "Felipe Llinares, Mahito Sugiyama, Karsten M. Borgwardt", "title": "Identifying Higher-order Combinations of Binary Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding statistically significant interactions between binary variables is\ncomputationally and statistically challenging in high-dimensional settings, due\nto the combinatorial explosion in the number of hypotheses. Terada et al.\nrecently showed how to elegantly address this multiple testing problem by\nexcluding non-testable hypotheses. Still, it remains unclear how their approach\nscales to large datasets.\n  We here proposed strategies to speed up the approach by Terada et al. and\nevaluate them thoroughly in 11 real-world benchmark datasets. We observe that\none approach, incremental search with early stopping, is orders of magnitude\nfaster than the current state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 10:17:43 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Llinares", "Felipe", ""], ["Sugiyama", "Mahito", ""], ["Borgwardt", "Karsten M.", ""]]}, {"id": "1407.1201", "submitter": "Piotr Plonski", "authors": "Piotr P{\\l}o\\'nski, Krzysztof Zaremba", "title": "Improving Performance of Self-Organising Maps with Distance Metric\n  Learning Method", "comments": "9 pages, 2 figures", "journal-ref": "Artificial Intelligence and Soft Computing, Lecture Notes in\n  Computer Science Volume 7267, 2012, pp 169-177", "doi": "10.1007/978-3-642-29347-4_20", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-Organising Maps (SOM) are Artificial Neural Networks used in Pattern\nRecognition tasks. Their major advantage over other architectures is human\nreadability of a model. However, they often gain poorer accuracy. Mostly used\nmetric in SOM is the Euclidean distance, which is not the best approach to some\nproblems. In this paper, we study an impact of the metric change on the SOM's\nperformance in classification problems. In order to change the metric of the\nSOM we applied a distance metric learning method, so-called 'Large Margin\nNearest Neighbour'. It computes the Mahalanobis matrix, which assures small\ndistance between nearest neighbour points from the same class and separation of\npoints belonging to different classes by large margin. Results are presented on\nseveral real data sets, containing for example recognition of written digits,\nspoken letters or faces.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 12:14:48 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["P\u0142o\u0144ski", "Piotr", ""], ["Zaremba", "Krzysztof", ""]]}, {"id": "1407.1208", "submitter": "Piotr Bojanowski", "authors": "Piotr Bojanowski, R\\'emi Lajugie, Francis Bach, Ivan Laptev, Jean\n  Ponce, Cordelia Schmid, Josef Sivic", "title": "Weakly Supervised Action Labeling in Videos Under Ordering Constraints", "comments": "17 pages, completed version of a ECCV2014 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are given a set of video clips, each one annotated with an {\\em ordered}\nlist of actions, such as \"walk\" then \"sit\" then \"answer phone\" extracted from,\nfor example, the associated text script. We seek to temporally localize the\nindividual actions in each clip as well as to learn a discriminative classifier\nfor each action. We formulate the problem as a weakly supervised temporal\nassignment with ordering constraints. Each video clip is divided into small\ntime intervals and each time interval of each video clip is assigned one action\nlabel, while respecting the order in which the action labels appear in the\ngiven annotations. We show that the action label assignment can be determined\ntogether with learning a classifier for each action in a discriminative manner.\nWe evaluate the proposed model on a new and challenging dataset of 937 video\nclips with a total of 787720 frames containing sequences of 16 different\nactions from 69 Hollywood movies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 12:53:15 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Lajugie", "R\u00e9mi", ""], ["Bach", "Francis", ""], ["Laptev", "Ivan", ""], ["Ponce", "Jean", ""], ["Schmid", "Cordelia", ""], ["Sivic", "Josef", ""]]}, {"id": "1407.1291", "submitter": "Stoyan Dimitrov", "authors": "Stoyan Dimitrov, Redouane Lguensat", "title": "Reinforcement Learning Based Algorithm for the Maximization of EV\n  Charging Station Revenue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an online reinforcement learning based application which\nincreases the revenue of one particular electric vehicles (EV) station,\nconnected to a renewable source of energy. Moreover, the proposed application\nadapts to changes in the trends of the station's average number of customers\nand their types. Most of the parameters in the model are simulated\nstochastically and the algorithm used is a Q-learning algorithm. A computer\nsimulation was implemented which demonstrates and confirms the utility of the\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 18:37:33 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 12:59:13 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Dimitrov", "Stoyan", ""], ["Lguensat", "Redouane", ""]]}, {"id": "1407.1399", "submitter": "Fanhua Shang", "authors": "Fanhua Shang and Yuanyuan Liu and James Cheng", "title": "Generalized Higher-Order Tensor Decomposition via Parallel ADMM", "comments": "9 pages, 5 figures, AAAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order tensors are becoming prevalent in many scientific areas such as\ncomputer vision, social network analysis, data mining and neuroscience.\nTraditional tensor decomposition approaches face three major challenges: model\nselecting, gross corruptions and computational efficiency. To address these\nproblems, we first propose a parallel trace norm regularized tensor\ndecomposition method, and formulate it as a convex optimization problem. This\nmethod does not require the rank of each mode to be specified beforehand, and\ncan automatically determine the number of factors in each mode through our\noptimization scheme. By considering the low-rank structure of the observed\ntensor, we analyze the equivalent relationship of the trace norm between a\nlow-rank tensor and its core tensor. Then, we cast a non-convex tensor\ndecomposition model into a weighted combination of multiple much smaller-scale\nmatrix trace norm minimization. Finally, we develop two parallel alternating\ndirection methods of multipliers (ADMM) to solve our problems. Experimental\nresults verify that our regularized formulation is effective, and our methods\nare robust to noise or outliers.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jul 2014 11:58:30 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Cheng", "James", ""]]}, {"id": "1407.1537", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Lorenzo Orecchia", "title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent", "comments": "A new section added; polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order methods play a central role in large-scale machine learning. Even\nthough many variations exist, each suited to a particular problem, almost all\nsuch methods fundamentally rely on two types of algorithmic steps: gradient\ndescent, which yields primal progress, and mirror descent, which yields dual\nprogress.\n  We observe that the performances of gradient and mirror descent are\ncomplementary, so that faster algorithms can be designed by LINEARLY COUPLING\nthe two. We show how to reconstruct Nesterov's accelerated gradient methods\nusing linear coupling, which gives a cleaner interpretation than Nesterov's\noriginal proofs. We also discuss the power of linear coupling by extending it\nto many other settings that Nesterov's methods cannot apply to.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:11:48 GMT"}, {"version": "v2", "created": "Sat, 9 Aug 2014 01:48:01 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 06:59:10 GMT"}, {"version": "v4", "created": "Fri, 2 Jan 2015 17:41:24 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 19:30:37 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1407.1538", "submitter": "Xiangnan Kong", "authors": "Xiangnan Kong and Zhaoming Wu and Li-Jia Li and Ruofei Zhang and\n  Philip S. Yu and Hang Wu and Wei Fan", "title": "Large-Scale Multi-Label Learning with Incomplete Label Assignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label learning deals with the classification problems where each\ninstance can be assigned with multiple labels simultaneously. Conventional\nmulti-label learning approaches mainly focus on exploiting label correlations.\nIt is usually assumed, explicitly or implicitly, that the label sets for\ntraining instances are fully labeled without any missing labels. However, in\nmany real-world multi-label datasets, the label assignments for training\ninstances can be incomplete. Some ground-truth labels can be missed by the\nlabeler from the label set. This problem is especially typical when the number\ninstances is very large, and the labeling cost is very high, which makes it\nalmost impossible to get a fully labeled training set. In this paper, we study\nthe problem of large-scale multi-label learning with incomplete label\nassignments. We propose an approach, called MPU, based upon positive and\nunlabeled stochastic gradient descent and stacked models. Unlike prior works,\nour method can effectively and efficiently consider missing labels and label\ncorrelations simultaneously, and is very scalable, that has linear time\ncomplexities over the size of the data. Extensive experiments on two real-world\nmulti-label datasets show that our MPU model consistently outperform other\ncommonly-used baselines.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:13:48 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Kong", "Xiangnan", ""], ["Wu", "Zhaoming", ""], ["Li", "Li-Jia", ""], ["Zhang", "Ruofei", ""], ["Yu", "Philip S.", ""], ["Wu", "Hang", ""], ["Fan", "Wei", ""]]}, {"id": "1407.1543", "submitter": "David Steurer", "authors": "Boaz Barak, Jonathan A. Kelner, David Steurer", "title": "Dictionary Learning and Tensor Decomposition via the Sum-of-Squares\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new approach to the dictionary learning (also known as \"sparse\ncoding\") problem of recovering an unknown $n\\times m$ matrix $A$ (for $m \\geq\nn$) from examples of the form \\[ y = Ax + e, \\] where $x$ is a random vector in\n$\\mathbb R^m$ with at most $\\tau m$ nonzero coordinates, and $e$ is a random\nnoise vector in $\\mathbb R^n$ with bounded magnitude. For the case $m=O(n)$,\nour algorithm recovers every column of $A$ within arbitrarily good constant\naccuracy in time $m^{O(\\log m/\\log(\\tau^{-1}))}$, in particular achieving\npolynomial time if $\\tau = m^{-\\delta}$ for any $\\delta>0$, and time $m^{O(\\log\nm)}$ if $\\tau$ is (a sufficiently small) constant. Prior algorithms with\ncomparable assumptions on the distribution required the vector $x$ to be much\nsparser---at most $\\sqrt{n}$ nonzero coordinates---and there were intrinsic\nbarriers preventing these algorithms from applying for denser $x$.\n  We achieve this by designing an algorithm for noisy tensor decomposition that\ncan recover, under quite general conditions, an approximate rank-one\ndecomposition of a tensor $T$, given access to a tensor $T'$ that is\n$\\tau$-close to $T$ in the spectral norm (when considered as a matrix). To our\nknowledge, this is the first algorithm for tensor decomposition that works in\nthe constant spectral-norm noise regime, where there is no guarantee that the\nlocal optima of $T$ and $T'$ have similar structures.\n  Our algorithm is based on a novel approach to using and analyzing the Sum of\nSquares semidefinite programming hierarchy (Parrilo 2000, Lasserre 2001), and\nit can be viewed as an indication of the utility of this very general and\npowerful tool for unsupervised learning problems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:42:05 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 21:32:44 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Barak", "Boaz", ""], ["Kelner", "Jonathan A.", ""], ["Steurer", "David", ""]]}, {"id": "1407.1640", "submitter": "Bin Gao", "authors": "Bin Gao, Jiang Bian, and Tie-Yan Liu", "title": "WordRep: A Benchmark for Research on Learning Word Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  WordRep is a benchmark collection for the research on learning distributed\nword representations (or word embeddings), released by Microsoft Research. In\nthis paper, we describe the details of the WordRep collection and show how to\nuse it in different types of machine learning research related to word\nembedding. Specifically, we describe how the evaluation tasks in WordRep are\nselected, how the data are sampled, and how the evaluation tool is built. We\nthen compare several state-of-the-art word representations on WordRep, report\ntheir evaluation performance, and make discussions on the results. After that,\nwe discuss new potential research topics that can be supported by WordRep, in\naddition to algorithm comparison. We hope that this paper can help people gain\ndeeper understanding of WordRep, and enable more interesting research on\nlearning distributed word representations and related topics.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 09:31:21 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Gao", "Bin", ""], ["Bian", "Jiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1407.1687", "submitter": "Bin Gao", "authors": "Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, and Tie-Yan Liu", "title": "KNET: A General Framework for Learning Word Embedding using\n  Morphological Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Neural network techniques are widely applied to obtain high-quality\ndistributed representations of words, i.e., word embeddings, to address text\nmining, information retrieval, and natural language processing tasks. Recently,\nefficient methods have been proposed to learn word embeddings from context that\ncaptures both semantic and syntactic relationships between words. However, it\nis challenging to handle unseen words or rare words with insufficient context.\nIn this paper, inspired by the study on word recognition process in cognitive\npsychology, we propose to take advantage of seemingly less obvious but\nessentially important morphological knowledge to address these challenges. In\nparticular, we introduce a novel neural network architecture called KNET that\nleverages both contextual information and morphological word similarity built\nbased on morphological knowledge to learn word embeddings. Meanwhile, the\nlearning architecture is also able to refine the pre-defined morphological\nknowledge and obtain more accurate word similarity. Experiments on an\nanalogical reasoning task and a word similarity task both demonstrate that the\nproposed KNET framework can greatly enhance the effectiveness of word\nembeddings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 12:45:10 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 16:03:47 GMT"}, {"version": "v3", "created": "Fri, 5 Sep 2014 15:58:35 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Cui", "Qing", ""], ["Gao", "Bin", ""], ["Bian", "Jiang", ""], ["Qiu", "Siyu", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1407.1890", "submitter": "Michael Smith", "authors": "Michael R. Smith, Logan Mitchell, Christophe Giraud-Carrier, Tony\n  Martinez", "title": "Recommending Learning Algorithms and Their Associated Hyperparameters", "comments": "Short paper--2 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of machine learning on a given task dependson, among other\nthings, which learning algorithm is selected and its associated\nhyperparameters. Selecting an appropriate learning algorithm and setting its\nhyperparameters for a given data set can be a challenging task, especially for\nusers who are not experts in machine learning. Previous work has examined using\nmeta-features to predict which learning algorithm and hyperparameters should be\nused. However, choosing a set of meta-features that are predictive of algorithm\nperformance is difficult. Here, we propose to apply collaborative filtering\ntechniques to learning algorithm and hyperparameter selection, and find that\ndoing so avoids determining which meta-features to use and outperforms\ntraditional meta-learning approaches in many cases.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 21:23:42 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Smith", "Michael R.", ""], ["Mitchell", "Logan", ""], ["Giraud-Carrier", "Christophe", ""], ["Martinez", "Tony", ""]]}, {"id": "1407.2433", "submitter": "Peter Foster", "authors": "Peter Foster, Simon Dixon, Anssi Klapuri", "title": "Identifying Cover Songs Using Information-Theoretic Measures of\n  Similarity", "comments": "13 pages, 5 figures, 4 tables. v3: Accepted version", "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol. 23 no. 6, pp. 993-1005, 2015", "doi": "10.1109/TASLP.2015.2416655", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates methods for quantifying similarity between audio\nsignals, specifically for the task of of cover song detection. We consider an\ninformation-theoretic approach, where we compute pairwise measures of\npredictability between time series. We compare discrete-valued approaches\noperating on quantised audio features, to continuous-valued approaches. In the\ndiscrete case, we propose a method for computing the normalised compression\ndistance, where we account for correlation between time series. In the\ncontinuous case, we propose to compute information-based measures of similarity\nas statistics of the prediction error between time series. We evaluate our\nmethods on two cover song identification tasks using a data set comprised of\n300 Jazz standards and using the Million Song Dataset. For both datasets, we\nobserve that continuous-valued approaches outperform discrete-valued\napproaches. We consider approaches to estimating the normalised compression\ndistance (NCD) based on string compression and prediction, where we observe\nthat our proposed normalised compression distance with alignment (NCDA)\nimproves average performance over NCD, for sequential compression algorithms.\nFinally, we demonstrate that continuous-valued distances may be combined to\nimprove performance with respect to baseline approaches. Using a large-scale\nfilter-and-refine approach, we demonstrate state-of-the-art performance for\ncover song identification using the Million Song Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 11:04:15 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 00:53:47 GMT"}, {"version": "v3", "created": "Sun, 17 May 2015 15:53:43 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Foster", "Peter", ""], ["Dixon", "Simon", ""], ["Klapuri", "Anssi", ""]]}, {"id": "1407.2483", "submitter": "Shyam Visweswaran", "authors": "Shyam Visweswaran and Gregory F. Cooper", "title": "Counting Markov Blanket Structures", "comments": "5 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Learning Markov blanket (MB) structures has proven useful in performing\nfeature selection, learning Bayesian networks (BNs), and discovering causal\nrelationships. We present a formula for efficiently determining the number of\nMB structures given a target variable and a set of other variables. As\nexpected, the number of MB structures grows exponentially. However, we show\nquantitatively that there are many fewer MB structures that contain the target\nvariable than there are BN structures that contain it. In particular, the ratio\nof BN structures to MB structures appears to increase exponentially in the\nnumber of variables.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 14:02:01 GMT"}, {"version": "v2", "created": "Sat, 12 Jul 2014 17:23:57 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Visweswaran", "Shyam", ""], ["Cooper", "Gregory F.", ""]]}, {"id": "1407.2515", "submitter": "Lionel Tabourier", "authors": "Lionel Tabourier, Daniel Faria Bernardes, Anne-Sophie Libert, Renaud\n  Lambiotte", "title": "RankMerging: A supervised learning-to-rank framework to predict links in\n  large social network", "comments": "43 pages, published in Machine Learning Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering unknown or missing links in social networks is a difficult task\nbecause of their sparsity and because links may represent different types of\nrelationships, characterized by different structural patterns. In this paper,\nwe define a simple yet efficient supervised learning-to-rank framework, called\nRankMerging, which aims at combining information provided by various\nunsupervised rankings. We illustrate our method on three different kinds of\nsocial networks and show that it substantially improves the performances of\nunsupervised metrics of ranking. We also compare it to other combination\nstrategies based on standard methods. Finally, we explore various aspects of\nRankMerging, such as feature selection and parameter estimation and discuss its\narea of relevance: the prediction of an adjustable number of links on large\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 15:05:56 GMT"}, {"version": "v2", "created": "Sun, 28 Sep 2014 14:16:57 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2015 22:32:04 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 14:23:37 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Tabourier", "Lionel", ""], ["Bernardes", "Daniel Faria", ""], ["Libert", "Anne-Sophie", ""], ["Lambiotte", "Renaud", ""]]}, {"id": "1407.2538", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen and Alexander G. Schwing and Alan L. Yuille and\n  Raquel Urtasun", "title": "Learning Deep Structured Models", "comments": "11 pages including reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in real-world applications involve predicting several random\nvariables which are statistically related. Markov random fields (MRFs) are a\ngreat mathematical tool to encode such relationships. The goal of this paper is\nto combine MRFs with deep learning algorithms to estimate complex\nrepresentations while taking into account the dependencies between the output\nrandom variables. Towards this goal, we propose a training algorithm that is\nable to learn structured models jointly with deep features that form the MRF\npotentials. Our approach is efficient as it blends learning and inference and\nmakes use of GPU acceleration. We demonstrate the effectiveness of our\nalgorithm in the tasks of predicting words from noisy images, as well as\nmulti-class classification of Flickr photographs. We show that joint learning\nof the deep features and the MRF parameters results in significant performance\ngains.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 15:54:27 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 21:50:10 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2015 21:11:32 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Schwing", "Alexander G.", ""], ["Yuille", "Alan L.", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1407.2646", "submitter": "Yura Perov N", "authors": "Yura N. Perov, Frank D. Wood", "title": "Learning Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a technique for generalising from data in which models are\nsamplers represented as program text. We establish encouraging empirical\nresults that suggest that Markov chain Monte Carlo probabilistic programming\ninference techniques coupled with higher-order probabilistic programming\nlanguages are now sufficiently powerful to enable successful inference of this\nkind in nontrivial domains. We also introduce a new notion of probabilistic\nprogram compilation and show how the same machinery might be used in the future\nto compile probabilistic programs for efficient reusable predictive inference.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 22:06:18 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Perov", "Yura N.", ""], ["Wood", "Frank D.", ""]]}, {"id": "1407.2657", "submitter": "Chicheng Zhang", "authors": "Chicheng Zhang and Kamalika Chaudhuri", "title": "Beyond Disagreement-based Agnostic Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study agnostic active learning, where the goal is to learn a classifier in\na pre-specified hypothesis class interactively with as few label queries as\npossible, while making no assumptions on the true function generating the\nlabels. The main algorithms for this problem are {\\em{disagreement-based active\nlearning}}, which has a high label requirement, and {\\em{margin-based active\nlearning}}, which only applies to fairly restricted settings. A major challenge\nis to find an algorithm which achieves better label complexity, is consistent\nin an agnostic setting, and applies to general classification problems.\n  In this paper, we provide such an algorithm. Our solution is based on two\nnovel contributions -- a reduction from consistent active learning to\nconfidence-rated prediction with guaranteed error, and a novel confidence-rated\npredictor.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 00:34:16 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 23:35:49 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Zhang", "Chicheng", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1407.2662", "submitter": "Uri Stemmer", "authors": "Amos Beimel, Kobbi Nissim, Uri Stemmer", "title": "Learning Privately with Labeled and Unlabeled Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A private learner is an algorithm that given a sample of labeled individual\nexamples outputs a generalizing hypothesis while preserving the privacy of each\nindividual. In 2008, Kasiviswanathan et al. (FOCS 2008) gave a generic\nconstruction of private learners, in which the sample complexity is (generally)\nhigher than what is needed for non-private learners. This gap in the sample\ncomplexity was then further studied in several followup papers, showing that\n(at least in some cases) this gap is unavoidable. Moreover, those papers\nconsidered ways to overcome the gap, by relaxing either the privacy or the\nlearning guarantees of the learner.\n  We suggest an alternative approach, inspired by the (non-private) models of\nsemi-supervised learning and active-learning, where the focus is on the sample\ncomplexity of labeled examples whereas unlabeled examples are of a\nsignificantly lower cost. We consider private semi-supervised learners that\noperate on a random sample, where only a (hopefully small) portion of this\nsample is labeled. The learners have no control over which of the sample\nelements are labeled. Our main result is that the labeled sample complexity of\nprivate learners is characterized by the VC dimension.\n  We present two generic constructions of private semi-supervised learners. The\nfirst construction is of learners where the labeled sample complexity is\nproportional to the VC dimension of the concept class, however, the unlabeled\nsample complexity of the algorithm is as big as the representation length of\ndomain elements. Our second construction presents a new technique for\ndecreasing the labeled sample complexity of a given private learner, while\nroughly maintaining its unlabeled sample complexity. In addition, we show that\nin some settings the labeled sample complexity does not depend on the privacy\nparameters of the learner.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 00:55:39 GMT"}, {"version": "v2", "created": "Thu, 7 Aug 2014 03:39:39 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2015 20:28:50 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Beimel", "Amos", ""], ["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""]]}, {"id": "1407.2674", "submitter": "Kobbi Nissim", "authors": "Amos Beimel, Kobbi Nissim, Uri Stemmer", "title": "Private Learning and Sanitization: Pure vs. Approximate Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the sample complexity of private learning [Kasiviswanathan et al.\n2008] and sanitization~[Blum et al. 2008] under pure $\\epsilon$-differential\nprivacy [Dwork et al. TCC 2006] and approximate\n$(\\epsilon,\\delta)$-differential privacy [Dwork et al. Eurocrypt 2006]. We show\nthat the sample complexity of these tasks under approximate differential\nprivacy can be significantly lower than that under pure differential privacy.\n  We define a family of optimization problems, which we call Quasi-Concave\nPromise Problems, that generalizes some of our considered tasks. We observe\nthat a quasi-concave promise problem can be privately approximated using a\nsolution to a smaller instance of a quasi-concave promise problem. This allows\nus to construct an efficient recursive algorithm solving such problems\nprivately. Specifically, we construct private learners for point functions,\nthreshold functions, and axis-aligned rectangles in high dimension. Similarly,\nwe construct sanitizers for point functions and threshold functions.\n  We also examine the sample complexity of label-private learners, a relaxation\nof private learning where the learner is required to only protect the privacy\nof the labels in the sample. We show that the VC dimension completely\ncharacterizes the sample complexity of such learners, that is, the sample\ncomplexity of learning with label privacy is equal (up to constants) to\nlearning without privacy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 01:42:44 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Beimel", "Amos", ""], ["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""]]}, {"id": "1407.2676", "submitter": "Peter Frazier", "authors": "Ilya O. Ryzhov and Peter I. Frazier and Warren B. Powell", "title": "A New Optimal Stepsize For Approximate Dynamic Programming", "comments": "Matlab files are included with the paper source", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate dynamic programming (ADP) has proven itself in a wide range of\napplications spanning large-scale transportation problems, health care, revenue\nmanagement, and energy systems. The design of effective ADP algorithms has many\ndimensions, but one crucial factor is the stepsize rule used to update a value\nfunction approximation. Many operations research applications are\ncomputationally intensive, and it is important to obtain good results quickly.\nFurthermore, the most popular stepsize formulas use tunable parameters and can\nproduce very poor results if tuned improperly. We derive a new stepsize rule\nthat optimizes the prediction error in order to improve the short-term\nperformance of an ADP algorithm. With only one, relatively insensitive tunable\nparameter, the new rule adapts to the level of noise in the problem and\nproduces faster convergence in numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 02:34:15 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 00:24:14 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Ryzhov", "Ilya O.", ""], ["Frazier", "Peter I.", ""], ["Powell", "Warren B.", ""]]}, {"id": "1407.2697", "submitter": "Aaron Defazio Mr", "authors": "Aaron J. Defazio and Tiberio S. Caetano", "title": "A Convex Formulation for Learning Scale-Free Networks via Submodular\n  Relaxation", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 25 (NIPS 2012)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in statistics and machine learning is the determination of\nnetwork structure from data. We consider the case where the structure of the\ngraph to be reconstructed is known to be scale-free. We show that in such cases\nit is natural to formulate structured sparsity inducing priors using submodular\nfunctions, and we use their Lov\\'asz extension to obtain a convex relaxation.\nFor tractable classes such as Gaussian graphical models, this leads to a convex\noptimization problem that can be efficiently solved. We show that our method\nresults in an improvement in the accuracy of reconstructed networks for\nsynthetic data. We also show how our prior encourages scale-free\nreconstructions on a bioinfomatics dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 05:45:17 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Defazio", "Aaron J.", ""], ["Caetano", "Tiberio S.", ""]]}, {"id": "1407.2710", "submitter": "Aaron Defazio Mr", "authors": "Aaron J. Defazio and Tib\\'erio S. Caetano and Justin Domke", "title": "Finito: A Faster, Permutable Incremental Gradient Method for Big Data\n  Problems", "comments": null, "journal-ref": "International Conference on Machine Learning 2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in optimization theory have shown that smooth strongly convex\nfinite sums can be minimized faster than by treating them as a black box\n\"batch\" problem. In this work we introduce a new method in this class with a\ntheoretical convergence rate four times faster than existing methods, for sums\nwith sufficiently many terms. This method is also amendable to a sampling\nwithout replacement scheme that in practice gives further speed-ups. We give\nempirical results showing state of the art performance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 07:01:31 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Defazio", "Aaron J.", ""], ["Caetano", "Tib\u00e9rio S.", ""], ["Domke", "Justin", ""]]}, {"id": "1407.2736", "submitter": "Hima Patel", "authors": "Ramasubramanian Sundararajan, Hima Patel, Manisha Srivastava", "title": "A multi-instance learning algorithm based on a stacked ensemble of lazy\n  learners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This document describes a novel learning algorithm that classifies \"bags\" of\ninstances rather than individual instances. A bag is labeled positive if it\ncontains at least one positive instance (which may or may not be specifically\nidentified), and negative otherwise. This class of problems is known as\nmulti-instance learning problems, and is useful in situations where the class\nlabel at an instance level may be unavailable or imprecise or difficult to\nobtain, or in situations where the problem is naturally posed as one of\nclassifying instance groups. The algorithm described here is an ensemble-based\nmethod, wherein the members of the ensemble are lazy learning classifiers\nlearnt using the Citation Nearest Neighbour method. Diversity among the\nensemble members is achieved by optimizing their parameters using a\nmulti-objective optimization method, with the objectives being to maximize\nClass 1 accuracy and minimize false positive rate. The method has been found to\nbe effective on the Musk1 benchmark dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 09:39:24 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Sundararajan", "Ramasubramanian", ""], ["Patel", "Hima", ""], ["Srivastava", "Manisha", ""]]}, {"id": "1407.2776", "submitter": "Seyed-Mahdi Khaligh-Razavi", "authors": "Seyed-Mahdi Khaligh-Razavi", "title": "What you need to know about the state-of-the-art computational models of\n  object-vision: A tour through the models", "comments": "36 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of object vision have been of great interest in computer vision and\nvisual neuroscience. During the last decades, several models have been\ndeveloped to extract visual features from images for object recognition tasks.\nSome of these were inspired by the hierarchical structure of primate visual\nsystem, and some others were engineered models. The models are varied in\nseveral aspects: models that are trained by supervision, models trained without\nsupervision, and models (e.g. feature extractors) that are fully hard-wired and\ndo not need training. Some of the models come with a deep hierarchical\nstructure consisting of several layers, and some others are shallow and come\nwith only one or two layers of processing. More recently, new models have been\ndeveloped that are not hand-tuned but trained using millions of images, through\nwhich they learn how to extract informative task-related features. Here I will\nsurvey all these different models and provide the reader with an intuitive, as\nwell as a more detailed, understanding of the underlying computations in each\nof the models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 13:15:18 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Khaligh-Razavi", "Seyed-Mahdi", ""]]}, {"id": "1407.2806", "submitter": "Preux Philippe", "authors": "J\\'er\\'emie Mary (INRIA Lille - Nord Europe, LIFL), Romaric Gaudel\n  (INRIA Lille - Nord Europe, LIFL), Preux Philippe (INRIA Lille - Nord Europe,\n  LIFL)", "title": "Bandits Warm-up Cold Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-8563", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the cold start problem in recommendation systems assuming no\ncontextual information is available neither about users, nor items. We consider\nthe case in which we only have access to a set of ratings of items by users.\nMost of the existing works consider a batch setting, and use cross-validation\nto tune parameters. The classical method consists in minimizing the root mean\nsquare error over a training subset of the ratings which provides a\nfactorization of the matrix of ratings, interpreted as a latent representation\nof items and users. Our contribution in this paper is 5-fold. First, we\nexplicit the issues raised by this kind of batch setting for users or items\nwith very few ratings. Then, we propose an online setting closer to the actual\nuse of recommender systems; this setting is inspired by the bandit framework.\nThe proposed methodology can be used to turn any recommender system dataset\n(such as Netflix, MovieLens,...) into a sequential dataset. Then, we explicit a\nstrong and insightful link between contextual bandit algorithms and matrix\nfactorization; this leads us to a new algorithm that tackles the\nexploration/exploitation dilemma associated to the cold start problem in a\nstrikingly new perspective. Finally, experimental evidence confirm that our\nalgorithm is effective in dealing with the cold start problem on publicly\navailable datasets. Overall, the goal of this paper is to bridge the gap\nbetween recommender systems based on matrix factorizations and those based on\ncontextual bandits.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 14:32:37 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Mary", "J\u00e9r\u00e9mie", "", "INRIA Lille - Nord Europe, LIFL"], ["Gaudel", "Romaric", "", "INRIA Lille - Nord Europe, LIFL"], ["Philippe", "Preux", "", "INRIA Lille - Nord Europe,\n  LIFL"]]}, {"id": "1407.2845", "submitter": "Emilio Ferrara", "authors": "Santa Agreste, Pasquale De Meo, Emilio Ferrara, Domenico Ursino", "title": "XML Matchers: approaches and challenges", "comments": "34 pages, 8 tables, 7 figures", "journal-ref": "Knowledge-based systems 66: 190-209, 2014", "doi": "10.1016/j.knosys.2014.04.044", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema Matching, i.e. the process of discovering semantic correspondences\nbetween concepts adopted in different data source schemas, has been a key topic\nin Database and Artificial Intelligence research areas for many years. In the\npast, it was largely investigated especially for classical database models\n(e.g., E/R schemas, relational databases, etc.). However, in the latest years,\nthe widespread adoption of XML in the most disparate application fields pushed\na growing number of researchers to design XML-specific Schema Matching\napproaches, called XML Matchers, aiming at finding semantic matchings between\nconcepts defined in DTDs and XSDs. XML Matchers do not just take well-known\ntechniques originally designed for other data models and apply them on\nDTDs/XSDs, but they exploit specific XML features (e.g., the hierarchical\nstructure of a DTD/XSD) to improve the performance of the Schema Matching\nprocess. The design of XML Matchers is currently a well-established research\narea. The main goal of this paper is to provide a detailed description and\nclassification of XML Matchers. We first describe to what extent the\nspecificities of DTDs/XSDs impact on the Schema Matching task. Then we\nintroduce a template, called XML Matcher Template, that describes the main\ncomponents of an XML Matcher, their role and behavior. We illustrate how each\nof these components has been implemented in some popular XML Matchers. We\nconsider our XML Matcher Template as the baseline for objectively comparing\napproaches that, at first glance, might appear as unrelated. The introduction\nof this template can be useful in the design of future XML Matchers. Finally,\nwe analyze commercial tools implementing XML Matchers and introduce two\nchallenging issues strictly related to this topic, namely XML source clustering\nand uncertainty management in XML Matchers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 16:14:11 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Agreste", "Santa", ""], ["De Meo", "Pasquale", ""], ["Ferrara", "Emilio", ""], ["Ursino", "Domenico", ""]]}, {"id": "1407.2904", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "An eigenanalysis of data centering in machine learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many pattern recognition methods rely on statistical information from\ncentered data, with the eigenanalysis of an empirical central moment, such as\nthe covariance matrix in principal component analysis (PCA), as well as partial\nleast squares regression, canonical-correlation analysis and Fisher\ndiscriminant analysis. Recently, many researchers advocate working on\nnon-centered data. This is the case for instance with the singular value\ndecomposition approach, with the (kernel) entropy component analysis, with the\ninformation-theoretic learning framework, and even with nonnegative matrix\nfactorization. Moreover, one can also consider a non-centered PCA by using the\nsecond-order non-central moment.\n  The main purpose of this paper is to bridge the gap between these two\nviewpoints in designing machine learning methods. To provide a study at the\ncornerstone of kernel-based machines, we conduct an eigenanalysis of the inner\nproduct matrices from centered and non-centered data. We derive several results\nconnecting their eigenvalues and their eigenvectors. Furthermore, we explore\nthe outer product matrices, by providing several results connecting the largest\neigenvectors of the covariance matrix and its non-centered counterpart. These\nresults lay the groundwork to several extensions beyond conventional centering,\nwith the weighted mean shift, the rank-one update, and the multidimensional\nscaling. Experiments conducted on simulated and real data illustrate the\nrelevance of this work.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 19:04:49 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1407.2919", "submitter": "Weike Pan", "authors": "Weike Pan", "title": "Collaborative Recommendation with Auxiliary Data: A Transfer Learning\n  View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent recommendation technology has been playing an increasingly\nimportant role in various industry applications such as e-commerce product\npromotion and Internet advertisement display. Besides users' feedbacks (e.g.,\nnumerical ratings) on items as usually exploited by some typical recommendation\nalgorithms, there are often some additional data such as users' social circles\nand other behaviors. Such auxiliary data are usually related to users'\npreferences on items behind the numerical ratings. Collaborative recommendation\nwith auxiliary data (CRAD) aims to leverage such additional information so as\nto improve the personalization services, which have received much attention\nfrom both researchers and practitioners.\n  Transfer learning (TL) is proposed to extract and transfer knowledge from\nsome auxiliary data in order to assist the learning task on some target data.\nIn this paper, we consider the CRAD problem from a transfer learning view,\nespecially on how to achieve knowledge transfer from some auxiliary data.\nFirst, we give a formal definition of transfer learning for CRAD (TL-CRAD).\nSecond, we extend the existing categorization of TL techniques (i.e., adaptive,\ncollective and integrative knowledge transfer algorithm styles) with three\nknowledge transfer strategies (i.e., prediction rule, regularization and\nconstraint). Third, we propose a novel generic knowledge transfer framework for\nTL-CRAD. Fourth, we describe some representative works of each specific\nknowledge transfer strategy of each algorithm style in detail, which are\nexpected to inspire further works. Finally, we conclude the paper with some\nsummary discussions and several future directions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 01:39:03 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Pan", "Weike", ""]]}, {"id": "1407.2987", "submitter": "Eren Golge", "authors": "Eren Golge and Pinar Duygulu", "title": "FAME: Face Association through Model Evolution", "comments": "Draft version of the study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We attack the problem of learning face models for public faces from\nweakly-labelled images collected from web through querying a name. The data is\nvery noisy even after face detection, with several irrelevant faces\ncorresponding to other people. We propose a novel method, Face Association\nthrough Model Evolution (FAME), that is able to prune the data in an iterative\nway, for the face models associated to a name to evolve. The idea is based on\ncapturing discriminativeness and representativeness of each instance and\neliminating the outliers. The final models are used to classify faces on novel\ndatasets with possibly different characteristics. On benchmark datasets, our\nresults are comparable to or better than state-of-the-art studies for the task\nof face identification.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 23:52:44 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Golge", "Eren", ""], ["Duygulu", "Pinar", ""]]}, {"id": "1407.3026", "submitter": "Hima Patel", "authors": "Ramasubramanian Sundararajan, Hima Patel, Dattesh Shanbhag, Vivek\n  Vaidya", "title": "An SVM Based Approach for Cardiac View Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We consider the problem of automatically prescribing oblique planes (short\naxis, 4 chamber and 2 chamber views) in Cardiac Magnetic Resonance Imaging\n(MRI). A concern with technologist-driven acquisitions of these planes is the\nquality and time taken for the total examination. We propose an automated\nsolution incorporating anatomical features external to the cardiac region. The\nsolution uses support vector machine regression models wherein complexity and\nfeature selection are optimized using multi-objective genetic algorithms.\nAdditionally, we examine the robustness of our approach by training our models\non images with additive Rician-Gaussian mixtures at varying Signal to Noise\n(SNR) levels. Our approach has shown promising results, with an angular\ndeviation of less than 15 degrees on 90% cases across oblique planes, measured\nin terms of average 6-fold cross validation performance -- this is generally\nwithin acceptable bounds of variation as specified by clinicians.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 04:56:49 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Sundararajan", "Ramasubramanian", ""], ["Patel", "Hima", ""], ["Shanbhag", "Dattesh", ""], ["Vaidya", "Vivek", ""]]}, {"id": "1407.3068", "submitter": "Marijn Stollenga", "authors": "Marijn Stollenga, Jonathan Masci, Faustino Gomez, Juergen Schmidhuber", "title": "Deep Networks with Internal Selective Attention through Feedback\n  Connections", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional convolutional neural networks (CNN) are stationary and\nfeedforward. They neither change their parameters during evaluation nor use\nfeedback from higher to lower layers. Real brains, however, do. So does our\nDeep Attention Selective Network (dasNet) architecture. DasNets feedback\nstructure can dynamically alter its convolutional filter sensitivities during\nclassification. It harnesses the power of sequential processing to improve\nclassification performance, by allowing the network to iteratively focus its\ninternal attention on some of its convolutional filters. Feedback is trained\nthrough direct policy search in a huge million-dimensional parameter space,\nthrough scalable natural evolution strategies (SNES). On the CIFAR-10 and\nCIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 08:56:54 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 08:22:50 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Stollenga", "Marijn", ""], ["Masci", "Jonathan", ""], ["Gomez", "Faustino", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1407.3242", "submitter": "Marcello La Rocca", "authors": "Marcello La Rocca", "title": "Density Adaptive Parallel Clustering", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are going to introduce a new nearest neighbours based\napproach to clustering, and compare it with previous solutions; the resulting\nalgorithm, which takes inspiration from both DBscan and minimum spanning tree\napproaches, is deterministic but proves simpler, faster and doesnt require to\nset in advance a value for k, the number of clusters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 18:24:15 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["La Rocca", "Marcello", ""]]}, {"id": "1407.3269", "submitter": "Sakyasingha Dasgupta", "authors": "Guanjiao Ren, Weihai Chen, Sakyasingha Dasgupta, Christoph\n  Kolodziejski, Florentin W\\\"org\\\"otter, Poramate Manoonpong", "title": "Multiple chaotic central pattern generators with learning for legged\n  locomotion and malfunction compensation", "comments": "48 pages, 16 figures, Information Sciences 2014", "journal-ref": null, "doi": "10.1016/j.ins.2014.05.001", "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  An originally chaotic system can be controlled into various periodic\ndynamics. When it is implemented into a legged robot's locomotion control as a\ncentral pattern generator (CPG), sophisticated gait patterns arise so that the\nrobot can perform various walking behaviors. However, such a single chaotic CPG\ncontroller has difficulties dealing with leg malfunction. Specifically, in the\nscenarios presented here, its movement permanently deviates from the desired\ntrajectory. To address this problem, we extend the single chaotic CPG to\nmultiple CPGs with learning. The learning mechanism is based on a simulated\nannealing algorithm. In a normal situation, the CPGs synchronize and their\ndynamics are identical. With leg malfunction or disability, the CPGs lose\nsynchronization leading to independent dynamics. In this case, the learning\nmechanism is applied to automatically adjust the remaining legs' oscillation\nfrequencies so that the robot adapts its locomotion to deal with the\nmalfunction. As a consequence, the trajectory produced by the multiple chaotic\nCPGs resembles the original trajectory far better than the one produced by only\na single CPG. The performance of the system is evaluated first in a physical\nsimulation of a quadruped as well as a hexapod robot and finally in a real\nsix-legged walking machine called AMOSII. The experimental results presented\nhere reveal that using multiple CPGs with learning is an effective approach for\nadaptive locomotion generation where, for instance, different body parts have\nto perform independent movements for malfunction compensation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 14:21:22 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Ren", "Guanjiao", ""], ["Chen", "Weihai", ""], ["Dasgupta", "Sakyasingha", ""], ["Kolodziejski", "Christoph", ""], ["W\u00f6rg\u00f6tter", "Florentin", ""], ["Manoonpong", "Poramate", ""]]}, {"id": "1407.3289", "submitter": "Stefan Wager", "authors": "Stefan Wager, William Fithian, Sida Wang, and Percy Liang", "title": "Altitude Training: Strong Bounds for Single-Layer Dropout", "comments": "Advances in Neural Information Processing Systems (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout training, originally designed for deep neural networks, has been\nsuccessful on high-dimensional single-layer natural language tasks. This paper\nproposes a theoretical explanation for this phenomenon: we show that, under a\ngenerative Poisson topic model with long documents, dropout training improves\nthe exponent in the generalization bound for empirical risk minimization.\nDropout achieves this gain much like a marathon runner who practices at\naltitude: once a classifier learns to perform reasonably well on training\nexamples that have been artificially corrupted by dropout, it will do very well\non the uncorrupted test set. We also show that, under similar conditions,\ndropout preserves the Bayes decision boundary and should therefore induce\nminimal bias in high dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 20:32:34 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 18:30:18 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Wager", "Stefan", ""], ["Fithian", "William", ""], ["Wang", "Sida", ""], ["Liang", "Percy", ""]]}, {"id": "1407.3334", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Offline to Online Conversion", "comments": "20 LaTeX pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of converting offline estimators into an online\npredictor or estimator with small extra regret. Formally this is the problem of\nmerging a collection of probability measures over strings of length 1,2,3,...\ninto a single probability measure over infinite sequences. We describe various\napproaches and their pros and cons on various examples. As a side-result we\ngive an elementary non-heuristic purely combinatoric derivation of Turing's\nfamous estimator. Our main technical contribution is to determine the\ncomputational complexity of online estimators with good guarantees in general.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 01:30:59 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "1407.3341", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Extreme State Aggregation Beyond MDPs", "comments": "28 LaTeX pages. 8 Theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Reinforcement Learning setup where an agent interacts with an\nenvironment in observation-reward-action cycles without any (esp.\\ MDP)\nassumptions on the environment. State aggregation and more generally feature\nreinforcement learning is concerned with mapping histories/raw-states to\nreduced/aggregated states. The idea behind both is that the resulting reduced\nprocess (approximately) forms a small stationary finite-state MDP, which can\nthen be efficiently solved or learnt. We considerably generalize existing\naggregation results by showing that even if the reduced process is not an MDP,\nthe (q-)value functions and (optimal) policies of an associated MDP with same\nstate-space size solve the original problem, as long as the solution can\napproximately be represented as a function of the reduced states. This implies\nan upper bound on the required state space size that holds uniformly for all RL\nproblems. It may also explain why RL algorithms designed for MDPs sometimes\nperform well beyond MDPs.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 04:10:43 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "1407.3422", "submitter": "Igor Melnyk", "authors": "Igor Melnyk and Arindam Banerjee", "title": "A Spectral Algorithm for Inference in Hidden Semi-Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden semi-Markov models (HSMMs) are latent variable models which allow\nlatent state persistence and can be viewed as a generalization of the popular\nhidden Markov models (HMMs). In this paper, we introduce a novel spectral\nalgorithm to perform inference in HSMMs. Unlike expectation maximization (EM),\nour approach correctly estimates the probability of given observation sequence\nbased on a set of training sequences. Our approach is based on estimating\nmoments from the sample, whose number of dimensions depends only\nlogarithmically on the maximum length of the hidden state persistence.\nMoreover, the algorithm requires only a few matrix inversions and is therefore\ncomputationally efficient. Empirical evaluations on synthetic and real data\ndemonstrate the advantage of the algorithm over EM in terms of speed and\naccuracy, especially for large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 23:57:07 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 19:29:03 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 00:29:23 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Melnyk", "Igor", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1407.3501", "submitter": "Jean-Baptiste Mouret", "authors": "Antoine Cully, Jeff Clune, Danesh Tarapore, Jean-Baptiste Mouret", "title": "Robots that can adapt like animals", "comments": null, "journal-ref": null, "doi": "10.1038/nature14422", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots leave the controlled environments of factories to autonomously\nfunction in more complex, natural environments, they will have to respond to\nthe inevitable fact that they will become damaged. However, while animals can\nquickly adapt to a wide variety of injuries, current robots cannot \"think\noutside the box\" to find a compensatory behavior when damaged: they are limited\nto their pre-specified self-sensing abilities, can diagnose only anticipated\nfailure modes, and require a pre-programmed contingency plan for every type of\npotential damage, an impracticality for complex robots. Here we introduce an\nintelligent trial and error algorithm that allows robots to adapt to damage in\nless than two minutes, without requiring self-diagnosis or pre-specified\ncontingency plans. Before deployment, a robot exploits a novel algorithm to\ncreate a detailed map of the space of high-performing behaviors: This map\nrepresents the robot's intuitions about what behaviors it can perform and their\nvalue. If the robot is damaged, it uses these intuitions to guide a\ntrial-and-error learning algorithm that conducts intelligent experiments to\nrapidly discover a compensatory behavior that works in spite of the damage.\nExperiments reveal successful adaptations for a legged robot injured in five\ndifferent ways, including damaged, broken, and missing legs, and for a robotic\narm with joints broken in 14 different ways. This new technique will enable\nmore robust, effective, autonomous robots, and suggests principles that animals\nmay use to adapt to injury.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jul 2014 19:06:08 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 16:08:20 GMT"}, {"version": "v3", "created": "Mon, 18 Aug 2014 14:07:53 GMT"}, {"version": "v4", "created": "Wed, 27 May 2015 22:43:04 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Cully", "Antoine", ""], ["Clune", "Jeff", ""], ["Tarapore", "Danesh", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1407.3619", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy and Aarti Singh", "title": "On the Power of Adaptivity in Matrix Completion and Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the related tasks of matrix completion and matrix approximation\nfrom missing data and propose adaptive sampling procedures for both problems.\nWe show that adaptive sampling allows one to eliminate standard incoherence\nassumptions on the matrix row space that are necessary for passive sampling\nprocedures. For exact recovery of a low-rank matrix, our algorithm judiciously\nselects a few columns to observe in full and, with few additional measurements,\nprojects the remaining columns onto their span. This algorithm exactly recovers\nan $n \\times n$ rank $r$ matrix using $O(nr\\mu_0 \\log^2(r))$ observations,\nwhere $\\mu_0$ is a coherence parameter on the column space of the matrix. In\naddition to completely eliminating any row space assumptions that have pervaded\nthe literature, this algorithm enjoys a better sample complexity than any\nexisting matrix completion algorithm. To certify that this improvement is due\nto adaptive sampling, we establish that row space coherence is necessary for\npassive sampling algorithms to achieve non-trivial sample complexity bounds.\n  For constructing a low-rank approximation to a high-rank input matrix, we\npropose a simple algorithm that thresholds the singular values of a zero-filled\nversion of the input matrix. The algorithm computes an approximation that is\nnearly as good as the best rank-$r$ approximation using $O(nr\\mu \\log^2(n))$\nsamples, where $\\mu$ is a slightly different coherence parameter on the matrix\ncolumns. Again we eliminate assumptions on the row space.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 12:14:08 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Singh", "Aarti", ""]]}, {"id": "1407.3685", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall, Jon Hills and Jason Lines", "title": "Finding Motif Sets in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMPC14-03", "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series motifs are representative subsequences that occur frequently in a\ntime series; a motif set is the set of subsequences deemed to be instances of a\ngiven motif. We focus on finding motif sets. Our motivation is to detect motif\nsets in household electricity-usage profiles, representing repeated patterns of\nhousehold usage.\n  We propose three algorithms for finding motif sets. Two are greedy algorithms\nbased on pairwise comparison, and the third uses a heuristic measure of set\nquality to find the motif set directly. We compare these algorithms on\nsimulated datasets and on electricity-usage data. We show that Scan MK, the\nsimplest way of using the best-matching pair to find motif sets, is less\naccurate on our synthetic data than Set Finder and Cluster MK, although the\nlatter is very sensitive to parameter settings. We qualitatively analyse the\noutputs for the electricity-usage data and demonstrate that both Scan MK and\nSet Finder can discover useful motif sets in such data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 15:01:57 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Bagnall", "Anthony", ""], ["Hills", "Jon", ""], ["Lines", "Jason", ""]]}, {"id": "1407.3897", "submitter": "Bryan O'Gorman", "authors": "Bryan O'Gorman, Alejandro Perdomo-Ortiz, Ryan Babbush, Alan\n  Aspuru-Guzik, and Vadim Smelyanskiy", "title": "Bayesian Network Structure Learning Using Quantum Annealing", "comments": null, "journal-ref": "Eur. Phys. J. Spec. Top., 225 (1), 163 (2015)", "doi": "10.1140/epjst/e2015-02349-9", "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for the problem of learning the structure of a Bayesian\nnetwork using the quantum adiabatic algorithm. We do so by introducing an\nefficient reformulation of a standard posterior-probability scoring function on\ngraphs as a pseudo-Boolean function, which is equivalent to a system of 2-body\nIsing spins, as well as suitable penalty terms for enforcing the constraints\nnecessary for the reformulation; our proposed method requires $\\mathcal O(n^2)$\nqubits for $n$ Bayesian network variables. Furthermore, we prove lower bounds\non the necessary weighting of these penalty terms. The logical structure\nresulting from the mapping has the appealing property that it is\ninstance-independent for a given number of Bayesian network variables, as well\nas being independent of the number of data cases.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 07:22:13 GMT"}, {"version": "v2", "created": "Thu, 2 Oct 2014 19:46:35 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["O'Gorman", "Bryan", ""], ["Perdomo-Ortiz", "Alejandro", ""], ["Babbush", "Ryan", ""], ["Aspuru-Guzik", "Alan", ""], ["Smelyanskiy", "Vadim", ""]]}, {"id": "1407.3939", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (DI-ENS, INRIA Paris - Rocquencourt), Robin Genuer\n  (ISPED, INRIA Bordeaux - Sud-Ouest)", "title": "Analysis of purely random forests bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are a very effective and commonly used statistical method, but\ntheir full theoretical analysis is still an open problem. As a first step,\nsimplified models such as purely random forests have been introduced, in order\nto shed light on the good performance of random forests. In this paper, we\nstudy the approximation error (the bias) of some purely random forest models in\na regression framework, focusing in particular on the influence of the number\nof trees in the forest. Under some regularity assumptions on the regression\nfunction, we show that the bias of an infinite forest decreases at a faster\nrate (with respect to the size of each tree) than a single tree. As a\nconsequence, infinite forests attain a strictly better risk rate (with respect\nto the sample size) than single trees. Furthermore, our results allow to derive\na minimum number of trees sufficient to reach the same rate as an infinite\nforest. As a by-product of our analysis, we also show a link between the bias\nof purely random forests and the bias of some kernel estimators.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 11:12:54 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Arlot", "Sylvain", "", "DI-ENS, INRIA Paris - Rocquencourt"], ["Genuer", "Robin", "", "ISPED, INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1407.4070", "submitter": "Mary Wootters", "authors": "Moritz Hardt and Mary Wootters", "title": "Fast matrix completion without the condition number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first algorithm for Matrix Completion whose running time and\nsample complexity is polynomial in the rank of the unknown target matrix,\nlinear in the dimension of the matrix, and logarithmic in the condition number\nof the matrix. To the best of our knowledge, all previous algorithms either\nincurred a quadratic dependence on the condition number of the unknown matrix\nor a quadratic dependence on the dimension of the matrix in the running time.\nOur algorithm is based on a novel extension of Alternating Minimization which\nwe show has theoretical guarantees under standard assumptions even in the\npresence of noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 17:47:44 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Hardt", "Moritz", ""], ["Wootters", "Mary", ""]]}, {"id": "1407.4075", "submitter": "Grigori Fursin", "authors": "Lianjie Luo and Yang Chen and Chengyong Wu and Shun Long and Grigori\n  Fursin", "title": "Finding representative sets of optimizations for adaptive\n  multiversioning applications", "comments": "3rd Workshop on Statistical and Machine Learning Approaches Applied\n  to Architectures and Compilation (SMART'09), co-located with HiPEAC'09\n  conference, Paphos, Cyprus, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative compilation is a widely adopted technique to optimize programs for\ndifferent constraints such as performance, code size and power consumption in\nrapidly evolving hardware and software environments. However, in case of\nstatically compiled programs, it is often restricted to optimizations for a\nspecific dataset and may not be applicable to applications that exhibit\ndifferent run-time behavior across program phases, multiple datasets or when\nexecuted in heterogeneous, reconfigurable and virtual environments. Several\nframeworks have been recently introduced to tackle these problems and enable\nrun-time optimization and adaptation for statically compiled programs based on\nstatic function multiversioning and monitoring of online program behavior. In\nthis article, we present a novel technique to select a minimal set of\nrepresentative optimization variants (function versions) for such frameworks\nwhile avoiding performance loss across available datasets and code-size\nexplosion. We developed a novel mapping mechanism using popular decision tree\nor rule induction based machine learning techniques to rapidly select best code\nversions at run-time based on dataset features and minimize selection overhead.\nThese techniques enable creation of self-tuning static binaries or libraries\nadaptable to changing behavior and environments at run-time using staged\ncompilation that do not require complex recompilation frameworks while\neffectively outperforming traditional single-version non-adaptable code.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 17:55:07 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Luo", "Lianjie", ""], ["Chen", "Yang", ""], ["Wu", "Chengyong", ""], ["Long", "Shun", ""], ["Fursin", "Grigori", ""]]}, {"id": "1407.4416", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "In Defense of MinHash Over SimHash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MinHash and SimHash are the two widely adopted Locality Sensitive Hashing\n(LSH) algorithms for large-scale data processing applications. Deciding which\nLSH to use for a particular problem at hand is an important question, which has\nno clear answer in the existing literature. In this study, we provide a\ntheoretical answer (validated by experiments) that MinHash virtually always\noutperforms SimHash when the data are binary, as common in practice such as\nsearch.\n  The collision probability of MinHash is a function of resemblance similarity\n($\\mathcal{R}$), while the collision probability of SimHash is a function of\ncosine similarity ($\\mathcal{S}$). To provide a common basis for comparison, we\nevaluate retrieval results in terms of $\\mathcal{S}$ for both MinHash and\nSimHash. This evaluation is valid as we can prove that MinHash is a valid LSH\nwith respect to $\\mathcal{S}$, by using a general inequality $\\mathcal{S}^2\\leq\n\\mathcal{R}\\leq \\frac{\\mathcal{S}}{2-\\mathcal{S}}$. Our worst case analysis can\nshow that MinHash significantly outperforms SimHash in high similarity region.\n  Interestingly, our intensive experiments reveal that MinHash is also\nsubstantially better than SimHash even in datasets where most of the data\npoints are not too similar to each other. This is partly because, in practical\ndata, often $\\mathcal{R}\\geq \\frac{\\mathcal{S}}{z-\\mathcal{S}}$ holds where $z$\nis only slightly larger than 2 (e.g., $z\\leq 2.1$). Our restricted worst case\nanalysis by assuming $\\frac{\\mathcal{S}}{z-\\mathcal{S}}\\leq \\mathcal{R}\\leq\n\\frac{\\mathcal{S}}{2-\\mathcal{S}}$ shows that MinHash indeed significantly\noutperforms SimHash even in low similarity region.\n  We believe the results in this paper will provide valuable guidelines for\nsearch in practice, especially when the data are sparse.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:27:02 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1407.4420", "submitter": "Paul Honeine", "authors": "Fei Zhu, Paul Honeine, Maya Kallas", "title": "Kernel Nonnegative Matrix Factorization Without the Curse of the\n  Pre-image - Application to Unmixing Hyperspectral Images", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonnegative matrix factorization (NMF) is widely used in signal and image\nprocessing, including bio-informatics, blind source separation and\nhyperspectral image analysis in remote sensing. A great challenge arises when\ndealing with a nonlinear formulation of the NMF. Within the framework of kernel\nmachines, the models suggested in the literature do not allow the\nrepresentation of the factorization matrices, which is a fallout of the curse\nof the pre-image. In this paper, we propose a novel kernel-based model for the\nNMF that does not suffer from the pre-image problem, by investigating the\nestimation of the factorization matrices directly in the input space. For\ndifferent kernel functions, we describe two schemes for iterative algorithms:\nan additive update rule based on a gradient descent scheme and a multiplicative\nupdate rule in the same spirit as in the Lee and Seung algorithm. Within the\nproposed framework, we develop several extensions to incorporate constraints,\nincluding sparseness, smoothness, and spatial regularization with a\ntotal-variation-like penalty. The effectiveness of the proposed method is\ndemonstrated with the problem of unmixing hyperspectral images, using\nwell-known real images and results with state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:46:41 GMT"}, {"version": "v2", "created": "Sun, 27 Mar 2016 20:44:42 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Zhu", "Fei", ""], ["Honeine", "Paul", ""], ["Kallas", "Maya", ""]]}, {"id": "1407.4422", "submitter": "Jakub Tomczak Ph.D.", "authors": "Jakub M. Tomczak and Adam Gonczarek", "title": "Subspace Restricted Boltzmann Machine", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subspace Restricted Boltzmann Machine (subspaceRBM) is a third-order\nBoltzmann machine where multiplicative interactions are between one visible and\ntwo hidden units. There are two kinds of hidden units, namely, gate units and\nsubspace units. The subspace units reflect variations of a pattern in data and\nthe gate unit is responsible for activating the subspace units. Additionally,\nthe gate unit can be seen as a pooling feature. We evaluate the behavior of\nsubspaceRBM through experiments with MNIST digit recognition task, measuring\nreconstruction error and classification error.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:50:40 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Tomczak", "Jakub M.", ""], ["Gonczarek", "Adam", ""]]}, {"id": "1407.4430", "submitter": "Zhaoyi Kang", "authors": "Zhaoyi Kang and Costas J. Spanos", "title": "Sequential Logistic Principal Component Analysis (SLPCA): Dimensional\n  Reduction in Streaming Multivariate Binary-State System", "comments": "6 pages, 4 figures, conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sequential or online dimensional reduction is of interests due to the\nexplosion of streaming data based applications and the requirement of adaptive\nstatistical modeling, in many emerging fields, such as the modeling of energy\nend-use profile. Principal Component Analysis (PCA), is the classical way of\ndimensional reduction. However, traditional Singular Value Decomposition (SVD)\nbased PCA fails to model data which largely deviates from Gaussian\ndistribution. The Bregman Divergence was recently introduced to achieve a\ngeneralized PCA framework. If the random variable under dimensional reduction\nfollows Bernoulli distribution, which occurs in many emerging fields, the\ngeneralized PCA is called Logistic PCA (LPCA). In this paper, we extend the\nbatch LPCA to a sequential version (i.e. SLPCA), based on the sequential convex\noptimization theory. The convergence property of this algorithm is discussed\ncompared to the batch version of LPCA (i.e. BLPCA), as well as its performance\nin reducing the dimension for multivariate binary-state systems. Its\napplication in building energy end-use profile modeling is also investigated.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 19:05:55 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Kang", "Zhaoyi", ""], ["Spanos", "Costas J.", ""]]}, {"id": "1407.4443", "submitter": "Emilie Kaufmann", "authors": "Emilie Kaufmann (SEQUEL, LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien\n  Garivier (IMT)", "title": "On the Complexity of Best Arm Identification in Multi-Armed Bandit\n  Models", "comments": "arXiv admin note: text overlap with arXiv:1405.3224", "journal-ref": "Journal of Machine Learning Research, Journal of Machine Learning\n  Research, 2016, 17, pp.1-42", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic multi-armed bandit model is a simple abstraction that has\nproven useful in many different contexts in statistics and machine learning.\nWhereas the achievable limit in terms of regret minimization is now well known,\nour aim is to contribute to a better understanding of the performance in terms\nof identifying the m best arms. We introduce generic notions of complexity for\nthe two dominant frameworks considered in the literature: fixed-budget and\nfixed-confidence settings. In the fixed-confidence setting, we provide the\nfirst known distribution-dependent lower bound on the complexity that involves\ninformation-theoretic quantities and holds when m is larger than 1 under\ngeneral assumptions. In the specific case of two armed-bandits, we derive\nrefined lower bounds in both the fixed-confidence and fixed-budget settings,\nalong with matching algorithms for Gaussian and Bernoulli bandit models. These\nresults show in particular that the complexity of the fixed-budget setting may\nbe smaller than the complexity of the fixed-confidence setting, contradicting\nthe familiar behavior observed when testing fully specified alternatives. In\naddition, we also provide improved sequential stopping rules that have\nguaranteed error probabilities and shorter average running times. The proofs\nrely on two technical results that are of independent interest : a deviation\nlemma for self-normalized sums (Lemma 19) and a novel change of measure\ninequality for bandit models (Lemma 1).\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 19:44:15 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 12:38:45 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kaufmann", "Emilie", "", "SEQUEL, LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"], ["Garivier", "Aur\u00e9lien", "", "IMT"]]}, {"id": "1407.4446", "submitter": "Weidong Han", "authors": "Weidong Han, Purnima Rajan, Peter I. Frazier, Bruno M. Jedynak", "title": "Probabilistic Group Testing under Sum Observations: A Parallelizable\n  2-Approximation for Entropy Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of group testing with sum observations and noiseless\nanswers, in which we aim to locate multiple objects by querying the number of\nobjects in each of a sequence of chosen sets. We study a probabilistic setting\nwith entropy loss, in which we assume a joint Bayesian prior density on the\nlocations of the objects and seek to choose the sets queried to minimize the\nexpected entropy of the Bayesian posterior distribution after a fixed number of\nquestions. We present a new non-adaptive policy, called the dyadic policy, show\nit is optimal among non-adaptive policies, and is within a factor of two of\noptimal among adaptive policies. This policy is quick to compute, its\nnonadaptive nature makes it easy to parallelize, and our bounds show it\nperforms well even when compared with adaptive policies. We also study an\nadaptive greedy policy, which maximizes the one-step expected reduction in\nentropy, and show that it performs at least as well as the dyadic policy,\noffering greater query efficiency but reduced parallelism. Numerical\nexperiments demonstrate that both procedures outperform a divide-and-conquer\nbenchmark policy from the literature, called sequential bifurcation, and show\nhow these procedures may be applied in a stylized computer vision problem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 19:55:51 GMT"}, {"version": "v2", "created": "Sat, 26 Jul 2014 15:28:35 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 03:32:33 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Han", "Weidong", ""], ["Rajan", "Purnima", ""], ["Frazier", "Peter I.", ""], ["Jedynak", "Bruno M.", ""]]}, {"id": "1407.4668", "submitter": "Albrecht Zimmermann", "authors": "Albrecht Zimmermann", "title": "A feature construction framework based on outlier detection and\n  discriminative pattern mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No matter the expressive power and sophistication of supervised learning\nalgorithms, their effectiveness is restricted by the features describing the\ndata. This is not a new insight in ML and many methods for feature selection,\ntransformation, and construction have been developed. But while this is\non-going for general techniques for feature selection and transformation, i.e.\ndimensionality reduction, work on feature construction, i.e. enriching the\ndata, is by now mainly the domain of image, particularly character,\nrecognition, and NLP.\n  In this work, we propose a new general framework for feature construction.\nThe need for feature construction in a data set is indicated by class outliers\nand discriminative pattern mining used to derive features on their\nk-neighborhoods. We instantiate the framework with LOF and C4.5-Rules, and\nevaluate the usefulness of the derived features on a diverse collection of UCI\ndata sets. The derived features are more often useful than ones derived by\nDC-Fringe, and our approach is much less likely to overfit. But while a weak\nlearner, Naive Bayes, benefits strongly from the feature construction, the\neffect is less pronounced for C4.5, and almost vanishes for an SVM leaner.\n  Keywords: feature construction, classification, outlier detection\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 13:51:55 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Zimmermann", "Albrecht", ""]]}, {"id": "1407.4729", "submitter": "Yin Lou", "authors": "Yin Lou, Jacob Bien, Rich Caruana, Johannes Gehrke", "title": "Sparse Partially Linear Additive Models", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized partially linear additive model (GPLAM) is a flexible and\ninterpretable approach to building predictive models. It combines features in\nan additive manner, allowing each to have either a linear or nonlinear effect\non the response. However, the choice of which features to treat as linear or\nnonlinear is typically assumed known. Thus, to make a GPLAM a viable approach\nin situations in which little is known $a~priori$ about the features, one must\novercome two primary model selection challenges: deciding which features to\ninclude in the model and determining which of these features to treat\nnonlinearly. We introduce the sparse partially linear additive model (SPLAM),\nwhich combines model fitting and $both$ of these model selection challenges\ninto a single convex optimization problem. SPLAM provides a bridge between the\nlasso and sparse additive models. Through a statistical oracle inequality and\nthorough simulation, we demonstrate that SPLAM can outperform other methods\nacross a broad spectrum of statistical regimes, including the high-dimensional\n($p\\gg N$) setting. We develop efficient algorithms that are applied to real\ndata sets with half a million samples and over 45,000 features with excellent\npredictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 16:27:36 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 19:17:59 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 19:02:45 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Lou", "Yin", ""], ["Bien", "Jacob", ""], ["Caruana", "Rich", ""], ["Gehrke", "Johannes", ""]]}, {"id": "1407.4739", "submitter": "T Sarath", "authors": "T.Sarath, G.Nagalakshmi", "title": "An landcover fuzzy logic classification by maximumlikelihood", "comments": "5 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In present days remote sensing is most used application in many sectors. This\nremote sensing uses different images like multispectral, hyper spectral or\nultra spectral. The remote sensing image classification is one of the\nsignificant method to classify image. In this state we classify the maximum\nlikelihood classification with fuzzy logic. In this we experimenting fuzzy\nlogic like spatial, spectral texture methods in that different sub methods to\nbe used for image classification.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 17:10:06 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Sarath", "T.", ""], ["Nagalakshmi", "G.", ""]]}, {"id": "1407.4764", "submitter": "Ken Chatfield", "authors": "Ken Chatfield, Karen Simonyan and Andrew Zisserman", "title": "Efficient On-the-fly Category Retrieval using ConvNets and GPUs", "comments": "Published in proceedings of ACCV 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the gains in precision and speed, that can be obtained by\nusing Convolutional Networks (ConvNets) for on-the-fly retrieval - where\nclassifiers are learnt at run time for a textual query from downloaded images,\nand used to rank large image or video datasets.\n  We make three contributions: (i) we present an evaluation of state-of-the-art\nimage representations for object category retrieval over standard benchmark\ndatasets containing 1M+ images; (ii) we show that ConvNets can be used to\nobtain features which are incredibly performant, and yet much lower dimensional\nthan previous state-of-the-art image representations, and that their\ndimensionality can be reduced further without loss in performance by\ncompression using product quantization or binarization. Consequently, features\nwith the state-of-the-art performance on large-scale datasets of millions of\nimages can fit in the memory of even a commodity GPU card; (iii) we show that\nan SVM classifier can be learnt within a ConvNet framework on a GPU in parallel\nwith downloading the new training images, allowing for a continuous refinement\nof the model as more images become available, and simultaneous training and\nranking. The outcome is an on-the-fly system that significantly outperforms its\npredecessors in terms of: precision of retrieval, memory requirements, and\nspeed, facilitating accurate on-the-fly learning and ranking in under a second\non a single GPU.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 18:29:38 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 08:27:23 GMT"}, {"version": "v3", "created": "Mon, 17 Nov 2014 12:10:23 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Chatfield", "Ken", ""], ["Simonyan", "Karen", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1407.4832", "submitter": "Ernesto Diaz-Aviles", "authors": "Bernat Coma-Puig and Ernesto Diaz-Aviles and Wolfgang Nejdl", "title": "Collaborative Filtering Ensemble for Personalized Name Recommendation", "comments": "Top-N recommendation; personalized ranking; given name recommendation", "journal-ref": "Proceedings of the ECML PKDD Discovery Challenge - Recommending\n  Given Names. Co-located with ECML PKDD 2013. Prague, Czech Republic,\n  September 27, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out of thousands of names to choose from, picking the right one for your\nchild is a daunting task. In this work, our objective is to help parents making\nan informed decision while choosing a name for their baby. We follow a\nrecommender system approach and combine, in an ensemble, the individual\nrankings produced by simple collaborative filtering algorithms in order to\nproduce a personalized list of names that meets the individual parents' taste.\nOur experiments were conducted using real-world data collected from the query\nlogs of 'nameling' (nameling.net), an online portal for searching and exploring\nnames, which corresponds to the dataset released in the context of the ECML\nPKDD Discover Challenge 2013. Our approach is intuitive, easy to implement, and\nfeatures fast training and prediction steps.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 12:07:36 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Coma-Puig", "Bernat", ""], ["Diaz-Aviles", "Ernesto", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1407.4979", "submitter": "Dong Yi", "authors": "Dong Yi and Zhen Lei and Stan Z. Li", "title": "Deep Metric Learning for Practical Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various hand-crafted features and metric learning methods prevail in the\nfield of person re-identification. Compared to these methods, this paper\nproposes a more general way that can learn a similarity metric from image\npixels directly. By using a \"siamese\" deep neural network, the proposed method\ncan jointly learn the color feature, texture feature and metric in a unified\nframework. The network has a symmetry structure with two sub-networks which are\nconnected by Cosine function. To deal with the big variations of person images,\nbinomial deviance is used to evaluate the cost between similarities and labels,\nwhich is proved to be robust to outliers.\n  Compared to existing researches, a more practical setting is studied in the\nexperiments that is training and test on different datasets (cross dataset\nperson re-identification). Both in \"intra dataset\" and \"cross dataset\"\nsettings, the superiorities of the proposed method are illustrated on VIPeR and\nPRID.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 13:07:16 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Yi", "Dong", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1407.5093", "submitter": "Michael Horton", "authors": "Michael Horton, Joachim Gudmundsson, Sanjay Chawla, Jo\\\"el Estephan", "title": "Classification of Passes in Football Matches using Spatiotemporal Data", "comments": "10 pages", "journal-ref": null, "doi": "10.1145/3105576", "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A knowledgeable observer of a game of football (soccer) can make a subjective\nevaluation of the quality of passes made between players during the game. We\ninvestigate the problem of producing an automated system to make the same\nevaluation of passes. We present a model that constructs numerical predictor\nvariables from spatiotemporal match data using feature functions based on\nmethods from computational geometry, and then learns a classification function\nfrom labelled examples of the predictor variables. Furthermore, the learned\nclassifiers are analysed to determine if there is a relationship between the\ncomplexity of the algorithm that computed the predictor variable and the\nimportance of the variable to the classifier. Experimental results show that we\nare able to produce a classifier with 85.8% accuracy on classifying passes as\nGood, OK or Bad, and that the predictor variables computed using complex\nmethods from computational geometry are of moderate importance to the learned\nclassifiers. Finally, we show that the inter-rater agreement on pass\nclassification between the machine classifier and a human observer is of\nsimilar magnitude to the agreement between two observers.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 06:42:35 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Horton", "Michael", ""], ["Gudmundsson", "Joachim", ""], ["Chawla", "Sanjay", ""], ["Estephan", "Jo\u00ebl", ""]]}, {"id": "1407.5155", "submitter": "Remi Gribonval", "authors": "R\\'emi Gribonval (PANAMA), Rodolphe Jenatton (CMAP), Francis Bach\n  (SIERRA, LIENS)", "title": "Sparse and spurious: dictionary learning with noise and outliers", "comments": "This is a substantially revised version of a first draft that\n  appeared as a preprint titled \"Local stability and robustness of sparse\n  dictionary learning in the presence of noise\",\n  http://hal.inria.fr/hal-00737152, IEEE Transactions on Information Theory,\n  Institute of Electrical and Electronics Engineers (IEEE), 2015, pp.22", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach within the signal processing and machine learning\ncommunities consists in modelling signals as sparse linear combinations of\natoms selected from a learned dictionary. While this paradigm has led to\nnumerous empirical successes in various fields ranging from image to audio\nprocessing, there have only been a few theoretical arguments supporting these\nevidences. In particular, sparse coding, or sparse dictionary learning, relies\non a non-convex procedure whose local minima have not been fully analyzed yet.\nIn this paper, we consider a probabilistic model of sparse signals, and show\nthat, with high probability, sparse coding admits a local minimum around the\nreference dictionary generating the signals. Our study takes into account the\ncase of over-complete dictionaries, noisy signals, and possible outliers, thus\nextending previous work limited to noiseless settings and/or under-complete\ndictionaries. The analysis we conduct is non-asymptotic and makes it possible\nto understand how the key quantities of the problem, such as the coherence or\nthe level of noise, can scale with respect to the dimension of the signals, the\nnumber of atoms, the sparsity and the number of observations.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jul 2014 06:50:19 GMT"}, {"version": "v2", "created": "Wed, 6 Aug 2014 15:39:15 GMT"}, {"version": "v3", "created": "Thu, 4 Sep 2014 18:39:21 GMT"}, {"version": "v4", "created": "Sat, 22 Aug 2015 12:46:49 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Gribonval", "R\u00e9mi", "", "PANAMA"], ["Jenatton", "Rodolphe", "", "CMAP"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1407.5158", "submitter": "Jean-Philippe Vert", "authors": "Emile Richard, Guillaume Obozinski (LIGM), Jean-Philippe Vert (CBIO)", "title": "Tight convex relaxations for sparse matrix factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a new atomic norm, we propose a new convex formulation for sparse\nmatrix factorization problems in which the number of nonzero elements of the\nfactors is assumed fixed and known. The formulation counts sparse PCA with\nmultiple factors, subspace clustering and low-rank sparse bilinear regression\nas potential applications. We compute slow rates and an upper bound on the\nstatistical dimension of the suggested norm for rank 1 matrices, showing that\nits statistical dimension is an order of magnitude smaller than the usual\n$\\ell\\_1$-norm, trace norm and their combinations. Even though our convex\nformulation is in theory hard and does not lead to provably polynomial time\nalgorithmic schemes, we propose an active set algorithm leveraging the\nstructure of the convex problem to solve it and show promising numerical\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jul 2014 07:04:08 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 11:19:07 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Richard", "Emile", "", "LIGM"], ["Obozinski", "Guillaume", "", "LIGM"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1407.5245", "submitter": "Liantao Wang", "authors": "Ji Zhao, Liantao Wang, Ricardo Cabral, Fernando De la Torre", "title": "Feature and Region Selection for Visual Learning", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2016, vol. 25, pp.\n  1084-1094", "doi": "10.1109/TIP.2016.2514503", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual learning problems such as object classification and action recognition\nare typically approached using extensions of the popular bag-of-words (BoW)\nmodel. Despite its great success, it is unclear what visual features the BoW\nmodel is learning: Which regions in the image or video are used to discriminate\namong classes? Which are the most discriminative visual words? Answering these\nquestions is fundamental for understanding existing BoW models and inspiring\nbetter models for visual recognition.\n  To answer these questions, this paper presents a method for feature selection\nand region selection in the visual BoW model. This allows for an intermediate\nvisualization of the features and regions that are important for visual\nlearning. The main idea is to assign latent weights to the features or regions,\nand jointly optimize these latent variables with the parameters of a classifier\n(e.g., support vector machine). There are four main benefits of our approach:\n(1) Our approach accommodates non-linear additive kernels such as the popular\n$\\chi^2$ and intersection kernel; (2) our approach is able to handle both\nregions in images and spatio-temporal regions in videos in a unified way; (3)\nthe feature selection problem is convex, and both problems can be solved using\na scalable reduced gradient method; (4) we point out strong connections with\nmultiple kernel learning and multiple instance learning approaches.\nExperimental results in the PASCAL VOC 2007, MSR Action Dataset II and YouTube\nillustrate the benefits of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 04:42:50 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 03:27:59 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Zhao", "Ji", ""], ["Wang", "Liantao", ""], ["Cabral", "Ricardo", ""], ["De la Torre", "Fernando", ""]]}, {"id": "1407.5358", "submitter": "Andr\\'e Barreto", "authors": "Andr\\'e M. S. Barreto, Doina Precup, and Joelle Pineau", "title": "Practical Kernel-Based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based reinforcement learning (KBRL) stands out among reinforcement\nlearning algorithms for its strong theoretical guarantees. By casting the\nlearning problem as a local kernel approximation, KBRL provides a way of\ncomputing a decision policy which is statistically consistent and converges to\na unique solution. Unfortunately, the model constructed by KBRL grows with the\nnumber of sample transitions, resulting in a computational cost that precludes\nits application to large-scale or on-line domains. In this paper we introduce\nan algorithm that turns KBRL into a practical reinforcement learning tool.\nKernel-based stochastic factorization (KBSF) builds on a simple idea: when a\ntransition matrix is represented as the product of two stochastic matrices, one\ncan swap the factors of the multiplication to obtain another transition matrix,\npotentially much smaller, which retains some fundamental properties of its\nprecursor. KBSF exploits such an insight to compress the information contained\nin KBRL's model into an approximator of fixed size. This makes it possible to\nbuild an approximation that takes into account both the difficulty of the\nproblem and the associated computational cost. KBSF's computational complexity\nis linear in the number of sample transitions, which is the best one can do\nwithout discarding data. Moreover, the algorithm's simple mechanics allow for a\nfully incremental implementation that makes the amount of memory used\nindependent of the number of sample transitions. The result is a kernel-based\nreinforcement learning algorithm that can be applied to large-scale problems in\nboth off-line and on-line regimes. We derive upper bounds for the distance\nbetween the value functions computed by KBRL and KBSF using the same data. We\nalso illustrate the potential of our algorithm in an extensive empirical study\nin which KBSF is applied to difficult tasks based on real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 01:20:45 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Barreto", "Andr\u00e9 M. S.", ""], ["Precup", "Doina", ""], ["Pineau", "Joelle", ""]]}, {"id": "1407.5397", "submitter": "EPTCS", "authors": "Susmit Jha (Strategic CAD Labs, Intel), Sanjit A. Seshia (EECS, UC\n  Berkeley)", "title": "Are There Good Mistakes? A Theoretical Analysis of CEGIS", "comments": "In Proceedings SYNT 2014, arXiv:1407.4937", "journal-ref": "EPTCS 157, 2014, pp. 84-99", "doi": "10.4204/EPTCS.157.10", "report-no": null, "categories": "cs.LO cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterexample-guided inductive synthesis CEGIS is used to synthesize\nprograms from a candidate space of programs. The technique is guaranteed to\nterminate and synthesize the correct program if the space of candidate programs\nis finite. But the technique may or may not terminate with the correct program\nif the candidate space of programs is infinite. In this paper, we perform a\ntheoretical analysis of counterexample-guided inductive synthesis technique. We\ninvestigate whether the set of candidate spaces for which the correct program\ncan be synthesized using CEGIS depends on the counterexamples used in inductive\nsynthesis, that is, whether there are good mistakes which would increase the\nsynthesis power. We investigate whether the use of minimal counterexamples\ninstead of arbitrary counterexamples expands the set of candidate spaces of\nprograms for which inductive synthesis can successfully synthesize a correct\nprogram. We consider two kinds of counterexamples: minimal counterexamples and\nhistory bounded counterexamples. The history bounded counterexample used in any\niteration of CEGIS is bounded by the examples used in previous iterations of\ninductive synthesis. We examine the relative change in power of inductive\nsynthesis in both cases. We show that the synthesis technique using minimal\ncounterexamples MinCEGIS has the same synthesis power as CEGIS but the\nsynthesis technique using history bounded counterexamples HCEGIS has different\npower than that of CEGIS, but none dominates the other.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 07:28:49 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Jha", "Susmit", "", "Strategic CAD Labs, Intel"], ["Seshia", "Sanjit A.", "", "EECS, UC\n  Berkeley"]]}, {"id": "1407.5599", "submitter": "Bo Dai", "authors": "Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina\n  Balcan, Le Song", "title": "Scalable Kernel Methods via Doubly Stochastic Gradients", "comments": "32 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general perception is that kernel methods are not scalable, and neural\nnets are the methods of choice for nonlinear learning problems. Or have we\nsimply not tried hard enough for kernel methods? Here we propose an approach\nthat scales up kernel methods using a novel concept called \"doubly stochastic\nfunctional gradients\". Our approach relies on the fact that many kernel methods\ncan be expressed as convex optimization problems, and we solve the problems by\nmaking two unbiased stochastic approximations to the functional gradient, one\nusing random training points and another using random functions associated with\nthe kernel, and then descending using this noisy functional gradient. We show\nthat a function produced by this procedure after $t$ iterations converges to\nthe optimal function in the reproducing kernel Hilbert space in rate $O(1/t)$,\nand achieves a generalization performance of $O(1/\\sqrt{t})$. This doubly\nstochasticity also allows us to avoid keeping the support vectors and to\nimplement the algorithm in a small memory footprint, which is linear in number\nof iterations and independent of data dimension. Our approach can readily scale\nkernel methods up to the regimes which are dominated by neural nets. We show\nthat our method can achieve competitive performance to neural nets in datasets\nsuch as 8 million handwritten digits from MNIST, 2.3 million energy materials\nfrom MolecularSpace, and 1 million photos from ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 19:05:47 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 17:58:57 GMT"}, {"version": "v3", "created": "Tue, 23 Sep 2014 15:39:03 GMT"}, {"version": "v4", "created": "Thu, 10 Sep 2015 16:40:45 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Dai", "Bo", ""], ["Xie", "Bo", ""], ["He", "Niao", ""], ["Liang", "Yingyu", ""], ["Raj", "Anant", ""], ["Balcan", "Maria-Florina", ""], ["Song", "Le", ""]]}, {"id": "1407.5656", "submitter": "Khalifeh AlJadda", "authors": "Khalifeh AlJadda, Mohammed Korayem, Camilo Ortiz, Trey Grainger, John\n  A. Miller, William S. York", "title": "PGMHD: A Scalable Probabilistic Graphical Model for Massive Hierarchical\n  Data Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the big data era, scalability has become a crucial requirement for any\nuseful computational model. Probabilistic graphical models are very useful for\nmining and discovering data insights, but they are not scalable enough to be\nsuitable for big data problems. Bayesian Networks particularly demonstrate this\nlimitation when their data is represented using few random variables while each\nrandom variable has a massive set of values. With hierarchical data - data that\nis arranged in a treelike structure with several levels - one would expect to\nsee hundreds of thousands or millions of values distributed over even just a\nsmall number of levels. When modeling this kind of hierarchical data across\nlarge data sets, Bayesian networks become infeasible for representing the\nprobability distributions for the following reasons: i) Each level represents a\nsingle random variable with hundreds of thousands of values, ii) The number of\nlevels is usually small, so there are also few random variables, and iii) The\nstructure of the network is predefined since the dependency is modeled top-down\nfrom each parent to each of its child nodes, so the network would contain a\nsingle linear path for the random variables from each parent to each child\nnode. In this paper we present a scalable probabilistic graphical model to\novercome these limitations for massive hierarchical data. We believe the\nproposed model will lead to an easily-scalable, more readable, and expressive\nimplementation for problems that require probabilistic-based solutions for\nmassive amounts of hierarchical data. We successfully applied this model to\nsolve two different challenging probabilistic-based problems on massive\nhierarchical data sets for different domains, namely, bioinformatics and latent\nsemantic discovery over search logs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 20:26:32 GMT"}, {"version": "v2", "created": "Tue, 19 Aug 2014 21:06:27 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["AlJadda", "Khalifeh", ""], ["Korayem", "Mohammed", ""], ["Ortiz", "Camilo", ""], ["Grainger", "Trey", ""], ["Miller", "John A.", ""], ["York", "William S.", ""]]}, {"id": "1407.5908", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi", "title": "Exploiting Smoothness in Statistical Learning, Sequential Prediction,\n  and Stochastic Optimization", "comments": "Ph.D. Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last several years, the intimate connection between convex\noptimization and learning problems, in both statistical and sequential\nframeworks, has shifted the focus of algorithmic machine learning to examine\nthis interplay. In particular, on one hand, this intertwinement brings forward\nnew challenges in reassessment of the performance of learning algorithms\nincluding generalization and regret bounds under the assumptions imposed by\nconvexity such as analytical properties of loss functions (e.g., Lipschitzness,\nstrong convexity, and smoothness). On the other hand, emergence of datasets of\nan unprecedented size, demands the development of novel and more efficient\noptimization algorithms to tackle large-scale learning problems.\n  The overarching goal of this thesis is to reassess the smoothness of loss\nfunctions in statistical learning, sequential prediction/online learning, and\nstochastic optimization and explicate its consequences. In particular we\nexamine how smoothness of loss function could be beneficial or detrimental in\nthese settings in terms of sample complexity, statistical consistency, regret\nanalysis, and convergence rate, and investigate how smoothness can be leveraged\nto devise more efficient learning algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jul 2014 15:16:40 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Mahdavi", "Mehrdad", ""]]}, {"id": "1407.5978", "submitter": "Yao Xie", "authors": "David Marangoni-Simonsen and Yao Xie", "title": "Sequential Changepoint Approach for Online Community Detection", "comments": "Submitted to 2014 INFORMS Workshop on Data Mining and Analytics and\n  an IEEE journal", "journal-ref": null, "doi": "10.1109/LSP.2014.2381553", "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for detecting the emergence of a community in large\nnetworks from sequential observations. The networks are modeled using\nErdos-Renyi random graphs with edges forming between nodes in the community\nwith higher probability. Based on statistical changepoint detection\nmethodology, we develop three algorithms: the Exhaustive Search (ES), the\nmixture, and the Hierarchical Mixture (H-Mix) methods. Performance of these\nmethods is evaluated by the average run length (ARL), which captures the\nfrequency of false alarms, and the detection delay. Numerical comparisons show\nthat the ES method performs the best; however, it is exponentially complex. The\nmixture method is polynomially complex by exploiting the fact that the size of\nthe community is typically small in a large network. However, it may react to a\ngroup of active edges that do not form a community. This issue is resolved by\nthe H-Mix method, which is based on a dendrogram decomposition of the network.\nWe present an asymptotic analytical expression for ARL of the mixture method\nwhen the threshold is large. Numerical simulation verifies that our\napproximation is accurate even in the non-asymptotic regime. Hence, it can be\nused to determine a desired threshold efficiently. Finally, numerical examples\nshow that the mixture and the H-Mix methods can both detect a community quickly\nwith a lower complexity than the ES method.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 19:16:01 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 19:54:17 GMT"}, {"version": "v3", "created": "Thu, 24 Jul 2014 06:27:05 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Marangoni-Simonsen", "David", ""], ["Xie", "Yao", ""]]}, {"id": "1407.6067", "submitter": "Marcelo S. Reis", "authors": "Marcelo S. Reis, Carlos E. Ferreira, and Junior Barrera", "title": "The U-curve optimization problem: improvements on the original algorithm\n  and time complexity analysis", "comments": "Original results from the Ph.D. thesis of Marcelo S. Reis. This\n  thesis can be accessed through the following link:\n  http://www.teses.usp.br/teses/disponiveis/45/45134/tde-05022013-123757/en.php", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The U-curve optimization problem is characterized by a decomposable in\nU-shaped curves cost function over the chains of a Boolean lattice. This\nproblem can be applied to model the classical feature selection problem in\nMachine Learning. Recently, the U-Curve algorithm was proposed to give optimal\nsolutions to the U-curve problem. In this article, we point out that the\nU-Curve algorithm is in fact suboptimal, and introduce the U-Curve-Search (UCS)\nalgorithm, which is actually optimal. We also present the results of optimal\nand suboptimal experiments, in which UCS is compared with the UBB optimal\nbranch-and-bound algorithm and the SFFS heuristic, respectively. We show that,\nin both experiments, $\\proc{UCS}$ had a better performance than its competitor.\nFinally, we analyze the obtained results and point out improvements on UCS that\nmight enhance the performance of this algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 23:18:08 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Reis", "Marcelo S.", ""], ["Ferreira", "Carlos E.", ""], ["Barrera", "Junior", ""]]}, {"id": "1407.6089", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Learning Rank Functionals: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking is a key aspect of many applications, such as information retrieval,\nquestion answering, ad placement and recommender systems. Learning to rank has\nthe goal of estimating a ranking model automatically from training data. In\npractical settings, the task often reduces to estimating a rank functional of\nan object with respect to a query. In this paper, we investigate key issues in\ndesigning an effective learning to rank algorithm. These include data\nrepresentation, the choice of rank functionals, the design of the loss function\nso that it is correlated with the rank metrics used in evaluation. For the loss\nfunction, we study three techniques: approximating the rank metric by a smooth\nfunction, decomposition of the loss into a weighted sum of element-wise losses\nand into a weighted sum of pairwise losses. We then present derivations of\npiecewise losses using the theory of high-order Markov chains and Markov random\nfields. In experiments, we evaluate these design aspects on two tasks: answer\nranking in a Social Question Answering site, and Web Information Retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 01:54:31 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 23:50:43 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6094", "submitter": "Shivapratap Gopakumar", "authors": "Shivapratap Gopakumar, Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Stabilizing Sparse Cox Model using Clinical Structures in Electronic\n  Medical Records", "comments": "Submitted to International Workshop on Pattern Recognition for\n  Healthcare Analytics 2014, Sweden. Contains 4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability in clinical prediction models is crucial for transferability\nbetween studies, yet has received little attention. The problem is paramount in\nhigh dimensional data which invites sparse models with feature selection\ncapability. We introduce an effective method to stabilize sparse Cox model of\ntime-to-events using clinical structures inherent in Electronic Medical\nRecords. Model estimation is stabilized using a feature graph derived from two\ntypes of EMR structures: temporal structure of disease and intervention\nrecurrences, and hierarchical structure of medical knowledge and practices. We\ndemonstrate the efficacy of the method in predicting time-to-readmission of\nheart failure patients. On two stability measures - the Jaccard index and the\nConsistency index - the use of clinical structures significantly increased\nfeature stability without hurting discriminative power. Our model reported a\ncompetitive AUC of 0.64 (95% CIs: [0.58,0.69]) for 6 months prediction.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 02:47:47 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Gopakumar", "Shivapratap", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6128", "submitter": "Truyen Tran", "authors": "Truyen Tran and Svetha Venkatesh", "title": "Permutation Models for Collaborative Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of collaborative filtering where ranking information is\navailable. Focusing on the core of the collaborative ranking process, the user\nand their community, we propose new models for representation of the underlying\npermutations and prediction of ranks. The first approach is based on the\nassumption that the user makes successive choice of items in a stage-wise\nmanner. In particular, we extend the Plackett-Luce model in two ways -\nintroducing parameter factoring to account for user-specific contribution, and\nmodelling the latent community in a generative setting. The second approach\nrelies on log-linear parameterisation, which relaxes the discrete-choice\nassumption, but makes learning and inference much more involved. We propose\nMCMC-based learning and inference methods and derive linear-time prediction\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 08:20:09 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6154", "submitter": "Pol  Blasco Moreno", "authors": "Pol Blasco and Deniz G\\\"und\\\"uz", "title": "Content-Level Selective Offloading in Heterogeneous Networks:\n  Multi-armed Bandit Optimization and Regret Bounds", "comments": "submitted for publication. arXiv admin note: text overlap with\n  arXiv:1402.3247", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider content-level selective offloading of cellular downlink traffic\nto a wireless infostation terminal which stores high data-rate content in its\ncache memory. Cellular users in the vicinity of the infostation can directly\ndownload the stored content from the infostation through a broadband connection\n(e.g., WiFi), reducing the latency and load on the cellular network. The goal\nof the infostation cache controller (CC) is to store the most popular content\nin the cache memory such that the maximum amount of traffic is offloaded to the\ninfostation. In practice, the popularity profile of the files is not known by\nthe CC, which observes only the instantaneous demands for those contents stored\nin the cache. Hence, the cache content placement is optimised based on the\ndemand history and on the cost associated to placing each content in the cache.\nBy refreshing the cache content at regular time intervals, the CC gradually\nlearns the popularity profile, while at the same time exploiting the limited\ncache capacity in the best way possible. This is formulated as a multi-armed\nbandit (MAB) problem with switching cost. Several algorithms are presented to\ndecide on the cache content over time. The performance is measured in terms of\ncache efficiency, defined as the amount of net traffic that is offloaded to the\ninfostation. In addition to theoretical regret bounds, the proposed algorithms\nare analysed through numerical simulations. In particular, the impact of system\nparameters, such as the number of files, number of users, cache size, and\nskewness of the popularity profile, on the performance is studied numerically.\nIt is shown that the proposed algorithms learn the popularity profile quickly\nfor a wide range of system parameters.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 10:01:17 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Blasco", "Pol", ""], ["G\u00fcnd\u00fcz", "Deniz", ""]]}, {"id": "1407.6267", "submitter": "Panayotis Mertikopoulos", "authors": "Panayotis Mertikopoulos and William H. Sandholm", "title": "Learning in games via reinforcement and regularization", "comments": "39 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a class of reinforcement learning dynamics where players\nadjust their strategies based on their actions' cumulative payoffs over time -\nspecifically, by playing mixed strategies that maximize their expected\ncumulative payoff minus a regularization term. A widely studied example is\nexponential reinforcement learning, a process induced by an entropic\nregularization term which leads mixed strategies to evolve according to the\nreplicator dynamics. However, in contrast to the class of regularization\nfunctions used to define smooth best responses in models of stochastic\nfictitious play, the functions used in this paper need not be infinitely steep\nat the boundary of the simplex; in fact, dropping this requirement gives rise\nto an important dichotomy between steep and nonsteep cases. In this general\nframework, we extend several properties of exponential learning, including the\nelimination of dominated strategies, the asymptotic stability of strict Nash\nequilibria, and the convergence of time-averaged trajectories in zero-sum games\nwith an interior Nash equilibrium.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 15:37:38 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 23:29:36 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Mertikopoulos", "Panayotis", ""], ["Sandholm", "William H.", ""]]}, {"id": "1407.6315", "submitter": "Deepak Kumar", "authors": "Deepak Kumar, A G Ramakrishnan", "title": "Quadratically constrained quadratic programming for classification using\n  particle swarms and applications", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle swarm optimization is used in several combinatorial optimization\nproblems. In this work, particle swarms are used to solve quadratic programming\nproblems with quadratic constraints. The approach of particle swarms is an\nexample for interior point methods in optimization as an iterative technique.\nThis approach is novel and deals with classification problems without the use\nof a traditional classifier. Our method determines the optimal hyperplane or\nclassification boundary for a data set. In a binary classification problem, we\nconstrain each class as a cluster, which is enclosed by an ellipsoid. The\nestimation of the optimal hyperplane between the two clusters is posed as a\nquadratically constrained quadratic problem. The optimization problem is solved\nin distributed format using modified particle swarms. Our method has the\nadvantage of using the direction towards optimal solution rather than searching\nthe entire feasible region. Our results on the Iris, Pima, Wine, and Thyroid\ndatasets show that the proposed method works better than a neural network and\nthe performance is close to that of SVM.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 18:04:23 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Kumar", "Deepak", ""], ["Ramakrishnan", "A G", ""]]}, {"id": "1407.6432", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Learning Structured Outputs from Partial Labels using Forest Ensemble", "comments": "Conference version appeared in Truyen et al, AdaBoost.MRF: Boosted\n  Markov random forests and application to multilevel activity recognition.\n  CVPR'06", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning structured outputs with general structures is computationally\nchallenging, except for tree-structured models. Thus we propose an efficient\nboosting-based algorithm AdaBoost.MRF for this task. The idea is based on the\nrealization that a graph is a superimposition of trees. Different from most\nexisting work, our algorithm can handle partial labelling, and thus is\nparticularly attractive in practice where reliable labels are often sparsely\nobserved. In addition, our method works exclusively on trees and thus is\nguaranteed to converge. We apply the AdaBoost.MRF algorithm to an indoor video\nsurveillance scenario, where activities are modelled at multiple levels.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 02:53:52 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6439", "submitter": "Ce Zhang", "authors": "Christopher R\\'e, Amir Abbas Sadeghian, Zifei Shan, Jaeho Shin, Feiran\n  Wang, Sen Wu, Ce Zhang", "title": "Feature Engineering for Knowledge Base Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge base construction (KBC) is the process of populating a knowledge\nbase, i.e., a relational database together with inference rules, with\ninformation extracted from documents and structured sources. KBC blurs the\ndistinction between two traditional database problems, information extraction\nand information integration. For the last several years, our group has been\nbuilding knowledge bases with scientific collaborators. Using our approach, we\nhave built knowledge bases that have comparable and sometimes better quality\nthan those constructed by human volunteers. In contrast to these knowledge\nbases, which took experts a decade or more human years to construct, many of\nour projects are constructed by a single graduate student.\n  Our approach to KBC is based on joint probabilistic inference and learning,\nbut we do not see inference as either a panacea or a magic bullet: inference is\na tool that allows us to be systematic in how we construct, debug, and improve\nthe quality of such systems. In addition, inference allows us to construct\nthese systems in a more loosely coupled way than traditional approaches. To\nsupport this idea, we have built the DeepDive system, which has the design goal\nof letting the user \"think about features---not algorithms.\" We think of\nDeepDive as declarative in that one specifies what they want but not how to get\nit. We describe our approach with a focus on feature engineering, which we\nargue is an understudied problem relative to its importance to end-to-end\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 03:34:41 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 17:00:00 GMT"}, {"version": "v3", "created": "Thu, 18 Sep 2014 14:38:06 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["R\u00e9", "Christopher", ""], ["Sadeghian", "Amir Abbas", ""], ["Shan", "Zifei", ""], ["Shin", "Jaeho", ""], ["Wang", "Feiran", ""], ["Wu", "Sen", ""], ["Zhang", "Ce", ""]]}, {"id": "1407.6810", "submitter": "Ehsan Elhamifar", "authors": "Ehsan Elhamifar, Guillermo Sapiro and S. Shankar Sastry", "title": "Dissimilarity-based Sparse Subset Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding an informative subset of a large collection of data points or models\nis at the center of many problems in computer vision, recommender systems,\nbio/health informatics as well as image and natural language processing. Given\npairwise dissimilarities between the elements of a `source set' and a `target\nset,' we consider the problem of finding a subset of the source set, called\nrepresentatives or exemplars, that can efficiently describe the target set. We\nformulate the problem as a row-sparsity regularized trace minimization problem.\nSince the proposed formulation is, in general, NP-hard, we consider a convex\nrelaxation. The solution of our optimization finds representatives and the\nassignment of each element of the target set to each representative, hence,\nobtaining a clustering. We analyze the solution of our proposed optimization as\na function of the regularization parameter. We show that when the two sets\njointly partition into multiple groups, our algorithm finds representatives\nfrom all groups and reveals clustering of the sets. In addition, we show that\nthe proposed framework can effectively deal with outliers. Our algorithm works\nwith arbitrary dissimilarities, which can be asymmetric or violate the triangle\ninequality. To efficiently implement our algorithm, we consider an Alternating\nDirection Method of Multipliers (ADMM) framework, which results in quadratic\ncomplexity in the problem size. We show that the ADMM implementation allows to\nparallelize the algorithm, hence further reducing the computational time.\nFinally, by experiments on real-world datasets, we show that our proposed\nalgorithm improves the state of the art on the two problems of scene\ncategorization using representative images and time-series modeling and\nsegmentation using representative~models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 08:30:04 GMT"}, {"version": "v2", "created": "Sat, 9 Apr 2016 03:09:18 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Elhamifar", "Ehsan", ""], ["Sapiro", "Guillermo", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1407.6872", "submitter": "Ivan Ivek", "authors": "Ivan Ivek", "title": "Interpretable Low-Rank Document Representations with Label-Dependent\n  Sparsity Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In context of document classification, where in a corpus of documents their\nlabel tags are readily known, an opportunity lies in utilizing label\ninformation to learn document representation spaces with better discriminative\nproperties. To this end, in this paper application of a Variational Bayesian\nSupervised Nonnegative Matrix Factorization (supervised vbNMF) with\nlabel-driven sparsity structure of coefficients is proposed for learning of\ndiscriminative nonsubtractive latent semantic components occuring in TF-IDF\ndocument representations. Constraints are such that the components pursued are\nmade to be frequently occuring in a small set of labels only, making it\npossible to yield document representations with distinctive label-specific\nsparse activation patterns. A simple measure of quality of this kind of\nsparsity structure, dubbed inter-label sparsity, is introduced and\nexperimentally brought into tight connection with classification performance.\nRepresenting a great practical convenience, inter-label sparsity is shown to be\neasily controlled in supervised vbNMF by a single parameter.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 12:46:18 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Ivek", "Ivan", ""]]}, {"id": "1407.7131", "submitter": "Tanmay Sinha", "authors": "Tanmay Sinha, Patrick Jermann, Nan Li, Pierre Dillenbourg", "title": "Your click decides your fate: Inferring Information Processing and\n  Attrition Behavior from MOOC Video Clickstream Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore video lecture interaction in Massive Open Online\nCourses (MOOCs), which is central to student learning experience on these\neducational platforms. As a research contribution, we operationalize video\nlecture clickstreams of students into cognitively plausible higher level\nbehaviors, and construct a quantitative information processing index, which can\naid instructors to better understand MOOC hurdles and reason about\nunsatisfactory learning outcomes. Our results illustrate how such a metric\ninspired by cognitive psychology can help answer critical questions regarding\nstudents' engagement, their future click interactions and participation\ntrajectories that lead to in-video & course dropouts. Implications for research\nand practice are discussed\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 14:18:00 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 23:41:58 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Sinha", "Tanmay", ""], ["Jermann", "Patrick", ""], ["Li", "Nan", ""], ["Dillenbourg", "Pierre", ""]]}, {"id": "1407.7159", "submitter": "James P. Crutchfield", "authors": "P. M. Riechers and D. P. Varn and J. P. Crutchfield", "title": "Pairwise Correlations in Layered Close-Packed Structures", "comments": "21 pages, 21 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/cfem_prb.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a description of the stacking statistics of layered close-packed\nstructures in the form of a hidden Markov model, we develop analytical\nexpressions for the pairwise correlation functions between the layers. These\nmay be calculated analytically as explicit functions of model parameters or the\nexpressions may be used as a fast, accurate, and efficient way to obtain\nnumerical values. We present several examples, finding agreement with previous\nwork as well as deriving new relations.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 20:36:08 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Riechers", "P. M.", ""], ["Varn", "D. P.", ""], ["Crutchfield", "J. P.", ""]]}, {"id": "1407.7260", "submitter": "Tanmay Sinha", "authors": "Tanmay Sinha, Ankit Banka, Dae Ki Kang", "title": "Leveraging user profile attributes for improving pedagogical accuracy of\n  learning pathways", "comments": "3rd Annual International Conference on Education and E-Learning(EeL\n  2013), Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the enormous explosion of web based learning resources,\npersonalization has become a critical factor for the success of services that\nwish to leverage the power of Web 2.0. However, the relevance, significance and\nimpact of tailored content delivery in the learning domain is still\nquestionable. Apart from considering only interaction based features like\nratings and inferring learner preferences from them, if these services were to\nincorporate innate user profile attributes which affect learning activities,\nthe quality of recommendations produced could be vastly improved. Recognizing\nthe crucial role of effective guidance in informal educational settings, we\nprovide a principled way of utilizing multiple sources of information from the\nuser profile itself for the recommendation task. We explore factors that affect\nthe choice of learning resources and explain in what way are they helpful to\nimprove the pedagogical accuracy of learning objects recommended. Through a\nsystematical application of machine learning techniques, we further provide a\ntechnological solution to convert these indirectly mapped learner specific\nattributes into a direct mapping with the learning resources. This mapping has\na distinct advantage of tagging learning resources to make their metadata more\ninformative. The results of our empirical study depict the similarity of\nnominal learning attributes with respect to each other. We further succeed in\ncapturing the learner subset, whose preferences are most likely to be an\nindication of learning resource usage. Our novel system filters learner profile\nattributes to discover a tag that links them with learning resources.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 17:24:14 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Sinha", "Tanmay", ""], ["Banka", "Ankit", ""], ["Kang", "Dae Ki", ""]]}, {"id": "1407.7294", "submitter": "Lili Dworkin", "authors": "Kareem Amin, Rachel Cummings, Lili Dworkin, Michael Kearns, Aaron Roth", "title": "Online Learning and Profit Maximization from Revealed Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from revealed preferences in an online\nsetting. In our framework, each period a consumer buys an optimal bundle of\ngoods from a merchant according to her (linear) utility function and current\nprices, subject to a budget constraint. The merchant observes only the\npurchased goods, and seeks to adapt prices to optimize his profits. We give an\nefficient algorithm for the merchant's problem that consists of a learning\nphase in which the consumer's utility function is (perhaps partially) inferred,\nfollowed by a price optimization step. We also consider an alternative online\nlearning algorithm for the setting where prices are set exogenously, but the\nmerchant would still like to predict the bundle that will be bought by the\nconsumer for purposes of inventory or supply chain management. In contrast with\nmost prior work on the revealed preferences problem, we demonstrate that by\nmaking stronger assumptions on the form of utility functions, efficient\nalgorithms for both learning and profit maximization are possible, even in\nadaptive, online settings.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 23:38:09 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 21:45:08 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Amin", "Kareem", ""], ["Cummings", "Rachel", ""], ["Dworkin", "Lili", ""], ["Kearns", "Michael", ""], ["Roth", "Aaron", ""]]}, {"id": "1407.7299", "submitter": "Carl Meyer Dr.", "authors": "Amy N. Langville, Carl D. Meyer, Russell Albright, James Cox, David\n  Duling", "title": "Algorithms, Initializations, and Convergence for the Nonnegative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that good initializations can improve the speed and accuracy\nof the solutions of many nonnegative matrix factorization (NMF) algorithms.\nMany NMF algorithms are sensitive with respect to the initialization of W or H\nor both. This is especially true of algorithms of the alternating least squares\n(ALS) type, including the two new ALS algorithms that we present in this paper.\nWe compare the results of six initialization procedures (two standard and four\nnew) on our ALS algorithms. Lastly, we discuss the practical issue of choosing\nan appropriate convergence criterion.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 00:41:12 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Langville", "Amy N.", ""], ["Meyer", "Carl D.", ""], ["Albright", "Russell", ""], ["Cox", "James", ""], ["Duling", "David", ""]]}, {"id": "1407.7417", "submitter": "Nabarun Mondal Mr", "authors": "Nabarun Mondal, Partha P. Ghosh", "title": "'Almost Sure' Chaotic Properties of Machine Learning Methods", "comments": "10 pages : to be submitted to Theoretical Computer Science. arXiv\n  admin note: text overlap with arXiv:1111.4949", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been demonstrated earlier that universal computation is 'almost\nsurely' chaotic. Machine learning is a form of computational fixed point\niteration, iterating over the computable function space. We showcase some\nproperties of this iteration, and establish in general that the iteration is\n'almost surely' of chaotic nature. This theory explains the observation in the\ncounter intuitive properties of deep learning methods. This paper demonstrates\nthat these properties are going to be universal to any learning method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 13:44:25 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Mondal", "Nabarun", ""], ["Ghosh", "Partha P.", ""]]}, {"id": "1407.7449", "submitter": "Xinquan Chen", "authors": "Xinquan Chen", "title": "A Fast Synchronization Clustering Algorithm", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Fast Synchronization Clustering algorithm (FSynC),\nwhich is an improved version of SynC algorithm. In order to decrease the time\ncomplexity of the original SynC algorithm, we combine grid cell partitioning\nmethod and Red-Black tree to construct the near neighbor point set of every\npoint. By simulated experiments of some artificial data sets and several real\ndata sets, we observe that FSynC algorithm can often get less time than SynC\nalgorithm for many kinds of data sets. At last, it gives some research\nexpectations to popularize this algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 09:14:49 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Chen", "Xinquan", ""]]}, {"id": "1407.7508", "submitter": "Zhenqiu  Liu", "authors": "Zhenqiu Liu and Gang Li", "title": "Efficient Regularized Regression for Variable Selection with L0 Penalty", "comments": "26 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable (feature, gene, model, which we use interchangeably) selections for\nregression with high-dimensional BIGDATA have found many applications in\nbioinformatics, computational biology, image processing, and engineering. One\nappealing approach is the L0 regularized regression which penalizes the number\nof nonzero features in the model directly. L0 is known as the most essential\nsparsity measure and has nice theoretical properties, while the popular L1\nregularization is only a best convex relaxation of L0. Therefore, it is natural\nto expect that L0 regularized regression performs better than LASSO. However,\nit is well-known that L0 optimization is NP-hard and computationally\nchallenging. Instead of solving the L0 problems directly, most publications so\nfar have tried to solve an approximation problem that closely resembles L0\nregularization.\n  In this paper, we propose an efficient EM algorithm (L0EM) that directly\nsolves the L0 optimization problem. $L_0$EM is efficient with high dimensional\ndata. It also provides a natural solution to all Lp p in [0,2] problems. The\nregularized parameter can be either determined through cross-validation or AIC\nand BIC. Theoretical properties of the L0-regularized estimator are given under\nmild conditions that permit the number of variables to be much larger than the\nsample size. We demonstrate our methods through simulation and high-dimensional\ngenomic data. The results indicate that L0 has better performance than LASSO\nand L0 with AIC or BIC has similar performance as computationally intensive\ncross-validation. The proposed algorithms are efficient in identifying the\nnon-zero variables with less-bias and selecting biologically important genes\nand pathways with high dimensional BIGDATA.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 19:28:26 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Liu", "Zhenqiu", ""], ["Li", "Gang", ""]]}, {"id": "1407.7556", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Alireza Sadeghian, Witold Pedrycz", "title": "Entropic one-class classifiers", "comments": "To appear in IEEE-TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2015.2418332", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-class classification problem is a well-known research endeavor in\npattern recognition. The problem is also known under different names, such as\noutlier and novelty/anomaly detection. The core of the problem consists in\nmodeling and recognizing patterns belonging only to a so-called target class.\nAll other patterns are termed non-target, and therefore they should be\nrecognized as such. In this paper, we propose a novel one-class classification\nsystem that is based on an interplay of different techniques. Primarily, we\nfollow a dissimilarity representation based approach; we embed the input data\ninto the dissimilarity space by means of an appropriate parametric\ndissimilarity measure. This step allows us to process virtually any type of\ndata. The dissimilarity vectors are then represented through a weighted\nEuclidean graphs, which we use to (i) determine the entropy of the data\ndistribution in the dissimilarity space, and at the same time (ii) derive\neffective decision regions that are modeled as clusters of vertices. Since the\ndissimilarity measure for the input data is parametric, we optimize its\nparameters by means of a global optimization scheme, which considers both\nmesoscopic and structural characteristics of the data represented through the\ngraphs. The proposed one-class classifier is designed to provide both hard\n(Boolean) and soft decisions about the recognition of test patterns, allowing\nan accurate description of the classification process. We evaluate the\nperformance of the system on different benchmarking datasets, containing either\nfeature-based or structured patterns. Experimental results demonstrate the\neffectiveness of the proposed technique.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 20:26:24 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 20:46:21 GMT"}, {"version": "v3", "created": "Sun, 11 Jan 2015 16:27:23 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Livi", "Lorenzo", ""], ["Sadeghian", "Alireza", ""], ["Pedrycz", "Witold", ""]]}, {"id": "1407.7559", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Alessandro Giuliani, Antonello Rizzi", "title": "Toward a multilevel representation of protein molecules: comparative\n  approaches to the aggregation/folding propensity problem", "comments": "17 pages, 3 figures, 46 references", "journal-ref": null, "doi": "10.1016/j.ins.2015.07.043", "report-no": null, "categories": "cs.CE cs.LG q-bio.BM q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper builds upon the fundamental work of Niwa et al. [34], which\nprovides the unique possibility to analyze the relative aggregation/folding\npropensity of the elements of the entire Escherichia coli (E. coli) proteome in\na cell-free standardized microenvironment. The hardness of the problem comes\nfrom the superposition between the driving forces of intra- and inter-molecule\ninteractions and it is mirrored by the evidences of shift from folding to\naggregation phenotypes by single-point mutations [10]. Here we apply several\nstate-of-the-art classification methods coming from the field of structural\npattern recognition, with the aim to compare different representations of the\nsame proteins gathered from the Niwa et al. data base; such representations\ninclude sequences and labeled (contact) graphs enriched with chemico-physical\nattributes. By this comparison, we are able to identify also some interesting\ngeneral properties of proteins. Notably, (i) we suggest a threshold around 250\nresidues discriminating \"easily foldable\" from \"hardly foldable\" molecules\nconsistent with other independent experiments, and (ii) we highlight the\nrelevance of contact graph spectra for folding behavior discrimination and\ncharacterization of the E. coli solubility data. The soundness of the\nexperimental results presented in this paper is proved by the statistically\nrelevant relationships discovered among the chemico-physical description of\nproteins and the developed cost matrix of substitution used in the various\ndiscrimination systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 20:29:52 GMT"}, {"version": "v2", "created": "Sun, 11 Jan 2015 15:33:51 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2015 00:06:14 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Livi", "Lorenzo", ""], ["Giuliani", "Alessandro", ""], ["Rizzi", "Antonello", ""]]}, {"id": "1407.7566", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Shyam Visweswaran", "title": "Dependence versus Conditional Dependence in Local Causal Discovery from\n  Gene Expression Data", "comments": "11 pages, 2 algorithms, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Algorithms that discover variables which are causally related to\na target may inform the design of experiments. With observational gene\nexpression data, many methods discover causal variables by measuring each\nvariable's degree of statistical dependence with the target using dependence\nmeasures (DMs). However, other methods measure each variable's ability to\nexplain the statistical dependence between the target and the remaining\nvariables in the data using conditional dependence measures (CDMs), since this\nstrategy is guaranteed to find the target's direct causes, direct effects, and\ndirect causes of the direct effects in the infinite sample limit. In this\npaper, we design a new algorithm in order to systematically compare the\nrelative abilities of DMs and CDMs in discovering causal variables from gene\nexpression data.\n  Results: The proposed algorithm using a CDM is sample efficient, since it\nconsistently outperforms other state-of-the-art local causal discovery\nalgorithms when samples sizes are small. However, the proposed algorithm using\na CDM outperforms the proposed algorithm using a DM only when sample sizes are\nabove several hundred. These results suggest that accurate causal discovery\nfrom gene expression data using current CDM-based algorithms requires datasets\nwith at least several hundred samples.\n  Availability: The proposed algorithm is freely available at\nhttps://github.com/ericstrobl/DvCD.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 20:52:18 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Strobl", "Eric V.", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1407.7584", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala", "title": "Dynamic Feature Scaling for Online Learning of Binary Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling feature values is an important step in numerous machine learning\ntasks. Different features can have different value ranges and some form of a\nfeature scaling is often required in order to learn an accurate classifier.\nHowever, feature scaling is conducted as a preprocessing task prior to\nlearning. This is problematic in an online setting because of two reasons.\nFirst, it might not be possible to accurately determine the value range of a\nfeature at the initial stages of learning when we have observed only a few\nnumber of training instances. Second, the distribution of data can change over\nthe time, which render obsolete any feature scaling that we perform in a\npre-processing step. We propose a simple but an effective method to dynamically\nscale features at train time, thereby quickly adapting to any changes in the\ndata stream. We compare the proposed dynamic feature scaling method against\nmore complex methods for estimating scaling parameters using several benchmark\ndatasets for binary classification. Our proposed feature scaling method\nconsistently outperforms more complex methods on all of the benchmark datasets\nand improves classification accuracy of a state-of-the-art online binary\nclassifier algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 21:59:06 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Bollegala", "Danushka", ""]]}, {"id": "1407.7635", "submitter": "Tomer Koren", "authors": "Uriel Feige, Tomer Koren, Moshe Tennenholtz", "title": "Chasing Ghosts: Competing with Stateful Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sequential decision making in a setting where regret is measured\nwith respect to a set of stateful reference policies, and feedback is limited\nto observing the rewards of the actions performed (the so called \"bandit\"\nsetting). If either the reference policies are stateless rather than stateful,\nor the feedback includes the rewards of all actions (the so called \"expert\"\nsetting), previous work shows that the optimal regret grows like\n$\\Theta(\\sqrt{T})$ in terms of the number of decision rounds $T$.\n  The difficulty in our setting is that the decision maker unavoidably loses\ntrack of the internal states of the reference policies, and thus cannot\nreliably attribute rewards observed in a certain round to any of the reference\npolicies. In fact, in this setting it is impossible for the algorithm to\nestimate which policy gives the highest (or even approximately highest) total\nreward. Nevertheless, we design an algorithm that achieves expected regret that\nis sublinear in $T$, of the form $O( T/\\log^{1/4}{T})$. Our algorithm is based\non a certain local repetition lemma that may be of independent interest. We\nalso show that no algorithm can guarantee expected regret better than $O(\nT/\\log^{3/2} T)$.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 06:17:49 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Feige", "Uriel", ""], ["Koren", "Tomer", ""], ["Tennenholtz", "Moshe", ""]]}, {"id": "1407.7644", "submitter": "Ariel Jaffe", "authors": "Ariel Jaffe, Boaz Nadler and Yuval Kluger", "title": "Estimating the Accuracies of Multiple Classifiers Without Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various situations one is given only the predictions of multiple\nclassifiers over a large unlabeled test data. This scenario raises the\nfollowing questions: Without any labeled data and without any a-priori\nknowledge about the reliability of these different classifiers, is it possible\nto consistently and computationally efficiently estimate their accuracies?\nFurthermore, also in a completely unsupervised manner, can one construct a more\naccurate unsupervised ensemble classifier? In this paper, focusing on the\nbinary case, we present simple, computationally efficient algorithms to solve\nthese questions. Furthermore, under standard classifier independence\nassumptions, we prove our methods are consistent and study their asymptotic\nerror. Our approach is spectral, based on the fact that the off-diagonal\nentries of the classifiers' covariance matrix and 3-d tensor are rank-one. We\nillustrate the competitive performance of our algorithms via extensive\nexperiments on both artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 07:19:08 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 11:23:37 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Jaffe", "Ariel", ""], ["Nadler", "Boaz", ""], ["Kluger", "Yuval", ""]]}, {"id": "1407.7691", "submitter": "J\\'er\\'emy Rapin", "authors": "J\\'er\\'emy Rapin and J\\'er\\^ome Bobin and Anthony Larue and Jean-Luc\n  Starck", "title": "NMF with Sparse Regularizations in Transformed Domains", "comments": "26 pages, 19 figures, accepted in SIAM Journal on Imaging Sciences", "journal-ref": "SIAM J. Imaging Sci., 7(4), 2020-2047. (28 pages)", "doi": "10.1137/140952314", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative blind source separation (non-negative BSS), which is also\nreferred to as non-negative matrix factorization (NMF), is a very active field\nin domains as different as astrophysics, audio processing or biomedical signal\nprocessing. In this context, the efficient retrieval of the sources requires\nthe use of signal priors such as sparsity. If NMF has now been well studied\nwith sparse constraints in the direct domain, only very few algorithms can\nencompass non-negativity together with sparsity in a transformed domain since\nsimultaneously dealing with two priors in two different domains is challenging.\nIn this article, we show how a sparse NMF algorithm coined non-negative\ngeneralized morphological component analysis (nGMCA) can be extended to impose\nnon-negativity in the direct domain along with sparsity in a transformed\ndomain, with both analysis and synthesis formulations. To our knowledge, this\nwork presents the first comparison of analysis and synthesis priors ---as well\nas their reweighted versions--- in the context of blind source separation.\nComparisons with state-of-the-art NMF algorithms on realistic data show the\nefficiency as well as the robustness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 11:09:59 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Rapin", "J\u00e9r\u00e9my", ""], ["Bobin", "J\u00e9r\u00f4me", ""], ["Larue", "Anthony", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1407.7722", "submitter": "Joaquin Vanschoren", "authors": "Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo", "title": "OpenML: networked science in machine learning", "comments": "12 pages, 10 figures", "journal-ref": "SIGKDD Explor. Newsl. 15, 2 (June 2014), 49-60", "doi": "10.1145/2641190.2641198", "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many sciences have made significant breakthroughs by adopting online tools\nthat help organize, structure and mine information that is too detailed to be\nprinted in journals. In this paper, we introduce OpenML, a place for machine\nlearning researchers to share and organize data in fine detail, so that they\ncan work more effectively, be more visible, and collaborate with others to\ntackle harder problems. We discuss how OpenML relates to other examples of\nnetworked science and what benefits it brings for machine learning research,\nindividual scientists, as well as students and practitioners.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 13:32:44 GMT"}, {"version": "v2", "created": "Fri, 1 Aug 2014 13:03:28 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Vanschoren", "Joaquin", ""], ["van Rijn", "Jan N.", ""], ["Bischl", "Bernd", ""], ["Torgo", "Luis", ""]]}, {"id": "1407.7753", "submitter": "Fabricio de Franca Olivetti", "authors": "Fabricio Olivetti de Fran\\c{c}a", "title": "A Hash-based Co-Clustering Algorithm for Categorical Data", "comments": "This work was submitted to IEEE TKDE on July 29, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-life data are described by categorical attributes without a\npre-classification. A common data mining method used to extract information\nfrom this type of data is clustering. This method group together the samples\nfrom the data that are more similar than all other samples. But, categorical\ndata pose a challenge when extracting information because: the calculation of\ntwo objects similarity is usually done by measuring the number of common\nfeatures, but ignore a possible importance weighting; if the data may be\ndivided differently according to different subsets of the features, the\nalgorithm may find clusters with different meanings from each other,\ndifficulting the post analysis. Data Co-Clustering of categorical data is the\ntechnique that tries to find subsets of samples that share a subset of features\nin common. By doing so, not only a sample may belong to more than one cluster\nbut, the feature selection of each cluster describe its own characteristics. In\nthis paper a novel Co-Clustering technique for categorical data is proposed by\nusing Locality Sensitive Hashing technique in order to preprocess a list of\nCo-Clusters seeds based on a previous research. Results indicate this technique\nis capable of finding high quality Co-Clusters in many different categorical\ndata sets and scales linearly with the data set size.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 15:23:11 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["de Fran\u00e7a", "Fabricio Olivetti", ""]]}, {"id": "1407.7819", "submitter": "Rui Song", "authors": "Shikai Luo, Rui Song, Daniela Witten", "title": "Sure Screening for Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose {graphical sure screening}, or GRASS, a very simple and\ncomputationally-efficient screening procedure for recovering the structure of a\nGaussian graphical model in the high-dimensional setting. The GRASS estimate of\nthe conditional dependence graph is obtained by thresholding the elements of\nthe sample covariance matrix. The proposed approach possesses the sure\nscreening property: with very high probability, the GRASS estimated edge set\ncontains the true edge set. Furthermore, with high probability, the size of the\nestimated edge set is controlled. We provide a choice of threshold for GRASS\nthat can control the expected false positive rate. We illustrate the\nperformance of GRASS in a simulation study and on a gene expression data set,\nand show that in practice it performs quite competitively with more complex and\ncomputationally-demanding techniques for graph estimation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 18:37:15 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Luo", "Shikai", ""], ["Song", "Rui", ""], ["Witten", "Daniela", ""]]}, {"id": "1407.7906", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio", "title": "How Auto-Encoders Could Provide Credit Assignment in Deep Networks via\n  Target Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to exploit {\\em reconstruction} as a layer-local training signal\nfor deep learning. Reconstructions can be propagated in a form of target\npropagation playing a role similar to back-propagation but helping to reduce\nthe reliance on derivatives in order to perform credit assignment across many\nlevels of possibly strong non-linearities (which is difficult for\nback-propagation). A regularized auto-encoder tends produce a reconstruction\nthat is a more likely version of its input, i.e., a small move in the direction\nof higher likelihood. By generalizing gradients, target propagation may also\nallow to train deep networks with discrete hidden units. If the auto-encoder\ntakes both a representation of input and target (or of any side information) in\ninput, then its reconstruction of input representation provides a target\ntowards a representation that is more likely, conditioned on all the side\ninformation. A deep auto-encoder decoding path generalizes gradient propagation\nin a learned way that can could thus handle not just infinitesimal changes but\nlarger, discrete changes, hopefully allowing credit assignment through a long\nchain of non-linear operations. In addition to each layer being a good\nauto-encoder, the encoder also learns to please the upper layers by\ntransforming the data into a space where it is easier to model by them,\nflattening manifolds and disentangling factors. The motivations and theoretical\njustifications for this approach are laid down in this paper, along with\nconjectures that will have to be verified either mathematically or\nexperimentally, including a hypothesis stating that such auto-encoder mediated\ntarget propagation could play in brains the role of credit assignment through\nmany non-linear, noisy and discrete transformations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 23:32:44 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 18:34:52 GMT"}, {"version": "v3", "created": "Thu, 18 Sep 2014 13:30:31 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Bengio", "Yoshua", ""]]}, {"id": "1407.7937", "submitter": "Ruta Mehta", "authors": "Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, and Vijay\n  V. Vazirani", "title": "Learning Economic Parameters from Revealed Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work, starting with Beigman and Vohra (2006) and\nZadimoghaddam and Roth (2012), has addressed the problem of {\\em learning} a\nutility function from revealed preference data. The goal here is to make use of\npast data describing the purchases of a utility maximizing agent when faced\nwith certain prices and budget constraints in order to produce a hypothesis\nfunction that can accurately forecast the {\\em future} behavior of the agent.\n  In this work we advance this line of work by providing sample complexity\nguarantees and efficient algorithms for a number of important classes. By\ndrawing a connection to recent advances in multi-class learning, we provide a\ncomputationally efficient algorithm with tight sample complexity guarantees\n($\\Theta(d/\\epsilon)$ for the case of $d$ goods) for learning linear utility\nfunctions under a linear price model. This solves an open question in\nZadimoghaddam and Roth (2012). Our technique yields numerous generalizations\nincluding the ability to learn other well-studied classes of utility functions,\nto deal with a misspecified model, and with non-linear prices.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 04:00:29 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Daniely", "Amit", ""], ["Mehta", "Ruta", ""], ["Urner", "Ruth", ""], ["Vazirani", "Vijay V.", ""]]}, {"id": "1407.8042", "submitter": "Lewis Evans Mr", "authors": "Lewis P. G. Evans and Niall M. Adams and Christoforos Anagnostopoulos", "title": "Targeting Optimal Active Learning via Example Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many classification problems unlabelled data is abundant and a subset can\nbe chosen for labelling. This defines the context of active learning (AL),\nwhere methods systematically select that subset, to improve a classifier by\nretraining. Given a classification problem, and a classifier trained on a small\nnumber of labelled examples, consider the selection of a single further\nexample. This example will be labelled by the oracle and then used to retrain\nthe classifier. This example selection raises a central question: given a fully\nspecified stochastic description of the classification problem, which example\nis the optimal selection? If optimality is defined in terms of loss, this\ndefinition directly produces expected loss reduction (ELR), a central quantity\nwhose maximum yields the optimal example selection. This work presents a new\ntheoretical approach to AL, example quality, which defines optimal AL behaviour\nin terms of ELR. Once optimal AL behaviour is defined mathematically, reasoning\nabout this abstraction provides insights into AL. In a theoretical context the\noptimal selection is compared to existing AL methods, showing that heuristics\ncan make sub-optimal selections. Algorithms are constructed to estimate example\nquality directly. A large-scale experimental study shows these algorithms to be\ncompetitive with standard AL methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 13:54:58 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Evans", "Lewis P. G.", ""], ["Adams", "Niall M.", ""], ["Anagnostopoulos", "Christoforos", ""]]}, {"id": "1407.8067", "submitter": "Fei Yu", "authors": "Fei Yu, Michal Rybar, Caroline Uhler, Stephen E. Fienberg", "title": "Differentially-Private Logistic Regression for Detecting Multiple-SNP\n  Association in GWAS Databases", "comments": "To appear in Proceedings of the 2014 International Conference on\n  Privacy in Statistical Databases", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the publication of an attack on genome-wide association studies\n(GWAS) data proposed by Homer et al., considerable attention has been given to\ndeveloping methods for releasing GWAS data in a privacy-preserving way. Here,\nwe develop an end-to-end differentially private method for solving regression\nproblems with convex penalty functions and selecting the penalty parameters by\ncross-validation. In particular, we focus on penalized logistic regression with\nelastic-net regularization, a method widely used to in GWAS analyses to\nidentify disease-causing genes. We show how a differentially private procedure\nfor penalized logistic regression with elastic-net regularization can be\napplied to the analysis of GWAS data and evaluate our method's performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 14:51:19 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Yu", "Fei", ""], ["Rybar", "Michal", ""], ["Uhler", "Caroline", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1407.8088", "submitter": "Alejandro Edera", "authors": "Alejandro Edera, Yanela Strappa and Facundo Bromberg", "title": "The Grow-Shrink strategy for learning Markov network structures\n  constrained by context-specific independences", "comments": "12 pages, and 8 figures. This works was presented in IBERAMIA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks are models for compactly representing complex probability\ndistributions. They are composed by a structure and a set of numerical weights.\nThe structure qualitatively describes independences in the distribution, which\ncan be exploited to factorize the distribution into a set of compact functions.\nA key application for learning structures from data is to automatically\ndiscover knowledge. In practice, structure learning algorithms focused on\n\"knowledge discovery\" present a limitation: they use a coarse-grained\nrepresentation of the structure. As a result, this representation cannot\ndescribe context-specific independences. Very recently, an algorithm called\nCSPC was designed to overcome this limitation, but it has a high computational\ncomplexity. This work tries to mitigate this downside presenting CSGS, an\nalgorithm that uses the Grow-Shrink strategy for reducing unnecessary\ncomputations. On an empirical evaluation, the structures learned by CSGS\nachieve competitive accuracies and lower computational complexity with respect\nto those obtained by CSPC.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 15:24:46 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Edera", "Alejandro", ""], ["Strappa", "Yanela", ""], ["Bromberg", "Facundo", ""]]}, {"id": "1407.8147", "submitter": "Binbin Lin", "authors": "Binbin Lin, Qingyang Li, Qian Sun, Ming-Jun Lai, Ian Davidson, Wei\n  Fan, Jieping Ye", "title": "Stochastic Coordinate Coding and Its Application for Drosophila Gene\n  Expression Pattern Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textit{Drosophila melanogaster} has been established as a model organism for\ninvestigating the fundamental principles of developmental gene interactions.\nThe gene expression patterns of \\textit{Drosophila melanogaster} can be\ndocumented as digital images, which are annotated with anatomical ontology\nterms to facilitate pattern discovery and comparison. The automated annotation\nof gene expression pattern images has received increasing attention due to the\nrecent expansion of the image database. The effectiveness of gene expression\npattern annotation relies on the quality of feature representation. Previous\nstudies have demonstrated that sparse coding is effective for extracting\nfeatures from gene expression images. However, solving sparse coding remains a\ncomputationally challenging problem, especially when dealing with large-scale\ndata sets and learning large size dictionaries. In this paper, we propose a\nnovel algorithm to solve the sparse coding problem, called Stochastic\nCoordinate Coding (SCC). The proposed algorithm alternatively updates the\nsparse codes via just a few steps of coordinate descent and updates the\ndictionary via second order stochastic gradient descent. The computational cost\nis further reduced by focusing on the non-zero components of the sparse codes\nand the corresponding columns of the dictionary only in the updating procedure.\nThus, the proposed algorithm significantly improves the efficiency and the\nscalability, making sparse coding applicable for large-scale data sets and\nlarge dictionary sizes. Our experiments on Drosophila gene expression data sets\ndemonstrate the efficiency and the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 18:04:20 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 07:03:36 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Lin", "Binbin", ""], ["Li", "Qingyang", ""], ["Sun", "Qian", ""], ["Lai", "Ming-Jun", ""], ["Davidson", "Ian", ""], ["Fan", "Wei", ""], ["Ye", "Jieping", ""]]}, {"id": "1407.8187", "submitter": "Charles Fisher", "authors": "Charles K. Fisher, Pankaj Mehta", "title": "Fast Bayesian Feature Selection for High Dimensional Linear Regression\n  in Genomics via the Ising Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection, identifying a subset of variables that are relevant for\npredicting a response, is an important and challenging component of many\nmethods in statistics and machine learning. Feature selection is especially\ndifficult and computationally intensive when the number of variables approaches\nor exceeds the number of samples, as is often the case for many genomic\ndatasets. Here, we introduce a new approach -- the Bayesian Ising Approximation\n(BIA) -- to rapidly calculate posterior probabilities for feature relevance in\nL2 penalized linear regression. In the regime where the regression problem is\nstrongly regularized by the prior, we show that computing the marginal\nposterior probabilities for features is equivalent to computing the\nmagnetizations of an Ising model. Using a mean field approximation, we show it\nis possible to rapidly compute the feature selection path described by the\nposterior probabilities as a function of the L2 penalty. We present simulations\nand analytical results illustrating the accuracy of the BIA on some simple\nregression problems. Finally, we demonstrate the applicability of the BIA to\nhigh dimensional regression by analyzing a gene expression dataset with nearly\n30,000 features.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 20:00:14 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Fisher", "Charles K.", ""], ["Mehta", "Pankaj", ""]]}, {"id": "1407.8289", "submitter": "Lifang He", "authors": "Lifang He, Xiangnan Kong, Philip S. Yu, Ann B. Ragin, Zhifeng Hao,\n  Xiaowei Yang", "title": "DuSK: A Dual Structure-preserving Kernel for Supervised Tensor Learning\n  with Applications to Neuroimages", "comments": "9 pages, 6 figures, conference,Proceedings of the 14th SIAM\n  International Conference on Data Mining (SDM14), Philadelphia, USA, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in data collection technologies, tensor data is assuming\nincreasing prominence in many applications and the problem of supervised tensor\nlearning has emerged as a topic of critical significance in the data mining and\nmachine learning community. Conventional methods for supervised tensor learning\nmainly focus on learning kernels by flattening the tensor into vectors or\nmatrices, however structural information within the tensors will be lost. In\nthis paper, we introduce a new scheme to design structure-preserving kernels\nfor supervised tensor learning. Specifically, we demonstrate how to leverage\nthe naturally available structure within the tensorial representation to encode\nprior knowledge in the kernel. We proposed a tensor kernel that can preserve\ntensor structures based upon dual-tensorial mapping. The dual-tensorial mapping\nfunction can map each tensor instance in the input space to another tensor in\nthe feature space while preserving the tensorial structure. Theoretically, our\napproach is an extension of the conventional kernels in the vector space to\ntensor space. We applied our novel kernel in conjunction with SVM to real-world\ntensor classification problems including brain fMRI classification for three\ndifferent diseases (i.e., Alzheimer's disease, ADHD and brain damage by HIV).\nExtensive empirical studies demonstrate that our proposed approach can\neffectively boost tensor classification performances, particularly with small\nsample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 06:33:42 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 07:43:54 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["He", "Lifang", ""], ["Kong", "Xiangnan", ""], ["Yu", "Philip S.", ""], ["Ragin", "Ann B.", ""], ["Hao", "Zhifeng", ""], ["Yang", "Xiaowei", ""]]}, {"id": "1407.8339", "submitter": "Wei Chen", "authors": "Wei Chen, Yajun Wang, Yang Yuan, Qinshi Wang", "title": "Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically\n  Triggered Arms", "comments": "A preliminary version of the paper is published in ICML'2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a general framework for a large class of combinatorial multi-armed\nbandit (CMAB) problems, where subsets of base arms with unknown distributions\nform super arms. In each round, a super arm is played and the base arms\ncontained in the super arm are played and their outcomes are observed. We\nfurther consider the extension in which more based arms could be\nprobabilistically triggered based on the outcomes of already triggered arms.\nThe reward of the super arm depends on the outcomes of all played arms, and it\nonly needs to satisfy two mild assumptions, which allow a large class of\nnonlinear reward instances. We assume the availability of an offline\n(\\alpha,\\beta)-approximation oracle that takes the means of the outcome\ndistributions of arms and outputs a super arm that with probability {\\beta}\ngenerates an {\\alpha} fraction of the optimal expected reward. The objective of\nan online learning algorithm for CMAB is to minimize\n(\\alpha,\\beta)-approximation regret, which is the difference between the\n\\alpha{\\beta} fraction of the expected reward when always playing the optimal\nsuper arm, and the expected reward of playing super arms according to the\nalgorithm. We provide CUCB algorithm that achieves O(log n)\ndistribution-dependent regret, where n is the number of rounds played, and we\nfurther provide distribution-independent bounds for a large class of reward\nfunctions. Our regret analysis is tight in that it matches the bound of UCB1\nalgorithm (up to a constant factor) for the classical MAB problem, and it\nsignificantly improves the regret bound in a earlier paper on combinatorial\nbandits with linear rewards. We apply our CMAB framework to two new\napplications, probabilistic maximum coverage and social influence maximization,\nboth having nonlinear reward structures. In particular, application to social\ninfluence maximization requires our extension on probabilistically triggered\narms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 10:09:11 GMT"}, {"version": "v2", "created": "Wed, 10 Sep 2014 01:33:08 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 06:26:10 GMT"}, {"version": "v4", "created": "Sun, 15 Nov 2015 06:02:45 GMT"}, {"version": "v5", "created": "Mon, 28 Mar 2016 05:02:24 GMT"}, {"version": "v6", "created": "Tue, 29 Mar 2016 01:00:59 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Chen", "Wei", ""], ["Wang", "Yajun", ""], ["Yuan", "Yang", ""], ["Wang", "Qinshi", ""]]}, {"id": "1407.8518", "submitter": "Roberto Rigamonti", "authors": "Roberto Rigamonti, Vincent Lepetit, Pascal Fua", "title": "Beyond KernelBoost", "comments": null, "journal-ref": null, "doi": null, "report-no": "EPFL-REPORT-200378", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Technical Report we propose a set of improvements with respect to the\nKernelBoost classifier presented in [Becker et al., MICCAI 2013]. We start with\na scheme inspired by Auto-Context, but that is suitable in situations where the\nlack of large training sets poses a potential problem of overfitting. The aim\nis to capture the interactions between neighboring image pixels to better\nregularize the boundaries of segmented regions. As in Auto-Context [Tu et al.,\nPAMI 2009] the segmentation process is iterative and, at each iteration, the\nsegmentation results for the previous iterations are taken into account in\nconjunction with the image itself. However, unlike in [Tu et al., PAMI 2009],\nwe organize our recursion so that the classifiers can progressively focus on\ndifficult-to-classify locations. This lets us exploit the power of the\ndecision-tree paradigm while avoiding over-fitting. In the context of this\narchitecture, KernelBoost represents a powerful building block due to its\nability to learn on the score maps coming from previous iterations. We first\nintroduce two important mechanisms to empower the KernelBoost classifier,\nnamely pooling and the clustering of positive samples based on the appearance\nof the corresponding ground-truth. These operations significantly contribute to\nincrease the effectiveness of the system on biomedical images, where texture\nplays a major role in the recognition of the different image components. We\nthen present some other techniques that can be easily integrated in the\nKernelBoost framework to further improve the accuracy of the final\nsegmentation. We show extensive results on different medical image datasets,\nincluding some multi-label tasks, on which our method is shown to outperform\nstate-of-the-art approaches. The resulting segmentations display high accuracy,\nneat contours, and reduced noise.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 09:07:03 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Rigamonti", "Roberto", ""], ["Lepetit", "Vincent", ""], ["Fua", "Pascal", ""]]}]