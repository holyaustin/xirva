[{"id": "1505.00146", "submitter": "Tao Qin Dr.", "authors": "Yingce Xia, Haifang Li, Tao Qin, Nenghai Yu, Tie-Yan Liu", "title": "Thompson Sampling for Budgeted Multi-armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling is one of the earliest randomized algorithms for\nmulti-armed bandits (MAB). In this paper, we extend the Thompson sampling to\nBudgeted MAB, where there is random cost for pulling an arm and the total cost\nis constrained by a budget. We start with the case of Bernoulli bandits, in\nwhich the random rewards (costs) of an arm are independently sampled from a\nBernoulli distribution. To implement the Thompson sampling algorithm in this\ncase, at each round, we sample two numbers from the posterior distributions of\nthe reward and cost for each arm, obtain their ratio, select the arm with the\nmaximum ratio, and then update the posterior distributions. We prove that the\ndistribution-dependent regret bound of this algorithm is $O(\\ln B)$, where $B$\ndenotes the budget. By introducing a Bernoulli trial, we further extend this\nalgorithm to the setting that the rewards (costs) are drawn from general\ndistributions, and prove that its regret bound remains almost the same. Our\nsimulation results demonstrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 10:35:35 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Xia", "Yingce", ""], ["Li", "Haifang", ""], ["Qin", "Tao", ""], ["Yu", "Nenghai", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1505.00199", "submitter": "Shameem Puthiya Parambath Mr.", "authors": "Shameem A Puthiya Parambath, Nicolas Usunier, Yves Grandvalet", "title": "Theory of Optimizing Pseudolinear Performance Measures: Application to\n  F-measure", "comments": "Extended Version of the NIPS 2014 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear performance measures are widely used for the evaluation of\nlearning algorithms. For example, $F$-measure is a commonly used performance\nmeasure for classification problems in machine learning and information\nretrieval community. We study the theoretical properties of a subset of\nnon-linear performance measures called pseudo-linear performance measures which\nincludes $F$-measure, \\emph{Jaccard Index}, among many others. We establish\nthat many notions of $F$-measures and \\emph{Jaccard Index} are pseudo-linear\nfunctions of the per-class false negatives and false positives for binary,\nmulticlass and multilabel classification. Based on this observation, we present\na general reduction of such performance measure optimization problem to\ncost-sensitive classification problem with unknown costs. We then propose an\nalgorithm with provable guarantees to obtain an approximately optimal\nclassifier for the $F$-measure by solving a series of cost-sensitive\nclassification problems. The strength of our analysis is to be valid on any\ndataset and any class of classifiers, extending the existing theoretical\nresults on pseudo-linear measures, which are asymptotic in nature. We also\nestablish the multi-objective nature of the $F$-score maximization problem by\nlinking the algorithm with the weighted-sum approach used in multi-objective\noptimization. We present numerical experiments to illustrate the relative\nimportance of cost asymmetry and thresholding when learning linear classifiers\non various $F$-measure optimization tasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 15:25:59 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 19:21:58 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2015 18:53:59 GMT"}, {"version": "v4", "created": "Mon, 1 Jan 2018 06:34:30 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Parambath", "Shameem A Puthiya", ""], ["Usunier", "Nicolas", ""], ["Grandvalet", "Yves", ""]]}, {"id": "1505.00277", "submitter": "Dana Movshovitz-Attias", "authors": "Dana Movshovitz-Attias, William W. Cohen", "title": "Grounded Discovery of Coordinate Term Relationships between Software\n  Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for the detection of coordinate-term relationships\nbetween entities from the software domain, that refer to Java classes. Usually,\nrelations are found by examining corpus statistics associated with text\nentities. In some technical domains, however, we have access to additional\ninformation about the real-world objects named by the entities, suggesting that\ncoupling information about the \"grounded\" entities with corpus statistics might\nlead to improved methods for relation discovery. To this end, we develop a\nsimilarity measure for Java classes using distributional information about how\nthey are used in software, which we combine with corpus statistics on the\ndistribution of contexts in which the classes appear in text. Using our\napproach, cross-validation accuracy on this dataset can be improved\ndramatically, from around 60% to 88%. Human labeling results show that our\nclassifier has an F1 score of 86% over the top 1000 predicted pairs.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 20:40:00 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Movshovitz-Attias", "Dana", ""], ["Cohen", "William W.", ""]]}, {"id": "1505.00290", "submitter": "Anup Rao", "authors": "Rasmus Kyng, Anup Rao, Sushant Sachdeva and Daniel A. Spielman", "title": "Algorithms for Lipschitz Learning on Graphs", "comments": "Code used in this work is available at\n  https://github.com/danspielman/YINSlex 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop fast algorithms for solving regression problems on graphs where\none is given the value of a function at some vertices, and must find its\nsmoothest possible extension to all vertices. The extension we compute is the\nabsolutely minimal Lipschitz extension, and is the limit for large $p$ of\n$p$-Laplacian regularization. We present an algorithm that computes a minimal\nLipschitz extension in expected linear time, and an algorithm that computes an\nabsolutely minimal Lipschitz extension in expected time $\\widetilde{O} (m n)$.\nThe latter algorithm has variants that seem to run much faster in practice.\nThese extensions are particularly amenable to regularization: we can perform\n$l_{0}$-regularization on the given values in polynomial time and\n$l_{1}$-regularization on the initial function values and on graph edge weights\nin time $\\widetilde{O} (m^{3/2})$.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 22:30:45 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 15:16:01 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Kyng", "Rasmus", ""], ["Rao", "Anup", ""], ["Sachdeva", "Sushant", ""], ["Spielman", "Daniel A.", ""]]}, {"id": "1505.00294", "submitter": "Nirav Bhatt", "authors": "Nirav Bhatt and Arun Ayyar", "title": "Monotonous (Semi-)Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": "10.1145/2732587.2732600", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) factorizes a non-negative matrix into\nproduct of two non-negative matrices, namely a signal matrix and a mixing\nmatrix. NMF suffers from the scale and ordering ambiguities. Often, the source\nsignals can be monotonous in nature. For example, in source separation problem,\nthe source signals can be monotonously increasing or decreasing while the\nmixing matrix can have nonnegative entries. NMF methods may not be effective\nfor such cases as it suffers from the ordering ambiguity. This paper proposes\nan approach to incorporate notion of monotonicity in NMF, labeled as monotonous\nNMF. An algorithm based on alternating least-squares is proposed for recovering\nmonotonous signals from a data matrix. Further, the assumption on mixing matrix\nis relaxed to extend monotonous NMF for data matrix with real numbers as\nentries. The approach is illustrated using synthetic noisy data. The results\nobtained by monotonous NMF are compared with standard NMF algorithms in the\nliterature, and it is shown that monotonous NMF estimates source signals well\nin comparison to standard NMF algorithms when the underlying sources signals\nare monotonous.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 23:58:17 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Bhatt", "Nirav", ""], ["Ayyar", "Arun", ""]]}, {"id": "1505.00308", "submitter": "Tejaswi Nimmagadda", "authors": "Tejaswi Nimmagadda and Anima Anandkumar", "title": "Multi-Object Classification and Unsupervised Scene Understanding Using\n  Deep Learning Features and Latent Tree Probabilistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown state-of-art classification performance on datasets\nsuch as ImageNet, which contain a single object in each image. However,\nmulti-object classification is far more challenging. We present a unified\nframework which leverages the strengths of multiple machine learning methods,\nviz deep learning, probabilistic models and kernel methods to obtain\nstate-of-art performance on Microsoft COCO, consisting of non-iconic images. We\nincorporate contextual information in natural images through a conditional\nlatent tree probabilistic model (CLTM), where the object co-occurrences are\nconditioned on the extracted fc7 features from pre-trained Imagenet CNN as\ninput. We learn the CLTM tree structure using conditional pairwise\nprobabilities for object co-occurrences, estimated through kernel methods, and\nwe learn its node and edge potentials by training a new 3-layer neural network,\nwhich takes fc7 features as input. Object classification is carried out via\ninference on the learnt conditional tree model, and we obtain significant gain\nin precision-recall and F-measures on MS-COCO, especially for difficult object\ncategories. Moreover, the latent variables in the CLTM capture scene\ninformation: the images with top activations for a latent node have common\nthemes such as being a grasslands or a food scene, and on on. In addition, we\nshow that a simple k-means clustering of the inferred latent nodes alone\nsignificantly improves scene classification performance on the MIT-Indoor\ndataset, without the need for any retraining, and without using scene labels\nduring training. Thus, we present a unified framework for multi-object\nclassification and unsupervised scene understanding.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 03:23:46 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Nimmagadda", "Tejaswi", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1505.00314", "submitter": "Nirav Bhatt", "authors": "Shankar Narasimhan and Nirav Bhatt", "title": "Deconstructing Principal Component Analysis Using a Data Reconciliation\n  Perspective", "comments": null, "journal-ref": "Computers and Chemical Engineering 77 (2015) 74-84", "doi": "10.1016/j.compchemeng.2015.03.016", "report-no": null, "categories": "cs.LG cs.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data reconciliation (DR) and Principal Component Analysis (PCA) are two\npopular data analysis techniques in process industries. Data reconciliation is\nused to obtain accurate and consistent estimates of variables and parameters\nfrom erroneous measurements. PCA is primarily used as a method for reducing the\ndimensionality of high dimensional data and as a preprocessing technique for\ndenoising measurements. These techniques have been developed and deployed\nindependently of each other. The primary purpose of this article is to\nelucidate the close relationship between these two seemingly disparate\ntechniques. This leads to a unified framework for applying PCA and DR. Further,\nwe show how the two techniques can be deployed together in a collaborative and\nconsistent manner to process data. The framework has been extended to deal with\npartially measured systems and to incorporate partial knowledge available about\nthe process model.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 06:20:08 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Narasimhan", "Shankar", ""], ["Bhatt", "Nirav", ""]]}, {"id": "1505.00322", "submitter": "William Curran", "authors": "William Curran, Tim Brys, Matthew Taylor, William Smart", "title": "Using PCA to Efficiently Represent State Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms need to deal with the exponential growth of\nstates and actions when exploring optimal control in high-dimensional spaces.\nThis is known as the curse of dimensionality. By projecting the agent's state\nonto a low-dimensional manifold, we can represent the state space in a smaller\nand more efficient representation. By using this representation during\nlearning, the agent can converge to a good policy much faster. We test this\napproach in the Mario Benchmarking Domain. When using dimensionality reduction\nin Mario, learning converges much faster to a good policy. But, there is a\ncritical convergence-performance trade-off. By projecting onto a\nlow-dimensional manifold, we are ignoring important data. In this paper, we\nexplore this trade-off of convergence and performance. We find that learning in\nas few as 4 dimensions (instead of 9), we can improve performance past learning\nin the full dimensional space at a faster convergence rate.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 08:29:09 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 16:20:56 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Curran", "William", ""], ["Brys", "Tim", ""], ["Taylor", "Matthew", ""], ["Smart", "William", ""]]}, {"id": "1505.00359", "submitter": "Harm de Vries", "authors": "Harm de Vries, Jason Yosinski", "title": "Can deep learning help you find the perfect match?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is he/she my type or not? The answer to this question depends on the personal\npreferences of the one asking it. The individual process of obtaining a full\nanswer may generally be difficult and time consuming, but often an approximate\nanswer can be obtained simply by looking at a photo of the potential match.\nSuch approximate answers based on visual cues can be produced in a fraction of\na second, a phenomenon that has led to a series of recently successful dating\napps in which users rate others positively or negatively using primarily a\nsingle photo. In this paper we explore using convolutional networks to create a\nmodel of an individual's personal preferences based on rated photos. This\nintroduced task is difficult due to the large number of variations in profile\npictures and the noise in attractiveness labels. Toward this task we collect a\ndataset comprised of $9364$ pictures and binary labels for each. We compare\nperformance of convolutional models trained in three ways: first directly on\nthe collected dataset, second with features transferred from a network trained\nto predict gender, and third with features transferred from a network trained\non ImageNet. Our findings show that ImageNet features transfer best, producing\na model that attains $68.1\\%$ accuracy on the test set and is moderately\nsuccessful at predicting matches.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 17:20:23 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2015 15:41:45 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["de Vries", "Harm", ""], ["Yosinski", "Jason", ""]]}, {"id": "1505.00384", "submitter": "Abhinav Tushar", "authors": "Abhinav Tushar", "title": "Making Sense of Hidden Layer Information in Deep Networks by Learning\n  Hierarchical Targets", "comments": "Updated to add a note with commentary on original (v1) submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an architecture for deep neural networks with hidden\nlayer branches that learn targets of lower hierarchy than final layer targets.\nThe branches provide a channel for enforcing useful information in hidden layer\nwhich helps in attaining better accuracy, both for the final layer and hidden\nlayers. The shared layers modify their weights using the gradients of all cost\nfunctions higher than the branching layer. This model provides a flexible\ninference system with many levels of targets which is modular and can be used\nefficiently in situations requiring different levels of results according to\ncomplexity. This paper applies the idea to a text classification task on 20\nNewsgroups data set with two level of hierarchical targets and a comparison is\nmade with training without the use of hidden layer branches.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 00:58:38 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 07:31:47 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Tushar", "Abhinav", ""]]}, {"id": "1505.00387", "submitter": "Rupesh Kumar Srivastava", "authors": "Rupesh Kumar Srivastava, Klaus Greff, J\\\"urgen Schmidhuber", "title": "Highway Networks", "comments": "6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop.\n  Full paper is at arXiv:1507.06228", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is plenty of theoretical and empirical evidence that depth of neural\nnetworks is a crucial ingredient for their success. However, network training\nbecomes more difficult with increasing depth and training of very deep networks\nremains an open problem. In this extended abstract, we introduce a new\narchitecture designed to ease gradient-based training of very deep networks. We\nrefer to networks with this architecture as highway networks, since they allow\nunimpeded information flow across several layers on \"information highways\". The\narchitecture is characterized by the use of gating units which learn to\nregulate the flow of information through a network. Highway networks with\nhundreds of layers can be trained directly using stochastic gradient descent\nand with a variety of activation functions, opening up the possibility of\nstudying extremely deep and efficient architectures.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 01:56:57 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 18:15:15 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Srivastava", "Rupesh Kumar", ""], ["Greff", "Klaus", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1505.00388", "submitter": "Mark Bun", "authors": "Mark Bun and Mark Zhandry", "title": "Order-Revealing Encryption and the Hardness of Private Learning", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An order-revealing encryption scheme gives a public procedure by which two\nciphertexts can be compared to reveal the ordering of their underlying\nplaintexts. We show how to use order-revealing encryption to separate\ncomputationally efficient PAC learning from efficient $(\\epsilon,\n\\delta)$-differentially private PAC learning. That is, we construct a concept\nclass that is efficiently PAC learnable, but for which every efficient learner\nfails to be differentially private. This answers a question of Kasiviswanathan\net al. (FOCS '08, SIAM J. Comput. '11).\n  To prove our result, we give a generic transformation from an order-revealing\nencryption scheme into one with strongly correct comparison, which enables the\nconsistent comparison of ciphertexts that are not obtained as the valid\nencryption of any message. We believe this construction may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 02:23:49 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Bun", "Mark", ""], ["Zhandry", "Mark", ""]]}, {"id": "1505.00398", "submitter": "Ruoxi Wang", "authors": "Ruoxi Wang, Yingzhou Li, Michael W. Mahoney, Eric Darve", "title": "Block Basis Factorization for Scalable Kernel Matrix Evaluation", "comments": "16 pages, 5 figures", "journal-ref": "SIAM Journal on Matrix Analysis and Applications, 2019, Vol. 40,\n  No. 4 : pp. 1497-1526", "doi": "10.1137/18M1212586", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are widespread in machine learning; however, they are limited\nby the quadratic complexity of the construction, application, and storage of\nkernel matrices. Low-rank matrix approximation algorithms are widely used to\naddress this problem and reduce the arithmetic and storage cost. However, we\nobserved that for some datasets with wide intra-class variability, the optimal\nkernel parameter for smaller classes yields a matrix that is less well\napproximated by low-rank methods. In this paper, we propose an efficient\nstructured low-rank approximation method -- the Block Basis Factorization (BBF)\n-- and its fast construction algorithm to approximate radial basis function\n(RBF) kernel matrices. Our approach has linear memory cost and floating-point\noperations for many machine learning kernels. BBF works for a wide range of\nkernel bandwidth parameters and extends the domain of applicability of low-rank\napproximation methods significantly. Our empirical results demonstrate the\nstability and superiority over the state-of-art kernel approximation\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 07:05:45 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 21:48:23 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 07:06:20 GMT"}, {"version": "v4", "created": "Tue, 4 May 2021 06:02:31 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Wang", "Ruoxi", ""], ["Li", "Yingzhou", ""], ["Mahoney", "Michael W.", ""], ["Darve", "Eric", ""]]}, {"id": "1505.00401", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Visualization of Tradeoff in Evaluation: from Precision-Recall & PN to\n  LIFT, ROC & BIRD", "comments": "23 pages, 12 equations, 2 figures, 2 tables, 1 sidebar", "journal-ref": null, "doi": null, "report-no": "KIT-14-002", "categories": "cs.LG cs.AI cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation often aims to reduce the correctness or error characteristics of a\nsystem down to a single number, but that always involves trade-offs. Another\nway of dealing with this is to quote two numbers, such as Recall and Precision,\nor Sensitivity and Specificity. But it can also be useful to see more than\nthis, and a graphical approach can explore sensitivity to cost, prevalence,\nbias, noise, parameters and hyper-parameters.\n  Moreover, most techniques are implicitly based on two balanced classes, and\nour ability to visualize graphically is intrinsically two dimensional, but we\noften want to visualize in a multiclass context. We review the dichotomous\napproaches relating to Precision, Recall, and ROC as well as the related LIFT\nchart, exploring how they handle unbalanced and multiclass data, and deriving\nnew probabilistic and information theoretic variants of LIFT that help deal\nwith the issues associated with the handling of multiple and unbalanced\nclasses.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 07:27:27 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 04:02:22 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1505.00423", "submitter": "Josif Grabocka", "authors": "Josif Grabocka and Nicolas Schilling and Lars Schmidt-Thieme", "title": "Optimal Time-Series Motifs", "comments": "Submitted to KDD2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motifs are the most repetitive/frequent patterns of a time-series. The\ndiscovery of motifs is crucial for practitioners in order to understand and\ninterpret the phenomena occurring in sequential data. Currently, motifs are\nsearched among series sub-sequences, aiming at selecting the most frequently\noccurring ones. Search-based methods, which try out series sub-sequence as\nmotif candidates, are currently believed to be the best methods in finding the\nmost frequent patterns.\n  However, this paper proposes an entirely new perspective in finding motifs.\nWe demonstrate that searching is non-optimal since the domain of motifs is\nrestricted, and instead we propose a principled optimization approach able to\nfind optimal motifs. We treat the occurrence frequency as a function and\ntime-series motifs as its parameters, therefore we \\textit{learn} the optimal\nmotifs that maximize the frequency function. In contrast to searching, our\nmethod is able to discover the most repetitive patterns (hence optimal), even\nin cases where they do not explicitly occur as sub-sequences. Experiments on\nseveral real-life time-series datasets show that the motifs found by our method\nare highly more frequent than the ones found through searching, for exactly the\nsame distance threshold.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 12:11:43 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Grabocka", "Josif", ""], ["Schilling", "Nicolas", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1505.00477", "submitter": "Rocco Langone", "authors": "Rocco Langone, Raghvendra Mall, Carlos Alzate, Johan A. K. Suykens", "title": "Kernel Spectral Clustering and applications", "comments": "chapter contribution to the book \"Unsupervised Learning Algorithms\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we review the main literature related to kernel spectral\nclustering (KSC), an approach to clustering cast within a kernel-based\noptimization setting. KSC represents a least-squares support vector machine\nbased formulation of spectral clustering described by a weighted kernel PCA\nobjective. Just as in the classifier case, the binary clustering model is\nexpressed by a hyperplane in a high dimensional space induced by a kernel. In\naddition, the multi-way clustering can be obtained by combining a set of binary\ndecision functions via an Error Correcting Output Codes (ECOC) encoding scheme.\nBecause of its model-based nature, the KSC method encompasses three main steps:\ntraining, validation, testing. In the validation stage model selection is\nperformed to obtain tuning parameters, like the number of clusters present in\nthe data. This is a major advantage compared to classical spectral clustering\nwhere the determination of the clustering parameters is unclear and relies on\nheuristics. Once a KSC model is trained on a small subset of the entire data,\nit is able to generalize well to unseen test points. Beyond the basic\nformulation, sparse KSC algorithms based on the Incomplete Cholesky\nDecomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization are\nreviewed. In that respect, we show how it is possible to handle large scale\ndata. Also, two possible ways to perform hierarchical clustering and a soft\nclustering method are presented. Finally, real-world applications such as image\nsegmentation, power load time-series clustering, document clustering and big\ndata learning are considered.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 21:07:09 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Langone", "Rocco", ""], ["Mall", "Raghvendra", ""], ["Alzate", "Carlos", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1505.00482", "submitter": "Larry Wasserman", "authors": "Martin Azizyan, Yen-Chi Chen, Aarti Singh and Larry Wasserman", "title": "Risk Bounds For Mode Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density mode clustering is a nonparametric clustering method. The clusters\nare the basins of attraction of the modes of a density estimator. We study the\nrisk of mode-based clustering. We show that the clustering risk over the\ncluster cores --- the regions where the density is high --- is very small even\nin high dimensions. And under a low noise condition, the overall cluster risk\nis small even beyond the cores, in high dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 21:46:42 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Azizyan", "Martin", ""], ["Chen", "Yen-Chi", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1505.00521", "submitter": "Wojciech Zaremba", "authors": "Wojciech Zaremba and Ilya Sutskever", "title": "Reinforcement Learning Neural Turing Machines - Revised", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neural Turing Machine (NTM) is more expressive than all previously\nconsidered models because of its external memory. It can be viewed as a broader\neffort to use abstract external Interfaces and to learn a parametric model that\ninteracts with them.\n  The capabilities of a model can be extended by providing it with proper\nInterfaces that interact with the world. These external Interfaces include\nmemory, a database, a search engine, or a piece of software such as a theorem\nverifier. Some of these Interfaces are provided by the developers of the model.\nHowever, many important existing Interfaces, such as databases and search\nengines, are discrete.\n  We examine feasibility of learning models to interact with discrete\nInterfaces. We investigate the following discrete Interfaces: a memory Tape, an\ninput Tape, and an output Tape. We use a Reinforcement Learning algorithm to\ntrain a neural network that interacts with such Interfaces to solve simple\nalgorithmic tasks. Our Interfaces are expressive enough to make our model\nTuring complete.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 04:14:54 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 19:37:59 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 06:35:48 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Zaremba", "Wojciech", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1505.00553", "submitter": "Naumaan Nayyar", "authors": "Naumaan Nayyar, Dileep Kalathil and Rahul Jain", "title": "On Regret-Optimal Learning in Decentralized Multi-player Multi-armed\n  Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning in single-player and multiplayer\nmultiarmed bandit models. Bandit problems are classes of online learning\nproblems that capture exploration versus exploitation tradeoffs. In a\nmultiarmed bandit model, players can pick among many arms, and each play of an\narm generates an i.i.d. reward from an unknown distribution. The objective is\nto design a policy that maximizes the expected reward over a time horizon for a\nsingle player setting and the sum of expected rewards for the multiplayer\nsetting. In the multiplayer setting, arms may give different rewards to\ndifferent players. There is no separate channel for coordination among the\nplayers. Any attempt at communication is costly and adds to regret. We propose\ntwo decentralizable policies, $\\tt E^3$ ($\\tt E$-$\\tt cubed$) and $\\tt\nE^3$-$\\tt TS$, that can be used in both single player and multiplayer settings.\nThese policies are shown to yield expected regret that grows at most as\nO($\\log^{1+\\epsilon} T$). It is well known that $\\log T$ is the lower bound on\nthe rate of growth of regret even in a centralized case. The proposed\nalgorithms improve on prior work where regret grew at O($\\log^2 T$). More\nfundamentally, these policies address the question of additional cost incurred\nin decentralized online learning, suggesting that there is at most an\n$\\epsilon$-factor cost in terms of order of regret. This solves a problem of\nrelevance in many domains and had been open for a while.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 08:22:04 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 20:23:38 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Nayyar", "Naumaan", ""], ["Kalathil", "Dileep", ""], ["Jain", "Rahul", ""]]}, {"id": "1505.00641", "submitter": "Immanuel Bayer", "authors": "Immanuel Bayer", "title": "fastFM: A Library for Factorization Machines", "comments": "Source Code is available at https://github.com/ibayer/fastFM", "journal-ref": "Journal of Machine Learning Research 17, pp. 1-5 (2016)", "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization Machines (FM) are only used in a narrow range of applications\nand are not part of the standard toolbox of machine learning models. This is a\npity, because even though FMs are recognized as being very successful for\nrecommender system type applications they are a general model to deal with\nsparse and high dimensional features. Our Factorization Machine implementation\nprovides easy access to many solvers and supports regression, classification\nand ranking tasks. Such an implementation simplifies the use of FM's for a wide\nfield of applications. This implementation has the potential to improve our\nunderstanding of the FM model and drive new development.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 14:06:11 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 18:43:34 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 14:25:55 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Bayer", "Immanuel", ""]]}, {"id": "1505.00662", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Optimal Learning via the Fourier Transform for Sums of Independent\n  Integer Random Variables", "comments": "Main differences from v1: Changed title and restructured\n  introduction. Added new sample optimal algorithm. Generalized sample lower\n  bound for any value of k", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the structure and learnability of sums of independent integer random\nvariables (SIIRVs). For $k \\in \\mathbb{Z}_{+}$, a $k$-SIIRV of order $n \\in\n\\mathbb{Z}_{+}$ is the probability distribution of the sum of $n$ independent\nrandom variables each supported on $\\{0, 1, \\dots, k-1\\}$. We denote by ${\\cal\nS}_{n,k}$ the set of all $k$-SIIRVs of order $n$.\n  In this paper, we tightly characterize the sample and computational\ncomplexity of learning $k$-SIIRVs. More precisely, we design a computationally\nefficient algorithm that uses $\\widetilde{O}(k/\\epsilon^2)$ samples, and learns\nan arbitrary $k$-SIIRV within error $\\epsilon,$ in total variation distance.\nMoreover, we show that the {\\em optimal} sample complexity of this learning\nproblem is $\\Theta((k/\\epsilon^2)\\sqrt{\\log(1/\\epsilon)}).$ Our algorithm\nproceeds by learning the Fourier transform of the target $k$-SIIRV in its\neffective support. Its correctness relies on the {\\em approximate sparsity} of\nthe Fourier transform of $k$-SIIRVs -- a structural property that we establish,\nroughly stating that the Fourier transform of $k$-SIIRVs has small magnitude\noutside a small set.\n  Along the way we prove several new structural results about $k$-SIIRVs. As\none of our main structural contributions, we give an efficient algorithm to\nconstruct a sparse {\\em proper} $\\epsilon$-cover for ${\\cal S}_{n,k},$ in total\nvariation distance. We also obtain a novel geometric characterization of the\nspace of $k$-SIIRVs. Our characterization allows us to prove a tight lower\nbound on the size of $\\epsilon$-covers for ${\\cal S}_{n,k}$, and is the key\ningredient in our tight sample complexity lower bound.\n  Our approach of exploiting the sparsity of the Fourier transform in\ndistribution learning is general, and has recently found additional\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 14:48:01 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 07:03:28 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1505.00670", "submitter": "Hoo Chang Shin", "authors": "Hoo-Chang Shin, Le Lu, Lauren Kim, Ari Seff, Jianhua Yao, Ronald M.\n  Summers", "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database\n  for Automated Image Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite tremendous progress in computer vision, there has not been an attempt\nfor machine learning on very large-scale medical image databases. We present an\ninterleaved text/image deep learning system to extract and mine the semantic\ninteractions of radiology images and reports from a national research\nhospital's Picture Archiving and Communication System. With natural language\nprocessing, we mine a collection of representative ~216K two-dimensional key\nimages selected by clinicians for diagnostic reference, and match the images\nwith their descriptions in an automated manner. Our system interleaves between\nunsupervised learning and supervised learning on document- and sentence-level\ntext collections, to generate semantic labels and to predict them given an\nimage. Given an image of a patient scan, semantic topics in radiology levels\nare predicted, and associated key-words are generated. Also, a number of\nfrequent disease types are detected as present or absent, to provide more\nspecific interpretation of a patient scan. This shows the potential of\nlarge-scale learning and prediction in electronic patient records available in\nmost modern clinical institutions.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 15:05:59 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Shin", "Hoo-Chang", ""], ["Lu", "Le", ""], ["Kim", "Lauren", ""], ["Seff", "Ari", ""], ["Yao", "Jianhua", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1505.00824", "submitter": "Eva Dyer", "authors": "Eva L. Dyer, Tom A. Goldstein, Raajen Patel, Konrad P. Kording, and\n  Richard G. Baraniuk", "title": "Self-Expressive Decompositions for Matrix Approximation and Clustering", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-aware methods for dimensionality reduction and matrix decomposition aim\nto find low-dimensional structure in a collection of data. Classical approaches\ndiscover such structure by learning a basis that can efficiently express the\ncollection. Recently, \"self expression\", the idea of using a small subset of\ndata vectors to represent the full collection, has been developed as an\nalternative to learning. Here, we introduce a scalable method for computing\nsparse SElf-Expressive Decompositions (SEED). SEED is a greedy method that\nconstructs a basis by sequentially selecting incoherent vectors from the\ndataset. After forming a basis from a subset of vectors in the dataset, SEED\nthen computes a sparse representation of the dataset with respect to this\nbasis. We develop sufficient conditions under which SEED exactly represents low\nrank matrices and vectors sampled from a unions of independent subspaces. We\nshow how SEED can be used in applications ranging from matrix approximation and\ndenoising to clustering, and apply it to numerous real-world datasets. Our\nresults demonstrate that SEED is an attractive low-complexity alternative to\nother sparse matrix factorization approaches such as sparse PCA and\nself-expressive methods for clustering.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 21:56:54 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Dyer", "Eva L.", ""], ["Goldstein", "Tom A.", ""], ["Patel", "Raajen", ""], ["Kording", "Konrad P.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1505.00835", "submitter": "Georg Martius", "authors": "Ralf Der and Georg Martius", "title": "A novel plasticity rule can explain the development of sensorimotor\n  intelligence", "comments": "18 pages, 5 figures, 7 videos", "journal-ref": "PNAS November 10, 2015 vol. 112 no. 45 E6224-E6232", "doi": "10.1073/pnas.1508400112", "report-no": null, "categories": "cs.RO cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding autonomous behavior in the nervous system is a fundamental\nchallenge for neuroscience. In particular, the self-organized behavioral\ndevelopment provides more questions than answers. Are there special functional\nunits for curiosity, motivation, and creativity? This paper argues that these\nfeatures can be grounded in synaptic plasticity itself, without requiring any\nhigher level constructs. We propose differential extrinsic plasticity (DEP) as\na new synaptic rule for self-learning systems and apply it to a number of\ncomplex robotic systems as a test case. Without specifying any purpose or goal,\nseemingly purposeful and adaptive behavior is developed, displaying a certain\nlevel of sensorimotor intelligence. These surprising results require no system\nspecific modifications of the DEP rule but arise rather from the underlying\nmechanism of spontaneous symmetry breaking due to the tight\nbrain-body-environment coupling. The new synaptic rule is biologically\nplausible and it would be an interesting target for a neurobiolocal\ninvestigation. We also argue that this neuronal mechanism may have been a\ncatalyst in natural evolution.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 22:48:25 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Der", "Ralf", ""], ["Martius", "Georg", ""]]}, {"id": "1505.00853", "submitter": "Bing Xu", "authors": "Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li", "title": "Empirical Evaluation of Rectified Activations in Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the performance of different types of rectified\nactivation functions in convolutional neural network: standard rectified linear\nunit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified\nlinear unit (PReLU) and a new randomized leaky rectified linear units (RReLU).\nWe evaluate these activation function on standard image classification task.\nOur experiments suggest that incorporating a non-zero slope for negative part\nin rectified activation units could consistently improve the results. Thus our\nfindings are negative on the common belief that sparsity is the key of good\nperformance in ReLU. Moreover, on small scale dataset, using deterministic\nnegative slope or learning it are both prone to overfitting. They are not as\neffective as using their randomized counterpart. By using RReLU, we achieved\n75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 01:16:39 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 06:58:14 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Xu", "Bing", ""], ["Wang", "Naiyan", ""], ["Chen", "Tianqi", ""], ["Li", "Mu", ""]]}, {"id": "1505.00855", "submitter": "Babak Saleh", "authors": "Babak Saleh and Ahmed Elgammal", "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right\n  Metric on The Right Feature", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, the number of fine-art collections that are digitized\nand publicly available has been growing rapidly. With the availability of such\nlarge collections of digitized artworks comes the need to develop multimedia\nsystems to archive and retrieve this pool of data. Measuring the visual\nsimilarity between artistic items is an essential step for such multimedia\nsystems, which can benefit more high-level multimedia tasks. In order to model\nthis similarity between paintings, we should extract the appropriate visual\nfeatures for paintings and find out the best approach to learn the similarity\nmetric based on these features. We investigate a comprehensive list of visual\nfeatures and metric learning approaches to learn an optimized similarity\nmeasure between paintings. We develop a machine that is able to make\naesthetic-related semantic-level judgments, such as predicting a painting's\nstyle, genre, and artist, as well as providing similarity measures optimized\nbased on the knowledge available in the domain of art historical\ninterpretation. Our experiments show the value of using this similarity measure\nfor the aforementioned prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 01:25:26 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1505.00870", "submitter": "Damek Davis", "authors": "Damek Davis", "title": "An $O(n\\log(n))$ Algorithm for Projecting Onto the Ordered Weighted\n  $\\ell_1$ Norm Ball", "comments": "1 Figures, 1 table, 14 pages, Example added to appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ordered weighted $\\ell_1$ (OWL) norm is a newly developed generalization\nof the Octogonal Shrinkage and Clustering Algorithm for Regression (OSCAR)\nnorm. This norm has desirable statistical properties and can be used to perform\nsimultaneous clustering and regression. In this paper, we show how to compute\nthe projection of an $n$-dimensional vector onto the OWL norm ball in\n$O(n\\log(n))$ operations. In addition, we illustrate the performance of our\nalgorithm on a synthetic regression test.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 03:10:07 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 02:11:38 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2015 17:34:21 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Davis", "Damek", ""]]}, {"id": "1505.00908", "submitter": "Ludovic Denoyer", "authors": "Aur\\'elia L\\'eon and Ludovic Denoyer", "title": "Reinforced Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": "Accepted as a poster at EWRL 2015", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to speed-up classification models when facing a large number of\ncategories, one usual approach consists in organizing the categories in a\nparticular structure, this structure being then used as a way to speed-up the\nprediction computation. This is for example the case when using\nerror-correcting codes or even hierarchies of categories. But in the majority\nof approaches, this structure is chosen \\textit{by hand}, or during a\npreliminary step, and not integrated in the learning process. We propose a new\nmodel called Reinforced Decision Tree which simultaneously learns how to\norganize categories in a tree structure and how to classify any input based on\nthis structure. This approach keeps the advantages of existing techniques (low\ninference complexity) but allows one to build efficient classifiers in one\nlearning step. The learning algorithm is inspired by reinforcement learning and\npolicy-gradient techniques which allows us to integrate the two steps (building\nthe tree, and learning the classifier) in one single algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 07:58:40 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["L\u00e9on", "Aur\u00e9lia", ""], ["Denoyer", "Ludovic", ""]]}, {"id": "1505.01221", "submitter": "Marius Lindauer", "authors": "Frank Hutter and Marius Lindauer and Adrian Balint and Sam Bayless and\n  Holger Hoos and Kevin Leyton-Brown", "title": "The Configurable SAT Solver Challenge (CSSC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that different solution strategies work well for different\ntypes of instances of hard combinatorial problems. As a consequence, most\nsolvers for the propositional satisfiability problem (SAT) expose parameters\nthat allow them to be customized to a particular family of instances. In the\ninternational SAT competition series, these parameters are ignored: solvers are\nrun using a single default parameter setting (supplied by the authors) for all\nbenchmark instances in a given track. While this competition format rewards\nsolvers with robust default settings, it does not reflect the situation faced\nby a practitioner who only cares about performance on one particular\napplication and can invest some time into tuning solver parameters for this\napplication. The new Configurable SAT Solver Competition (CSSC) compares\nsolvers in this latter setting, scoring each solver by the performance it\nachieved after a fully automated configuration step. This article describes the\nCSSC in more detail, and reports the results obtained in its two instantiations\nso far, CSSC 2013 and 2014.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 23:39:24 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 08:48:53 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Hutter", "Frank", ""], ["Lindauer", "Marius", ""], ["Balint", "Adrian", ""], ["Bayless", "Sam", ""], ["Hoos", "Holger", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1505.01345", "submitter": "Mohnish Chakravarti", "authors": "Mohnish Chakravarti, Tanay Kothari", "title": "A Comprehensive Study On The Applications Of Machine Learning For\n  Diagnosis Of Cancer", "comments": "18 pages, 11 figures, 3 tables, 1 equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collectively, lung cancer, breast cancer and melanoma was diagnosed in over\n535,340 people out of which, 209,400 deaths were reported [13]. It is estimated\nthat over 600,000 people will be diagnosed with these forms of cancer in 2015.\nMost of the deaths from lung cancer, breast cancer and melanoma result due to\nlate detection. All of these cancers, if detected early, are 100% curable. In\nthis study, we develop and evaluate algorithms to diagnose Breast cancer,\nMelanoma, and Lung cancer. In the first part of the study, we employed a\nnormalised Gradient Descent and an Artificial Neural Network to diagnose breast\ncancer with an overall accuracy of 91% and 95% respectively. In the second part\nof the study, an artificial neural network coupled with image processing and\nanalysis algorithms was employed to achieve an overall accuracy of 93% A naive\nmobile based application that allowed people to take diagnostic tests on their\nphones was developed. Finally, a Support Vector Machine algorithm incorporating\nimage processing and image analysis algorithms was developed to diagnose lung\ncancer with an accuracy of 94%. All of the aforementioned systems had very low\nfalse positive and false negative rates. We are developing an online network\nthat incorporates all of these systems and allows people to collaborate\nglobally.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 12:52:48 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 08:38:47 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 06:07:55 GMT"}, {"version": "v4", "created": "Mon, 28 Sep 2015 06:20:17 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Chakravarti", "Mohnish", ""], ["Kothari", "Tanay", ""]]}, {"id": "1505.01371", "submitter": "Shaobo Lin", "authors": "Shaobo Lin, Yao Wang and Lin Xu", "title": "Re-scale boosting for regression and classification", "comments": "13 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is a learning scheme that combines weak prediction rules to produce\na strong composite estimator, with the underlying intuition that one can obtain\naccurate prediction rules by combining \"rough\" ones. Although boosting is\nproved to be consistent and overfitting-resistant, its numerical convergence\nrate is relatively slow. The aim of this paper is to develop a new boosting\nstrategy, called the re-scale boosting (RBoosting), to accelerate the numerical\nconvergence rate and, consequently, improve the learning performance of\nboosting. Our studies show that RBoosting possesses the almost optimal\nnumerical convergence rate in the sense that, up to a logarithmic factor, it\ncan reach the minimax nonlinear approximation rate. We then use RBoosting to\ntackle both the classification and regression problems, and deduce a tight\ngeneralization error estimate. The theoretical and experimental results show\nthat RBoosting outperforms boosting in terms of generalization.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 14:05:08 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Lin", "Shaobo", ""], ["Wang", "Yao", ""], ["Xu", "Lin", ""]]}, {"id": "1505.01419", "submitter": "Ziqi Liu", "authors": "Ziqi Liu, Yu-Xiang Wang, Alexander J. Smola", "title": "Fast Differentially Private Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially private collaborative filtering is a challenging task, both in\nterms of accuracy and speed. We present a simple algorithm that is provably\ndifferentially private, while offering good performance, using a novel\nconnection of differential privacy to Bayesian posterior sampling via\nStochastic Gradient Langevin Dynamics. Due to its simplicity the algorithm\nlends itself to efficient implementation. By careful systems design and by\nexploiting the power law behavior of the data to maximize CPU cache bandwidth\nwe are able to generate 1024 dimensional models at a rate of 8.5 million\nrecommendations per second on a single PC.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 16:18:06 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 06:32:40 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Liu", "Ziqi", ""], ["Wang", "Yu-Xiang", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1505.01462", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh,\n  Kannan Ramchandran, Martin J. Wainwright", "title": "Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology\n  Dependence", "comments": "39 pages, 5 figures. Significant extension of arXiv:1406.6618", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the form of pairwise comparisons arises in many domains, including\npreference elicitation, sporting competitions, and peer grading among others.\nWe consider parametric ordinal models for such pairwise comparison data\ninvolving a latent vector $w^* \\in \\mathbb{R}^d$ that represents the\n\"qualities\" of the $d$ items being compared; this class of models includes the\ntwo most widely used parametric models--the Bradley-Terry-Luce (BTL) and the\nThurstone models. Working within a standard minimax framework, we provide tight\nupper and lower bounds on the optimal error in estimating the quality score\nvector $w^*$ under this class of models. The bounds depend on the topology of\nthe comparison graph induced by the subset of pairs being compared via its\nLaplacian spectrum. Thus, in settings where the subset of pairs may be chosen,\nour results provide principled guidelines for making this choice. Finally, we\ncompare these error rates to those under cardinal measurement models and show\nthat the error rates in the ordinal and cardinal settings have identical\nscalings apart from constant pre-factors.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 19:04:09 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Bradley", "Joseph", ""], ["Parekh", "Abhay", ""], ["Ramchandran", "Kannan", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1505.01504", "submitter": "Hui Jiang", "authors": "Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai", "title": "A Fixed-Size Encoding Method for Variable-Length Sequences with its\n  Application to Neural Network Language Models", "comments": "7 pages, 4 figures, Technical report (A shorter version will appear\n  in ACL 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the new fixed-size ordinally-forgetting encoding\n(FOFE) method, which can almost uniquely encode any variable-length sequence of\nwords into a fixed-size representation. FOFE can model the word order in a\nsequence using a simple ordinally-forgetting mechanism according to the\npositions of words. In this work, we have applied FOFE to feedforward neural\nnetwork language models (FNN-LMs). Experimental results have shown that without\nusing any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform\nnot only the standard fixed-input FNN-LMs but also the popular RNN-LMs.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 20:14:25 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 18:41:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Zhang", "Shiliang", ""], ["Jiang", "Hui", ""], ["Xu", "Mingbin", ""], ["Hou", "Junfeng", ""], ["Dai", "Lirong", ""]]}, {"id": "1505.01576", "submitter": "Bharath Sankaran", "authors": "Bharath Sankaran, Marjan Ghazvininejad, Xinran He, David Kale, Liron\n  Cohen", "title": "Learning and Optimization with Submodular Functions", "comments": "Tech Report - USC Computer Science CS-599, Convex and Combinatorial\n  Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many naturally occurring optimization problems one needs to ensure that\nthe definition of the optimization problem lends itself to solutions that are\ntractable to compute. In cases where exact solutions cannot be computed\ntractably, it is beneficial to have strong guarantees on the tractable\napproximate solutions. In order operate under these criterion most optimization\nproblems are cast under the umbrella of convexity or submodularity. In this\nreport we will study design and optimization over a common class of functions\ncalled submodular functions. Set functions, and specifically submodular set\nfunctions, characterize a wide variety of naturally occurring optimization\nproblems, and the property of submodularity of set functions has deep\ntheoretical consequences with wide ranging applications. Informally, the\nproperty of submodularity of set functions concerns the intuitive \"principle of\ndiminishing returns. This property states that adding an element to a smaller\nset has more value than adding it to a larger set. Common examples of\nsubmodular monotone functions are entropies, concave functions of cardinality,\nand matroid rank functions; non-monotone examples include graph cuts, network\nflows, and mutual information.\n  In this paper we will review the formal definition of submodularity; the\noptimization of submodular functions, both maximization and minimization; and\nfinally discuss some applications in relation to learning and reasoning using\nsubmodular functions.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 04:04:02 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Sankaran", "Bharath", ""], ["Ghazvininejad", "Marjan", ""], ["He", "Xinran", ""], ["Kale", "David", ""], ["Cohen", "Liron", ""]]}, {"id": "1505.01621", "submitter": "Anupriya Gogna", "authors": "Anupriya Gogna, Angshul Majumdar", "title": "Blind Compressive Sensing Framework for Collaborative Filtering", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works based on latent factor models have focused on representing the\nrating matrix as a product of user and item latent factor matrices, both being\ndense. Latent (factor) vectors define the degree to which a trait is possessed\nby an item or the affinity of user towards that trait. A dense user matrix is a\nreasonable assumption as each user will like/dislike a trait to certain extent.\nHowever, any item will possess only a few of the attributes and never all.\nHence, the item matrix should ideally have a sparse structure rather than a\ndense one as formulated in earlier works. Therefore we propose to factor the\nratings matrix into a dense user matrix and a sparse item matrix which leads us\nto the Blind Compressed Sensing (BCS) framework. We derive an efficient\nalgorithm for solving the BCS problem based on Majorization Minimization (MM)\ntechnique. Our proposed approach is able to achieve significantly higher\naccuracy and shorter run times as compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 08:19:05 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Gogna", "Anupriya", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1505.01625", "submitter": "Meryem Simsek Dr.", "authors": "Meryem Simsek, Mehdi Bennis, Ismail G\\\"uvenc", "title": "Context-Aware Mobility Management in HetNets: A Reinforcement Learning\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of small cell deployments in heterogeneous network (HetNet)\nenvironments is expected to be a key feature of 4G networks and beyond, and\nessential for providing higher user throughput and cell-edge coverage. However,\ndue to different coverage sizes of macro and pico base stations (BSs), such a\nparadigm shift introduces additional requirements and challenges in dense\nnetworks. Among these challenges is the handover performance of user equipment\n(UEs), which will be impacted especially when high velocity UEs traverse\npicocells. In this paper, we propose a coordination-based and context-aware\nmobility management (MM) procedure for small cell networks using tools from\nreinforcement learning. Here, macro and pico BSs jointly learn their long-term\ntraffic loads and optimal cell range expansion, and schedule their UEs based on\ntheir velocities and historical rates (exchanged among tiers). The proposed\napproach is shown to not only outperform the classical MM in terms of UE\nthroughput, but also to enable better fairness. In average, a gain of up to\n80\\% is achieved for UE throughput, while the handover failure probability is\nreduced up to a factor of three by the proposed learning based MM approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 08:45:45 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Simsek", "Meryem", ""], ["Bennis", "Mehdi", ""], ["G\u00fcvenc", "Ismail", ""]]}, {"id": "1505.01658", "submitter": "Luis Torgo", "authors": "Paula Branco and Luis Torgo and Rita Ribeiro", "title": "A Survey of Predictive Modelling under Imbalanced Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world data mining applications involve obtaining predictive models\nusing data sets with strongly imbalanced distributions of the target variable.\nFrequently, the least common values of this target variable are associated with\nevents that are highly relevant for end users (e.g. fraud detection, unusual\nreturns on stock markets, anticipation of catastrophes, etc.). Moreover, the\nevents may have different costs and benefits, which when associated with the\nrarity of some of them on the available training data creates serious problems\nto predictive modelling techniques. This paper presents a survey of existing\ntechniques for handling these important applications of predictive analytics.\nAlthough most of the existing work addresses classification tasks (nominal\ntarget variables), we also describe methods designed to handle similar problems\nwithin regression tasks (numeric target variables). In this survey we discuss\nthe main challenges raised by imbalanced distributions, describe the main\napproaches to these problems, propose a taxonomy of these methods and refer to\nsome related problems within predictive modelling.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 10:44:57 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 17:13:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Branco", "Paula", ""], ["Torgo", "Luis", ""], ["Ribeiro", "Rita", ""]]}, {"id": "1505.01728", "submitter": "Yamuna Prasad", "authors": "Yamuna Prasad, K. K. Biswas", "title": "Integrating K-means with Quadratic Programming Feature Selection", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several data mining problems are characterized by data in high dimensions.\nOne of the popular ways to reduce the dimensionality of the data is to perform\nfeature selection, i.e, select a subset of relevant and non-redundant features.\nRecently, Quadratic Programming Feature Selection (QPFS) has been proposed\nwhich formulates the feature selection problem as a quadratic program. It has\nbeen shown to outperform many of the existing feature selection methods for a\nvariety of applications. Though, better than many existing approaches, the\nrunning time complexity of QPFS is cubic in the number of features, which can\nbe quite computationally expensive even for moderately sized datasets. In this\npaper we propose a novel method for feature selection by integrating k-means\nclustering with QPFS. The basic variant of our approach runs k-means to bring\ndown the number of features which need to be passed on to QPFS. We then enhance\nthis idea, wherein we gradually refine the feature space from a very coarse\nclustering to a fine-grained one, by interleaving steps of QPFS with k-means\nclustering. Every step of QPFS helps in identifying the clusters of irrelevant\nfeatures (which can then be thrown away), whereas every step of k-means further\nrefines the clusters which are potentially relevant. We show that our iterative\nrefinement of clusters is guaranteed to converge. We provide bounds on the\nnumber of distance computations involved in the k-means algorithm. Further,\neach QPFS run is now cubic in number of clusters, which can be much smaller\nthan actual number of features. Experiments on eight publicly available\ndatasets show that our approach gives significant computational gains (both in\ntime and memory), over standard QPFS as well as other state of the art feature\nselection methods, even while improving the overall accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 14:45:11 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 18:06:36 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Prasad", "Yamuna", ""], ["Biswas", "K. K.", ""]]}, {"id": "1505.01749", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Nikos Komodakis", "title": "Object detection via a multi-region & semantic segmentation-aware CNN\n  model", "comments": "Extended technical report -- short version to appear at ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an object detection system that relies on a multi-region deep\nconvolutional neural network (CNN) that also encodes semantic\nsegmentation-aware features. The resulting CNN-based representation aims at\ncapturing a diverse set of discriminative appearance factors and exhibits\nlocalization sensitivity that is essential for accurate object localization. We\nexploit the above properties of our recognition module by integrating it on an\niterative localization mechanism that alternates between scoring a box proposal\nand refining its location with a deep CNN regression model. Thanks to the\nefficient use of our modules, we detect objects with very high localization\naccuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we\nachieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published\nwork by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 15:42:07 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 16:49:44 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 22:24:42 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1505.01802", "submitter": "Nagarajan Natarajan", "authors": "Nagarajan Natarajan, Oluwasanmi Koyejo, Pradeep Ravikumar, Inderjit S.\n  Dhillon", "title": "Optimal Decision-Theoretic Classification Using Non-Decomposable\n  Performance Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general theoretical analysis of expected out-of-sample utility,\nalso referred to as decision-theoretic classification, for non-decomposable\nbinary classification metrics such as F-measure and Jaccard coefficient. Our\nkey result is that the expected out-of-sample utility for many performance\nmetrics is provably optimized by a classifier which is equivalent to a signed\nthresholding of the conditional probability of the positive class. Our analysis\nbridges a gap in the literature on binary classification, revealed in light of\nrecent results for non-decomposable metrics in population utility maximization\nstyle classification. Our results identify checkable properties of a\nperformance metric which are sufficient to guarantee a probability ranking\nprinciple. We propose consistent estimators for optimal expected out-of-sample\nclassification. As a consequence of the probability ranking principle,\ncomputational requirements can be reduced from exponential to cubic complexity\nin the general case, and further reduced to quadratic complexity in special\ncases. We provide empirical results on simulated and benchmark datasets\nevaluating the performance of the proposed algorithms for decision-theoretic\nclassification and comparing them to baseline and state-of-the-art methods in\npopulation utility maximization for non-decomposable metrics.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 18:19:24 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Natarajan", "Nagarajan", ""], ["Koyejo", "Oluwasanmi", ""], ["Ravikumar", "Pradeep", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1505.01809", "submitter": "Jacob Devlin", "authors": "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong\n  He, Geoffrey Zweig, Margaret Mitchell", "title": "Language Models for Image Captioning: The Quirks and What Works", "comments": "See http://research.microsoft.com/en-us/projects/image_captioning for\n  project information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two recent approaches have achieved state-of-the-art results in image\ncaptioning. The first uses a pipelined process where a set of candidate words\nis generated by a convolutional neural network (CNN) trained on images, and\nthen a maximum entropy (ME) language model is used to arrange these words into\na coherent sentence. The second uses the penultimate activation layer of the\nCNN as input to a recurrent neural network (RNN) that then generates the\ncaption sequence. In this paper, we compare the merits of these different\nlanguage modeling approaches for the first time by using the same\nstate-of-the-art CNN as input. We examine issues in the different approaches,\nincluding linguistic irregularities, caption repetition, and data set overlap.\nBy combining key aspects of the ME and RNN methods, we achieve a new record\nperformance over previously published results on the benchmark COCO dataset.\nHowever, the gains we see in BLEU do not translate to human judgments.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 18:36:14 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 22:10:49 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 22:03:40 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Devlin", "Jacob", ""], ["Cheng", "Hao", ""], ["Fang", "Hao", ""], ["Gupta", "Saurabh", ""], ["Deng", "Li", ""], ["He", "Xiaodong", ""], ["Zweig", "Geoffrey", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1505.01866", "submitter": "K. V. Rashmi", "authors": "K. V. Rashmi and Ran Gilad-Bachrach", "title": "DART: Dropouts meet Multiple Additive Regression Trees", "comments": "AIStats 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Additive Regression Trees (MART), an ensemble model of boosted\nregression trees, is known to deliver high prediction accuracy for diverse\ntasks, and it is widely used in practice. However, it suffers an issue which we\ncall over-specialization, wherein trees added at later iterations tend to\nimpact the prediction of only a few instances, and make negligible contribution\ntowards the remaining instances. This negatively affects the performance of the\nmodel on unseen data, and also makes the model over-sensitive to the\ncontributions of the few, initially added tress. We show that the commonly used\ntool to address this issue, that of shrinkage, alleviates the problem only to a\ncertain extent and the fundamental issue of over-specialization still remains.\nIn this work, we explore a different approach to address the problem that of\nemploying dropouts, a tool that has been recently proposed in the context of\nlearning deep neural networks. We propose a novel way of employing dropouts in\nMART, resulting in the DART algorithm. We evaluate DART on ranking, regression\nand classification tasks, using large scale, publicly available datasets, and\nshow that DART outperforms MART in each of the tasks, with a significant\nmargin. We also show that DART overcomes the issue of over-specialization to a\nconsiderable extent.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 20:38:48 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Rashmi", "K. V.", ""], ["Gilad-Bachrach", "Ran", ""]]}, {"id": "1505.01918", "submitter": "Michael Katehakis", "authors": "Wesley Cowan and Michael N. Katehakis", "title": "An Asymptotically Optimal Policy for Uniform Bandits of Unknown Support", "comments": "arXiv admin note: text overlap with arXiv:1504.05823", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of a controller sampling sequentially from a finite\nnumber of $N \\geq 2$ populations, specified by random variables $X^i_k$, $ i =\n1,\\ldots , N,$ and $k = 1, 2, \\ldots$; where $X^i_k$ denotes the outcome from\npopulation $i$ the $k^{th}$ time it is sampled. It is assumed that for each\nfixed $i$, $\\{ X^i_k \\}_{k \\geq 1}$ is a sequence of i.i.d. uniform random\nvariables over some interval $[a_i, b_i]$, with the support (i.e., $a_i, b_i$)\nunknown to the controller. The objective is to have a policy $\\pi$ for\ndeciding, based on available data, from which of the $N$ populations to sample\nfrom at any time $n=1,2,\\ldots$ so as to maximize the expected sum of outcomes\nof $n$ samples or equivalently to minimize the regret due to lack on\ninformation of the parameters $\\{ a_i \\}$ and $\\{ b_i \\}$. In this paper, we\npresent a simple inflated sample mean (ISM) type policy that is asymptotically\noptimal in the sense of its regret achieving the asymptotic lower bound of\nBurnetas and Katehakis (1996). Additionally, finite horizon regret bounds are\ngiven.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 03:15:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 16:46:34 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 12:07:17 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Cowan", "Wesley", ""], ["Katehakis", "Michael N.", ""]]}, {"id": "1505.02000", "submitter": "Matthew Lai", "authors": "Matthew Lai", "title": "Deep Learning for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an overview of the current state of the art deep\nlearning architectures and optimisation techniques, and uses the ADNI\nhippocampus MRI dataset as an example to compare the effectiveness and\nefficiency of different convolutional architectures on the task of patch-based\n3-dimensional hippocampal segmentation, which is important in the diagnosis of\nAlzheimer's Disease. We found that a slightly unconventional \"stacked 2D\"\napproach provides much better classification performance than simple 2D patches\nwithout requiring significantly more computational power. We also examined the\npopular \"tri-planar\" approach used in some recently published studies, and\nfound that it provides much better results than the 2D approaches, but also\nwith a moderate increase in computational power requirement. Finally, we\nevaluated a full 3D convolutional architecture, and found that it provides\nmarginally better results than the tri-planar approach, but at the cost of a\nvery significant increase in computational power requirement.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 11:35:53 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Lai", "Matthew", ""]]}, {"id": "1505.02074", "submitter": "Mengye Ren", "authors": "Mengye Ren, Ryan Kiros, Richard Zemel", "title": "Exploring Models and Data for Image Question Answering", "comments": "12 pages. Conference paper at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to address the problem of image-based question-answering (QA)\nwith new models and datasets. In our work, we propose to use neural networks\nand visual semantic embeddings, without intermediate stages such as object\ndetection and image segmentation, to predict answers to simple questions about\nimages. Our model performs 1.8 times better than the only published results on\nan existing image QA dataset. We also present a question generation algorithm\nthat converts image descriptions, which are widely available, into QA form. We\nused this algorithm to produce an order-of-magnitude larger dataset, with more\nevenly distributed answers. A suite of baseline results on this new dataset are\nalso presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 15:59:44 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 19:55:07 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2015 06:44:44 GMT"}, {"version": "v4", "created": "Sun, 29 Nov 2015 22:45:12 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Ren", "Mengye", ""], ["Kiros", "Ryan", ""], ["Zemel", "Richard", ""]]}, {"id": "1505.02137", "submitter": "Mohamed Amer", "authors": "Mohamed R. Amer, Behjat Siddiquie, Amir Tamrakar, David A. Salter,\n  Brian Lande, Darius Mehri and Ajay Divakaran", "title": "Human Social Interaction Modeling Using Temporal Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present a novel approach to computational modeling of social interactions\nbased on modeling of essential social interaction predicates (ESIPs) such as\njoint attention and entrainment. Based on sound social psychological theory and\nmethodology, we collect a new \"Tower Game\" dataset consisting of audio-visual\ncapture of dyadic interactions labeled with the ESIPs. We expect this dataset\nto provide a new avenue for research in computational social interaction\nmodeling. We propose a novel joint Discriminative Conditional Restricted\nBoltzmann Machine (DCRBM) model that combines a discriminative component with\nthe generative power of CRBMs. Such a combination enables us to uncover\nactionable constituents of the ESIPs in two steps. First, we train the DCRBM\nmodel on the labeled data and get accurate (76\\%-49\\% across various ESIPs)\ndetection of the predicates. Second, we exploit the generative capability of\nDCRBMs to activate the trained model so as to generate the lower-level data\ncorresponding to the specific ESIP that closely matches the actual training\ndata (with mean square error 0.01-0.1 for generating 100 frames). We are thus\nable to decompose the ESIPs into their constituent actionable behaviors. Such a\npurely computational determination of how to establish an ESIP such as\nengagement is unprecedented.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 18:17:56 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 16:05:07 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Amer", "Mohamed R.", ""], ["Siddiquie", "Behjat", ""], ["Tamrakar", "Amir", ""], ["Salter", "David A.", ""], ["Lande", "Brian", ""], ["Mehri", "Darius", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1505.02212", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael M.\n  Mitzenmacher", "title": "Equitability, interval estimation, and statistical power", "comments": "Yakir A. Reshef and David N. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors. This paper, together\n  with arXiv:1505.02212, subsumes arXiv:1408.4908", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG q-bio.QM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For analysis of a high-dimensional dataset, a common approach is to test a\nnull hypothesis of statistical independence on all variable pairs using a\nnon-parametric measure of dependence. However, because this approach attempts\nto identify any non-trivial relationship no matter how weak, it often\nidentifies too many relationships to be useful. What is needed is a way of\nidentifying a smaller set of relationships that merit detailed further\nanalysis.\n  Here we formally present and characterize equitability, a property of\nmeasures of dependence that aims to overcome this challenge. Notionally, an\nequitable statistic is a statistic that, given some measure of noise, assigns\nsimilar scores to equally noisy relationships of different types [Reshef et al.\n2011]. We begin by formalizing this idea via a new object called the\ninterpretable interval, which functions as an interval estimate of the amount\nof noise in a relationship of unknown type. We define an equitable statistic as\none with small interpretable intervals.\n  We then draw on the equivalence of interval estimation and hypothesis testing\nto show that under moderate assumptions an equitable statistic is one that\nyields well powered tests for distinguishing not only between trivial and\nnon-trivial relationships of all kinds but also between non-trivial\nrelationships of different strengths. This means that equitability allows us to\nspecify a threshold relationship strength $x_0$ and to search for relationships\nof all kinds with strength greater than $x_0$. Thus, equitability can be\nthought of as a strengthening of power against independence that enables\nfruitful analysis of data sets with a small number of strong, interesting\nrelationships and a large number of weaker ones. We conclude with a\ndemonstration of how our two equivalent characterizations of equitability can\nbe used to evaluate the equitability of a statistic in practice.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:31:23 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 20:05:17 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02213", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Hilary K. Finucane, Pardis C.\n  Sabeti, Michael M. Mitzenmacher", "title": "Measuring dependence powerfully and equitably", "comments": "Yakir A. Reshef and David N. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors. This paper, together\n  with arXiv:1505.02212, subsumes arXiv:1408.4908. v3 includes new analyses and\n  exposition", "journal-ref": "J.Mach.Learn.Res. 17 (2016), 1-63", "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a high-dimensional data set we often wish to find the strongest\nrelationships within it. A common strategy is to evaluate a measure of\ndependence on every variable pair and retain the highest-scoring pairs for\nfollow-up. This strategy works well if the statistic used is equitable [Reshef\net al. 2015a], i.e., if, for some measure of noise, it assigns similar scores\nto equally noisy relationships regardless of relationship type (e.g., linear,\nexponential, periodic).\n  In this paper, we introduce and characterize a population measure of\ndependence called MIC*. We show three ways that MIC* can be viewed: as the\npopulation value of MIC, a highly equitable statistic from [Reshef et al.\n2011], as a canonical \"smoothing\" of mutual information, and as the supremum of\nan infinite sequence defined in terms of optimal one-dimensional partitions of\nthe marginals of the joint distribution. Based on this theory, we introduce an\nefficient approach for computing MIC* from the density of a pair of random\nvariables, and we define a new consistent estimator MICe for MIC* that is\nefficiently computable. In contrast, there is no known polynomial-time\nalgorithm for computing the original equitable statistic MIC. We show through\nsimulations that MICe has better bias-variance properties than MIC. We then\nintroduce and prove the consistency of a second statistic, TICe, that is a\ntrivial side-product of the computation of MICe and whose goal is powerful\nindependence testing rather than equitability.\n  We show in simulations that MICe and TICe have good equitability and power\nagainst independence respectively. The analyses here complement a more in-depth\nempirical evaluation of several leading measures of dependence [Reshef et al.\n2015b] that shows state-of-the-art performance for MICe and TICe.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:31:54 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 19:51:46 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 18:42:36 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Finucane", "Hilary K.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02214", "submitter": "Yakir Reshef", "authors": "David N. Reshef, Yakir A. Reshef, Pardis C. Sabeti, Michael M.\n  Mitzenmacher", "title": "An Empirical Study of Leading Measures of Dependence", "comments": "David N. Reshef and Yakir A. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors", "journal-ref": "Ann.Appl.Stat. 12 (2018) 123-155", "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exploratory data analysis, we are often interested in identifying\npromising pairwise associations for further analysis while filtering out\nweaker, less interesting ones. This can be accomplished by computing a measure\nof dependence on all variable pairs and examining the highest-scoring pairs,\nprovided the measure of dependence used assigns similar scores to equally noisy\nrelationships of different types. This property, called equitability, is\nformalized in Reshef et al. [2015b]. In addition to equitability, measures of\ndependence can also be assessed by the power of their corresponding\nindependence tests as well as their runtime.\n  Here we present extensive empirical evaluation of the equitability, power\nagainst independence, and runtime of several leading measures of dependence.\nThese include two statistics introduced in Reshef et al. [2015a]: MICe, which\nhas equitability as its primary goal, and TICe, which has power against\nindependence as its goal. Regarding equitability, our analysis finds that MICe\nis the most equitable method on functional relationships in most of the\nsettings we considered, although mutual information estimation proves the most\nequitable at large sample sizes in some specific settings. Regarding power\nagainst independence, we find that TICe, along with Heller and Gorfine's S^DDP,\nis the state of the art on the relationships we tested. Our analyses also show\na trade-off between power against independence and equitability consistent with\nthe theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe are\nsignificantly faster than many other measures of dependence tested, and\ncomputing either one makes computing the other trivial. This suggests that a\nfast and useful strategy for achieving a combination of power against\nindependence and equitability may be to filter relationships by TICe and then\nto examine the MICe of only the significant ones.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:32:20 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 19:54:34 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Reshef", "David N.", ""], ["Reshef", "Yakir A.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02250", "submitter": "Mert Pilanci", "authors": "Mert Pilanci, Martin J. Wainwright", "title": "Newton Sketch: A Linear-time Optimization Algorithm with\n  Linear-Quadratic Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized second-order method for optimization known as the\nNewton Sketch: it is based on performing an approximate Newton step using a\nrandomly projected or sub-sampled Hessian. For self-concordant functions, we\nprove that the algorithm has super-linear convergence with exponentially high\nprobability, with convergence and complexity guarantees that are independent of\ncondition numbers and related problem-dependent quantities. Given a suitable\ninitialization, similar guarantees also hold for strongly convex and smooth\nobjectives without self-concordance. When implemented using randomized\nprojections based on a sub-sampled Hadamard basis, the algorithm typically has\nsubstantially lower complexity than Newton's method. We also describe\nextensions of our methods to programs involving convex constraints that are\nequipped with self-concordant barriers. We discuss and illustrate applications\nto linear programs, quadratic programs with convex constraints, logistic\nregression and other generalized linear models, as well as semidefinite\nprograms.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 09:36:26 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1505.02251", "submitter": "Aris Kosmopoulos", "authors": "Aris Kosmopoulos and Georgios Paliouras and Ion Androutsopoulos", "title": "Probabilistic Cascading for Large Scale Hierarchical Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchies are frequently used for the organization of objects. Given a\nhierarchy of classes, two main approaches are used, to automatically classify\nnew instances: flat classification and cascade classification. Flat\nclassification ignores the hierarchy, while cascade classification greedily\ntraverses the hierarchy from the root to the predicted leaf. In this paper we\npropose a new approach, which extends cascade classification to predict the\nright leaf by estimating the probability of each root-to-leaf path. We provide\nexperimental results which indicate that, using the same classification\nalgorithm, one can achieve better results with our approach, compared to the\ntraditional flat and cascade classifications.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 09:39:04 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Kosmopoulos", "Aris", ""], ["Paliouras", "Georgios", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "1505.02288", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli and Giorgio Corani and Francesca Mangili", "title": "Should we really use post-hoc tests based on mean-ranks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST physics.data-an q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical comparison of multiple algorithms over multiple data sets is\nfundamental in machine learning. This is typically carried out by the Friedman\ntest. When the Friedman test rejects the null hypothesis, multiple comparisons\nare carried out to establish which are the significant differences among\nalgorithms. The multiple comparisons are usually performed using the mean-ranks\ntest. The aim of this technical note is to discuss the inconsistencies of the\nmean-ranks post-hoc test with the goal of discouraging its use in machine\nlearning as well as in medicine, psychology, etc.. We show that the outcome of\nthe mean-ranks test depends on the pool of algorithms originally included in\nthe experiment. In other words, the outcome of the comparison between\nalgorithms A and B depends also on the performance of the other algorithms\nincluded in the original experiment. This can lead to paradoxical situations.\nFor instance the difference between A and B could be declared significant if\nthe pool comprises algorithms C, D, E and not significant if the pool comprises\nalgorithms F, G, H. To overcome these issues, we suggest instead to perform the\nmultiple comparison using a test whose outcome only depends on the two\nalgorithms being compared, such as the sign-test or the Wilcoxon signed-rank\ntest.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 15:54:56 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Benavoli", "Alessio", ""], ["Corani", "Giorgio", ""], ["Mangili", "Francesca", ""]]}, {"id": "1505.02294", "submitter": "Farideh Fazayeli", "authors": "Arindam Banerjee, Sheng Chen, Farideh Fazayeli, Vidyashankar Sivakumar", "title": "Estimation with Norm Regularization", "comments": "Fixed technical issues. Generalized some results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of non-asymptotic estimation error and structured statistical\nrecovery based on norm regularized regression, such as Lasso, needs to consider\nfour aspects: the norm, the loss function, the design matrix, and the noise\nmodel. This paper presents generalizations of such estimation error analysis on\nall four aspects compared to the existing literature. We characterize the\nrestricted error set where the estimation error vector lies, establish\nrelations between error sets for the constrained and regularized problems, and\npresent an estimation error bound applicable to any norm. Precise\ncharacterizations of the bound is presented for isotropic as well as\nanisotropic subGaussian design matrices, subGaussian noise models, and convex\nloss functions, including least squares and generalized linear models. Generic\nchaining and associated results play an important role in the analysis. A key\nresult from the analysis is that the sample complexity of all such estimators\ndepends on the Gaussian width of a spherical cap corresponding to the\nrestricted error set. Further, once the number of samples $n$ crosses the\nrequired sample complexity, the estimation error decreases as\n$\\frac{c}{\\sqrt{n}}$, where $c$ depends on the Gaussian width of the unit norm\nball.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 17:25:14 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 16:24:01 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 20:47:14 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Banerjee", "Arindam", ""], ["Chen", "Sheng", ""], ["Fazayeli", "Farideh", ""], ["Sivakumar", "Vidyashankar", ""]]}, {"id": "1505.02324", "submitter": "Md Abul Hasnat", "authors": "Md. Abul Hasnat, Julien Velcin, St\\'ephane Bonnevay and Julien Jacques", "title": "Simultaneous Clustering and Model Selection for Multinomial\n  Distribution: A Comparative Study", "comments": "Accepted in the International Symposium on Intelligent Data Analysis\n  (IDA 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study different discrete data clustering methods, which use\nthe Model-Based Clustering (MBC) framework with the Multinomial distribution.\nOur study comprises several relevant issues, such as initialization, model\nestimation and model selection. Additionally, we propose a novel MBC method by\nefficiently combining the partitional and hierarchical clustering techniques.\nWe conduct experiments on both synthetic and real data and evaluate the methods\nusing accuracy, stability and computation time. Our study identifies\nappropriate strategies to be used for discrete data analysis with the MBC\nmethods. Moreover, our proposed method is very competitive w.r.t. clustering\naccuracy and better w.r.t. stability and computation time.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 22:29:40 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2015 09:03:48 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Hasnat", "Md. Abul", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Jacques", "Julien", ""]]}, {"id": "1505.02343", "submitter": "Qibin Zhao Dr", "authors": "Qibin Zhao, Liqing Zhang, Andrzej Cichocki", "title": "Bayesian Sparse Tucker Models for Dimension Reduction and Tensor\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tucker decomposition is the cornerstone of modern machine learning on\ntensorial data analysis, which have attracted considerable attention for\nmultiway feature extraction, compressive sensing, and tensor completion. The\nmost challenging problem is related to determination of model complexity (i.e.,\nmultilinear rank), especially when noise and missing data are present. In\naddition, existing methods cannot take into account uncertainty information of\nlatent factors, resulting in low generalization performance. To address these\nissues, we present a class of probabilistic generative Tucker models for tensor\ndecomposition and completion with structural sparsity over multilinear latent\nspace. To exploit structural sparse modeling, we introduce two group sparsity\ninducing priors by hierarchial representation of Laplace and Student-t\ndistributions, which facilitates fully posterior inference. For model learning,\nwe derived variational Bayesian inferences over all model (hyper)parameters,\nand developed efficient and scalable algorithms based on multilinear\noperations. Our methods can automatically adapt model complexity and infer an\noptimal multilinear rank by the principle of maximum lower bound of model\nevidence. Experimental results and comparisons on synthetic, chemometrics and\nneuroimaging data demonstrate remarkable performance of our models for\nrecovering ground-truth of multilinear rank and missing entries.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 05:17:34 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Zhao", "Qibin", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1505.02377", "submitter": "Renjie Liao", "authors": "Renjie Liao, Jianping Shi, Ziyang Ma, Jun Zhu and Jiaya Jia", "title": "Bounded-Distortion Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning aims to embed one metric space into another to benefit tasks\nlike classification and clustering. Although a greatly distorted metric space\nhas a high degree of freedom to fit training data, it is prone to overfitting\nand numerical inaccuracy. This paper presents {\\it bounded-distortion metric\nlearning} (BDML), a new metric learning framework which amounts to finding an\noptimal Mahalanobis metric space with a bounded-distortion constraint. An\nefficient solver based on the multiplicative weights update method is proposed.\nMoreover, we generalize BDML to pseudo-metric learning and devise the\nsemidefinite relaxation and a randomized algorithm to approximately solve it.\nWe further provide theoretical analysis to show that distortion is a key\ningredient for stability and generalization ability of our BDML algorithm.\nExtensive experiments on several benchmark datasets yield promising results.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 13:27:36 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Liao", "Renjie", ""], ["Shi", "Jianping", ""], ["Ma", "Ziyang", ""], ["Zhu", "Jun", ""], ["Jia", "Jiaya", ""]]}, {"id": "1505.02417", "submitter": "Dustin Tran", "authors": "Panos Toulis, Dustin Tran, Edoardo M. Airoldi", "title": "Towards stability and optimality in stochastic gradient descent", "comments": "Appears in Artificial Intelligence and Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative procedures for parameter estimation based on stochastic gradient\ndescent allow the estimation to scale to massive data sets. However, in both\ntheory and practice, they suffer from numerical instability. Moreover, they are\nstatistically inefficient as estimators of the true parameter value. To address\nthese two issues, we propose a new iterative procedure termed averaged implicit\nSGD (AI-SGD). For statistical efficiency, AI-SGD employs averaging of the\niterates, which achieves the optimal Cram\\'{e}r-Rao bound under strong\nconvexity, i.e., it is an optimal unbiased estimator of the true parameter\nvalue. For numerical stability, AI-SGD employs an implicit update at each\niteration, which is related to proximal operators in optimization. In practice,\nAI-SGD achieves competitive performance with other state-of-the-art procedures.\nFurthermore, it is more stable than averaging procedures that do not employ\nproximal updates, and is simple to implement as it requires fewer tunable\nhyperparameters than procedures that do employ proximal updates.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 18:10:07 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 03:01:53 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 23:11:21 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 04:02:43 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Toulis", "Panos", ""], ["Tran", "Dustin", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1505.02419", "submitter": "Mo Yu", "authors": "Matthew R. Gormley and Mo Yu and Mark Dredze", "title": "Improved Relation Extraction with Feature-Rich Compositional Embedding\n  Models", "comments": "12 pages for EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional embedding models build a representation (or embedding) for a\nlinguistic structure based on its component word embeddings. We propose a\nFeature-rich Compositional Embedding Model (FCM) for relation extraction that\nis expressive, generalizes to new domains, and is easy-to-implement. The key\nidea is to combine both (unlexicalized) hand-crafted features with learned word\nembeddings. The model is able to directly tackle the difficulties met by\ntraditional compositional embeddings models, such as handling arbitrary types\nof sentence annotations and utilizing global information for composition. We\ntest the proposed model on two relation extraction tasks, and demonstrate that\nour model outperforms both previous compositional models and traditional\nfeature rich models on the ACE 2005 relation extraction task, and the SemEval\n2010 relation classification task. The combination of our model and a\nlog-linear classifier with hand-crafted features gives state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 18:47:06 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 01:01:34 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 02:01:34 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Gormley", "Matthew R.", ""], ["Yu", "Mo", ""], ["Dredze", "Mark", ""]]}, {"id": "1505.02434", "submitter": "Zhenwen Dai", "authors": "Zhenwen Dai and James Hensman and Neil Lawrence", "title": "Spike and Slab Gaussian Process Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process latent variable model (GP-LVM) is a popular approach to\nnon-linear probabilistic dimensionality reduction. One design choice for the\nmodel is the number of latent variables. We present a spike and slab prior for\nthe GP-LVM and propose an efficient variational inference procedure that gives\na lower bound of the log marginal likelihood. The new model provides a more\nprincipled approach for selecting latent dimensions than the standard way of\nthresholding the length-scale parameters. The effectiveness of our approach is\ndemonstrated through experiments on real and simulated data. Further, we extend\nmulti-view Gaussian processes that rely on sharing latent dimensions (known as\nmanifold relevance determination) with spike and slab priors. This allows a\nmore principled approach for selecting a subset of the latent space for each\nview of data. The extended model outperforms the previous state-of-the-art when\napplied to a cross-modal multimedia retrieval task.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 21:00:21 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Dai", "Zhenwen", ""], ["Hensman", "James", ""], ["Lawrence", "Neil", ""]]}, {"id": "1505.02462", "submitter": "Taichi Kiwaki Mr", "authors": "Taichi Kiwaki", "title": "Soft-Deep Boltzmann Machines", "comments": "Major revision after bug fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a layered Boltzmann machine (BM) that can better exploit the\nadvantages of a distributed representation. It is widely believed that deep BMs\n(DBMs) have far greater representational power than its shallow counterpart,\nrestricted Boltzmann machines (RBMs). However, this expectation on the\nsupremacy of DBMs over RBMs has not ever been validated in a theoretical\nfashion. In this paper, we provide both theoretical and empirical evidences\nthat the representational power of DBMs can be actually rather limited in\ntaking advantages of distributed representations. We propose an approximate\nmeasure for the representational power of a BM regarding to the efficiency of a\ndistributed representation. With this measure, we show a surprising fact that\nDBMs can make inefficient use of distributed representations. Based on these\nobservations, we propose an alternative BM architecture, which we dub soft-deep\nBMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed\nrepresentations in terms of the measure. Experiments demonstrate that sDBMs\noutperform several state-of-the-art models, including DBMs, in generative tasks\non binarized MNIST and Caltech-101 silhouettes.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 00:54:43 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 18:58:39 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2015 01:41:46 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kiwaki", "Taichi", ""]]}, {"id": "1505.02581", "submitter": "Jakub Tomczak Ph.D.", "authors": "Jakub Mikolaj Tomczak", "title": "Improving neural networks with bunches of neurons modeled by Kumaraswamy\n  units: Preliminary study", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently achieved state-of-the-art results in many\nmachine learning problems, e.g., speech recognition or object recognition.\nHitherto, work on rectified linear units (ReLU) provides empirical and\ntheoretical evidence on performance increase of neural networks comparing to\ntypically used sigmoid activation function. In this paper, we investigate a new\nmanner of improving neural networks by introducing a bunch of copies of the\nsame neuron modeled by the generalized Kumaraswamy distribution. As a result,\nwe propose novel non-linear activation function which we refer to as\nKumaraswamy unit which is closely related to ReLU. In the experimental study\nwith MNIST image corpora we evaluate the Kumaraswamy unit applied to\nsingle-layer (shallow) neural network and report a significant drop in test\nclassification error and test cross-entropy in comparison to sigmoid unit, ReLU\nand Noisy ReLU.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 12:14:40 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Tomczak", "Jakub Mikolaj", ""]]}, {"id": "1505.02729", "submitter": "Nakul Verma", "authors": "Nakul Verma and Kristin Branson", "title": "Sample complexity of learning Mahalanobis distance metrics", "comments": "26 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning seeks a transformation of the feature space that enhances\nprediction quality for the given task at hand. In this work we provide\nPAC-style sample complexity rates for supervised metric learning. We give\nmatching lower- and upper-bounds showing that the sample complexity scales with\nthe representation dimension when no assumptions are made about the underlying\ndata distribution. However, by leveraging the structure of the data\ndistribution, we show that one can achieve rates that are fine-tuned to a\nspecific notion of intrinsic complexity for a given dataset. Our analysis\nreveals that augmenting the metric learning optimization criterion with a\nsimple norm-based regularization can help adapt to a dataset's intrinsic\ncomplexity, yielding better generalization. Experiments on benchmark datasets\nvalidate our analysis and show that regularizing the metric can help discern\nthe signal even when the data contains high amounts of noise.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 18:55:42 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Verma", "Nakul", ""], ["Branson", "Kristin", ""]]}, {"id": "1505.02865", "submitter": "Michael Katehakis", "authors": "Wesley Cowan and Michael N. Katehakis", "title": "Asymptotic Behavior of Minimal-Exploration Allocation Policies: Almost\n  Sure, Arbitrarily Slow Growing Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to provide further understanding into the\nstructure of the sequential allocation (\"stochastic multi-armed bandit\", or\nMAB) problem by establishing probability one finite horizon bounds and\nconvergence rates for the sample (or \"pseudo\") regret associated with two\nsimple classes of allocation policies $\\pi$.\n  For any slowly increasing function $g$, subject to mild regularity\nconstraints, we construct two policies (the $g$-Forcing, and the $g$-Inflated\nSample Mean) that achieve a measure of regret of order $ O(g(n))$ almost surely\nas $n \\to \\infty$, bound from above and below. Additionally, almost sure upper\nand lower bounds on the remainder term are established. In the constructions\nherein, the function $g$ effectively controls the \"exploration\" of the\nclassical \"exploration/exploitation\" tradeoff.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 03:35:47 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 16:35:04 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Cowan", "Wesley", ""], ["Katehakis", "Michael N.", ""]]}, {"id": "1505.02867", "submitter": "Charles Mathy", "authors": "Charles Mathy, Nate Derbinsky, Jos\\'e Bento, Jonathan Rosenthal and\n  Jonathan Yedidia", "title": "The Boundary Forest Algorithm for Online Supervised and Unsupervised\n  Learning", "comments": "7 pages, 4 figs, 1 page supp. info", "journal-ref": "Proc. of the 29th AAAI Conference on Artificial Intelligence\n  (AAAI), 2864-2870. Austin, TX, USA. (2015)", "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new instance-based learning algorithm called the Boundary\nForest (BF) algorithm, that can be used for supervised and unsupervised\nlearning. The algorithm builds a forest of trees whose nodes store previously\nseen examples. It can be shown data points one at a time and updates itself\nincrementally, hence it is naturally online. Few instance-based algorithms have\nthis property while being simultaneously fast, which the BF is. This is crucial\nfor applications where one needs to respond to input data in real time. The\nnumber of children of each node is not set beforehand but obtained from the\ntraining procedure, which makes the algorithm very flexible with regards to\nwhat data manifolds it can learn. We test its generalization performance and\nspeed on a range of benchmark datasets and detail in which settings it\noutperforms the state of the art. Empirically we find that training time scales\nas O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N\nthe amount of data,\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 03:45:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Mathy", "Charles", ""], ["Derbinsky", "Nate", ""], ["Bento", "Jos\u00e9", ""], ["Rosenthal", "Jonathan", ""], ["Yedidia", "Jonathan", ""]]}, {"id": "1505.02870", "submitter": "Eliot Brenner", "authors": "Eliot Brenner, David Sontag", "title": "Incorporating Type II Error Probabilities from Independence Tests into\n  Score-Based Learning of Bayesian Network Structure", "comments": "118 pages, 13 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new consistent scoring function for structure learning of Bayesian\nnetworks. In contrast to traditional approaches to score-based structure\nlearning, such as BDeu or MDL, the complexity penalty that we propose is\ndata-dependent and is given by the probability that a conditional independence\ntest correctly shows that an edge cannot exist. What really distinguishes this\nnew scoring function from earlier work is that it has the property of becoming\ncomputationally easier to maximize as the amount of data increases. We prove a\npolynomial sample complexity result, showing that maximizing this score is\nguaranteed to correctly learn a structure with no false edges and a\ndistribution close to the generating distribution, whenever there exists a\nBayesian network which is a perfect map for the data generating distribution.\nAlthough the new score can be used with any search algorithm, in our related\nUAI 2013 paper [BS13], we have given empirical results showing that it is\nparticularly effective when used together with a linear programming relaxation\napproach to Bayesian network structure learning. The present paper contains all\ndetails of the proofs of the finite-sample complexity results in [BS13] as well\nas detailed explanation of the computation of the certain error probabilities\ncalled beta-values, whose precomputation and tabulation is necessary for the\nimplementation of the algorithm in [BS13].\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 04:31:08 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Brenner", "Eliot", ""], ["Sontag", "David", ""]]}, {"id": "1505.02910", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin, Nikita Zhivotovskiy, Gilles Blanchard", "title": "Permutational Rademacher Complexity: a New Complexity Measure for\n  Transductive Learning", "comments": "Corrected error in Inequality (1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transductive learning considers situations when a learner observes $m$\nlabelled training points and $u$ unlabelled test points with the final goal of\ngiving correct answers for the test points. This paper introduces a new\ncomplexity measure for transductive learning called Permutational Rademacher\nComplexity (PRC) and studies its properties. A novel symmetrization inequality\nis proved, which shows that PRC provides a tighter control over expected\nsuprema of empirical processes compared to what happens in the standard i.i.d.\nsetting. A number of comparison results are also provided, which show the\nrelation between PRC and other popular complexity measures used in statistical\nlearning theory, including Rademacher complexity and Transductive Rademacher\nComplexity (TRC). We argue that PRC is a more suitable complexity measure for\ntransductive learning. Finally, these results are combined with a standard\nconcentration argument to provide novel data-dependent risk bounds for\ntransductive learning.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 08:49:54 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 11:56:33 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Zhivotovskiy", "Nikita", ""], ["Blanchard", "Gilles", ""]]}, {"id": "1505.03001", "submitter": "Ofer Shwartz", "authors": "Ofer Shwartz and Boaz Nadler", "title": "Detecting the large entries of a sparse covariance matrix in\n  sub-quadratic time", "comments": null, "journal-ref": null, "doi": "10.1093/imaiai/iaw004", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The covariance matrix of a $p$-dimensional random variable is a fundamental\nquantity in data analysis. Given $n$ i.i.d. observations, it is typically\nestimated by the sample covariance matrix, at a computational cost of\n$O(np^{2})$ operations. When $n,p$ are large, this computation may be\nprohibitively slow. Moreover, in several contemporary applications, the\npopulation matrix is approximately sparse, and only its few large entries are\nof interest. This raises the following question, at the focus of our work:\nAssuming approximate sparsity of the covariance matrix, can its large entries\nbe detected much faster, say in sub-quadratic time, without explicitly\ncomputing all its $p^{2}$ entries? In this paper, we present and theoretically\nanalyze two randomized algorithms that detect the large entries of an\napproximately sparse sample covariance matrix using only $O(np\\text{ poly log }\np)$ operations. Furthermore, assuming sparsity of the population matrix, we\nderive sufficient conditions on the underlying random variable and on the\nnumber of samples $n$, for the sample covariance matrix to satisfy our\napproximate sparsity requirements. Finally, we illustrate the performance of\nour algorithms via several simulations.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 13:30:06 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 08:58:27 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Shwartz", "Ofer", ""], ["Nadler", "Boaz", ""]]}, {"id": "1505.03036", "submitter": "Bernhard Sch\\\"olkopf", "authors": "Bernhard Sch\\\"olkopf, David W. Hogg, Dun Wang, Daniel Foreman-Mackey,\n  Dominik Janzing, Carl-Johann Simon-Gabriel, Jonas Peters", "title": "Removing systematic errors for exoplanet search via latent causes", "comments": "Extended version of a paper appearing in the Proceedings of the 32nd\n  International Conference on Machine Learning, Lille, France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML astro-ph.EP astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for removing the effect of confounders in order to\nreconstruct a latent quantity of interest. The method, referred to as\nhalf-sibling regression, is inspired by recent work in causal inference using\nadditive noise models. We provide a theoretical justification and illustrate\nthe potential of the method in a challenging astronomy application.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 14:49:08 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Sch\u00f6lkopf", "Bernhard", ""], ["Hogg", "David W.", ""], ["Wang", "Dun", ""], ["Foreman-Mackey", "Daniel", ""], ["Janzing", "Dominik", ""], ["Simon-Gabriel", "Carl-Johann", ""], ["Peters", "Jonas", ""]]}, {"id": "1505.03236", "submitter": "Jensi", "authors": "R. Jensi and G. Wiselin Jiji", "title": "Hybrid data clustering approach using K-Means and Flower Pollination\n  Algorithm", "comments": "11 pages, Journal. Advanced Computational Intelligence: An\n  International Journal (ACII), Vol.2, No.2, April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a technique for clustering set of objects into known\nnumber of groups. Several approaches are widely applied to data clustering so\nthat objects within the clusters are similar and objects in different clusters\nare far away from each other. K-Means, is one of the familiar center based\nclustering algorithms since implementation is very easy and fast convergence.\nHowever, K-Means algorithm suffers from initialization, hence trapped in local\noptima. Flower Pollination Algorithm (FPA) is the global optimization\ntechnique, which avoids trapping in local optimum solution. In this paper, a\nnovel hybrid data clustering approach using Flower Pollination Algorithm and\nK-Means (FPAKM) is proposed. The proposed algorithm results are compared with\nK-Means and FPA on eight datasets. From the experimental results, FPAKM is\nbetter than FPA and K-Means.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 04:24:50 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Jensi", "R.", ""], ["Jiji", "G. Wiselin", ""]]}, {"id": "1505.03410", "submitter": "Joseph  Salmon", "authors": "Olivier Fercoq, Alexandre Gramfort, Joseph Salmon", "title": "Mind the duality gap: safer rules for the Lasso", "comments": "erratum to ICML 2015, \"The authors would like to thanks Jalal Fadili\n  and Jingwei Liang for helping clarifying some misleading statements on the\n  equicorrelation set\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening rules allow to early discard irrelevant variables from the\noptimization in Lasso problems, or its derivatives, making solvers faster. In\nthis paper, we propose new versions of the so-called $\\textit{safe rules}$ for\nthe Lasso. Based on duality gap considerations, our new rules create safe test\nregions whose diameters converge to zero, provided that one relies on a\nconverging solver. This property helps screening out more variables, for a\nwider range of regularization parameter values. In addition to faster\nconvergence, we prove that we correctly identify the active sets (supports) of\nthe solutions in finite time. While our proposed strategy can cope with any\nsolver, its performance is demonstrated using a coordinate descent algorithm\nparticularly adapted to machine learning use cases. Significant computing time\nreductions are obtained with respect to previous safe rules.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 14:50:34 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 14:52:12 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 21:12:34 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1505.03654", "submitter": "Sho Sonoda", "authors": "Sho Sonoda and Noboru Murata", "title": "Neural Network with Unbounded Activation Functions is Universal\n  Approximator", "comments": "under review; first revised version", "journal-ref": "Applied and Computational Harmonic Analysis, 43(2):233-268, 2017", "doi": "10.1016/j.acha.2015.12.005", "report-no": null, "categories": "cs.NE cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an investigation of the approximation property of neural\nnetworks with unbounded activation functions, such as the rectified linear unit\n(ReLU), which is the new de-facto standard of deep learning. The ReLU network\ncan be analyzed by the ridgelet transform with respect to Lizorkin\ndistributions. By showing three reconstruction formulas by using the Fourier\nslice theorem, the Radon transform, and Parseval's relation, it is shown that a\nneural network with unbounded activation functions still satisfies the\nuniversal approximation property. As an additional consequence, the ridgelet\ntransform, or the backprojection filter in the Radon domain, is what the\nnetwork learns after backpropagation. Subject to a constructive admissibility\ncondition, the trained network can be obtained by simply discretizing the\nridgelet transform, without backpropagation. Numerical examples not only\nsupport the consistency of the admissibility condition but also imply that some\nnon-admissible cases result in low-pass filtering.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 09:03:19 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2015 21:07:19 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Sonoda", "Sho", ""], ["Murata", "Noboru", ""]]}, {"id": "1505.03703", "submitter": "Yanhai Gan", "authors": "Yanhai Gan, Jun Liu, Junyu Dong, Guoqiang Zhong", "title": "A PCA-Based Convolutional Network", "comments": "8 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised deep learning model, called\nPCA-based Convolutional Network (PCN). The architecture of PCN is composed of\nseveral feature extraction stages and a nonlinear output stage. Particularly,\neach feature extraction stage includes two layers: a convolutional layer and a\nfeature pooling layer. In the convolutional layer, the filter banks are simply\nlearned by PCA. In the nonlinear output stage, binary hashing is applied. For\nthe higher convolutional layers, the filter banks are learned from the feature\nmaps that were obtained in the previous stage. To test PCN, we conducted\nextensive experiments on some challenging tasks, including handwritten digits\nrecognition, face recognition and texture classification. The results show that\nPCN performs competitive with or even better than state-of-the-art deep\nlearning models. More importantly, since there is no back propagation for\nsupervised finetuning, PCN is much more efficient than existing deep networks.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 12:35:19 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Gan", "Yanhai", ""], ["Liu", "Jun", ""], ["Dong", "Junyu", ""], ["Zhong", "Guoqiang", ""]]}, {"id": "1505.03906", "submitter": "Daniel Roy", "authors": "Gintare Karolina Dziugaite and Daniel M. Roy and Zoubin Ghahramani", "title": "Training generative neural networks via Maximum Mean Discrepancy\n  optimization", "comments": "10 pages, to appear in Uncertainty in Artificial Intelligence (UAI)\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider training a deep neural network to generate samples from an\nunknown distribution given i.i.d. data. We frame learning as an optimization\nminimizing a two-sample test statistic---informally speaking, a good generator\nnetwork produces samples that cause a two-sample test to fail to reject the\nnull hypothesis. As our two-sample test statistic, we use an unbiased estimate\nof the maximum mean discrepancy, which is the centerpiece of the nonparametric\nkernel two-sample test proposed by Gretton et al. (2012). We compare to the\nadversarial nets framework introduced by Goodfellow et al. (2014), in which\nlearning is a two-player game between a generator network and an adversarial\ndiscriminator network, both trained to outwit the other. From this perspective,\nthe MMD statistic plays the role of the discriminator. In addition to empirical\ncomparisons, we prove bounds on the generalization error incurred by optimizing\nthe empirical MMD.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 22:18:42 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1505.03924", "submitter": "Colin White", "authors": "Maria-Florina Balcan, Nika Haghtalab, Colin White", "title": "$k$-center Clustering under Perturbation Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-center problem is a canonical and long-studied facility location and\nclustering problem with many applications in both its symmetric and asymmetric\nforms. Both versions of the problem have tight approximation factors on worst\ncase instances. Therefore to improve on these ratios, one must go beyond the\nworst case.\n  In this work, we take this approach and provide strong positive results both\nfor the asymmetric and symmetric $k$-center problems under a natural input\nstability (promise) condition called $\\alpha$-perturbation resilience [Bilu and\nLinia 2012], which states that the optimal solution does not change under any\nalpha-factor perturbation to the input distances. We provide algorithms that\ngive strong guarantees simultaneously for stable and non-stable instances: our\nalgorithms always inherit the worst-case guarantees of clustering approximation\nalgorithms, and output the optimal solution if the input is $2$-perturbation\nresilient. Furthermore, we prove our result is tight by showing symmetric\n$k$-center under $(2-\\epsilon)$-perturbation resilience is hard unless $NP=RP$.\n  The impact of our results are multifaceted. This is the first tight result\nfor any problem under perturbation resilience. Furthermore, our results\nillustrate a surprising relationship between symmetric and asymmetric\n$k$-center instances under perturbation resilience. Unlike approximation ratio,\nfor which symmetric $k$-center is easily solved to a factor of 2 but asymmetric\n$k$-center cannot be approximated to any constant factor, both symmetric and\nasymmetric $k$-center can be solved optimally under resilience to\n2-perturbations. Finally, our guarantees in the setting where only part of the\ndata satisfies perturbation resilience makes these algorithms more applicable\nto real-life instances.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 23:59:14 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 15:42:23 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2016 22:49:26 GMT"}, {"version": "v4", "created": "Fri, 28 Dec 2018 22:02:13 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Haghtalab", "Nika", ""], ["White", "Colin", ""]]}, {"id": "1505.03932", "submitter": "Giancarlo Crocetti", "authors": "Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum,\n  Tamba Lamin", "title": "Using Ensemble Models in the Histological Examination of Tissue\n  Abnormalities", "comments": "4 pages, 4 tables, 3 figures. Proceedings of 12th Annual Research\n  Day, 2014 - Pace University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification models for the automatic detection of abnormalities on\nhistological samples do exists, with an active debate on the cost associated\nwith false negative diagnosis (underdiagnosis) and false positive diagnosis\n(overdiagnosis). Current models tend to underdiagnose, failing to recognize a\npotentially fatal disease.\n  The objective of this study is to investigate the possibility of\nautomatically identifying abnormalities in tissue samples through the use of an\nensemble model on data generated by histological examination and to minimize\nthe number of false negative cases.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 00:59:48 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Crocetti", "Giancarlo", ""], ["Coakley", "Michael", ""], ["Dressner", "Phil", ""], ["Kellum", "Wanda", ""], ["Lamin", "Tamba", ""]]}, {"id": "1505.04073", "submitter": "Jie  Wang", "authors": "Jie Wang and Jieping Ye", "title": "Safe Screening for Multi-Task Feature Learning with Multiple Data\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task feature learning (MTFL) is a powerful technique in boosting the\npredictive performance by learning multiple related\nclassification/regression/clustering tasks simultaneously. However, solving the\nMTFL problem remains challenging when the feature dimension is extremely large.\nIn this paper, we propose a novel screening rule---that is based on the dual\nprojection onto convex sets (DPC)---to quickly identify the inactive\nfeatures---that have zero coefficients in the solution vectors across all\ntasks. One of the appealing features of DPC is that: it is safe in the sense\nthat the detected inactive features are guaranteed to have zero coefficients in\nthe solution vectors across all tasks. Thus, by removing the inactive features\nfrom the training phase, we may have substantial savings in the computational\ncost and memory usage without sacrificing accuracy. To the best of our\nknowledge, it is the first screening rule that is applicable to sparse models\nwith multiple data matrices. A key challenge in deriving DPC is to solve a\nnonconvex problem. We show that we can solve for the global optimum efficiently\nvia a properly chosen parametrization of the constraint set. Moreover, DPC has\nvery low computational cost and can be integrated with any existing solvers. We\nhave evaluated the proposed DPC rule on both synthetic and real data sets. The\nexperiments indicate that DPC is very effective in identifying the inactive\nfeatures---especially for high dimensional data---which leads to a speedup up\nto several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 14:31:09 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Wang", "Jie", ""], ["Ye", "Jieping", ""]]}, {"id": "1505.04085", "submitter": "Parikshit Shah", "authors": "Parikshit Shah, Nikhil Rao, Gongguo Tang", "title": "Optimal Low-Rank Tensor Recovery from Separable Measurements: Four\n  Contractions Suffice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors play a central role in many modern machine learning and signal\nprocessing applications. In such applications, the target tensor is usually of\nlow rank, i.e., can be expressed as a sum of a small number of rank one\ntensors. This motivates us to consider the problem of low rank tensor recovery\nfrom a class of linear measurements called separable measurements. As specific\nexamples, we focus on two distinct types of separable measurement mechanisms\n(a) Random projections, where each measurement corresponds to an inner product\nof the tensor with a suitable random tensor, and (b) the completion problem\nwhere measurements constitute revelation of a random set of entries. We present\na computationally efficient algorithm, with rigorous and order-optimal sample\ncomplexity results (upto logarithmic factors) for tensor recovery. Our method\nis based on reduction to matrix completion sub-problems and adaptation of\nLeurgans' method for tensor decomposition. We extend the methodology and sample\ncomplexity results to higher order tensors, and experimentally validate our\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 15:01:08 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Shah", "Parikshit", ""], ["Rao", "Nikhil", ""], ["Tang", "Gongguo", ""]]}, {"id": "1505.04097", "submitter": "Charmgil Hong", "authors": "Charmgil Hong, Milos Hauskrecht", "title": "MCODE: Multivariate Conditional Outlier Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection aims to identify unusual data instances that deviate from\nexpected patterns. The outlier detection is particularly challenging when\noutliers are context dependent and when they are defined by unusual\ncombinations of multiple outcome variable values. In this paper, we develop and\nstudy a new conditional outlier detection approach for multivariate outcome\nspaces that works by (1) transforming the conditional detection to the outlier\ndetection problem in a new (unconditional) space and (2) defining outlier\nscores by analyzing the data in the new space. Our approach relies on the\nclassifier chain decomposition of the multi-dimensional classification problem\nthat lets us transform the output space into a probability vector, one\nprobability for each dimension of the output space. Outlier scores applied to\nthese transformed vectors are then used to detect the outliers. Experiments on\nmultiple multi-dimensional classification problems with the different outlier\ninjection rates show that our methodology is robust and able to successfully\nidentify outliers when outliers are either sparse (manifested in one or very\nfew dimensions) or dense (affecting multiple dimensions).\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 15:37:51 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Hong", "Charmgil", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1505.04123", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Javier Pe\\~na", "title": "Margins, Kernels and Non-linear Smoothed Perceptrons", "comments": "17 pages, published in the proceedings of the International\n  Conference on Machine Learning, 2014", "journal-ref": "Ramdas, Aaditya, and Javier Pena. \"Margins, kernels and non-linear\n  smoothed perceptrons.\" Proceedings of the 31st International Conference on\n  Machine Learning (ICML-14). 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of finding a non-linear classification function that\nlies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of\nview (finding a perfect separator when one exists) and the dual point of view\n(giving a certificate of non-existence), with special focus on generalizations\nof two classical schemes - the Perceptron (primal) and Von-Neumann (dual)\nalgorithms.\n  We cast our problem as one of maximizing the regularized normalized\nhard-margin ($\\rho$) in an RKHS and %use the Representer Theorem to rephrase it\nin terms of a Mahalanobis dot-product/semi-norm associated with the kernel's\n(normalized and signed) Gram matrix. We derive an accelerated smoothed\nalgorithm with a convergence rate of $\\tfrac{\\sqrt {\\log n}}{\\rho}$ given $n$\nseparable points, which is strikingly similar to the classical kernelized\nPerceptron algorithm whose rate is $\\tfrac1{\\rho^2}$. When no such classifier\nexists, we prove a version of Gordan's separation theorem for RKHSs, and give a\nreinterpretation of negative margins. This allows us to give guarantees for a\nprimal-dual algorithm that halts in $\\min\\{\\tfrac{\\sqrt n}{|\\rho|},\n\\tfrac{\\sqrt n}{\\epsilon}\\}$ iterations with a perfect separator in the RKHS if\nthe primal is feasible or a dual $\\epsilon$-certificate of near-infeasibility.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 16:54:58 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Pe\u00f1a", "Javier", ""]]}, {"id": "1505.04137", "submitter": "Harish Ramaswamy", "authors": "Harish G. Ramaswamy and Ambuj Tewari and Shivani Agarwal", "title": "Consistent Algorithms for Multiclass Classification with a Reject Option", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of $n$-class classification ($n\\geq 2$), where the\nclassifier can choose to abstain from making predictions at a given cost, say,\na factor $\\alpha$ of the cost of misclassification. Designing consistent\nalgorithms for such $n$-class classification problems with a `reject option' is\nthe main goal of this paper, thereby extending and generalizing previously\nknown results for $n=2$. We show that the Crammer-Singer surrogate and the one\nvs all hinge loss, albeit with a different predictor than the standard argmax,\nyield consistent algorithms for this problem when $\\alpha=\\frac{1}{2}$. More\ninterestingly, we design a new convex surrogate that is also consistent for\nthis problem when $\\alpha=\\frac{1}{2}$ and operates on a much lower dimensional\nspace ($\\log(n)$ as opposed to $n$). We also generalize all three surrogates to\nbe consistent for any $\\alpha\\in[0, \\frac{1}{2}]$.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 17:49:47 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Ramaswamy", "Harish G.", ""], ["Tewari", "Ambuj", ""], ["Agarwal", "Shivani", ""]]}, {"id": "1505.04214", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Aarti Singh", "title": "Algorithmic Connections Between Active Learning and Stochastic Convex\n  Optimization", "comments": "15 pages, published in proceedings of Algorithmic Learning Theory,\n  Springer Berlin Heidelberg, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interesting theoretical associations have been established by recent papers\nbetween the fields of active learning and stochastic convex optimization due to\nthe common role of feedback in sequential querying mechanisms. In this paper,\nwe continue this thread in two parts by exploiting these relations for the\nfirst time to yield novel algorithms in both fields, further motivating the\nstudy of their intersection. First, inspired by a recent optimization algorithm\nthat was adaptive to unknown uniform convexity parameters, we present a new\nactive learning algorithm for one-dimensional thresholds that can yield minimax\nrates by adapting to unknown noise parameters. Next, we show that one can\nperform $d$-dimensional stochastic minimization of smooth uniformly convex\nfunctions when only granted oracle access to noisy gradient signs along any\ncoordinate instead of real-valued gradients, by using a simple randomized\ncoordinate descent procedure where each line search can be solved by\n$1$-dimensional active learning, provably achieving the same error convergence\nrate as having the entire real-valued gradient. Combining these two parts\nyields an algorithm that solves stochastic convex optimization of uniformly\nconvex and smooth functions using only noisy gradient signs by repeatedly\nperforming active learning, achieves optimal rates and is adaptive to all\nunknown convexity and smoothness parameters.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 22:38:28 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Singh", "Aarti", ""]]}, {"id": "1505.04215", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Barnabas Poczos, Aarti Singh, Larry Wasserman", "title": "An Analysis of Active Learning With Uniform Feature Noise", "comments": "24 pages, 2 figures, published in the proceedings of the 17th\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In active learning, the user sequentially chooses values for feature $X$ and\nan oracle returns the corresponding label $Y$. In this paper, we consider the\neffect of feature noise in active learning, which could arise either because\n$X$ itself is being measured, or it is corrupted in transmission to the oracle,\nor the oracle returns the label of a noisy version of the query point. In\nstatistics, feature noise is known as \"errors in variables\" and has been\nstudied extensively in non-active settings. However, the effect of feature\nnoise in active learning has not been studied before. We consider the\nwell-known Berkson errors-in-variables model with additive uniform noise of\nwidth $\\sigma$.\n  Our simple but revealing setting is that of one-dimensional binary\nclassification setting where the goal is to learn a threshold (point where the\nprobability of a $+$ label crosses half). We deal with regression functions\nthat are antisymmetric in a region of size $\\sigma$ around the threshold and\nalso satisfy Tsybakov's margin condition around the threshold. We prove minimax\nlower and upper bounds which demonstrate that when $\\sigma$ is smaller than the\nminimiax active/passive noiseless error derived in \\cite{CN07}, then noise has\nno effect on the rates and one achieves the same noiseless rates. For larger\n$\\sigma$, the \\textit{unflattening} of the regression function on convolution\nwith uniform noise, along with its local antisymmetry around the threshold,\ntogether yield a behaviour where noise \\textit{appears} to be beneficial. Our\nkey result is that active learning can buy significant improvement over a\npassive strategy even in the presence of feature noise.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 22:54:01 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1505.04243", "submitter": "Paul Grigas", "authors": "Robert M. Freund, Paul Grigas, Rahul Mazumder", "title": "A New Perspective on Boosting in Linear Regression via Subgradient\n  Optimization and Relatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze boosting algorithms in linear regression from a new\nperspective: that of modern first-order methods in convex optimization. We show\nthat classic boosting algorithms in linear regression, namely the incremental\nforward stagewise algorithm (FS$_\\varepsilon$) and least squares boosting\n(LS-Boost($\\varepsilon$)), can be viewed as subgradient descent to minimize the\nloss function defined as the maximum absolute correlation between the features\nand residuals. We also propose a modification of FS$_\\varepsilon$ that yields\nan algorithm for the Lasso, and that may be easily extended to an algorithm\nthat computes the Lasso path for different values of the regularization\nparameter. Furthermore, we show that these new algorithms for the Lasso may\nalso be interpreted as the same master algorithm (subgradient descent), applied\nto a regularized version of the maximum absolute correlation loss function. We\nderive novel, comprehensive computational guarantees for several boosting\nalgorithms in linear regression (including LS-Boost($\\varepsilon$) and\nFS$_\\varepsilon$) by using techniques of modern first-order methods in convex\noptimization. Our computational guarantees inform us about the statistical\nproperties of boosting algorithms. In particular they provide, for the first\ntime, a precise theoretical description of the amount of data-fidelity and\nregularization imparted by running a boosting algorithm with a prespecified\nlearning rate for a fixed but arbitrary number of iterations, for any dataset.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 04:23:08 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Freund", "Robert M.", ""], ["Grigas", "Paul", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1505.04252", "submitter": "Shiqian Ma", "authors": "Tianyi Lin, Shiqian Ma, Shuzhong Zhang", "title": "Global Convergence of Unmodified 3-Block ADMM for a Class of Convex\n  Minimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) has been successfully\napplied to solve structured convex optimization problems due to its superior\npractical performance. The convergence properties of the 2-block ADMM have been\nstudied extensively in the literature. Specifically, it has been proven that\nthe 2-block ADMM globally converges for any penalty parameter $\\gamma>0$. In\nthis sense, the 2-block ADMM allows the parameter to be free, i.e., there is no\nneed to restrict the value for the parameter when implementing this algorithm\nin order to ensure convergence. However, for the 3-block ADMM, Chen \\etal\n\\cite{Chen-admm-failure-2013} recently constructed a counter-example showing\nthat it can diverge if no further condition is imposed. The existing results on\nstudying further sufficient conditions on guaranteeing the convergence of the\n3-block ADMM usually require $\\gamma$ to be smaller than a certain bound, which\nis usually either difficult to compute or too small to make it a practical\nalgorithm. In this paper, we show that the 3-block ADMM still globally\nconverges with any penalty parameter $\\gamma>0$ if the third function $f_3$ in\nthe objective is smooth and strongly convex, and its condition number is in\n$[1,1.0798)$, besides some other mild conditions. This requirement covers an\nimportant class of problems to be called regularized least squares\ndecomposition (RLSD) in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 08:00:54 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 03:28:26 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 07:22:52 GMT"}, {"version": "v4", "created": "Wed, 17 Jan 2018 22:44:55 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Lin", "Tianyi", ""], ["Ma", "Shiqian", ""], ["Zhang", "Shuzhong", ""]]}, {"id": "1505.04343", "submitter": "Yining Wang", "authors": "Yining Wang, Aarti Singh", "title": "Provably Correct Algorithms for Matrix Column Subset Selection with\n  Selectively Sampled Data", "comments": "42 pages. Accepted to Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of matrix column subset selection, which selects a\nsubset of columns from an input matrix such that the input can be well\napproximated by the span of the selected columns. Column subset selection has\nbeen applied to numerous real-world data applications such as population\ngenetics summarization, electronic circuits testing and recommendation systems.\nIn many applications the complete data matrix is unavailable and one needs to\nselect representative columns by inspecting only a small portion of the input\nmatrix. In this paper we propose the first provably correct column subset\nselection algorithms for partially observed data matrices. Our proposed\nalgorithms exhibit different merits and limitations in terms of statistical\naccuracy, computational efficiency, sample complexity and sampling schemes,\nwhich provides a nice exploration of the tradeoff between these desired\nproperties for column subset selection. The proposed methods employ the idea of\nfeedback driven sampling and are inspired by several sampling schemes\npreviously introduced for low-rank matrix approximation tasks (Drineas et al.,\n2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and\nSingh, 2014). Our analysis shows that, under the assumption that the input data\nmatrix has incoherent rows but possibly coherent columns, all algorithms\nprovably converge to the best low-rank approximation of the original data as\nnumber of selected columns increases. Furthermore, two of the proposed\nalgorithms enjoy a relative error bound, which is preferred for column subset\nselection and matrix approximation purposes. We also demonstrate through both\ntheoretical and empirical analysis the power of feedback driven sampling\ncompared to uniform random sampling on input matrices with highly correlated\ncolumns.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 01:36:27 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 21:06:40 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 00:13:23 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Wang", "Yining", ""], ["Singh", "Aarti", ""]]}, {"id": "1505.04369", "submitter": "Lin Xu", "authors": "Lin Xu, Shaobo Lin, Yao Wang and Zongben Xu", "title": "Shrinkage degree in $L_2$-re-scale boosting for regression", "comments": "11 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-scale boosting (RBoosting) is a variant of boosting which can essentially\nimprove the generalization performance of boosting learning. The key feature of\nRBoosting lies in introducing a shrinkage degree to re-scale the ensemble\nestimate in each gradient-descent step. Thus, the shrinkage degree determines\nthe performance of RBoosting.\n  The aim of this paper is to develop a concrete analysis concerning how to\ndetermine the shrinkage degree in $L_2$-RBoosting. We propose two feasible ways\nto select the shrinkage degree. The first one is to parameterize the shrinkage\ndegree and the other one is to develope a data-driven approach of it. After\nrigorously analyzing the importance of the shrinkage degree in $L_2$-RBoosting\nlearning, we compare the pros and cons of the proposed methods. We find that\nalthough these approaches can reach the same learning rates, the structure of\nthe final estimate of the parameterized approach is better, which sometimes\nyields a better generalization capability when the number of sample is finite.\nWith this, we recommend to parameterize the shrinkage degree of\n$L_2$-RBoosting. To this end, we present an adaptive parameter-selection\nstrategy for shrinkage degree and verify its feasibility through both\ntheoretical analysis and numerical verification.\n  The obtained results enhance the understanding of RBoosting and further give\nguidance on how to use $L_2$-RBoosting for regression tasks.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 08:09:43 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Xu", "Lin", ""], ["Lin", "Shaobo", ""], ["Wang", "Yao", ""], ["Xu", "Zongben", ""]]}, {"id": "1505.04406", "submitter": "Stephen Bach", "authors": "Stephen H. Bach, Matthias Broecheler, Bert Huang, Lise Getoor", "title": "Hinge-Loss Markov Random Fields and Probabilistic Soft Logic", "comments": null, "journal-ref": "Journal of Machine Learning Research (JMLR), 18(109):1-67, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in developing high-impact machine learning\ntechnologies is balancing the need to model rich, structured domains with the\nability to scale to big data. Many important problem areas are both richly\nstructured and large scale, from social and biological networks, to knowledge\ngraphs and the Web, to images, video, and natural language. In this paper, we\nintroduce two new formalisms for modeling structured data, and show that they\ncan both capture rich structure and scale to big data. The first, hinge-loss\nMarkov random fields (HL-MRFs), is a new kind of probabilistic graphical model\nthat generalizes different approaches to convex inference. We unite three\napproaches from the randomized algorithms, probabilistic graphical models, and\nfuzzy logic communities, showing that all three lead to the same inference\nobjective. We then define HL-MRFs by generalizing this unified objective. The\nsecond new formalism, probabilistic soft logic (PSL), is a probabilistic\nprogramming language that makes HL-MRFs easy to define using a syntax based on\nfirst-order logic. We introduce an algorithm for inferring most-probable\nvariable assignments (MAP inference) that is much more scalable than\ngeneral-purpose convex optimization methods, because it uses message passing to\ntake advantage of sparse dependency structures. We then show how to learn the\nparameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous\ndiscrete models, but much more scalable. Together, these algorithms enable\nHL-MRFs and PSL to model rich, structured data at scales not previously\npossible.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 15:19:09 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 04:32:24 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 00:10:03 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Bach", "Stephen H.", ""], ["Broecheler", "Matthias", ""], ["Huang", "Bert", ""], ["Getoor", "Lise", ""]]}, {"id": "1505.04627", "submitter": "Michal Valko", "authors": "Alexandra Carpentier, Michal Valko", "title": "Simple regret for infinitely many armed bandits", "comments": "in 32th International Conference on Machine Learning (ICML 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic bandit problem with infinitely many arms. In this\nsetting, the learner has no chance of trying all the arms even once and has to\ndedicate its limited number of samples only to a certain number of arms. All\nprevious algorithms for this setting were designed for minimizing the\ncumulative regret of the learner. In this paper, we propose an algorithm aiming\nat minimizing the simple regret. As in the cumulative regret setting of\ninfinitely many armed bandits, the rate of the simple regret will depend on a\nparameter $\\beta$ characterizing the distribution of the near-optimal arms. We\nprove that depending on $\\beta$, our algorithm is minimax optimal either up to\na multiplicative constant or up to a $\\log(n)$ factor. We also provide\nextensions to several important cases: when $\\beta$ is unknown, in a natural\nsetting where the near-optimal arms have a small variance, and in the case of\nunknown time horizon.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:16:42 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Valko", "Michal", ""]]}, {"id": "1505.04630", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang and Zhiyong Zhang", "title": "Recurrent Neural Network Training with Dark Knowledge Transfer", "comments": "ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472809", "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs), particularly long short-term memory (LSTM),\nhave gained much attention in automatic speech recognition (ASR). Although some\nsuccessful stories have been reported, training RNNs remains highly\nchallenging, especially with limited training data. Recent research found that\na well-trained model can be used as a teacher to train other child models, by\nusing the predictions generated by the teacher model as supervision. This\nknowledge transfer learning has been employed to train simple neural nets with\na complex one, so that the final performance can reach a level that is\ninfeasible to obtain by regular training. In this paper, we employ the\nknowledge transfer learning approach to train RNNs (precisely LSTM) using a\ndeep neural network (DNN) model as the teacher. This is different from most of\nthe existing research on knowledge transfer learning, since the teacher (DNN)\nis assumed to be weaker than the child (RNN); however, our experiments on an\nASR task showed that it works fairly well: without applying any tricks on the\nlearning scheme, this approach can train RNNs successfully even with limited\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:26:02 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 05:58:15 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 03:31:51 GMT"}, {"version": "v4", "created": "Sat, 12 Mar 2016 07:51:36 GMT"}, {"version": "v5", "created": "Sun, 8 May 2016 12:40:35 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Zhang", "Zhiyong", ""]]}, {"id": "1505.04636", "submitter": "Alex Smola J", "authors": "Mu Li, Dave G. Andersen, Alexander J. Smola", "title": "Graph Partitioning via Parallel Submodular Approximation to Accelerate\n  Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing excels at processing large scale data, but the\ncommunication cost for synchronizing the shared parameters may slow down the\noverall performance. Fortunately, the interactions between parameter and data\nin many problems are sparse, which admits efficient partition in order to\nreduce the communication overhead.\n  In this paper, we formulate data placement as a graph partitioning problem.\nWe propose a distributed partitioning algorithm. We give both theoretical\nguarantees and a highly efficient implementation. We also provide a highly\nefficient implementation of the algorithm and demonstrate its promising results\non both text datasets and social networks. We show that the proposed algorithm\nleads to 1.6x speedup of a state-of-the-start distributed machine learning\nsystem by eliminating 90\\% of the network communication.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:43:46 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Li", "Mu", ""], ["Andersen", "Dave G.", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1505.04637", "submitter": "Alejandro Correa Bahnsen", "authors": "Alejandro Correa Bahnsen, Djamila Aouada, Bjorn Ottersten", "title": "Ensemble of Example-Dependent Cost-Sensitive Decision Trees", "comments": "13 pages, 6 figures, Submitted for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several real-world classification problems are example-dependent\ncost-sensitive in nature, where the costs due to misclassification vary between\nexamples and not only within classes. However, standard classification methods\ndo not take these costs into account, and assume a constant cost of\nmisclassification errors. In previous works, some methods that take into\naccount the financial costs into the training of different algorithms have been\nproposed, with the example-dependent cost-sensitive decision tree algorithm\nbeing the one that gives the highest savings. In this paper we propose a new\nframework of ensembles of example-dependent cost-sensitive decision-trees. The\nframework consists in creating different example-dependent cost-sensitive\ndecision trees on random subsamples of the training set, and then combining\nthem using three different combination approaches. Moreover, we propose two new\ncost-sensitive combination approaches; cost-sensitive weighted voting and\ncost-sensitive stacking, the latter being based on the cost-sensitive logistic\nregression method. Finally, using five different databases, from four\nreal-world applications: credit card fraud detection, churn modeling, credit\nscoring and direct marketing, we evaluate the proposed method against\nstate-of-the-art example-dependent cost-sensitive techniques, namely,\ncost-proportionate sampling, Bayes minimum risk and cost-sensitive decision\ntrees. The results show that the proposed algorithms have better results for\nall databases, in the sense of higher savings.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:43:53 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Bahnsen", "Alejandro Correa", ""], ["Aouada", "Djamila", ""], ["Ottersten", "Bjorn", ""]]}, {"id": "1505.04650", "submitter": "Mariano Tepper", "authors": "Mariano Tepper and Guillermo Sapiro", "title": "Compressed Nonnegative Matrix Factorization is Fast and Accurate", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2516971", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has an established reputation as a\nuseful data analysis technique in numerous applications. However, its usage in\npractical situations is undergoing challenges in recent years. The fundamental\nfactor to this is the increasingly growing size of the datasets available and\nneeded in the information sciences. To address this, in this work we propose to\nuse structured random compression, that is, random projections that exploit the\ndata structure, for two NMF variants: classical and separable. In separable NMF\n(SNMF) the left factors are a subset of the columns of the input matrix. We\npresent suitable formulations for each problem, dealing with different\nrepresentative algorithms within each one. We show that the resulting\ncompressed techniques are faster than their uncompressed variants, vastly\nreduce memory demands, and do not encompass any significant deterioration in\nperformance. The proposed structured random projections for SNMF allow to deal\nwith arbitrarily shaped large matrices, beyond the standard limit of\ntall-and-skinny matrices, granting access to very efficient computations in\nthis general setting. We accompany the algorithmic presentation with\ntheoretical foundations and numerous and diverse examples, showing the\nsuitability of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 14:12:22 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2015 20:22:36 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Tepper", "Mariano", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1505.04732", "submitter": "Luca Martino", "authors": "L. Martino, V. Elvira, D. Luengo, J. Corander", "title": "Layered Adaptive Importance Sampling", "comments": "Related Matlab codes: an iterative version at\n  http://www.lucamartino.altervista.org/CODE_LAIS_v03.zip and a non-iterative\n  version at http://www.lucamartino.altervista.org/LAIS_non_iterative_code.zip,\n  Statistics and Computing, 2016", "journal-ref": null, "doi": "10.1007/s11222-016-9642-5", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods represent the \"de facto\" standard for approximating\ncomplicated integrals involving multidimensional target distributions. In order\nto generate random realizations from the target distribution, Monte Carlo\ntechniques use simpler proposal probability densities to draw candidate\nsamples. The performance of any such method is strictly related to the\nspecification of the proposal distribution, such that unfortunate choices\neasily wreak havoc on the resulting estimators. In this work, we introduce a\nlayered (i.e., hierarchical) procedure to generate samples employed within a\nMonte Carlo scheme. This approach ensures that an appropriate equivalent\nproposal density is always obtained automatically (thus eliminating the risk of\na catastrophic performance), although at the expense of a moderate increase in\nthe complexity. Furthermore, we provide a general unified importance sampling\n(IS) framework, where multiple proposal densities are employed and several IS\nschemes are introduced by applying the so-called deterministic mixture\napproach. Finally, given these schemes, we also propose a novel class of\nadaptive importance samplers using a population of proposals, where the\nadaptation is driven by independent parallel or interacting Markov Chain Monte\nCarlo (MCMC) chains. The resulting algorithms efficiently combine the benefits\nof both IS and MCMC methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 17:40:51 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 18:34:26 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 23:55:32 GMT"}, {"version": "v4", "created": "Sun, 27 Nov 2016 15:48:13 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Martino", "L.", ""], ["Elvira", "V.", ""], ["Luengo", "D.", ""], ["Corander", "J.", ""]]}, {"id": "1505.04771", "submitter": "Eric Malmi", "authors": "Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani Raiko, Aristides\n  Gionis", "title": "DopeLearning: A Computational Approach to Rap Lyrics Generation", "comments": "This is a pre-print of an article appearing at KDD'16", "journal-ref": null, "doi": "10.1145/2939672.2939679", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing rap lyrics requires both creativity to construct a meaningful,\ninteresting story and lyrical skills to produce complex rhyme patterns, which\nform the cornerstone of good flow. We present a rap lyrics generation method\nthat captures both of these aspects. First, we develop a prediction model to\nidentify the next line of existing lyrics from a set of candidate next lines.\nThis model is based on two machine-learning techniques: the RankSVM algorithm\nand a deep neural network model with a novel structure. Results show that the\nprediction model can identify the true next line among 299 randomly selected\nlines with an accuracy of 17%, i.e., over 50 times more likely than by random.\nSecond, we employ the prediction model to combine lines from existing songs,\nproducing lyrics with rhyme and a meaning. An evaluation of the produced lyrics\nshows that in terms of quantitative rhyme density, the method outperforms the\nbest human rappers by 21%. The rap lyrics generator has been deployed as an\nonline tool called DeepBeat, and the performance of the tool has been assessed\nby analyzing its usage logs. This analysis shows that machine-learned rankings\ncorrelate with user preferences.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:35:21 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 20:51:02 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Malmi", "Eric", ""], ["Takala", "Pyry", ""], ["Toivonen", "Hannu", ""], ["Raiko", "Tapani", ""], ["Gionis", "Aristides", ""]]}, {"id": "1505.04778", "submitter": "Soledad Villar", "authors": "Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar", "title": "On the tightness of an SDP relaxation of k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Recently, Awasthi et al. introduced an SDP relaxation of the $k$-means\nproblem in $\\mathbb R^m$. In this work, we consider a random model for the data\npoints in which $k$ balls of unit radius are deterministically distributed\nthroughout $\\mathbb R^m$, and then in each ball, $n$ points are drawn according\nto a common rotationally invariant probability distribution. For any fixed ball\nconfiguration and probability distribution, we prove that the SDP relaxation of\nthe $k$-means problem exactly recovers these planted clusters with probability\n$1-e^{-\\Omega(n)}$ provided the distance between any two of the ball centers is\n$>2+\\epsilon$, where $\\epsilon$ is an explicit function of the configuration of\nthe ball centers, and can be arbitrarily small when $m$ is large.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:50:00 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Iguchi", "Takayuki", ""], ["Mixon", "Dustin G.", ""], ["Peterson", "Jesse", ""], ["Villar", "Soledad", ""]]}, {"id": "1505.04966", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Mathieu Sinn, Pascal Frossard", "title": "Multi-task additive models with shared transfer functions based on\n  dictionary learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive models form a widely popular class of regression models which\nrepresent the relation between covariates and response variables as the sum of\nlow-dimensional transfer functions. Besides flexibility and accuracy, a key\nbenefit of these models is their interpretability: the transfer functions\nprovide visual means for inspecting the models and identifying domain-specific\nrelations between inputs and outputs. However, in large-scale problems\ninvolving the prediction of many related tasks, learning independently additive\nmodels results in a loss of model interpretability, and can cause overfitting\nwhen training data is scarce. We introduce a novel multi-task learning approach\nwhich provides a corpus of accurate and interpretable additive models for a\nlarge number of related forecasting tasks. Our key idea is to share transfer\nfunctions across models in order to reduce the model complexity and ease the\nexploration of the corpus. We establish a connection with sparse dictionary\nlearning and propose a new efficient fitting algorithm which alternates between\nsparse coding and transfer function updates. The former step is solved via an\nextension of Orthogonal Matching Pursuit, whose properties are analyzed using a\nnovel recovery condition which extends existing results in the literature. The\nlatter step is addressed using a traditional dictionary update rule.\nExperiments on real-world data demonstrate that our approach compares favorably\nto baseline methods while yielding an interpretable corpus of models, revealing\nstructure among the individual tasks and being more robust when training data\nis scarce. Our framework therefore extends the well-known benefits of additive\nmodels to common regression settings possibly involving thousands of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 12:36:22 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Sinn", "Mathieu", ""], ["Frossard", "Pascal", ""]]}, {"id": "1505.04984", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins and Joshua B. Tenenbaum", "title": "Risk and Regret of Hierarchical Bayesian Learners", "comments": "In Proceedings of the 32nd International Conference on Machine\n  Learning (ICML 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common statistical practice has shown that the full power of Bayesian methods\nis not realized until hierarchical priors are used, as these allow for greater\n\"robustness\" and the ability to \"share statistical strength.\" Yet it is an\nongoing challenge to provide a learning-theoretically sound formalism of such\nnotions that: offers practical guidance concerning when and how best to utilize\nhierarchical models; provides insights into what makes for a good hierarchical\nprior; and, when the form of the prior has been chosen, can guide the choice of\nhyperparameter settings. We present a set of analytical tools for understanding\nhierarchical priors in both the online and batch learning settings. We provide\nregret bounds under log-loss, which show how certain hierarchical models\ncompare, in retrospect, to the best single model in the model class. We also\nshow how to convert a Bayesian log-loss regret bound into a Bayesian risk bound\nfor any bounded loss, a result which may be of independent interest. Risk and\nregret bounds for Student's $t$ and hierarchical Gaussian priors allow us to\nformalize the concepts of \"robustness\" and \"sharing statistical strength.\"\nPriors for feature selection are investigated as well. Our results suggest that\nthe learning-theoretic benefits of using hierarchical priors can often come at\nlittle cost on practical problems.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 13:12:41 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1505.05004", "submitter": "Maxime Gasse", "authors": "Maxime Gasse (DM2L), Alex Aussem (DM2L), Haytham Elghazel (DM2L)", "title": "An Experimental Comparison of Hybrid Algorithms for Bayesian Network\n  Structure Learning", "comments": "arXiv admin note: text overlap with arXiv:1101.5184 by other authors.\n  Lecture notes in computer science, springer, 2012, Machine Learning and\n  Knowledge Discovery in Databases, 7523, pp.58-73", "journal-ref": null, "doi": "10.1007/978-3-642-33460-3_9", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hybrid algorithm for Bayesian network structure learning,\ncalled Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesian\nnetwork and then performs a Bayesian-scoring greedy hill-climbing search to\norient the edges. It is based on a subroutine called HPC, that combines ideas\nfrom incremental and divide-and-conquer constraint-based methods to learn the\nparents and children of a target variable. We conduct an experimental\ncomparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the\nmost powerful state-of-the-art algorithm for Bayesian network structure\nlearning, on several benchmarks with various data sizes. Our extensive\nexperiments show that H2PC outperforms MMHC both in terms of goodness of fit to\nnew data and in terms of the quality of the network structure itself, which is\ncloser to the true dependence structure of the data. The source code (in R) of\nH2PC as well as all data sets used for the empirical tests are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:15:10 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 10:19:46 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Gasse", "Maxime", "", "DM2L"], ["Aussem", "Alex", "", "DM2L"], ["Elghazel", "Haytham", "", "DM2L"]]}, {"id": "1505.05007", "submitter": "Paul Blomstedt PhD", "authors": "Paul Blomstedt, Ritabrata Dutta, Sohan Seth, Alvis Brazma and Samuel\n  Kaski", "title": "Modelling-based experiment retrieval: A case study with gene expression\n  clustering", "comments": "Updated figures. The final version of this article will appear in\n  Bioinformatics (https://bioinformatics.oxfordjournals.org/)", "journal-ref": null, "doi": "10.1093/bioinformatics/btv762", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Public and private repositories of experimental data are growing\nto sizes that require dedicated methods for finding relevant data. To improve\non the state of the art of keyword searches from annotations, methods for\ncontent-based retrieval have been proposed. In the context of gene expression\nexperiments, most methods retrieve gene expression profiles, requiring each\nexperiment to be expressed as a single profile, typically of case vs. control.\nA more general, recently suggested alternative is to retrieve experiments whose\nmodels are good for modelling the query dataset. However, for very noisy and\nhigh-dimensional query data, this retrieval criterion turns out to be very\nnoisy as well.\n  Results: We propose doing retrieval using a denoised model of the query\ndataset, instead of the original noisy dataset itself. To this end, we\nintroduce a general probabilistic framework, where each experiment is modelled\nseparately and the retrieval is done by finding related models. For retrieval\nof gene expression experiments, we use a probabilistic model called product\npartition model, which induces a clustering of genes that show similar\nexpression patterns across a number of samples. The suggested metric for\nretrieval using clusterings is the normalized information distance. Empirical\nresults finally suggest that inference for the full probabilistic model can be\napproximated with good performance using computationally faster heuristic\nclustering approaches (e.g. $k$-means). The method is highly scalable and\nstraightforward to apply to construct a general-purpose gene expression\nexperiment retrieval method.\n  Availability: The method can be implemented using standard clustering\nalgorithms and normalized information distance, available in many statistical\nsoftware packages.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:21:34 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 11:53:47 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 09:12:58 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2016 15:08:26 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Blomstedt", "Paul", ""], ["Dutta", "Ritabrata", ""], ["Seth", "Sohan", ""], ["Brazma", "Alvis", ""], ["Kaski", "Samuel", ""]]}, {"id": "1505.05114", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Emmanuel J. Candes", "title": "Solving Random Quadratic Systems of Equations Is Nearly as Easy as\n  Solving Linear Systems", "comments": "accepted to Communications on Pure and Applied Mathematics (CPAM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of solving quadratic systems of equations\nin $n$ variables, where $y_i = |\\langle \\boldsymbol{a}_i, \\boldsymbol{x}\n\\rangle|^2$, $i = 1, \\ldots, m$ and $\\boldsymbol{x} \\in \\mathbb{R}^n$ is\nunknown. We propose a novel method, which starting with an initial guess\ncomputed by means of a spectral method, proceeds by minimizing a nonconvex\nfunctional as in the Wirtinger flow approach. There are several key\ndistinguishing features, most notably, a distinct objective functional and\nnovel update rules, which operate in an adaptive fashion and drop terms bearing\ntoo much influence on the search direction. These careful selection rules\nprovide a tighter initial guess, better descent directions, and thus enhanced\npractical performance. On the theoretical side, we prove that for certain\nunstructured models of quadratic systems, our algorithms return the correct\nsolution in linear time, i.e. in time proportional to reading the data\n$\\{\\boldsymbol{a}_i\\}$ and $\\{y_i\\}$ as soon as the ratio $m/n$ between the\nnumber of equations and unknowns exceeds a fixed numerical constant. We extend\nthe theory to deal with noisy systems in which we only have $y_i \\approx\n|\\langle \\boldsymbol{a}_i, \\boldsymbol{x} \\rangle|^2$ and prove that our\nalgorithms achieve a statistical accuracy, which is nearly un-improvable. We\ncomplement our theoretical study with numerical examples showing that solving\nrandom quadratic systems is both computationally and statistically not much\nharder than solving linear systems of the same size---hence the title of this\npaper. For instance, we demonstrate empirically that the computational cost of\nour algorithm is about four times that of solving a least-squares problem of\nthe same size.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 18:37:07 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 17:05:16 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Chen", "Yuxin", ""], ["Candes", "Emmanuel J.", ""]]}, {"id": "1505.05208", "submitter": "Raajen Patel", "authors": "Raajen Patel, Thomas A. Goldstein, Eva L. Dyer, Azalia Mirhoseini, and\n  Richard G. Baraniuk", "title": "oASIS: Adaptive Column Sampling for Kernel Matrix Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel matrices (e.g. Gram or similarity matrices) are essential for many\nstate-of-the-art approaches to classification, clustering, and dimensionality\nreduction. For large datasets, the cost of forming and factoring such kernel\nmatrices becomes intractable. To address this challenge, we introduce a new\nadaptive sampling algorithm called Accelerated Sequential Incoherence Selection\n(oASIS) that samples columns without explicitly computing the entire kernel\nmatrix. We provide conditions under which oASIS is guaranteed to exactly\nrecover the kernel matrix with an optimal number of columns selected. Numerical\nexperiments on both synthetic and real-world datasets demonstrate that oASIS\nachieves performance comparable to state-of-the-art adaptive sampling methods\nat a fraction of the computational cost. The low runtime complexity of oASIS\nand its low memory footprint enable the solution of large problems that are\nsimply intractable using other adaptive methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 23:12:01 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Patel", "Raajen", ""], ["Goldstein", "Thomas A.", ""], ["Dyer", "Eva L.", ""], ["Mirhoseini", "Azalia", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1505.05215", "submitter": "Steve Hanneke", "authors": "Steve Hanneke, Varun Kanade, Liu Yang", "title": "Learning with a Drifting Target Concept", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning in the presence of a drifting target\nconcept. Specifically, we provide bounds on the error rate at a given time,\ngiven a learner with access to a history of independent samples labeled\naccording to a target concept that can change on each round. One of our main\ncontributions is a refinement of the best previous results for polynomial-time\nalgorithms for the space of linear separators under a uniform distribution. We\nalso provide general results for an algorithm capable of adapting to a variable\nrate of drift of the target concept. Some of the results also describe an\nactive learning variant of this setting, and provide bounds on the number of\nqueries for the labels of points in the sequence sufficient to obtain the\nstated bounds on the error rates.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 00:41:23 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Hanneke", "Steve", ""], ["Kanade", "Varun", ""], ["Yang", "Liu", ""]]}, {"id": "1505.05231", "submitter": "Steve Hanneke", "authors": "Liu Yang, Steve Hanneke, Jaime Carbonell", "title": "Bounds on the Minimax Rate for Estimating a Prior over a VC Class from\n  Independent Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal rates of convergence for estimating a prior distribution\nover a VC class from a sequence of independent data sets respectively labeled\nby independent target functions sampled from the prior. We specifically derive\nupper and lower bounds on the optimal rates under a smoothness condition on the\ncorrect prior, with the number of samples per data set equal the VC dimension.\nThese results have implications for the improvements achievable via transfer\nlearning. We additionally extend this setting to real-valued function, where we\nestablish consistency of an estimator for the prior, and discuss an additional\napplication to a preference elicitation problem in algorithmic economics.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 02:43:24 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Yang", "Liu", ""], ["Hanneke", "Steve", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1505.05233", "submitter": "Lei Zhang", "authors": "Lei Zhang and David Zhang", "title": "Visual Understanding via Multi-Feature Shared Learning with Global\n  Consistency", "comments": "13 pages,6 figures, this paper is accepted for publication in IEEE\n  Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2015.2510509", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image/video data is usually represented with multiple visual features. Fusion\nof multi-source information for establishing the attributes has been widely\nrecognized. Multi-feature visual recognition has recently received much\nattention in multimedia applications. This paper studies visual understanding\nvia a newly proposed l_2-norm based multi-feature shared learning framework,\nwhich can simultaneously learn a global label matrix and multiple\nsub-classifiers with the labeled multi-feature data. Additionally, a group\ngraph manifold regularizer composed of the Laplacian and Hessian graph is\nproposed for better preserving the manifold structure of each feature, such\nthat the label prediction power is much improved through the semi-supervised\nlearning with global label consistency. For convenience, we call the proposed\napproach Global-Label-Consistent Classifier (GLCC). The merits of the proposed\nmethod include: 1) the manifold structure information of each feature is\nexploited in learning, resulting in a more faithful classification owing to the\nglobal label consistency; 2) a group graph manifold regularizer based on the\nLaplacian and Hessian regularization is constructed; 3) an efficient\nalternative optimization method is introduced as a fast solver owing to the\nconvex sub-problems. Experiments on several benchmark visual datasets for\nmultimedia understanding, such as the 17-category Oxford Flower dataset, the\nchallenging 101-category Caltech dataset, the YouTube & Consumer Videos dataset\nand the large-scale NUS-WIDE dataset, demonstrate that the proposed approach\ncompares favorably with the state-of-the-art algorithms. An extensive\nexperiment on the deep convolutional activation features also show the\neffectiveness of the proposed approach. The code is available on\nhttp://www.escience.cn/people/lei/index.html\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 03:01:08 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 10:07:11 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1505.05310", "submitter": "Ahmed Hefny", "authors": "Ahmed Hefny, Carlton Downey and Geoffrey Gordon", "title": "Supervised Learning for Dynamical System Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been substantial interest in spectral methods for learning\ndynamical systems. These methods are popular since they often offer a good\ntradeoff between computational and statistical efficiency. Unfortunately, they\ncan be difficult to use and extend in practice: e.g., they can make it\ndifficult to incorporate prior information such as sparsity or structure. To\naddress this problem, we present a new view of dynamical system learning: we\nshow how to learn dynamical systems by solving a sequence of ordinary\nsupervised learning problems, thereby allowing users to incorporate prior\nknowledge via standard techniques such as L1 regularization. Many existing\nspectral methods are special cases of this new framework, using linear\nregression as the supervised learner. We demonstrate the effectiveness of our\nframework by showing examples where nonlinear regression or lasso let us learn\nbetter state representations than plain linear regression does; the correctness\nof these instances follows directly from our general analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 10:38:44 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 16:16:04 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Hefny", "Ahmed", ""], ["Downey", "Carlton", ""], ["Gordon", "Geoffrey", ""]]}, {"id": "1505.05401", "submitter": "Alfredo Braunstein", "authors": "Carlo Baldassi and Alfredo Braunstein", "title": "A Max-Sum algorithm for training discrete neural networks", "comments": null, "journal-ref": "Journal of Statistical Mechanics: Theory and Experiment 2015, no.\n  8, P08008", "doi": "10.1088/1742-5468/2015/08/P08008", "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient learning algorithm for the problem of training neural\nnetworks with discrete synapses, a well-known hard (NP-complete) discrete\noptimization problem. The algorithm is a variant of the so-called Max-Sum (MS)\nalgorithm. In particular, we show how, for bounded integer weights with $q$\ndistinct states and independent concave a priori distribution (e.g. $l_{1}$\nregularization), the algorithm's time complexity can be made to scale as\n$O\\left(N\\log N\\right)$ per node update, thus putting it on par with\nalternative schemes, such as Belief Propagation (BP), without resorting to\napproximations. Two special cases are of particular interest: binary synapses\n$W\\in\\{-1,1\\}$ and ternary synapses $W\\in\\{-1,0,1\\}$ with $l_{0}$\nregularization. The algorithm we present performs as well as BP on binary\nperceptron learning problems, and may be better suited to address the problem\non fully-connected two-layer networks, since inherent symmetries in two layer\nnetworks are naturally broken using the MS approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 14:34:23 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 11:49:49 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Baldassi", "Carlo", ""], ["Braunstein", "Alfredo", ""]]}, {"id": "1505.05424", "submitter": "Charles Blundell", "authors": "Charles Blundell and Julien Cornebise and Koray Kavukcuoglu and Daan\n  Wierstra", "title": "Weight Uncertainty in Neural Networks", "comments": "In Proceedings of the 32nd International Conference on Machine\n  Learning (ICML 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new, efficient, principled and backpropagation-compatible\nalgorithm for learning a probability distribution on the weights of a neural\nnetwork, called Bayes by Backprop. It regularises the weights by minimising a\ncompression cost, known as the variational free energy or the expected lower\nbound on the marginal likelihood. We show that this principled kind of\nregularisation yields comparable performance to dropout on MNIST\nclassification. We then demonstrate how the learnt uncertainty in the weights\ncan be used to improve generalisation in non-linear regression problems, and\nhow this weight uncertainty can be used to drive the exploration-exploitation\ntrade-off in reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 15:39:48 GMT"}, {"version": "v2", "created": "Thu, 21 May 2015 14:07:23 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Blundell", "Charles", ""], ["Cornebise", "Julien", ""], ["Kavukcuoglu", "Koray", ""], ["Wierstra", "Daan", ""]]}, {"id": "1505.05451", "submitter": "Nasser Ghadiri", "authors": "Javad Salimi Sartakhti, Homayun Afrabandpey, Nasser Ghadiri", "title": "Fuzzy Least Squares Twin Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least Squares Twin Support Vector Machine (LST-SVM) has been shown to be an\nefficient and fast algorithm for binary classification. It combines the\noperating principles of Least Squares SVM (LS-SVM) and Twin SVM (T-SVM); it\nconstructs two non-parallel hyperplanes (as in T-SVM) by solving two systems of\nlinear equations (as in LS-SVM). Despite its efficiency, LST-SVM is still\nunable to cope with two features of real-world problems. First, in many\nreal-world applications, labels of samples are not deterministic; they come\nnaturally with their associated membership degrees. Second, samples in\nreal-world applications may not be equally important and their importance\ndegrees affect the classification. In this paper, we propose Fuzzy LST-SVM\n(FLST-SVM) to deal with these two characteristics of real-world data. Two\nmodels are introduced for FLST-SVM: the first model builds up crisp hyperplanes\nusing training samples and their corresponding membership degrees. The second\nmodel, on the other hand, constructs fuzzy hyperplanes using training samples\nand their membership degrees. Numerical evaluation of the proposed method with\nsynthetic and real datasets demonstrate significant improvement in the\nclassification accuracy of FLST-SVM when compared to well-known existing\nversions of SVM.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 16:57:02 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 08:59:40 GMT"}, {"version": "v3", "created": "Wed, 21 Nov 2018 21:05:16 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Sartakhti", "Javad Salimi", ""], ["Afrabandpey", "Homayun", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1505.05561", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Yingbo Zhou, Hung Ngo, Venu Govindaraju", "title": "Why Regularized Auto-Encoders learn Sparse Representation?", "comments": "8 pages of content, 1 page of reference, 4 pages of supplementary.\n  ICML 2016; bug fix in lemma 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the authors of Batch Normalization (BN) identify and address an\nimportant problem involved in training deep networks-- \\textit{Internal\nCovariate Shift}-- the current solution has certain drawbacks. For instance, BN\ndepends on batch statistics for layerwise input normalization during training\nwhich makes the estimates of mean and standard deviation of input\n(distribution) to hidden layers inaccurate due to shifting parameter values\n(especially during initial training epochs). Another fundamental problem with\nBN is that it cannot be used with batch-size $ 1 $ during training. We address\nthese drawbacks of BN by proposing a non-adaptive normalization technique for\nremoving covariate shift, that we call \\textit{Normalization Propagation}. Our\napproach does not depend on batch statistics, but rather uses a\ndata-independent parametric estimate of mean and standard-deviation in every\nlayer thus being computationally faster compared with BN. We exploit the\nobservation that the pre-activation before Rectified Linear Units follow\nGaussian distribution in deep networks, and that once the first and second\norder statistics of any given dataset are normalized, we can forward propagate\nthis normalization without the need for recalculating the approximate\nstatistics for hidden layers.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 00:10:46 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 19:22:37 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 15:29:29 GMT"}, {"version": "v4", "created": "Mon, 23 May 2016 23:04:21 GMT"}, {"version": "v5", "created": "Fri, 17 Jun 2016 23:01:20 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Arpit", "Devansh", ""], ["Zhou", "Yingbo", ""], ["Ngo", "Hung", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1505.05572", "submitter": "Colin LaMont", "authors": "Paul A. Wiggins, Colin H. LaMont", "title": "The development of an information criterion for Change-Point Analysis", "comments": "10 pages + supplement. 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change-point analysis is a flexible and computationally tractable tool for\nthe analysis of times series data from systems that transition between discrete\nstates and whose observables are corrupted by noise. The change-point algorithm\nis used to identify the time indices (change points) at which the system\ntransitions between these discrete states. We present a unified\ninformation-based approach to testing for the existence of change points. This\nnew approach reconciles two previously disparate approaches to Change-Point\nAnalysis (frequentist and information-based) for testing transitions between\nstates. The resulting method is statistically principled, parameter and prior\nfree and widely applicable to a wide range of change-point problems.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 01:17:27 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Wiggins", "Paul A.", ""], ["LaMont", "Colin H.", ""]]}, {"id": "1505.05612", "submitter": "Junhua Mao", "authors": "Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu", "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image\n  Question Answering", "comments": "Dataset released on the project page, see\n  http://idl.baidu.com/FM-IQA.html ; NIPS 2015 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the mQA model, which is able to answer questions\nabout the content of an image. The answer can be a sentence, a phrase or a\nsingle word. Our model contains four components: a Long Short-Term Memory\n(LSTM) to extract the question representation, a Convolutional Neural Network\n(CNN) to extract the visual representation, an LSTM for storing the linguistic\ncontext in an answer, and a fusing component to combine the information from\nthe first three components and generate the answer. We construct a Freestyle\nMultilingual Image Question Answering (FM-IQA) dataset to train and evaluate\nour mQA model. It contains over 150,000 images and 310,000 freestyle Chinese\nquestion-answer pairs and their English translations. The quality of the\ngenerated answers of our mQA model on this dataset is evaluated by human judges\nthrough a Turing Test. Specifically, we mix the answers provided by humans and\nour model. The human judges need to distinguish our model from the human. They\nwill also provide a score (i.e. 0, 1, 2, the larger the better) indicating the\nquality of the answer. We propose strategies to monitor the quality of this\nevaluation process. The experiments show that in 64.7% of cases, the human\njudges cannot distinguish our model from humans. The average score is 1.454\n(1.918 for human). The details of this work, including the FM-IQA dataset, can\nbe found on the project page: http://idl.baidu.com/FM-IQA.html\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 06:09:36 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 07:45:46 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 21:12:15 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Gao", "Haoyuan", ""], ["Mao", "Junhua", ""], ["Zhou", "Jie", ""], ["Huang", "Zhiheng", ""], ["Wang", "Lei", ""], ["Xu", "Wei", ""]]}, {"id": "1505.05629", "submitter": "Stefano Trac\\`a", "authors": "Stefano Trac\\`a, Cynthia Rudin, and Weiyu Yan", "title": "Regulating Greed Over Time in Multi-Armed Bandits", "comments": "Published in JMLR (Journal of Machine Learning Research). The paper\n  contains algorithms, theorems, proofs, and experimental results with\n  synthetic and real data. url: http://jmlr.org/papers/v22/17-720.html", "journal-ref": "Journal of Machine Learning Research, volume 22, number 3, pages\n  1-99, year 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In retail, there are predictable yet dramatic time-dependent patterns in\ncustomer behavior, such as periodic changes in the number of visitors, or\nincreases in customers just before major holidays. The current paradigm of\nmulti-armed bandit analysis does not take these known patterns into account.\nThis means that for applications in retail, where prices are fixed for periods\nof time, current bandit algorithms will not suffice. This work provides a\nremedy that takes the time-dependent patterns into account, and we show how\nthis remedy is implemented for the UCB, $\\varepsilon$-greedy, and UCB-L\nalgorithms, and also through a new policy called the variable arm pool\nalgorithm. In the corrected methods, exploitation (greed) is regulated over\ntime, so that more exploitation occurs during higher reward periods, and more\nexploration occurs in periods of low reward. In order to understand why regret\nis reduced with the corrected methods, we present a set of bounds that provide\ninsight into why we would want to exploit during periods of high reward, and\ndiscuss the impact on regret. Our proposed methods perform well in experiments,\nand were inspired by a high-scoring entry in the Exploration and Exploitation 3\ncontest using data from Yahoo$!$ Front Page. That entry heavily used\ntime-series methods to regulate greed over time, which was substantially more\neffective than other contextual bandit methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 07:34:56 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 03:50:58 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 22:18:33 GMT"}, {"version": "v4", "created": "Sat, 13 Feb 2021 05:55:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Trac\u00e0", "Stefano", ""], ["Rudin", "Cynthia", ""], ["Yan", "Weiyu", ""]]}, {"id": "1505.05663", "submitter": "Thibaut Horel", "authors": "Jean Pouget-Abadie, Thibaut Horel", "title": "Inferring Graphs from Cascades: A Sparse Recovery Framework", "comments": "Full version of the ICML paper with the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Network Inference problem, one seeks to recover the edges of an\nunknown graph from the observations of cascades propagating over this graph. In\nthis paper, we approach this problem from the sparse recovery perspective. We\nintroduce a general model of cascades, including the voter model and the\nindependent cascade model, for which we provide the first algorithm which\nrecovers the graph's edges with high probability and $O(s\\log m)$ measurements\nwhere $s$ is the maximum degree of the graph and $m$ is the number of nodes.\nFurthermore, we show that our algorithm also recovers the edge weights (the\nparameters of the diffusion process) and is robust in the context of\napproximate sparsity. Finally we prove an almost matching lower bound of\n$\\Omega(s\\log\\frac{m}{s})$ and validate our approach empirically on synthetic\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 10:04:42 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Pouget-Abadie", "Jean", ""], ["Horel", "Thibaut", ""]]}, {"id": "1505.05667", "submitter": "Xipeng Qiu", "authors": "Chenxi Zhu, Xipeng Qiu, Xinchi Chen, Xuanjing Huang", "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem to model all the nodes (words or\nphrases) in a dependency tree with the dense representations. We propose a\nrecursive convolutional neural network (RCNN) architecture to capture syntactic\nand compositional-semantic representations of phrases and words in a dependency\ntree. Different with the original recursive neural network, we introduce the\nconvolution and pooling layers, which can model a variety of compositions by\nthe feature maps and choose the most informative compositions by the pooling\nlayers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list\nof candidate dependency parsing trees. The experiments show that RCNN is very\neffective to improve the state-of-the-art dependency parsing on both English\nand Chinese datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 10:23:10 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Zhu", "Chenxi", ""], ["Qiu", "Xipeng", ""], ["Chen", "Xinchi", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1505.05723", "submitter": "Indre Zliobaite", "authors": "Indre Zliobaite", "title": "On the relation between accuracy and fairness in binary classification", "comments": "Accepted for presentation to the 2nd workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (http://www.fatml.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our study revisits the problem of accuracy-fairness tradeoff in binary\nclassification. We argue that comparison of non-discriminatory classifiers\nneeds to account for different rates of positive predictions, otherwise\nconclusions about performance may be misleading, because accuracy and\ndiscrimination of naive baselines on the same dataset vary with different rates\nof positive predictions. We provide methodological recommendations for sound\ncomparison of non-discriminatory classifiers, and present a brief theoretical\nand empirical analysis of tradeoffs between accuracy and non-discrimination.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 13:20:06 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Zliobaite", "Indre", ""]]}, {"id": "1505.05770", "submitter": "Danilo Jimenez Rezende", "authors": "Danilo Jimenez Rezende and Shakir Mohamed", "title": "Variational Inference with Normalizing Flows", "comments": "Proceedings of the 32nd International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 15:36:37 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 09:13:28 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 15:46:33 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 18:36:32 GMT"}, {"version": "v5", "created": "Mon, 13 Jun 2016 08:46:44 GMT"}, {"version": "v6", "created": "Tue, 14 Jun 2016 09:01:36 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1505.05798", "submitter": "Haitham Bou Ammar PhD", "authors": "Haitham Bou Ammar, Rasul Tutunov, Eric Eaton", "title": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear\n  Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong reinforcement learning provides a promising framework for developing\nversatile agents that can accumulate knowledge over a lifetime of experience\nand rapidly learn new tasks by building upon prior knowledge. However, current\nlifelong learning methods exhibit non-vanishing regret as the amount of\nexperience increases and include limitations that can lead to suboptimal or\nunsafe control policies. To address these issues, we develop a lifelong policy\ngradient learner that operates in an adversarial set- ting to learn multiple\ntasks online while enforcing safety constraints on the learned policies. We\ndemonstrate, for the first time, sublinear regret for lifelong policy search,\nand validate our algorithm on several benchmark dynamical systems and an\napplication to quadrotor control.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 17:24:57 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Ammar", "Haitham Bou", ""], ["Tutunov", "Rasul", ""], ["Eaton", "Eric", ""]]}, {"id": "1505.05800", "submitter": "Amit Daniely", "authors": "Amit Daniely", "title": "Complexity Theoretic Limitations on Learning Halfspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of agnostically learning halfspaces which is defined by\na fixed but unknown distribution $\\mathcal{D}$ on $\\mathbb{Q}^n\\times \\{\\pm\n1\\}$. We define $\\mathrm{Err}_{\\mathrm{HALF}}(\\mathcal{D})$ as the least error\nof a halfspace classifier for $\\mathcal{D}$. A learner who can access\n$\\mathcal{D}$ has to return a hypothesis whose error is small compared to\n$\\mathrm{Err}_{\\mathrm{HALF}}(\\mathcal{D})$.\n  Using the recently developed method of the author, Linial and Shalev-Shwartz\nwe prove hardness of learning results under a natural assumption on the\ncomplexity of refuting random $K$-$\\mathrm{XOR}$ formulas. We show that no\nefficient learning algorithm has non-trivial worst-case performance even under\nthe guarantees that $\\mathrm{Err}_{\\mathrm{HALF}}(\\mathcal{D}) \\le \\eta$ for\narbitrarily small constant $\\eta>0$, and that $\\mathcal{D}$ is supported in\n$\\{\\pm 1\\}^n\\times \\{\\pm 1\\}$. Namely, even under these favorable conditions\nits error must be $\\ge \\frac{1}{2}-\\frac{1}{n^c}$ for every $c>0$. In\nparticular, no efficient algorithm can achieve a constant approximation ratio.\nUnder a stronger version of the assumption (where $K$ can be poly-logarithmic\nin $n$), we can take $\\eta = 2^{-\\log^{1-\\nu}(n)}$ for arbitrarily small\n$\\nu>0$. Interestingly, this is even stronger than the best known lower bounds\n(Arora et. al. 1993, Feldamn et. al. 2006, Guruswami and Raghavendra 2006) for\nthe case that the learner is restricted to return a halfspace classifier (i.e.\nproper learning).\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 17:30:54 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2016 06:23:35 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Daniely", "Amit", ""]]}, {"id": "1505.05969", "submitter": "Jonathan Huang", "authors": "Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran\n  Sahami, Leonidas Guibas", "title": "Learning Program Embeddings to Propagate Feedback on Student Code", "comments": "Accepted to International Conference on Machine Learning (ICML 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing feedback, both assessing final work and giving hints to stuck\nstudents, is difficult for open-ended assignments in massive online classes\nwhich can range from thousands to millions of students. We introduce a neural\nnetwork method to encode programs as a linear mapping from an embedded\nprecondition space to an embedded postcondition space and propose an algorithm\nfor feedback at scale using these linear maps as features. We apply our\nalgorithm to assessments from the Code.org Hour of Code and Stanford\nUniversity's CS1 course, where we propagate human comments on student\nassignments to orders of magnitude more submissions.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 07:03:45 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Piech", "Chris", ""], ["Huang", "Jonathan", ""], ["Nguyen", "Andy", ""], ["Phulsuksombati", "Mike", ""], ["Sahami", "Mehran", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1505.05972", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Instant Learning: Parallel Deep Neural Networks and Convolutional\n  Bootstrapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks (DNN) are able to scale with direct advances in\ncomputational power (e.g., memory and processing speed), they are not well\nsuited to exploit the recent trends for parallel architectures. In particular,\ngradient descent is a sequential process and the resulting serial dependencies\nmean that DNN training cannot be parallelized effectively. Here, we show that a\nDNN may be replicated over a massive parallel architecture and used to provide\na cumulative sampling of local solution space which results in rapid and robust\nlearning. We introduce a complimentary convolutional bootstrapping approach\nthat enhances performance of the parallel architecture further. Our\nparallelized convolutional bootstrapping DNN out-performs an identical\nfully-trained traditional DNN after only a single iteration of training.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 07:24:14 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 09:33:33 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1505.06125", "submitter": "David Mascharka", "authors": "David Mascharka and Eric Manley", "title": "Machine Learning for Indoor Localization Using Mobile Phone-Based\n  Sensors", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": "10.1109/CCNC.2016.7444919", "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of localizing a mobile device based\non readings from its embedded sensors utilizing machine learning methodologies.\nWe consider a real-world environment, collect a large dataset of 3110\ndatapoints, and examine the performance of a substantial number of machine\nlearning algorithms in localizing a mobile device. We have found algorithms\nthat give a mean error as accurate as 0.76 meters, outperforming other indoor\nlocalization systems reported in the literature. We also propose a hybrid\ninstance-based approach that results in a speed increase by a factor of ten\nwith no loss of accuracy in a live deployment over standard instance-based\nmethods, allowing for fast and accurate localization. Further, we determine how\nsmaller datasets collected with less density affect accuracy of localization,\nimportant for use in real-world environments. Finally, we demonstrate that\nthese approaches are appropriate for real-world deployment by evaluating their\nperformance in an online, in-motion experiment.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 15:39:52 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Mascharka", "David", ""], ["Manley", "Eric", ""]]}, {"id": "1505.06169", "submitter": "Emma Strubell", "authors": "Emma Strubell, Luke Vilnis, Kate Silverstein, Andrew McCallum", "title": "Learning Dynamic Feature Selection for Fast Sequential Prediction", "comments": "Appears in The 53rd Annual Meeting of the Association for\n  Computational Linguistics, Beijing, China, July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present paired learning and inference algorithms for significantly\nreducing computation and increasing speed of the vector dot products in the\nclassifiers that are at the heart of many NLP components. This is accomplished\nby partitioning the features into a sequence of templates which are ordered\nsuch that high confidence can often be reached using only a small fraction of\nall features. Parameter estimation is arranged to maximize accuracy and early\nconfidence in this sequence. Our approach is simpler and better suited to NLP\nthan other related cascade methods. We present experiments in left-to-right\npart-of-speech tagging, named entity recognition, and transition-based\ndependency parsing. On the typical benchmarking datasets we can preserve POS\ntagging accuracy above 97% and parsing LAS above 88.5% both with over a\nfive-fold reduction in run-time, and NER F1 above 88 with more than 2x increase\nin speed.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 18:28:21 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Strubell", "Emma", ""], ["Vilnis", "Luke", ""], ["Silverstein", "Kate", ""], ["McCallum", "Andrew", ""]]}, {"id": "1505.06249", "submitter": "Alexandre Drouin", "authors": "Alexandre Drouin, S\\'ebastien Gigu\\`ere, Maxime D\\'eraspe,\n  Fran\\c{c}ois Laviolette, Mario Marchand, Jacques Corbeil", "title": "Greedy Biomarker Discovery in the Genome with Applications to\n  Antimicrobial Resistance", "comments": "Peer-reviewed and accepted for an oral presentation in the Greed is\n  Great workshop at the International Conference on Machine Learning, Lille,\n  France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Set Covering Machine (SCM) is a greedy learning algorithm that produces\nsparse classifiers. We extend the SCM for datasets that contain a huge number\nof features. The whole genetic material of living organisms is an example of\nsuch a case, where the number of feature exceeds 10^7. Three human pathogens\nwere used to evaluate the performance of the SCM at predicting antimicrobial\nresistance. Our results show that the SCM compares favorably in terms of\nsparsity and accuracy against L1 and L2 regularized Support Vector Machines and\nCART decision trees. Moreover, the SCM was the only algorithm that could\nconsider the full feature space. For all other algorithms, the latter had to be\nfiltered as a preprocessing step.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 23:29:40 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Drouin", "Alexandre", ""], ["Gigu\u00e8re", "S\u00e9bastien", ""], ["D\u00e9raspe", "Maxime", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""]]}, {"id": "1505.06279", "submitter": "Bernardino Romera-Paredes", "authors": "Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes", "title": "The Benefit of Multitask Representation Learning", "comments": "To appear in Journal of Machine Learning Research (JMLR). 31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a general method to learn data representations from multiple\ntasks. We provide a justification for this method in both settings of multitask\nlearning and learning-to-learn. The method is illustrated in detail in the\nspecial case of linear feature learning. Conditions on the theoretical\nadvantage offered by multitask representation learning over independent task\nlearning are established. In particular, focusing on the important example of\nhalf-space learning, we derive the regime in which multitask representation\nlearning is beneficial over independent task learning, as a function of the\nsample size, the number of tasks and the intrinsic data dimensionality. Other\npotential applications of our results include multitask feature learning in\nreproducing kernel Hilbert spaces and multilayer, deep networks.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 06:37:17 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 11:03:42 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""], ["Romera-Paredes", "Bernardino", ""]]}, {"id": "1505.06292", "submitter": "Or Zuk", "authors": "Avishai Wagner and Or Zuk", "title": "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements", "comments": "ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a row-and-column affine measurement scheme for low-rank\nmatrix recovery. Each measurement is a linear combination of elements in one\nrow or one column of a matrix $X$. This setting arises naturally in\napplications from different domains. However, current algorithms developed for\nstandard matrix recovery problems do not perform well in our case, hence the\nneed for developing new algorithms and theory for our problem. We propose a\nsimple algorithm for the problem based on Singular Value Decomposition ($SVD$)\nand least-squares ($LS$), which we term \\alg. We prove that (a simplified\nversion of) our algorithm can recover $X$ exactly with the minimum possible\nnumber of measurements in the noiseless case. In the general noisy case, we\nprove performance guarantees on the reconstruction accuracy under the Frobenius\nnorm. In simulations, our row-and-column design and \\alg algorithm show\nimproved speed, and comparable and in some cases better accuracy compared to\nstandard measurements designs and algorithms. Our theoretical and experimental\nresults suggest that the proposed row-and-column affine measurements scheme,\ntogether with our recovery algorithm, may provide a powerful framework for\naffine matrix reconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 08:45:20 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Wagner", "Avishai", ""], ["Zuk", "Or", ""]]}, {"id": "1505.06378", "submitter": "Maya Gupta", "authors": "Maya Gupta, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin\n  Canini, Alexander Mangylov, Wojtek Moczydlowski and Alex van Esbroeck", "title": "Monotonic Calibrated Interpolated Look-Up Tables", "comments": "To appear (with minor revisions), Journal Machine Learning Research\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world machine learning applications may require functions that are\nfast-to-evaluate and interpretable. In particular, guaranteed monotonicity of\nthe learned function can be critical to user trust. We propose meeting these\ngoals for low-dimensional machine learning problems by learning flexible,\nmonotonic functions using calibrated interpolated look-up tables. We extend the\nstructural risk minimization framework of lattice regression to train monotonic\nlook-up tables by solving a convex problem with appropriate linear inequality\nconstraints. In addition, we propose jointly learning interpretable\ncalibrations of each feature to normalize continuous features and handle\ncategorical or missing data, at the cost of making the objective non-convex. We\naddress large-scale learning through parallelization, mini-batching, and\npropose random sampling of additive regularizer terms. Case studies with\nreal-world problems with five to sixteen features and thousands to millions of\ntraining samples demonstrate the proposed monotonic functions can achieve\nstate-of-the-art accuracy on practical problems while providing greater\ntransparency to users.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 20:57:58 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 21:49:53 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2016 22:54:21 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Gupta", "Maya", ""], ["Cotter", "Andrew", ""], ["Pfeifer", "Jan", ""], ["Voevodski", "Konstantin", ""], ["Canini", "Kevin", ""], ["Mangylov", "Alexander", ""], ["Moczydlowski", "Wojtek", ""], ["van Esbroeck", "Alex", ""]]}, {"id": "1505.06405", "submitter": "Lei Zhang", "authors": "Lei Zhang and David Zhang", "title": "Domain Adaptation Extreme Learning Machines for Drift Compensation in\n  E-nose Systems", "comments": "11 pages, 9 figures, to appear in IEEE Transactions on\n  Instrumentation and Measurement", "journal-ref": null, "doi": "10.1109/TIM.2014.2367775", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses an important issue, known as sensor drift that behaves a\nnonlinear dynamic property in electronic nose (E-nose), from the viewpoint of\nmachine learning. Traditional methods for drift compensation are laborious and\ncostly due to the frequent acquisition and labeling process for gases samples\nrecalibration. Extreme learning machines (ELMs) have been confirmed to be\nefficient and effective learning techniques for pattern recognition and\nregression. However, ELMs primarily focus on the supervised, semi-supervised\nand unsupervised learning problems in single domain (i.e. source domain). To\nour best knowledge, ELM with cross-domain learning capability has never been\nstudied. This paper proposes a unified framework, referred to as Domain\nAdaptation Extreme Learning Machine (DAELM), which learns a robust classifier\nby leveraging a limited number of labeled data from target domain for drift\ncompensation as well as gases recognition in E-nose systems, without loss of\nthe computational efficiency and learning ability of traditional ELM. In the\nunified framework, two algorithms called DAELM-S and DAELM-T are proposed for\nthe purpose of this paper, respectively. In order to percept the differences\namong ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on the\npopular sensor drift data with multiple batches collected by E-nose system\nclearly demonstrate that the proposed DAELM significantly outperforms existing\ndrift compensation methods without cumbersome measures, and also bring new\nperspectives for ELM.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 04:34:27 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1505.06427", "submitter": "Lantian Li Mr.", "authors": "Lantian Li and Dong Wang and Zhiyong Zhang and Thomas Fang Zheng", "title": "Deep Speaker Vectors for Semi Text-independent Speaker Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that deep neural networks (DNNs) can be used to extract\ndeep speaker vectors (d-vectors) that preserve speaker characteristics and can\nbe used in speaker verification. This new method has been tested on\ntext-dependent speaker verification tasks, and improvement was reported when\ncombined with the conventional i-vector method.\n  This paper extends the d-vector approach to semi text-independent speaker\nverification tasks, i.e., the text of the speech is in a limited set of short\nphrases. We explore various settings of the DNN structure used for d-vector\nextraction, and present a phone-dependent training which employs the posterior\nfeatures obtained from an ASR system. The experimental results show that it is\npossible to apply d-vectors on semi text-independent speaker recognition, and\nthe phone-dependent training improves system performance.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 11:22:40 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Li", "Lantian", ""], ["Wang", "Dong", ""], ["Zhang", "Zhiyong", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1505.06443", "submitter": "Timos Papadopoulos", "authors": "Timos Papadopoulos, Stephen Roberts and Kathy Willis", "title": "Detecting bird sound in unknown acoustic background using crowdsourced\n  training data", "comments": "Submitted to 'Big Data Sciences for Bioacoustic Environmental\n  Survey', 10th Advanced Multimodal Information Retrieval int'l summer school,\n  Ermites 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biodiversity monitoring using audio recordings is achievable at a truly\nglobal scale via large-scale deployment of inexpensive, unattended recording\nstations or by large-scale crowdsourcing using recording and species\nrecognition on mobile devices. The ability, however, to reliably identify\nvocalising animal species is limited by the fact that acoustic signatures of\ninterest in such recordings are typically embedded in a diverse and complex\nacoustic background. To avoid the problems associated with modelling such\nbackgrounds, we build generative models of bird sounds and use the concept of\nnovelty detection to screen recordings to detect sections of data which are\nlikely bird vocalisations. We present detection results against various\nacoustic environments and different signal-to-noise ratios. We discuss the\nissues related to selecting the cost function and setting detection thresholds\nin such algorithms. Our methods are designed to be scalable and automatically\napplicable to arbitrary selections of species depending on the specific\ngeographic region and time period of deployment.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 14:58:41 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Papadopoulos", "Timos", ""], ["Roberts", "Stephen", ""], ["Willis", "Kathy", ""]]}, {"id": "1505.06449", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Charles Elkan", "title": "Efficient Elastic Net Regularization for Sparse Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for efficient training of sparse linear\nmodels with elastic net regularization. Extending previous work on delayed\nupdates, the new algorithm applies stochastic gradient updates to non-zero\nfeatures only, bringing weights current as needed with closed-form updates.\nClosed-form delayed updates for the $\\ell_1$, $\\ell_{\\infty}$, and rarely used\n$\\ell_2$ regularizers have been described previously. This paper provides\nclosed-form updates for the popular squared norm $\\ell^2_2$ and elastic net\nregularizers.\n  We provide dynamic programming algorithms that perform each delayed update in\nconstant time. The new $\\ell^2_2$ and elastic net methods handle both fixed and\nvarying learning rates, and both standard {stochastic gradient descent} (SGD)\nand {forward backward splitting (FoBoS)}. Experimental results show that on a\nbag-of-words dataset with $260,941$ features, but only $88$ nonzero features on\naverage per training example, the dynamic programming method trains a logistic\nregression classifier with elastic net regularization over $2000$ times faster\nthan otherwise.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 15:42:58 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 07:28:50 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2015 20:44:57 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Elkan", "Charles", ""]]}, {"id": "1505.06478", "submitter": "Syama Sundar Rangapuram", "authors": "Syama Sundar Rangapuram, Pramod Kaushik Mudrakarta and Matthias Hein", "title": "Tight Continuous Relaxation of the Balanced $k$-Cut Problem", "comments": "Long version of paper accepted at NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral Clustering as a relaxation of the normalized/ratio cut has become\none of the standard graph-based clustering methods. Existing methods for the\ncomputation of multiple clusters, corresponding to a balanced $k$-cut of the\ngraph, are either based on greedy techniques or heuristics which have weak\nconnection to the original motivation of minimizing the normalized cut. In this\npaper we propose a new tight continuous relaxation for any balanced $k$-cut\nproblem and show that a related recently proposed relaxation is in most cases\nloose leading to poor performance in practice. For the optimization of our\ntight continuous relaxation we propose a new algorithm for the difficult\nsum-of-ratios minimization problem which achieves monotonic descent. Extensive\ncomparisons show that our method outperforms all existing approaches for ratio\ncut and other balanced $k$-cut criteria.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 20:37:10 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Rangapuram", "Syama Sundar", ""], ["Mudrakarta", "Pramod Kaushik", ""], ["Hein", "Matthias", ""]]}, {"id": "1505.06485", "submitter": "Syama Sundar Rangapuram", "authors": "Syama Sundar Rangapuram and Matthias Hein", "title": "Constrained 1-Spectral Clustering", "comments": "Long version of paper accepted at AISTATS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important form of prior information in clustering comes in form of\ncannot-link and must-link constraints. We present a generalization of the\npopular spectral clustering technique which integrates such constraints.\nMotivated by the recently proposed $1$-spectral clustering for the\nunconstrained problem, our method is based on a tight relaxation of the\nconstrained normalized cut into a continuous optimization problem. Opposite to\nall other methods which have been suggested for constrained spectral\nclustering, we can always guarantee to satisfy all constraints. Moreover, our\nsoft formulation allows to optimize a trade-off between normalized cut and the\nnumber of violated constraints. An efficient implementation is provided which\nscales to large datasets. We outperform consistently all other proposed methods\nin the experiments.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 21:25:44 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Rangapuram", "Syama Sundar", ""], ["Hein", "Matthias", ""]]}, {"id": "1505.06531", "submitter": "Tsu-Wei Chen", "authors": "Tsu-Wei Chen, Meena Abdelmaseeh, Daniel Stashuk", "title": "Affine and Regional Dynamic Time Warpng", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pointwise matches between two time series are of great importance in time\nseries analysis, and dynamic time warping (DTW) is known to provide generally\nreasonable matches. There are situations where time series alignment should be\ninvariant to scaling and offset in amplitude or where local regions of the\nconsidered time series should be strongly reflected in pointwise matches. Two\ndifferent variants of DTW, affine DTW (ADTW) and regional DTW (RDTW), are\nproposed to handle scaling and offset in amplitude and provide regional\nemphasis respectively. Furthermore, ADTW and RDTW can be combined in two\ndifferent ways to generate alignments that incorporate advantages from both\nmethods, where the affine model can be applied either globally to the entire\ntime series or locally to each region. The proposed alignment methods\noutperform DTW on specific simulated datasets, and one-nearest-neighbor\nclassifiers using their associated difference measures are competitive with the\ndifference measures associated with state-of-the-art alignment methods on real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 03:23:31 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Chen", "Tsu-Wei", ""], ["Abdelmaseeh", "Meena", ""], ["Stashuk", "Daniel", ""]]}, {"id": "1505.06538", "submitter": "J Massey Cashore", "authors": "J. Massey Cashore, Xiaoting Zhao, Alexander A. Alemi, Yujia Liu, Peter\n  I. Frazier", "title": "Clustering via Content-Augmented Stochastic Blockmodels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the data being created on the web contains interactions between users\nand items. Stochastic blockmodels, and other methods for community detection\nand clustering of bipartite graphs, can infer latent user communities and\nlatent item clusters from this interaction data. These methods, however,\ntypically ignore the items' contents and the information they provide about\nitem clusters, despite the tendency of items in the same latent cluster to\nshare commonalities in content. We introduce content-augmented stochastic\nblockmodels (CASB), which use item content together with user-item interaction\ndata to enhance the user communities and item clusters learned. Comparisons to\nseveral state-of-the-art benchmark methods, on datasets arising from scientists\ninteracting with scientific articles, show that content-augmented stochastic\nblockmodels provide highly accurate clusters with respect to metrics\nrepresentative of the underlying community structure.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 04:19:12 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Cashore", "J. Massey", ""], ["Zhao", "Xiaoting", ""], ["Alemi", "Alexander A.", ""], ["Liu", "Yujia", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1505.06556", "submitter": "Chencheng Li", "authors": "Chencheng Li and Pan Zhou", "title": "Differentially Private Distributed Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning has been in the spotlight from the machine learning society\nfor a long time. To handle massive data in Big Data era, one single learner\ncould never efficiently finish this heavy task. Hence, in this paper, we\npropose a novel distributed online learning algorithm to solve the problem.\nComparing to typical centralized online learner, the distributed learners\noptimize their own learning parameters based on local data sources and timely\ncommunicate with neighbors. However, communication may lead to a privacy\nbreach. Thus, we use differential privacy to preserve the privacy of learners,\nand study the influence of guaranteeing differential privacy on the utility of\nthe distributed online learning algorithm. Furthermore, by using the results\nfrom Kakade and Tewari (2009), we use the regret bounds of online learning to\nachieve fast convergence rates for offline learning algorithms in distributed\nscenarios, which provides tighter utility performance than the existing\nstate-of-the-art results. In simulation, we demonstrate that the differentially\nprivate offline learning algorithm has high variance, but we can use mini-batch\nto improve the performance. Finally, the simulations show that the analytical\nresults of our proposed theorems are right and our private distributed online\nlearning algorithm is a general framework.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 07:59:36 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 09:19:03 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Li", "Chencheng", ""], ["Zhou", "Pan", ""]]}, {"id": "1505.06614", "submitter": "Renato De Leone", "authors": "Renato De Leone, Valentina Minnetti", "title": "Electre Tri-Machine Learning Approach to the Record Linkage Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper, the Electre Tri-Machine Learning Method, generally used\nto solve ordinal classification problems, is proposed for solving the Record\nLinkage problem. Preliminary experimental results show that, using the Electre\nTri method, high accuracy can be achieved and more than 99% of the matches and\nnonmatches were correctly identified by the procedure.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 13:02:32 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["De Leone", "Renato", ""], ["Minnetti", "Valentina", ""]]}, {"id": "1505.06770", "submitter": "Yang Cao", "authors": "Yang Cao, Andrew Thompson, Meng Wang, Yao Xie", "title": "Sketching for Sequential Change-Point Detection", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sequential change-point detection procedures based on linear\nsketches of high-dimensional signal vectors using generalized likelihood ratio\n(GLR) statistics. The GLR statistics allow for an unknown post-change mean that\nrepresents an anomaly or novelty. We consider both fixed and time-varying\nprojections, derive theoretical approximations to two fundamental performance\nmetrics: the average run length (ARL) and the expected detection delay (EDD);\nthese approximations are shown to be highly accurate by numerical simulations.\nWe further characterize the relative performance measure of the sketching\nprocedure compared to that without sketching and show that there can be little\nperformance loss when the signal strength is sufficiently large, and enough\nnumber of sketches are used. Finally, we demonstrate the good performance of\nsketching procedures using simulation and real-data examples on solar flare\ndetection and failure detection in power networks.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 22:19:07 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 18:58:56 GMT"}, {"version": "v3", "created": "Tue, 26 Apr 2016 15:56:42 GMT"}, {"version": "v4", "created": "Wed, 7 Sep 2016 17:31:08 GMT"}, {"version": "v5", "created": "Thu, 20 Jul 2017 03:25:01 GMT"}, {"version": "v6", "created": "Mon, 30 Apr 2018 04:34:03 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Cao", "Yang", ""], ["Thompson", "Andrew", ""], ["Wang", "Meng", ""], ["Xie", "Yao", ""]]}, {"id": "1505.06795", "submitter": "Nikolaos Karianakis", "authors": "Nikolaos Karianakis, Jingming Dong and Stefano Soatto", "title": "An Empirical Evaluation of Current Convolutional Architectures' Ability\n  to Manage Nuisance Location and Scale Variability", "comments": "10 pages, 5 figures, 3 tables -- CVPR 2016, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an empirical study to test the ability of Convolutional Neural\nNetworks (CNNs) to reduce the effects of nuisance transformations of the input\ndata, such as location, scale and aspect ratio. We isolate factors by adopting\na common convolutional architecture either deployed globally on the image to\ncompute class posterior distributions, or restricted locally to compute class\nconditional distributions given location, scale and aspect ratios of bounding\nboxes determined by proposal heuristics. In theory, averaging the latter should\nyield inferior performance compared to proper marginalization. Yet empirical\nevidence suggests the converse, leading us to conclude that - at the current\nlevel of complexity of convolutional architectures and scale of the data sets\nused to train them - CNNs are not very effective at marginalizing nuisance\nvariability. We also quantify the effects of context on the overall\nclassification task and its impact on the performance of CNNs, and propose\nimproved sampling techniques for heuristic proposal schemes that improve\nend-to-end performance to state-of-the-art levels. We test our hypothesis on a\nclassification task using the ImageNet Challenge benchmark and on a\nwide-baseline matching task using the Oxford and Fischer's datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:11:11 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 05:20:40 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Karianakis", "Nikolaos", ""], ["Dong", "Jingming", ""], ["Soatto", "Stefano", ""]]}, {"id": "1505.06798", "submitter": "Kaiming He", "authors": "Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun", "title": "Accelerating Very Deep Convolutional Networks for Classification and\n  Detection", "comments": "TPAMI, accepted. arXiv admin note: substantial text overlap with\n  arXiv:1411.4229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to accelerate the test-time computation of convolutional\nneural networks (CNNs), especially very deep CNNs that have substantially\nimpacted the computer vision community. Unlike previous methods that are\ndesigned for approximating linear filters or linear responses, our method takes\nthe nonlinear units into account. We develop an effective solution to the\nresulting nonlinear optimization problem without the need of stochastic\ngradient descent (SGD). More importantly, while previous methods mainly focus\non optimizing one or two layers, our nonlinear method enables an asymmetric\nreconstruction that reduces the rapidly accumulated error when multiple (e.g.,\n>=10) layers are approximated. For the widely used very deep VGG-16 model, our\nmethod achieves a whole-model speedup of 4x with merely a 0.3% increase of\ntop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also\nshows a graceful accuracy degradation for object detection when plugged into\nthe Fast R-CNN detector.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:30:59 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 06:16:59 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Zhang", "Xiangyu", ""], ["Zou", "Jianhua", ""], ["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1505.06800", "submitter": "Baochang Zhang", "authors": "Lei Wang, Baochang Zhang", "title": "Boosting-like Deep Learning For Pedestrian Detection", "comments": "9 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes boosting-like deep learning (BDL) framework for\npedestrian detection. Due to overtraining on the limited training samples,\noverfitting is a major problem of deep learning. We incorporate a boosting-like\ntechnique into deep learning to weigh the training samples, and thus prevent\novertraining in the iterative process. We theoretically give the details of\nderivation of our algorithm, and report the experimental results on open data\nsets showing that BDL achieves a better stable performance than the\nstate-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the\naverage miss rate compared with ACF and JointDeep on the largest Caltech\nbenchmark dataset, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:52:52 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Wang", "Lei", ""], ["Zhang", "Baochang", ""]]}, {"id": "1505.06807", "submitter": "Ameet Talwalkar", "authors": "Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram\n  Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen,\n  Doris Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, Ameet\n  Talwalkar", "title": "MLlib: Machine Learning in Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Spark is a popular open-source platform for large-scale data\nprocessing that is well-suited for iterative machine learning tasks. In this\npaper we present MLlib, Spark's open-source distributed machine learning\nlibrary. MLlib provides efficient functionality for a wide range of learning\nsettings and includes several underlying statistical, optimization, and linear\nalgebra primitives. Shipped with Spark, MLlib supports several languages and\nprovides a high-level API that leverages Spark's rich ecosystem to simplify the\ndevelopment of end-to-end machine learning pipelines. MLlib has experienced a\nrapid growth due to its vibrant open-source community of over 140 contributors,\nand includes extensive documentation to support further growth and to let users\nquickly get up to speed.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 05:12:23 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Meng", "Xiangrui", ""], ["Bradley", "Joseph", ""], ["Yavuz", "Burak", ""], ["Sparks", "Evan", ""], ["Venkataraman", "Shivaram", ""], ["Liu", "Davies", ""], ["Freeman", "Jeremy", ""], ["Tsai", "DB", ""], ["Amde", "Manish", ""], ["Owen", "Sean", ""], ["Xin", "Doris", ""], ["Xin", "Reynold", ""], ["Franklin", "Michael J.", ""], ["Zadeh", "Reza", ""], ["Zaharia", "Matei", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1505.06812", "submitter": "Purushottam Kar", "authors": "Harikrishna Narasimhan and Purushottam Kar and Prateek Jain", "title": "Optimizing Non-decomposable Performance Measures: A Tale of Two Classes", "comments": "To appear in proceedings of the 32nd International Conference on\n  Machine Learning (ICML 2015)", "journal-ref": "Journal of Machine Learning Research, W&CP 37 (2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern classification problems frequently present mild to severe label\nimbalance as well as specific requirements on classification characteristics,\nand require optimizing performance measures that are non-decomposable over the\ndataset, such as F-measure. Such measures have spurred much interest and pose\nspecific challenges to learning algorithms since their non-additive nature\nprecludes a direct application of well-studied large scale optimization methods\nsuch as stochastic gradient descent.\n  In this paper we reveal that for two large families of performance measures\nthat can be expressed as functions of true positive/negative rates, it is\nindeed possible to implement point stochastic updates. The families we consider\nare concave and pseudo-linear functions of TPR, TNR which cover several\npopularly used performance measures such as F-measure, G-mean and H-mean.\n  Our core contribution is an adaptive linearization scheme for these families,\nusing which we develop optimization techniques that enable truly point-based\nstochastic updates. For concave performance measures we propose SPADE, a\nstochastic primal dual solver; for pseudo-linear measures we propose STAMP, a\nstochastic alternate maximization procedure. Both methods have crisp\nconvergence guarantees, demonstrate significant speedups over existing methods\n- often by an order of magnitude or more, and give similar or more accurate\npredictions on test data.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 05:59:33 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Narasimhan", "Harikrishna", ""], ["Kar", "Purushottam", ""], ["Jain", "Prateek", ""]]}, {"id": "1505.06813", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Harikrishna Narasimhan and Prateek Jain", "title": "Surrogate Functions for Maximizing Precision at the Top", "comments": "To appear in the the proceedings of the 32nd International Conference\n  on Machine Learning (ICML 2015)", "journal-ref": "Journal of Machine Learning Research, W&CP 37 (2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of maximizing precision at the top of a ranked list, often dubbed\nPrecision@k (prec@k), finds relevance in myriad learning applications such as\nranking, multi-label classification, and learning with severe label imbalance.\nHowever, despite its popularity, there exist significant gaps in our\nunderstanding of this problem and its associated performance measure.\n  The most notable of these is the lack of a convex upper bounding surrogate\nfor prec@k. We also lack scalable perceptron and stochastic gradient descent\nalgorithms for optimizing this performance measure. In this paper we make key\ncontributions in these directions. At the heart of our results is a family of\ntruly upper bounding surrogates for prec@k. These surrogates are motivated in a\nprincipled manner and enjoy attractive properties such as consistency to prec@k\nunder various natural margin/noise conditions.\n  These surrogates are then used to design a class of novel perceptron\nalgorithms for optimizing prec@k with provable mistake bounds. We also devise\nscalable stochastic gradient descent style methods for this problem with\nprovable convergence bounds. Our proofs rely on novel uniform convergence\nbounds which require an in-depth analysis of the structural properties of\nprec@k and its surrogates. We conclude with experimental results comparing our\nalgorithms with state-of-the-art cutting plane and stochastic gradient\nalgorithms for maximizing prec@k.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 06:01:24 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Kar", "Purushottam", ""], ["Narasimhan", "Harikrishna", ""], ["Jain", "Prateek", ""]]}, {"id": "1505.06814", "submitter": "Francesco  Palmieri A. N.", "authors": "Francesco A. N. Palmieri and Amedeo Buonanno", "title": "Discrete Independent Component Analysis (DICA) with Belief Propagation", "comments": "Sumbitted for publication (May 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply belief propagation to a Bayesian bipartite graph composed of\ndiscrete independent hidden variables and discrete visible variables. The\nnetwork is the Discrete counterpart of Independent Component Analysis (DICA)\nand it is manipulated in a factor graph form for inference and learning. A full\nset of simulations is reported for character images from the MNIST dataset. The\nresults show that the factorial code implemented by the sources contributes to\nbuild a good generative model for the data that can be used in various\ninference modes.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 06:02:05 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Palmieri", "Francesco A. N.", ""], ["Buonanno", "Amedeo", ""]]}, {"id": "1505.06897", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (IRISA)", "title": "Times series averaging from a probabilistic interpretation of\n  time-elastic kernel", "comments": null, "journal-ref": null, "doi": "10.2478/amcs-2019-0028", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the light of regularized dynamic time warping kernels, this paper\nreconsider the concept of time elastic centroid (TEC) for a set of time series.\nFrom this perspective, we show first how TEC can easily be addressed as a\npreimage problem. Unfortunately this preimage problem is ill-posed, may suffer\nfrom over-fitting especially for long time series and getting a sub-optimal\nsolution involves heavy computational costs. We then derive two new algorithms\nbased on a probabilistic interpretation of kernel alignment matrices that\nexpresses in terms of probabilistic distributions over sets of alignment paths.\nThe first algorithm is an iterative agglomerative heuristics inspired from the\nstate of the art DTW barycenter averaging (DBA) algorithm proposed specifically\nfor the Dynamic Time Warping measure. The second proposed algorithm achieves a\nclassical averaging of the aligned samples but also implements an averaging of\nthe time of occurrences of the aligned samples. It exploits a straightforward\nprogressive agglomerative heuristics. An experimentation that compares for 45\ntime series datasets classification error rates obtained by first near\nneighbors classifiers exploiting a single medoid or centroid estimate to\nrepresent each categories show that: i) centroids based approaches\nsignificantly outperform medoids based approaches, ii) on the considered\nexperience, the two proposed algorithms outperform the state of the art DBA\nalgorithm, and iii) the second proposed algorithm that implements an averaging\njointly in the sample space and along the time axes emerges as the most\nsignificantly robust time elastic averaging heuristic with an interesting noise\nreduction capability. Index Terms-Time series averaging Time elastic kernel\nDynamic Time Warping Time series clustering and classification.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 11:02:36 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 07:17:13 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2015 12:00:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "IRISA"]]}, {"id": "1505.06907", "submitter": "Andreas Gr\\\"unauer", "authors": "Andreas Gr\\\"unauer and Markus Vincze", "title": "Using Dimension Reduction to Improve the Classification of\n  High-dimensional Data", "comments": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "journal-ref": null, "doi": null, "report-no": "OAGM/2015/09", "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show that the classification performance of high-dimensional\nstructural MRI data with only a small set of training examples is improved by\nthe usage of dimension reduction methods. We assessed two different dimension\nreduction variants: feature selection by ANOVA F-test and feature\ntransformation by PCA. On the reduced datasets, we applied common learning\nalgorithms using 5-fold cross-validation. Training, tuning of the\nhyperparameters, as well as the performance evaluation of the classifiers was\nconducted using two different performance measures: Accuracy, and Receiver\nOperating Characteristic curve (AUC). Our hypothesis is supported by\nexperimental results.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 11:33:04 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Gr\u00fcnauer", "Andreas", ""], ["Vincze", "Markus", ""]]}, {"id": "1505.06915", "submitter": "Jean-Philippe Vert", "authors": "K\\'evin Vervier (CBIO), Pierre Mah\\'e, Maud Tournoud, Jean-Baptiste\n  Veyrieras, Jean-Philippe Vert (CBIO)", "title": "Large-scale Machine Learning for Metagenomics Sequence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metagenomics characterizes the taxonomic diversity of microbial communities\nby sequencing DNA directly from an environmental sample. One of the main\nchallenges in metagenomics data analysis is the binning step, where each\nsequenced read is assigned to a taxonomic clade. Due to the large volume of\nmetagenomics datasets, binning methods need fast and accurate algorithms that\ncan operate with reasonable computing requirements. While standard\nalignment-based methods provide state-of-the-art performance, compositional\napproaches that assign a taxonomic class to a DNA read based on the k-mers it\ncontains have the potential to provide faster solutions. In this work, we\ninvestigate the potential of modern, large-scale machine learning\nimplementations for taxonomic affectation of next-generation sequencing reads\nbased on their k-mers profile. We show that machine learning-based\ncompositional approaches benefit from increasing the number of fragments\nsampled from reference genome to tune their parameters, up to a coverage of\nabout 10, and from increasing the k-mer size to about 12. Tuning these models\ninvolves training a machine learning model on about 10 8 samples in 10 7\ndimensions, which is out of reach of standard soft-wares but can be done\nefficiently with modern implementations for large-scale machine learning. The\nresulting models are competitive in terms of accuracy with well-established\nalignment tools for problems involving a small to moderate number of candidate\nspecies, and for reasonable amounts of sequencing errors. We show, however,\nthat compositional approaches are still limited in their ability to deal with\nproblems involving a greater number of species, and more sensitive to\nsequencing errors. We finally confirm that compositional approach achieve\nfaster prediction times, with a gain of 3 to 15 times with respect to the\nBWA-MEM short read mapper, depending on the number of candidate species and the\nlevel of sequencing noise.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 12:02:04 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Vervier", "K\u00e9vin", "", "CBIO"], ["Mah\u00e9", "Pierre", "", "CBIO"], ["Tournoud", "Maud", "", "CBIO"], ["Veyrieras", "Jean-Baptiste", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1505.06918", "submitter": "Roman Lutz", "authors": "Roman Lutz", "title": "Fantasy Football Prediction", "comments": "class project, 7 pages (1 sources, 1 appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The ubiquity of professional sports and specifically the NFL have lead to an\nincrease in popularity for Fantasy Football. Users have many tools at their\ndisposal: statistics, predictions, rankings of experts and even recommendations\nof peers. There are issues with all of these, though. Especially since many\npeople pay money to play, the prediction tools should be enhanced as they\nprovide unbiased and easy-to-use assistance for users. This paper provides and\ndiscusses approaches to predict Fantasy Football scores of Quarterbacks with\nrelatively limited data. In addition to that, it includes several suggestions\non how the data could be enhanced to achieve better results. The dataset\nconsists only of game data from the last six NFL seasons. I used two different\nmethods to predict the Fantasy Football scores of NFL players: Support Vector\nRegression (SVR) and Neural Networks. The results of both are promising given\nthe limited data that was used.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 12:14:56 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Lutz", "Roman", ""]]}, {"id": "1505.06957", "submitter": "Nicolas Gillis", "authors": "Gabriella Casalino, Nicolas Gillis", "title": "Sequential Dimensionality Reduction for Extracting Localized Features", "comments": "24 pages, 12 figures. New numerical experiments on synthetic data\n  sets, discussion about the convergence", "journal-ref": "Pattern Recoginition 63, pp. 15-29, 2017", "doi": "10.1016/j.patcog.2016.09.006", "report-no": null, "categories": "cs.CV cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear dimensionality reduction techniques are powerful tools for image\nanalysis as they allow the identification of important features in a data set.\nIn particular, nonnegative matrix factorization (NMF) has become very popular\nas it is able to extract sparse, localized and easily interpretable features by\nimposing an additive combination of nonnegative basis elements. Nonnegative\nmatrix underapproximation (NMU) is a closely related technique that has the\nadvantage to identify features sequentially. In this paper, we propose a\nvariant of NMU that is particularly well suited for image analysis as it\nincorporates the spatial information, that is, it takes into account the fact\nthat neighboring pixels are more likely to be contained in the same features,\nand favors the extraction of localized features by looking for sparse basis\nelements. We show that our new approach competes favorably with comparable\nstate-of-the-art techniques on synthetic, facial and hyperspectral image data\nsets.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 14:06:16 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 06:44:58 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Casalino", "Gabriella", ""], ["Gillis", "Nicolas", ""]]}, {"id": "1505.06999", "submitter": "Luis Ortiz", "authors": "Joshua Belanich and Luis E. Ortiz", "title": "Some Open Problems in Optimal AdaBoost and Decision Stumps", "comments": "4 pages, rejected from COLT15 Open Problems May 19, 2015 (submitted\n  April 21, 2015; original 3 pages in COLT-conference format)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significance of the study of the theoretical and practical properties of\nAdaBoost is unquestionable, given its simplicity, wide practical use, and\neffectiveness on real-world datasets. Here we present a few open problems\nregarding the behavior of \"Optimal AdaBoost,\" a term coined by Rudin,\nDaubechies, and Schapire in 2004 to label the simple version of the standard\nAdaBoost algorithm in which the weak learner that AdaBoost uses always outputs\nthe weak classifier with lowest weighted error among the respective hypothesis\nclass of weak classifiers implicit in the weak learner. We concentrate on the\nstandard, \"vanilla\" version of Optimal AdaBoost for binary classification that\nresults from using an exponential-loss upper bound on the misclassification\ntraining error. We present two types of open problems. One deals with general\nweak hypotheses. The other deals with the particular case of decision stumps,\nas often and commonly used in practice. Answers to the open problems can have\nimmediate significant impact to (1) cementing previously established results on\nasymptotic convergence properties of Optimal AdaBoost, for finite datasets,\nwhich in turn can be the start to any convergence-rate analysis; (2)\nunderstanding the weak-hypotheses class of effective decision stumps generated\nfrom data, which we have empirically observed to be significantly smaller than\nthe typically obtained class, as well as the effect on the weak learner's\nrunning time and previously established improved bounds on the generalization\nperformance of Optimal AdaBoost classifiers; and (3) shedding some light on the\n\"self control\" that AdaBoost tends to exhibit in practice.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 15:18:33 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Belanich", "Joshua", ""], ["Ortiz", "Luis E.", ""]]}, {"id": "1505.07008", "submitter": "Tianwen Wei", "authors": "Tianwen Wei", "title": "An Overview of the Asymptotic Performance of the Family of the FastICA\n  Algorithms", "comments": "To appear in the 12th International Conference on Latent Variable\n  Analysis and Source Separation (LVA/ICA 2015), Liberec, Czech Republic", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution summarizes the results on the asymptotic performance of\nseveral variants of the FastICA algorithm. A number of new closed-form\nexpressions are presented.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 15:26:43 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Wei", "Tianwen", ""]]}, {"id": "1505.07067", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega and Koby Crammer and Daniel D. Lee", "title": "Belief Flows of Robust Online Learning", "comments": "Appears in Workshop on Information Theory and Applications (ITA),\n  February 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new probabilistic model for online learning which\ndynamically incorporates information from stochastic gradients of an arbitrary\nloss function. Similar to probabilistic filtering, the model maintains a\nGaussian belief over the optimal weight parameters. Unlike traditional Bayesian\nupdates, the model incorporates a small number of gradient evaluations at\nlocations chosen using Thompson sampling, making it computationally tractable.\nThe belief is then transformed via a linear flow field which optimally updates\nthe belief distribution using rules derived from information theoretic\nprinciples. Several versions of the algorithm are shown using different\nconstraints on the flow field and compared with conventional online learning\nalgorithms. Results are given for several classification tasks including\nlogistic regression and multilayer neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 17:57:32 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Crammer", "Koby", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1505.07428", "submitter": "Manuel L\\'opez-Antequera", "authors": "Ruben Gomez-Ojeda, Manuel Lopez-Antequera, Nicolai Petkov, Javier\n  Gonzalez-Jimenez", "title": "Training a Convolutional Neural Network for Appearance-Invariant Place\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition is one of the most challenging problems in computer vision,\nand has become a key part in mobile robotics and autonomous driving\napplications for performing loop closure in visual SLAM systems. Moreover, the\ndifficulty of recognizing a revisited location increases with appearance\nchanges caused, for instance, by weather or illumination variations, which\nhinders the long-term application of such algorithms in real environments. In\nthis paper we present a convolutional neural network (CNN), trained for the\nfirst time with the purpose of recognizing revisited locations under severe\nappearance changes, which maps images to a low dimensional space where\nEuclidean distances represent place dissimilarity. In order for the network to\nlearn the desired invariances, we train it with triplets of images selected\nfrom datasets which present a challenging variability in visual appearance. The\ntriplets are selected in such way that two samples are from the same location\nand the third one is taken from a different place. We validate our system\nthrough extensive experimentation, where we demonstrate better performance than\nstate-of-art algorithms in a number of popular datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 18:21:54 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Gomez-Ojeda", "Ruben", ""], ["Lopez-Antequera", "Manuel", ""], ["Petkov", "Nicolai", ""], ["Gonzalez-Jimenez", "Javier", ""]]}, {"id": "1505.07570", "submitter": "Shusen Wang", "authors": "Shusen Wang", "title": "A Practical Guide to Randomized Matrix Computations with MATLAB\n  Implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix operations such as matrix inversion, eigenvalue decomposition,\nsingular value decomposition are ubiquitous in real-world applications.\nUnfortunately, many of these matrix operations so time and memory expensive\nthat they are prohibitive when the scale of data is large. In real-world\napplications, since the data themselves are noisy, machine-precision matrix\noperations are not necessary at all, and one can sacrifice a reasonable amount\nof accuracy for computational efficiency.\n  In recent years, a bunch of randomized algorithms have been devised to make\nmatrix computations more scalable. Mahoney (2011) and Woodruff (2014) have\nwritten excellent but very technical reviews of the randomized algorithms.\nDifferently, the focus of this manuscript is on intuition, algorithm\nderivation, and implementation. This manuscript should be accessible to people\nwith knowledge in elementary matrix algebra but unfamiliar with randomized\nmatrix computations. The algorithms introduced in this manuscript are all\nsummarized in a user-friendly way, and they can be implemented in lines of\nMATLAB code. The readers can easily follow the implementations even if they do\nnot understand the maths and algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 07:33:21 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 14:33:48 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2015 07:52:43 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2015 09:26:48 GMT"}, {"version": "v5", "created": "Mon, 17 Aug 2015 06:39:40 GMT"}, {"version": "v6", "created": "Tue, 3 Nov 2015 03:26:49 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Wang", "Shusen", ""]]}, {"id": "1505.07634", "submitter": "Aditya Menon", "authors": "Brendan van Rooyen and Aditya Krishna Menon and Robert C. Williamson", "title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex potential minimisation is the de facto approach to binary\nclassification. However, Long and Servedio [2010] proved that under symmetric\nlabel noise (SLN), minimisation of any convex potential over a linear function\nclass can result in classification performance equivalent to random guessing.\nThis ostensibly shows that convex losses are not SLN-robust. In this paper, we\npropose a convex, classification-calibrated loss and prove that it is\nSLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of\nbeing negatively unbounded. The loss is a modification of the hinge loss, where\none does not clamp at zero; hence, we call it the unhinged loss. We show that\nthe optimal unhinged solution is equivalent to that of a strongly regularised\nSVM, and is the limiting solution for any convex potential; this implies that\nstrong l2 regularisation makes most standard learners SLN-robust. Experiments\nconfirm the SLN-robustness of the unhinged loss.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 10:38:56 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["van Rooyen", "Brendan", ""], ["Menon", "Aditya Krishna", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1505.07818", "submitter": "Pascal Germain", "authors": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\n  Larochelle, Fran\\c{c}ois Laviolette, Mario Marchand, Victor Lempitsky", "title": "Domain-Adversarial Training of Neural Networks", "comments": "Published in JMLR: http://jmlr.org/papers/v17/15-239.html", "journal-ref": "Journal of Machine Learning Research 2016, vol. 17, p. 1-35", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new representation learning approach for domain adaptation, in\nwhich data at training and test time come from similar but different\ndistributions. Our approach is directly inspired by the theory on domain\nadaptation suggesting that, for effective domain transfer to be achieved,\npredictions must be made based on features that cannot discriminate between the\ntraining (source) and test (target) domains. The approach implements this idea\nin the context of neural network architectures that are trained on labeled data\nfrom the source domain and unlabeled data from the target domain (no labeled\ntarget-domain data is necessary). As the training progresses, the approach\npromotes the emergence of features that are (i) discriminative for the main\nlearning task on the source domain and (ii) indiscriminate with respect to the\nshift between the domains. We show that this adaptation behaviour can be\nachieved in almost any feed-forward model by augmenting it with few standard\nlayers and a new gradient reversal layer. The resulting augmented architecture\ncan be trained using standard backpropagation and stochastic gradient descent,\nand can thus be implemented with little effort using any of the deep learning\npackages. We demonstrate the success of our approach for two distinct\nclassification problems (document sentiment analysis and image classification),\nwhere state-of-the-art domain adaptation performance on standard benchmarks is\nachieved. We also validate the approach for descriptor learning task in the\ncontext of person re-identification application.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 19:34:53 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 13:32:12 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2015 16:57:53 GMT"}, {"version": "v4", "created": "Thu, 26 May 2016 19:56:08 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Ustinova", "Evgeniya", ""], ["Ajakan", "Hana", ""], ["Germain", "Pascal", ""], ["Larochelle", "Hugo", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1505.07909", "submitter": "Bin Gao", "authors": "Huazheng Wang, Fei Tian, Bin Gao, Jiang Bian, Tie-Yan Liu", "title": "Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered\n  Word Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligence Quotient (IQ) Test is a set of standardized questions designed\nto evaluate human intelligence. Verbal comprehension questions appear very\nfrequently in IQ tests, which measure human's verbal ability including the\nunderstanding of the words with multiple senses, the synonyms and antonyms, and\nthe analogies among words. In this work, we explore whether such tests can be\nsolved automatically by artificial intelligence technologies, especially the\ndeep learning technologies that are recently developed and successfully applied\nin a number of fields. However, we found that the task was quite challenging,\nand simply applying existing technologies (e.g., word embedding) could not\nachieve a good performance, mainly due to the multiple senses of words and the\ncomplex relations among words. To tackle these challenges, we propose a novel\nframework consisting of three components. First, we build a classifier to\nrecognize the specific type of a verbal question (e.g., analogy,\nclassification, synonym, or antonym). Second, we obtain distributed\nrepresentations of words and relations by leveraging a novel word embedding\nmethod that considers the multi-sense nature of words and the relational\nknowledge among words (or their senses) contained in dictionaries. Third, for\neach type of questions, we propose a specific solver based on the obtained\ndistributed word representations and relation representations. Experimental\nresults have shown that the proposed framework can not only outperform existing\nmethods for solving verbal comprehension questions but also exceed the average\nperformance of the Amazon Mechanical Turk workers involved in the study. The\nresults indicate that with appropriate uses of the deep learning technologies\nwe might be a further step closer to the human intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 02:46:44 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 13:29:41 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2015 08:42:22 GMT"}, {"version": "v4", "created": "Tue, 26 Apr 2016 11:37:32 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Wang", "Huazheng", ""], ["Tian", "Fei", ""], ["Gao", "Bin", ""], ["Bian", "Jiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1505.07925", "submitter": "Yun Yang", "authors": "Yun Yang, Martin J. Wainwright, Michael I. Jordan", "title": "On the Computational Complexity of High-Dimensional Bayesian Variable\n  Selection", "comments": "42 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of Markov chain Monte Carlo (MCMC)\nmethods for high-dimensional Bayesian linear regression under sparsity\nconstraints. We first show that a Bayesian approach can achieve\nvariable-selection consistency under relatively mild conditions on the design\nmatrix. We then demonstrate that the statistical criterion of posterior\nconcentration need not imply the computational desideratum of rapid mixing of\nthe MCMC algorithm. By introducing a truncated sparsity prior for variable\nselection, we provide a set of conditions that guarantee both\nvariable-selection consistency and rapid mixing of a particular\nMetropolis-Hastings algorithm. The mixing time is linear in the number of\ncovariates up to a logarithmic factor. Our proof controls the spectral gap of\nthe Markov chain by constructing a canonical path ensemble that is inspired by\nthe steps taken by greedy algorithms for variable selection.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 05:33:22 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Yang", "Yun", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1505.08075", "submitter": "Chris Dyer", "authors": "Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, Noah A.\n  Smith", "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "comments": "Proceedings of ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for learning representations of parser states in\ntransition-based dependency parsers. Our primary innovation is a new control\nstructure for sequence-to-sequence neural networks---the stack LSTM. Like the\nconventional stack data structures used in transition-based parsing, elements\ncan be pushed to or popped from the top of the stack in constant time, but, in\naddition, an LSTM maintains a continuous space embedding of the stack contents.\nThis lets us formulate an efficient parsing model that captures three facets of\na parser's state: (i) unbounded look-ahead into the buffer of incoming words,\n(ii) the complete history of actions taken by the parser, and (iii) the\ncomplete contents of the stack of partially built tree fragments, including\ntheir internal structures. Standard backpropagation techniques are used for\ntraining and yield state-of-the-art parsing performance.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 14:58:12 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Dyer", "Chris", ""], ["Ballesteros", "Miguel", ""], ["Ling", "Wang", ""], ["Matthews", "Austin", ""], ["Smith", "Noah A.", ""]]}, {"id": "1505.08098", "submitter": "Gianluigi Ciocca", "authors": "Simone Bianco, Gianluigi Ciocca, Claudio Cusano", "title": "CURL: Co-trained Unsupervised Representation Learning for Image\n  Classification", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a strategy for semi-supervised image classification\nthat leverages unsupervised representation learning and co-training. The\nstrategy, that is called CURL from Co-trained Unsupervised Representation\nLearning, iteratively builds two classifiers on two different views of the\ndata. The two views correspond to different representations learned from both\nlabeled and unlabeled data and differ in the fusion scheme used to combine the\nimage features. To assess the performance of our proposal, we conducted several\nexperiments on widely used data sets for scene and object recognition. We\nconsidered three scenarios (inductive, transductive and self-taught learning)\nthat differ in the strategy followed to exploit the unlabeled data. As image\nfeatures we considered a combination of GIST, PHOG, and LBP as well as features\nextracted from a Convolutional Neural Network. Moreover, two embodiments of\nCURL are investigated: one using Ensemble Projection as unsupervised\nrepresentation learning coupled with Logistic Regression, and one based on\nLapSVM. The results show that CURL clearly outperforms other supervised and\nsemi-supervised learning methods in the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 15:57:40 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 12:21:20 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Bianco", "Simone", ""], ["Ciocca", "Gianluigi", ""], ["Cusano", "Claudio", ""]]}]