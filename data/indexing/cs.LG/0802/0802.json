[{"id": "0802.1002", "submitter": "Xavier Bry", "authors": "Xavier Bry (I3M)", "title": "New Estimation Procedures for PLS Path Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  Given R groups of numerical variables X1, ... XR, we assume that each group\nis the result of one underlying latent variable, and that all latent variables\nare bound together through a linear equation system. Moreover, we assume that\nsome explanatory latent variables may interact pairwise in one or more\nequations. We basically consider PLS Path Modelling's algorithm to estimate\nboth latent variables and the model's coefficients. New \"external\" estimation\nschemes are proposed that draw latent variables towards strong group structures\nin a more flexible way. New \"internal\" estimation schemes are proposed to\nenable PLSPM to make good use of variable group complementarity and to deal\nwith interactions. Application examples are given.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2008 15:18:27 GMT"}], "update_date": "2008-02-08", "authors_parsed": [["Bry", "Xavier", "", "I3M"]]}, {"id": "0802.1244", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou", "title": "Learning Balanced Mixtures of Discrete Distributions with Small Sample", "comments": "24 Pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": null, "abstract": "  We study the problem of partitioning a small sample of $n$ individuals from a\nmixture of $k$ product distributions over a Boolean cube $\\{0, 1\\}^K$ according\nto their distributions. Each distribution is described by a vector of allele\nfrequencies in $\\R^K$. Given two distributions, we use $\\gamma$ to denote the\naverage $\\ell_2^2$ distance in frequencies across $K$ dimensions, which\nmeasures the statistical divergence between them. We study the case assuming\nthat bits are independently distributed across $K$ dimensions. This work\ndemonstrates that, for a balanced input instance for $k = 2$, a certain\ngraph-based optimization function returns the correct partition with high\nprobability, where a weighted graph $G$ is formed over $n$ individuals, whose\npairwise hamming distances between their corresponding bit vectors define the\nedge weights, so long as $K = \\Omega(\\ln n/\\gamma)$ and $Kn = \\tilde\\Omega(\\ln\nn/\\gamma^2)$. The function computes a maximum-weight balanced cut of $G$, where\nthe weight of a cut is the sum of the weights across all edges in the cut. This\nresult demonstrates a nice property in the high-dimensional feature space: one\ncan trade off the number of features that are required with the size of the\nsample to accomplish certain tasks like clustering.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2008 07:38:49 GMT"}], "update_date": "2008-02-21", "authors_parsed": [["Zhou", "Shuheng", ""]]}, {"id": "0802.1258", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Bayesian Nonlinear Principal Component Analysis Using Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model for nonlinear dimension reduction motivated by the\nprobabilistic formulation of principal component analysis. Nonlinearity is\nachieved by specifying different transformation matrices at different locations\nof the latent space and smoothing the transformation using a Markov random\nfield type prior. The computation is made feasible by the recent advances in\nsampling from von Mises-Fisher distributions.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2008 12:22:47 GMT"}], "update_date": "2008-02-12", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "0802.1430", "submitter": "Francis Bach", "authors": "Jacob Abernethy, Francis Bach (INRIA Rocquencourt), Theodoros\n  Evgeniou, Jean-Philippe Vert (CB)", "title": "A New Approach to Collaborative Filtering: Operator Estimation with\n  Spectral Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general approach for collaborative filtering (CF) using spectral\nregularization to learn linear operators from \"users\" to the \"objects\" they\nrate. Recent low-rank type matrix completion approaches to CF are shown to be\nspecial cases. However, unlike existing regularization based CF methods, our\napproach can be used to also incorporate information such as attributes of the\nusers or the objects -- a limitation of existing regularization based CF\nmethods. We then provide novel representer theorems that we use to develop new\nestimation methods. We provide learning algorithms based on low-rank\ndecompositions, and test them on a standard CF dataset. The experiments\nindicate the advantages of generalizing the existing regularization based CF\nmethods to incorporate related information about users and objects. Finally, we\nshow that certain multi-task learning methods can be also seen as special cases\nof our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2008 12:55:34 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2008 14:05:14 GMT"}], "update_date": "2008-12-19", "authors_parsed": [["Abernethy", "Jacob", "", "INRIA Rocquencourt"], ["Bach", "Francis", "", "INRIA Rocquencourt"], ["Evgeniou", "Theodoros", "", "CB"], ["Vert", "Jean-Philippe", "", "CB"]]}, {"id": "0802.2015", "submitter": "Steven de Rooij", "authors": "Wouter Koolen and Steven de Rooij", "title": "Combining Expert Advice Efficiently", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT", "license": null, "abstract": "  We show how models for prediction with expert advice can be defined concisely\nand clearly using hidden Markov models (HMMs); standard HMM algorithms can then\nbe used to efficiently calculate, among other things, how the expert\npredictions should be weighted according to the model. We cast many existing\nmodels as HMMs and recover the best known running times in each case. We also\ndescribe two new models: the switch distribution, which was recently developed\nto improve Bayesian/Minimum Description Length model selection, and a new\ngeneralisation of the fixed share algorithm based on run-length coding. We give\nloss bounds for all models and shed new light on their relationships.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2008 14:54:57 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2008 10:59:15 GMT"}], "update_date": "2008-02-15", "authors_parsed": [["Koolen", "Wouter", ""], ["de Rooij", "Steven", ""]]}, {"id": "0802.2158", "submitter": "Olivier Roustant", "authors": "Jessica Franco, Laurent Carraro, Olivier Roustant, Astrid Jourdan\n  (LMA-PAU)", "title": "A Radar-Shaped Statistic for Testing and Visualizing Uniformity\n  Properties in Computer Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": null, "abstract": "  In the study of computer codes, filling space as uniformly as possible is\nimportant to describe the complexity of the investigated phenomenon. However,\nthis property is not conserved by reducing the dimension. Some numeric\nexperiment designs are conceived in this sense as Latin hypercubes or\northogonal arrays, but they consider only the projections onto the axes or the\ncoordinate planes. In this article we introduce a statistic which allows\nstudying the good distribution of points according to all 1-dimensional\nprojections. By angularly scanning the domain, we obtain a radar type\nrepresentation, allowing the uniformity defects of a design to be identified\nwith respect to its projections onto straight lines. The advantages of this new\ntool are demonstrated on usual examples of space-filling designs (SFD) and a\nglobal statistic independent of the angle of rotation is studied.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2008 09:06:25 GMT"}], "update_date": "2008-02-19", "authors_parsed": [["Franco", "Jessica", "", "LMA-PAU"], ["Carraro", "Laurent", "", "LMA-PAU"], ["Roustant", "Olivier", "", "LMA-PAU"], ["Jourdan", "Astrid", "", "LMA-PAU"]]}, {"id": "0802.2305", "submitter": "Ping Li", "authors": "Ping Li", "title": "Compressed Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DM cs.DS cs.LG math.IT", "license": null, "abstract": "  Counting is among the most fundamental operations in computing. For example,\ncounting the pth frequency moment has been a very active area of research, in\ntheoretical computer science, databases, and data mining. When p=1, the task\n(i.e., counting the sum) can be accomplished using a simple counter.\n  Compressed Counting (CC) is proposed for efficiently computing the pth\nfrequency moment of a data stream signal A_t, where 0<p<=2. CC is applicable if\nthe streaming data follow the Turnstile model, with the restriction that at the\ntime t for the evaluation, A_t[i]>= 0, which includes the strict Turnstile\nmodel as a special case. For natural data streams encountered in practice, this\nrestriction is minor.\n  The underly technique for CC is what we call skewed stable random\nprojections, which captures the intuition that, when p=1 a simple counter\nsuffices, and when p = 1+/\\Delta with small \\Delta, the sample complexity of a\ncounter system should be low (continuously as a function of \\Delta). We show at\nsmall \\Delta the sample complexity (number of projections) k = O(1/\\epsilon)\ninstead of O(1/\\epsilon^2).\n  Compressed Counting can serve a basic building block for other tasks in\nstatistics and computing, for example, estimation entropies of data streams,\nparameter estimations using the method of moments and maximum likelihood.\n  Finally, another contribution is an algorithm for approximating the\nlogarithmic norm, \\sum_{i=1}^D\\log A_t[i], and logarithmic distance. The\nlogarithmic distance is useful in machine learning practice with heavy-tailed\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2008 16:42:52 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2008 09:51:09 GMT"}], "update_date": "2008-02-24", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "0802.2428", "submitter": "Alexandre Benoit", "authors": "Oya Aran, Ismail Ari, Alexandre Benoit (GIPSA-lab), Ana Huerta\n  Carrillo, Fran\\c{c}ois-Xavier Fanard (TELE), Pavel Campr, Lale Akarun, Alice\n  Caplier (GIPSA-lab), Michele Rombaut (GIPSA-lab), Bulent Sankur", "title": "Sign Language Tutoring Tool", "comments": "eNTERFACE'06. Summer Workshop. on Multimodal Interfaces, Dubrovnik :\n  Croatie (2007)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": null, "abstract": "  In this project, we have developed a sign language tutor that lets users\nlearn isolated signs by watching recorded videos and by trying the same signs.\nThe system records the user's video and analyses it. If the sign is recognized,\nboth verbal and animated feedback is given to the user. The system is able to\nrecognize complex signs that involve both hand gestures and head movements and\nexpressions. Our performance tests yield a 99% recognition rate on signs\ninvolving only manual gestures and 85% recognition rate on signs that involve\nboth manual and non manual components, such as head movement and facial\nexpressions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2008 07:28:44 GMT"}], "update_date": "2008-09-11", "authors_parsed": [["Aran", "Oya", "", "GIPSA-lab"], ["Ari", "Ismail", "", "GIPSA-lab"], ["Benoit", "Alexandre", "", "GIPSA-lab"], ["Carrillo", "Ana Huerta", "", "TELE"], ["Fanard", "Fran\u00e7ois-Xavier", "", "TELE"], ["Campr", "Pavel", "", "GIPSA-lab"], ["Akarun", "Lale", "", "GIPSA-lab"], ["Caplier", "Alice", "", "GIPSA-lab"], ["Rombaut", "Michele", "", "GIPSA-lab"], ["Sankur", "Bulent", ""]]}, {"id": "0802.2655", "submitter": "Gilles Stoltz", "authors": "S\\'ebastien Bubeck (INRIA Futurs), R\\'emi Munos (INRIA Futurs), Gilles\n  Stoltz (DMA, GREGH)", "title": "Pure Exploration for Multi-Armed Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the framework of stochastic multi-armed bandit problems and study\nthe possibilities and limitations of forecasters that perform an on-line\nexploration of the arms. These forecasters are assessed in terms of their\nsimple regret, a regret notion that captures the fact that exploration is only\nconstrained by the number of available rounds (not necessarily known in\nadvance), in contrast to the case when the cumulative regret is considered and\nwhen exploitation needs to be performed at the same time. We believe that this\nperformance criterion is suited to situations when the cost of pulling an arm\nis expressed in terms of resources rather than rewards. We discuss the links\nbetween the simple and the cumulative regret. One of the main results in the\ncase of a finite number of arms is a general lower bound on the simple regret\nof a forecaster in terms of its cumulative regret: the smaller the latter, the\nlarger the former. Keeping this result in mind, we then exhibit upper bounds on\nthe simple regret of some forecasters. The paper ends with a study devoted to\ncontinuous-armed bandit problems; we show that the simple regret can be\nminimized with respect to a family of probability distributions if and only if\nthe cumulative regret can be minimized for it. Based on this equivalence, we\nare able to prove that the separable metric spaces are exactly the metric\nspaces on which these regrets can be minimized with respect to the family of\nall probability distributions with continuous mean-payoff functions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2008 14:05:22 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2008 07:03:22 GMT"}, {"version": "v3", "created": "Tue, 17 Jun 2008 07:07:03 GMT"}, {"version": "v4", "created": "Thu, 19 Feb 2009 10:33:29 GMT"}, {"version": "v5", "created": "Tue, 26 Jan 2010 10:10:42 GMT"}, {"version": "v6", "created": "Wed, 9 Jun 2010 09:08:50 GMT"}], "update_date": "2010-07-26", "authors_parsed": [["Bubeck", "S\u00e9bastien", "", "INRIA Futurs"], ["Munos", "R\u00e9mi", "", "INRIA Futurs"], ["Stoltz", "Gilles", "", "DMA, GREGH"]]}, {"id": "0802.3789", "submitter": "Viviana Sica", "authors": "Nick Milton", "title": "Knowledge Technologies", "comments": "130 pages, ISBN 978-88-7699-099-1 (Printed edition), ISBN\n  978-88-7699-100-4 (Electronic edition), printed edition available at\n  http://stores.lulu.com/polimetrica and on http://www.amazon.com/", "journal-ref": "\"Publishing studies\" book series, edited by Giandomenico Sica,\n  ISSN 1973-6061 (Printed edition), ISSN 1973-6053 (Electronic edition)", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG cs.SE", "license": null, "abstract": "  Several technologies are emerging that provide new ways to capture, store,\npresent and use knowledge. This book is the first to provide a comprehensive\nintroduction to five of the most important of these technologies: Knowledge\nEngineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and\nSemantic Webs. For each of these, answers are given to a number of key\nquestions (What is it? How does it operate? How is a system developed? What can\nit be used for? What tools are available? What are the main issues?). The book\nis aimed at students, researchers and practitioners interested in Knowledge\nManagement, Artificial Intelligence, Design Engineering and Web Technologies.\n  During the 1990s, Nick worked at the University of Nottingham on the\napplication of AI techniques to knowledge management and on various knowledge\nacquisition projects to develop expert systems for military applications. In\n1999, he joined Epistemics where he worked on numerous knowledge projects and\nhelped establish knowledge management programmes at large organisations in the\nengineering, technology and legal sectors. He is author of the book \"Knowledge\nAcquisition in Practice\", which describes a step-by-step procedure for\nacquiring and implementing expertise. He maintains strong links with leading\nresearch organisations working on knowledge technologies, such as\nknowledge-based engineering, ontologies and semantic technologies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2008 11:26:09 GMT"}], "update_date": "2008-02-27", "authors_parsed": [["Milton", "Nick", ""]]}]