[{"id": "1410.0095", "submitter": "Xu Wang", "authors": "Xu Wang, Konstantinos Slavakis, Gilad Lerman", "title": "Riemannian Multi-Manifold Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advocates a novel framework for segmenting a dataset in a\nRiemannian manifold $M$ into clusters lying around low-dimensional submanifolds\nof $M$. Important examples of $M$, for which the proposed clustering algorithm\nis computationally efficient, are the sphere, the set of positive definite\nmatrices, and the Grassmannian. The clustering problem with these examples of\n$M$ is already useful for numerous application domains such as action\nidentification in video sequences, dynamic texture clustering, brain fiber\nsegmentation in medical imaging, and clustering of deformed images. The\nproposed clustering algorithm constructs a data-affinity matrix by thoroughly\nexploiting the intrinsic geometry and then applies spectral clustering. The\nintrinsic local geometry is encoded by local sparse coding and more importantly\nby directional information of local tangent spaces and geodesics. Theoretical\nguarantees are established for a simplified variant of the algorithm even when\nthe clusters intersect. To avoid complication, these guarantees assume that the\nunderlying submanifolds are geodesic. Extensive validation on synthetic and\nreal data demonstrates the resiliency of the proposed method against deviations\nfrom the theoretical model as well as its superior performance over\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 02:37:12 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Wang", "Xu", ""], ["Slavakis", "Konstantinos", ""], ["Lerman", "Gilad", ""]]}, {"id": "1410.0123", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins, Heng Luo, Aaron Courville and Yoshua Bengio", "title": "Deep Tempering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBMs) are one of the fundamental building\nblocks of deep learning. Approximate maximum likelihood training of RBMs\ntypically necessitates sampling from these models. In many training scenarios,\ncomputationally efficient Gibbs sampling procedures are crippled by poor\nmixing. In this work we propose a novel method of sampling from Boltzmann\nmachines that demonstrates a computationally efficient way to promote mixing.\nOur approach leverages an under-appreciated property of deep generative models\nsuch as the Deep Belief Network (DBN), where Gibbs sampling from deeper levels\nof the latent variable hierarchy results in dramatically increased ergodicity.\nOur approach is thus to train an auxiliary latent hierarchical model, based on\nthe DBN. When used in conjunction with parallel-tempering, the method is\nasymptotically guaranteed to simulate samples from the target RBM. Experimental\nresults confirm the effectiveness of this sampling strategy in the context of\nRBM training.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 06:55:11 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Luo", "Heng", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1410.0210", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "A Multi-World Approach to Question Answering about Real-World Scenes\n  based on Uncertain Input", "comments": "Published in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for automatically answering questions about images by\nbringing together recent advances from natural language processing and computer\nvision. We combine discrete reasoning with uncertain predictions by a\nmulti-world approach that represents uncertainty about the perceived world in a\nbayesian framework. Our approach can handle human questions of high complexity\nabout realistic scenes and replies with range of answer like counts, object\nclasses, instances and lists of them. The system is directly trained from\nquestion-answer pairs. We establish a first benchmark for this task that can be\nseen as a modern attempt at a visual turing test.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 12:59:16 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 16:29:44 GMT"}, {"version": "v3", "created": "Tue, 11 Nov 2014 12:13:18 GMT"}, {"version": "v4", "created": "Tue, 5 May 2015 17:39:10 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1410.0260", "submitter": "William March", "authors": "William B. March, Bo Xiao, George Biros", "title": "ASKIT: Approximate Skeletonization Kernel-Independent Treecode in High\n  Dimensions", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast algorithm for kernel summation problems in high-dimensions.\nThese problems appear in computational physics, numerical approximation,\nnon-parametric statistics, and machine learning. In our context, the sums\ndepend on a kernel function that is a pair potential defined on a dataset of\npoints in a high-dimensional Euclidean space. A direct evaluation of the sum\nscales quadratically with the number of points. Fast kernel summation methods\ncan reduce this cost to linear complexity, but the constants involved do not\nscale well with the dimensionality of the dataset.\n  The main algorithmic components of fast kernel summation algorithms are the\nseparation of the kernel sum between near and far field (which is the basis for\npruning) and the efficient and accurate approximation of the far field.\n  We introduce novel methods for pruning and approximating the far field. Our\nfar field approximation requires only kernel evaluations and does not use\nanalytic expansions. Pruning is not done using bounding boxes but rather\ncombinatorially using a sparsified nearest-neighbor graph of the input. The\ntime complexity of our algorithm depends linearly on the ambient dimension. The\nerror in the algorithm depends on the low-rank approximability of the far\nfield, which in turn depends on the kernel function and on the intrinsic\ndimensionality of the distribution of the points. The error of the far field\napproximation does not depend on the ambient dimension.\n  We present the new algorithm along with experimental results that demonstrate\nits performance. We report results for Gaussian kernel sums for 100 million\npoints in 64 dimensions, for one million points in 1000 dimensions, and for\nproblems in which the Gaussian kernel has a variable bandwidth. To the best of\nour knowledge, all of these experiments are impossible or prohibitively\nexpensive with existing fast kernel summation methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 15:41:11 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 22:38:05 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2015 17:31:21 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["March", "William B.", ""], ["Xiao", "Bo", ""], ["Biros", "George", ""]]}, {"id": "1410.0311", "submitter": "Subhadip Mukherjee", "authors": "Subhadip Mukherjee, Rupam Basu, and Chandra Sekhar Seelamantula", "title": "$\\ell_1$-K-SVD: A Robust Dictionary Learning Algorithm With Simultaneous\n  Update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a dictionary learning algorithm by minimizing the $\\ell_1$\ndistortion metric on the data term, which is known to be robust for\nnon-Gaussian noise contamination. The proposed algorithm exploits the idea of\niterative minimization of weighted $\\ell_2$ error. We refer to this algorithm\nas $\\ell_1$-K-SVD, where the dictionary atoms and the corresponding sparse\ncoefficients are simultaneously updated to minimize the $\\ell_1$ objective,\nresulting in noise-robustness. We demonstrate through experiments that the\n$\\ell_1$-K-SVD algorithm results in higher atom recovery rate compared with the\nK-SVD and the robust dictionary learning (RDL) algorithm proposed by Lu et al.,\nboth in Gaussian and non-Gaussian noise conditions. We also show that, for\nfixed values of sparsity, number of dictionary atoms, and data-dimension, the\n$\\ell_1$-K-SVD algorithm outperforms the K-SVD and RDL algorithms when the\ntraining set available is small. We apply the proposed algorithm for denoising\nnatural images corrupted by additive Gaussian and Laplacian noise. The images\ndenoised using $\\ell_1$-K-SVD are observed to have slightly higher peak\nsignal-to-noise ratio (PSNR) over K-SVD for Laplacian noise, but the\nimprovement in structural similarity index (SSIM) is significant (approximately\n$0.1$) for lower values of input PSNR, indicating the efficacy of the $\\ell_1$\nmetric.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 07:23:04 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 04:38:37 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Mukherjee", "Subhadip", ""], ["Basu", "Rupam", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1410.0334", "submitter": "Emilie Morvant", "authors": "Emilie Morvant (LHC)", "title": "Domain adaptation of weighted majority votes via perturbed\n  variation-based self-labeling", "comments": null, "journal-ref": "Pattern Recognition Letters (2014) To be published", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the domain adaptation problem arrives when the test\n(target) and the train (source) data are generated from different\ndistributions. A key applied issue is thus the design of algorithms able to\ngeneralize on a new distribution, for which we have no label information. We\nfocus on learning classification models defined as a weighted majority vote\nover a set of real-val ued functions. In this context, Germain et al. (2013)\nhave shown that a measure of disagreement between these functions is crucial to\ncontrol. The core of this measure is a theoretical bound--the C-bound (Lacasse\net al., 2007)--which involves the disagreement and leads to a well performing\nmajority vote learning algorithm in usual non-adaptative supervised setting:\nMinCq. In this work, we propose a framework to extend MinCq to a domain\nadaptation scenario. This procedure takes advantage of the recent perturbed\nvariation divergence between distributions proposed by Harel and Mannor (2012).\nJustified by a theoretical bound on the target risk of the vote, we provide to\nMinCq a target sample labeled thanks to a perturbed variation-based\nself-labeling focused on the regions where the source and target marginals\nappear similar. We also study the influence of our self-labeling, from which we\ndeduce an original process for tuning the hyperparameters. Finally, our\nframework called PV-MinCq shows very promising results on a rotation and\ntranslation synthetic problem.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 19:09:02 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Morvant", "Emilie", "", "LHC"]]}, {"id": "1410.0342", "submitter": "Madeleine Udell", "authors": "Madeleine Udell, Corinne Horn, Reza Zadeh and Stephen Boyd", "title": "Generalized Low Rank Models", "comments": "84 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal components analysis (PCA) is a well-known technique for\napproximating a tabular data set by a low rank matrix. Here, we extend the idea\nof PCA to handle arbitrary data sets consisting of numerical, Boolean,\ncategorical, ordinal, and other data types. This framework encompasses many\nwell known techniques in data analysis, such as nonnegative matrix\nfactorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD,\nand maximum margin matrix factorization. The method handles heterogeneous data\nsets, and leads to coherent schemes for compressing, denoising, and imputing\nmissing entries across all data types simultaneously. It also admits a number\nof interesting interpretations of the low rank factors, which allow clustering\nof examples or of features. We propose several parallel algorithms for fitting\ngeneralized low rank models, and describe implementations and numerical\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 19:31:40 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 01:48:22 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 06:27:48 GMT"}, {"version": "v4", "created": "Tue, 5 May 2015 18:53:24 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Udell", "Madeleine", ""], ["Horn", "Corinne", ""], ["Zadeh", "Reza", ""], ["Boyd", "Stephen", ""]]}, {"id": "1410.0440", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Alina Beygelzimer, Daniel Hsu, John Langford, Matus\n  Telgarsky", "title": "Scalable Nonlinear Learning with Adaptive Polynomial Expansions", "comments": "To appear in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we effectively learn a nonlinear representation in time comparable to\nlinear learning? We describe a new algorithm that explicitly and adaptively\nexpands higher-order interaction features over base linear representations. The\nalgorithm is designed for extreme computational efficiency, and an extensive\nexperimental study shows that its computation/prediction tradeoff ability\ncompares very favorably against strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 02:28:04 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Agarwal", "Alekh", ""], ["Beygelzimer", "Alina", ""], ["Hsu", "Daniel", ""], ["Langford", "John", ""], ["Telgarsky", "Matus", ""]]}, {"id": "1410.0446", "submitter": "Arash Mahyari", "authors": "Arash Golibagh Mahyari, Selin Aviyente", "title": "Identification of Dynamic functional brain network states Through Tensor\n  Decomposition", "comments": "2014 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP)", "journal-ref": null, "doi": "10.1109/ICASSP.2014.6853969", "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances in high resolution neuroimaging, there has been a growing\ninterest in the detection of functional brain connectivity. Complex network\ntheory has been proposed as an attractive mathematical representation of\nfunctional brain networks. However, most of the current studies of functional\nbrain networks have focused on the computation of graph theoretic indices for\nstatic networks, i.e. long-time averages of connectivity networks. It is\nwell-known that functional connectivity is a dynamic process and the\nconstruction and reorganization of the networks is key to understanding human\ncognition. Therefore, there is a growing need to track dynamic functional brain\nnetworks and identify time intervals over which the network is\nquasi-stationary. In this paper, we present a tensor decomposition based method\nto identify temporally invariant 'network states' and find a common topographic\nrepresentation for each state. The proposed methods are applied to\nelectroencephalogram (EEG) data during the study of error-related negativity\n(ERN).\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 03:41:53 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Mahyari", "Arash Golibagh", ""], ["Aviyente", "Selin", ""]]}, {"id": "1410.0510", "submitter": "Ludovic Denoyer", "authors": "Ludovic Denoyer and Patrick Gallinari", "title": "Deep Sequential Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks sequentially build high-level features through their\nsuccessive layers. We propose here a new neural network model where each layer\nis associated with a set of candidate mappings. When an input is processed, at\neach layer, one mapping among these candidates is selected according to a\nsequential decision process. The resulting model is structured according to a\nDAG like architecture, so that a path from the root to a leaf node defines a\nsequence of transformations. Instead of considering global transformations,\nlike in classical multilayer networks, this model allows us for learning a set\nof local transformations. It is thus able to process data with different\ncharacteristics through specific sequences of such local transformations,\nincreasing the expression power of this model w.r.t a classical multilayered\nnetwork. The learning algorithm is inspired from policy gradient techniques\ncoming from the reinforcement learning domain and is used here instead of the\nclassical back-propagation based gradient descent techniques. Experiments on\ndifferent datasets show the relevance of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 10:58:17 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Denoyer", "Ludovic", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1410.0576", "submitter": "Kewei Tu", "authors": "Maria Pavlovskaia, Kewei Tu and Song-Chun Zhu", "title": "Mapping Energy Landscapes of Non-Convex Learning Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical learning problems, the target functions to be optimized\nare highly non-convex in various model spaces and thus are difficult to\nanalyze. In this paper, we compute \\emph{Energy Landscape Maps} (ELMs) which\ncharacterize and visualize an energy function with a tree structure, in which\neach leaf node represents a local minimum and each non-leaf node represents the\nbarrier between adjacent energy basins. The ELM also associates each node with\nthe estimated probability mass and volume for the corresponding energy basin.\nWe construct ELMs by adopting the generalized Wang-Landau algorithm and\nmulti-domain sampler that simulates a Markov chain traversing the model space\nby dynamically reweighting the energy function. We construct ELMs in the model\nspace for two classic statistical learning problems: i) clustering with\nGaussian mixture models or Bernoulli templates; and ii) bi-clustering. We\npropose a way to measure the difficulties (or complexity) of these learning\nproblems and study how various conditions affect the landscape complexity, such\nas separability of the clusters, the number of examples, and the level of\nsupervision; and we also visualize the behaviors of different algorithms, such\nas K-mean, EM, two-step EM and Swendsen-Wang cuts, in the energy landscapes.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 14:49:59 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Pavlovskaia", "Maria", ""], ["Tu", "Kewei", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1410.0630", "submitter": "Sherjil Ozair", "authors": "Sherjil Ozair and Yoshua Bengio", "title": "Deep Directed Generative Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For discrete data, the likelihood $P(x)$ can be rewritten exactly and\nparametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$\nhas enough capacity to put no probability mass on any $x'$ for which $f(x')\\neq\nf(x)$, where $f(\\cdot)$ is a deterministic discrete function. The log of the\nfirst factor gives rise to the log-likelihood reconstruction error of an\nautoencoder with $f(\\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic)\ndecoder. The log of the second term can be seen as a regularizer on the encoded\nactivations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder\ncan be represented by a deep neural network and trained to maximize the average\nof the optimal log-likelihood $\\log p(x)$. The objective is to learn an encoder\n$f(\\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than\n$X$ itself, estimated by $P(H)$. This \"flattens the manifold\" or concentrates\nprobability mass in a smaller number of (relevant) dimensions over which the\ndistribution factorizes. Generating samples from the model is straightforward\nusing ancestral sampling. One challenge is that regular back-propagation cannot\nbe used to obtain the gradient on the parameters of the encoder, but we find\nthat using the straight-through estimator works well here. We also find that\nalthough optimizing a single level of such architecture may be difficult, much\nbetter results can be obtained by pre-training and stacking them, gradually\ntransforming the data distribution into one that is more easily captured by a\nsimple parametric model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 18:09:42 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Ozair", "Sherjil", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1410.0633", "submitter": "Daniel L. Pimentel-Alarc\\'on", "authors": "Daniel L. Pimentel-Alarc\\'on, Robert D. Nowak, Nigel Boston", "title": "Deterministic Conditions for Subspace Identifiability from Incomplete\n  Sampling", "comments": "To appear in Proc. of IEEE ISIT, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a generic $r$-dimensional subspace of $\\mathbb{R}^d$, $r<d$, and\nsuppose that we are only given projections of this subspace onto small subsets\nof the canonical coordinates. The paper establishes necessary and sufficient\ndeterministic conditions on the subsets for subspace identifiability.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 18:20:04 GMT"}, {"version": "v2", "created": "Sun, 5 Oct 2014 15:43:50 GMT"}, {"version": "v3", "created": "Sun, 24 May 2015 17:31:55 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Pimentel-Alarc\u00f3n", "Daniel L.", ""], ["Nowak", "Robert D.", ""], ["Boston", "Nigel", ""]]}, {"id": "1410.0640", "submitter": "Hugo Jair  Escalante", "authors": "Hugo Jair Escalante, Mauricio A. Garc\\'ia-Lim\\'on, Alicia\n  Morales-Reyes, Mario Graff, Manuel Montes-y-G\\'omez, Eduardo F. Morales", "title": "Term-Weighting Learning via Genetic Programming for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach to learning term-weighting schemes\n(TWSs) in the context of text classification. In text mining a TWS determines\nthe way in which documents will be represented in a vector space model, before\napplying a classifier. Whereas acceptable performance has been obtained with\nstandard TWSs (e.g., Boolean and term-frequency schemes), the definition of\nTWSs has been traditionally an art. Further, it is still a difficult task to\ndetermine what is the best TWS for a particular problem and it is not clear\nyet, whether better schemes, than those currently available, can be generated\nby combining known TWS. We propose in this article a genetic program that aims\nat learning effective TWSs that can improve the performance of current schemes\nin text classification. The genetic program learns how to combine a set of\nbasic units to give rise to discriminative TWSs. We report an extensive\nexperimental study comprising data sets from thematic and non-thematic text\nclassification as well as from image classification. Our study shows the\nvalidity of the proposed method; in fact, we show that TWSs learned with the\ngenetic program outperform traditional schemes and other TWSs proposed in\nrecent works. Further, we show that TWSs learned from a specific domain can be\neffectively used for other tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 18:38:11 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 19:47:03 GMT"}, {"version": "v3", "created": "Mon, 6 Oct 2014 20:48:29 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Escalante", "Hugo Jair", ""], ["Garc\u00eda-Lim\u00f3n", "Mauricio A.", ""], ["Morales-Reyes", "Alicia", ""], ["Graff", "Mario", ""], ["Montes-y-G\u00f3mez", "Manuel", ""], ["Morales", "Eduardo F.", ""]]}, {"id": "1410.0719", "submitter": "Laurent Jacques", "authors": "L. Jacques, C. De Vleeschouwer, Y. Boursier, P. Sudhakar, C. De Mol,\n  A. Pizurica, S. Anthoine, P. Vandergheynst, P. Frossard, C. Bilen, S. Kitic,\n  N. Bertin, R. Gribonval, N. Boumal, B. Mishra, P.-A. Absil, R. Sepulchre, S.\n  Bundervoet, C. Schretter, A. Dooms, P. Schelkens, O. Chabiron, F. Malgouyres,\n  J.-Y. Tourneret, N. Dobigeon, P. Chainais, C. Richard, B. Cornelis, I.\n  Daubechies, D. Dunson, M. Dankova, P. Rajmic, K. Degraux, V. Cambareri, B.\n  Geelen, G. Lafruit, G. Setti, J.-F. Determe, J. Louveaux, F. Horlin, A.\n  Dr\\'emeau, P. Heas, C. Herzet, V. Duval, G. Peyr\\'e, A. Fawzi, M. Davies, N.\n  Gillis, S. A. Vavasis, C. Soussen, L. Le Magoarou, J. Liang, J. Fadili, A.\n  Liutkus, D. Martina, S. Gigan, L. Daudet, M. Maggioni, S. Minsker, N. Strawn,\n  C. Mory, F. Ngole, J.-L. Starck, I. Loris, S. Vaiter, M. Golbabaee, D.\n  Vukobratovic", "title": "Proceedings of the second \"international Traveling Workshop on\n  Interactions between Sparse models and Technology\" (iTWIST'14)", "comments": "69 pages, 24 extended abstracts, iTWIST'14 website:\n  http://sites.google.com/site/itwist14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implicit objective of the biennial \"international - Traveling Workshop on\nInteractions between Sparse models and Technology\" (iTWIST) is to foster\ncollaboration between international scientific teams by disseminating ideas\nthrough both specific oral/poster presentations and free discussions. For its\nsecond edition, the iTWIST workshop took place in the medieval and picturesque\ntown of Namur in Belgium, from Wednesday August 27th till Friday August 29th,\n2014. The workshop was conveniently located in \"The Arsenal\" building within\nwalking distance of both hotels and town center. iTWIST'14 has gathered about\n70 international participants and has featured 9 invited talks, 10 oral\npresentations, and 14 posters on the following themes, all related to the\ntheory, application and generalization of the \"sparsity paradigm\":\nSparsity-driven data sensing and processing; Union of low dimensional\nsubspaces; Beyond linear and convex inverse problem; Matrix/manifold/graph\nsensing/processing; Blind inverse problems and dictionary learning; Sparsity\nand computational neuroscience; Information theory, geometry and randomness;\nComplexity/accuracy tradeoffs in numerical methods; Sparsity? What's next?;\nSparse machine learning and inference.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 21:40:08 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 07:55:35 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Jacques", "L.", ""], ["De Vleeschouwer", "C.", ""], ["Boursier", "Y.", ""], ["Sudhakar", "P.", ""], ["De Mol", "C.", ""], ["Pizurica", "A.", ""], ["Anthoine", "S.", ""], ["Vandergheynst", "P.", ""], ["Frossard", "P.", ""], ["Bilen", "C.", ""], ["Kitic", "S.", ""], ["Bertin", "N.", ""], ["Gribonval", "R.", ""], ["Boumal", "N.", ""], ["Mishra", "B.", ""], ["Absil", "P. -A.", ""], ["Sepulchre", "R.", ""], ["Bundervoet", "S.", ""], ["Schretter", "C.", ""], ["Dooms", "A.", ""], ["Schelkens", "P.", ""], ["Chabiron", "O.", ""], ["Malgouyres", "F.", ""], ["Tourneret", "J. -Y.", ""], ["Dobigeon", "N.", ""], ["Chainais", "P.", ""], ["Richard", "C.", ""], ["Cornelis", "B.", ""], ["Daubechies", "I.", ""], ["Dunson", "D.", ""], ["Dankova", "M.", ""], ["Rajmic", "P.", ""], ["Degraux", "K.", ""], ["Cambareri", "V.", ""], ["Geelen", "B.", ""], ["Lafruit", "G.", ""], ["Setti", "G.", ""], ["Determe", "J. -F.", ""], ["Louveaux", "J.", ""], ["Horlin", "F.", ""], ["Dr\u00e9meau", "A.", ""], ["Heas", "P.", ""], ["Herzet", "C.", ""], ["Duval", "V.", ""], ["Peyr\u00e9", "G.", ""], ["Fawzi", "A.", ""], ["Davies", "M.", ""], ["Gillis", "N.", ""], ["Vavasis", "S. A.", ""], ["Soussen", "C.", ""], ["Magoarou", "L. Le", ""], ["Liang", "J.", ""], ["Fadili", "J.", ""], ["Liutkus", "A.", ""], ["Martina", "D.", ""], ["Gigan", "S.", ""], ["Daudet", "L.", ""], ["Maggioni", "M.", ""], ["Minsker", "S.", ""], ["Strawn", "N.", ""], ["Mory", "C.", ""], ["Ngole", "F.", ""], ["Starck", "J. -L.", ""], ["Loris", "I.", ""], ["Vaiter", "S.", ""], ["Golbabaee", "M.", ""], ["Vukobratovic", "D.", ""]]}, {"id": "1410.0736", "submitter": "Zhicheng Yan", "authors": "Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis\n  DeCoste, Wei Di, Yizhou Yu", "title": "HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale\n  Visual Recognition", "comments": "Add new results on ImageNet using VGG-16-layer building block net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image classification, visual separability between different object\ncategories is highly uneven, and some categories are more difficult to\ndistinguish than others. Such difficult categories demand more dedicated\nclassifiers. However, existing deep convolutional neural networks (CNN) are\ntrained as flat N-way classifiers, and few efforts have been made to leverage\nthe hierarchical structure of categories. In this paper, we introduce\nhierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category\nhierarchy. An HD-CNN separates easy classes using a coarse category classifier\nwhile distinguishing difficult classes using fine category classifiers. During\nHD-CNN training, component-wise pretraining is followed by global finetuning\nwith a multinomial logistic loss regularized by a coarse category consistency\nterm. In addition, conditional executions of fine category classifiers and\nlayer parameter compression make HD-CNNs scalable for large-scale visual\nrecognition. We achieve state-of-the-art results on both CIFAR100 and\nlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, we\nbuild up three different HD-CNNs and they lower the top-1 error of the standard\nCNNs by 2.65%, 3.1% and 1.1%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 01:17:20 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 07:51:51 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 03:11:49 GMT"}, {"version": "v4", "created": "Sat, 16 May 2015 03:36:32 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Yan", "Zhicheng", ""], ["Zhang", "Hao", ""], ["Piramuthu", "Robinson", ""], ["Jagadeesh", "Vignesh", ""], ["DeCoste", "Dennis", ""], ["Di", "Wei", ""], ["Yu", "Yizhou", ""]]}, {"id": "1410.0741", "submitter": "Brett Israelsen", "authors": "Brett W. Israelsen, Dale A. Smith", "title": "Generalized Laguerre Reduction of the Volterra Kernel for Practical\n  Identification of Nonlinear Dynamic Systems", "comments": "16 pages", "journal-ref": "AIChE Spring Meeting 2014, Paper 349438", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Volterra series can be used to model a large subset of nonlinear, dynamic\nsystems. A major drawback is the number of coefficients required model such\nsystems. In order to reduce the number of required coefficients, Laguerre\npolynomials are used to estimate the Volterra kernels. Existing literature\nproposes algorithms for a fixed number of Volterra kernels, and Laguerre\nseries. This paper presents a novel algorithm for generalized calculation of\nthe finite order Volterra-Laguerre (VL) series for a MIMO system. An example\naddresses the utility of the algorithm in practical application.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 01:59:25 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Israelsen", "Brett W.", ""], ["Smith", "Dale A.", ""]]}, {"id": "1410.0759", "submitter": "Bryan Catanzaro", "authors": "Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,\n  John Tran, Bryan Catanzaro, Evan Shelhamer", "title": "cuDNN: Efficient Primitives for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a library of efficient implementations of deep learning\nprimitives. Deep learning workloads are computationally intensive, and\noptimizing their kernels is difficult and time-consuming. As parallel\narchitectures evolve, kernels must be reoptimized, which makes maintaining\ncodebases difficult over time. Similar issues have long been addressed in the\nHPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS).\nHowever, there is no analogous library for deep learning. Without such a\nlibrary, researchers implementing deep learning workloads on parallel\nprocessors must create and optimize their own implementations of the main\ncomputational kernels, and this work must be repeated as new parallel\nprocessors emerge. To address this problem, we have created a library similar\nin intent to BLAS, with optimized routines for deep learning workloads. Our\nimplementation contains routines for GPUs, although similarly to the BLAS\nlibrary, these routines could be implemented for other platforms. The library\nis easy to integrate into existing frameworks, and provides optimized\nperformance and memory usage. For example, integrating cuDNN into Caffe, a\npopular framework for convolutional networks, improves performance by 36% on a\nstandard model while also reducing memory consumption.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 06:16:43 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 06:00:21 GMT"}, {"version": "v3", "created": "Thu, 18 Dec 2014 01:13:16 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Chetlur", "Sharan", ""], ["Woolley", "Cliff", ""], ["Vandermersch", "Philippe", ""], ["Cohen", "Jonathan", ""], ["Tran", "John", ""], ["Catanzaro", "Bryan", ""], ["Shelhamer", "Evan", ""]]}, {"id": "1410.0781", "submitter": "Nadav Cohen", "authors": "Nadav Cohen and Amnon Shashua", "title": "SimNets: A Generalization of Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep layered architecture that generalizes classical\nconvolutional neural networks (ConvNets). The architecture, called SimNets, is\ndriven by two operators, one being a similarity function whose family contains\nthe convolution operator used in ConvNets, and the other is a new soft\nmax-min-mean operator called MEX that realizes classical operators like ReLU\nand max pooling, but has additional capabilities that make SimNets a powerful\ngeneralization of ConvNets. Three interesting properties emerge from the\narchitecture: (i) the basic input to hidden layer to output machinery contains\nas special cases kernel machines with the Exponential and Generalized Gaussian\nkernels, the output units being \"neurons in feature space\" (ii) in its general\nform, the basic machinery has a higher abstraction level than kernel machines,\nand (iii) initializing networks using unsupervised learning is natural.\nExperiments demonstrate the capability of achieving state of the art accuracy\nwith networks that are an order of magnitude smaller than comparable ConvNets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 08:47:03 GMT"}, {"version": "v2", "created": "Sat, 25 Oct 2014 09:47:07 GMT"}, {"version": "v3", "created": "Sun, 7 Dec 2014 15:51:28 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Cohen", "Nadav", ""], ["Shashua", "Amnon", ""]]}, {"id": "1410.0908", "submitter": "Ernest Fokoue", "authors": "Xingchen Yu and Ernest Fokoue", "title": "Probit Normal Correlated Topic Models", "comments": "11 pages, 2 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logistic normal distribution has recently been adapted via the\ntransformation of multivariate Gaus- sian variables to model the topical\ndistribution of documents in the presence of correlations among topics. In this\npaper, we propose a probit normal alternative approach to modelling correlated\ntopical structures. Our use of the probit model in the context of topic\ndiscovery is novel, as many authors have so far con- centrated solely of the\nlogistic model partly due to the formidable inefficiency of the multinomial\nprobit model even in the case of very small topical spaces. We herein\ncircumvent the inefficiency of multinomial probit estimation by using an\nadaptation of the diagonal orthant multinomial probit in the topic models\ncontext, resulting in the ability of our topic modelling scheme to handle\ncorpuses with a large number of latent topics. An additional and very important\nbenefit of our method lies in the fact that unlike with the logistic normal\nmodel whose non-conjugacy leads to the need for sophisticated sampling schemes,\nour ap- proach exploits the natural conjugacy inherent in the auxiliary\nformulation of the probit model to achieve greater simplicity. The application\nof our proposed scheme to a well known Associated Press corpus not only helps\ndiscover a large number of meaningful topics but also reveals the capturing of\ncompellingly intuitive correlations among certain topics. Besides, our proposed\napproach lends itself to even further scalability thanks to various existing\nhigh performance algorithms and architectures capable of handling millions of\ndocuments.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 16:38:53 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Yu", "Xingchen", ""], ["Fokoue", "Ernest", ""]]}, {"id": "1410.0949", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari", "title": "Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits", "comments": "Proceedings of the 18th International Conference on Artificial\n  Intelligence and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stochastic combinatorial semi-bandit is an online learning problem where at\neach step a learning agent chooses a subset of ground items subject to\nconstraints, and then observes stochastic weights of these items and receives\ntheir sum as a payoff. In this paper, we close the problem of computationally\nand sample efficient learning in stochastic combinatorial semi-bandits. In\nparticular, we analyze a UCB-like algorithm for solving the problem, which is\nknown to be computationally efficient; and prove $O(K L (1 / \\Delta) \\log n)$\nand $O(\\sqrt{K L n \\log n})$ upper bounds on its $n$-step regret, where $L$ is\nthe number of ground items, $K$ is the maximum number of chosen items, and\n$\\Delta$ is the gap between the expected returns of the optimal and best\nsuboptimal solutions. The gap-dependent bound is tight up to a constant factor\nand the gap-free bound is tight up to a polylogarithmic factor.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 19:38:16 GMT"}, {"version": "v2", "created": "Sun, 26 Oct 2014 04:30:17 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 05:15:20 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1410.0996", "submitter": "Steve Hanneke", "authors": "Steve Hanneke and Liu Yang", "title": "Minimax Analysis of Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work establishes distribution-free upper and lower bounds on the minimax\nlabel complexity of active learning with general hypothesis classes, under\nvarious noise models. The results reveal a number of surprising facts. In\nparticular, under the noise model of Tsybakov (2004), the minimax label\ncomplexity of active learning with a VC class is always asymptotically smaller\nthan that of passive learning, and is typically significantly smaller than the\nbest previously-published upper bounds in the active learning literature. In\nhigh-noise regimes, it turns out that all active learning problems of a given\nVC dimension have roughly the same minimax label complexity, which contrasts\nwith well-known results for bounded noise. In low-noise regimes, we find that\nthe label complexity is well-characterized by a simple combinatorial complexity\nmeasure we call the star number. Interestingly, we find that almost all of the\ncomplexity measures previously explored in the active learning literature have\nworst-case values exactly equal to the star number. We also propose new active\nlearning strategies that nearly achieve these minimax label complexities.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 23:30:16 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1410.1068", "submitter": "Anirban Roychowdhury", "authors": "Anirban Roychowdhury, Brian Kulis", "title": "Gamma Processes, Stick-Breaking, and Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most Bayesian nonparametric models in machine learning have focused on\nthe Dirichlet process, the beta process, or their variants, the gamma process\nhas recently emerged as a useful nonparametric prior in its own right. Current\ninference schemes for models involving the gamma process are restricted to\nMCMC-based methods, which limits their scalability. In this paper, we present a\nvariational inference framework for models involving gamma process priors. Our\napproach is based on a novel stick-breaking constructive definition of the\ngamma process. We prove correctness of this stick-breaking process by using the\ncharacterization of the gamma process as a completely random measure (CRM), and\nwe explicitly derive the rate measure of our construction using Poisson process\nmachinery. We also derive error bounds on the truncation of the infinite\nprocess required for variational inference, similar to the truncation analyses\nfor other nonparametric models based on the Dirichlet and beta processes. Our\nrepresentation is then used to derive a variational inference algorithm for a\nparticular Bayesian nonparametric latent structure formulation known as the\ninfinite Gamma-Poisson model, where the latent variables are drawn from a gamma\nprocess prior with Poisson likelihoods. Finally, we present results for our\nalgorithms on nonnegative matrix factorization tasks on document corpora, and\nshow that we compare favorably to both sampling-based techniques and\nvariational approaches based on beta-Bernoulli priors.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 17:36:58 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Roychowdhury", "Anirban", ""], ["Kulis", "Brian", ""]]}, {"id": "1410.1090", "submitter": "Junhua Mao", "authors": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille", "title": "Explain Images with Multimodal Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel sentence descriptions to explain the content of images. It\ndirectly models the probability distribution of generating a word given\nprevious words and the image. Image descriptions are generated by sampling from\nthis distribution. The model consists of two sub-networks: a deep recurrent\nneural network for sentences and a deep convolutional network for images. These\ntwo sub-networks interact with each other in a multimodal layer to form the\nwhole m-RNN model. The effectiveness of our model is validated on three\nbenchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model\noutperforms the state-of-the-art generative method. In addition, the m-RNN\nmodel can be applied to retrieval tasks for retrieving images or sentences, and\nachieves significant performance improvement over the state-of-the-art methods\nwhich directly optimize the ranking objective function for retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 20:24:34 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Wei", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1410.1103", "submitter": "Sougata Chaudhuri", "authors": "Sougata Chaudhuri and Ambuj Tewari", "title": "Online Ranking with Top-1 Feedback", "comments": "Previous version being replaced by conference version. Appeared in\n  AISTATS 2015", "journal-ref": "AISTATS 15, volume 38 of JMLR Workshop and Conference Proceedings,\n  pg.- 129-137, 2015", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a setting where a system learns to rank a fixed set of $m$ items.\nThe goal is produce good item rankings for users with diverse interests who\ninteract online with the system for $T$ rounds. We consider a novel top-$1$\nfeedback model: at the end of each round, the relevance score for only the top\nranked object is revealed. However, the performance of the system is judged on\nthe entire ranked list. We provide a comprehensive set of results regarding\nlearnability under this challenging setting. For PairwiseLoss and DCG, two\npopular ranking measures, we prove that the minimax regret is\n$\\Theta(T^{2/3})$. Moreover, the minimax regret is achievable using an\nefficient strategy that only spends $O(m \\log m)$ time per round. The same\nefficient strategy achieves $O(T^{2/3})$ regret for Precision@$k$.\nSurprisingly, we show that for normalized versions of these ranking measures,\ni.e., AUC, NDCG \\& MAP, no online ranking algorithm can have sublinear regret.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 00:51:59 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 18:10:14 GMT"}, {"version": "v3", "created": "Sun, 6 Mar 2016 20:42:02 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1410.1141", "submitter": "Roi Livni", "authors": "Roi Livni and Shai Shalev-Shwartz and Ohad Shamir", "title": "On the Computational Efficiency of Training Neural Networks", "comments": "Section 2 is revised due to a mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that neural networks are computationally hard to train. On\nthe other hand, in practice, modern day neural networks are trained efficiently\nusing SGD and a variety of tricks that include different activation functions\n(e.g. ReLU), over-specification (i.e., train networks which are larger than\nneeded), and regularization. In this paper we revisit the computational\ncomplexity of training neural networks from a modern perspective. We provide\nboth positive and negative results, some of them yield new provably efficient\nand practical algorithms for training certain types of neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 10:54:07 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 19:14:37 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Livni", "Roi", ""], ["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""]]}, {"id": "1410.1165", "submitter": "Rupesh Kumar Srivastava", "authors": "Rupesh Kumar Srivastava, Jonathan Masci, Faustino Gomez, J\\\"urgen\n  Schmidhuber", "title": "Understanding Locally Competitive Networks", "comments": "9 pages + 2 supplementary, Accepted to ICLR 2015 Conference track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed neural network activation functions such as rectified\nlinear, maxout, and local winner-take-all have allowed for faster and more\neffective training of deep neural architectures on large and complex datasets.\nThe common trait among these functions is that they implement local competition\nbetween small groups of computational units within a layer, so that only part\nof the network is activated for any given input pattern. In this paper, we\nattempt to visualize and understand this self-modularization, and suggest a\nunified explanation for the beneficial properties of such networks. We also\nshow how our insights can be directly useful for efficiently performing\nretrieval over large datasets using neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 14:46:47 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 20:07:17 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 01:22:49 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Srivastava", "Rupesh Kumar", ""], ["Masci", "Jonathan", ""], ["Gomez", "Faustino", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1410.1228", "submitter": "Jonathan Ullman", "authors": "Thomas Steinke and Jonathan Ullman", "title": "Interactive Fingerprinting Codes and the Hardness of Preventing False\n  Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an essentially tight bound on the number of adaptively chosen\nstatistical queries that a computationally efficient algorithm can answer\naccurately given $n$ samples from an unknown distribution. A statistical query\nasks for the expectation of a predicate over the underlying distribution, and\nan answer to a statistical query is accurate if it is \"close\" to the correct\nexpectation over the distribution. This question was recently studied by Dwork\net al., who showed how to answer $\\tilde{\\Omega}(n^2)$ queries efficiently, and\nalso by Hardt and Ullman, who showed that answering $\\tilde{O}(n^3)$ queries is\nhard. We close the gap between the two bounds and show that, under a standard\nhardness assumption, there is no computationally efficient algorithm that,\ngiven $n$ samples from an unknown distribution, can give valid answers to\n$O(n^2)$ adaptively chosen statistical queries. An implication of our results\nis that computationally efficient algorithms for answering arbitrary,\nadaptively chosen statistical queries may as well be differentially private.\n  We obtain our results using a new connection between the problem of answering\nadaptively chosen statistical queries and a combinatorial object called an\ninteractive fingerprinting code. In order to optimize our hardness result, we\ngive a new Fourier-analytic approach to analyzing fingerprinting codes that is\nsimpler, more flexible, and yields better parameters than previous\nconstructions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 23:55:22 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 19:29:47 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1410.1462", "submitter": "Zhi-Hua Zhou", "authors": "Nan Li and Rong Jin and Zhi-Hua Zhou", "title": "Top Rank Optimization in Linear Time", "comments": null, "journal-ref": "NIPS 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite ranking aims to learn a real-valued ranking function that orders\npositive instances before negative instances. Recent efforts of bipartite\nranking are focused on optimizing ranking accuracy at the top of the ranked\nlist. Most existing approaches are either to optimize task specific metrics or\nto extend the ranking loss by emphasizing more on the error associated with the\ntop ranked instances, leading to a high computational cost that is super-linear\nin the number of training instances. We propose a highly efficient approach,\ntitled TopPush, for optimizing accuracy at the top that has computational\ncomplexity linear in the number of training instances. We present a novel\nanalysis that bounds the generalization error for the top ranked instances for\nthe proposed approach. Empirical study shows that the proposed approach is\nhighly competitive to the state-of-the-art approaches and is 10-100 times\nfaster.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 17:10:23 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Li", "Nan", ""], ["Jin", "Rong", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1410.1784", "submitter": "Andres Masegosa R", "authors": "Andres R. Masegosa", "title": "Stochastic Discriminative EM", "comments": "UAI 2014 paper + Supplementary Material. In Proceedings of the\n  Thirtieth Conference on Uncertainty in Artificial Intelligence (UAI 2014),\n  edited by Nevin L. Zhang and Jian Tian. AUAI Press", "journal-ref": "Proceedings of the Thirtieth Conference on Uncertainty in\n  Artificial Intelligence UAI-2014 (pp. 573-582). AUAI Press", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic discriminative EM (sdEM) is an online-EM-type algorithm for\ndiscriminative training of probabilistic generative models belonging to the\nexponential family. In this work, we introduce and justify this algorithm as a\nstochastic natural gradient descent method, i.e. a method which accounts for\nthe information geometry in the parameter space of the statistical model. We\nshow how this learning algorithm can be used to train probabilistic generative\nmodels by minimizing different discriminative loss functions, such as the\nnegative conditional log-likelihood and the Hinge loss. The resulting models\ntrained by sdEM are always generative (i.e. they define a joint probability\ndistribution) and, in consequence, allows to deal with missing data and latent\nvariables in a principled way either when being learned or when making\npredictions. The performance of this method is illustrated by several text\nclassification problems for which a multinomial naive Bayes and a latent\nDirichlet allocation based classifier are learned using different\ndiscriminative loss functions.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 12:10:40 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Masegosa", "Andres R.", ""]]}, {"id": "1410.1940", "submitter": "Qi(Rose) Yu", "authors": "Qi (Rose) Yu, Xinran He and Yan Liu", "title": "GLAD: Group Anomaly Detection in Social Media Analysis- Extended\n  Abstract", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional anomaly detection on social media mostly focuses on individual\npoint anomalies while anomalous phenomena usually occur in groups. Therefore it\nis valuable to study the collective behavior of individuals and detect group\nanomalies. Existing group anomaly detection approaches rely on the assumption\nthat the groups are known, which can hardly be true in real world social media\napplications. In this paper, we take a generative approach by proposing a\nhierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLAD\ntakes both pair-wise and point-wise data as input, automatically infers the\ngroups and detects group anomalies simultaneously. To account for the dynamic\nproperties of the social media data, we further generalize GLAD to its dynamic\nextension d-GLAD. We conduct extensive experiments to evaluate our models on\nboth synthetic and real world datasets. The empirical results demonstrate that\nour approach is effective and robust in discovering latent groups and detecting\ngroup anomalies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 23:11:37 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Qi", "", "", "Rose"], ["Yu", "", ""], ["He", "Xinran", ""], ["Liu", "Yan", ""]]}, {"id": "1410.2045", "submitter": "Ashis  Mandal", "authors": "Ashis Kumar Mandal and Rikta Sen", "title": "Supervised learning Methods for Bangla Web Document Categorization", "comments": "13 pages, International Journal of Artificial Intelligence &\n  Applications (IJAIA), Vol. 5, No. 5, September 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 10:01:47 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Mandal", "Ashis Kumar", ""], ["Sen", "Rikta", ""]]}, {"id": "1410.2191", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang, Xin Gao", "title": "Learning manifold to regularize nonnegative matrix factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inthischapterwediscusshowtolearnanoptimalmanifoldpresentationto regularize\nnonegative matrix factorization (NMF) for data representation problems.\nNMF,whichtriestorepresentanonnegativedatamatrixasaproductoftwolowrank\nnonnegative matrices, has been a popular method for data representation due to\nits ability to explore the latent part-based structure of data. Recent study\nshows that lots of data distributions have manifold structures, and we should\nrespect the manifold structure when the data are represented. Recently,\nmanifold regularized NMF used a nearest neighbor graph to regulate the learning\nof factorization parameter matrices and has shown its advantage over\ntraditional NMF methods for data representation problems. However, how to\nconstruct an optimal graph to present the manifold prop- erly remains a\ndifficultproblem due to the graph modelselection, noisy features, and nonlinear\ndistributed data. In this chapter, we introduce three effective methods to\nsolve these problems of graph construction for manifold regularized NMF.\nMultiple graph learning is proposed to solve the problem of graph model\nselection, adaptive graph learning via feature selection is proposed to solve\nthe problem of constructing a graph from noisy features, while multi-kernel\nlearning-based graph construction is used to solve the problem of learning a\ngraph from nonlinearly distributed data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 09:25:43 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Wang", "Jim Jing-Yan", ""], ["Gao", "Xin", ""]]}, {"id": "1410.2386", "submitter": "Qibin Zhao Dr", "authors": "Qibin Zhao, Guoxu Zhou, Liqing Zhang, Andrzej Cichocki, and Shun-ichi\n  Amari", "title": "Bayesian Robust Tensor Factorization for Incomplete Multiway Data", "comments": "in IEEE Transactions on Neural Networks and Learning Systems, 2015", "journal-ref": null, "doi": "10.1109/TNNLS.2015.2423694", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative model for robust tensor factorization in the presence\nof both missing data and outliers. The objective is to explicitly infer the\nunderlying low-CP-rank tensor capturing the global information and a sparse\ntensor capturing the local information (also considered as outliers), thus\nproviding the robust predictive distribution over missing entries. The\nlow-CP-rank tensor is modeled by multilinear interactions between multiple\nlatent factors on which the column sparsity is enforced by a hierarchical\nprior, while the sparse tensor is modeled by a hierarchical view of Student-$t$\ndistribution that associates an individual hyperparameter with each element\nindependently. For model learning, we develop an efficient closed-form\nvariational inference under a fully Bayesian treatment, which can effectively\nprevent the overfitting problem and scales linearly with data size. In contrast\nto existing related works, our method can perform model selection automatically\nand implicitly without need of tuning parameters. More specifically, it can\ndiscover the groundtruth of CP rank and automatically adapt the sparsity\ninducing priors to various types of outliers. In addition, the tradeoff between\nthe low-rank approximation and the sparse representation can be optimized in\nthe sense of maximum model evidence. The extensive experiments and comparisons\nwith many state-of-the-art algorithms on both synthetic and real-world datasets\ndemonstrate the superiorities of our method from several perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 08:50:31 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 05:36:23 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zhao", "Qibin", ""], ["Zhou", "Guoxu", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""], ["Amari", "Shun-ichi", ""]]}, {"id": "1410.2455", "submitter": "Stephan Gouws", "authors": "Stephan Gouws, Yoshua Bengio, Greg Corrado", "title": "BilBOWA: Fast Bilingual Distributed Representations without Word\n  Alignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple\nand computationally-efficient model for learning bilingual distributed\nrepresentations of words which can scale to large monolingual datasets and does\nnot require word-aligned parallel training data. Instead it trains directly on\nmonolingual data and extracts a bilingual signal from a smaller set of raw-text\nsentence-aligned data. This is achieved using a novel sampled bag-of-words\ncross-lingual objective, which is used to regularize two noise-contrastive\nlanguage models for efficient cross-lingual feature learning. We show that\nbilingual embeddings learned using the proposed model outperform\nstate-of-the-art methods on a cross-lingual document classification task as\nwell as a lexical translation task on WMT11 data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 13:41:18 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 20:52:32 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 05:51:59 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Gouws", "Stephan", ""], ["Bengio", "Yoshua", ""], ["Corrado", "Greg", ""]]}, {"id": "1410.2500", "submitter": "Eric Bax", "authors": "Eric Bax, Lingjie Weng, Xu Tian", "title": "Speculate-Correct Error Bounds for k-Nearest Neighbor Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the speculate-correct method to derive error bounds for local\nclassifiers. Using it, we show that k nearest neighbor classifiers, in spite of\ntheir famously fractured decision boundaries, have exponential error bounds\nwith O(sqrt((k + ln n) / n)) error bound range for n in-sample examples.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 15:08:57 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 22:52:02 GMT"}, {"version": "v3", "created": "Thu, 17 Mar 2016 05:10:42 GMT"}, {"version": "v4", "created": "Sat, 7 Jan 2017 17:27:11 GMT"}, {"version": "v5", "created": "Mon, 27 Feb 2017 20:20:36 GMT"}, {"version": "v6", "created": "Fri, 15 Sep 2017 23:31:16 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bax", "Eric", ""], ["Weng", "Lingjie", ""], ["Tian", "Xu", ""]]}, {"id": "1410.2505", "submitter": "Ping Li", "authors": "Jian Wang, Ping Li", "title": "Recovery of Sparse Signals Using Multiple Orthogonal Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering sparse signals from compressed linear\nmeasurements. This problem, often referred to as sparse recovery or sparse\nreconstruction, has generated a great deal of interest in recent years. To\nrecover the sparse signals, we propose a new method called multiple orthogonal\nleast squares (MOLS), which extends the well-known orthogonal least squares\n(OLS) algorithm by allowing multiple $L$ indices to be chosen per iteration.\nOwing to inclusion of multiple support indices in each selection, the MOLS\nalgorithm converges in much fewer iterations and improves the computational\nefficiency over the conventional OLS algorithm. Theoretical analysis shows that\nMOLS ($L > 1$) performs exact recovery of all $K$-sparse signals within $K$\niterations if the measurement matrix satisfies the restricted isometry property\n(RIP) with isometry constant $\\delta_{LK} < \\frac{\\sqrt{L}}{\\sqrt{K} + 2\n\\sqrt{L}}.$ The recovery performance of MOLS in the noisy scenario is also\nstudied. It is shown that stable recovery of sparse signals can be achieved\nwith the MOLS algorithm when the signal-to-noise ratio (SNR) scales linearly\nwith the sparsity level of input signals.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 15:17:54 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 04:27:22 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Wang", "Jian", ""], ["Li", "Ping", ""]]}, {"id": "1410.2686", "submitter": "F. Ozgur Catak", "authors": "Ferhat \\\"Ozg\\\"ur \\c{C}atak", "title": "Polarization Measurement of High Dimensional Social Media Messages With\n  Support Vector Machine Algorithm Using Mapreduce", "comments": "12 pages, in Turkish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new Support Vector Machine (SVM) training\nalgorithm based on distributed MapReduce technique. In literature, there are a\nlots of research that shows us SVM has highest generalization property among\nclassification algorithms used in machine learning area. Also, SVM classifier\nmodel is not affected by correlations of the features. But SVM uses quadratic\noptimization techniques in its training phase. The SVM algorithm is formulated\nas quadratic optimization problem. Quadratic optimization problem has $O(m^3)$\ntime and $O(m^2)$ space complexity, where m is the training set size. The\ncomputation time of SVM training is quadratic in the number of training\ninstances. In this reason, SVM is not a suitable classification algorithm for\nlarge scale dataset classification. To solve this training problem we developed\na new distributed MapReduce method developed. Accordingly, (i) SVM algorithm is\ntrained in distributed dataset individually; (ii) then merge all support\nvectors of classifier model in every trained node; and (iii) iterate these two\nsteps until the classifier model converges to the optimal classifier function.\nIn the implementation phase, large scale social media dataset is presented in\nTFxIDF matrix. The matrix is used for sentiment analysis to get polarization\nvalue. Two and three class models are created for classification method.\nConfusion matrices of each classification model are presented in tables. Social\nmedia messages corpus consists of 108 public and 66 private universities\nmessages in Turkey. Twitter is used for source of corpus. Twitter user messages\nare collected using Twitter Streaming API. Results are shown in graphics and\ntables.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 06:42:25 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2015 05:56:51 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["\u00c7atak", "Ferhat \u00d6zg\u00fcr", ""]]}, {"id": "1410.2786", "submitter": "Hanli Qiao", "authors": "Hanli Qiao", "title": "New SVD based initialization strategy for Non-negative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two problems need to be dealt with for Non-negative Matrix\nFactorization (NMF): choose a suitable rank of the factorization and provide a\ngood initialization method for NMF algorithms. This paper aims to solve these\ntwo problems using Singular Value Decomposition (SVD). At first we extract the\nnumber of main components as the rank, actually this method is inspired from\n[1, 2]. Second, we use the singular value and its vectors to initialize NMF\nalgorithm. In 2008, Boutsidis and Gollopoulos [3] provided the method titled\nNNDSVD to enhance initialization of NMF algorithms. They extracted the positive\nsection and respective singular triplet information of the unit matrices\n{C(j)}k j=1 which were obtained from singular vector pairs. This strategy aims\nto use positive section to cope with negative elements of the singular vectors,\nbut in experiments we found that even replacing negative elements by their\nabsolute values could get better results than NNDSVD. Hence, we give another\nmethod based SVD to fulfil initialization for NMF algorithms (SVD-NMF).\nNumerical experiments on two face databases ORL and YALE [16, 17] show that our\nmethod is better than NNDSVD.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 13:56:58 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Qiao", "Hanli", ""]]}, {"id": "1410.2838", "submitter": "Ender Konukoglu", "authors": "Ender Konukoglu and Melanie Ganz", "title": "Approximate False Positive Rate Control in Selection Frequency for\n  Random Forest", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forest has become one of the most popular tools for feature selection.\nIts ability to deal with high-dimensional data makes this algorithm especially\nuseful for studies in neuroimaging and bioinformatics. Despite its popularity\nand wide use, feature selection in Random Forest still lacks a crucial\ningredient: false positive rate control. To date there is no efficient,\nprincipled and computationally light-weight solution to this shortcoming. As a\nresult, researchers using Random Forest for feature selection have to resort to\nusing heuristically set thresholds on feature rankings. This article builds an\napproximate probabilistic model for the feature selection process in random\nforest training, which allows us to compute an estimated false positive rate\nfor a given threshold on selection frequency. Hence, it presents a principled\nway to determine thresholds for the selection of relevant features without any\nadditional computational load. Experimental analysis with synthetic data\ndemonstrates that the proposed approach can limit false positive rates on the\norder of the desired values and keep false negative rates low. Results show\nthat this holds even in the presence of a complex correlation structure between\nfeatures. Its good statistical properties and light-weight computational needs\nmake this approach widely applicable to feature selection for a wide-range of\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 16:43:16 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Konukoglu", "Ender", ""], ["Ganz", "Melanie", ""]]}, {"id": "1410.3059", "submitter": "Greg Yang", "authors": "Greg Yang", "title": "Computabilities of Validity and Satisfiability in Probability Logics\n  over Finite and Countable Models", "comments": "47 pages, 4 tables. Comments welcome. Fixed errors found by Rutger\n  Kuyper", "journal-ref": "Journal of Applied Non-Classical Logics 25, no. 4 (2015): 324-72", "doi": "10.1080/11663081.2016.1139967", "report-no": null, "categories": "cs.LO cs.LG math.LO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\epsilon$-logic (which is called $\\epsilon$E-logic in this paper) of\nKuyper and Terwijn is a variant of first order logic with the same syntax, in\nwhich the models are equipped with probability measures and in which the\n$\\forall x$ quantifier is interpreted as \"there exists a set $A$ of measure\n$\\ge 1 - \\epsilon$ such that for each $x \\in A$, ....\" Previously, Kuyper and\nTerwijn proved that the general satisfiability and validity problems for this\nlogic are, i) for rational $\\epsilon \\in (0, 1)$, respectively\n$\\Sigma^1_1$-complete and $\\Pi^1_1$-hard, and ii) for $\\epsilon = 0$,\nrespectively decidable and $\\Sigma^0_1$-complete. The adjective \"general\" here\nmeans \"uniformly over all languages.\"\n  We extend these results in the scenario of finite models. In particular, we\nshow that the problems of satisfiability by and validity over finite models in\n$\\epsilon$E-logic are, i) for rational $\\epsilon \\in (0, 1)$, respectively\n$\\Sigma^0_1$- and $\\Pi^0_1$-complete, and ii) for $\\epsilon = 0$, respectively\ndecidable and $\\Pi^0_1$-complete. Although partial results toward the countable\ncase are also achieved, the computability of $\\epsilon$E-logic over countable\nmodels still remains largely unsolved. In addition, most of the results, of\nthis paper and of Kuyper and Terwijn, do not apply to individual languages with\na finite number of unary predicates. Reducing this requirement continues to be\na major point of research.\n  On the positive side, we derive the decidability of the corresponding\nproblems for monadic relational languages --- equality- and function-free\nlanguages with finitely many unary and zero other predicates. This result holds\nfor all three of the unrestricted, the countable, and the finite model cases.\n  Applications in computational learning theory, weighted graphs, and neural\nnetworks are discussed in the context of these decidability and undecidability\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 12 Oct 2014 07:53:00 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 16:47:40 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Yang", "Greg", ""]]}, {"id": "1410.3145", "submitter": "Peter Hossain", "authors": "Peter Hossain, Adaulfo Komisarczuk, Garin Pawetczak, Sarah Van Dijk,\n  Isabella Axelsen", "title": "Machine Learning Techniques in Cognitive Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive radio is an intelligent radio that can be programmed and configured\ndynamically to fully use the frequency resources that are not used by licensed\nusers. It defines the radio devices that are capable of learning and adapting\nto their transmission to the external radio environment, which means it has\nsome kind of intelligence for monitoring the radio environment, learning the\nenvironment and make smart decisions. In this paper, we are reviewing some\nexamples of the usage of machine learning techniques in cognitive radio\nnetworks for implementing the intelligent radio.\n", "versions": [{"version": "v1", "created": "Sun, 12 Oct 2014 20:43:04 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Hossain", "Peter", ""], ["Komisarczuk", "Adaulfo", ""], ["Pawetczak", "Garin", ""], ["Van Dijk", "Sarah", ""], ["Axelsen", "Isabella", ""]]}, {"id": "1410.3169", "submitter": "Ellen Gasparovic", "authors": "Paul Bendich, Ellen Gasparovic, John Harer, Rauf Izmailov, and Linda\n  Ness", "title": "Multi-Scale Local Shape Analysis and Feature Selection in Machine\n  Learning Applications", "comments": "15 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method called multi-scale local shape analysis, or MLSA, for\nextracting features that describe the local structure of points within a\ndataset. The method uses both geometric and topological features at multiple\nlevels of granularity to capture diverse types of local information for\nsubsequent machine learning algorithms operating on the dataset. Using\nsynthetic and real dataset examples, we demonstrate significant performance\nimprovement of classification algorithms constructed for these datasets with\ncorrespondingly augmented features.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 00:21:59 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Bendich", "Paul", ""], ["Gasparovic", "Ellen", ""], ["Harer", "John", ""], ["Izmailov", "Rauf", ""], ["Ness", "Linda", ""]]}, {"id": "1410.3314", "submitter": "Roman Garnett", "authors": "Marion Neumann and Roman Garnett and Christian Bauckhage and Kristian\n  Kersting", "title": "Propagation Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce propagation kernels, a general graph-kernel framework for\nefficiently measuring the similarity of structured data. Propagation kernels\nare based on monitoring how information spreads through a set of given graphs.\nThey leverage early-stage distributions from propagation schemes such as random\nwalks to capture structural information encoded in node labels, attributes, and\nedge information. This has two benefits. First, off-the-shelf propagation\nschemes can be used to naturally construct kernels for many graph types,\nincluding labeled, partially labeled, unlabeled, directed, and attributed\ngraphs. Second, by leveraging existing efficient and informative propagation\nschemes, propagation kernels can be considerably faster than state-of-the-art\napproaches without sacrificing predictive performance. We will also show that\nif the graphs at hand have a regular structure, for instance when modeling\nimage or video data, one can exploit this regularity to scale the kernel\ncomputation to large databases of graphs with thousands of nodes. We support\nour contributions by exhaustive experiments on a number of real-world graphs\nfrom a variety of application domains.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 14:04:15 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Neumann", "Marion", ""], ["Garnett", "Roman", ""], ["Bauckhage", "Christian", ""], ["Kersting", "Kristian", ""]]}, {"id": "1410.3341", "submitter": "Fei Tian", "authors": "Haifang Li, Fei Tian, Wei Chen, Tao Qin, Tie-Yan Liu", "title": "Generalization Analysis for Game-Theoretic Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Internet applications like sponsored search, cautions need to be taken\nwhen using machine learning to optimize their mechanisms (e.g., auction) since\nself-interested agents in these applications may change their behaviors (and\nthus the data distribution) in response to the mechanisms. To tackle this\nproblem, a framework called game-theoretic machine learning (GTML) was recently\nproposed, which first learns a Markov behavior model to characterize agents'\nbehaviors, and then learns the optimal mechanism by simulating agents' behavior\nchanges in response to the mechanism. While GTML has demonstrated practical\nsuccess, its generalization analysis is challenging because the behavior data\nare non-i.i.d. and dependent on the mechanism. To address this challenge,\nfirst, we decompose the generalization error for GTML into the behavior\nlearning error and the mechanism learning error; second, for the behavior\nlearning error, we obtain novel non-asymptotic error bounds for both parametric\nand non-parametric behavior learning methods; third, for the mechanism learning\nerror, we derive a uniform convergence bound based on a new concept called\nnested covering number of the mechanism space and the generalization analysis\ntechniques developed for mixing sequences. To the best of our knowledge, this\nis the first work on the generalization analysis of GTML, and we believe it has\ngeneral implications to the theoretical analysis of other complicated machine\nlearning problems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 03:51:19 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Li", "Haifang", ""], ["Tian", "Fei", ""], ["Chen", "Wei", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1410.3348", "submitter": "Ilya Safro", "authors": "Talayeh Razzaghi and Ilya Safro", "title": "Fast Multilevel Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving different types of optimization models (including parameters fitting)\nfor support vector machines on large-scale training data is often an expensive\ncomputational task. This paper proposes a multilevel algorithmic framework that\nscales efficiently to very large data sets. Instead of solving the whole\ntraining set in one optimization process, the support vectors are obtained and\ngradually refined at multiple levels of coarseness of the data. The proposed\nframework includes: (a) construction of hierarchy of large-scale data coarse\nrepresentations, and (b) a local processing of updating the hyperplane\nthroughout this hierarchy. Our multilevel framework substantially improves the\ncomputational time without loosing the quality of classifiers. The algorithms\nare demonstrated for both regular and weighted support vector machines.\nExperimental results are presented for balanced and imbalanced classification\nproblems. Quality improvement on several imbalanced data sets has been\nobserved.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 15:27:45 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Razzaghi", "Talayeh", ""], ["Safro", "Ilya", ""]]}, {"id": "1410.3351", "submitter": "Antonio Ache", "authors": "Antonio G. Ache and Micah W. Warren", "title": "Ricci Curvature and the Manifold Learning Problem", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.LG math.MG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a sample of $n$ points taken i.i.d from a submanifold $\\Sigma$ of\nEuclidean space. We show that there is a way to estimate the Ricci curvature of\n$\\Sigma$ with respect to the induced metric from the sample. Our method is\ngrounded in the notions of Carr\\'e du Champ for diffusion semi-groups, the\ntheory of Empirical processes and local Principal Component Analysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 15:37:20 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 14:38:23 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 05:06:39 GMT"}, {"version": "v4", "created": "Fri, 20 Oct 2017 02:11:21 GMT"}, {"version": "v5", "created": "Wed, 21 Mar 2018 20:47:22 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Ache", "Antonio G.", ""], ["Warren", "Micah W.", ""]]}, {"id": "1410.3386", "submitter": "Jayadev Acharya", "authors": "Jayadev Acharya and Constantinos Daskalakis", "title": "Testing Poisson Binomial Distributions", "comments": "To appear in ACM-SIAM Symposium on Discrete Algorithms (SODA) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Poisson Binomial distribution over $n$ variables is the distribution of the\nsum of $n$ independent Bernoullis. We provide a sample near-optimal algorithm\nfor testing whether a distribution $P$ supported on $\\{0,...,n\\}$ to which we\nhave sample access is a Poisson Binomial distribution, or far from all Poisson\nBinomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$\nto which we provide a matching lower bound. We note that our sample complexity\nimproves quadratically upon that of the naive \"learn followed by tolerant-test\"\napproach, while instance optimal identity testing [VV14] is not applicable\nsince we are looking to simultaneously test against a whole family of\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 16:36:10 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 00:27:21 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Acharya", "Jayadev", ""], ["Daskalakis", "Constantinos", ""]]}, {"id": "1410.3463", "submitter": "Lavanya Sita Tekumalla", "authors": "Lavanya Sita Tekumalla, Chiranjib Bhattacharyya", "title": "Mining Block I/O Traces for Cache Preloading with Sparse Temporal\n  Non-parametric Mixture of Multivariate Poisson", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing caching strategies, in the storage domain, though well suited to\nexploit short range spatio-temporal patterns, are unable to leverage long-range\nmotifs for improving hitrates. Motivated by this, we investigate novel Bayesian\nnon-parametric modeling(BNP) techniques for count vectors, to capture long\nrange correlations for cache preloading, by mining Block I/O traces. Such\ntraces comprise of a sequence of memory accesses that can be aggregated into\nhigh-dimensional sparse correlated count vector sequences.\n  While there are several state of the art BNP algorithms for clustering and\ntheir temporal extensions for prediction, there has been no work on exploring\nthese for correlated count vectors. Our first contribution addresses this gap\nby proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and its\ntemporal extension(HMM-DP-MMVP) that captures the full covariance structure of\nmultivariate count data. However, modeling full covariance structure for count\nvectors is computationally expensive, particularly for high dimensional data.\nHence, we exploit sparsity in our count vectors, and as our main contribution,\nintroduce the Sparse DP mixture of multivariate Poisson(Sparse-DP-MMVP),\ngeneralizing our DP-MMVP mixture model, also leading to more efficient\ninference. We then discuss a temporal extension to our model for cache\npreloading.\n  We take the first step towards mining historical data, to capture long range\npatterns in storage traces for cache preloading. Experimentally, we show a\ndramatic improvement in hitrates on benchmark traces and lay the groundwork for\nfurther research in storage domain to reduce latencies using data mining\ntechniques to capture long range motifs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 14:26:28 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Tekumalla", "Lavanya Sita", ""], ["Bhattacharyya", "Chiranjib", ""]]}, {"id": "1410.3469", "submitter": "Daniel Whiteson", "authors": "Pierre Baldi, Peter Sadowski, Daniel Whiteson", "title": "Enhanced Higgs to $\\tau^+\\tau^-$ Searches with Deep Learning", "comments": "For submission to PRL", "journal-ref": "Phys. Rev. Lett. 114, 111801 (2015)", "doi": "10.1103/PhysRevLett.114.111801", "report-no": null, "categories": "hep-ph cs.LG hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Higgs boson is thought to provide the interaction that imparts mass to\nthe fundamental fermions, but while measurements at the Large Hadron Collider\n(LHC) are consistent with this hypothesis, current analysis techniques lack the\nstatistical power to cross the traditional 5$\\sigma$ significance barrier\nwithout more data. \\emph{Deep learning} techniques have the potential to\nincrease the statistical power of this analysis by \\emph{automatically}\nlearning complex, high-level data representations. In this work, deep neural\nnetworks are used to detect the decay of the Higgs to a pair of tau leptons. A\nBayesian optimization algorithm is used to tune the network architecture and\ntraining algorithm hyperparameters, resulting in a deep network of eight\nnon-linear processing layers that improves upon the performance of shallow\nclassifiers even without the use of features specifically engineered by\nphysicists for this application. The improvement in discovery significance is\nequivalent to an increase in the accumulated dataset of 25\\%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 20:00:03 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Baldi", "Pierre", ""], ["Sadowski", "Peter", ""], ["Whiteson", "Daniel", ""]]}, {"id": "1410.3595", "submitter": "Masahiro Yukawa", "authors": "Masa-aki Takizawa, Masahiro Yukawa, and Cedric Richard", "title": "A stochastic behavior analysis of stochastic restricted-gradient descent\n  algorithm in reproducing kernel Hilbert spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a stochastic behavior analysis of a kernel-based\nstochastic restricted-gradient descent method. The restricted gradient gives a\nsteepest ascent direction within the so-called dictionary subspace. The\nanalysis provides the transient and steady state performance in the mean\nsquared error criterion. It also includes stability conditions in the mean and\nmean-square sense. The present study is based on the analysis of the kernel\nnormalized least mean square (KNLMS) algorithm initially proposed by Chen et\nal. Simulation results validate the analysis.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 07:29:35 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Takizawa", "Masa-aki", ""], ["Yukawa", "Masahiro", ""], ["Richard", "Cedric", ""]]}, {"id": "1410.3596", "submitter": "Masayuki Ohzeki", "authors": "Shogo Yamanaka, Masayuki Ohzeki, Aurelien Decelle", "title": "Detection of cheating by decimation algorithm", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": "10.7566/JPSJ.84.024801", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We expand the item response theory to study the case of \"cheating students\"\nfor a set of exams, trying to detect them by applying a greedy algorithm of\ninference. This extended model is closely related to the Boltzmann machine\nlearning. In this paper we aim to infer the correct biases and interactions of\nour model by considering a relatively small number of sets of training data.\nNevertheless, the greedy algorithm that we employed in the present study\nexhibits good performance with a few number of training data. The key point is\nthe sparseness of the interactions in our problem in the context of the\nBoltzmann machine learning: the existence of cheating students is expected to\nbe very rare (possibly even in real world). We compare a standard approach to\ninfer the sparse interactions in the Boltzmann machine learning to our greedy\nalgorithm and we find the latter to be superior in several aspects.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 07:41:34 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 02:24:07 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Yamanaka", "Shogo", ""], ["Ohzeki", "Masayuki", ""], ["Decelle", "Aurelien", ""]]}, {"id": "1410.3791", "submitter": "Rami Al-Rfou", "authors": "Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, Steven Skiena", "title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition", "comments": "9 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing diversity of languages used on the web introduces a new level\nof complexity to Information Retrieval (IR) systems. We can no longer assume\nthat textual content is written in one language or even the same language\nfamily. In this paper, we demonstrate how to build massive multilingual\nannotators with minimal human expertise and intervention. We describe a system\nthat builds Named Entity Recognition (NER) annotators for 40 major languages\nusing Wikipedia and Freebase. Our approach does not require NER human annotated\ndatasets or language specific resources like treebanks, parallel corpora, and\northographic rules. The novelty of approach lies therein - using only language\nagnostic techniques, while achieving competitive performance.\n  Our method learns distributed word representations (word embeddings) which\nencode semantic and syntactic features of words in each language. Then, we\nautomatically generate datasets from Wikipedia link structure and Freebase\nattributes. Finally, we apply two preprocessing stages (oversampling and exact\nsurface form matching) which do not require any linguistic expertise.\n  Our evaluation is two fold: First, we demonstrate the system performance on\nhuman annotated datasets. Second, for languages where no gold-standard\nbenchmarks are available, we propose a new method, distant evaluation, based on\nstatistical machine translation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 18:37:32 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Al-Rfou", "Rami", ""], ["Kulkarni", "Vivek", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1410.3831", "submitter": "Pankaj Mehta", "authors": "Pankaj Mehta and David J. Schwab", "title": "An exact mapping between the Variational Renormalization Group and Deep\n  Learning", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a broad set of techniques that uses multiple layers of\nrepresentation to automatically learn relevant features directly from\nstructured data. Recently, such techniques have yielded record-breaking results\non a diverse set of difficult machine learning tasks in computer vision, speech\nrecognition, and natural language processing. Despite the enormous success of\ndeep learning, relatively little is understood theoretically about why these\ntechniques are so successful at feature learning and compression. Here, we show\nthat deep learning is intimately related to one of the most important and\nsuccessful techniques in theoretical physics, the renormalization group (RG).\nRG is an iterative coarse-graining scheme that allows for the extraction of\nrelevant features (i.e. operators) as a physical system is examined at\ndifferent length scales. We construct an exact mapping from the variational\nrenormalization group, first introduced by Kadanoff, and deep learning\narchitectures based on Restricted Boltzmann Machines (RBMs). We illustrate\nthese ideas using the nearest-neighbor Ising Model in one and two-dimensions.\nOur results suggests that deep learning algorithms may be employing a\ngeneralized RG-like scheme to learn relevant features from data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 20:00:09 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Mehta", "Pankaj", ""], ["Schwab", "David J.", ""]]}, {"id": "1410.3886", "submitter": "Srinadh Bhojanapalli", "authors": "Srinadh Bhojanapalli, Prateek Jain, Sujay Sanghavi", "title": "Tighter Low-rank Approximation via Sampling the Leveraged Element", "comments": "36 pages, 3 figures, Extended abstract to appear in the proceedings\n  of ACM-SIAM Symposium on Discrete Algorithms (SODA15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new randomized algorithm for computing a low-rank\napproximation to a given matrix. Taking an approach different from existing\nliterature, our method first involves a specific biased sampling, with an\nelement being chosen based on the leverage scores of its row and column, and\nthen involves weighted alternating minimization over the factored form of the\nintended low-rank matrix, to minimize error only on these samples. Our method\ncan leverage input sparsity, yet produce approximations in {\\em spectral} (as\nopposed to the weaker Frobenius) norm; this combines the best aspects of\notherwise disparate current results, but with a dependence on the condition\nnumber $\\kappa = \\sigma_1/\\sigma_r$. In particular we require $O(nnz(M) +\n\\frac{n\\kappa^2 r^5}{\\epsilon^2})$ computations to generate a rank-$r$\napproximation to $M$ in spectral norm. In contrast, the best existing method\nrequires $O(nnz(M)+ \\frac{nr^2}{\\epsilon^4})$ time to compute an approximation\nin Frobenius norm. Besides the tightness in spectral norm, we have a better\ndependence on the error $\\epsilon$. Our method is naturally and highly\nparallelizable.\n  Our new approach enables two extensions that are interesting on their own.\nThe first is a new method to directly compute a low-rank approximation (in\nefficient factored form) to the product of two given matrices; it computes a\nsmall random set of entries of the product, and then executes weighted\nalternating minimization (as before) on these. The sampling strategy is\ndifferent because now we cannot access leverage scores of the product matrix\n(but instead have to work with input matrices). The second extension is an\nimproved algorithm with smaller communication complexity for the distributed\nPCA setting (where each server has small set of rows of the matrix, and want to\ncompute low rank approximation with small amount of communication with other\nservers).\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 22:41:20 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Jain", "Prateek", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1410.3915", "submitter": "Neil Shah", "authors": "Neil Shah, Alex Beutel, Brian Gallagher, Christos Faloutsos", "title": "Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we detect suspicious users in large online networks? Online\npopularity of a user or product (via follows, page-likes, etc.) can be\nmonetized on the premise of higher ad click-through rates or increased sales.\nWeb services and social networks which incentivize popularity thus suffer from\na major problem of fake connections from link fraudsters looking to make a\nquick buck. Typical methods of catching this suspicious behavior use spectral\ntechniques to spot large groups of often blatantly fraudulent (but sometimes\nhonest) users. However, small-scale, stealthy attacks may go unnoticed due to\nthe nature of low-rank eigenanalysis used in practice.\n  In this work, we take an adversarial approach to find and prove claims about\nthe weaknesses of modern, state-of-the-art spectral methods and propose fBox,\nan algorithm designed to catch small-scale, stealth attacks that slip below the\nradar. Our algorithm has the following desirable properties: (a) it has\ntheoretical underpinnings, (b) it is shown to be highly effective on real data\nand (c) it is scalable (linear on the input size). We evaluate fBox on a large,\npublic 41.7 million node, 1.5 billion edge who-follows-whom social graph from\nTwitter in 2010 and with high precision identify many suspicious accounts which\nhave persisted without suspension even to this day.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 03:10:26 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Shah", "Neil", ""], ["Beutel", "Alex", ""], ["Gallagher", "Brian", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1410.3935", "submitter": "Taisuke Sato", "authors": "Taisuke Sato, Keiichi Kubota, Yoshitaka Kameya", "title": "A Logic-based Approach to Generatively Defined Discriminative Modeling", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional random fields (CRFs) are usually specified by graphical models\nbut in this paper we propose to use probabilistic logic programs and specify\nthem generatively. Our intension is first to provide a unified approach to CRFs\nfor complex modeling through the use of a Turing complete language and second\nto offer a convenient way of realizing generative-discriminative pairs in\nmachine learning to compare generative and discriminative models and choose the\nbest model. We implemented our approach as the D-PRISM language by modifying\nPRISM, a logic-based probabilistic modeling language for generative modeling,\nwhile exploiting its dynamic programming mechanism for efficient probability\ncomputation. We tested D-PRISM with logistic regression, a linear-chain CRF and\na CRF-CFG and empirically confirmed their excellent discriminative performance\ncompared to their generative counterparts, i.e.\\ naive Bayes, an HMM and a\nPCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRF\nversions of Bayesian network classifiers and probabilistic left-corner grammars\nrespectively and easily implementable in D-PRISM. We empirically showed that\nthey outperform their generative counterparts as expected.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 06:01:03 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Sato", "Taisuke", ""], ["Kubota", "Keiichi", ""], ["Kameya", "Yoshitaka", ""]]}, {"id": "1410.4009", "submitter": "Dean Eckles", "authors": "Dean Eckles and Maurits Kaptein", "title": "Thompson sampling with the online bootstrap", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling provides a solution to bandit problems in which new\nobservations are allocated to arms with the posterior probability that an arm\nis optimal. While sometimes easy to implement and asymptotically optimal,\nThompson sampling can be computationally demanding in large scale bandit\nproblems, and its performance is dependent on the model fit to the observed\ndata. We introduce bootstrap Thompson sampling (BTS), a heuristic method for\nsolving bandit problems which modifies Thompson sampling by replacing the\nposterior distribution used in Thompson sampling by a bootstrap distribution.\nWe first explain BTS and show that the performance of BTS is competitive to\nThompson sampling in the well-studied Bernoulli bandit case. Subsequently, we\ndetail why BTS using the online bootstrap is more scalable than regular\nThompson sampling, and we show through simulation that BTS is more robust to a\nmisspecified error distribution. BTS is an appealing modification of Thompson\nsampling, especially when samples from the posterior are otherwise not\navailable or are costly.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 11:01:52 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Eckles", "Dean", ""], ["Kaptein", "Maurits", ""]]}, {"id": "1410.4062", "submitter": "Emanuele Frandi", "authors": "Emanuele Frandi, Ricardo Nanculef, Johan Suykens", "title": "Complexity Issues and Randomization Strategies in Frank-Wolfe Algorithms\n  for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frank-Wolfe algorithms for convex minimization have recently gained\nconsiderable attention from the Optimization and Machine Learning communities,\nas their properties make them a suitable choice in a variety of applications.\nHowever, as each iteration requires to optimize a linear model, a clever\nimplementation is crucial to make such algorithms viable on large-scale\ndatasets. For this purpose, approximation strategies based on a random sampling\nhave been proposed by several researchers. In this work, we perform an\nexperimental study on the effectiveness of these techniques, analyze possible\nalternatives and provide some guidelines based on our results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 13:50:34 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Suykens", "Johan", ""]]}, {"id": "1410.4210", "submitter": "Jie  Wang", "authors": "Jie Wang and Jieping Ye", "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of\n  Convex Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique\nfor simultaneously discovering group and within-group sparse patterns by using\na combination of the $\\ell_1$ and $\\ell_2$ norms. However, in large-scale\napplications, the complexity of the regularizers entails great computational\nchallenges. In this paper, we propose a novel Two-Layer Feature REduction\nmethod (TLFre) for SGL via a decomposition of its dual feasible set. The\ntwo-layer reduction is able to quickly identify the inactive groups and the\ninactive features, respectively, which are guaranteed to be absent from the\nsparse representation and can be removed from the optimization. Existing\nfeature reduction methods are only applicable for sparse models with one\nsparsity-inducing regularizer. To our best knowledge, TLFre is the first one\nthat is capable of dealing with multiple sparsity-inducing regularizers.\nMoreover, TLFre has a very low computational cost and can be integrated with\nany existing solvers. We also develop a screening method---called DPC\n(DecomPosition of Convex set)---for the nonnegative Lasso problem. Experiments\non both synthetic and real data sets show that TLFre and DPC improve the\nefficiency of SGL and nonnegative Lasso by several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 20:08:21 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Wang", "Jie", ""], ["Ye", "Jieping", ""]]}, {"id": "1410.4341", "submitter": "Manasij Venkatesh", "authors": "Manasij Venkatesh, Vikas Majjagi, and Deepu Vijayasenan", "title": "Implicit segmentation of Kannada characters in offline handwriting\n  recognition using hidden Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for classification of handwritten Kannada characters\nusing Hidden Markov Models (HMMs). Kannada script is agglutinative, where\nsimple shapes are concatenated horizontally to form a character. This results\nin a large number of characters making the task of classification difficult.\nCharacter segmentation plays a significant role in reducing the number of\nclasses. Explicit segmentation techniques suffer when overlapping shapes are\npresent, which is common in the case of handwritten text. We use HMMs to take\nadvantage of the agglutinative nature of Kannada script, which allows us to\nperform implicit segmentation of characters along with recognition. All the\nexperiments are performed on the Chars74k dataset that consists of 657\nhandwritten characters collected across multiple users. Gradient-based features\nare extracted from individual characters and are used to train character HMMs.\nThe use of implicit segmentation technique at the character level resulted in\nan improvement of around 10%. This system also outperformed an existing system\ntested on the same dataset by around 16%. Analysis based on learning curves\nshowed that increasing the training data could result in better accuracy.\nAccordingly, we collected additional data and obtained an improvement of 4%\nwith 6 additional samples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 09:09:45 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Venkatesh", "Manasij", ""], ["Majjagi", "Vikas", ""], ["Vijayasenan", "Deepu", ""]]}, {"id": "1410.4355", "submitter": "Erik Ferragut", "authors": "Robert A. Bridges, John Collins, Erik M. Ferragut, Jason Laska, Blair\n  D. Sullivan", "title": "Multi-Level Anomaly Detection on Time-Varying Graph Data", "comments": "8 pages. Updated paper to address reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel modeling and analysis framework for graph\nsequences which addresses the challenge of detecting and contextualizing\nanomalies in labelled, streaming graph data. We introduce a generalization of\nthe BTER model of Seshadhri et al. by adding flexibility to community\nstructure, and use this model to perform multi-scale graph anomaly detection.\nSpecifically, probability models describing coarse subgraphs are built by\naggregating probabilities at finer levels, and these closely related\nhierarchical models simultaneously detect deviations from expectation. This\ntechnique provides insight into a graph's structure and internal context that\nmay shed light on a detected event. Additionally, this multi-scale analysis\nfacilitates intuitive visualizations by allowing users to narrow focus from an\nanomalous graph to particular subgraphs or nodes causing the anomaly.\n  For evaluation, two hierarchical anomaly detectors are tested against a\nbaseline Gaussian method on a series of sampled graphs. We demonstrate that our\ngraph statistics-based approach outperforms both a distribution-based detector\nand the baseline in a labeled setting with community structure, and it\naccurately detects anomalies in synthetic and real-world datasets at the node,\nsubgraph, and graph levels. To illustrate the accessibility of information made\npossible via this technique, the anomaly detector and an associated interactive\nvisualization tool are tested on NCAA football data, where teams and\nconferences that moved within the league are identified with perfect recall,\nand precision greater than 0.786.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 09:57:20 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 19:08:37 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 16:58:08 GMT"}, {"version": "v4", "created": "Mon, 20 Apr 2015 11:55:53 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Bridges", "Robert A.", ""], ["Collins", "John", ""], ["Ferragut", "Erik M.", ""], ["Laska", "Jason", ""], ["Sullivan", "Blair D.", ""]]}, {"id": "1410.4391", "submitter": "Justin Bedo", "authors": "Justin Bedo and Cheng Soon Ong", "title": "Multivariate Spearman's rho for aggregating ranks using copulas", "comments": null, "journal-ref": "Journal of Machine Learning Research, 17(201):1-30, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of rank aggregation: given a set of ranked lists, we\nwant to form a consensus ranking. Furthermore, we consider the case of extreme\nlists: i.e., only the rank of the best or worst elements are known. We impute\nmissing ranks by the average value and generalise Spearman's \\rho to extreme\nranks. Our main contribution is the derivation of a non-parametric estimator\nfor rank aggregation based on multivariate extensions of Spearman's \\rho, which\nmeasures correlation between a set of ranked lists. Multivariate Spearman's\n\\rho is defined using copulas, and we show that the geometric mean of\nnormalised ranks maximises multivariate correlation. Motivated by this, we\npropose a weighted geometric mean approach for learning to rank which has a\nclosed form least squares solution. When only the best or worst elements of a\nranked list are known, we impute the missing ranks by the average value,\nallowing us to apply Spearman's \\rho. Finally, we demonstrate good performance\non the rank aggregation benchmarks MQ2007 and MQ2008.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 12:15:17 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 22:59:24 GMT"}, {"version": "v3", "created": "Wed, 7 Sep 2016 06:44:16 GMT"}, {"version": "v4", "created": "Fri, 2 Dec 2016 00:05:32 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Bedo", "Justin", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1410.4461", "submitter": "Ming Xu", "authors": "Xu Ming, Du Yi-man, Wu Jian-ping, Zhou Yang", "title": "Map Matching based on Conditional Random Fields and Route Preference\n  Mining for Uncertain Trajectories", "comments": null, "journal-ref": null, "doi": "10.1155/2015/717095", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve offline map matching accuracy of low-sampling-rate GPS, a\nmap matching algorithm based on conditional random fields (CRF) and route\npreference mining is proposed. In this algorithm, road offset distance and the\ntemporal-spatial relationship between the sampling points are used as features\nof GPS trajectory in CRF model, which can utilize the advantages of integrating\nthe context information into features flexibly. When the sampling rate is too\nlow, it is difficult to guarantee the effectiveness using temporal-spatial\ncontext modeled in CRF, and route preference of a driver is used as\nreplenishment to be superposed on the temporal-spatial transition features. The\nexperimental results show that this method can improve the accuracy of the\nmatching, especially in the case of low sampling rate.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 15:10:59 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 15:51:37 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Ming", "Xu", ""], ["Yi-man", "Du", ""], ["Jian-ping", "Wu", ""], ["Yang", "Zhou", ""]]}, {"id": "1410.4470", "submitter": "Raviteja Vemulapalli", "authors": "Raviteja Vemulapalli, Vinay Praneeth Boda, and Rama Chellappa", "title": "MKL-RT: Multiple Kernel Learning for Ratio-trace Problems via Convex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent past, automatic selection or combination of kernels (or\nfeatures) based on multiple kernel learning (MKL) approaches has been receiving\nsignificant attention from various research communities. Though MKL has been\nextensively studied in the context of support vector machines (SVM), it is\nrelatively less explored for ratio-trace problems. In this paper, we show that\nMKL can be formulated as a convex optimization problem for a general class of\nratio-trace problems that encompasses many popular algorithms used in various\ncomputer vision applications. We also provide an optimization procedure that is\nguaranteed to converge to the global optimum of the proposed optimization\nproblem. We experimentally demonstrate that the proposed MKL approach, which we\nrefer to as MKL-RT, can be successfully used to select features for\ndiscriminative dimensionality reduction and cross-modal retrieval. We also show\nthat the proposed convex MKL-RT approach performs better than the recently\nproposed non-convex MKL-DR approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 15:51:50 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 06:12:37 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Vemulapalli", "Raviteja", ""], ["Boda", "Vinay Praneeth", ""], ["Chellappa", "Rama", ""]]}, {"id": "1410.4510", "submitter": "Finale Doshi-Velez", "authors": "Finale Doshi-Velez and Byron Wallace and Ryan Adams", "title": "Graph-Sparse LDA: A Topic Model with Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Originally designed to model text, topic modeling has become a powerful tool\nfor uncovering latent structure in domains including medicine, finance, and\nvision. The goals for the model vary depending on the application: in some\ncases, the discovered topics may be used for prediction or some other\ndownstream task. In other cases, the content of the topic itself may be of\nintrinsic scientific interest.\n  Unfortunately, even using modern sparse techniques, the discovered topics are\noften difficult to interpret due to the high dimensionality of the underlying\nspace. To improve topic interpretability, we introduce Graph-Sparse LDA, a\nhierarchical topic model that leverages knowledge of relationships between\nwords (e.g., as encoded by an ontology). In our model, topics are summarized by\na few latent concept-words from the underlying graph that explain the observed\nwords. Graph-Sparse LDA recovers sparse, interpretable summaries on two\nreal-world biomedical datasets while matching state-of-the-art prediction\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 17:35:31 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 16:38:59 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Wallace", "Byron", ""], ["Adams", "Ryan", ""]]}, {"id": "1410.4573", "submitter": "Jayadeva", "authors": "Jayadeva, Suresh Chandra, Siddarth Sabharwal, and Sanjit S. Batra", "title": "Learning a hyperplane regressor by minimizing an exact bound on the VC\n  dimension", "comments": "see\n  http://www.sciencedirect.com/science/article/pii/S0925231214010194 or\n  arXiv:1408.2803 for background information", "journal-ref": "Neurocomputing, Volume 171, 1 January 2016, Pages 1610-1616, ISSN\n  0925-2312", "doi": "10.1016/j.neucom.2015.06.065", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity of a learning machine is measured by its Vapnik-Chervonenkis\ndimension, and learning machines with a low VC dimension generalize better. It\nis well known that the VC dimension of SVMs can be very large or unbounded,\neven though they generally yield state-of-the-art learning performance. In this\npaper, we show how to learn a hyperplane regressor by minimizing an exact, or\n\\boldmath{$\\Theta$} bound on its VC dimension. The proposed approach, termed as\nthe Minimal Complexity Machine (MCM) Regressor, involves solving a simple\nlinear programming problem. Experimental results show, that on a number of\nbenchmark datasets, the proposed approach yields regressors with error rates\nmuch less than those obtained with conventional SVM regresssors, while often\nusing fewer support vectors. On some benchmark datasets, the number of support\nvectors is less than one tenth the number used by SVMs, indicating that the MCM\ndoes indeed learn simpler representations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 20:04:49 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Jayadeva", "", ""], ["Chandra", "Suresh", ""], ["Sabharwal", "Siddarth", ""], ["Batra", "Sanjit S.", ""]]}, {"id": "1410.4599", "submitter": "Erte Pan", "authors": "Erte Pan and Zhu Han", "title": "Non-parametric Bayesian Learning with Deep Learning Structure and Its\n  Applications in Wireless Networks", "comments": "5 pages, 2 figures and 1 algorithm list", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an infinite hierarchical non-parametric Bayesian\nmodel to extract the hidden factors over observed data, where the number of\nhidden factors for each layer is unknown and can be potentially infinite.\nMoreover, the number of layers can also be infinite. We construct the model\nstructure that allows continuous values for the hidden factors and weights,\nwhich makes the model suitable for various applications. We use the\nMetropolis-Hastings method to infer the model structure. Then the performance\nof the algorithm is evaluated by the experiments. Simulation results show that\nthe model fits the underlying structure of simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 22:29:12 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 21:55:30 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Pan", "Erte", ""], ["Han", "Zhu", ""]]}, {"id": "1410.4604", "submitter": "Marlos C. Machado", "authors": "Marlos C. Machado, Sriram Srinivasan and Michael Bowling", "title": "Domain-Independent Optimistic Initialization for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Reinforcement Learning (RL), it is common to use optimistic initialization\nof value functions to encourage exploration. However, such an approach\ngenerally depends on the domain, viz., the scale of the rewards must be known,\nand the feature representation must have a constant norm. We present a simple\napproach that performs optimistic initialization with less dependence on the\ndomain.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 23:30:08 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Machado", "Marlos C.", ""], ["Srinivasan", "Sriram", ""], ["Bowling", "Michael", ""]]}, {"id": "1410.4615", "submitter": "Wojciech Zaremba", "authors": "Wojciech Zaremba, Ilya Sutskever", "title": "Learning to Execute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are\nwidely used because they are expressive and are easy to train. Our interest\nlies in empirically evaluating the expressiveness and the learnability of LSTMs\nin the sequence-to-sequence regime by training them to evaluate short computer\nprograms, a domain that has traditionally been seen as too complex for neural\nnetworks. We consider a simple class of programs that can be evaluated with a\nsingle left-to-right pass using constant memory. Our main result is that LSTMs\ncan learn to map the character-level representations of such programs to their\ncorrect outputs. Notably, it was necessary to use curriculum learning, and\nwhile conventional curriculum learning proved ineffective, we developed a new\nvariant of curriculum learning that improved our networks' performance in all\nexperimental conditions. The improved curriculum had a dramatic impact on an\naddition problem, making it possible to train an LSTM to add two 9-digit\nnumbers with 99% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 01:35:12 GMT"}, {"version": "v2", "created": "Sun, 21 Dec 2014 03:46:49 GMT"}, {"version": "v3", "created": "Thu, 19 Feb 2015 15:33:35 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zaremba", "Wojciech", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1410.4673", "submitter": "Zhiding Yu", "authors": "Weiyang Liu, Zhiding Yu, Lijia Lu, Yandong Wen, Hui Li and Yuexian Zou", "title": "KCRC-LCD: Discriminative Kernel Collaborative Representation with\n  Locality Constrained Dictionary for Visual Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the image classification problem via kernel collaborative\nrepresentation classification with locality constrained dictionary (KCRC-LCD).\nSpecifically, we propose a kernel collaborative representation classification\n(KCRC) approach in which kernel method is used to improve the discrimination\nability of collaborative representation classification (CRC). We then measure\nthe similarities between the query and atoms in the global dictionary in order\nto construct a locality constrained dictionary (LCD) for KCRC. In addition, we\ndiscuss several similarity measure approaches in LCD and further present a\nsimple yet effective unified similarity measure whose superiority is validated\nin experiments. There are several appealing aspects associated with LCD. First,\nLCD can be nicely incorporated under the framework of KCRC. The LCD similarity\nmeasure can be kernelized under KCRC, which theoretically links CRC and LCD\nunder the kernel method. Second, KCRC-LCD becomes more scalable to both the\ntraining set size and the feature dimension. Example shows that KCRC is able to\nperfectly classify data with certain distribution, while conventional CRC fails\ncompletely. Comprehensive experiments on many public datasets also show that\nKCRC-LCD is a robust discriminative classifier with both excellent performance\nand good scalability, being comparable or outperforming many other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 09:40:20 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Liu", "Weiyang", ""], ["Yu", "Zhiding", ""], ["Lu", "Lijia", ""], ["Wen", "Yandong", ""], ["Li", "Hui", ""], ["Zou", "Yuexian", ""]]}, {"id": "1410.4744", "submitter": "Martin Takac", "authors": "Jakub Kone\\v{c}n\\'y, Jie Liu, Peter Richt\\'arik, Martin Tak\\'a\\v{c}", "title": "mS2GD: Mini-Batch Semi-Stochastic Gradient Descent in the Proximal\n  Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mini-batching scheme for improving the theoretical complexity\nand practical performance of semi-stochastic gradient descent applied to the\nproblem of minimizing a strongly convex composite function represented as the\nsum of an average of a large number of smooth convex functions, and simple\nnonsmooth convex function. Our method first performs a deterministic step\n(computation of the gradient of the objective function at the starting point),\nfollowed by a large number of stochastic steps. The process is repeated a few\ntimes with the last iterate becoming the new starting point. The novelty of our\nmethod is in introduction of mini-batching into the computation of stochastic\nsteps. In each step, instead of choosing a single function, we sample $b$\nfunctions, compute their gradients, and compute the direction based on this. We\nanalyze the complexity of the method and show that the method benefits from two\nspeedup effects. First, we prove that as long as $b$ is below a certain\nthreshold, we can reach predefined accuracy with less overall work than without\nmini-batching. Second, our mini-batching scheme admits a simple parallel\nimplementation, and hence is suitable for further acceleration by\nparallelization.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 14:43:43 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Liu", "Jie", ""], ["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1410.4777", "submitter": "Michael Smith", "authors": "Richard G. Morris and Tony Martinez and Michael R. Smith", "title": "A Hierarchical Multi-Output Nearest Neighbor Model for Multi-Output\n  Dependence Learning", "comments": "10 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Output Dependence (MOD) learning is a generalization of standard\nclassification problems that allows for multiple outputs that are dependent on\neach other. A primary issue that arises in the context of MOD learning is that\nfor any given input pattern there can be multiple correct output patterns. This\nchanges the learning task from function approximation to relation\napproximation. Previous algorithms do not consider this problem, and thus\ncannot be readily applied to MOD problems. To perform MOD learning, we\nintroduce the Hierarchical Multi-Output Nearest Neighbor model (HMONN) that\nemploys a basic learning model for each output and a modified nearest neighbor\napproach to refine the initial results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 15:55:46 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Morris", "Richard G.", ""], ["Martinez", "Tony", ""], ["Smith", "Michael R.", ""]]}, {"id": "1410.4828", "submitter": "Yao-Liang Yu", "authors": "Yaoliang Yu, Xinhua Zhang, and Dale Schuurmans", "title": "Generalized Conditional Gradient for Sparse Estimation", "comments": "67 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured sparsity is an important modeling tool that expands the\napplicability of convex formulations for data analysis, however it also creates\nsignificant challenges for efficient algorithm design. In this paper we\ninvestigate the generalized conditional gradient (GCG) algorithm for solving\nstructured sparse optimization problems---demonstrating that, with some\nenhancements, it can provide a more efficient alternative to current state of\nthe art approaches. After providing a comprehensive overview of the convergence\nproperties of GCG, we develop efficient methods for evaluating polar operators,\na subroutine that is required in each GCG iteration. In particular, we show how\nthe polar operator can be efficiently evaluated in two important scenarios:\ndictionary learning and structured sparse estimation. A further improvement is\nachieved by interleaving GCG with fixed-rank local subspace optimization. A\nseries of experiments on matrix completion, multi-class classification,\nmulti-view dictionary learning and overlapping group lasso shows that the\nproposed method can significantly reduce the training cost of current\nalternatives.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 19:19:29 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Yu", "Yaoliang", ""], ["Zhang", "Xinhua", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1410.4984", "submitter": "Zhenwen Dai", "authors": "Zhenwen Dai, Andreas Damianou, James Hensman, Neil Lawrence", "title": "Gaussian Process Models with Parallelization and GPU acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an extension of Gaussian process (GP) models with\nsophisticated parallelization and GPU acceleration. The parallelization scheme\narises naturally from the modular computational structure w.r.t. datapoints in\nthe sparse Gaussian process formulation. Additionally, the computational\nbottleneck is implemented with GPU acceleration for further speed up. Combining\nboth techniques allows applying Gaussian process models to millions of\ndatapoints. The efficiency of our algorithm is demonstrated with a synthetic\ndataset. Its source code has been integrated into our popular software library\nGPy.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 18:12:57 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Dai", "Zhenwen", ""], ["Damianou", "Andreas", ""], ["Hensman", "James", ""], ["Lawrence", "Neil", ""]]}, {"id": "1410.5102", "submitter": "Diego Didona Mr", "authors": "Diego Didona and Paolo Romano", "title": "On Bootstrapping Machine Learning Performance Predictors via Analytical\n  Models", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance modeling typically relies on two antithetic methodologies: white\nbox models, which exploit knowledge on system's internals and capture its\ndynamics using analytical approaches, and black box techniques, which infer\nrelations among the input and output variables of a system based on the\nevidences gathered during an initial training phase. In this paper we\ninvestigate a technique, which we name Bootstrapping, which aims at reconciling\nthese two methodologies and at compensating the cons of the one with the pros\nof the other. We thoroughly analyze the design space of this gray box modeling\ntechnique, and identify a number of algorithmic and parametric trade-offs which\nwe evaluate via two realistic case studies, a Key-Value Store and a Total Order\nBroadcast service.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 18:32:37 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Didona", "Diego", ""], ["Romano", "Paolo", ""]]}, {"id": "1410.5137", "submitter": "Purushottam Kar", "authors": "Prateek Jain, Ambuj Tewari, Purushottam Kar", "title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation", "comments": "20 pages, 3 figures, To appear in the proceedings of the 28th Annual\n  Conference on Neural Information Processing Systems, NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of M-estimators in generalized linear regression models in high\ndimensional settings requires risk minimization with hard $L_0$ constraints. Of\nthe known methods, the class of projected gradient descent (also known as\niterative hard thresholding (IHT)) methods is known to offer the fastest and\nmost scalable solutions. However, the current state-of-the-art is only able to\nanalyze these methods in extremely restrictive settings which do not hold in\nhigh dimensional statistical models. In this work we bridge this gap by\nproviding the first analysis for IHT-style methods in the high dimensional\nstatistical setting. Our bounds are tight and match known minimax lower bounds.\nOur results rely on a general analysis framework that enables us to analyze\nseveral popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in\nthe high dimensional regression setting. We also extend our analysis to a large\nfamily of \"fully corrective methods\" that includes two-stage and partial\nhard-thresholding algorithms. We show that our results hold for the problem of\nsparse regression, as well as low-rank matrix recovery.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 02:29:27 GMT"}, {"version": "v2", "created": "Tue, 21 Oct 2014 08:45:56 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Jain", "Prateek", ""], ["Tewari", "Ambuj", ""], ["Kar", "Purushottam", ""]]}, {"id": "1410.5329", "submitter": "Sebastian Raschka SR", "authors": "Sebastian Raschka", "title": "Naive Bayes and Text Classification I - Introduction and Theory", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naive Bayes classifiers, a family of classifiers that are based on the\npopular Bayes' probability theorem, are known for creating simple yet well\nperforming models, especially in the fields of document classification and\ndisease prediction. In this article, we will look at the main concepts of naive\nBayes classification in the context of document categorization.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 22:11:34 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 12:34:18 GMT"}, {"version": "v3", "created": "Tue, 19 May 2015 00:40:00 GMT"}, {"version": "v4", "created": "Tue, 14 Feb 2017 19:14:01 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Raschka", "Sebastian", ""]]}, {"id": "1410.5330", "submitter": "Sebastian Raschka SR", "authors": "Sebastian Raschka", "title": "An Overview of General Performance Metrics of Binary Classifier Systems", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document provides a brief overview of different metrics and terminology\nthat is used to measure the performance of binary classification systems.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 00:50:42 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Raschka", "Sebastian", ""]]}, {"id": "1410.5392", "submitter": "Yu Cheng", "authors": "Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng and Shang-Hua Teng", "title": "Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling\n  for Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a sampling problem basic to computational statistical inference,\nwe develop a nearly optimal algorithm for a fundamental problem in spectral\ngraph theory and numerical analysis. Given an $n\\times n$ SDDM matrix ${\\bf\n\\mathbf{M}}$, and a constant $-1 \\leq p \\leq 1$, our algorithm gives efficient\naccess to a sparse $n\\times n$ linear operator $\\tilde{\\mathbf{C}}$ such that\n$${\\mathbf{M}}^{p} \\approx \\tilde{\\mathbf{C}} \\tilde{\\mathbf{C}}^\\top.$$ The\nsolution is based on factoring ${\\bf \\mathbf{M}}$ into a product of simple and\nsparse matrices using squaring and spectral sparsification. For ${\\mathbf{M}}$\nwith $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and\npolylogarithmic depth on a parallel machine with $m$ processors. This gives the\nfirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.\nrandom univariate Gaussian samples to generate i.i.d. random samples for\n$n$-dimensional Gaussian random fields with SDDM precision matrices. For\nsampling this natural subclass of Gaussian random fields, it is optimal in the\nrandomness and nearly optimal in the work and parallel complexity. In addition,\nour sampling algorithm can be directly extended to Gaussian random fields with\nSDD precision matrices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 18:59:58 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Cheng", "Dehua", ""], ["Cheng", "Yu", ""], ["Liu", "Yan", ""], ["Peng", "Richard", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1410.5410", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner\n  Product Search (MIPS)", "comments": "arXiv admin note: text overlap with arXiv:1405.5869", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it was shown that the problem of Maximum Inner Product Search (MIPS)\nis efficient and it admits provably sub-linear hashing algorithms. Asymmetric\ntransformations before hashing were the key in solving MIPS which was otherwise\nhard. In the prior work, the authors use asymmetric transformations which\nconvert the problem of approximate MIPS into the problem of approximate near\nneighbor search which can be efficiently solved using hashing. In this work, we\nprovide a different transformation which converts the problem of approximate\nMIPS into the problem of approximate cosine similarity search which can be\nefficiently solved using signed random projections. Theoretical analysis show\nthat the new scheme is significantly better than the original scheme for MIPS.\nExperimental evaluations strongly support the theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 19:54:58 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 20:48:36 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1410.5467", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk, Lionel Mamane, Josef Urban", "title": "Machine Learning of Coq Proof Guidance: First Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the results of the first experiments with learning proof\ndependencies from the formalizations done with the Coq system. We explain the\nprocess of obtaining the dependencies from the Coq proofs, the characterization\nof formulas that is used for the learning, and the evaluation method. Various\nmachine learning methods are compared on a dataset of 5021 toplevel Coq proofs\ncoming from the CoRN repository. The best resulting method covers on average\n75% of the needed proof dependencies among the first 100 predictions, which is\na comparable performance of such initial experiments on other large-theory\ncorpora.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 21:16:52 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Mamane", "Lionel", ""], ["Urban", "Josef", ""]]}, {"id": "1410.5473", "submitter": "Chang Liu", "authors": "Chang Liu and Yi Xu", "title": "Feature Selection Based on Confidence Machine", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning and pattern recognition, feature selection has been a hot\ntopic in the literature. Unsupervised feature selection is challenging due to\nthe loss of labels which would supply the related information.How to define an\nappropriate metric is the key for feature selection. We propose a filter method\nfor unsupervised feature selection which is based on the Confidence Machine.\nConfidence Machine offers an estimation of confidence on a feature'reliability.\nIn this paper, we provide the math model of Confidence Machine in the context\nof feature selection, which maximizes the relevance and minimizes the\nredundancy of the selected feature. We compare our method against classic\nfeature selection methods Laplacian Score, Pearson Correlation and Principal\nComponent Analysis on benchmark data sets. The experimental results demonstrate\nthe efficiency and effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 21:32:05 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 16:58:03 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Liu", "Chang", ""], ["Xu", "Yi", ""]]}, {"id": "1410.5491", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Chris Callison-Burch", "title": "Using Mechanical Turk to Build Machine Translation Evaluation Sets", "comments": "4 pages, 2 tables; appeared in Proceedings of the NAACL HLT 2010\n  Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk,\n  June 2010", "journal-ref": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech\n  and Language Data with Amazon's Mechanical Turk, pages 208-211, Los Angeles,\n  California, June 2010. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building machine translation (MT) test sets is a relatively expensive task.\nAs MT becomes increasingly desired for more and more language pairs and more\nand more domains, it becomes necessary to build test sets for each case. In\nthis paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT\ntest sets cheaply. We find that MTurk can be used to make test sets much\ncheaper than professionally-produced test sets. More importantly, in\nexperiments with multiple MT systems, we find that the MTurk-produced test sets\nyield essentially the same conclusions regarding system performance as the\nprofessionally-produced test sets yield.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 22:28:55 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Bloodgood", "Michael", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "1410.5518", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Nathan Srebro", "title": "On Symmetric and Asymmetric LSHs for Inner Product Search", "comments": "11 pages, 3 figures, In Proceedings of The 32nd International\n  Conference on Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing locality sensitive hashes (LSH) for\ninner product similarity, and of the power of asymmetric hashes in this\ncontext. Shrivastava and Li argue that there is no symmetric LSH for the\nproblem and propose an asymmetric LSH based on different mappings for query and\ndatabase points. However, we show there does exist a simple symmetric LSH that\nenjoys stronger guarantees and better empirical performance than the asymmetric\nLSH they suggest. We also show a variant of the settings where asymmetry is\nin-fact needed, but there a different asymmetric LSH is required.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 02:00:34 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2014 21:31:06 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 19:30:35 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Srebro", "Nathan", ""]]}, {"id": "1410.5557", "submitter": "Matthias Rolf", "authors": "Matthias Rolf and Minoru Asada", "title": "Where do goals come from? A Generic Approach to Autonomous Goal-System\n  Development", "comments": "Draft submitted to IEEE Transactions on Autonomous Mental Development\n  (TAMD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goals express agents' intentions and allow them to organize their behavior\nbased on low-dimensional abstractions of high-dimensional world states. How can\nagents develop such goals autonomously? This paper proposes a detailed\nconceptual and computational account to this longstanding problem. We argue to\nconsider goals as high-level abstractions of lower-level intention mechanisms\nsuch as rewards and values, and point out that goals need to be considered\nalongside with a detection of the own actions' effects. We propose Latent Goal\nAnalysis as a computational learning formulation thereof, and show\nconstructively that any reward or value function can by explained by goals and\nsuch self-detection as latent mechanisms. We first show that learned goals\nprovide a highly effective dimensionality reduction in a practical\nreinforcement learning problem. Then, we investigate a developmental scenario\nin which entirely task-unspecific rewards induced by visual saliency lead to\nself and goal representations that constitute goal-directed reaching.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 07:24:03 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Rolf", "Matthias", ""], ["Asada", "Minoru", ""]]}, {"id": "1410.5684", "submitter": "Saahil Ognawala", "authors": "Saahil Ognawala and Justin Bayer", "title": "Regularizing Recurrent Networks - On Injected Noise and Norm-based\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in parallel processing have lead to a surge in multilayer\nperceptrons' (MLP) applications and deep learning in the past decades.\nRecurrent Neural Networks (RNNs) give additional representational power to\nfeedforward MLPs by providing a way to treat sequential data. However, RNNs are\nhard to train using conventional error backpropagation methods because of the\ndifficulty in relating inputs over many time-steps. Regularization approaches\nfrom MLP sphere, like dropout and noisy weight training, have been\ninsufficiently applied and tested on simple RNNs. Moreover, solutions have been\nproposed to improve convergence in RNNs but not enough to improve the long term\ndependency remembering capabilities thereof.\n  In this study, we aim to empirically evaluate the remembering and\ngeneralization ability of RNNs on polyphonic musical datasets. The models are\ntrained with injected noise, random dropout, norm-based regularizers and their\nrespective performances compared to well-initialized plain RNNs and advanced\nregularization methods like fast-dropout. We conclude with evidence that\ntraining with noise does not improve performance as conjectured by a few works\nin RNN optimization before ours.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 14:36:26 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Ognawala", "Saahil", ""], ["Bayer", "Justin", ""]]}, {"id": "1410.5703", "submitter": "Yaron Velner", "authors": "Yaron Velner", "title": "Robust Multidimensional Mean-Payoff Games are Undecidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean-payoff games play a central role in quantitative synthesis and\nverification. In a single-dimensional game a weight is assigned to every\ntransition and the objective of the protagonist is to assure a non-negative\nlimit-average weight. In the multidimensional setting, a weight vector is\nassigned to every transition and the objective of the protagonist is to satisfy\na boolean condition over the limit-average weight of each dimension, e.g.,\n$\\LimAvg(x_1) \\leq 0 \\vee \\LimAvg(x_2)\\geq 0 \\wedge \\LimAvg(x_3) \\geq 0$. We\nrecently proved that when one of the players is restricted to finite-memory\nstrategies then the decidability of determining the winner is inter-reducible\nwith Hilbert's Tenth problem over rationals (a fundamental long-standing open\nproblem). In this work we allow arbitrary (infinite-memory) strategies for both\nplayers and we show that the problem is undecidable.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 19:57:42 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Velner", "Yaron", ""]]}, {"id": "1410.5784", "submitter": "Amartya Hatua", "authors": "Amartya Hatua", "title": "Optimal Feature Selection from VMware ESXi 5.1 Feature Set", "comments": "8 Pagee, http://airccse.org/journal/ijccms/current2014.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A study of VMware ESXi 5.1 server has been carried out to find the optimal\nset of parameters which suggest usage of different resources of the server.\nFeature selection algorithms have been used to extract the optimum set of\nparameters of the data obtained from VMware ESXi 5.1 server using esxtop\ncommand. Multiple virtual machines (VMs) are running in the mentioned server.\nK-means algorithm is used for clustering the VMs. The goodness of each cluster\nis determined by Davies Bouldin index and Dunn index respectively. The best\ncluster is further identified by the determined indices. The features of the\nbest cluster are considered into a set of optimal parameters.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 05:00:31 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Hatua", "Amartya", ""]]}, {"id": "1410.5801", "submitter": "Saptarshi Das", "authors": "Valentina Bono, Wasifa Jamal, Saptarshi Das, Koushik Maharatna", "title": "Artifact reduction in multichannel pervasive EEG using hybrid WPT-ICA\n  and WPT-EMD signal decomposition techniques", "comments": "5 pages, 6 figures", "journal-ref": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE\n  International Conference on, pp. 5864 - 5868, May 2014", "doi": "10.1109/ICASSP.2014.6854728", "report-no": null, "categories": "physics.med-ph cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to reduce the muscle artifacts in multi-channel pervasive\nElectroencephalogram (EEG) signals, we here propose and compare two hybrid\nalgorithms by combining the concept of wavelet packet transform (WPT),\nempirical mode decomposition (EMD) and Independent Component Analysis (ICA).\nThe signal cleaning performances of WPT-EMD and WPT-ICA algorithms have been\ncompared using a signal-to-noise ratio (SNR)-like criterion for artifacts. The\nalgorithms have been tested on multiple trials of four different artifact cases\nviz. eye-blinking and muscle artifacts including left and right hand movement\nand head-shaking.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 16:57:17 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Bono", "Valentina", ""], ["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""]]}, {"id": "1410.5816", "submitter": "Andrey Bogomolov", "authors": "Andrey Bogomolov, Bruno Lepri, Michela Ferron, Fabio Pianesi, Alex\n  (Sandy) Pentland", "title": "Daily Stress Recognition from Mobile Phone Data, Weather Conditions and\n  Individual Traits", "comments": "ACM Multimedia 2014, November 3-7, 2014, Orlando, Florida, USA", "journal-ref": null, "doi": "10.1145/2647868.2654933", "report-no": null, "categories": "cs.CY cs.LG physics.data-an stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has proven that stress reduces quality of life and causes many\ndiseases. For this reason, several researchers devised stress detection systems\nbased on physiological parameters. However, these systems require that\nobtrusive sensors are continuously carried by the user. In our paper, we\npropose an alternative approach providing evidence that daily stress can be\nreliably recognized based on behavioral metrics, derived from the user's mobile\nphone activity and from additional indicators, such as the weather conditions\n(data pertaining to transitory properties of the environment) and the\npersonality traits (data concerning permanent dispositions of individuals). Our\nmultifactorial statistical model, which is person-independent, obtains the\naccuracy score of 72.28% for a 2-class daily stress recognition problem. The\nmodel is efficient to implement for most of multimedia applications due to\nhighly reduced low-dimensional feature space (32d). Moreover, we identify and\ndiscuss the indicators which have strong predictive power.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 18:54:53 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Bogomolov", "Andrey", "", "Sandy"], ["Lepri", "Bruno", "", "Sandy"], ["Ferron", "Michela", "", "Sandy"], ["Pianesi", "Fabio", "", "Sandy"], ["Alex", "", "", "Sandy"], ["Pentland", "", ""]]}, {"id": "1410.5877", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Chris Callison-Burch", "title": "Bucking the Trend: Large-Scale Cost-Focused Active Learning for\n  Statistical Machine Translation", "comments": "11 pages, 14 figures; appeared in Proceedings of the 48th Annual\n  Meeting of the Association for Computational Linguistics, July 2010", "journal-ref": "In Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics, pages 854-864, Uppsala, Sweden, July 2010.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how to improve machine translation systems by adding more\ntranslation data in situations where we already have substantial resources. The\nmain challenge is how to buck the trend of diminishing returns that is commonly\nencountered. We present an active learning-style data solicitation algorithm to\nmeet this challenge. We test it, gathering annotations via Amazon Mechanical\nTurk, and find that we get an order of magnitude increase in performance rates\nof improvement.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 22:55:48 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Bloodgood", "Michael", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "1410.5884", "submitter": "Yujia Li", "authors": "Yujia Li and Richard Zemel", "title": "Mean-Field Networks", "comments": "Published in ICML 2014 workshop on Learning Tractable Probabilistic\n  Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The mean field algorithm is a widely used approximate inference algorithm for\ngraphical models whose exact inference is intractable. In each iteration of\nmean field, the approximate marginals for each variable are updated by getting\ninformation from the neighbors. This process can be equivalently converted into\na feedforward network, with each layer representing one iteration of mean field\nand with tied weights on all layers. This conversion enables a few natural\nextensions, e.g. untying the weights in the network. In this paper, we study\nthese mean field networks (MFNs), and use them as inference tools as well as\ndiscriminative models. Preliminary experiment results show that MFNs can learn\nto do inference very efficiently and perform significantly better than mean\nfield as discriminative models.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 23:32:24 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Li", "Yujia", ""], ["Zemel", "Richard", ""]]}, {"id": "1410.5920", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Remi Munos", "title": "Active Regression by Stratification", "comments": null, "journal-ref": "S. Sabato and R. Munos, \"Active Regression by Stratification\",\n  Advances in Neural Information Processing Systems (NIPS) 469-477, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new active learning algorithm for parametric linear regression\nwith random design. We provide finite sample convergence guarantees for general\ndistributions in the misspecified model. This is the first active learner for\nthis setting that provably can improve over passive learning. Unlike other\nlearning settings (such as classification), in regression the passive learning\nrate of $O(1/\\epsilon)$ cannot in general be improved upon. Nonetheless, the\nso-called `constant' in the rate of convergence, which is characterized by a\ndistribution-dependent risk, can be improved in many cases. For a given\ndistribution, achieving the optimal risk requires prior knowledge of the\ndistribution. Following the stratification technique advocated in Monte-Carlo\nfunction integration, our active learner approaches the optimal risk using\npiecewise constant approximations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 06:09:58 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Sabato", "Sivan", ""], ["Munos", "Remi", ""]]}, {"id": "1410.6093", "submitter": "Osman  G\\\"unay", "authors": "Osman Gunay, Cem Emre Akbas, A. Enis Cetin", "title": "Cosine Similarity Measure According to a Convex Cost Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a new vector similarity measure associated with a\nconvex cost function. Given two vectors, we determine the surface normals of\nthe convex function at the vectors. The angle between the two surface normals\nis the similarity measure. Convex cost function can be the negative entropy\nfunction, total variation (TV) function and filtered variation function. The\nconvex cost function need not be differentiable everywhere. In general, we need\nto compute the gradient of the cost function to compute the surface normals. If\nthe gradient does not exist at a given vector, it is possible to use the\nsubgradients and the normal producing the smallest angle between the two\nvectors is used to compute the similarity measure.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 16:13:36 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Gunay", "Osman", ""], ["Akbas", "Cem Emre", ""], ["Cetin", "A. Enis", ""]]}, {"id": "1410.6095", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos, Georgios B. Giannakis, and Ross Baldick", "title": "Online Energy Price Matrix Factorization for Power Grid Topology\n  Tracking", "comments": "Submitted to the IEEE Trans. on Smart Grid", "journal-ref": null, "doi": "10.1109/TSG.2015.2469098", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid security and open markets are two major smart grid goals. Transparency\nof market data facilitates a competitive and efficient energy environment, yet\nit may also reveal critical physical system information. Recovering the grid\ntopology based solely on publicly available market data is explored here.\nReal-time energy prices are calculated as the Lagrange multipliers of\nnetwork-constrained economic dispatch; that is, via a linear program (LP)\ntypically solved every 5 minutes. Granted the grid Laplacian is a parameter of\nthis LP, one could infer such a topology-revealing matrix upon observing\nsuccessive LP dual outcomes. The matrix of spatio-temporal prices is first\nshown to factor as the product of the inverse Laplacian times a sparse matrix.\nLeveraging results from sparse matrix decompositions, topology recovery schemes\nwith complementary strengths are subsequently formulated. Solvers scalable to\nhigh-dimensional and streaming market data are devised. Numerical validation\nusing real load data on the IEEE 30-bus grid provide useful input for current\nand future market designs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 16:14:38 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""], ["Baldick", "Ross", ""]]}, {"id": "1410.6382", "submitter": "Doron Kukliansky", "authors": "Doron Kukliansky, Ohad Shamir", "title": "Attribute Efficient Linear Regression with Data-Dependent Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze a budgeted learning setting, in which the learner\ncan only choose and observe a small subset of the attributes of each training\nexample. We develop efficient algorithms for ridge and lasso linear regression,\nwhich utilize the geometry of the data by a novel data-dependent sampling\nscheme. When the learner has prior knowledge on the second moments of the\nattributes, the optimal sampling probabilities can be calculated precisely, and\nresult in data-dependent improvements factors for the excess risk over the\nstate-of-the-art that may be as large as $O(\\sqrt{d})$, where $d$ is the\nproblem's dimension. Moreover, under reasonable assumptions our algorithms can\nuse less attributes than full-information algorithms, which is the main concern\nin budgeted learning settings. To the best of our knowledge, these are the\nfirst algorithms able to do so in our setting. Where no such prior knowledge is\navailable, we develop a simple estimation technique that given a sufficient\namount of training examples, achieves similar improvements. We complement our\ntheoretical analysis with experiments on several data sets which support our\nclaims.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 14:55:09 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Kukliansky", "Doron", ""], ["Shamir", "Ohad", ""]]}, {"id": "1410.6387", "submitter": "Yossi Arjevani", "authors": "Yossi Arjevani", "title": "On Lower and Upper Bounds in Smooth Strongly Convex Optimization - A\n  Unified Approach via Linear Iterative Methods", "comments": "A related paper co-authored with Shai Shalev-Shwartz and Ohad Shamir\n  is to be published soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis we develop a novel framework to study smooth and strongly\nconvex optimization algorithms, both deterministic and stochastic. Focusing on\nquadratic functions we are able to examine optimization algorithms as a\nrecursive application of linear operators. This, in turn, reveals a powerful\nconnection between a class of optimization algorithms and the analytic theory\nof polynomials whereby new lower and upper bounds are derived. In particular,\nwe present a new and natural derivation of Nesterov's well-known Accelerated\nGradient Descent method by employing simple 'economic' polynomials. This rather\nnatural interpretation of AGD contrasts with earlier ones which lacked a\nsimple, yet solid, motivation. Lastly, whereas existing lower bounds are only\nvalid when the dimensionality scales with the number of iterations, our lower\nbound holds in the natural regime where the dimensionality is fixed.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 15:05:44 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Arjevani", "Yossi", ""]]}, {"id": "1410.6414", "submitter": "Jingbo Shang", "authors": "Jingbo Shang, Tianqi Chen, Hang Li, Zhengdong Lu, Yong Yu", "title": "A Parallel and Efficient Algorithm for Learning to Match", "comments": "10 pages, short version was published in ICDM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in data mining and related fields can be formalized as matching\nbetween objects in two heterogeneous domains, including collaborative\nfiltering, link prediction, image tagging, and web search. Machine learning\ntechniques, referred to as learning-to-match in this paper, have been\nsuccessfully applied to the problems. Among them, a class of state-of-the-art\nmethods, named feature-based matrix factorization, formalize the task as an\nextension to matrix factorization by incorporating auxiliary features into the\nmodel. Unfortunately, making those algorithms scale to real world problems is\nchallenging, and simple parallelization strategies fail due to the complex\ncross talking patterns between sub-tasks. In this paper, we tackle this\nchallenge with a novel parallel and efficient algorithm for feature-based\nmatrix factorization. Our algorithm, based on coordinate descent, can easily\nhandle hundreds of millions of instances and features on a single machine. The\nkey recipe of this algorithm is an iterative relaxation of the objective to\nfacilitate parallel updates of parameters, with guaranteed convergence on\nminimizing the original objective function. Experimental results demonstrate\nthat the proposed method is effective on a wide range of matching problems,\nwith efficiency significantly improved upon the baselines while accuracy\nretained unchanged.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 01:04:00 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Shang", "Jingbo", ""], ["Chen", "Tianqi", ""], ["Li", "Hang", ""], ["Lu", "Zhengdong", ""], ["Yu", "Yong", ""]]}, {"id": "1410.6466", "submitter": "Dehua Cheng", "authors": "Dehua Cheng, Xinran He, Yan Liu", "title": "Model Selection for Topic Models via Spectral Decomposition", "comments": "accepted in AISTATS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have achieved significant successes in analyzing large-scale\ntext corpus. In practical applications, we are always confronted with the\nchallenge of model selection, i.e., how to appropriately set the number of\ntopics. Following recent advances in topic model inference via tensor\ndecomposition, we make a first attempt to provide theoretical analysis on model\nselection in latent Dirichlet allocation. Under mild conditions, we derive the\nupper bound and lower bound on the number of topics given a text collection of\nfinite size. Experimental results demonstrate that our bounds are accurate and\ntight. Furthermore, using Gaussian mixture model as an example, we show that\nour methodology can be easily generalized to model selection analysis for other\nlatent models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 19:38:44 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 01:39:14 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Cheng", "Dehua", ""], ["He", "Xinran", ""], ["Liu", "Yan", ""]]}, {"id": "1410.6776", "submitter": "Purushottam Kar", "authors": "Purushottam Kar, Harikrishna Narasimhan, Prateek Jain", "title": "Online and Stochastic Gradient Methods for Non-decomposable Loss\n  Functions", "comments": "25 pages, 3 figures, To appear in the proceedings of the 28th Annual\n  Conference on Neural Information Processing Systems, NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications in sensitive domains such as biometrics and medicine\nfrequently require the use of non-decomposable loss functions such as\nprecision@k, F-measure etc. Compared to point loss functions such as\nhinge-loss, these offer much more fine grained control over prediction, but at\nthe same time present novel challenges in terms of algorithm design and\nanalysis. In this work we initiate a study of online learning techniques for\nsuch non-decomposable loss functions with an aim to enable incremental learning\nas well as design scalable solvers for batch problems. To this end, we propose\nan online learning framework for such loss functions. Our model enjoys several\nnice properties, chief amongst them being the existence of efficient online\nlearning algorithms with sublinear regret and online to batch conversion\nbounds. Our model is a provable extension of existing online learning models\nfor point loss functions. We instantiate two popular losses, prec@k and pAUC,\nin our model and prove sublinear regret bounds for both of them. Our proofs\nrequire a novel structural lemma over ranked lists which may be of independent\ninterest. We then develop scalable stochastic gradient descent solvers for\nnon-decomposable loss functions. We show that for a large family of loss\nfunctions satisfying a certain uniform convergence property (that includes\nprec@k, pAUC, and F-measure), our methods provably converge to the empirical\nrisk minimizer. Such uniform convergence results were not known for these\nlosses and we establish these using novel proof techniques. We then use\nextensive experimentation on real life and benchmark datasets to establish that\nour method can be orders of magnitude faster than a recently proposed cutting\nplane method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 18:45:23 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Kar", "Purushottam", ""], ["Narasimhan", "Harikrishna", ""], ["Jain", "Prateek", ""]]}, {"id": "1410.6801", "submitter": "Christopher Musco", "authors": "Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco,\n  Madalina Persu", "title": "Dimensionality Reduction for k-Means Clustering and Low Rank\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to approximate a data matrix $\\mathbf{A}$ with a much smaller\nsketch $\\mathbf{\\tilde A}$ that can be used to solve a general class of\nconstrained k-rank approximation problems to within $(1+\\epsilon)$ error.\nImportantly, this class of problems includes $k$-means clustering and\nunconstrained low rank approximation (i.e. principal component analysis). By\nreducing data points to just $O(k)$ dimensions, our methods generically\naccelerate any exact, approximate, or heuristic algorithm for these ubiquitous\nproblems.\n  For $k$-means dimensionality reduction, we provide $(1+\\epsilon)$ relative\nerror results for many common sketching techniques, including random row\nprojection, column selection, and approximate SVD. For approximate principal\ncomponent analysis, we give a simple alternative to known algorithms that has\napplications in the streaming setting. Additionally, we extend recent work on\ncolumn-based matrix reconstruction, giving column subsets that not only `cover'\na good subspace for $\\bv{A}$, but can be used directly to compute this\nsubspace.\n  Finally, for $k$-means clustering, we show how to achieve a $(9+\\epsilon)$\napproximation by Johnson-Lindenstrauss projecting data points to just $O(\\log\nk/\\epsilon^2)$ dimensions. This gives the first result that leverages the\nspecific structure of $k$-means to achieve dimension independent of input size\nand sublinear in $k$.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 19:43:16 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 03:42:52 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2015 02:33:24 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Cohen", "Michael B.", ""], ["Elder", "Sam", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Persu", "Madalina", ""]]}, {"id": "1410.6830", "submitter": "Isik Baris Fidaner", "authors": "I\\c{s}{\\i}k Bar{\\i}\\c{s} Fidaner, Ali Taylan Cemgil", "title": "Clustering Words by Projection Entropy", "comments": "Accepted to NIPS 2014 Modern ML+NLP Workshop:\n  http://www.cs.cmu.edu/~apparikh/nips2014ml-nlp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We apply entropy agglomeration (EA), a recently introduced algorithm, to\ncluster the words of a literary text. EA is a greedy agglomerative procedure\nthat minimizes projection entropy (PE), a function that can quantify the\nsegmentedness of an element set. To apply it, the text is reduced to a feature\nallocation, a combinatorial object to represent the word occurences in the\ntext's paragraphs. The experiment results demonstrate that EA, despite its\nreduction and simplicity, is useful in capturing significant relationships\namong the words in the text. This procedure was implemented in Python and\npublished as a free software: REBUS.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 20:34:01 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Fidaner", "I\u015f\u0131k Bar\u0131\u015f", ""], ["Cemgil", "Ali Taylan", ""]]}, {"id": "1410.6853", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick", "title": "Covariance Matrices for Mean Field Variational Bayes", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean Field Variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is its (sometimes severe) underestimates of\nthe uncertainty of model variables and lack of information about model variable\ncovariance. We develop a fast, general methodology for exponential families\nthat augments MFVB to deliver accurate uncertainty estimates for model\nvariables -- both for individual variables and coherently across variables.\nMFVB for exponential families defines a fixed-point equation in the means of\nthe approximating posterior, and our approach yields a covariance estimate by\nperturbing this fixed point. Inspired by linear response theory, we call our\nmethod linear response variational Bayes (LRVB). We demonstrate the accuracy of\nour method on simulated data sets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 23:31:44 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 22:07:47 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""]]}, {"id": "1410.6880", "submitter": "Seunghak Lee", "authors": "Seunghak Lee and Eric P. Xing", "title": "Screening Rules for Overlapping Group Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, to solve large-scale lasso and group lasso problems, screening\nrules have been developed, the goal of which is to reduce the problem size by\nefficiently discarding zero coefficients using simple rules independently of\nthe others. However, screening for overlapping group lasso remains an open\nchallenge because the overlaps between groups make it infeasible to test each\ngroup independently. In this paper, we develop screening rules for overlapping\ngroup lasso. To address the challenge arising from groups with overlaps, we\ntake into account overlapping groups only if they are inclusive of the group\nbeing tested, and then we derive screening rules, adopting the dual polytope\nprojection approach. This strategy allows us to screen each group independently\nof each other. In our experiments, we demonstrate the efficiency of our\nscreening rules on various datasets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 04:06:49 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Lee", "Seunghak", ""], ["Xing", "Eric P.", ""]]}, {"id": "1410.6973", "submitter": "Anna Choromanska", "authors": "Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Yann LeCun", "title": "Differentially- and non-differentially-private random decision trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider supervised learning with random decision trees, where the tree\nconstruction is completely random. The method is popularly used and works well\nin practice despite the simplicity of the setting, but its statistical\nmechanism is not yet well-understood. In this paper we provide strong\ntheoretical guarantees regarding learning with random decision trees. We\nanalyze and compare three different variants of the algorithm that have minimal\nmemory requirements: majority voting, threshold averaging and probabilistic\naveraging. The random structure of the tree enables us to adapt these methods\nto a differentially-private setting thus we also propose differentially-private\nversions of all three schemes. We give upper-bounds on the generalization error\nand mathematically explain how the accuracy depends on the number of random\ndecision trees. Furthermore, we prove that only logarithmic (in the size of the\ndataset) number of independently selected random decision trees suffice to\ncorrectly classify most of the data, even when differential-privacy guarantees\nmust be maintained. We empirically show that majority voting and threshold\naveraging give the best accuracy, also for conservative users requiring high\nprivacy guarantees. Furthermore, we demonstrate that a simple majority voting\nrule is an especially good candidate for the differentially-private classifier\nsince it is much less sensitive to the choice of forest parameters than other\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 00:16:16 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 20:48:11 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Bojarski", "Mariusz", ""], ["Choromanska", "Anna", ""], ["Choromanski", "Krzysztof", ""], ["LeCun", "Yann", ""]]}, {"id": "1410.6975", "submitter": "Anna Choromanska", "authors": "Apoorv Agarwal, Anna Choromanska, Krzysztof Choromanski", "title": "Notes on using Determinantal Point Processes for Clustering with\n  Applications to Text Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare three initialization schemes for the KMEANS\nclustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and\n3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k\nneeds to be set by the user of the algorithms. (Kang 2013) recently proposed a\nnovel use of determinantal point processes for sampling the initial centroids\nfor the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide\nany evaluation establishing that KMEANSD++ is better than other algorithms. In\nthis paper, we show that the performance of KMEANSD++ is comparable to KMEANS++\n(both of which are better than KMEANSRAND) with KMEANSD++ having an additional\nthat it can automatically approximate the value of k.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 00:59:11 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Agarwal", "Apoorv", ""], ["Choromanska", "Anna", ""], ["Choromanski", "Krzysztof", ""]]}, {"id": "1410.6990", "submitter": "Dacheng Tao", "authors": "Chang Xu, Tongliang Liu, Dacheng Tao, Chao Xu", "title": "Local Rademacher Complexity for Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the local Rademacher complexity of empirical risk minimization\n(ERM)-based multi-label learning algorithms, and in doing so propose a new\nalgorithm for multi-label learning. Rather than using the trace norm to\nregularize the multi-label predictor, we instead minimize the tail sum of the\nsingular values of the predictor in multi-label learning. Benefiting from the\nuse of the local Rademacher complexity, our algorithm, therefore, has a sharper\ngeneralization error bound and a faster convergence rate. Compared to methods\nthat minimize over all singular values, concentrating on the tail singular\nvalues results in better recovery of the low-rank structure of the multi-label\npredictor, which plays an import role in exploiting label correlations. We\npropose a new conditional singular value thresholding algorithm to solve the\nresulting objective function. Empirical studies on real-world datasets validate\nour theoretical results and demonstrate the effectiveness of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 05:52:33 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Xu", "Chang", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1410.6991", "submitter": "Trapit Bansal", "authors": "Trapit Bansal, Chiranjib Bhattacharyya, Ravindran Kannan", "title": "A provable SVD-based algorithm for learning topics in dominant admixture\n  corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents\nare drawn from admixtures of distributions over words, known as topics. The\ninference problem of recovering topics from admixtures, is NP-hard. Assuming\nseparability, a strong assumption, [4] gave the first provable algorithm for\ninference. For LDA model, [6] gave a provable algorithm using tensor-methods.\nBut [4,6] do not learn topic vectors with bounded $l_1$ error (a natural\nmeasure for probability vectors). Our aim is to develop a model which makes\nintuitive and empirically supported assumptions and to design an algorithm with\nnatural, simple components such as SVD, which provably solves the inference\nproblem for the model with bounded $l_1$ error. A topic in LDA and other models\nis essentially characterized by a group of co-occurring words. Motivated by\nthis, we introduce topic specific Catchwords, group of words which occur with\nstrictly greater frequency in a topic than any other topic individually and are\nrequired to have high frequency together rather than individually. A major\ncontribution of the paper is to show that under this more realistic assumption,\nwhich is empirically verified on real corpora, a singular value decomposition\n(SVD) based algorithm with a crucial pre-processing step of thresholding, can\nprovably recover the topics from a collection of documents drawn from Dominant\nadmixtures. Dominant admixtures are convex combination of distributions in\nwhich one distribution has a significantly higher contribution than others.\nApart from the simplicity of the algorithm, the sample complexity has near\noptimal dependence on $w_0$, the lowest probability that a topic is dominant,\nand is better than [4]. Empirical evidence shows that on several real world\ncorpora, both Catchwords and Dominant admixture assumptions hold and the\nproposed algorithm substantially outperforms the state of the art [5].\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 06:00:36 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 17:27:07 GMT"}, {"version": "v3", "created": "Tue, 4 Nov 2014 05:14:25 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Bansal", "Trapit", ""], ["Bhattacharyya", "Chiranjib", ""], ["Kannan", "Ravindran", ""]]}, {"id": "1410.7050", "submitter": "Amit Daniely", "authors": "Amit Daniely", "title": "A PTAS for Agnostically Learning Halfspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a PTAS for agnostically learning halfspaces w.r.t. the uniform\ndistribution on the $d$ dimensional sphere. Namely, we show that for every\n$\\mu>0$ there is an algorithm that runs in time\n$\\mathrm{poly}(d,\\frac{1}{\\epsilon})$, and is guaranteed to return a classifier\nwith error at most $(1+\\mu)\\mathrm{opt}+\\epsilon$, where $\\mathrm{opt}$ is the\nerror of the best halfspace classifier. This improves on Awasthi, Balcan and\nLong [ABL14] who showed an algorithm with an (unspecified) constant\napproximation ratio. Our algorithm combines the classical technique of\npolynomial regression (e.g. [LMN89, KKMS05]), together with the new\nlocalization technique of [ABL14].\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 15:41:37 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 09:59:55 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2015 06:28:49 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Daniely", "Amit", ""]]}, {"id": "1410.7057", "submitter": "Bijit Kumar Das", "authors": "Bijit Kumar Das, Mrityunjoy Chakraborty and Jer\\'onimo Arenas-Garc\\'ia", "title": "Sparse Distributed Learning via Heterogeneous Diffusion Adaptive\n  Networks", "comments": "4 pages, 1 figure, conference, submitted to IEEE ISCAS 2015, Lisbon,\n  Portugal", "journal-ref": null, "doi": "10.1109/ISCAS.2015.7168664", "report-no": null, "categories": "cs.LG cs.DC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-network distributed estimation of sparse parameter vectors via diffusion\nLMS strategies has been studied and investigated in recent years. In all the\nexisting works, some convex regularization approach has been used at each node\nof the network in order to achieve an overall network performance superior to\nthat of the simple diffusion LMS, albeit at the cost of increased computational\noverhead. In this paper, we provide analytical as well as experimental results\nwhich show that the convex regularization can be selectively applied only to\nsome chosen nodes keeping rest of the nodes sparsity agnostic, while still\nenjoying the same optimum behavior as can be realized by deploying the convex\nregularization at all the nodes. Due to the incorporation of unregularized\nlearning at a subset of nodes, less computational cost is needed in the\nproposed approach. We also provide a guideline for selection of the sparsity\naware nodes and a closed form expression for the optimum regularization\nparameter.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 16:38:38 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Das", "Bijit Kumar", ""], ["Chakraborty", "Mrityunjoy", ""], ["Arenas-Garc\u00eda", "Jer\u00f3nimo", ""]]}, {"id": "1410.7074", "submitter": "Oscar Beijbom Mr", "authors": "Oscar Beijbom", "title": "Random Sampling in an Age of Automation: Minimizing Expenditures through\n  Balanced Collection and Annotation", "comments": "PDF contains 9 pages of manuscript followed by 7 pages of\n  Supplementary Information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for automated collection and annotation are changing the\ncost-structures of sampling surveys for a wide range of applications. Digital\nsamples in the form of images or audio recordings can be collected rapidly, and\nannotated by computer programs or crowd workers. We consider the problem of\nestimating a population mean under these new cost-structures, and propose a\nHybrid-Offset sampling design. This design utilizes two annotators: a primary,\nwhich is accurate but costly (e.g. a human expert) and an auxiliary which is\nnoisy but cheap (e.g. a computer program), in order to minimize total sampling\nexpenditures. Our analysis gives necessary conditions for the Hybrid-Offset\ndesign and specifies optimal sample sizes for both annotators. Simulations on\ndata from a coral reef survey program indicate that the Hybrid-Offset design\noutperforms several alternative sampling designs. In particular, sampling\nexpenditures are reduced 50% compared to the Conventional design currently\ndeployed by the coral ecologists.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 20:12:32 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 15:59:30 GMT"}, {"version": "v3", "created": "Wed, 19 Nov 2014 01:19:14 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2015 16:18:15 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Beijbom", "Oscar", ""]]}, {"id": "1410.7140", "submitter": "Nevin L.  Zhang", "authors": "Nevin L. Zhang, Chen Fu, Teng Fei Liu, Bao Xin Chen, Kin Man Poon, Pei\n  Xian Chen, Yun Ling Zhang", "title": "A data-driven method for syndrome type identification and classification\n  in traditional Chinese medicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The efficacy of traditional Chinese medicine (TCM) treatments for\nWestern medicine (WM) diseases relies heavily on the proper classification of\npatients into TCM syndrome types. We develop a data-driven method for solving\nthe classification problem, where syndrome types are identified and quantified\nbased on patterns detected in unlabeled symptom survey data.\n  Method: Latent class analysis (LCA) has been applied in WM research to solve\na similar problem, i.e., to identify subtypes of a patient population in the\nabsence of a gold standard. A widely known weakness of LCA is that it makes an\nunrealistically strong independence assumption. We relax the assumption by\nfirst detecting symptom co-occurrence patterns from survey data and use those\npatterns instead of the symptoms as features for LCA. Results: The result of\nthe investigation is a six-step method: Data collection, symptom co-occurrence\npattern discovery, pattern interpretation, syndrome identification, syndrome\ntype identification, and syndrome type classification. A software package\ncalled Lantern is developed to support the application of the method. The\nmethod is illustrated using a data set on Vascular Mild Cognitive Impairment\n(VMCI).\n  Conclusions: A data-driven method for TCM syndrome identification and\nclassification is presented. The method can be used to answer the following\nquestions about a Western medicine disease: What TCM syndrome types are there\namong the patients with the disease? What is the prevalence of each syndrome\ntype? What are the statistical characteristics of each syndrome type in terms\nof occurrence of symptoms? How can we determine the syndrome type(s) of a\npatient?\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 07:32:36 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 04:13:22 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2015 10:58:52 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2016 08:29:49 GMT"}, {"version": "v5", "created": "Wed, 24 Feb 2016 16:05:53 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Zhang", "Nevin L.", ""], ["Fu", "Chen", ""], ["Liu", "Teng Fei", ""], ["Chen", "Bao Xin", ""], ["Poon", "Kin Man", ""], ["Chen", "Pei Xian", ""], ["Zhang", "Yun Ling", ""]]}, {"id": "1410.7171", "submitter": "Reza Eghbali", "authors": "Reza Eghbali, Jon Swenson, Maryam Fazel", "title": "Exponentiated Subgradient Algorithm for Online Optimization under the\n  Random Permutation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online optimization problems arise in many resource allocation tasks, where\nthe future demands for each resource and the associated utility functions\nchange over time and are not known apriori, yet resources need to be allocated\nat every point in time despite the future uncertainty. In this paper, we\nconsider online optimization problems with general concave utilities. We modify\nand extend an online optimization algorithm proposed by Devanur et al. for\nlinear programming to this general setting. The model we use for the arrival of\nthe utilities and demands is known as the random permutation model, where a\nfixed collection of utilities and demands are presented to the algorithm in\nrandom order. We prove that under this model the algorithm achieves a\ncompetitive ratio of $1-O(\\epsilon)$ under a near-optimal assumption that the\nbid to budget ratio is $O (\\frac{\\epsilon^2}{\\log({m}/{\\epsilon})})$, where $m$\nis the number of resources, while enjoying a significantly lower computational\ncost than the optimal algorithm proposed by Kesselheim et al. We draw a\nconnection between the proposed algorithm and subgradient methods used in\nconvex optimization. In addition, we present numerical experiments that\ndemonstrate the performance and speed of this algorithm in comparison to\nexisting algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 10:28:12 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 07:58:42 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Eghbali", "Reza", ""], ["Swenson", "Jon", ""], ["Fazel", "Maryam", ""]]}, {"id": "1410.7172", "submitter": "John-Alexander Assael", "authors": "John-Alexander M. Assael, Ziyu Wang, Bobak Shahriari, Nando de Freitas", "title": "Heteroscedastic Treed Bayesian Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimising black-box functions is important in many disciplines, such as\ntuning machine learning models, robotics, finance and mining exploration.\nBayesian optimisation is a state-of-the-art technique for the global\noptimisation of black-box functions which are expensive to evaluate. At the\ncore of this approach is a Gaussian process prior that captures our belief\nabout the distribution over functions. However, in many cases a single Gaussian\nprocess is not flexible enough to capture non-stationarity in the objective\nfunction. Consequently, heteroscedasticity negatively affects performance of\ntraditional Bayesian methods. In this paper, we propose a novel prior model\nwith hierarchical parameter learning that tackles the problem of\nnon-stationarity in Bayesian optimisation. Our results demonstrate substantial\nimprovements in a wide range of applications, including automatic machine\nlearning and mining exploration.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 10:28:36 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 20:03:23 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Assael", "John-Alexander M.", ""], ["Wang", "Ziyu", ""], ["Shahriari", "Bobak", ""], ["de Freitas", "Nando", ""]]}, {"id": "1410.7220", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis and Abhishek Kumar", "title": "Exact and Heuristic Algorithms for Semi-Nonnegative Matrix Factorization", "comments": "22 pages, 6 figures. New: comparison with k-means initialization,\n  numerical results on real data, ill-posedness of semi-NMF", "journal-ref": "SIAM J. Matrix Anal. & Appl. 36 (4), pp. 1404-1424, 2015", "doi": "10.1137/140993272", "report-no": null, "categories": "math.NA cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix $M$ (not necessarily nonnegative) and a factorization rank\n$r$, semi-nonnegative matrix factorization (semi-NMF) looks for a matrix $U$\nwith $r$ columns and a nonnegative matrix $V$ with $r$ rows such that $UV$ is\nthe best possible approximation of $M$ according to some metric. In this paper,\nwe study the properties of semi-NMF from which we develop exact and heuristic\nalgorithms. Our contribution is threefold. First, we prove that the error of a\nsemi-NMF of rank $r$ has to be smaller than the best unconstrained\napproximation of rank $r-1$. This leads us to a new initialization procedure\nbased on the singular value decomposition (SVD) with a guarantee on the quality\nof the approximation. Second, we propose an exact algorithm (that is, an\nalgorithm that finds an optimal solution), also based on the SVD, for a certain\nclass of matrices (including nonnegative irreducible matrices) from which we\nderive an initialization for matrices not belonging to that class. Numerical\nexperiments illustrate that this second approach performs extremely well, and\nallows us to compute optimal semi-NMF decompositions in many situations.\nFinally, we analyze the computational complexity of semi-NMF proving its\nNP-hardness, already in the rank-one case (that is, for $r = 1$), and we show\nthat semi-NMF is sometimes ill-posed (that is, an optimal solution does not\nexist).\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 13:09:29 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 07:05:37 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 05:55:15 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Gillis", "Nicolas", ""], ["Kumar", "Abhishek", ""]]}, {"id": "1410.7372", "submitter": "Jayadeva", "authors": "Jayadeva, Sanjit S. Batra, and Siddharth Sabharwal", "title": "Feature Selection through Minimization of the VC dimension", "comments": "arXiv admin note: text overlap with arXiv:1410.4573", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection involes identifying the most relevant subset of input\nfeatures, with a view to improving generalization of predictive models by\nreducing overfitting. Directly searching for the most relevant combination of\nattributes is NP-hard. Variable selection is of critical importance in many\napplications, such as micro-array data analysis, where selecting a small number\nof discriminative features is crucial to developing useful models of disease\nmechanisms, as well as for prioritizing targets for drug discovery. The\nrecently proposed Minimal Complexity Machine (MCM) provides a way to learn a\nhyperplane classifier by minimizing an exact (\\boldmath{$\\Theta$}) bound on its\nVC dimension. It is well known that a lower VC dimension contributes to good\ngeneralization. For a linear hyperplane classifier in the input space, the VC\ndimension is upper bounded by the number of features; hence, a linear\nclassifier with a small VC dimension is parsimonious in the set of features it\nemploys. In this paper, we use the linear MCM to learn a classifier in which a\nlarge number of weights are zero; features with non-zero weights are the ones\nthat are chosen. Selected features are used to learn a kernel SVM classifier.\nOn a number of benchmark datasets, the features chosen by the linear MCM yield\ncomparable or better test set accuracy than when methods such as ReliefF and\nFCBF are used for the task. The linear MCM typically chooses one-tenth the\nnumber of attributes chosen by the other methods; on some very high dimensional\ndatasets, the MCM chooses about $0.6\\%$ of the features; in comparison, ReliefF\nand FCBF choose 70 to 140 times more features, thus demonstrating that\nminimizing the VC dimension may provide a new, and very effective route for\nfeature selection and for learning sparse representations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 19:46:55 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Jayadeva", "", ""], ["Batra", "Sanjit S.", ""], ["Sabharwal", "Siddharth", ""]]}, {"id": "1410.7404", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg and Aram Galstyan", "title": "Maximally Informative Hierarchical Representations of High-Dimensional\n  Data", "comments": "13 pages, 8 figures. Appearing in Proceedings of the 18th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a set of probabilistic functions of some input variables as a\nrepresentation of the inputs. We present bounds on how informative a\nrepresentation is about input data. We extend these bounds to hierarchical\nrepresentations so that we can quantify the contribution of each layer towards\ncapturing the information in the original data. The special form of these\nbounds leads to a simple, bottom-up optimization procedure to construct\nhierarchical representations that are also maximally informative about the\ndata. This optimization has linear computational complexity and constant sample\ncomplexity in the number of variables. These results establish a new approach\nto unsupervised learning of deep representations that is both principled and\npractical. We demonstrate the usefulness of the approach on both synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 20:00:40 GMT"}, {"version": "v2", "created": "Sat, 31 Jan 2015 00:38:54 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1410.7414", "submitter": "Junier Oliva", "authors": "Junier Oliva, Willie Neiswanger, Barnabas Poczos, Eric Xing, Jeff\n  Schneider", "title": "Fast Function to Function Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the problem of regression when both input covariates and output\nresponses are functions from a nonparametric function class. Function to\nfunction regression (FFR) covers a large range of interesting applications\nincluding time-series prediction problems, and also more general tasks like\nstudying a mapping between two separate types of distributions. However,\nprevious nonparametric estimators for FFR type problems scale badly\ncomputationally with the number of input/output pairs in a data-set. Given the\ncomplexity of a mapping between general functions it may be necessary to\nconsider large data-sets in order to achieve a low estimation risk. To address\nthis issue, we develop a novel scalable nonparametric estimator, the\nTriple-Basis Estimator (3BE), which is capable of operating over datasets with\nmany instances. To the best of our knowledge, the 3BE is the first\nnonparametric FFR estimator that can scale to massive datasets. We analyze the\n3BE's risk and derive an upperbound rate. Furthermore, we show an improvement\nof several orders of magnitude in terms of prediction speed and a reduction in\nerror over previous estimators in various real-world data-sets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 20:15:18 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Oliva", "Junier", ""], ["Neiswanger", "Willie", ""], ["Poczos", "Barnabas", ""], ["Xing", "Eric", ""], ["Schneider", "Jeff", ""]]}, {"id": "1410.7452", "submitter": "Varun Jampani", "authors": "Varun Jampani, S. M. Ali Eslami, Daniel Tarlow, Pushmeet Kohli and\n  John Winn", "title": "Consensus Message Passing for Layered Graphical Models", "comments": "Appearing in Proceedings of the 18th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Generative models provide a powerful framework for probabilistic reasoning.\nHowever, in many domains their use has been hampered by the practical\ndifficulties of inference. This is particularly the case in computer vision,\nwhere models of the imaging process tend to be large, loopy and layered. For\nthis reason bottom-up conditional models have traditionally dominated in such\ndomains. We find that widely-used, general-purpose message passing inference\nalgorithms such as Expectation Propagation (EP) and Variational Message Passing\n(VMP) fail on the simplest of vision models. With these models in mind, we\nintroduce a modification to message passing that learns to exploit their\nlayered structure by passing 'consensus' messages that guide inference towards\ngood solutions. Experiments on a variety of problems show that the proposed\ntechnique leads to significantly more accurate inference results, not only when\ncompared to standard EP and VMP, but also when compared to competitive\nbottom-up conditional models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 22:40:52 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 21:36:36 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Jampani", "Varun", ""], ["Eslami", "S. M. Ali", ""], ["Tarlow", "Daniel", ""], ["Kohli", "Pushmeet", ""], ["Winn", "John", ""]]}, {"id": "1410.7455", "submitter": "Daniel Povey", "authors": "Daniel Povey, Xiaohui Zhang, Sanjeev Khudanpur", "title": "Parallel training of DNNs with Natural Gradient and Parameter Averaging", "comments": "Accepted as workshop contribution to ICLR 2015. 12 pages plus 16\n  pages of appendices, International Conference on Learning Representations\n  (ICLR): Workshop track, 2015. [2 sets of minor fixes post-publication.]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe the neural-network training framework used in the Kaldi speech\nrecognition toolkit, which is geared towards training DNNs with large amounts\nof training data using multiple GPU-equipped or multi-core machines. In order\nto be as hardware-agnostic as possible, we needed a way to use multiple\nmachines without generating excessive network traffic. Our method is to average\nthe neural network parameters periodically (typically every minute or two), and\nredistribute the averaged parameters to the machines for further training. Each\nmachine sees different data. By itself, this method does not work very well.\nHowever, we have another method, an approximate and efficient implementation of\nNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow\nour periodic-averaging method to work well, as well as substantially improving\nthe convergence of SGD on a single machine.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 22:45:41 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 21:53:05 GMT"}, {"version": "v3", "created": "Sat, 8 Nov 2014 03:35:52 GMT"}, {"version": "v4", "created": "Fri, 19 Dec 2014 02:16:31 GMT"}, {"version": "v5", "created": "Fri, 20 Feb 2015 05:12:34 GMT"}, {"version": "v6", "created": "Sun, 29 Mar 2015 18:25:06 GMT"}, {"version": "v7", "created": "Tue, 9 Jun 2015 22:34:22 GMT"}, {"version": "v8", "created": "Mon, 22 Jun 2015 22:07:56 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Povey", "Daniel", ""], ["Zhang", "Xiaohui", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "1410.7550", "submitter": "Marc Deisenroth", "authors": "Niklas Wahlstr\\\"om, Thomas B. Sch\\\"on, Marc Peter Deisenroth", "title": "Learning deep dynamical models from image pixels", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling dynamical systems is important in many disciplines, e.g., control,\nrobotics, or neurotechnology. Commonly the state of these systems is not\ndirectly observed, but only available through noisy and potentially\nhigh-dimensional observations. In these cases, system identification, i.e.,\nfinding the measurement mapping and the transition mapping (system dynamics) in\nlatent space can be challenging. For linear system dynamics and measurement\nmappings efficient solutions for system identification are available. However,\nin practical applications, the linearity assumptions does not hold, requiring\nnon-linear system identification techniques. If additionally the observations\nare high-dimensional (e.g., images), non-linear system identification is\ninherently hard. To address the problem of non-linear system identification\nfrom high-dimensional observations, we combine recent advances in deep learning\nand system identification. In particular, we jointly learn a low-dimensional\nembedding of the observation by means of deep auto-encoders and a predictive\ntransition model in this low-dimensional space. We demonstrate that our model\nenables learning good predictive models of dynamical systems from pixel\ninformation only.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 08:37:01 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1410.7596", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal, Nikhil R. Devanur", "title": "Fast Algorithms for Online Stochastic Convex Programming", "comments": "To appear in SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the online stochastic Convex Programming (CP) problem, a very\ngeneral version of stochastic online problems which allows arbitrary concave\nobjectives and convex feasibility constraints. Many well-studied problems like\nonline stochastic packing and covering, online stochastic matching with concave\nreturns, etc. form a special case of online stochastic CP. We present fast\nalgorithms for these problems, which achieve near-optimal regret guarantees for\nboth the i.i.d. and the random permutation models of stochastic inputs. When\napplied to the special case online packing, our ideas yield a simpler and\nfaster primal-dual algorithm for this well studied problem, which achieves the\noptimal competitive ratio. Our techniques make explicit the connection of\nprimal-dual paradigm and online learning to online stochastic CP.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 11:57:54 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Agrawal", "Shipra", ""], ["Devanur", "Nikhil R.", ""]]}, {"id": "1410.7659", "submitter": "Guy Bresler", "authors": "Guy Bresler and David Gamarnik and Devavrat Shah", "title": "Learning graphical models from the Glauber dynamics", "comments": "9 pages. Appeared in Allerton Conference 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of learning undirected graphical models\nfrom data generated according to the Glauber dynamics. The Glauber dynamics is\na Markov chain that sequentially updates individual nodes (variables) in a\ngraphical model and it is frequently used to sample from the stationary\ndistribution (to which it converges given sufficient time). Additionally, the\nGlauber dynamics is a natural dynamical model in a variety of settings. This\nwork deviates from the standard formulation of graphical model learning in the\nliterature, where one assumes access to i.i.d. samples from the distribution.\n  Much of the research on graphical model learning has been directed towards\nfinding algorithms with low computational cost. As the main result of this\nwork, we establish that the problem of reconstructing binary pairwise graphical\nmodels is computationally tractable when we observe the Glauber dynamics.\nSpecifically, we show that a binary pairwise graphical model on $p$ nodes with\nmaximum degree $d$ can be learned in time $f(d)p^2\\log p$, for a function\n$f(d)$, using nearly the information-theoretic minimum number of samples.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 15:32:09 GMT"}, {"version": "v2", "created": "Sat, 29 Nov 2014 02:31:20 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Bresler", "Guy", ""], ["Gamarnik", "David", ""], ["Shah", "Devavrat", ""]]}, {"id": "1410.7660", "submitter": "Praneeth Netrapalli", "authors": "Praneeth Netrapalli and U N Niranjan and Sujay Sanghavi and Animashree\n  Anandkumar and Prateek Jain", "title": "Non-convex Robust PCA", "comments": "Extended abstract to appear in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for robust PCA -- the task of recovering a low-rank\nmatrix from sparse corruptions that are of unknown value and support. Our\nmethod involves alternating between projecting appropriate residuals onto the\nset of low-rank matrices, and the set of sparse matrices; each projection is\n{\\em non-convex} but easy to compute. In spite of this non-convexity, we\nestablish exact recovery of the low-rank matrix, under the same conditions that\nare required by existing methods (which are based on convex optimization). For\nan $m \\times n$ input matrix ($m \\leq n)$, our method has a running time of\n$O(r^2mn)$ per iteration, and needs $O(\\log(1/\\epsilon))$ iterations to reach\nan accuracy of $\\epsilon$. This is close to the running time of simple PCA via\nthe power method, which requires $O(rmn)$ per iteration, and\n$O(\\log(1/\\epsilon))$ iterations. In contrast, existing methods for robust PCA,\nwhich are based on convex optimization, have $O(m^2n)$ complexity per\niteration, and take $O(1/\\epsilon)$ iterations, i.e., exponentially more\niterations for the same accuracy.\n  Experiments on both synthetic and real data establishes the improved speed\nand accuracy of our method over existing convex implementations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 15:33:13 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Netrapalli", "Praneeth", ""], ["Niranjan", "U N", ""], ["Sanghavi", "Sujay", ""], ["Anandkumar", "Animashree", ""], ["Jain", "Prateek", ""]]}, {"id": "1410.7690", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, James Sharpnack, Alex Smola, Ryan J. Tibshirani", "title": "Trend Filtering on Graphs", "comments": "A short version appeared in AISTATS'2015", "journal-ref": "Journal of Machine Learning Research Volume (2016) Volume 17\n  Article 15-147", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of adaptive estimators on graphs, based on penalizing\nthe $\\ell_1$ norm of discrete graph differences. This generalizes the idea of\ntrend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate\nnonparametric regression, to graphs. Analogous to the univariate case, graph\ntrend filtering exhibits a level of local adaptivity unmatched by the usual\n$\\ell_2$-based graph smoothers. It is also defined by a convex minimization\nproblem that is readily solved (e.g., by fast ADMM or Newton algorithms). We\ndemonstrate the merits of graph trend filtering through examples and theory.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 16:22:32 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 01:21:54 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 06:16:02 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2015 00:42:15 GMT"}, {"version": "v5", "created": "Sat, 4 Jun 2016 17:03:24 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Sharpnack", "James", ""], ["Smola", "Alex", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1410.7709", "submitter": "Tuomo Sipola", "authors": "Antti Juvonen and Tuomo Sipola", "title": "Anomaly Detection Framework Using Rule Extraction for Efficient\n  Intrusion Detection", "comments": "35 pages, 12 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge datasets in cyber security, such as network traffic logs, can be\nanalyzed using machine learning and data mining methods. However, the amount of\ncollected data is increasing, which makes analysis more difficult. Many machine\nlearning methods have not been designed for big datasets, and consequently are\nslow and difficult to understand. We address the issue of efficient network\ntraffic classification by creating an intrusion detection framework that\napplies dimensionality reduction and conjunctive rule extraction. The system\ncan perform unsupervised anomaly detection and use this information to create\nconjunctive rules that classify huge amounts of traffic in real time. We test\nthe implemented system with the widely used KDD Cup 99 dataset and real-world\nnetwork logs to confirm that the performance is satisfactory. This system is\ntransparent and does not work like a black box, making it intuitive for domain\nexperts, such as network administrators.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 17:29:42 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Juvonen", "Antti", ""], ["Sipola", "Tuomo", ""]]}, {"id": "1410.7827", "submitter": "Yanshuai Cao", "authors": "Yanshuai Cao, David J. Fleet", "title": "Generalized Product of Experts for Automatic and Principled Fusion of\n  Gaussian Process Predictions", "comments": "Modern Nonparametrics 3: Automating the Learning Pipeline workshop at\n  NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a generalized product of experts (gPoE) framework\nfor combining the predictions of multiple probabilistic models. We identify\nfour desirable properties that are important for scalability, expressiveness\nand robustness, when learning and inferring with a combination of multiple\nmodels. Through analysis and experiments, we show that gPoE of Gaussian\nprocesses (GP) have these qualities, while no other existing combination\nschemes satisfy all of them at the same time. The resulting GP-gPoE is highly\nscalable as individual GP experts can be independently learned in parallel;\nvery expressive as the way experts are combined depends on the input rather\nthan fixed; the combined prediction is still a valid probabilistic model with\nnatural interpretation; and finally robust to unreliable predictions from\nindividual experts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 22:04:30 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 03:12:57 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Cao", "Yanshuai", ""], ["Fleet", "David J.", ""]]}, {"id": "1410.7835", "submitter": "Zhensong Qian", "authors": "Oliver Schulte, Zhensong Qian, Arthur E. Kirkpatrick, Xiaoqian Yin,\n  Yan Sun", "title": "Fast Learning of Relational Dependency Networks", "comments": "17 pages, 2 figures, 3 tables, Accepted as long paper by ILP 2014,\n  September 14- 16th, Nancy, France. Added the Appendix: Proof of Consistency\n  Characterization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Relational Dependency Network (RDN) is a directed graphical model widely\nused for multi-relational data. These networks allow cyclic dependencies,\nnecessary to represent relational autocorrelations. We describe an approach for\nlearning both the RDN's structure and its parameters, given an input relational\ndatabase: First learn a Bayesian network (BN), then transform the Bayesian\nnetwork to an RDN. Thus fast Bayes net learning can provide fast RDN learning.\nThe BN-to-RDN transform comprises a simple, local adjustment of the Bayes net\nstructure and a closed-form transform of the Bayes net parameters. This method\ncan learn an RDN for a dataset with a million tuples in minutes. We empirically\ncompare our approach to state-of-the art RDN learning methods that use\nfunctional gradient boosting, on five benchmark datasets. Learning RDNs via BNs\nscales much better to large datasets than learning RDNs with boosting, and\nprovides competitive accuracy in predictions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 23:14:56 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 01:07:36 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Schulte", "Oliver", ""], ["Qian", "Zhensong", ""], ["Kirkpatrick", "Arthur E.", ""], ["Yin", "Xiaoqian", ""], ["Sun", "Yan", ""]]}, {"id": "1410.7852", "submitter": "Xiaoting Zhao", "authors": "Xiaoting Zhao, Peter I. Frazier", "title": "A Markov Decision Process Analysis of the Cold Start Problem in Bayesian\n  Information Filtering", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the information filtering problem, in which we face a stream of\nitems, and must decide which ones to forward to a user to maximize the number\nof relevant items shown, minus a penalty for each irrelevant item shown.\nForwarding decisions are made separately in a personalized way for each user.\nWe focus on the cold-start setting for this problem, in which we have limited\nhistorical data on the user's preferences, and must rely on feedback from\nforwarded articles to learn which the fraction of items relevant to the user in\neach of several item categories. Performing well in this setting requires\ntrading exploration vs. exploitation, forwarding items that are likely to be\nirrelevant, to allow learning that will improve later performance. In a\nBayesian setting, and using Markov decision processes, we show how the\nBayes-optimal forwarding algorithm can be computed efficiently when the user\nwill examine each forwarded article, and how an upper bound on the\nBayes-optimal procedure and a heuristic index policy can be obtained for the\nsetting when the user will examine only a limited number of forwarded items. We\npresent results from simulation experiments using parameters estimated using\nhistorical data from arXiv.org.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 01:15:20 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Zhao", "Xiaoting", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1410.7876", "submitter": "Minh Dao", "authors": "Minh Dao, Nam H. Nguyen, Nasser M. Nasrabadi, and Trac D. Tran", "title": "Collaborative Multi-sensor Classification via Sparsity-based\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general collaborative sparse representation\nframework for multi-sensor classification, which takes into account the\ncorrelations as well as complementary information between heterogeneous sensors\nsimultaneously while considering joint sparsity within each sensor's\nobservations. We also robustify our models to deal with the presence of sparse\nnoise and low-rank interference signals. Specifically, we demonstrate that\nincorporating the noise or interference signal as a low-rank component in our\nmodels is essential in a multi-sensor classification problem when multiple\nco-located sources/sensors simultaneously record the same physical event. We\nfurther extend our frameworks to kernelized models which rely on sparsely\nrepresenting a test sample in terms of all the training samples in a feature\nspace induced by a kernel function. A fast and efficient algorithm based on\nalternative direction method is proposed where its convergence to an optimal\nsolution is guaranteed. Extensive experiments are conducted on several real\nmulti-sensor data sets and results are compared with the conventional\nclassifiers to verify the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 05:25:44 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 16:46:36 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Dao", "Minh", ""], ["Nguyen", "Nam H.", ""], ["Nasrabadi", "Nasser M.", ""], ["Tran", "Trac D.", ""]]}, {"id": "1410.7890", "submitter": "Onur Atan", "authors": "Onur Atan, Cem Tekin, Mihaela van der Schaar", "title": "Global Bandits with Holder Continuity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Multi-Armed Bandit (MAB) problems assume that the arms are\nindependent. However, in many application scenarios, the information obtained\nby playing an arm provides information about the remainder of the arms. Hence,\nin such applications, this informativeness can and should be exploited to\nenable faster convergence to the optimal solution. In this paper, we introduce\nand formalize the Global MAB (GMAB), in which arms are globally informative\nthrough a global parameter, i.e., choosing an arm reveals information about all\nthe arms. We propose a greedy policy for the GMAB which always selects the arm\nwith the highest estimated expected reward, and prove that it achieves bounded\nparameter-dependent regret. Hence, this policy selects suboptimal arms only\nfinitely many times, and after a finite number of initial time steps, the\noptimal arm is selected in all of the remaining time steps with probability\none. In addition, we also study how the informativeness of the arms about each\nother's rewards affects the speed of learning. Specifically, we prove that the\nparameter-free (worst-case) regret is sublinear in time, and decreases with the\ninformativeness of the arms. We also prove a sublinear in time Bayesian risk\nbound for the GMAB which reduces to the well-known Bayesian risk bound for\nlinearly parameterized bandits when the arms are fully informative. GMABs have\napplications ranging from drug and treatment discovery to dynamic pricing.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 07:05:21 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Atan", "Onur", ""], ["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1410.8027", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Towards a Visual Turing Challenge", "comments": "Published in the NIPS 2014 Workshop on Learning Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.GL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As language and visual understanding by machines progresses rapidly, we are\nobserving an increasing interest in holistic architectures that tightly\ninterlink both modalities in a joint learning and inference process. This trend\nhas allowed the community to progress towards more challenging and open tasks\nand refueled the hope at achieving the old AI dream of building machines that\ncould pass a turing test in open domains. In order to steadily make progress\ntowards this goal, we realize that quantifying performance becomes increasingly\ndifficult. Therefore we ask how we can precisely define such challenges and how\nwe can evaluate different algorithms on this open tasks? In this paper, we\nsummarize and discuss such challenges as well as try to give answers where\nappropriate options are available in the literature. We exemplify some of the\nsolutions on a recently presented dataset of question-answering task based on\nreal-world indoor images that establishes a visual turing challenge. Finally,\nwe argue despite the success of unique ground-truth annotation, we likely have\nto step away from carefully curated dataset and rather rely on 'social\nconsensus' as the main driving force to create suitable benchmarks. Providing\ncoverage in this inherently ambiguous output space is an emerging challenge\nthat we face in order to make quantifiable progress in this area.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 15:38:29 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 12:09:53 GMT"}, {"version": "v3", "created": "Tue, 5 May 2015 18:03:56 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1410.8034", "submitter": "Xudong Liu", "authors": "Xudong Liu, Bin Zhang, Ting Zhang and Chang Liu", "title": "Latent Feature Based FM Model For Rating Prediction", "comments": "4 pages, 3 figures, Large Scale Recommender Systems:workshop of\n  Recsys 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating Prediction is a basic problem in Recommender System, and one of the\nmost widely used method is Factorization Machines(FM). However, traditional\nmatrix factorization methods fail to utilize the benefit of implicit feedback,\nwhich has been proved to be important in Rating Prediction problem. In this\nwork, we consider a specific situation, movie rating prediction, where we\nassume that watching history has a big influence on his/her rating behavior on\nan item. We introduce two models, Latent Dirichlet Allocation(LDA) and\nword2vec, both of which perform state-of-the-art results in training latent\nfeatures. Based on that, we propose two feature based models. One is the\nTopic-based FM Model which provides the implicit feedback to the matrix\nfactorization. The other is the Vector-based FM Model which expresses the order\ninfo of watching history. Empirical results on three datasets demonstrate that\nour method performs better than the baseline model and confirm that\nVector-based FM Model usually works better as it contains the order info.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 15:51:54 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Liu", "Xudong", ""], ["Zhang", "Bin", ""], ["Zhang", "Ting", ""], ["Liu", "Chang", ""]]}, {"id": "1410.8043", "submitter": "Wei Dai", "authors": "Wei Dai, Abhimanu Kumar, Jinliang Wei, Qirong Ho, Garth Gibson, Eric\n  P. Xing", "title": "High-Performance Distributed ML at Scale through Parameter Server\n  Consistency Models", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Machine Learning (ML) applications increase in data size and model\ncomplexity, practitioners turn to distributed clusters to satisfy the increased\ncomputational and memory demands. Unfortunately, effective use of clusters for\nML requires considerable expertise in writing distributed code, while\nhighly-abstracted frameworks like Hadoop have not, in practice, approached the\nperformance seen in specialized ML implementations. The recent Parameter Server\n(PS) paradigm is a middle ground between these extremes, allowing easy\nconversion of single-machine parallel ML applications into distributed ones,\nwhile maintaining high throughput through relaxed \"consistency models\" that\nallow inconsistent parameter reads. However, due to insufficient theoretical\nstudy, it is not clear which of these consistency models can really ensure\ncorrect ML algorithm output; at the same time, there remain many\ntheoretically-motivated but undiscovered opportunities to maximize\ncomputational throughput. Motivated by this challenge, we study both the\ntheoretical guarantees and empirical behavior of iterative-convergent ML\nalgorithms in existing PS consistency models. We then use the gleaned insights\nto improve a consistency model using an \"eager\" PS communication mechanism, and\nimplement it as a new PS system that enables ML algorithms to reach their\nsolution more quickly.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 16:19:21 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Dai", "Wei", ""], ["Kumar", "Abhimanu", ""], ["Wei", "Jinliang", ""], ["Ho", "Qirong", ""], ["Gibson", "Garth", ""], ["Xing", "Eric P.", ""]]}, {"id": "1410.8149", "submitter": "Michael Bloodgood", "authors": "Paul Rodrigues, David Zajic, David Doermann, Michael Bloodgood and\n  Peng Ye", "title": "Detecting Structural Irregularity in Electronic Dictionaries Using\n  Language Modeling", "comments": "6 pages, 2 figures, 11 tables; appeared in Proceedings of Electronic\n  Lexicography in the 21st Century (eLex), November 2011", "journal-ref": "In Proceedings of Electronic Lexicography in the 21st Century\n  (eLex), pages 227-232, Bled, Slovenia, November 2011. Trojina Institute for\n  Applied Slovene Studies", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionaries are often developed using tools that save to Extensible Markup\nLanguage (XML)-based standards. These standards often allow high-level\nrepeating elements to represent lexical entries, and utilize descendants of\nthese repeating elements to represent the structure within each lexical entry,\nin the form of an XML tree. In many cases, dictionaries are published that have\nerrors and inconsistencies that are expensive to find manually. This paper\ndiscusses a method for dictionary writers to quickly audit structural\nregularity across entries in a dictionary by using statistical language\nmodeling. The approach learns the patterns of XML nodes that could occur within\nan XML tree, and then calculates the probability of each XML tree in the\ndictionary against these patterns to look for entries that diverge from the\nnorm.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 20:07:21 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Rodrigues", "Paul", ""], ["Zajic", "David", ""], ["Doermann", "David", ""], ["Bloodgood", "Michael", ""], ["Ye", "Peng", ""]]}, {"id": "1410.8206", "submitter": "Minh-Thang Luong", "authors": "Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, Wojciech\n  Zaremba", "title": "Addressing the Rare Word Problem in Neural Machine Translation", "comments": "ACL 2015 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) is a new approach to machine translation\nthat has shown promising results that are comparable to traditional approaches.\nA significant weakness in conventional NMT systems is their inability to\ncorrectly translate very rare words: end-to-end NMTs tend to have relatively\nsmall vocabularies with a single unk symbol that represents every possible\nout-of-vocabulary (OOV) word. In this paper, we propose and implement an\neffective technique to address this problem. We train an NMT system on data\nthat is augmented by the output of a word alignment algorithm, allowing the NMT\nsystem to emit, for each OOV word in the target sentence, the position of its\ncorresponding word in the source sentence. This information is later utilized\nin a post-processing step that translates every OOV word using a dictionary.\nOur experiments on the WMT14 English to French translation task show that this\nmethod provides a substantial improvement of up to 2.8 BLEU points over an\nequivalent NMT system that does not use this technique. With 37.5 BLEU points,\nour NMT system is the first to surpass the best result achieved on a WMT14\ncontest task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 00:20:31 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 19:44:50 GMT"}, {"version": "v3", "created": "Tue, 9 Dec 2014 23:11:46 GMT"}, {"version": "v4", "created": "Sat, 30 May 2015 19:57:28 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Luong", "Minh-Thang", ""], ["Sutskever", "Ilya", ""], ["Le", "Quoc V.", ""], ["Vinyals", "Oriol", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1410.8251", "submitter": "Chris Dyer", "authors": "Chris Dyer", "title": "Notes on Noise Contrastive Estimation and Negative Sampling", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the parameters of probabilistic models of language such as maxent\nmodels and probabilistic neural models is computationally difficult since it\ninvolves evaluating partition functions by summing over an entire vocabulary,\nwhich may be millions of word types in size. Two closely related\nstrategies---noise contrastive estimation (Mnih and Teh, 2012; Mnih and\nKavukcuoglu, 2013; Vaswani et al., 2013) and negative sampling (Mikolov et al.,\n2012; Goldberg and Levy, 2014)---have emerged as popular solutions to this\ncomputational problem, but some confusion remains as to which is more\nappropriate and when. This document explicates their relationships to each\nother and to other estimation techniques. The analysis shows that, although\nthey are superficially similar, NCE is a general parameter estimation technique\nthat is asymptotically unbiased, while negative sampling is best understood as\na family of binary classification models that are useful for learning word\nrepresentations but not as a general-purpose estimator.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 04:33:36 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Dyer", "Chris", ""]]}, {"id": "1410.8275", "submitter": "Stefan Wager", "authors": "Julie Josse and Stefan Wager", "title": "Bootstrap-Based Regularization for Low-Rank Matrix Estimation", "comments": "To appear in the Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a flexible framework for low-rank matrix estimation that allows us\nto transform noise models into regularization schemes via a simple bootstrap\nalgorithm. Effectively, our procedure seeks an autoencoding basis for the\nobserved matrix that is stable with respect to the specified noise model; we\ncall the resulting procedure a stable autoencoder. In the simplest case, with\nan isotropic noise model, our method is equivalent to a classical singular\nvalue shrinkage estimator. For non-isotropic noise models, e.g., Poisson noise,\nthe method does not reduce to singular value shrinkage, and instead yields new\nestimators that perform well in experiments. Moreover, by iterating our stable\nautoencoding scheme, we can automatically generate low-rank estimates without\nspecifying the target rank as a tuning parameter.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 07:22:33 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 02:11:14 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 05:13:33 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Josse", "Julie", ""], ["Wager", "Stefan", ""]]}, {"id": "1410.8326", "submitter": "Nicholas H. Kirk", "authors": "Nicholas H. Kirk", "title": "Towards Learning Object Affordance Priors from Technical Texts", "comments": "\"Active Learning in Robotics\" Workshop, IEEE-RAS International\n  Conference on Humanoid Robots [accepted]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everyday activities performed by artificial assistants can potentially be\nexecuted naively and dangerously given their lack of common sense knowledge.\nThis paper presents conceptual work towards obtaining prior knowledge on the\nusual modality (passive or active) of any given entity, and their affordance\nestimates, by extracting high-confidence ability modality semantic relations (X\ncan Y relationship) from non-figurative texts, by analyzing co-occurrence of\ngrammatical instances of subjects and verbs, and verbs and objects. The\ndiscussion includes an outline of the concept, potential and limitations, and\npossible feature and learning framework adoption.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 11:02:39 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Kirk", "Nicholas H.", ""]]}, {"id": "1410.8420", "submitter": "Cl\\'ement Canonne", "authors": "Eric Blais, Cl\\'ement L. Canonne, Igor C. Oliveira, Rocco A. Servedio\n  and Li-Yang Tan", "title": "Learning circuits with few negations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monotone Boolean functions, and the monotone Boolean circuits that compute\nthem, have been intensively studied in complexity theory. In this paper we\nstudy the structure of Boolean functions in terms of the minimum number of\nnegations in any circuit computing them, a complexity measure that interpolates\nbetween monotone functions and the class of all functions. We study this\ngeneralization of monotonicity from the vantage point of learning theory,\ngiving near-matching upper and lower bounds on the uniform-distribution\nlearnability of circuits in terms of the number of negations they contain. Our\nupper bounds are based on a new structural characterization of negation-limited\ncircuits that extends a classical result of A. A. Markov. Our lower bounds,\nwhich employ Fourier-analytic tools from hardness amplification, give new\nresults even for circuits with no negations (i.e. monotone functions).\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 16:10:26 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Blais", "Eric", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Oliveira", "Igor C.", ""], ["Servedio", "Rocco A.", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1410.8516", "submitter": "Laurent Dinh", "authors": "Laurent Dinh, David Krueger and Yoshua Bengio", "title": "NICE: Non-linear Independent Components Estimation", "comments": "11 pages and 2 pages Appendix, workshop paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning framework for modeling complex high-dimensional\ndensities called Non-linear Independent Component Estimation (NICE). It is\nbased on the idea that a good representation is one in which the data has a\ndistribution that is easy to model. For this purpose, a non-linear\ndeterministic transformation of the data is learned that maps it to a latent\nspace so as to make the transformed data conform to a factorized distribution,\ni.e., resulting in independent latent variables. We parametrize this\ntransformation so that computing the Jacobian determinant and inverse transform\nis trivial, yet we maintain the ability to learn complex non-linear\ntransformations, via a composition of simple building blocks, each based on a\ndeep neural network. The training criterion is simply the exact log-likelihood,\nwhich is tractable. Unbiased ancestral sampling is also easy. We show that this\napproach yields good generative models on four image datasets and can be used\nfor inpainting.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 19:44:20 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 22:40:18 GMT"}, {"version": "v3", "created": "Tue, 6 Jan 2015 18:10:44 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2015 18:06:58 GMT"}, {"version": "v5", "created": "Thu, 12 Mar 2015 06:25:20 GMT"}, {"version": "v6", "created": "Fri, 10 Apr 2015 12:27:56 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Dinh", "Laurent", ""], ["Krueger", "David", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1410.8553", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood, Peng Ye, Paul Rodrigues, David Zajic and David\n  Doermann", "title": "A random forest system combination approach for error detection in\n  digital dictionaries", "comments": "9 pages, 7 figures, 10 tables; appeared in Proceedings of the\n  Workshop on Innovative Hybrid Approaches to the Processing of Textual Data,\n  April 2012", "journal-ref": "In Proceedings of the Workshop on Innovative Hybrid Approaches to\n  the Processing of Textual Data, pages 78-86, Avignon, France, April 2012.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When digitizing a print bilingual dictionary, whether via optical character\nrecognition or manual entry, it is inevitable that errors are introduced into\nthe electronic version that is created. We investigate automating the process\nof detecting errors in an XML representation of a digitized print dictionary\nusing a hybrid approach that combines rule-based, feature-based, and language\nmodel-based methods. We investigate combining methods and show that using\nrandom forests is a promising approach. We find that in isolation, unsupervised\nmethods rival the performance of supervised methods. Random forests typically\nrequire training data so we investigate how we can apply random forests to\ncombine individual base methods that are themselves unsupervised without\nrequiring large amounts of training data. Experiments reveal empirically that a\nrelatively small amount of data is sufficient and can potentially be further\nreduced through specific selection criteria.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 20:52:48 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Bloodgood", "Michael", ""], ["Ye", "Peng", ""], ["Rodrigues", "Paul", ""], ["Zajic", "David", ""], ["Doermann", "David", ""]]}, {"id": "1410.8576", "submitter": "Balint Antal", "authors": "Balint Antal, Andras Hajdu", "title": "An ensemble-based system for automatic screening of diabetic retinopathy", "comments": null, "journal-ref": "Knowledge-Based Systems, Elsevier, Volume 60, April 2014, Pages\n  20-27", "doi": "10.1016/j.knosys.2013.12.023", "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an ensemble-based method for the screening of diabetic\nretinopathy (DR) is proposed. This approach is based on features extracted from\nthe output of several retinal image processing algorithms, such as image-level\n(quality assessment, pre-screening, AM/FM), lesion-specific (microaneurysms,\nexudates) and anatomical (macula, optic disc) components. The actual decision\nabout the presence of the disease is then made by an ensemble of machine\nlearning classifiers. We have tested our approach on the publicly available\nMessidor database, where 90% sensitivity, 91% specificity and 90% accuracy and\n0.989 AUC are achieved in a disease/no-disease setting. These results are\nhighly competitive in this field and suggest that retinal image processing is a\nvalid approach for automatic DR screening.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:14:18 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Antal", "Balint", ""], ["Hajdu", "Andras", ""]]}, {"id": "1410.8580", "submitter": "Matthew Lawlor", "authors": "Matthew Lawlor and Steven Zucker", "title": "An Online Algorithm for Learning Selectivity to Mixture Means", "comments": "Extended technical companion to a presentation at NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a biologically-plausible learning rule called Triplet BCM that\nprovably converges to the class means of general mixture models. This rule\ngeneralizes the classical BCM neural rule, and provides a novel interpretation\nof classical BCM as performing a kind of tensor decomposition. It achieves a\nsubstantial generalization over classical BCM by incorporating triplets of\nsamples from the mixtures, which provides a novel information processing\ninterpretation to spike-timing-dependent plasticity. We provide complete proofs\nof convergence of this learning rule, and an extended discussion of the\nconnection between BCM and tensor learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:37:41 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Lawlor", "Matthew", ""], ["Zucker", "Steven", ""]]}, {"id": "1410.8586", "submitter": "Tao Chen", "authors": "Tao Chen, Damian Borth, Trevor Darrell and Shih-Fu Chang", "title": "DeepSentiBank: Visual Sentiment Concept Classification with Deep\n  Convolutional Neural Networks", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a visual sentiment concept classification method based\non deep convolutional neural networks (CNNs). The visual sentiment concepts are\nadjective noun pairs (ANPs) automatically discovered from the tags of web\nphotos, and can be utilized as effective statistical cues for detecting\nemotions depicted in the images. Nearly one million Flickr images tagged with\nthese ANPs are downloaded to train the classifiers of the concepts. We adopt\nthe popular model of deep convolutional neural networks which recently shows\ngreat performance improvement on classifying large-scale web-based image\ndataset such as ImageNet. Our deep CNNs model is trained based on Caffe, a\nnewly developed deep learning framework. To deal with the biased training data\nwhich only contains images with strong sentiment and to prevent overfitting, we\ninitialize the model with the model weights trained from ImageNet. Performance\nevaluation shows the newly trained deep CNNs model SentiBank 2.0 (or called\nDeepSentiBank) is significantly improved in both annotation accuracy and\nretrieval performance, compared to its predecessors which mainly use binary SVM\nclassification models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:57:12 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Chen", "Tao", ""], ["Borth", "Damian", ""], ["Darrell", "Trevor", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1410.8620", "submitter": "Aaron Defazio Mr", "authors": "Aaron Defazio and Thore Graepel", "title": "A Comparison of learning algorithms on the Arcade Learning Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Reinforcement learning agents have traditionally been evaluated on small toy\nproblems. With advances in computing power and the advent of the Arcade\nLearning Environment, it is now possible to evaluate algorithms on diverse and\ndifficult problems within a consistent framework. We discuss some challenges\nposed by the arcade learning environment which do not manifest in simpler\nenvironments. We then provide a comparison of model-free, linear learning\nalgorithms on this challenging problem set.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 02:19:19 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Defazio", "Aaron", ""], ["Graepel", "Thore", ""]]}, {"id": "1410.8675", "submitter": "Hidekazu Oiwa", "authors": "Hidekazu Oiwa, Ryohei Fujimaki", "title": "Partition-wise Linear Models", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-specific linear models are widely used in practical applications\nbecause of their non-linear but highly interpretable model representations. One\nof the key challenges in their use is non-convexity in simultaneous\noptimization of regions and region-specific models. This paper proposes novel\nconvex region-specific linear models, which we refer to as partition-wise\nlinear models. Our key ideas are 1) assigning linear models not to regions but\nto partitions (region-specifiers) and representing region-specific linear\nmodels by linear combinations of partition-specific models, and 2) optimizing\nregions via partition selection from a large number of given partition\ncandidates by means of convex structured regularizations. In addition to\nproviding initialization-free globally-optimal solutions, our convex\nformulation makes it possible to derive a generalization bound and to use such\nadvanced optimization techniques as proximal methods and decomposition of the\nproximal maps for sparsity-inducing regularizations. Experimental results\ndemonstrate that our partition-wise linear models perform better than or are at\nleast competitive with state-of-the-art region-specific or locally linear\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 09:01:27 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Oiwa", "Hidekazu", ""], ["Fujimaki", "Ryohei", ""]]}, {"id": "1410.8750", "submitter": "Pranjal Awasthi", "authors": "Pranjal Awasthi, Avrim Blum, Or Sheffet, Aravindan Vijayaraghavan", "title": "Learning Mixtures of Ranking Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work concerns learning probabilistic models for ranking data in a\nheterogeneous population. The specific problem we study is learning the\nparameters of a Mallows Mixture Model. Despite being widely studied, current\nheuristics for this problem do not have theoretical guarantees and can get\nstuck in bad local optima. We present the first polynomial time algorithm which\nprovably learns the parameters of a mixture of two Mallows models. A key\ncomponent of our algorithm is a novel use of tensor decomposition techniques to\nlearn the top-k prefix in both the rankings. Before this work, even the\nquestion of identifiability in the case of a mixture of two Mallows models was\nunresolved.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 14:31:54 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Blum", "Avrim", ""], ["Sheffet", "Or", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1410.8783", "submitter": "Nabil Khoufi", "authors": "Nabil Khoufi, Chafik Aloulou, Lamia Hadrich Belguith", "title": "Supervised learning model for parsing Arabic language", "comments": "8 pages,1 figure, Proceedings of the 10th International Workshop on\n  Natural Language Processing and Cognitive Science (NLPCS 2013),2013,\n  Marseille, France, pp129-136", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing the Arabic language is a difficult task given the specificities of\nthis language and given the scarcity of digital resources (grammars and\nannotated corpora). In this paper, we suggest a method for Arabic parsing based\non supervised machine learning. We used the SVMs algorithm to select the\nsyntactic labels of the sentence. Furthermore, we evaluated our parser\nfollowing the cross validation method by using the Penn Arabic Treebank. The\nobtained results are very encouraging.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 15:53:49 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Khoufi", "Nabil", ""], ["Aloulou", "Chafik", ""], ["Belguith", "Lamia Hadrich", ""]]}, {"id": "1410.8864", "submitter": "Dohyung Park", "authors": "Dohyung Park, Constantine Caramanis, Sujay Sanghavi", "title": "Greedy Subspace Clustering", "comments": "To appear in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of subspace clustering: given points that lie on or\nnear the union of many low-dimensional linear subspaces, recover the subspaces.\nTo this end, one first identifies sets of points close to the same subspace and\nuses the sets to estimate the subspaces. As the geometric structure of the\nclusters (linear subspaces) forbids proper performance of general distance\nbased approaches such as K-means, many model-specific methods have been\nproposed. In this paper, we provide new simple and efficient algorithms for\nthis problem. Our statistical analysis shows that the algorithms are guaranteed\nexact (perfect) clustering performance under certain conditions on the number\nof points and the affinity between subspaces. These conditions are weaker than\nthose considered in the standard statistical literature. Experimental results\non synthetic data generated from the standard unions of subspaces model\ndemonstrate our theory. We also show that our algorithm performs competitively\nagainst state-of-the-art algorithms on real-world applications such as motion\nsegmentation and face clustering, with much simpler implementation and lower\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 19:50:42 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Park", "Dohyung", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}]