[{"id": "1308.0187", "submitter": "Stephen Pasteris", "authors": "Stephen Pasteris", "title": "A Time and Space Efficient Junction Tree Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The junction tree algorithm is a way of computing marginals of boolean\nmultivariate probability distributions that factorise over sets of random\nvariables. The junction tree algorithm first constructs a tree called a\njunction tree who's vertices are sets of random variables. The algorithm then\nperforms a generalised version of belief propagation on the junction tree. The\nShafer-Shenoy and Hugin architectures are two ways to perform this belief\npropagation that tradeoff time and space complexities in different ways: Hugin\npropagation is at least as fast as Shafer-Shenoy propagation and in the cases\nthat we have large vertices of high degree is significantly faster. However,\nthis speed increase comes at the cost of an increased space complexity. This\npaper first introduces a simple novel architecture, ARCH-1, which has the best\nof both worlds: the speed of Hugin propagation and the low space requirements\nof Shafer-Shenoy propagation. A more complicated novel architecture, ARCH-2, is\nthen introduced which has, up to a factor only linear in the maximum\ncardinality of any vertex, time and space complexities at least as good as\nARCH-1 and in the cases that we have large vertices of high degree is\nsignificantly faster than ARCH-1.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 16:56:59 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 19:58:11 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2013 18:52:49 GMT"}, {"version": "v4", "created": "Sun, 27 Oct 2013 11:00:46 GMT"}, {"version": "v5", "created": "Mon, 11 Nov 2013 20:51:14 GMT"}, {"version": "v6", "created": "Fri, 15 Nov 2013 20:55:47 GMT"}, {"version": "v7", "created": "Mon, 25 Nov 2013 20:30:34 GMT"}, {"version": "v8", "created": "Thu, 13 Mar 2014 14:52:01 GMT"}, {"version": "v9", "created": "Tue, 23 Dec 2014 20:21:52 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Pasteris", "Stephen", ""]]}, {"id": "1308.0227", "submitter": "Roberto Amadini", "authors": "Roberto Amadini and Maurizio Gabbrielli and Jacopo Mauro", "title": "An Enhanced Features Extractor for a Portfolio of Constraint Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that a single arbitrarily efficient solver can be\nsignificantly outperformed by a portfolio of possibly slower on-average\nsolvers. The solver selection is usually done by means of (un)supervised\nlearning techniques which exploit features extracted from the problem\nspecification. In this paper we present an useful and flexible framework that\nis able to extract an extensive set of features from a Constraint\n(Satisfaction/Optimization) Problem defined in possibly different modeling\nlanguages: MiniZinc, FlatZinc or XCSP. We also report some empirical results\nshowing that the performances that can be obtained using these features are\neffective and competitive with state of the art CSP portfolio techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 14:40:14 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2013 14:53:58 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2013 07:33:15 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2013 00:53:57 GMT"}, {"version": "v5", "created": "Thu, 19 Dec 2013 23:57:50 GMT"}, {"version": "v6", "created": "Sat, 29 Mar 2014 07:54:42 GMT"}, {"version": "v7", "created": "Wed, 2 Apr 2014 03:02:51 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Amadini", "Roberto", ""], ["Gabbrielli", "Maurizio", ""], ["Mauro", "Jacopo", ""]]}, {"id": "1308.0356", "submitter": "Shervan Fekri ershad", "authors": "Shervan Fekri-Ershad, Hadi Tajalizadeh, Shahram Jafari", "title": "Design and Development of an Expert System to Help Head of University\n  Departments", "comments": "4 pages, 2 figures, 2 tables", "journal-ref": "International Journal of Science and Modern Engineering (IJISME),\n  ISSN: 2319-6386, Volume-1, Issue-2, January 2013", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  One of the basic tasks which is responded for head of each university\ndepartment, is employing lecturers based on some default factors such as\nexperience, evidences, qualifies and etc. In this respect, to help the heads,\nsome automatic systems have been proposed until now using machine learning\nmethods, decision support systems (DSS) and etc. According to advantages and\ndisadvantages of the previous methods, a full automatic system is designed in\nthis paper using expert systems. The proposed system is included two main\nsteps. In the first one, the human expert's knowledge is designed as decision\ntrees. The second step is included an expert system which is evaluated using\nextracted rules of these decision trees. Also, to improve the quality of the\nproposed system, a majority voting algorithm is proposed as post processing\nstep to choose the best lecturer which satisfied more expert's decision trees\nfor each course. The results are shown that the designed system average\naccuracy is 78.88. Low computational complexity, simplicity to program and are\nsome of other advantages of the proposed system.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 21:04:07 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Fekri-Ershad", "Shervan", ""], ["Tajalizadeh", "Hadi", ""], ["Jafari", "Shahram", ""]]}, {"id": "1308.0484", "submitter": "Bin Yang", "authors": "Bin Yang, Manohar Kaul, Christian S. Jensen", "title": "Using Incomplete Information for Complete Weight Annotation of Road\n  Networks -- Extended Version", "comments": "This is an extended version of \"Using Incomplete Information for\n  Complete Weight Annotation of Road Networks,\" which is accepted for\n  publication in IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are witnessing increasing interests in the effective use of road networks.\nFor example, to enable effective vehicle routing, weighted-graph models of\ntransportation networks are used, where the weight of an edge captures some\ncost associated with traversing the edge, e.g., greenhouse gas (GHG) emissions\nor travel time. It is a precondition to using a graph model for routing that\nall edges have weights. Weights that capture travel times and GHG emissions can\nbe extracted from GPS trajectory data collected from the network. However, GPS\ntrajectory data typically lack the coverage needed to assign weights to all\nedges. This paper formulates and addresses the problem of annotating all edges\nin a road network with travel cost based weights from a set of trips in the\nnetwork that cover only a small fraction of the edges, each with an associated\nground-truth travel cost. A general framework is proposed to solve the problem.\nSpecifically, the problem is modeled as a regression problem and solved by\nminimizing a judiciously designed objective function that takes into account\nthe topology of the road network. In particular, the use of weighted PageRank\nvalues of edges is explored for assigning appropriate weights to all edges, and\nthe property of directional adjacency of edges is also taken into account to\nassign weights. Empirical studies with weights capturing travel time and GHG\nemissions on two road networks (Skagen, Denmark, and North Jutland, Denmark)\noffer insight into the design properties of the proposed techniques and offer\nevidence that the techniques are effective.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 12:56:19 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2013 20:00:22 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Yang", "Bin", ""], ["Kaul", "Manohar", ""], ["Jensen", "Christian S.", ""]]}, {"id": "1308.0658", "submitter": "Jimmy Ren", "authors": "Jimmy SJ. Ren, Wei Wang, Jiawei Wang, Stephen Shaoyi Liao", "title": "Exploring The Contribution of Unlabeled Data in Financial Sentiment\n  Analysis", "comments": "Appeared in The 27th AAAI Conference on Artificial Intelligence\n  (AAAI-13); Proceedings of AAAI-13 (AAAI Press 2013) pp. 1149-1155", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of its applications in various industries, sentiment\nanalysis by using publicly available web data has become an active research\narea in text classification during these years. It is argued by researchers\nthat semi-supervised learning is an effective approach to this problem since it\nis capable to mitigate the manual labeling effort which is usually expensive\nand time-consuming. However, there was a long-term debate on the effectiveness\nof unlabeled data in text classification. This was partially caused by the fact\nthat many assumptions in theoretic analysis often do not hold in practice. We\nargue that this problem may be further understood by adding an additional\ndimension in the experiment. This allows us to address this problem in the\nperspective of bias and variance in a broader view. We show that the well-known\nperformance degradation issue caused by unlabeled data can be reproduced as a\nsubset of the whole scenario. We argue that if the bias-variance trade-off is\nto be better balanced by a more effective feature selection method unlabeled\ndata is very likely to boost the classification performance. We then propose a\nfeature selection framework in which labeled and unlabeled training samples are\nboth considered. We discuss its potential in achieving such a balance. Besides,\nthe application in financial sentiment analysis is chosen because it not only\nexemplifies an important application, the data possesses better illustrative\npower as well. The implications of this study in text classification and\nfinancial sentiment analysis are both discussed.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 04:20:21 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Ren", "Jimmy SJ.", ""], ["Wang", "Wei", ""], ["Wang", "Jiawei", ""], ["Liao", "Stephen Shaoyi", ""]]}, {"id": "1308.0768", "submitter": "Ibrahim Sabek", "authors": "Ibrahim Sabek and Moustafa Youssef", "title": "MonoStream: A Minimal-Hardware High Accuracy Device-free WLAN\n  Localization System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Device-free (DF) localization is an emerging technology that allows the\ndetection and tracking of entities that do not carry any devices nor\nparticipate actively in the localization process. Typically, DF systems require\na large number of transmitters and receivers to achieve acceptable accuracy,\nwhich is not available in many scenarios such as homes and small businesses. In\nthis paper, we introduce MonoStream as an accurate single-stream DF\nlocalization system that leverages the rich Channel State Information (CSI) as\nwell as MIMO information from the physical layer to provide accurate DF\nlocalization with only one stream. To boost its accuracy and attain low\ncomputational requirements, MonoStream models the DF localization problem as an\nobject recognition problem and uses a novel set of CSI-context features and\ntechniques with proven accuracy and efficiency. Experimental evaluation in two\ntypical testbeds, with a side-by-side comparison with the state-of-the-art,\nshows that MonoStream can achieve an accuracy of 0.95m with at least 26%\nenhancement in median distance error using a single stream only. This\nenhancement in accuracy comes with an efficient execution of less than 23ms per\nlocation update on a typical laptop. This highlights the potential of\nMonoStream usage for real-time DF tracking applications.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 02:07:54 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Sabek", "Ibrahim", ""], ["Youssef", "Moustafa", ""]]}, {"id": "1308.0900", "submitter": "Donny Lee", "authors": "Donny Lee", "title": "Trading USDCHF filtered by Gold dynamics via HMM coupling", "comments": "Abridge version titled \"The profitable, hidden and Markovian couple\n  of Swiss and gold\" in Nov '13 issue of Futures. Read it online at\n  http://www.futuresmag.com/2013/10/21/the-profitable-hidden-and-markovian-couple-of-swi", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a USDCHF trading strategy using the dynamics of gold as a filter.\nOur strategy involves modelling both USDCHF and gold using a coupled hidden\nMarkov model (CHMM). The observations will be indicators, RSI and CCI, which\nwill be used as triggers for our trading signals. Upon decoding the model in\neach iteration, we can get the next most probable state and the next most\nprobable observation. Hopefully by taking advantage of intermarket analysis and\nthe Markov property implicit in the model, trading with these most probable\nvalues will produce profitable results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 08:16:30 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 02:13:47 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Lee", "Donny", ""]]}, {"id": "1308.1006", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer, Stefanie Jegelka and Jeff Bilmes", "title": "Fast Semidifferential-based Submodular Function Optimization", "comments": "This work appeared in Proc. International Conference of Machine\n  Learning (ICML, 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical and powerful new framework for both unconstrained and\nconstrained submodular function optimization based on discrete\nsemidifferentials (sub- and super-differentials). The resulting algorithms,\nwhich repeatedly compute and then efficiently optimize submodular\nsemigradients, offer new and generalize many old methods for submodular\noptimization. Our approach, moreover, takes steps towards providing a unifying\nparadigm applicable to both submodular min- imization and maximization,\nproblems that historically have been treated quite distinctly. The practicality\nof our algorithms is important since interest in submodularity, owing to its\nnatural and wide applicability, has recently been in ascendance within machine\nlearning. We analyze theoretical properties of our algorithms for minimization\nand maximization, and show that many state-of-the-art maximization algorithms\nare special cases. Lastly, we complement our theoretical analyses with\nsupporting empirical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 15:19:48 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Iyer", "Rishabh", ""], ["Jegelka", "Stefanie", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1308.1009", "submitter": "Ping Li", "authors": "Ping Li, Gennady Samorodnitsky, John Hopcroft", "title": "Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of stable random projections is popular for efficiently computing\nthe Lp distances in high dimension (where 0<p<=2), using small space. Because\nit adopts nonadaptive linear projections, this method is naturally suitable\nwhen the data are collected in a dynamic streaming fashion (i.e., turnstile\ndata streams). In this paper, we propose to use only the signs of the projected\ndata and analyze the probability of collision (i.e., when the two signs\ndiffer). We derive a bound of the collision probability which is exact when p=2\nand becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e.,\nCauchy random projections), we show that the probability of collision can be\naccurately approximated as functions of the chi-square similarity. For example,\nwhen the (un-normalized) data are binary, the maximum approximation error of\nthe collision probability is smaller than 0.0192. In text and vision\napplications, the chi-square similarity is a popular measure for nonnegative\ndata when the features are generated from histograms. Our experiments confirm\nthat the proposed method is promising for large-scale learning applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 15:25:51 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Li", "Ping", ""], ["Samorodnitsky", "Gennady", ""], ["Hopcroft", "John", ""]]}, {"id": "1308.1049", "submitter": "Ardeshir Kianercy", "authors": "Ardeshir Kianercy and Aram Galstyan", "title": "Coevolutionary networks of reinforcement-learning agents", "comments": null, "journal-ref": "Phys. Rev. E 88, 012815 (2013)", "doi": "10.1103/PhysRevE.88.012815", "report-no": null, "categories": "cs.MA cs.LG nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a model of network formation in repeated games where the\nplayers adapt their strategies and network ties simultaneously using a simple\nreinforcement-learning scheme. It is demonstrated that the coevolutionary\ndynamics of such systems can be described via coupled replicator equations. We\nprovide a comprehensive analysis for three-player two-action games, which is\nthe minimum system size with nontrivial structural dynamics. In particular, we\ncharacterize the Nash equilibria (NE) in such games and examine the local\nstability of the rest points corresponding to those equilibria. We also study\ngeneral n-player networks via both simulations and analytical methods and find\nthat in the absence of exploration, the stable equilibria consist of star\nmotifs as the main building blocks of the network. Furthermore, in all stable\nequilibria the agents play pure strategies, even when the game allows mixed NE.\nFinally, we study the impact of exploration on learning outcomes, and observe\nthat there is a critical exploration rate above which the symmetric and\nuniformly connected network topology becomes stable.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 17:43:58 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Kianercy", "Ardeshir", ""], ["Galstyan", "Aram", ""]]}, {"id": "1308.1066", "submitter": "Jeff Shrager", "authors": "Jeff Shrager", "title": "Theoretical Issues for Global Cumulative Treatment Analysis (GCTA)", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive trials are now mainstream science. Recently, researchers have taken\nthe adaptive trial concept to its natural conclusion, proposing what we call\n\"Global Cumulative Treatment Analysis\" (GCTA). Similar to the adaptive trial,\ndecision making and data collection and analysis in the GCTA are continuous and\nintegrated, and treatments are ranked in accord with the statistics of this\ninformation, combined with what offers the most information gain. Where GCTA\ndiffers from an adaptive trial, or, for that matter, from any trial design, is\nthat all patients are implicitly participants in the GCTA process, regardless\nof whether they are formally enrolled in a trial. This paper discusses some of\nthe theoretical and practical issues that arise in the design of a GCTA, along\nwith some preliminary thoughts on how they might be approached.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 18:44:17 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Shrager", "Jeff", ""]]}, {"id": "1308.1147", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan, Alexandre B. Tsybakov", "title": "Empirical entropy, minimax regret and minimax risk", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ679 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2017, Vol. 23, No. 2, 789-824", "doi": "10.3150/14-BEJ679", "report-no": "IMS-BEJ-BEJ679", "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the random design regression model with square loss. We propose a\nmethod that aggregates empirical minimizers (ERM) over appropriately chosen\nrandom subsets and reduces to ERM in the extreme case, and we establish sharp\noracle inequalities for its risk. We show that, under the $\\varepsilon^{-p}$\ngrowth of the empirical $\\varepsilon$-entropy, the excess risk of the proposed\nmethod attains the rate $n^{-2/(2+p)}$ for $p\\in(0,2)$ and $n^{-1/p}$ for $p>2$\nwhere $n$ is the sample size. Furthermore, for $p\\in(0,2)$, the excess risk\nrate matches the behavior of the minimax risk of function estimation in\nregression problems under the well-specified model. This yields a conclusion\nthat the rates of statistical estimation in well-specified models (minimax\nrisk) and in misspecified models (minimax regret) are equivalent in the regime\n$p\\in(0,2)$. In other words, for $p\\in(0,2)$ the problem of statistical\nlearning enjoys the same minimax rate as the problem of statistical estimation.\nOn the contrary, for $p>2$ we show that the rates of the minimax regret are, in\ngeneral, slower than for the minimax risk. Our oracle inequalities also imply\nthe $v\\log(n/v)/n$ rates for Vapnik-Chervonenkis type classes of dimension $v$\nwithout the usual convexity assumption on the class; we show that these rates\nare optimal. Finally, for a slightly modified method, we derive a bound on the\nexcess risk of $s$-sparse convex aggregation improving that of Lounici [Math.\nMethods Statist. 16 (2007) 246-259] and providing the optimal rate.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 01:05:52 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 13:21:04 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 13:29:39 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "1308.1187", "submitter": "Abbas Hosseini", "authors": "Ali Soltani-Farani, Hamid R. Rabiee, Seyyed Abbas Hosseini", "title": "Spatial-Aware Dictionary Learning for Hyperspectral Image Classification", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a structured dictionary-based model for hyperspectral\ndata that incorporates both spectral and contextual characteristics of a\nspectral sample, with the goal of hyperspectral image classification. The idea\nis to partition the pixels of a hyperspectral image into a number of spatial\nneighborhoods called contextual groups and to model each pixel with a linear\ncombination of a few dictionary elements learned from the data. Since pixels\ninside a contextual group are often made up of the same materials, their linear\ncombinations are constrained to use common elements from the dictionary. To\nthis end, dictionary learning is carried out with a joint sparse regularizer to\ninduce a common sparsity pattern in the sparse coefficients of each contextual\ngroup. The sparse coefficients are then used for classification using a linear\nSVM. Experimental results on a number of real hyperspectral images confirm the\neffectiveness of the proposed representation for hyperspectral image\nclassification. Moreover, experiments with simulated multispectral data show\nthat the proposed model is capable of finding representations that may\neffectively be used for classification of multispectral-resolution samples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 05:57:08 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Soltani-Farani", "Ali", ""], ["Rabiee", "Hamid R.", ""], ["Hosseini", "Seyyed Abbas", ""]]}, {"id": "1308.1792", "submitter": "Michal Aharon", "authors": "Michal Aharon, Natalie Aizenberg, Edward Bortnikov, Ronny Lempel, Roi\n  Adadi, Tomer Benyamini, Liron Levin, Ran Roth, Ohad Serfaty", "title": "OFF-Set: One-pass Factorization of Feature Sets for Online\n  Recommendation in Persistent Cold Start Settings", "comments": "8 pages, 3 figures, a shorter version is supposed to be published in\n  RecSys13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging recommendation tasks is recommending to a new,\npreviously unseen user. This is known as the 'user cold start' problem.\nAssuming certain features or attributes of users are known, one approach for\nhandling new users is to initially model them based on their features.\n  Motivated by an ad targeting application, this paper describes an extreme\nonline recommendation setting where the cold start problem is perpetual. Every\nuser is encountered by the system just once, receives a recommendation, and\neither consumes or ignores it, registering a binary reward.\n  We introduce One-pass Factorization of Feature Sets, OFF-Set, a novel\nrecommendation algorithm based on Latent Factor analysis, which models users by\nmapping their features to a latent space. Furthermore, OFF-Set is able to model\nnon-linear interactions between pairs of features. OFF-Set is designed for\npurely online recommendation, performing lightweight updates of its model per\neach recommendation-reward observation. We evaluate OFF-Set against several\nstate of the art baselines, and demonstrate its superiority on real\nad-targeting data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 09:24:24 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Aharon", "Michal", ""], ["Aizenberg", "Natalie", ""], ["Bortnikov", "Edward", ""], ["Lempel", "Ronny", ""], ["Adadi", "Roi", ""], ["Benyamini", "Tomer", ""], ["Levin", "Liron", ""], ["Roth", "Ran", ""], ["Serfaty", "Ohad", ""]]}, {"id": "1308.1975", "submitter": "Zhiyong Wang", "authors": "Zhiyong Wang and Jinbo Xu", "title": "Predicting protein contact map using evolutionary and physical\n  constraints by integer programming (extended version)", "comments": "14 pages, 13 figures, 10 tables", "journal-ref": "Bioinformatics (2013) 29 (13): i266-i273", "doi": "10.1093/bioinformatics/btt211", "report-no": null, "categories": "q-bio.QM cs.CE cs.LG math.OC q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation. Protein contact map describes the pairwise spatial and functional\nrelationship of residues in a protein and contains key information for protein\n3D structure prediction. Although studied extensively, it remains very\nchallenging to predict contact map using only sequence information. Most\nexisting methods predict the contact map matrix element-by-element, ignoring\ncorrelation among contacts and physical feasibility of the whole contact map. A\ncouple of recent methods predict contact map based upon residue co-evolution,\ntaking into consideration contact correlation and enforcing a sparsity\nrestraint, but these methods require a very large number of sequence homologs\nfor the protein under consideration and the resultant contact map may be still\nphysically unfavorable.\n  Results. This paper presents a novel method PhyCMAP for contact map\nprediction, integrating both evolutionary and physical restraints by machine\nlearning and integer linear programming (ILP). The evolutionary restraints\ninclude sequence profile, residue co-evolution and context-specific statistical\npotential. The physical restraints specify more concrete relationship among\ncontacts than the sparsity restraint. As such, our method greatly reduces the\nsolution space of the contact map matrix and thus, significantly improves\nprediction accuracy. Experimental results confirm that PhyCMAP outperforms\ncurrently popular methods no matter how many sequence homologs are available\nfor the protein under consideration. PhyCMAP can predict contacts within\nminutes after PSIBLAST search for sequence homologs is done, much faster than\nthe two recent methods PSICOV and EvFold.\n  See http://raptorx.uchicago.edu for the web server.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 20:44:01 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 16:24:06 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Wang", "Zhiyong", ""], ["Xu", "Jinbo", ""]]}, {"id": "1308.2218", "submitter": "Ping Li", "authors": "Ping Li, Michael Mitzenmacher, Anshumali Shrivastava", "title": "Coding for Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of random projections has become very popular for large-scale\napplications in statistical learning, information retrieval, bio-informatics\nand other applications. Using a well-designed coding scheme for the projected\ndata, which determines the number of bits needed for each projected value and\nhow to allocate these bits, can significantly improve the effectiveness of the\nalgorithm, in storage cost as well as computational speed. In this paper, we\nstudy a number of simple coding schemes, focusing on the task of similarity\nestimation and on an application to training linear classifiers. We demonstrate\nthat uniform quantization outperforms the standard existing influential method\n(Datar et. al. 2004). Indeed, we argue that in many cases coding with just a\nsmall number of bits suffices. Furthermore, we also develop a non-uniform 2-bit\ncoding scheme that generally performs well in practice, as confirmed by our\nexperiments on training linear support vector machines (SVM).\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 19:50:24 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Li", "Ping", ""], ["Mitzenmacher", "Michael", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1308.2302", "submitter": "Antoine Deleforge", "authors": "Antoine Deleforge and Florence Forbes and Radu Horaud", "title": "High-Dimensional Regression with Gaussian Mixtures and Partially-Latent\n  Response Variables", "comments": null, "journal-ref": "Statistics and Computing, 25(5), 893-911, 2015", "doi": "10.1007/s11222-014-9461-5", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of approximating high-dimensional data\nwith a low-dimensional representation. We make the following contributions. We\npropose an inverse regression method which exchanges the roles of input and\nresponse, such that the low-dimensional variable becomes the regressor, and\nwhich is tractable. We introduce a mixture of locally-linear probabilistic\nmapping model that starts with estimating the parameters of inverse regression,\nand follows with inferring closed-form solutions for the forward parameters of\nthe high-dimensional regression problem of interest. Moreover, we introduce a\npartially-latent paradigm, such that the vector-valued response variable is\ncomposed of both observed and latent entries, thus being able to deal with data\ncontaminated by experimental artifacts that cannot be explained with noise\nmodels. The proposed probabilistic formulation could be viewed as a\nlatent-variable augmentation of regression. We devise expectation-maximization\n(EM) procedures based on a data augmentation strategy which facilitates the\nmaximum-likelihood search over the model parameters. We propose two\naugmentation schemes and we describe in detail the associated EM inference\nprocedures that may well be viewed as generalizations of a number of EM\nregression, dimension reduction, and factor analysis algorithms. The proposed\nframework is validated with both synthetic and real data. We provide\nexperimental evidence that our method outperforms several existing regression\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2013 10:47:25 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 15:24:50 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2013 15:15:53 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Deleforge", "Antoine", ""], ["Forbes", "Florence", ""], ["Horaud", "Radu", ""]]}, {"id": "1308.2350", "submitter": "Bonny Banerjee", "authors": "Jayanta K. Dutta, Bonny Banerjee", "title": "Learning Features and their Transformations by Spatial and Temporal\n  Spherical Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning features invariant to arbitrary transformations in the data is a\nrequirement for any recognition system, biological or artificial. It is now\nwidely accepted that simple cells in the primary visual cortex respond to\nfeatures while the complex cells respond to features invariant to different\ntransformations. We present a novel two-layered feedforward neural model that\nlearns features in the first layer by spatial spherical clustering and\ninvariance to transformations in the second layer by temporal spherical\nclustering. Learning occurs in an online and unsupervised manner following the\nHebbian rule. When exposed to natural videos acquired by a camera mounted on a\ncat's head, the first and second layer neurons in our model develop simple and\ncomplex cell-like receptive field properties. The model can predict by learning\nlateral connections among the first layer neurons. A topographic map to their\nspatial features emerges by exponentially decaying the flow of activation with\ndistance from one neuron to another in the first layer that fire in close\ntemporal proximity, thereby minimizing the pooling length in an online manner\nsimultaneously with feature learning.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2013 22:56:26 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Dutta", "Jayanta K.", ""], ["Banerjee", "Bonny", ""]]}, {"id": "1308.2655", "submitter": "Loshchilov Ilya", "authors": "Ilya Loshchilov (LIS), Marc Schoenauer (INRIA Saclay - Ile de France,\n  LRI), Mich\\`ele Sebag (LRI)", "title": "KL-based Control of the Learning Schedule for Surrogate Black-Box\n  Optimization", "comments": null, "journal-ref": "Conf\\'erence sur l'Apprentissage Automatique (2013)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the control of an ML component within the Covariance\nMatrix Adaptation Evolution Strategy (CMA-ES) devoted to black-box\noptimization. The known CMA-ES weakness is its sample complexity, the number of\nevaluations of the objective function needed to approximate the global optimum.\nThis weakness is commonly addressed through surrogate optimization, learning an\nestimate of the objective function a.k.a. surrogate model, and replacing most\nevaluations of the true objective function with the (inexpensive) evaluation of\nthe surrogate model. This paper presents a principled control of the learning\nschedule (when to relearn the surrogate model), based on the Kullback-Leibler\ndivergence of the current search distribution and the training distribution of\nthe former surrogate model. The experimental validation of the proposed\napproach shows significant performance gains on a comprehensive set of\nill-conditioned benchmark problems, compared to the best state of the art\nincluding the quasi-Newton high-precision BFGS method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 19:31:59 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2013 19:30:19 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Loshchilov", "Ilya", "", "LIS"], ["Schoenauer", "Marc", "", "INRIA Saclay - Ile de France,\n  LRI"], ["Sebag", "Mich\u00e8le", "", "LRI"]]}, {"id": "1308.2853", "submitter": "Animashree Anandkumar", "authors": "Animashree Anandkumar, Daniel Hsu, Majid Janzamin, Sham Kakade", "title": "When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor\n  Tucker Decompositions with Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR math.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overcomplete latent representations have been very popular for unsupervised\nfeature learning in recent years. In this paper, we specify which overcomplete\nmodels can be identified given observable moments of a certain order. We\nconsider probabilistic admixture or topic models in the overcomplete regime,\nwhere the number of latent topics can greatly exceed the size of the observed\nword vocabulary. While general overcomplete topic models are not identifiable,\nwe establish generic identifiability under a constraint, referred to as topic\npersistence. Our sufficient conditions for identifiability involve a novel set\nof \"higher order\" expansion conditions on the topic-word matrix or the\npopulation structure of the model. This set of higher-order expansion\nconditions allow for overcomplete models, and require the existence of a\nperfect matching from latent topics to higher order observed words. We\nestablish that random structured topic models are identifiable w.h.p. in the\novercomplete regime. Our identifiability results allows for general\n(non-degenerate) distributions for modeling the topic proportions, and thus, we\ncan handle arbitrarily correlated topics in our framework. Our identifiability\nresults imply uniqueness of a class of tensor decompositions with structured\nsparsity which is contained in the class of Tucker decompositions, but is more\ngeneral than the Candecomp/Parafac (CP) decomposition.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 13:16:10 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Hsu", "Daniel", ""], ["Janzamin", "Majid", ""], ["Kakade", "Sham", ""]]}, {"id": "1308.2867", "submitter": "Anastasios Kyrillidis", "authors": "Quoc Tran-Dinh, Anastasios Kyrillidis and Volkan Cevher", "title": "Composite Self-Concordant Minimization", "comments": "46 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variable metric framework for minimizing the sum of a\nself-concordant function and a possibly non-smooth convex function, endowed\nwith an easily computable proximal operator. We theoretically establish the\nconvergence of our framework without relying on the usual Lipschitz gradient\nassumption on the smooth part. An important highlight of our work is a new set\nof analytic step-size selection and correction procedures based on the\nstructure of the problem. We describe concrete algorithmic instances of our\nframework for several interesting applications and demonstrate them numerically\non both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 13:55:12 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 15:20:52 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Tran-Dinh", "Quoc", ""], ["Kyrillidis", "Anastasios", ""], ["Cevher", "Volkan", ""]]}, {"id": "1308.2893", "submitter": "Amit Daniely", "authors": "Amit Daniely and Sivan Sabato and Shai Ben-David and Shai\n  Shalev-Shwartz", "title": "Multiclass learnability and the ERM principle", "comments": null, "journal-ref": "Journal of Machine Learning Research, 16(Jul):1275-1304, 2015", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of multiclass prediction in several learning\nsettings. For the PAC setting our analysis reveals a surprising phenomenon: In\nsharp contrast to binary classification, we show that there exist multiclass\nhypothesis classes for which some Empirical Risk Minimizers (ERM learners) have\nlower sample complexity than others. Furthermore, there are classes that are\nlearnable by some ERM learners, while other ERM learners will fail to learn\nthem. We propose a principle for designing good ERM learners, and use this\nprinciple to prove tight bounds on the sample complexity of learning {\\em\nsymmetric} multiclass hypothesis classes---classes that are invariant under\npermutations of label names. We further provide a characterization of mistake\nand regret bounds for multiclass learning in the online setting and the bandit\nsetting, using new generalizations of Littlestone's dimension.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 15:15:37 GMT"}, {"version": "v2", "created": "Mon, 24 Nov 2014 09:34:18 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Daniely", "Amit", ""], ["Sabato", "Sivan", ""], ["Ben-David", "Shai", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1308.3101", "submitter": "Christian H\\\"ane", "authors": "Christopher Zach and Christian H\\\"ane", "title": "Compact Relaxations for MAP Inference in Pairwise MRFs with Piecewise\n  Linear Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label assignment problems with large state spaces are important tasks\nespecially in computer vision. Often the pairwise interaction (or smoothness\nprior) between labels assigned at adjacent nodes (or pixels) can be described\nas a function of the label difference. Exact inference in such labeling tasks\nis still difficult, and therefore approximate inference methods based on a\nlinear programming (LP) relaxation are commonly used in practice. In this work\nwe study how compact linear programs can be constructed for general piecwise\nlinear smoothness priors. The number of unknowns is O(LK) per pairwise clique\nin terms of the state space size $L$ and the number of linear segments K. This\ncompares to an O(L^2) size complexity of the standard LP relaxation if the\npiecewise linear structure is ignored. Our compact construction and the\nstandard LP relaxation are equivalent and lead to the same (approximate) label\nassignment.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 12:27:24 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 17:51:30 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Zach", "Christopher", ""], ["H\u00e4ne", "Christian", ""]]}, {"id": "1308.3177", "submitter": "Paul Vitanyi", "authors": "Andrew R. Cohen (Dept Electrical and Comput. Engin., Drexel Univ.),\n  P.M.B. Vitanyi (CWI and Comput. Sci., Univ. Amsterdam)", "title": "Normalized Google Distance of Multisets with Applications", "comments": "25 pages, LaTeX, 3 figures/tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized Google distance (NGD) is a relative semantic distance based on the\nWorld Wide Web (or any other large electronic database, for instance Wikipedia)\nand a search engine that returns aggregate page counts. The earlier NGD between\npairs of search terms (including phrases) is not sufficient for all\napplications. We propose an NGD of finite multisets of search terms that is\nbetter for many applications. This gives a relative semantics shared by a\nmultiset of search terms. We give applications and compare the results with\nthose obtained using the pairwise NGD. The derivation of NGD method is based on\nKolmogorov complexity.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 17:04:15 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Cohen", "Andrew R.", "", "Dept Electrical and Comput. Engin., Drexel Univ."], ["Vitanyi", "P. M. B.", "", "CWI and Comput. Sci., Univ. Amsterdam"]]}, {"id": "1308.3314", "submitter": "Sebastien Loustau", "authors": "Camille Brunet (LAREMA), S\\'ebastien Loustau (LAREMA)", "title": "The algorithm of noisy k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we introduce a new algorithm to deal with finite dimensional\nclustering with errors in variables. The design of this algorithm is based on\nrecent theoretical advances (see Loustau (2013a,b)) in statistical learning\nwith errors in variables. As the previous mentioned papers, the algorithm mixes\ndifferent tools from the inverse problem literature and the machine learning\ncommunity. Coarsely, it is based on a two-step procedure: (1) a deconvolution\nstep to deal with noisy inputs and (2) Newton's iterations as the popular\nk-means.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 06:15:21 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Brunet", "Camille", "", "LAREMA"], ["Loustau", "S\u00e9bastien", "", "LAREMA"]]}, {"id": "1308.3381", "submitter": "Ananl Lotsi", "authors": "Anani Lotsi and Ernst Wit", "title": "High dimensional Sparse Gaussian Graphical Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of networks reconstruction from\nheterogeneous data using a Gaussian Graphical Mixture Model (GGMM). It is well\nknown that parameter estimation in this context is challenging due to large\nnumbers of variables coupled with the degeneracy of the likelihood. We propose\nas a solution a penalized maximum likelihood technique by imposing an $l_{1}$\npenalty on the precision matrix. Our approach shrinks the parameters thereby\nresulting in better identifiability and variable selection. We use the\nExpectation Maximization (EM) algorithm which involves the graphical LASSO to\nestimate the mixing coefficients and the precision matrices. We show that under\ncertain regularity conditions the Penalized Maximum Likelihood (PML) estimates\nare consistent. We demonstrate the performance of the PML estimator through\nsimulations and we show the utility of our method for high dimensional data\nanalysis in a genomic application.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 13:17:47 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2013 16:57:00 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2013 13:18:05 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Lotsi", "Anani", ""], ["Wit", "Ernst", ""]]}, {"id": "1308.3383", "submitter": "Twan van Laarhoven", "authors": "Twan van Laarhoven, Elena Marchiori", "title": "Axioms for graph clustering quality functions", "comments": "23 pages. Full text and sources available on:\n  http://www.cs.ru.nl/~T.vanLaarhoven/graph-clustering-axioms-2014/", "journal-ref": "Journal of Machine Learning Research, 15(Jan):193-215, 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate properties that intuitively ought to be satisfied by graph\nclustering quality functions, that is, functions that assign a score to a\nclustering of a graph. Graph clustering, also known as network community\ndetection, is often performed by optimizing such a function. Two axioms\ntailored for graph clustering quality functions are introduced, and the four\naxioms introduced in previous work on distance based clustering are\nreformulated and generalized for the graph setting. We show that modularity, a\nstandard quality function for graph clustering, does not satisfy all of these\nsix properties. This motivates the derivation of a new family of quality\nfunctions, adaptive scale modularity, which does satisfy the proposed axioms.\nAdaptive scale modularity has two parameters, which give greater flexibility in\nthe kinds of clusterings that can be found. Standard graph clustering quality\nfunctions, such as normalized cut and unnormalized cut, are obtained as special\ncases of adaptive scale modularity.\n  In general, the results of our investigation indicate that the considered\naxiomatic framework covers existing `good' quality functions for graph\nclustering, and can be used to derive an interesting new family of quality\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 13:22:24 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 16:22:29 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["van Laarhoven", "Twan", ""], ["Marchiori", "Elena", ""]]}, {"id": "1308.3432", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio, Nicholas L\\'eonard and Aaron Courville", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for\n  Conditional Computation", "comments": "arXiv admin note: substantial text overlap with arXiv:1305.2982", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic neurons and hard non-linearities can be useful for a number of\nreasons in deep learning models, but in many cases they pose a challenging\nproblem: how to estimate the gradient of a loss function with respect to the\ninput of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\"\nthrough these stochastic neurons? We examine this question, existing\napproaches, and compare four families of solutions, applicable in different\nsettings. One of them is the minimum variance unbiased gradient estimator for\nstochatic binary neurons (a special case of the REINFORCE algorithm). A second\napproach, introduced here, decomposes the operation of a binary stochastic\nneuron into a stochastic binary part and a smooth differentiable part, which\napproximates the expected effect of the pure stochatic binary neuron to first\norder. A third approach involves the injection of additive or multiplicative\nnoise in a computational graph that is otherwise differentiable. A fourth\napproach heuristically copies the gradient with respect to the stochastic\noutput directly as an estimator of the gradient with respect to the sigmoid\nargument (we call this the straight-through estimator). To explore a context\nwhere these estimators are useful, we consider a small-scale version of {\\em\nconditional computation}, where sparse stochastic units form a distributed\nrepresentation of gaters that can turn off in combinatorially many ways large\nchunks of the computation performed in the rest of the neural network. In this\ncase, it is important that the gating units produce an actual 0 most of the\ntime. The resulting sparsity can be potentially be exploited to greatly reduce\nthe computational cost of large deep networks for which conditional computation\nwould be useful.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 15:19:34 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Bengio", "Yoshua", ""], ["L\u00e9onard", "Nicholas", ""], ["Courville", "Aaron", ""]]}, {"id": "1308.3506", "submitter": "Kevin Waugh", "authors": "Kevin Waugh and Brian D. Ziebart and J. Andrew Bagnell", "title": "Computational Rationalization: The Inverse Equilibrium Problem", "comments": "In submission to JMLR, conference version: arXiv:1103.5254", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the purposeful behavior of imperfect agents from a small number of\nobservations is a challenging task. When restricted to the single-agent\ndecision-theoretic setting, inverse optimal control techniques assume that\nobserved behavior is an approximately optimal solution to an unknown decision\nproblem. These techniques learn a utility function that explains the example\nbehavior and can then be used to accurately predict or imitate future behavior\nin similar observed or unobserved situations.\n  In this work, we consider similar tasks in competitive and cooperative\nmulti-agent domains. Here, unlike single-agent settings, a player cannot\nmyopically maximize its reward; it must speculate on how the other agents may\nact to influence the game's outcome. Employing the game-theoretic notion of\nregret and the principle of maximum entropy, we introduce a technique for\npredicting and generalizing behavior.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 20:43:47 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Waugh", "Kevin", ""], ["Ziebart", "Brian D.", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1308.3509", "submitter": "Andrew Cotter", "authors": "Andrew Cotter", "title": "Stochastic Optimization for Machine Learning", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been found that stochastic algorithms often find good solutions much\nmore rapidly than inherently-batch approaches. Indeed, a very useful rule of\nthumb is that often, when solving a machine learning problem, an iterative\ntechnique which relies on performing a very large number of\nrelatively-inexpensive updates will often outperform one which performs a\nsmaller number of much \"smarter\" but computationally-expensive updates.\n  In this thesis, we will consider the application of stochastic algorithms to\ntwo of the most important machine learning problems. Part i is concerned with\nthe supervised problem of binary classification using kernelized linear\nclassifiers, for which the data have labels belonging to exactly two classes\n(e.g. \"has cancer\" or \"doesn't have cancer\"), and the learning problem is to\nfind a linear classifier which is best at predicting the label. In Part ii, we\nwill consider the unsupervised problem of Principal Component Analysis, for\nwhich the learning task is to find the directions which contain most of the\nvariance of the data distribution.\n  Our goal is to present stochastic algorithms for both problems which are,\nabove all, practical--they work well on real-world data, in some cases better\nthan all known competing algorithms. A secondary, but still very important,\ngoal is to derive theoretical bounds on the performance of these algorithms\nwhich are at least competitive with, and often better than, those known for\nother approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 20:59:32 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Cotter", "Andrew", ""]]}, {"id": "1308.3513", "submitter": "George  Konidaris", "authors": "Finale Doshi-Velez and George Konidaris", "title": "Hidden Parameter Markov Decision Processes: A Semiparametric Regression\n  Approach for Discovering Latent Task Parametrizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control applications often feature tasks with similar, but not identical,\ndynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP),\na framework that parametrizes a family of related dynamical systems with a\nlow-dimensional set of latent factors, and introduce a semiparametric\nregression approach for learning its structure from data. In the control\nsetting, we show that a learned HiP-MDP rapidly identifies the dynamics of a\nnew task instance, allowing an agent to flexibly adapt to task variations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 21:21:05 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Konidaris", "George", ""]]}, {"id": "1308.3541", "submitter": "Jiaji Zhou", "authors": "Jiaji Zhou, Stephane Ross, Yisong Yue, Debadeepta Dey, J. Andrew\n  Bagnell", "title": "Knapsack Constrained Contextual Submodular List Prediction with\n  Application to Multi-document Summarization", "comments": "8 pages, ICML 2013 Workshop on Inferning: Interactions between\n  Inference and Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We study the problem of predicting a set or list of options under knapsack\nconstraint. The quality of such lists are evaluated by a submodular reward\nfunction that measures both quality and diversity. Similar to DAgger (Ross et\nal., 2010), by a reduction to online learning, we show how to adapt two\nsequence prediction models to imitate greedy maximization under knapsack\nconstraint problems: CONSEQOPT (Dey et al., 2012) and SCP (Ross et al., 2013).\nExperiments on extractive multi-document summarization show that our approach\noutperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 03:46:25 GMT"}, {"version": "v2", "created": "Sat, 15 Mar 2014 19:42:29 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Zhou", "Jiaji", ""], ["Ross", "Stephane", ""], ["Yue", "Yisong", ""], ["Dey", "Debadeepta", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1308.3558", "submitter": "Wenliang Zhong", "authors": "Leon Wenliang Zhong and James T. Kwok", "title": "Fast Stochastic Alternating Direction Method of Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new stochastic alternating direction method of\nmultipliers (ADMM) algorithm, which incrementally approximates the full\ngradient in the linearized ADMM formulation. Besides having a low per-iteration\ncomplexity as existing stochastic ADMM algorithms, the proposed algorithm\nimproves the convergence rate on convex problems from $O(\\frac 1 {\\sqrt{T}})$\nto $O(\\frac 1 T)$, where $T$ is the number of iterations. This matches the\nconvergence rate of the batch ADMM algorithm, but without the need to visit all\nthe samples in each iteration. Experiments on the graph-guided fused lasso\ndemonstrate that the new algorithm is significantly faster than\nstate-of-the-art stochastic and batch ADMM algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 05:48:29 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Zhong", "Leon Wenliang", ""], ["Kwok", "James T.", ""]]}, {"id": "1308.3740", "submitter": "Paul McNicholas", "authors": "Mateen Shaikh, Paul D. McNicholas, M. Luiza Antonie and T. Brendan\n  Murphy", "title": "Standardizing Interestingness Measures for Association Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interestingness measures provide information that can be used to prune or\nselect association rules. A given value of an interestingness measure is often\ninterpreted relative to the overall range of the values that the\ninterestingness measure can take. However, properties of individual association\nrules restrict the values an interestingness measure can achieve. An\ninteresting measure can be standardized to take this into account, but this has\nonly been done for one interestingness measure to date, i.e., the lift.\nStandardization provides greater insight than the raw value and may even alter\nresearchers' perception of the data. We derive standardized analogues of three\ninterestingness measures and use real and simulated data to compare them to\ntheir raw versions, each other, and the standardized lift.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 23:42:05 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Shaikh", "Mateen", ""], ["McNicholas", "Paul D.", ""], ["Antonie", "M. Luiza", ""], ["Murphy", "T. Brendan", ""]]}, {"id": "1308.3750", "submitter": "Yahya Forghani", "authors": "Yahya Forghani, Hadi Sadoghi Yazdi", "title": "Comment on \"robustness and regularization of support vector machines\" by\n  H. Xu, et al., (Journal of Machine Learning Research, vol. 10, pp. 1485-1510,\n  2009, arXiv:0803.3490)", "comments": "2 pages. This paper has been accepted with minor revision in journal\n  of machine learning research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper comments on the published work dealing with robustness and\nregularization of support vector machines (Journal of Machine Learning\nResearch, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. They\nproposed a theorem to show that it is possible to relate robustness in the\nfeature space and robustness in the sample space directly. In this paper, we\npropose a counter example that rejects their theorem.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2013 03:56:03 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Forghani", "Yahya", ""], ["Yazdi", "Hadi Sadoghi", ""]]}, {"id": "1308.3818", "submitter": "Yanpeng Li", "authors": "Yanpeng Li", "title": "Reference Distance Estimator", "comments": "10 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theoretical study is presented for a simple linear classifier called\nreference distance estimator (RDE), which assigns the weight of each feature j\nas P(r|j)-P(r), where r is a reference feature relevant to the target class y.\nThe analysis shows that if r performs better than random guess in predicting y\nand is conditionally independent with each feature j, the RDE will have the\nsame classification performance as that from P(y|j)-P(y), a classifier trained\nwith the gold standard y. Since the estimation of P(r|j)-P(r) does not require\nlabeled data, under the assumption above, RDE trained with a large number of\nunlabeled examples would be close to that trained with infinite labeled\nexamples. For the case the assumption does not hold, we theoretically analyze\nthe factors that influence the closeness of the RDE to the perfect one under\nthe assumption, and present an algorithm to select reference features and\ncombine multiple RDEs from different reference features using both labeled and\nunlabeled data. The experimental results on 10 text classification tasks show\nthat the semi-supervised learning method improves supervised methods using\n5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its\nperformance is even close to a classifier trained with 13 million labeled\nexamples. In addition, the bounds in the theorems provide good estimation of\nthe classification performance and can be useful for new algorithm design.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2013 01:08:55 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Li", "Yanpeng", ""]]}, {"id": "1308.3946", "submitter": "Gregory Valiant", "authors": "Siu-On Chan and Ilias Diakonikolas and Gregory Valiant and Paul\n  Valiant", "title": "Optimal Algorithms for Testing Closeness of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of closeness testing for two discrete distributions.\nMore precisely, given samples from two distributions $p$ and $q$ over an\n$n$-element set, we wish to distinguish whether $p=q$ versus $p$ is at least\n$\\eps$-far from $q$, in either $\\ell_1$ or $\\ell_2$ distance. Batu et al. gave\nthe first sub-linear time algorithms for these problems, which matched the\nlower bounds of Valiant up to a logarithmic factor in $n$, and a polynomial\nfactor of $\\eps.$\n  In this work, we present simple (and new) testers for both the $\\ell_1$ and\n$\\ell_2$ settings, with sample complexity that is information-theoretically\noptimal, to constant factors, both in the dependence on $n$, and the dependence\non $\\eps$; for the $\\ell_1$ testing problem we establish that the sample\ncomplexity is $\\Theta(\\max\\{n^{2/3}/\\eps^{4/3}, n^{1/2}/\\eps^2 \\}).$\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 07:45:07 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Chan", "Siu-On", ""], ["Diakonikolas", "Ilias", ""], ["Valiant", "Gregory", ""], ["Valiant", "Paul", ""]]}, {"id": "1308.4004", "submitter": "Steffen Borgwardt", "authors": "Steffen Borgwardt, Andreas Brieden and Peter Gritzmann", "title": "A balanced k-means algorithm for weighted point sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical $k$-means algorithm for partitioning $n$ points in\n$\\mathbb{R}^d$ into $k$ clusters is one of the most popular and widely spread\nclustering methods. The need to respect prescribed lower bounds on the cluster\nsizes has been observed in many scientific and business applications.\n  In this paper, we present and analyze a generalization of $k$-means that is\ncapable of handling weighted point sets and prescribed lower and upper bounds\non the cluster sizes. We call it weight-balanced $k$-means. The key difference\nto existing models lies in the ability to handle the combination of weighted\npoint sets with prescribed bounds on the cluster sizes. This imposes the need\nto perform partial membership clustering, and leads to significant differences.\n  For example, while finite termination of all $k$-means variants for\nunweighted point sets is a simple consequence of the existence of only finitely\nmany partitions of a given set of points, the situation is more involved for\nweighted point sets, as there are infinitely many partial membership\nclusterings. Using polyhedral theory, we show that the number of iterations of\nweight-balanced $k$-means is bounded above by $n^{O(dk)}$, so in particular it\nis polynomial for fixed $k$ and $d$. This is similar to the known worst-case\nupper bound for classical $k$-means for unweighted point sets and unrestricted\ncluster sizes, despite the much more general framework. We conclude with the\ndiscussion of some additional favorable properties of our method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 12:46:33 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 11:57:36 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Borgwardt", "Steffen", ""], ["Brieden", "Andreas", ""], ["Gritzmann", "Peter", ""]]}, {"id": "1308.4077", "submitter": "Morteza Ibrahimi", "authors": "Jose Bento, and Morteza Ibrahimi", "title": "Support Recovery for the Drift Coefficient of High-Dimensional\n  Diffusions", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of learning the drift coefficient of a $p$-dimensional\nstochastic differential equation from a sample path of length $T$. We assume\nthat the drift is parametrized by a high-dimensional vector, and study the\nsupport recovery problem when both $p$ and $T$ can tend to infinity. In\nparticular, we prove a general lower bound on the sample-complexity $T$ by\nusing a characterization of mutual information as a time integral of\nconditional variance, due to Kadota, Zakai, and Ziv. For linear stochastic\ndifferential equations, the drift coefficient is parametrized by a $p\\times p$\nmatrix which describes which degrees of freedom interact under the dynamics. In\nthis case, we analyze a $\\ell_1$-regularized least squares estimator and prove\nan upper bound on $T$ that nearly matches the lower bound on specific classes\nof sparse matrices.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 17:12:40 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2013 03:36:59 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Bento", "Jose", ""], ["Ibrahimi", "Morteza", ""]]}, {"id": "1308.4123", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "A Likelihood Ratio Approach for Probabilistic Inequalities", "comments": "38 pages, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for deriving probabilistic inequalities based on\nbounding likelihood ratios. We demonstrate that this approach is more general\nand powerful than the classical method frequently used for deriving\nconcentration inequalities such as Chernoff bounds. We discover that the\nproposed approach is inherently related to statistical concepts such as\nmonotone likelihood ratio, maximum likelihood, and the method of moments for\nparameter estimation. A connection between the proposed approach and the large\ndeviation theory is also established. We show that, without using moment\ngenerating functions, tightest possible concentration inequalities may be\nreadily derived by the proposed approach. We have derived new concentration\ninequalities using the proposed approach, which cannot be obtained by the\nclassical approach based on moment generating functions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2013 22:40:41 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "1308.4200", "submitter": "Erik Rodner", "authors": "Erik Rodner, Judy Hoffman, Jeff Donahue, Trevor Darrell, Kate Saenko", "title": "Towards Adapting ImageNet to Reality: Scalable Domain Adaptation with\n  Implicit Low-rank Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images seen during test time are often not from the same distribution as\nimages used for learning. This problem, known as domain shift, occurs when\ntraining classifiers from object-centric internet image databases and trying to\napply them directly to scene understanding tasks. The consequence is often\nsevere performance degradation and is one of the major barriers for the\napplication of classifiers in real-world systems. In this paper, we show how to\nlearn transform-based domain adaptation classifiers in a scalable manner. The\nkey idea is to exploit an implicit rank constraint, originated from a\nmax-margin domain adaptation formulation, to make optimization tractable.\nExperiments show that the transformation between domains can be very\nefficiently learned from data and easily applied to new categories. This begins\nto bridge the gap between large-scale internet image collections and object\nimages captured in everyday life environments.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 01:07:35 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Rodner", "Erik", ""], ["Hoffman", "Judy", ""], ["Donahue", "Jeff", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1308.4206", "submitter": "Lingsong Zhang Lingsong Zhang", "authors": "Lingsong Zhang and J. S. Marron and Shu Lu", "title": "Nested Nonnegative Cone Analysis", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the analysis of nonnegative data objects, a novel Nested\nNonnegative Cone Analysis (NNCA) approach is proposed to overcome some\ndrawbacks of existing methods. The application of traditional PCA/SVD method to\nnonnegative data often cause the approximation matrix leave the nonnegative\ncone, which leads to non-interpretable and sometimes nonsensical results. The\nnonnegative matrix factorization (NMF) approach overcomes this issue, however\nthe NMF approximation matrices suffer several drawbacks: 1) the factorization\nmay not be unique, 2) the resulting approximation matrix at a specific rank may\nnot be unique, and 3) the subspaces spanned by the approximation matrices at\ndifferent ranks may not be nested. These drawbacks will cause troubles in\ndetermining the number of components and in multi-scale (in ranks)\ninterpretability. The NNCA approach proposed in this paper naturally generates\na nested structure, and is shown to be unique at each rank. Simulations are\nused in this paper to illustrate the drawbacks of the traditional methods, and\nthe usefulness of the NNCA method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 01:59:49 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2013 02:50:54 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Zhang", "Lingsong", ""], ["Marron", "J. S.", ""], ["Lu", "Shu", ""]]}, {"id": "1308.4214", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent\n  Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Fr\\'ed\\'eric Bastien,\n  Yoshua Bengio", "title": "Pylearn2: a machine learning research library", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pylearn2 is a machine learning research library. This does not just mean that\nit is a collection of machine learning algorithms that share a common API; it\nmeans that it has been designed for flexibility and extensibility in order to\nfacilitate research projects that involve new or unusual use cases. In this\npaper we give a brief history of the library, an overview of its basic\nphilosophy, a summary of the library's architecture, and a description of how\nthe Pylearn2 community functions socially.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 02:50:43 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Warde-Farley", "David", ""], ["Lamblin", "Pascal", ""], ["Dumoulin", "Vincent", ""], ["Mirza", "Mehdi", ""], ["Pascanu", "Razvan", ""], ["Bergstra", "James", ""], ["Bastien", "Fr\u00e9d\u00e9ric", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1308.4565", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mihaela van der Schaar", "title": "Decentralized Online Big Data Classification - a Bandit Framework", "comments": "arXiv admin note: substantial text overlap with arXiv:1307.0781", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed, online data mining systems have emerged as a result of\napplications requiring analysis of large amounts of correlated and\nhigh-dimensional data produced by multiple distributed data sources. We propose\na distributed online data classification framework where data is gathered by\ndistributed data sources and processed by a heterogeneous set of distributed\nlearners which learn online, at run-time, how to classify the different data\nstreams either by using their locally available classification functions or by\nhelping each other by classifying each other's data. Importantly, since the\ndata is gathered at different locations, sending the data to another learner to\nprocess incurs additional costs such as delays, and hence this will be only\nbeneficial if the benefits obtained from a better classification will exceed\nthe costs. We assume that the classification functions available to each\nprocessing element are fixed, but their prediction accuracy for various types\nof incoming data are unknown and can change dynamically over time, and thus\nthey need to be learned online. We model the problem of joint classification by\nthe distributed and heterogeneous learners from multiple data sources as a\ndistributed contextual bandit problem where each data is characterized by a\nspecific context. We develop distributed online learning algorithms for which\nwe can prove that they have sublinear regret. Compared to prior work in\ndistributed online data mining, our work is the first to provide analytic\nregret results characterizing the performance of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 13:17:00 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2013 14:23:42 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1308.4568", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mihaela van der Schaar", "title": "Distributed Online Learning via Cooperative Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel framework for decentralized, online learning\nby many learners. At each moment of time, an instance characterized by a\ncertain context may arrive to each learner; based on the context, the learner\ncan select one of its own actions (which gives a reward and provides\ninformation) or request assistance from another learner. In the latter case,\nthe requester pays a cost and receives the reward but the provider learns the\ninformation. In our framework, learners are modeled as cooperative contextual\nbandits. Each learner seeks to maximize the expected reward from its arrivals,\nwhich involves trading off the reward received from its own actions, the\ninformation learned from its own actions, the reward received from the actions\nrequested of others and the cost paid for these actions - taking into account\nwhat it has learned about the value of assistance from each other learner. We\ndevelop distributed online learning algorithms and provide analytic bounds to\ncompare the efficiency of these with algorithms with the complete knowledge\n(oracle) benchmark (in which the expected reward of every action in every\ncontext is known by every learner). Our estimates show that regret - the loss\nincurred by the algorithm - is sublinear in time. Our theoretical framework can\nbe used in many practical applications including Big Data mining, event\ndetection in surveillance sensor networks and distributed online recommendation\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 13:28:43 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2013 14:19:28 GMT"}, {"version": "v3", "created": "Sat, 19 Apr 2014 09:40:06 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2015 14:06:27 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1308.4757", "submitter": "Ziqiang Shi", "authors": "Ziqiang Shi and Rujie Liu", "title": "Online and stochastic Douglas-Rachford splitting method for large scale\n  machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online and stochastic learning has emerged as powerful tool in large scale\noptimization. In this work, we generalize the Douglas-Rachford splitting (DRs)\nmethod for minimizing composite functions to online and stochastic settings (to\nour best knowledge this is the first time DRs been generalized to sequential\nversion). We first establish an $O(1/\\sqrt{T})$ regret bound for batch DRs\nmethod. Then we proved that the online DRs splitting method enjoy an $O(1)$\nregret bound and stochastic DRs splitting has a convergence rate of\n$O(1/\\sqrt{T})$. The proof is simple and intuitive, and the results and\ntechnique can be served as a initiate for the research on the large scale\nmachine learning employ the DRs method. Numerical experiments of the proposed\nmethod demonstrate the effectiveness of the online and stochastic update rule,\nand further confirm our regret and convergence analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 03:40:41 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2013 06:21:25 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2013 06:50:16 GMT"}, {"version": "v4", "created": "Sat, 7 Sep 2013 04:30:41 GMT"}, {"version": "v5", "created": "Wed, 25 Sep 2013 08:20:54 GMT"}, {"version": "v6", "created": "Tue, 16 Aug 2016 07:05:38 GMT"}, {"version": "v7", "created": "Mon, 10 Oct 2016 08:46:25 GMT"}, {"version": "v8", "created": "Tue, 11 Oct 2016 00:52:10 GMT"}, {"version": "v9", "created": "Wed, 21 Dec 2016 07:05:13 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Shi", "Ziqiang", ""], ["Liu", "Rujie", ""]]}, {"id": "1308.4828", "submitter": "Tor Lattimore", "authors": "Tor Lattimore and Marcus Hutter and Peter Sunehag", "title": "The Sample-Complexity of General Reinforcement Learning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for general reinforcement learning where the true\nenvironment is known to belong to a finite class of N arbitrary models. The\nalgorithm is shown to be near-optimal for all but O(N log^2 N) time-steps with\nhigh probability. Infinite classes are also considered where we show that\ncompactness is a key criterion for determining the existence of uniform\nsample-complexity bounds. A matching lower bound is given for the finite case.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 11:39:06 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Lattimore", "Tor", ""], ["Hutter", "Marcus", ""], ["Sunehag", "Peter", ""]]}, {"id": "1308.4915", "submitter": "Braxton Osting", "authors": "Braxton Osting, Chris D. White, Edouard Oudet", "title": "Minimal Dirichlet energy partitions for graphs", "comments": "17 pages, 6 figures", "journal-ref": "SIAM Journal of Scientific Computing 36 (2014), no. 4, pp.\n  A1635-A1651", "doi": "10.1137/130934568", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a geometric problem, we introduce a new non-convex graph\npartitioning objective where the optimality criterion is given by the sum of\nthe Dirichlet eigenvalues of the partition components. A relaxed formulation is\nidentified and a novel rearrangement algorithm is proposed, which we show is\nstrictly decreasing and converges in a finite number of iterations to a local\nminimum of the relaxed objective function. Our method is applied to several\nclustering problems on graphs constructed from synthetic data, MNIST\nhandwritten digits, and manifold discretizations. The model has a\nsemi-supervised extension and provides a natural representative for the\nclusters as well.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 17:02:57 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 04:13:06 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Osting", "Braxton", ""], ["White", "Chris D.", ""], ["Oudet", "Edouard", ""]]}, {"id": "1308.4922", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Learning Deep Representation Without Parameter Inference for Nonlinear\n  Dimensionality Reduction", "comments": "This paper has been withdrawn by the author due to a lack of full\n  empirical evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised deep learning is one of the most powerful representation\nlearning techniques. Restricted Boltzman machine, sparse coding, regularized\nauto-encoders, and convolutional neural networks are pioneering building blocks\nof deep learning. In this paper, we propose a new building block -- distributed\nrandom models. The proposed method is a special full implementation of the\nproduct of experts: (i) each expert owns multiple hidden units and different\nexperts have different numbers of hidden units; (ii) the model of each expert\nis a k-center clustering, whose k-centers are only uniformly sampled examples,\nand whose output (i.e. the hidden units) is a sparse code that only the\nsimilarity values from a few nearest neighbors are reserved. The relationship\nbetween the pioneering building blocks, several notable research branches and\nthe proposed method is analyzed. Experimental results show that the proposed\ndeep model can learn better representations than deep belief networks and\nmeanwhile can train a much larger network with much less time than deep belief\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 17:15:36 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 23:35:03 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1308.5038", "submitter": "Ivan Selesnick", "authors": "Po-Yu Chen, Ivan W. Selesnick", "title": "Group-Sparse Signal Denoising: Non-Convex Regularization, Convex\n  Optimization", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TSP.2014.2329274", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization with sparsity-promoting convex regularization is a\nstandard approach for estimating sparse signals in noise. In order to promote\nsparsity more strongly than convex regularization, it is also standard practice\nto employ non-convex optimization. In this paper, we take a third approach. We\nutilize a non-convex regularization term chosen such that the total cost\nfunction (consisting of data consistency and regularization terms) is convex.\nTherefore, sparsity is more strongly promoted than in the standard convex\nformulation, but without sacrificing the attractive aspects of convex\noptimization (unique minimum, robust algorithms, etc.). We use this idea to\nimprove the recently developed 'overlapping group shrinkage' (OGS) algorithm\nfor the denoising of group-sparse signals. The algorithm is applied to the\nproblem of speech enhancement with favorable results in terms of both SNR and\nperceptual quality.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 03:32:57 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2013 19:18:49 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Chen", "Po-Yu", ""], ["Selesnick", "Ivan W.", ""]]}, {"id": "1308.5200", "submitter": "Nicolas Boumal", "authors": "Nicolas Boumal and Bamdev Mishra and P.-A. Absil and Rodolphe\n  Sepulchre", "title": "Manopt, a Matlab toolbox for optimization on manifolds", "comments": null, "journal-ref": "The Journal of Machine Learning Research, 15(1), 1455-1459 (2014)", "doi": null, "report-no": null, "categories": "cs.MS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization on manifolds is a rapidly developing branch of nonlinear\noptimization. Its focus is on problems where the smooth geometry of the search\nspace can be leveraged to design efficient numerical algorithms. In particular,\noptimization on manifolds is well-suited to deal with rank and orthogonality\nconstraints. Such structured constraints appear pervasively in machine learning\napplications, including low-rank matrix completion, sensor network\nlocalization, camera network registration, independent component analysis,\nmetric learning, dimensionality reduction and so on. The Manopt toolbox,\navailable at www.manopt.org, is a user-friendly, documented piece of software\ndedicated to simplify experimenting with state of the art Riemannian\noptimization algorithms. We aim particularly at reaching practitioners outside\nour field.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 18:35:59 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Boumal", "Nicolas", ""], ["Mishra", "Bamdev", ""], ["Absil", "P. -A.", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "1308.5275", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer and Jeff Bilmes", "title": "The Lovasz-Bregman Divergence and connections to rank aggregation,\n  clustering, and web ranking", "comments": "18 pages. A shorter version appeared in Proc. Uncertainty in\n  Artificial Intelligence (UAI)-2013, Bellevue, WA", "journal-ref": "UAI-2013", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the recently introduced theory of Lovasz-Bregman (LB) divergences\n(Iyer & Bilmes, 2012) in several ways. We show that they represent a distortion\nbetween a 'score' and an 'ordering', thus providing a new view of rank\naggregation and order based clustering with interesting connections to web\nranking. We show how the LB divergences have a number of properties akin to\nmany permutation based metrics, and in fact have as special cases forms very\nsimilar to the Kendall-$\\tau$ metric. We also show how the LB divergences\nsubsume a number of commonly used ranking measures in information retrieval,\nlike the NDCG and AUC. Unlike the traditional permutation based metrics,\nhowever, the LB divergence naturally captures a notion of \"confidence\" in the\norderings, thus providing a new representation to applications involving\naggregating scores as opposed to just orderings. We show how a number of\nrecently used web ranking models are forms of Lovasz-Bregman rank aggregation\nand also observe that a natural form of Mallow's model using the LB divergence\nhas been used as conditional ranking models for the 'Learning to Rank' problem.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 01:31:22 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1308.5281", "submitter": "Luca Canzian", "authors": "Luca Canzian, Yu Zhang, and Mihaela van der Schaar", "title": "Ensemble of Distributed Learners for Online Classification of Dynamic\n  Data Streams", "comments": "14 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient distributed online learning scheme to classify data\ncaptured from distributed, heterogeneous, and dynamic data sources. Our scheme\nconsists of multiple distributed local learners, that analyze different streams\nof data that are correlated to a common event that needs to be classified. Each\nlearner uses a local classifier to make a local prediction. The local\npredictions are then collected by each learner and combined using a weighted\nmajority rule to output the final prediction. We propose a novel online\nensemble learning algorithm to update the aggregation rule in order to adapt to\nthe underlying data dynamics. We rigorously determine a bound for the worst\ncase misclassification probability of our algorithm which depends on the\nmisclassification probabilities of the best static aggregation rule, and of the\nbest local classifier. Importantly, the worst case misclassification\nprobability of our algorithm tends asymptotically to 0 if the misclassification\nprobability of the best static aggregation rule or the misclassification\nprobability of the best local classifier tend to 0. Then we extend our\nalgorithm to address challenges specific to the distributed implementation and\nwe prove new bounds that apply to these settings. Finally, we test our scheme\nby performing an evaluation study on several data sets. When applied to data\nsets widely used by the literature dealing with dynamic data streams and\nconcept drift, our scheme exhibits performance gains ranging from 34% to 71%\nwith respect to state of the art solutions.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 02:33:11 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Canzian", "Luca", ""], ["Zhang", "Yu", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1308.5329", "submitter": "EPTCS", "authors": "Ezio Bartocci (TU Wien), Radu Grosu (TU Wien)", "title": "Monitoring with uncertainty", "comments": "In Proceedings HAS 2013, arXiv:1308.4904", "journal-ref": "EPTCS 124, 2013, pp. 1-4", "doi": "10.4204/EPTCS.124.1", "report-no": null, "categories": "cs.LO cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the problem of runtime verification of an instrumented program\nthat misses to emit and to monitor some events. These gaps can occur when a\nmonitoring overhead control mechanism is introduced to disable the monitor of\nan application with real-time constraints. We show how to use statistical\nmodels to learn the application behavior and to \"fill in\" the introduced gaps.\nFinally, we present and discuss some techniques developed in the last three\nyears to estimate the probability that a property of interest is violated in\nthe presence of an incomplete trace.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 14:33:16 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Bartocci", "Ezio", "", "TU Wien"], ["Grosu", "Radu", "", "TU Wien"]]}, {"id": "1308.5338", "submitter": "EPTCS", "authors": "Andrea Ocone (School of Informatics, University of Edinburgh), Guido\n  Sanguinetti (School of Informatics, University of Edinburgh)", "title": "A stochastic hybrid model of a biological filter", "comments": "In Proceedings HAS 2013, arXiv:1308.4904", "journal-ref": "EPTCS 124, 2013, pp. 100-108", "doi": "10.4204/EPTCS.124.10", "report-no": null, "categories": "cs.LG cs.CE q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid model of a biological filter, a genetic circuit which\nremoves fast fluctuations in the cell's internal representation of the extra\ncellular environment. The model takes the classic feed-forward loop (FFL) motif\nand represents it as a network of continuous protein concentrations and binary,\nunobserved gene promoter states. We address the problem of statistical\ninference and parameter learning for this class of models from partial,\ndiscrete time observations. We show that the hybrid representation leads to an\nefficient algorithm for approximate statistical inference in this circuit, and\nshow its effectiveness on a simulated data set.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 14:34:38 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Ocone", "Andrea", "", "School of Informatics, University of Edinburgh"], ["Sanguinetti", "Guido", "", "School of Informatics, University of Edinburgh"]]}, {"id": "1308.5546", "submitter": "Jeremy Rapin", "authors": "J\\'er\\'emy Rapin, J\\'er\\^ome Bobin, Anthony Larue and Jean-Luc Starck", "title": "Sparse and Non-Negative BSS for Noisy Data", "comments": "13 pages, 18 figures, to be published in IEEE Transactions on Signal\n  Processing", "journal-ref": "IEEE Trans. Signal Process. 61 (2013) 5620-5632", "doi": "10.1109/TSP.2013.2279358", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative blind source separation (BSS) has raised interest in various\nfields of research, as testified by the wide literature on the topic of\nnon-negative matrix factorization (NMF). In this context, it is fundamental\nthat the sources to be estimated present some diversity in order to be\nefficiently retrieved. Sparsity is known to enhance such contrast between the\nsources while producing very robust approaches, especially to noise. In this\npaper we introduce a new algorithm in order to tackle the blind separation of\nnon-negative sparse sources from noisy measurements. We first show that\nsparsity and non-negativity constraints have to be carefully applied on the\nsought-after solution. In fact, improperly constrained solutions are unlikely\nto be stable and are therefore sub-optimal. The proposed algorithm, named nGMCA\n(non-negative Generalized Morphological Component Analysis), makes use of\nproximal calculus techniques to provide properly constrained solutions. The\nperformance of nGMCA compared to other state-of-the-art algorithms is\ndemonstrated by numerical experiments encompassing a wide variety of settings,\nwith negligible parameter tuning. In particular, nGMCA is shown to provide\nrobustness to noise and performs well on synthetic mixtures of real NMR\nspectra.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 11:31:38 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Rapin", "J\u00e9r\u00e9my", ""], ["Bobin", "J\u00e9r\u00f4me", ""], ["Larue", "Anthony", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1308.5835", "submitter": "Sumudu Samarakoon Mr.", "authors": "Sumudu Samarakoon and Mehdi Bennis and Walid Saad and Matti Latva-aho", "title": "Backhaul-Aware Interference Management in the Uplink of Wireless Small\n  Cell Networks", "comments": "14 pages, 9 figures, journal article to be appeared in Transaction of\n  Wireless Communication", "journal-ref": null, "doi": "10.1109/TWC.2013.092413.130221", "report-no": null, "categories": "cs.NI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of distributed mechanisms for interference management is one of\nthe key challenges in emerging wireless small cell networks whose backhaul is\ncapacity limited and heterogeneous (wired, wireless and a mix thereof). In this\npaper, a novel, backhaul-aware approach to interference management in wireless\nsmall cell networks is proposed. The proposed approach enables macrocell user\nequipments (MUEs) to optimize their uplink performance, by exploiting the\npresence of neighboring small cell base stations. The problem is formulated as\na noncooperative game among the MUEs that seek to optimize their delay-rate\ntradeoff, given the conditions of both the radio access network and the --\npossibly heterogeneous -- backhaul. To solve this game, a novel, distributed\nlearning algorithm is proposed using which the MUEs autonomously choose their\noptimal uplink transmission strategies, given a limited amount of available\ninformation. The convergence of the proposed algorithm is shown and its\nproperties are studied. Simulation results show that, under various types of\nbackhauls, the proposed approach yields significant performance gains, in terms\nof both average throughput and delay for the MUEs, when compared to existing\nbenchmark algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 12:02:50 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Samarakoon", "Sumudu", ""], ["Bennis", "Mehdi", ""], ["Saad", "Walid", ""], ["Latva-aho", "Matti", ""]]}, {"id": "1308.6181", "submitter": "Jesus Cerquides", "authors": "Victor Bellon and Jesus Cerquides and Ivo Grosse", "title": "Bayesian Conditional Gaussian Network Classifiers with Applications to\n  Mass Spectra Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers based on probabilistic graphical models are very effective. In\ncontinuous domains, maximum likelihood is usually used to assess the\npredictions of those classifiers. When data is scarce, this can easily lead to\noverfitting. In any probabilistic setting, Bayesian averaging (BA) provides\ntheoretically optimal predictions and is known to be robust to overfitting. In\nthis work we introduce Bayesian Conditional Gaussian Network Classifiers, which\nefficiently perform exact Bayesian averaging over the parameters. We evaluate\nthe proposed classifiers against the maximum likelihood alternatives proposed\nso far over standard UCI datasets, concluding that performing BA improves the\nquality of the assessed probabilities (conditional log likelihood) whilst\nmaintaining the error rate.\n  Overfitting is more likely to occur in domains where the number of data items\nis small and the number of variables is large. These two conditions are met in\nthe realm of bioinformatics, where the early diagnosis of cancer from mass\nspectra is a relevant task. We provide an application of our classification\nframework to that problem, comparing it with the standard maximum likelihood\nalternative, where the improvement of quality in the assessed probabilities is\nconfirmed.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 15:14:47 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Bellon", "Victor", ""], ["Cerquides", "Jesus", ""], ["Grosse", "Ivo", ""]]}, {"id": "1308.6273", "submitter": "Rong Ge", "authors": "Sanjeev Arora and Rong Ge and Ankur Moitra", "title": "New Algorithms for Learning Incoherent and Overcomplete Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse recovery we are given a matrix $A$ (the dictionary) and a vector of\nthe form $A X$ where $X$ is sparse, and the goal is to recover $X$. This is a\ncentral notion in signal processing, statistics and machine learning. But in\napplications such as sparse coding, edge detection, compression and super\nresolution, the dictionary $A$ is unknown and has to be learned from random\nexamples of the form $Y = AX$ where $X$ is drawn from an appropriate\ndistribution --- this is the dictionary learning problem. In most settings, $A$\nis overcomplete: it has more columns than rows. This paper presents a\npolynomial-time algorithm for learning overcomplete dictionaries; the only\npreviously known algorithm with provable guarantees is the recent work of\nSpielman, Wang and Wright who gave an algorithm for the full-rank case, which\nis rarely the case in applications. Our algorithm applies to incoherent\ndictionaries which have been a central object of study since they were\nintroduced in seminal work of Donoho and Huo. In particular, a dictionary is\n$\\mu$-incoherent if each pair of columns has inner product at most $\\mu /\n\\sqrt{n}$.\n  The algorithm makes natural stochastic assumptions about the unknown sparse\nvector $X$, which can contain $k \\leq c \\min(\\sqrt{n}/\\mu \\log n, m^{1/2\n-\\eta})$ non-zero entries (for any $\\eta > 0$). This is close to the best $k$\nallowable by the best sparse recovery algorithms even if one knows the\ndictionary $A$ exactly. Moreover, both the running time and sample complexity\ndepend on $\\log 1/\\epsilon$, where $\\epsilon$ is the target accuracy, and so\nour algorithms converge very quickly to the true dictionary. Our algorithm can\nalso tolerate substantial amounts of noise provided it is incoherent with\nrespect to the dictionary (e.g., Gaussian). In the noisy setting, our running\ntime and sample complexity depend polynomially on $1/\\epsilon$, and this is\nnecessary.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 19:57:31 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 19:46:05 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2013 19:34:59 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2013 18:35:17 GMT"}, {"version": "v5", "created": "Mon, 26 May 2014 17:38:58 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Moitra", "Ankur", ""]]}, {"id": "1308.6324", "submitter": "Jakub Tomczak Ph.D.", "authors": "Jakub M. Tomczak", "title": "Prediction of breast cancer recurrence using Classification Restricted\n  Boltzmann Machine with Dropping", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply Classification Restricted Boltzmann Machine\n(ClassRBM) to the problem of predicting breast cancer recurrence. According to\nthe Polish National Cancer Registry, in 2010 only, the breast cancer caused\nalmost 25% of all diagnosed cases of cancer in Poland. We propose how to use\nClassRBM for predicting breast cancer return and discovering relevant inputs\n(symptoms) in illness reappearance. Next, we outline a general probabilistic\nframework for learning Boltzmann machines with masks, which we refer to as\nDropping. The fashion of generating masks leads to different learning methods,\ni.e., DropOut, DropConnect. We propose a new method called DropPart which is a\ngeneralization of DropConnect. In DropPart the Beta distribution instead of\nBernoulli distribution in DropConnect is used. At the end, we carry out an\nexperiment using real-life dataset consisting of 949 cases, provided by the\nInstitute of Oncology Ljubljana.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 22:08:29 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 16:10:27 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Tomczak", "Jakub M.", ""]]}, {"id": "1308.6342", "submitter": "Nando de Freitas", "authors": "Yariv Dror Mizrahi, Misha Denil and Nando de Freitas", "title": "Linear and Parallel Learning of Markov Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new embarrassingly parallel parameter learning algorithm for\nMarkov random fields with untied parameters which is efficient for a large\nclass of practical models. Our algorithm parallelizes naturally over cliques\nand, for graphs of bounded degree, its complexity is linear in the number of\ncliques. Unlike its competitors, our algorithm is fully parallel and for\nlog-linear models it is also data efficient, requiring only the local\nsufficient statistics of the data to estimate parameters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 01:55:37 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 15:04:57 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2013 15:05:06 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2014 17:59:18 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Mizrahi", "Yariv Dror", ""], ["Denil", "Misha", ""], ["de Freitas", "Nando", ""]]}, {"id": "1308.6415", "submitter": "Ke Chen", "authors": "Jonathan Roberts and Ke Chen", "title": "Learning-Based Procedural Content Generation", "comments": "13 pages, 9 figures, manuscript submitted to IEEE Transactions on\n  Computational Intelligence and AI Games (Also a technical report, School of\n  Computer Science, The University of Manchester)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural content generation (PCG) has recently become one of the hottest\ntopics in computational intelligence and AI game researches. Among a variety of\nPCG techniques, search-based approaches overwhelmingly dominate PCG development\nat present. While SBPCG leads to promising results and successful applications,\nit poses a number of challenges ranging from representation to evaluation of\nthe content being generated. In this paper, we present an alternative yet\ngeneric PCG framework, named learning-based procedure content generation\n(LBPCG), to provide potential solutions to several challenging problems in\nexisting PCG techniques. By exploring and exploiting information gained in game\ndevelopment and public beta test via data-driven learning, our framework can\ngenerate robust content adaptable to end-user or target players on-line with\nminimal interruption to their experience. Furthermore, we develop enabling\ntechniques to implement the various models required in our framework. For a\nproof of concept, we have developed a prototype based on the classic open\nsource first-person shooter game, Quake. Simulation results suggest that our\nframework is promising in generating quality content.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 10:06:38 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2013 10:49:29 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Roberts", "Jonathan", ""], ["Chen", "Ke", ""]]}, {"id": "1308.6721", "submitter": "Puneet Kumar", "authors": "Pierre-Yves Baudin (INRIA Saclay - Ile de France), Danny Goodman,\n  Puneet Kumar (INRIA Saclay - Ile de France, CVN), Noura Azzabou (MIRCEN,\n  UPMC), Pierre G. Carlier (UPMC), Nikos Paragios (INRIA Saclay - Ile de\n  France, MAS, LIGM, ENPC), M. Pawan Kumar (INRIA Saclay - Ile de France, CVN)", "title": "Discriminative Parameter Estimation for Random Walks Segmentation", "comments": "Medical Image Computing and Computer Assisted Interventaion (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Random Walks (RW) algorithm is one of the most e - cient and easy-to-use\nprobabilistic segmentation methods. By combining contrast terms with prior\nterms, it provides accurate segmentations of medical images in a fully\nautomated manner. However, one of the main drawbacks of using the RW algorithm\nis that its parameters have to be hand-tuned. we propose a novel discriminative\nlearning framework that estimates the parameters using a training dataset. The\nmain challenge we face is that the training samples are not fully supervised.\nSpeci cally, they provide a hard segmentation of the images, instead of a\nproba- bilistic segmentation. We overcome this challenge by treating the opti-\nmal probabilistic segmentation that is compatible with the given hard\nsegmentation as a latent variable. This allows us to employ the latent support\nvector machine formulation for parameter estimation. We show that our approach\nsigni cantly outperforms the baseline methods on a challenging dataset\nconsisting of real clinical 3D MRI volumes of skeletal muscles.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 12:13:11 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Baudin", "Pierre-Yves", "", "INRIA Saclay - Ile de France"], ["Goodman", "Danny", "", "INRIA Saclay - Ile de France, CVN"], ["Kumar", "Puneet", "", "INRIA Saclay - Ile de France, CVN"], ["Azzabou", "Noura", "", "MIRCEN,\n  UPMC"], ["Carlier", "Pierre G.", "", "UPMC"], ["Paragios", "Nikos", "", "INRIA Saclay - Ile de\n  France, MAS, LIGM, ENPC"], ["Kumar", "M. Pawan", "", "INRIA Saclay - Ile de France, CVN"]]}, {"id": "1308.6744", "submitter": "Navaz Syed A S", "authors": "A.S.Syed Navaz, M.Ravi and T.Prabhu", "title": "Preventing Disclosure of Sensitive Knowledge by Hiding Inference", "comments": "7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Mining is a way of extracting data or uncovering hidden patterns of\ninformation from databases. So, there is a need to prevent the inference rules\nfrom being disclosed such that the more secure data sets cannot be identified\nfrom non sensitive attributes. This can be done through removing or adding\ncertain item sets in the transactions Sanitization. The purpose is to hide the\nInference rules, so that the user may not be able to discover any valuable\ninformation from other non sensitive data and any organisation can release all\nsamples of their data without the fear of Knowledge Discovery In Databases\nwhich can be achieved by investigating frequently occurring item sets, rules\nthat can be mined from them with the objective of hiding them. Another way is\nto release only limited samples in the new database so that there is no\ninformation loss and it also satisfies the legitimate needs of the users. The\nmajor problem is uncovering hidden patterns, which causes a threat to the\ndatabase security. Sensitive data are inferred from non-sensitive data based on\nthe semantics of the application the user has, commonly known as the inference\nproblem. Two fundamental approaches to protect sensitive rules from disclosure\nare that, preventing rules from being generated by hiding the frequent sets of\ndata items and reducing the importance of the rules by setting their confidence\nbelow a user-specified threshold.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 08:34:08 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Navaz", "A. S. Syed", ""], ["Ravi", "M.", ""], ["Prabhu", "T.", ""]]}, {"id": "1308.6797", "submitter": "Nir Ailon", "authors": "Nir Ailon", "title": "Online Ranking: Discrete Choice, Spearman Correlation and Other Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $V$ of $n$ objects, an online ranking system outputs at each time\nstep a full ranking of the set, observes a feedback of some form and suffers a\nloss. We study the setting in which the (adversarial) feedback is an element in\n$V$, and the loss is the position (0th, 1st, 2nd...) of the item in the\noutputted ranking. More generally, we study a setting in which the feedback is\na subset $U$ of at most $k$ elements in $V$, and the loss is the sum of the\npositions of those elements.\n  We present an algorithm of expected regret $O(n^{3/2}\\sqrt{Tk})$ over a time\nhorizon of $T$ steps with respect to the best single ranking in hindsight. This\nimproves previous algorithms and analyses either by a factor of either\n$\\Omega(\\sqrt{k})$, a factor of $\\Omega(\\sqrt{\\log n})$ or by improving running\ntime from quadratic to $O(n\\log n)$ per round. We also prove a matching lower\nbound. Our techniques also imply an improved regret bound for online rank\naggregation over the Spearman correlation measure, and to other more complex\nranking loss functions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 17:03:16 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2013 02:18:18 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2013 02:52:39 GMT"}, {"version": "v4", "created": "Wed, 25 Sep 2013 22:05:33 GMT"}, {"version": "v5", "created": "Mon, 14 Oct 2013 14:44:41 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Ailon", "Nir", ""]]}]