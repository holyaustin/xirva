[{"id": "0905.1546", "submitter": "Anthony Man-Cho So", "authors": "Zhisu Zhu, Anthony Man-Cho So, Yinyu Ye", "title": "Fast and Near-Optimal Matrix Completion via Randomized Basis Pursuit", "comments": "23 pages. New section (Section 3.3) added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the philosophy and phenomenal success of compressed sensing, the\nproblem of reconstructing a matrix from a sampling of its entries has attracted\nmuch attention recently. Such a problem can be viewed as an\ninformation-theoretic variant of the well-studied matrix completion problem,\nand the main objective is to design an efficient algorithm that can reconstruct\na matrix by inspecting only a small number of its entries. Although this is an\nimpossible task in general, Cand\\`es and co-authors have recently shown that\nunder a so-called incoherence assumption, a rank $r$ $n\\times n$ matrix can be\nreconstructed using semidefinite programming (SDP) after one inspects\n$O(nr\\log^6n)$ of its entries. In this paper we propose an alternative approach\nthat is much more efficient and can reconstruct a larger class of matrices by\ninspecting a significantly smaller number of the entries. Specifically, we\nfirst introduce a class of so-called stable matrices and show that it includes\nall those that satisfy the incoherence assumption. Then, we propose a\nrandomized basis pursuit (RBP) algorithm and show that it can reconstruct a\nstable rank $r$ $n\\times n$ matrix after inspecting $O(nr\\log n)$ of its\nentries. Our sampling bound is only a logarithmic factor away from the\ninformation-theoretic limit and is essentially optimal. Moreover, the runtime\nof the RBP algorithm is bounded by $O(nr^2\\log n+n^2r)$, which compares very\nfavorably with the $\\Omega(n^4r^2\\log^{12}n)$ runtime of the SDP-based\nalgorithm. Perhaps more importantly, our algorithm will provide an exact\nreconstruction of the input matrix in polynomial time. By contrast, the\nSDP-based algorithm can only provide an approximate one in polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2009 06:13:49 GMT"}, {"version": "v2", "created": "Fri, 15 May 2009 03:38:47 GMT"}], "update_date": "2009-05-15", "authors_parsed": [["Zhu", "Zhisu", ""], ["So", "Anthony Man-Cho", ""], ["Ye", "Yinyu", ""]]}, {"id": "0905.2125", "submitter": "Jenia Jitsev", "authors": "Jenia Jitsev, Christoph von der Malsburg", "title": "Experience-driven formation of parts-based representations in a model of\n  layered visual memory", "comments": "34 pages, 12 Figures, 1 Table, published in Frontiers in\n  Computational Neuroscience (Special Issue on Complex Systems Science and\n  Brain Dynamics),\n  http://www.frontiersin.org/neuroscience/computationalneuroscience/paper/10.3389/neuro.10/015.2009/", "journal-ref": "Front. Comput. Neurosci. 3:15 (2009)", "doi": "10.3389/neuro.10.015.2009", "report-no": null, "categories": "q-bio.NC cs.LG nlin.AO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Growing neuropsychological and neurophysiological evidence suggests that the\nvisual cortex uses parts-based representations to encode, store and retrieve\nrelevant objects. In such a scheme, objects are represented as a set of\nspatially distributed local features, or parts, arranged in stereotypical\nfashion. To encode the local appearance and to represent the relations between\nthe constituent parts, there has to be an appropriate memory structure formed\nby previous experience with visual objects. Here, we propose a model how a\nhierarchical memory structure supporting efficient storage and rapid recall of\nparts-based representations can be established by an experience-driven process\nof self-organization. The process is based on the collaboration of slow\nbidirectional synaptic plasticity and homeostatic unit activity regulation,\nboth running at the top of fast activity dynamics with winner-take-all\ncharacter modulated by an oscillatory rhythm. These neural mechanisms lay down\nthe basis for cooperation and competition between the distributed units and\ntheir synaptic connections. Choosing human face recognition as a test task, we\nshow that, under the condition of open-ended, unsupervised incremental\nlearning, the system is able to form memory traces for individual faces in a\nparts-based fashion. On a lower memory layer the synaptic structure is\ndeveloped to represent local facial features and their interrelations, while\nthe identities of different persons are captured explicitly on a higher layer.\nAn additional property of the resulting representations is the sparseness of\nboth the activity during the recall and the synaptic patterns comprising the\nmemory traces.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2009 14:23:36 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2009 12:58:02 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2010 15:38:57 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Jitsev", "Jenia", ""], ["von der Malsburg", "Christoph", ""]]}, {"id": "0905.2347", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Juan-Manuel Torres-Moreno and Laurent Bougrain and Fr\\'d\\'eric\n  Alexandre", "title": "Combining Supervised and Unsupervised Learning for GIS Classification", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new hybrid learning algorithm for unsupervised\nclassification tasks. We combined Fuzzy c-means learning algorithm and a\nsupervised version of Minimerror to develop a hybrid incremental strategy\nallowing unsupervised classifications. We applied this new approach to a\nreal-world database in order to know if the information contained in unlabeled\nfeatures of a Geographic Information System (GIS), allows to well classify it.\nFinally, we compared our results to a classical supervised classification\nobtained by a multilayer perceptron.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2009 14:59:15 GMT"}], "update_date": "2009-05-15", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["Bougrain", "Laurent", ""], ["Alexandre", "Frd\u00e9ric", ""]]}, {"id": "0905.2639", "submitter": "Narayana Santhanam", "authors": "Narayana Santhanam and Martin J. Wainwright", "title": "Information-theoretic limits of selecting binary graphical models in\n  high dimensions", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of graphical model selection is to correctly estimate the graph\nstructure of a Markov random field given samples from the underlying\ndistribution. We analyze the information-theoretic limitations of the problem\nof graph selection for binary Markov random fields under high-dimensional\nscaling, in which the graph size $p$ and the number of edges $k$, and/or the\nmaximal node degree $d$ are allowed to increase to infinity as a function of\nthe sample size $n$. For pairwise binary Markov random fields, we derive both\nnecessary and sufficient conditions for correct graph selection over the class\n$\\mathcal{G}_{p,k}$ of graphs on $p$ vertices with at most $k$ edges, and over\nthe class $\\mathcal{G}_{p,d}$ of graphs on $p$ vertices with maximum degree at\nmost $d$. For the class $\\mathcal{G}_{p, k}$, we establish the existence of\nconstants $c$ and $c'$ such that if $\\numobs < c k \\log p$, any method has\nerror probability at least 1/2 uniformly over the family, and we demonstrate a\ngraph decoder that succeeds with high probability uniformly over the family for\nsample sizes $\\numobs > c' k^2 \\log p$. Similarly, for the class\n$\\mathcal{G}_{p,d}$, we exhibit constants $c$ and $c'$ such that for $n < c d^2\n\\log p$, any method fails with probability at least 1/2, and we demonstrate a\ngraph decoder that succeeds with high probability for $n > c' d^3 \\log p$.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2009 00:41:30 GMT"}], "update_date": "2009-05-19", "authors_parsed": [["Santhanam", "Narayana", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "0905.2997", "submitter": "Andrew Guillory", "authors": "Andrew Guillory, Jeff Bilmes", "title": "Average-Case Active Learning with Costs", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "UWEETR-2009-0005", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the expected cost of a greedy active learning algorithm. Our\nanalysis extends previous work to a more general setting in which different\nqueries have different costs. Moreover, queries may have more than two possible\nresponses and the distribution over hypotheses may be non uniform. Specific\napplications include active learning with label costs, active learning for\nmulticlass and partial label queries, and batch mode active learning. We also\ndiscuss an approximate version of interest when there are very many queries.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 23:21:35 GMT"}], "update_date": "2009-05-20", "authors_parsed": [["Guillory", "Andrew", ""], ["Bilmes", "Jeff", ""]]}, {"id": "0905.3347", "submitter": "Paul Vitanyi", "authors": "Paul M.B. Vitanyi", "title": "Information Distance in Multiples", "comments": "LateX 14 pages, Submitted to a technical journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information distance is a parameter-free similarity measure based on\ncompression, used in pattern recognition, data mining, phylogeny, clustering,\nand classification. The notion of information distance is extended from pairs\nto multiples (finite lists). We study maximal overlap, metricity, universality,\nminimal overlap, additivity, and normalized information distance in multiples.\nWe use the theoretical notion of Kolmogorov complexity which for practical\npurposes is approximated by the length of the compressed version of the file\ninvolved, using a real-world compression program.\n  {\\em Index Terms}-- Information distance, multiples, pattern recognition,\ndata mining, similarity, Kolmogorov complexity\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2009 16:37:16 GMT"}], "update_date": "2009-05-21", "authors_parsed": [["Vitanyi", "Paul M. B.", ""]]}, {"id": "0905.3369", "submitter": "John Langford", "authors": "John Langford, Ruslan Salakhutdinov, and Tong Zhang", "title": "Learning Nonlinear Dynamic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for learning nonlinear dynamic models, which\nleads to a new set of tools capable of solving problems that are otherwise\ndifficult. We provide theory showing this new approach is consistent for models\nwith long range structure, and apply the approach to motion capture and\nhigh-dimensional video data, yielding results superior to standard\nalternatives.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2009 18:08:18 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2009 20:29:16 GMT"}], "update_date": "2009-06-03", "authors_parsed": [["Langford", "John", ""], ["Salakhutdinov", "Ruslan", ""], ["Zhang", "Tong", ""]]}, {"id": "0905.3428", "submitter": "Umaa Rebbapragada", "authors": "Umaa Rebbapragada, Pavlos Protopapas, Carla E. Brodley, Charles Alcock", "title": "Finding Anomalous Periodic Time Series: An Application to Catalogs of\n  Periodic Variable Stars", "comments": null, "journal-ref": "Machine Learning 74:281,2009", "doi": "10.1007/s10994-008-5093-3", "report-no": null, "categories": "cs.LG astro-ph.IM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catalogs of periodic variable stars contain large numbers of periodic\nlight-curves (photometric time series data from the astrophysics domain).\nSeparating anomalous objects from well-known classes is an important step\ntowards the discovery of new classes of astronomical objects. Most anomaly\ndetection methods for time series data assume either a single continuous time\nseries or a set of time series whose periods are aligned. Light-curve data\nprecludes the use of these methods as the periods of any given pair of\nlight-curves may be out of sync. One may use an existing anomaly detection\nmethod if, prior to similarity calculation, one performs the costly act of\naligning two light-curves, an operation that scales poorly to massive data\nsets. This paper presents PCAD, an unsupervised anomaly detection method for\nlarge sets of unsynchronized periodic time-series data, that outputs a ranked\nlist of both global and local anomalies. It calculates its anomaly score for\neach light-curve in relation to a set of centroids produced by a modified\nk-means clustering algorithm. Our method is able to scale to large data sets\nthrough the use of sampling. We validate our method on both light-curve data\nand other time series data sets. We demonstrate its effectiveness at finding\nknown anomalies, and discuss the effect of sample size and number of centroids\non our results. We compare our method to naive solutions and existing time\nseries anomaly detection methods for unphased data, and show that PCAD's\nreported anomalies are comparable to or better than all other methods. Finally,\nastrophysicists on our team have verified that PCAD finds true anomalies that\nmight be indicative of novel astrophysical phenomena.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2009 03:05:38 GMT"}], "update_date": "2009-06-19", "authors_parsed": [["Rebbapragada", "Umaa", ""], ["Protopapas", "Pavlos", ""], ["Brodley", "Carla E.", ""], ["Alcock", "Charles", ""]]}, {"id": "0905.3527", "submitter": "Shu Tanaka", "authors": "Kenichi Kurihara, Shu Tanaka, and Seiji Miyashita", "title": "Quantum Annealing for Clustering", "comments": "8 pages, 6 figures, Proceedings of the 25th Conference on Uncertainty\n  in Artificial Intelligence (UAI 2009) accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies quantum annealing (QA) for clustering, which can be seen\nas an extension of simulated annealing (SA). We derive a QA algorithm for\nclustering and propose an annealing schedule, which is crucial in practice.\nExperiments show the proposed QA algorithm finds better clustering assignments\nthan SA. Furthermore, QA is as easy as SA to implement.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2009 17:01:12 GMT"}, {"version": "v2", "created": "Thu, 28 May 2009 15:36:23 GMT"}], "update_date": "2009-05-28", "authors_parsed": [["Kurihara", "Kenichi", ""], ["Tanaka", "Shu", ""], ["Miyashita", "Seiji", ""]]}, {"id": "0905.3528", "submitter": "Shu Tanaka", "authors": "Issei Sato, Kenichi Kurihara, Shu Tanaka, Hiroshi Nakagawa, and Seiji\n  Miyashita", "title": "Quantum Annealing for Variational Bayes Inference", "comments": "9 pages, 4 figures, Proceedings of the 25th Conference on Uncertainty\n  in Artificial Intelligence (UAI 2009) accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents studies on a deterministic annealing algorithm based on\nquantum annealing for variational Bayes (QAVB) inference, which can be seen as\nan extension of the simulated annealing for variational Bayes (SAVB) inference.\nQAVB is as easy as SAVB to implement. Experiments revealed QAVB finds a better\nlocal optimum than SAVB in terms of the variational free energy in latent\nDirichlet allocation (LDA).\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2009 17:01:28 GMT"}, {"version": "v2", "created": "Wed, 27 May 2009 17:45:45 GMT"}, {"version": "v3", "created": "Thu, 28 May 2009 15:44:30 GMT"}], "update_date": "2009-05-28", "authors_parsed": [["Sato", "Issei", ""], ["Kurihara", "Kenichi", ""], ["Tanaka", "Shu", ""], ["Nakagawa", "Hiroshi", ""], ["Miyashita", "Seiji", ""]]}, {"id": "0905.3640", "submitter": "Mattheos Protopapas", "authors": "Mattheos K. Protopapas, Elias B. Kosmatopoulos, Francesco Battaglia", "title": "Coevolutionary Genetic Algorithms for Establishing Nash Equilibrium in\n  Symmetric Cournot Games", "comments": "18 pages, 4 figures", "journal-ref": "Advances in Decision Sciences, vol. 2010, Article ID 573107", "doi": "10.1155/2010/573107", "report-no": null, "categories": "cs.GT cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We use co-evolutionary genetic algorithms to model the players' learning\nprocess in several Cournot models, and evaluate them in terms of their\nconvergence to the Nash Equilibrium. The \"social-learning\" versions of the two\nco-evolutionary algorithms we introduce, establish Nash Equilibrium in those\nmodels, in contrast to the \"individual learning\" versions which, as we see\nhere, do not imply the convergence of the players' strategies to the Nash\noutcome. When players use \"canonical co-evolutionary genetic algorithms\" as\nlearning algorithms, the process of the game is an ergodic Markov Chain, and\ntherefore we analyze simulation results using both the relevant methodology and\nmore general statistical tests, to find that in the \"social\" case, states\nleading to NE play are highly frequent at the stationary distribution of the\nchain, in contrast to the \"individual learning\" case, when NE is not reached at\nall in our simulations; to find that the expected Hamming distance of the\nstates at the limiting distribution from the \"NE state\" is significantly\nsmaller in the \"social\" than in the \"individual learning case\"; to estimate the\nexpected time that the \"social\" algorithms need to get to the \"NE state\" and\nverify their robustness and finally to show that a large fraction of the games\nplayed are indeed at the Nash Equilibrium.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2009 19:07:21 GMT"}], "update_date": "2010-05-13", "authors_parsed": [["Protopapas", "Mattheos K.", ""], ["Kosmatopoulos", "Elias B.", ""], ["Battaglia", "Francesco", ""]]}, {"id": "0905.4022", "submitter": "Paramveer Dhillon", "authors": "Paramveer S. Dhillon, Dean Foster and Lyle Ungar", "title": "Transfer Learning Using Feature Selection", "comments": "Masters' Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present three related ways of using Transfer Learning to improve feature\nselection. The three methods address different problems, and hence share\ndifferent kinds of information between tasks or feature classes, but all three\nare based on the information theoretic Minimum Description Length (MDL)\nprinciple and share the same underlying Bayesian interpretation. The first\nmethod, MIC, applies when predictive models are to be built simultaneously for\nmultiple tasks (``simultaneous transfer'') that share the same set of features.\nMIC allows each feature to be added to none, some, or all of the task models\nand is most beneficial for selecting a small set of predictive features from a\nlarge pool of features, as is common in genomic and biological datasets. Our\nsecond method, TPC (Three Part Coding), uses a similar methodology for the case\nwhen the features can be divided into feature classes. Our third method,\nTransfer-TPC, addresses the ``sequential transfer'' problem in which the task\nto which we want to transfer knowledge may not be known in advance and may have\ndifferent amounts of data than the other tasks. Transfer-TPC is most beneficial\nwhen we want to transfer knowledge between tasks which have unequal amounts of\nlabeled data, for example the data for disambiguating the senses of different\nverbs. We demonstrate the effectiveness of these approaches with experimental\nresults on real world data pertaining to genomics and to Word Sense\nDisambiguation (WSD).\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2009 14:29:59 GMT"}], "update_date": "2009-05-26", "authors_parsed": [["Dhillon", "Paramveer S.", ""], ["Foster", "Dean", ""], ["Ungar", "Lyle", ""]]}]