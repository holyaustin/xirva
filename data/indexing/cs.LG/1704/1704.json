[{"id": "1704.00003", "submitter": "Hsiao-Yu Tung", "authors": "Hsiao-Yu Fish Tung and Chao-Yuan Wu and Manzil Zaheer and Alexander J.\n  Smola", "title": "Spectral Methods for Nonparametric Models", "comments": "Keywords: Spectral Methods, Indian Buffet Process, Hierarchical\n  Dirichlet Process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric models are versatile, albeit computationally expensive, tool\nfor modeling mixture models. In this paper, we introduce spectral methods for\nthe two most popular nonparametric models: the Indian Buffet Process (IBP) and\nthe Hierarchical Dirichlet Process (HDP). We show that using spectral methods\nfor the inference of nonparametric models are computationally and statistically\nefficient. In particular, we derive the lower-order moments of the IBP and the\nHDP, propose spectral algorithms for both models, and provide reconstruction\nguarantees for the algorithms. For the HDP, we further show that applying\nhierarchical models on dataset with hierarchical structure, which can be solved\nwith the generalized spectral HDP, produces better solutions to that of flat\nmodels regarding likelihood performance.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 03:50:03 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Tung", "Hsiao-Yu Fish", ""], ["Wu", "Chao-Yuan", ""], ["Zaheer", "Manzil", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1704.00023", "submitter": "Tegjyot Singh Sethi", "authors": "Tegjyot Singh Sethi, Mehmed Kantardzic", "title": "On the Reliable Detection of Concept Drift from Streaming Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers deployed in the real world operate in a dynamic environment,\nwhere the data distribution can change over time. These changes, referred to as\nconcept drift, can cause the predictive performance of the classifier to drop\nover time, thereby making it obsolete. To be of any real use, these classifiers\nneed to detect drifts and be able to adapt to them, over time. Detecting drifts\nhas traditionally been approached as a supervised task, with labeled data\nconstantly being used for validating the learned model. Although effective in\ndetecting drifts, these techniques are impractical, as labeling is a difficult,\ncostly and time consuming activity. On the other hand, unsupervised change\ndetection techniques are unreliable, as they produce a large number of false\nalarms. The inefficacy of the unsupervised techniques stems from the exclusion\nof the characteristics of the learned classifier, from the detection process.\nIn this paper, we propose the Margin Density Drift Detection (MD3) algorithm,\nwhich tracks the number of samples in the uncertainty region of a classifier,\nas a metric to detect drift. The MD3 algorithm is a distribution independent,\napplication independent, model independent, unsupervised and incremental\nalgorithm for reliably detecting drifts from data streams. Experimental\nevaluation on 6 drift induced datasets and 4 additional datasets from the\ncybersecurity domain demonstrates that the MD3 approach can reliably detect\ndrifts, with significantly fewer false alarms compared to unsupervised feature\nbased drift detectors. The reduced false alarms enables the signaling of drifts\nonly when they are most likely to affect classification performance. As such,\nthe MD3 approach leads to a detection scheme which is credible, label efficient\nand general in its applicability.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 18:55:48 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Sethi", "Tegjyot Singh", ""], ["Kantardzic", "Mehmed", ""]]}, {"id": "1704.00028", "submitter": "Ishaan Gulrajani", "authors": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin,\n  Aaron Courville", "title": "Improved Training of Wasserstein GANs", "comments": "NIPS camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are powerful generative models, but\nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN)\nmakes progress toward stable training of GANs, but sometimes can still generate\nonly low-quality samples or fail to converge. We find that these problems are\noften due to the use of weight clipping in WGAN to enforce a Lipschitz\nconstraint on the critic, which can lead to undesired behavior. We propose an\nalternative to clipping weights: penalize the norm of gradient of the critic\nwith respect to its input. Our proposed method performs better than standard\nWGAN and enables stable training of a wide variety of GAN architectures with\nalmost no hyperparameter tuning, including 101-layer ResNets and language\nmodels over discrete data. We also achieve high quality generations on CIFAR-10\nand LSUN bedrooms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 19:25:00 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 17:52:41 GMT"}, {"version": "v3", "created": "Mon, 25 Dec 2017 23:03:49 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Gulrajani", "Ishaan", ""], ["Ahmed", "Faruk", ""], ["Arjovsky", "Martin", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1704.00103", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Theerasit Issaranon, David Forsyth", "title": "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly", "comments": "Accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to produce a network where current methods such as\nDeepFool have great difficulty producing adversarial samples. Our construction\nsuggests some insights into how deep networks work. We provide a reasonable\nanalyses that our construction is difficult to defeat, and show experimentally\nthat our method is hard to defeat with both Type I and Type II attacks using\nseveral standard networks and datasets. This SafetyNet architecture is used to\nan important and novel application SceneProof, which can reliably detect\nwhether an image is a picture of a real scene or not. SceneProof applies to\nimages captured with depth maps (RGBD images) and checks if a pair of image and\ndepth map is consistent. It relies on the relative difficulty of producing\nnaturalistic depth maps for images in post processing. We demonstrate that our\nSafetyNet is robust to adversarial examples built from currently known\nattacking approaches.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 02:12:40 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 05:59:38 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Lu", "Jiajun", ""], ["Issaranon", "Theerasit", ""], ["Forsyth", "David", ""]]}, {"id": "1704.00108", "submitter": "Wang Chi Cheung", "authors": "Wang Chi Cheung, David Simchi-Levi", "title": "Assortment Optimization under Unknown MultiNomial Logit Choice Models", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by e-commerce, we study the online assortment optimization problem.\nThe seller offers an assortment, i.e. a subset of products, to each arriving\ncustomer, who then purchases one or no product from her offered assortment. A\ncustomer's purchase decision is governed by the underlying MultiNomial Logit\n(MNL) choice model. The seller aims to maximize the total revenue in a finite\nsales horizon, subject to resource constraints and uncertainty in the MNL\nchoice model. We first propose an efficient online policy which incurs a regret\n$\\tilde{O}(T^{2/3})$, where $T$ is the number of customers in the sales\nhorizon. Then, we propose a UCB policy that achieves a regret\n$\\tilde{O}(T^{1/2})$. Both regret bounds are sublinear in the number of\nassortments.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 02:37:53 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Cheung", "Wang Chi", ""], ["Simchi-Levi", "David", ""]]}, {"id": "1704.00109", "submitter": "Gao Huang", "authors": "Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft,\n  Kilian Q. Weinberger", "title": "Snapshot Ensembles: Train 1, get M for free", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of neural networks are known to be much more robust and accurate\nthan individual networks. However, training multiple deep networks for model\naveraging is computationally expensive. In this paper, we propose a method to\nobtain the seemingly contradictory goal of ensembling multiple neural networks\nat no additional training cost. We achieve this goal by training a single\nneural network, converging to several local minima along its optimization path\nand saving the model parameters. To obtain repeated rapid convergence, we\nleverage recent work on cyclic learning rate schedules. The resulting\ntechnique, which we refer to as Snapshot Ensembling, is simple, yet\nsurprisingly effective. We show in a series of experiments that our approach is\ncompatible with diverse network architectures and learning tasks. It\nconsistently yields lower error rates than state-of-the-art single models at no\nadditional training cost, and compares favorably with traditional network\nensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain\nerror rates of 3.4% and 17.4% respectively.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 02:42:55 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Huang", "Gao", ""], ["Li", "Yixuan", ""], ["Pleiss", "Geoff", ""], ["Liu", "Zhuang", ""], ["Hopcroft", "John E.", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1704.00158", "submitter": "Ozsel Kilinc", "authors": "Ozsel Kilinc, Ismail Uysal", "title": "Clustering-based Source-aware Assessment of True Robustness for Learning\n  Models", "comments": "Submitted to UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel validation framework to measure the true robustness of\nlearning models for real-world applications by creating source-inclusive and\nsource-exclusive partitions in a dataset via clustering. We develop a\nrobustness metric derived from source-aware lower and upper bounds of model\naccuracy even when data source labels are not readily available. We clearly\ndemonstrate that even on a well-explored dataset like MNIST, challenging\ntraining scenarios can be constructed under the proposed assessment framework\nfor two separate yet equally important applications: i) more rigorous learning\nmodel comparison and ii) dataset adequacy evaluation. In addition, our findings\nnot only promise a more complete identification of trade-offs between model\ncomplexity, accuracy and robustness but can also help researchers optimize\ntheir efforts in data collection by identifying the less robust and more\nchallenging class labels.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 11:58:24 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Kilinc", "Ozsel", ""], ["Uysal", "Ismail", ""]]}, {"id": "1704.00196", "submitter": "Patrick Johnstone", "authors": "Patrick R. Johnstone and Pierre Moulin", "title": "Faster Subgradient Methods for Functions with H\\\"olderian Growth", "comments": "50 pages. First revised version (under submission to Math\n  Programming)", "journal-ref": "Math. Program. 180, 417-450 (2020)", "doi": "10.1007/s10107-018-01361-0", "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this manuscript is to derive new convergence results for\nseveral subgradient methods applied to minimizing nonsmooth convex functions\nwith H\\\"olderian growth. The growth condition is satisfied in many applications\nand includes functions with quadratic growth and weakly sharp minima as special\ncases. To this end there are three main contributions. First, for a constant\nand sufficiently small stepsize, we show that the subgradient method achieves\nlinear convergence up to a certain region including the optimal set, with error\nof the order of the stepsize. Second, if appropriate problem parameters are\nknown, we derive a decaying stepsize which obtains a much faster convergence\nrate than is suggested by the classical $O(1/\\sqrt{k})$ result for the\nsubgradient method. Thirdly we develop a novel \"descending stairs\" stepsize\nwhich obtains this faster convergence rate and also obtains linear convergence\nfor the special case of weakly sharp functions. We also develop an adaptive\nvariant of the \"descending stairs\" stepsize which achieves the same convergence\nrate without requiring an error bound constant which is difficult to estimate\nin practice.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 16:28:27 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 16:33:55 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2018 21:34:42 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Johnstone", "Patrick R.", ""], ["Moulin", "Pierre", ""]]}, {"id": "1704.00217", "submitter": "Zhiting Hu", "authors": "Lianhui Qin, Zhisong Zhang, Hai Zhao, Zhiting Hu, Eric P. Xing", "title": "Adversarial Connective-exploiting Networks for Implicit Discourse\n  Relation Classification", "comments": "To appear in ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit discourse relation classification is of great challenge due to the\nlack of connectives as strong linguistic cues, which motivates the use of\nannotated implicit connectives to improve the recognition. We propose a feature\nimitation framework in which an implicit relation network is driven to learn\nfrom another neural network with access to connectives, and thus encouraged to\nextract similarly salient features for accurate classification. We develop an\nadversarial model to enable an adaptive imitation scheme through competition\nbetween the implicit network and a rival feature discriminator. Our method\neffectively transfers discriminability of connectives to the implicit features,\nand achieves state-of-the-art performance on the PDTB benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 19:29:21 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Qin", "Lianhui", ""], ["Zhang", "Zhisong", ""], ["Zhao", "Hai", ""], ["Hu", "Zhiting", ""], ["Xing", "Eric P.", ""]]}, {"id": "1704.00227", "submitter": "Michael Sandbichler", "authors": "Michael Sandbichler, Karin Schnass", "title": "Online and Stable Learning of Analysis Operators", "comments": "21 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper four iterative algorithms for learning analysis operators are\npresented. They are built upon the same optimisation principle underlying both\nAnalysis K-SVD and Analysis SimCO. The Forward and Sequential Analysis Operator\nLearning (FAOL and SAOL) algorithms are based on projected gradient descent\nwith optimally chosen step size. The Implicit AOL (IAOL) algorithm is inspired\nby the implicit Euler scheme for solving ordinary differential equations and\ndoes not require to choose a step size. The fourth algorithm, Singular Value\nAOL (SVAOL), uses a similar strategy as Analysis K-SVD while avoiding its high\ncomputational cost. All algorithms are proven to decrease or preserve the\ntarget function in each step and a characterisation of their stationary points\nis provided. Further they are tested on synthetic and image data, compared to\nAnalysis SimCO and found to give better recovery rates and faster decay of the\nobjective function respectively. In a final denoising experiment the presented\nalgorithms are again shown to perform similar to or better than the\nstate-of-the-art algorithm ASimCO.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 20:48:21 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 15:43:50 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Sandbichler", "Michael", ""], ["Schnass", "Karin", ""]]}, {"id": "1704.00260", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Kevin Shih, Saurabh Singh, and Derek Hoiem", "title": "Aligned Image-Word Representations Improve Inductive Transfer Across\n  Vision-Language Tasks", "comments": "Accepted in ICCV 2017. The arxiv version has an extra analysis on\n  correlation with human attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal of computer vision is to build systems that learn visual\nrepresentations over time that can be applied to many tasks. In this paper, we\ninvestigate a vision-language embedding as a core representation and show that\nit leads to better cross-task transfer than standard multi-task learning. In\nparticular, the task of visual recognition is aligned to the task of visual\nquestion answering by forcing each to use the same word-region embeddings. We\nshow this leads to greater inductive transfer from recognition to VQA than\nstandard multitask learning. Visual recognition also improves, especially for\ncategories that have relatively few recognition training labels but appear\noften in the VQA setting. Thus, our paper takes a small step towards creating\nmore general vision systems by showing the benefit of interpretable, flexible,\nand trainable core representations.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 08:01:30 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 05:34:24 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Gupta", "Tanmay", ""], ["Shih", "Kevin", ""], ["Singh", "Saurabh", ""], ["Hoiem", "Derek", ""]]}, {"id": "1704.00362", "submitter": "Geoffrey Webb", "authors": "Geoffrey I. Webb, Loong Kuan Lee, Fran\\c{c}ois Petitjean, Bart\n  Goethals", "title": "Understanding Concept Drift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept drift is a major issue that greatly affects the accuracy and\nreliability of many real-world applications of machine learning. We argue that\nto tackle concept drift it is important to develop the capacity to describe and\nanalyze it. We propose tools for this purpose, arguing for the importance of\nquantitative descriptions of drift in marginal distributions. We present\nquantitative drift analysis techniques along with methods for communicating\ntheir results. We demonstrate their effectiveness by application to three\nreal-world learning tasks.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 20:57:04 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Webb", "Geoffrey I.", ""], ["Lee", "Loong Kuan", ""], ["Petitjean", "Fran\u00e7ois", ""], ["Goethals", "Bart", ""]]}, {"id": "1704.00367", "submitter": "U. N. Niranjan", "authors": "U.N. Niranjan, Arun Rajkumar, Theja Tulabandhula", "title": "Provable Inductive Robust PCA via Iterative Hard Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust PCA problem, wherein, given an input data matrix that is the\nsuperposition of a low-rank matrix and a sparse matrix, we aim to separate out\nthe low-rank and sparse components, is a well-studied problem in machine\nlearning. One natural question that arises is that, as in the inductive\nsetting, if features are provided as input as well, can we hope to do better?\nAnswering this in the affirmative, the main goal of this paper is to study the\nrobust PCA problem while incorporating feature information. In contrast to\nprevious works in which recovery guarantees are based on the convex relaxation\nof the problem, we propose a simple iterative algorithm based on\nhard-thresholding of appropriate residuals. Under weaker assumptions than\nprevious works, we prove the global convergence of our iterative procedure;\nmoreover, it admits a much faster convergence rate and lesser computational\ncomplexity per iteration. In practice, through systematic synthetic and real\ndata simulations, we confirm our theoretical findings regarding improvements\nobtained by using feature information.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 21:32:34 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 19:47:57 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Niranjan", "U. N.", ""], ["Rajkumar", "Arun", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1704.00389", "submitter": "Yi Zhu", "authors": "Yi Zhu, Zhenzhong Lan, Shawn Newsam, Alexander G. Hauptmann", "title": "Hidden Two-Stream Convolutional Networks for Action Recognition", "comments": "Accepted at ACCV 2018, camera ready. Code available at\n  https://github.com/bryanyzhu/Hidden-Two-Stream", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing videos of human actions involves understanding the temporal\nrelationships among video frames. State-of-the-art action recognition\napproaches rely on traditional optical flow estimation methods to pre-compute\nmotion information for CNNs. Such a two-stage approach is computationally\nexpensive, storage demanding, and not end-to-end trainable. In this paper, we\npresent a novel CNN architecture that implicitly captures motion information\nbetween adjacent frames. We name our approach hidden two-stream CNNs because it\nonly takes raw video frames as input and directly predicts action classes\nwithout explicitly computing optical flow. Our end-to-end approach is 10x\nfaster than its two-stage baseline. Experimental results on four challenging\naction recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show\nthat our approach significantly outperforms the previous best real-time\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 23:39:51 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 21:48:54 GMT"}, {"version": "v3", "created": "Sun, 22 Oct 2017 03:53:21 GMT"}, {"version": "v4", "created": "Tue, 30 Oct 2018 04:55:03 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhu", "Yi", ""], ["Lan", "Zhenzhong", ""], ["Newsam", "Shawn", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1704.00445", "submitter": "Sayak Ray Chowdhury", "authors": "Sayak Ray Chowdhury and Aditya Gopalan", "title": "On Kernelized Multi-armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic bandit problem with a continuous set of arms, with\nthe expected reward function over the arms assumed to be fixed but unknown. We\nprovide two new Gaussian process-based algorithms for continuous bandit\noptimization-Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and\nderive corresponding regret bounds. Specifically, the bounds hold when the\nexpected reward function belongs to the reproducing kernel Hilbert space (RKHS)\nthat naturally corresponds to a Gaussian process kernel used as input by the\nalgorithms. Along the way, we derive a new self-normalized concentration\ninequality for vector- valued martingales of arbitrary, possibly infinite,\ndimension. Finally, experimental evaluation and comparisons to existing\nalgorithms on synthetic and real-world environments are carried out that\nhighlight the favorable gains of the proposed strategies in many cases.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 06:47:42 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 09:04:40 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Chowdhury", "Sayak Ray", ""], ["Gopalan", "Aditya", ""]]}, {"id": "1704.00454", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun", "title": "Clustering in Hilbert simplex geometry", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering categorical distributions in the probability simplex is a\nfundamental task met in many applications dealing with normalized histograms.\nTraditionally, the differential-geometric structures of the probability simplex\nhave been used either by (i) setting the Riemannian metric tensor to the Fisher\ninformation matrix of the categorical distributions, or (ii) defining the\ndualistic information-geometric structure induced by a smooth dissimilarity\nmeasure, the Kullback-Leibler divergence. In this work, we introduce for this\nclustering task a novel computationally-friendly framework for modeling the\nprobability simplex termed {\\em Hilbert simplex geometry}. In the Hilbert\nsimplex geometry, the distance function is described by a polytope. We discuss\nthe pros and cons of those different statistical modelings, and benchmark\nexperimentally these geometries for center-based $k$-means and $k$-center\nclusterings. We show that Hilbert metric in the probability simplex satisfies\nthe property of information monotonicity. Furthermore, since a canonical\nHilbert metric distance can be defined on any bounded convex subset of the\nEuclidean space, we also consider Hilbert's projective geometry of the\nelliptope of correlation matrices and study its clustering performances.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 07:23:37 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 15:30:07 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 03:12:09 GMT"}, {"version": "v4", "created": "Mon, 26 Nov 2018 05:03:27 GMT"}, {"version": "v5", "created": "Wed, 12 Dec 2018 07:46:11 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""]]}, {"id": "1704.00485", "submitter": "Vraj Shah", "authors": "Vraj Shah, Arun Kumar, Xiaojin Zhu", "title": "Are Key-Foreign Key Joins Safe to Avoid when Learning High-Capacity\n  Classifiers?", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) over relational data is a booming area of the database\nindustry and academia. While several projects aim to build scalable and fast ML\nsystems, little work has addressed the pains of sourcing data and features for\nML tasks. Real-world relational databases typically have many tables (often,\ndozens) and data scientists often struggle to even obtain and join all possible\ntables that provide features for ML. In this context, Kumar et al. showed\nrecently that key-foreign key dependencies (KFKDs) between tables often lets us\navoid such joins without significantly affecting prediction accuracy--an idea\nthey called avoiding joins safely. While initially controversial, this idea has\nsince been used by multiple companies to reduce the burden of data sourcing for\nML. But their work applied only to linear classifiers. In this work, we verify\nif their results hold for three popular complex classifiers: decision trees,\nSVMs, and ANNs. We conduct an extensive experimental study using both\nreal-world datasets and simulations to analyze the effects of avoiding KFK\njoins on such models. Our results show that these high-capacity classifiers are\nsurprisingly and counter-intuitively more robust to avoiding KFK joins compared\nto linear classifiers, refuting an intuition from the prior work's analysis. We\nexplain this behavior intuitively and identify open questions at the\nintersection of data management and ML theoretical research. All of our code\nand datasets are available for download from\nhttp://cseweb.ucsd.edu/~arunkk/hamlet.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 09:16:58 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 04:02:56 GMT"}, {"version": "v3", "created": "Sun, 4 Jun 2017 19:02:20 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Shah", "Vraj", ""], ["Kumar", "Arun", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1704.00607", "submitter": "Jalal Etesami", "authors": "Jalal Etesami, Kun Zhang, Negar Kiyavash", "title": "A New Measure of Conditional Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring conditional dependencies among the variables of a network is of\ngreat interest to many disciplines. This paper studies some shortcomings of the\nexisting dependency measures in detecting direct causal influences or their\nlack of ability for group selection to capture strong dependencies and\naccordingly introduces a new statistical dependency measure to overcome them.\nThis measure is inspired by Dobrushin's coefficients and based on the fact that\nthere is no dependency between $X$ and $Y$ given another variable $Z$, if and\nonly if the conditional distribution of $Y$ given $X=x$ and $Z=z$ does not\nchange when $X$ takes another realization $x'$ while $Z$ takes the same\nrealization $z$. We show the advantages of this measure over the related\nmeasures in the literature. Moreover, we establish the connection between our\nmeasure and the integral probability metric (IPM) that helps to develop\nestimators of the measure with lower complexity compared to other relevant\ninformation theoretic based measures. Finally, we show the performance of this\nmeasure through numerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 06:20:26 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 13:14:33 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Etesami", "Jalal", ""], ["Zhang", "Kun", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1704.00637", "submitter": "Lars Maal{\\o}e", "authors": "Lars Maal{\\o}e and Marco Fraccaro and Ole Winther", "title": "Semi-Supervised Generation with Cluster-aware Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models trained with large amounts of unlabelled data have\nproven to be powerful within the domain of unsupervised learning. Many real\nlife data sets contain a small amount of labelled data points, that are\ntypically disregarded when training generative models. We propose the\nCluster-aware Generative Model, that uses unlabelled information to infer a\nlatent representation that models the natural clustering of the data, and\nadditional labelled data points to refine this clustering. The generative\nperformances of the model significantly improve when labelled information is\nexploited, obtaining a log-likelihood of -79.38 nats on permutation invariant\nMNIST, while also achieving competitive semi-supervised classification\naccuracies. The model can also be trained fully unsupervised, and still improve\nthe log-likelihood performance with respect to related methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:25:47 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Maal\u00f8e", "Lars", ""], ["Fraccaro", "Marco", ""], ["Winther", "Ole", ""]]}, {"id": "1704.00642", "submitter": "Timothy Cannings", "authors": "Timothy I. Cannings, Thomas B. Berrett and Richard J. Samworth", "title": "Local nearest neighbour classification with applications to\n  semi-supervised learning", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a new asymptotic expansion for the global excess risk of a\nlocal-$k$-nearest neighbour classifier, where the choice of $k$ may depend upon\nthe test point. This expansion elucidates conditions under which the dominant\ncontribution to the excess risk comes from the decision boundary of the optimal\nBayes classifier, but we also show that if these conditions are not satisfied,\nthen the dominant contribution may arise from the tails of the marginal\ndistribution of the features. Moreover, we prove that, provided the\n$d$-dimensional marginal distribution of the features has a finite $\\rho$th\nmoment for some $\\rho > 4$ (as well as other regularity conditions), a local\nchoice of $k$ can yield a rate of convergence of the excess risk of\n$O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard\n$k$-nearest neighbour classifier, our theory would require $d \\geq 5$ and $\\rho\n> 4d/(d-4)$ finite moments to achieve this rate. These results motivate a new\n$k$-nearest neighbour classifier for semi-supervised learning problems, where\nthe unlabelled data are used to obtain an estimate of the marginal feature\ndensity, and fewer neighbours are used for classification when this density\nestimate is small. Our worst-case rates are complemented by a minimax lower\nbound, which reveals that the local, semi-supervised $k$-nearest neighbour\nclassifier attains the minimax optimal rate over our classes for the excess\nrisk, up to a subpolynomial factor in $n$. These theoretical improvements over\nthe standard $k$-nearest neighbour classifier are also illustrated through a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:34:11 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 11:16:30 GMT"}, {"version": "v3", "created": "Sat, 18 May 2019 10:49:46 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Cannings", "Timothy I.", ""], ["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1704.00648", "submitter": "Eirikur Agustsson", "authors": "Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli,\n  Radu Timofte, Luca Benini and Luc Van Gool", "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to learn compressible representations in deep\narchitectures with an end-to-end training strategy. Our method is based on a\nsoft (continuous) relaxation of quantization and entropy, which we anneal to\ntheir discrete counterparts throughout training. We showcase this method for\ntwo challenging applications: Image compression and neural network compression.\nWhile these tasks have typically been approached with different methods, our\nsoft-to-hard quantization approach gives results competitive with the\nstate-of-the-art for both.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:39:56 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 09:18:22 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Agustsson", "Eirikur", ""], ["Mentzer", "Fabian", ""], ["Tschannen", "Michael", ""], ["Cavigelli", "Lukas", ""], ["Timofte", "Radu", ""], ["Benini", "Luca", ""], ["Van Gool", "Luc", ""]]}, {"id": "1704.00708", "submitter": "Chi Jin", "authors": "Rong Ge, Chi Jin, Yi Zheng", "title": "No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified\n  Geometric Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a new framework that captures the common landscape\nunderlying the common non-convex low-rank matrix problems including matrix\nsensing, matrix completion and robust PCA. In particular, we show for all above\nproblems (including asymmetric cases): 1) all local minima are also globally\noptimal; 2) no high-order saddle points exists. These results explain why\nsimple algorithms such as stochastic gradient descent have global converge, and\nefficiently optimize these non-convex objective functions in practice. Our\nframework connects and simplifies the existing analyses on optimization\nlandscapes for matrix sensing and symmetric matrix completion. The framework\nnaturally leads to new results for asymmetric matrix completion and robust PCA.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 17:49:02 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Ge", "Rong", ""], ["Jin", "Chi", ""], ["Zheng", "Yi", ""]]}, {"id": "1704.00756", "submitter": "Romain Laroche", "authors": "Romain Laroche and Mehdi Fatemi and Joshua Romoff and Harm van Seijen", "title": "Multi-Advisor Reinforcement Learning", "comments": "Submitted at ICLR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider tackling a single-agent RL problem by distributing it to $n$\nlearners. These learners, called advisors, endeavour to solve the problem from\na different focus. Their advice, taking the form of action values, is then\ncommunicated to an aggregator, which is in control of the system. We show that\nthe local planning method for the advisors is critical and that none of the\nones found in the literature is flawless: the egocentric planning overestimates\nvalues of states where the other advisors disagree, and the agnostic planning\nis inefficient around danger zones. We introduce a novel approach called\nempathic and discuss its theoretical aspects. We empirically examine and\nvalidate our theoretical findings on a fruit collection task.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 18:37:12 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 20:35:27 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Laroche", "Romain", ""], ["Fatemi", "Mehdi", ""], ["Romoff", "Joshua", ""], ["van Seijen", "Harm", ""]]}, {"id": "1704.00767", "submitter": "Iain Carmichael", "authors": "Iain Carmichael and J.S. Marron", "title": "Geometric Insights into Support Vector Machine Behavior using the KKT\n  Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) is a powerful and widely used classification\nalgorithm. This paper uses the Karush-Kuhn-Tucker conditions to provide\nrigorous mathematical proof for new insights into the behavior of SVM. These\ninsights provide perhaps unexpected relationships between SVM and two other\nlinear classifiers: the mean difference and the maximal data piling direction.\nFor example, we show that in many cases SVM can be viewed as a cropped version\nof these classifiers. By carefully exploring these connections we show how SVM\ntuning behavior is affected by characteristics including: balanced vs.\nunbalanced classes, low vs. high dimension, separable vs. non-separable data.\nThese results provide further insights into tuning SVM via cross-validation by\nexplaining observed pathological behavior and motivating improved\ncross-validation methodology. Finally, we also provide new results on the\ngeometry of complete data piling directions in high dimensional space.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:08:54 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 13:43:27 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Carmichael", "Iain", ""], ["Marron", "J. S.", ""]]}, {"id": "1704.00773", "submitter": "Thomas Nedelec", "authors": "Thomas Nedelec, Nicolas Le Roux and Vianney Perchet", "title": "A comparative study of counterfactual estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a comparative study of several widely used off-policy estimators\n(Empirical Average, Basic Importance Sampling and Normalized Importance\nSampling), detailing the different regimes where they are individually\nsuboptimal. We then exhibit properties optimal estimators should possess. In\nthe case where examples have been gathered using multiple policies, we show\nthat fused estimators dominate basic ones but can still be improved.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:16:06 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 08:00:07 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2019 13:58:22 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Nedelec", "Thomas", ""], ["Roux", "Nicolas Le", ""], ["Perchet", "Vianney", ""]]}, {"id": "1704.00783", "submitter": "Gopal P. Sarma", "authors": "Gopal P. Sarma", "title": "Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated\n  Volition", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  I make some basic observations about hard takeoff, value alignment, and\ncoherent extrapolated volition, concepts which have been central in analyses of\nsuperintelligent AI systems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:45:04 GMT"}, {"version": "v2", "created": "Sat, 21 Apr 2018 20:32:30 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Sarma", "Gopal P.", ""]]}, {"id": "1704.00784", "submitter": "Colin Raffel", "authors": "Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas\n  Eck", "title": "Online and Linear-Time Attention by Enforcing Monotonic Alignments", "comments": "ICML camera-ready version; 10 pages + 9 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network models with an attention mechanism have proven to be\nextremely effective on a wide variety of sequence-to-sequence problems.\nHowever, the fact that soft attention mechanisms perform a pass over the entire\ninput sequence when producing each element in the output sequence precludes\ntheir use in online settings and results in a quadratic time complexity. Based\non the insight that the alignment between input and output sequence elements is\nmonotonic in many problems of interest, we propose an end-to-end differentiable\nmethod for learning monotonic alignments which, at test time, enables computing\nattention online and in linear time. We validate our approach on sentence\nsummarization, machine translation, and online speech recognition problems and\nachieve results competitive with existing sequence-to-sequence models.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:45:27 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 21:14:58 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Raffel", "Colin", ""], ["Luong", "Minh-Thang", ""], ["Liu", "Peter J.", ""], ["Weiss", "Ron J.", ""], ["Eck", "Douglas", ""]]}, {"id": "1704.00794", "submitter": "Karl {\\O}yvind Mikalsen", "authors": "Karl {\\O}yvind Mikalsen, Filippo Maria Bianchi, Cristina Soguero-Ruiz\n  and Robert Jenssen", "title": "Time Series Cluster Kernel for Learning Similarities between\n  Multivariate Time Series with Missing Data", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-based approaches represent a promising direction for time series\nanalysis. However, many such methods rely on parameter tuning, and some have\nshortcomings if the time series are multivariate (MTS), due to dependencies\nbetween attributes, or the time series contain missing data. In this paper, we\naddress these challenges within the powerful context of kernel methods by\nproposing the robust \\emph{time series cluster kernel} (TCK). The approach\ntaken leverages the missing data handling properties of Gaussian mixture models\n(GMM) augmented with informative prior distributions. An ensemble learning\napproach is exploited to ensure robustness to parameters by combining the\nclustering results of many GMM to form the final kernel.\n  We evaluate the TCK on synthetic and real data and compare to other\nstate-of-the-art techniques. The experimental results demonstrate that the TCK\nis robust to parameter choices, provides competitive results for MTS without\nmissing data and outstanding results for missing data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 20:16:58 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 12:23:24 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Mikalsen", "Karl \u00d8yvind", ""], ["Bianchi", "Filippo Maria", ""], ["Soguero-Ruiz", "Cristina", ""], ["Jenssen", "Robert", ""]]}, {"id": "1704.00805", "submitter": "Bolin Gao", "authors": "Bolin Gao, Lacra Pavel", "title": "On the Properties of the Softmax Function with Application in Game\n  Theory and Reinforcement Learning", "comments": "10 pages, 4 figures. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we utilize results from convex analysis and monotone operator\ntheory to derive additional properties of the softmax function that have not\nyet been covered in the existing literature. In particular, we show that the\nsoftmax function is the monotone gradient map of the log-sum-exp function. By\nexploiting this connection, we show that the inverse temperature parameter\ndetermines the Lipschitz and co-coercivity properties of the softmax function.\nWe then demonstrate the usefulness of these properties through an application\nin game-theoretic reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 20:50:29 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 22:41:48 GMT"}, {"version": "v3", "created": "Thu, 28 Dec 2017 06:57:41 GMT"}, {"version": "v4", "created": "Tue, 21 Aug 2018 00:02:44 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Gao", "Bolin", ""], ["Pavel", "Lacra", ""]]}, {"id": "1704.01041", "submitter": "Yan Shuo Tan", "authors": "Yan Shuo Tan, Roman Vershynin", "title": "Polynomial Time and Sample Complexity for Non-Gaussian Component\n  Analysis: Spectral Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Non-Gaussian Component Analysis (NGCA) is about finding a\nmaximal low-dimensional subspace $E$ in $\\mathbb{R}^n$ so that data points\nprojected onto $E$ follow a non-gaussian distribution. Although this is an\nappropriate model for some real world data analysis problems, there has been\nlittle progress on this problem over the last decade.\n  In this paper, we attempt to address this state of affairs in two ways.\nFirst, we give a new characterization of standard gaussian distributions in\nhigh-dimensions, which lead to effective tests for non-gaussianness. Second, we\npropose a simple algorithm, \\emph{Reweighted PCA}, as a method for solving the\nNGCA problem. We prove that for a general unknown non-gaussian distribution,\nthis algorithm recovers at least one direction in $E$, with sample and time\ncomplexity depending polynomially on the dimension of the ambient space. We\nconjecture that the algorithm actually recovers the entire $E$.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 14:46:00 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Tan", "Yan Shuo", ""], ["Vershynin", "Roman", ""]]}, {"id": "1704.01079", "submitter": "Tuo Zhao", "authors": "Haotian Pang, Robert Vanderbei, Han Liu, Tuo Zhao", "title": "Homotopy Parametric Simplex Method for Sparse Learning", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional sparse learning has imposed a great computational challenge\nto large scale data analysis. In this paper, we are interested in a broad class\nof sparse learning approaches formulated as linear programs parametrized by a\n{\\em regularization factor}, and solve them by the parametric simplex method\n(PSM). Our parametric simplex method offers significant advantages over other\ncompeting methods: (1) PSM naturally obtains the complete solution path for all\nvalues of the regularization parameter; (2) PSM provides a high precision dual\ncertificate stopping criterion; (3) PSM yields sparse solutions through very\nfew iterations, and the solution sparsity significantly reduces the\ncomputational cost per iteration. Particularly, we demonstrate the superiority\nof PSM over various sparse learning approaches, including Dantzig selector for\nsparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME\nfor sparse precision matrix estimation, sparse differential network estimation,\nand sparse Linear Programming Discriminant (LPD) analysis. We then provide\nsufficient conditions under which PSM always outputs sparse solutions such that\nits computational performance can be significantly boosted. Thorough numerical\nexperiments are provided to demonstrate the outstanding performance of the PSM\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 15:56:55 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 18:11:14 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Pang", "Haotian", ""], ["Vanderbei", "Robert", ""], ["Liu", "Han", ""], ["Zhao", "Tuo", ""]]}, {"id": "1704.01087", "submitter": "Feras Saad", "authors": "Feras Saad, Leonardo Casarsa, Vikash Mansinghka", "title": "Probabilistic Search for Structured Data via Probabilistic Programming\n  and Nonparametric Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases are widespread, yet extracting relevant data can be difficult.\nWithout substantial domain knowledge, multivariate search queries often return\nsparse or uninformative results. This paper introduces an approach for\nsearching structured data based on probabilistic programming and nonparametric\nBayes. Users specify queries in a probabilistic language that combines standard\nSQL database search operators with an information theoretic ranking function\ncalled predictive relevance. Predictive relevance can be calculated by a fast\nsparse matrix algorithm based on posterior samples from CrossCat, a\nnonparametric Bayesian model for high-dimensional, heterogeneously-typed data\ntables. The result is a flexible search technique that applies to a broad class\nof information retrieval problems, which we integrate into BayesDB, a\nprobabilistic programming platform for probabilistic data analysis. This paper\ndemonstrates applications to databases of US colleges, global macroeconomic\nindicators of public health, and classic cars. We found that human evaluators\noften prefer the results from probabilistic search to results from a standard\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 16:18:07 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Saad", "Feras", ""], ["Casarsa", "Leonardo", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1704.01133", "submitter": "Dong Ki Kim", "authors": "Dong-Ki Kim and Matthew R. Walter", "title": "Satellite Image-based Localization via Learned Embeddings", "comments": "To be published in IEEE International Conference on Robotics and\n  Automation (ICRA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a vision-based method that localizes a ground vehicle using\npublicly available satellite imagery as the only prior knowledge of the\nenvironment. Our approach takes as input a sequence of ground-level images\nacquired by the vehicle as it navigates, and outputs an estimate of the\nvehicle's pose relative to a georeferenced satellite image. We overcome the\nsignificant viewpoint and appearance variations between the images through a\nneural multi-view model that learns location-discriminative embeddings in which\nground-level images are matched with their corresponding satellite view of the\nscene. We use this learned function as an observation model in a filtering\nframework to maintain a distribution over the vehicle's pose. We evaluate our\nmethod on different benchmark datasets and demonstrate its ability localize\nground-level images in environments novel relative to training, despite the\nchallenges of significant viewpoint and appearance variations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 18:03:55 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Kim", "Dong-Ki", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1704.01137", "submitter": "Swagath Venkataramani", "authors": "Sanjay Ganapathy, Swagath Venkataramani, Balaraman Ravindran, Anand\n  Raghunathan", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety\nof machine learning tasks and are deployed in increasing numbers of products\nand services. However, the computational requirements of training and\nevaluating large-scale DNNs are growing at a much faster pace than the\ncapabilities of the underlying hardware platforms that they are executed upon.\nIn this work, we propose Dynamic Variable Effort Deep Neural Networks\n(DyVEDeep) to reduce the computational requirements of DNNs during inference.\nPrevious efforts propose specialized hardware implementations for DNNs,\nstatically prune the network, or compress the weights. Complementary to these\napproaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in\nthe inputs to DNNs to improve their compute efficiency with comparable\nclassification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms\nthat, in the course of processing an input, identify how critical a group of\ncomputations are to classify the input. DyVEDeep dynamically focuses its\ncompute effort only on the critical computa- tions, while skipping or\napproximating the rest. We propose 3 effort knobs that operate at different\nlevels of granularity viz. neuron, feature and layer levels. We build DyVEDeep\nversions for 5 popular image recognition benchmarks - one for CIFAR-10 and four\nfor ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across\nall benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar\noperations, which translates to 1.8x-2.3x performance improvement over a\nCaffe-based implementation, with < 0.5% loss in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 18:14:02 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Ganapathy", "Sanjay", ""], ["Venkataramani", "Swagath", ""], ["Ravindran", "Balaraman", ""], ["Raghunathan", "Anand", ""]]}, {"id": "1704.01155", "submitter": "Weilin Xu", "authors": "Weilin Xu, David Evans, Yanjun Qi", "title": "Feature Squeezing: Detecting Adversarial Examples in Deep Neural\n  Networks", "comments": "To appear in Network and Distributed Systems Security Symposium\n  (NDSS) 2018", "journal-ref": null, "doi": "10.14722/ndss.2018.23198", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks (DNNs) have achieved great success in many\ntasks, they can often be fooled by \\emph{adversarial examples} that are\ngenerated by adding small but purposeful distortions to natural examples.\nPrevious studies to defend against adversarial examples mostly focused on\nrefining the DNN models, but have either shown limited success or required\nexpensive computation. We propose a new strategy, \\emph{feature squeezing},\nthat can be used to harden DNN models by detecting adversarial examples.\nFeature squeezing reduces the search space available to an adversary by\ncoalescing samples that correspond to many different feature vectors in the\noriginal space into a single sample. By comparing a DNN model's prediction on\nthe original input with that on squeezed inputs, feature squeezing detects\nadversarial examples with high accuracy and few false positives. This paper\nexplores two feature squeezing methods: reducing the color bit depth of each\npixel and spatial smoothing. These simple strategies are inexpensive and\ncomplementary to other defenses, and can be combined in a joint detection\nframework to achieve high detection rates against state-of-the-art attacks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 18:56:53 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 23:45:08 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Xu", "Weilin", ""], ["Evans", "David", ""], ["Qi", "Yanjun", ""]]}, {"id": "1704.01184", "submitter": "Chao Lan", "authors": "Chao Lan, Sai Nivedita Chandrasekaran, Jun Huan", "title": "On the Unreported-Profile-is-Negative Assumption for Predictive\n  Cheminformatics", "comments": "the quality of the current version is unsatisfactory. we decide to\n  withdraw the manuscript. thank you", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cheminformatics, compound-target binding profiles has been a main source\nof data for research. For data repositories that only provide positive\nprofiles, a popular assumption is that unreported profiles are all negative. In\nthis paper, we caution audience not to take this assumption for granted, and\npresent empirical evidence of its ineffectiveness from a machine learning\nperspective. Our examination is based on a setting where binding profiles are\nused as features to train predictive models; we show (1) prediction performance\ndegrades when the assumption fails and (2) explicit recovery of unreported\nprofiles improves prediction performance. In particular, we propose a framework\nthat jointly recovers profiles and learns predictive model, and show it\nachieves further performance improvement. The presented study not only suggests\napplying matrix recovery methods to recover unreported profiles, but also\ninitiates a new missing feature problem which we called Learning with Positive\nand Unknown Features.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:33:10 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 14:48:57 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 19:31:49 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Lan", "Chao", ""], ["Chandrasekaran", "Sai Nivedita", ""], ["Huan", "Jun", ""]]}, {"id": "1704.01212", "submitter": "Justin Gilmer", "authors": "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals,\n  George E. Dahl", "title": "Neural Message Passing for Quantum Chemistry", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning on molecules has incredible potential to be useful in\nchemistry, drug discovery, and materials science. Luckily, several promising\nand closely related neural network models invariant to molecular symmetries\nhave already been described in the literature. These models learn a message\npassing algorithm and aggregation procedure to compute a function of their\nentire input graph. At this point, the next step is to find a particularly\neffective variant of this general approach and apply it to chemical prediction\nbenchmarks until we either solve them or reach the limits of the approach. In\nthis paper, we reformulate existing models into a single common framework we\ncall Message Passing Neural Networks (MPNNs) and explore additional novel\nvariations within this framework. Using MPNNs we demonstrate state of the art\nresults on an important molecular property prediction benchmark; these results\nare strong enough that we believe future work should focus on datasets with\nlarger molecules or more accurate ground truth labels.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 23:00:44 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 20:52:56 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Gilmer", "Justin", ""], ["Schoenholz", "Samuel S.", ""], ["Riley", "Patrick F.", ""], ["Vinyals", "Oriol", ""], ["Dahl", "George E.", ""]]}, {"id": "1704.01255", "submitter": "Maithra Raghu", "authors": "Ravi Kumar, Maithra Raghu, Tamas Sarlos, Andrew Tomkins", "title": "Linear Additive Markov Processes", "comments": "Accepted to WWW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP\nmay be influenced by states visited in the distant history of the process, but\nunlike higher-order Markov processes, LAMP retains an efficient\nparametrization. LAMP also allows the specific dependence on history to be\nlearned efficiently from data. We characterize some theoretical properties of\nLAMP, including its steady-state and mixing time. We then give an algorithm\nbased on alternating minimization to learn LAMP models from data. Finally, we\nperform a series of real-world experiments to show that LAMP is more powerful\nthan first-order Markov processes, and even holds its own against deep\nsequential models (LSTMs) with a negligible increase in parameter complexity.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 03:26:41 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Kumar", "Ravi", ""], ["Raghu", "Maithra", ""], ["Sarlos", "Tamas", ""], ["Tomkins", "Andrew", ""]]}, {"id": "1704.01265", "submitter": "Qiuwei Li", "authors": "Qiuwei Li, Zhihui Zhu and Gongguo Tang", "title": "Geometry of Factored Nuclear Norm Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the geometry of a nonconvex reformulation of\nminimizing a general convex loss function $f(X)$ regularized by the matrix\nnuclear norm $\\|X\\|_*$. Nuclear-norm regularized matrix inverse problems are at\nthe heart of many applications in machine learning, signal processing, and\ncontrol. The statistical performance of nuclear norm regularization has been\nstudied extensively in literature using convex analysis techniques. Despite its\noptimal performance, the resulting optimization has high computational\ncomplexity when solved using standard or even tailored fast convex solvers. To\ndevelop faster and more scalable algorithms, we follow the proposal of\nBurer-Monteiro to factor the matrix variable $X$ into the product of two\nsmaller rectangular matrices $X=UV^T$ and also replace the nuclear norm\n$\\|X\\|_*$ with $(\\|U\\|_F^2+\\|V\\|_F^2)/2$. In spite of the nonconvexity of the\nfactored formulation, we prove that when the convex loss function $f(X)$ is\n$(2r,4r)$-restricted well-conditioned, each critical point of the factored\nproblem either corresponds to the optimal solution $X^\\star$ of the original\nconvex optimization or is a strict saddle point where the Hessian matrix has a\nstrictly negative eigenvalue. Such a geometric structure of the factored\nformulation allows many local search algorithms to converge to the global\noptimum with random initializations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 04:22:58 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Li", "Qiuwei", ""], ["Zhu", "Zhihui", ""], ["Tang", "Gongguo", ""]]}, {"id": "1704.01279", "submitter": "Jesse Engel", "authors": "Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas\n  Eck, Karen Simonyan, Mohammad Norouzi", "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models in vision have seen rapid progress due to algorithmic\nimprovements and the availability of high-quality image datasets. In this\npaper, we offer contributions in both these areas to enable similar progress in\naudio modeling. First, we detail a powerful new WaveNet-style autoencoder model\nthat conditions an autoregressive decoder on temporal codes learned from the\nraw audio waveform. Second, we introduce NSynth, a large-scale and high-quality\ndataset of musical notes that is an order of magnitude larger than comparable\npublic datasets. Using NSynth, we demonstrate improved qualitative and\nquantitative performance of the WaveNet autoencoder over a well-tuned spectral\nautoencoder baseline. Finally, we show that the model learns a manifold of\nembeddings that allows for morphing between instruments, meaningfully\ninterpolating in timbre to create new types of sounds that are realistic and\nexpressive.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 06:34:22 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Engel", "Jesse", ""], ["Resnick", "Cinjon", ""], ["Roberts", "Adam", ""], ["Dieleman", "Sander", ""], ["Eck", "Douglas", ""], ["Simonyan", "Karen", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "1704.01280", "submitter": "Li-Chia Yang", "authors": "Li-Chia Yang, Szu-Yu Chou, Jen-Yu Liu, Yi-Hsuan Yang, Yi-An Chen", "title": "Revisiting the problem of audio-based hit song prediction using\n  convolutional neural networks", "comments": "To appear in the proceedings of 2017 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict whether a song can be a hit has impor- tant\napplications in the music industry. Although it is true that the popularity of\na song can be greatly affected by exter- nal factors such as social and\ncommercial influences, to which degree audio features computed from musical\nsignals (whom we regard as internal factors) can predict song popularity is an\ninteresting research question on its own. Motivated by the recent success of\ndeep learning techniques, we attempt to ex- tend previous work on hit song\nprediction by jointly learning the audio features and prediction models using\ndeep learning. Specifically, we experiment with a convolutional neural net-\nwork model that takes the primitive mel-spectrogram as the input for feature\nlearning, a more advanced JYnet model that uses an external song dataset for\nsupervised pre-training and auto-tagging, and the combination of these two\nmodels. We also consider the inception model to characterize audio infor-\nmation in different scales. Our experiments suggest that deep structures are\nindeed more accurate than shallow structures in predicting the popularity of\neither Chinese or Western Pop songs in Taiwan. We also use the tags predicted\nby JYnet to gain insights into the result of different models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 06:39:51 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Yang", "Li-Chia", ""], ["Chou", "Szu-Yu", ""], ["Liu", "Jen-Yu", ""], ["Yang", "Yi-Hsuan", ""], ["Chen", "Yi-An", ""]]}, {"id": "1704.01312", "submitter": "Pirmin Lemberger", "authors": "Pirmin Lemberger", "title": "On Generalization and Regularization in Deep Learning", "comments": "11 pages, 3 figures pedagogical paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why do large neural network generalize so well on complex tasks such as image\nclassification or speech recognition? What exactly is the role regularization\nfor them? These are arguably among the most important open questions in machine\nlearning today. In a recent and thought provoking paper [C. Zhang et al.]\nseveral authors performed a number of numerical experiments that hint at the\nneed for novel theoretical concepts to account for this phenomenon. The paper\nstirred quit a lot of excitement among the machine learning community but at\nthe same time it created some confusion as discussions on OpenReview.net\ntestifies. The aim of this pedagogical paper is to make this debate accessible\nto a wider audience of data scientists without advanced theoretical knowledge\nin statistical learning. The focus here is on explicit mathematical definitions\nand on a discussion of relevant concepts, not on proofs for which we provide\nreferences.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 08:48:01 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 19:58:27 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Lemberger", "Pirmin", ""]]}, {"id": "1704.01344", "submitter": "Ziwei Liu", "authors": "Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via\n  Deep Layer Cascade", "comments": "To appear in CVPR 2017 as a spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep layer cascade (LC) method to improve the accuracy and\nspeed of semantic segmentation. Unlike the conventional model cascade (MC) that\nis composed of multiple independent models, LC treats a single deep model as a\ncascade of several sub-models. Earlier sub-models are trained to handle easy\nand confident regions, and they progressively feed-forward harder regions to\nthe next sub-model for processing. Convolutions are only calculated on these\nregions to reduce computations. The proposed method possesses several\nadvantages. First, LC classifies most of the easy regions in the shallow stage\nand makes deeper stage focuses on a few hard regions. Such an adaptive and\n'difficulty-aware' learning improves segmentation performance. Second, LC\naccelerates both training and testing of deep network thanks to early decisions\nin the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable\nframework, allowing joint learning of all sub-models. We evaluate our method on\nPASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and\nfast speed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 09:58:51 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Liu", "Ziwei", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1704.01407", "submitter": "Cl\\'ement Moulin-Frier", "authors": "Cl\\'ement Moulin-Frier, Jordi-Ysard Puigb\\`o, Xerxes D. Arsiwalla,\n  Mart\\`i Sanchez-Fibla, Paul F. M. J. Verschure", "title": "Embodied Artificial Intelligence through Distributed Adaptive Control:\n  An Integrated Framework", "comments": "Updated version of the paper accepted to the ICDL-Epirob 2017\n  conference (Lisbon, Portugal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue that the future of Artificial Intelligence research\nresides in two keywords: integration and embodiment. We support this claim by\nanalyzing the recent advances of the field. Regarding integration, we note that\nthe most impactful recent contributions have been made possible through the\nintegration of recent Machine Learning methods (based in particular on Deep\nLearning and Recurrent Neural Networks) with more traditional ones (e.g.\nMonte-Carlo tree search, goal babbling exploration or addressable memory\nsystems). Regarding embodiment, we note that the traditional benchmark tasks\n(e.g. visual classification or board games) are becoming obsolete as\nstate-of-the-art learning algorithms approach or even surpass human performance\nin most of them, having recently encouraged the development of first-person 3D\ngame platforms embedding realistic physics. Building upon this analysis, we\nfirst propose an embodied cognitive architecture integrating heterogenous\nsub-fields of Artificial Intelligence into a unified framework. We demonstrate\nthe utility of our approach by showing how major contributions of the field can\nbe expressed within the proposed framework. We then claim that benchmarking\nenvironments need to reproduce ecologically-valid conditions for bootstrapping\nthe acquisition of increasingly complex cognitive skills through the concept of\na cognitive arms race between embodied agents.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 13:26:50 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 08:40:20 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 16:27:46 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Moulin-Frier", "Cl\u00e9ment", ""], ["Puigb\u00f2", "Jordi-Ysard", ""], ["Arsiwalla", "Xerxes D.", ""], ["Sanchez-Fibla", "Mart\u00ec", ""], ["Verschure", "Paul F. M. J.", ""]]}, {"id": "1704.01415", "submitter": "Zhi-Hua Zhou", "authors": "Yue Zhu and James T. Kwok and Zhi-Hua Zhou", "title": "Multi-Label Learning with Global and Local Label Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that exploiting label correlations is important to\nmulti-label learning. Existing approaches either assume that the label\ncorrelations are global and shared by all instances; or that the label\ncorrelations are local and shared only by a data subset. In fact, in the\nreal-world applications, both cases may occur that some label correlations are\nglobally applicable and some are shared only in a local group of instances.\nMoreover, it is also a usual case that only partial labels are observed, which\nmakes the exploitation of the label correlations much more difficult. That is,\nit is hard to estimate the label correlations when many labels are absent. In\nthis paper, we propose a new multi-label approach GLOCAL dealing with both the\nfull-label and the missing-label cases, exploiting global and local label\ncorrelations simultaneously, through learning a latent label representation and\noptimizing label manifolds. The extensive experimental studies validate the\neffectiveness of our approach on both full-label and missing-label data.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 12:50:25 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Zhu", "Yue", ""], ["Kwok", "James T.", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1704.01420", "submitter": "Neil Shah", "authors": "Neil Shah, Hemank Lamba, Alex Beutel, Christos Faloutsos", "title": "The Many Faces of Link Fraud", "comments": "\"full\" version of the ICDM2017 short paper, \"The Many Faces of Link\n  Fraud\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most past work on social network link fraud detection tries to separate\ngenuine users from fraudsters, implicitly assuming that there is only one type\nof fraudulent behavior. But is this assumption true? And, in either case, what\nare the characteristics of such fraudulent behaviors? In this work, we set up\nhoneypots (\"dummy\" social network accounts), and buy fake followers (after\ncareful IRB approval). We report the signs of such behaviors including oddities\nin local network connectivity, account attributes, and similarities and\ndifferences across fraud providers. Most valuably, we discover and characterize\nseveral types of fraud behaviors. We discuss how to leverage our insights in\npractice by engineering strongly performing entropy-based features and\ndemonstrating high classification accuracy. Our contributions are (a)\ninstrumentation: we detail our experimental setup and carefully engineered data\ncollection process to scrape Twitter data while respecting API rate-limits, (b)\nobservations on fraud multimodality: we analyze our honeypot fraudster\necosystem and give surprising insights into the multifaceted behaviors of these\nfraudster types, and (c) features: we propose novel features that give strong\n(>0.95 precision/recall) discriminative power on ground-truth Twitter data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 13:39:40 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 06:31:22 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 13:36:43 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Shah", "Neil", ""], ["Lamba", "Hemank", ""], ["Beutel", "Alex", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1704.01427", "submitter": "Andres Masegosa R", "authors": "Andr\\'es R. Masegosa, Ana M. Mart\\'inez, Dar\\'io Ramos-L\\'opez, Rafael\n  Caba\\~nas, Antonio Salmer\\'on, Thomas D. Nielsen, Helge Langseth, Anders L.\n  Madsen", "title": "AMIDST: a Java Toolbox for Scalable Probabilistic Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AMIDST Toolbox is a software for scalable probabilistic machine learning\nwith a spe- cial focus on (massive) streaming data. The toolbox supports a\nflexible modeling language based on probabilistic graphical models with latent\nvariables and temporal dependencies. The specified models can be learnt from\nlarge data sets using parallel or distributed implementa- tions of Bayesian\nlearning algorithms for either streaming or batch data. These algorithms are\nbased on a flexible variational message passing scheme, which supports discrete\nand continu- ous variables from a wide range of probability distributions.\nAMIDST also leverages existing functionality and algorithms by interfacing to\nsoftware tools such as Flink, Spark, MOA, Weka, R and HUGIN. AMIDST is an open\nsource toolbox written in Java and available at http://www.amidsttoolbox.com\nunder the Apache Software License version 2.0.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 11:58:21 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Masegosa", "Andr\u00e9s R.", ""], ["Mart\u00ednez", "Ana M.", ""], ["Ramos-L\u00f3pez", "Dar\u00edo", ""], ["Caba\u00f1as", "Rafael", ""], ["Salmer\u00f3n", "Antonio", ""], ["Nielsen", "Thomas D.", ""], ["Langseth", "Helge", ""], ["Madsen", "Anders L.", ""]]}, {"id": "1704.01444", "submitter": "Alec Radford", "authors": "Alec Radford, Rafal Jozefowicz, Ilya Sutskever", "title": "Learning to Generate Reviews and Discovering Sentiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the properties of byte-level recurrent language models. When given\nsufficient amounts of capacity, training data, and compute time, the\nrepresentations learned by these models include disentangled features\ncorresponding to high-level concepts. Specifically, we find a single unit which\nperforms sentiment analysis. These representations, learned in an unsupervised\nmanner, achieve state of the art on the binary subset of the Stanford Sentiment\nTreebank. They are also very data efficient. When using only a handful of\nlabeled examples, our approach matches the performance of strong baselines\ntrained on full datasets. We also demonstrate the sentiment unit has a direct\ninfluence on the generative process of the model. Simply fixing its value to be\npositive or negative generates samples with the corresponding positive or\nnegative sentiment.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 14:20:28 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 09:48:20 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Radford", "Alec", ""], ["Jozefowicz", "Rafal", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1704.01460", "submitter": "Siavash Haghiri", "authors": "Siavash Haghiri, Debarghya Ghoshdastidar and Ulrike von Luxburg", "title": "Comparison Based Nearest Neighbor Search", "comments": "16 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider machine learning in a comparison-based setting where we are given\na set of points in a metric space, but we have no access to the actual\ndistances between the points. Instead, we can only ask an oracle whether the\ndistance between two points $i$ and $j$ is smaller than the distance between\nthe points $i$ and $k$. We are concerned with data structures and algorithms to\nfind nearest neighbors based on such comparisons. We focus on a simple yet\neffective algorithm that recursively splits the space by first selecting two\nrandom pivot points and then assigning all other points to the closer of the\ntwo (comparison tree). We prove that if the metric space satisfies certain\nexpansion conditions, then with high probability the height of the comparison\ntree is logarithmic in the number of points, leading to efficient search\nperformance. We also provide an upper bound for the failure probability to\nreturn the true nearest neighbor. Experiments show that the comparison tree is\ncompetitive with algorithms that have access to the actual distance values, and\nneeds less triplet comparisons than other competitors.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 14:54:28 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Haghiri", "Siavash", ""], ["Ghoshdastidar", "Debarghya", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1704.01472", "submitter": "Min Xian", "authors": "Min Xian, Yingtao Zhang, H.D. Cheng, Fei Xu, Boyu Zhang, Jianrui Ding", "title": "Automatic Breast Ultrasound Image Segmentation: A Survey", "comments": "40 pages, 6 tables, 180 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the leading causes of cancer death among women\nworldwide. In clinical routine, automatic breast ultrasound (BUS) image\nsegmentation is very challenging and essential for cancer diagnosis and\ntreatment planning. Many BUS segmentation approaches have been studied in the\nlast two decades, and have been proved to be effective on private datasets.\nCurrently, the advancement of BUS image segmentation seems to meet its\nbottleneck. The improvement of the performance is increasingly challenging, and\nonly few new approaches were published in the last several years. It is the\ntime to look at the field by reviewing previous approaches comprehensively and\nto investigate the future directions. In this paper, we study the basic ideas,\ntheories, pros and cons of the approaches, group them into categories, and\nextensively review each category in depth by discussing the principles,\napplication issues, and advantages/disadvantages.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 14:23:26 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 23:19:21 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Xian", "Min", ""], ["Zhang", "Yingtao", ""], ["Cheng", "H. D.", ""], ["Xu", "Fei", ""], ["Zhang", "Boyu", ""], ["Ding", "Jianrui", ""]]}, {"id": "1704.01474", "submitter": "Kai Chen", "authors": "Kai Chen and Mathias Seuret", "title": "Convolutional Neural Networks for Page Segmentation of Historical\n  Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Convolutional Neural Network (CNN) based page\nsegmentation method for handwritten historical document images. We consider\npage segmentation as a pixel labeling problem, i.e., each pixel is classified\nas one of the predefined classes. Traditional methods in this area rely on\ncarefully hand-crafted features or large amounts of prior knowledge. In\ncontrast, we propose to learn features from raw image pixels using a CNN. While\nmany researchers focus on developing deep CNN architectures to solve different\nproblems, we train a simple CNN with only one convolution layer. We show that\nthe simple architecture achieves competitive results against other deep\narchitectures on different public datasets. Experiments also demonstrate the\neffectiveness and superiority of the proposed method compared to previous\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 15:12:25 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 10:16:49 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Chen", "Kai", ""], ["Seuret", "Mathias", ""]]}, {"id": "1704.01547", "submitter": "Wieland Brendel", "authors": "Wieland Brendel, Matthias Bethge", "title": "Comment on \"Biologically inspired protection of deep networks from\n  adversarial attacks\"", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent paper suggests that Deep Neural Networks can be protected from\ngradient-based adversarial perturbations by driving the network activations\ninto a highly saturated regime. Here we analyse such saturated networks and\nshow that the attacks fail due to numerical limitations in the gradient\ncomputations. A simple stabilisation of the gradient estimates enables\nsuccessful and efficient attacks. Thus, it has yet to be shown that the\nrobustness observed in highly saturated networks is not simply due to numerical\nlimitations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 17:47:25 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "1704.01552", "submitter": "Yoav Levine Mr.", "authors": "Yoav Levine, David Yakira, Nadav Cohen and Amnon Shashua", "title": "Deep Learning and Quantum Entanglement: Fundamental Connections with\n  Implications to Network Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have witnessed unprecedented success in various\nmachine learning applications. Formal understanding on what makes these\nnetworks so successful is gradually unfolding, but for the most part there are\nstill significant mysteries to unravel. The inductive bias, which reflects\nprior knowledge embedded in the network architecture, is one of them. In this\nwork, we establish a fundamental connection between the fields of quantum\nphysics and deep learning. We use this connection for asserting novel\ntheoretical observations regarding the role that the number of channels in each\nlayer of the convolutional network fulfills in the overall inductive bias.\nSpecifically, we show an equivalence between the function realized by a deep\nconvolutional arithmetic circuit (ConvAC) and a quantum many-body wave\nfunction, which relies on their common underlying tensorial structure. This\nfacilitates the use of quantum entanglement measures as well-defined\nquantifiers of a deep network's expressive ability to model intricate\ncorrelation structures of its inputs. Most importantly, the construction of a\ndeep ConvAC in terms of a Tensor Network is made available. This description\nenables us to carry a graph-theoretic analysis of a convolutional network, with\nwhich we demonstrate a direct control over the inductive bias of the deep\nnetwork via its channel numbers, that are related to the min-cut in the\nunderlying graph. This result is relevant to any practitioner designing a\nnetwork for a specific task. We theoretically analyze ConvACs, and empirically\nvalidate our findings on more common ConvNets which involve ReLU activations\nand max pooling. Beyond the results described above, the description of a deep\nconvolutional network in well-defined graph-theoretic tools and the formal\nconnection to quantum entanglement, are two interdisciplinary bridges that are\nbrought forth by this work.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 17:53:13 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 14:29:34 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Levine", "Yoav", ""], ["Yakira", "David", ""], ["Cohen", "Nadav", ""], ["Shashua", "Amnon", ""]]}, {"id": "1704.01574", "submitter": "Kevin Amaral", "authors": "Kevin M. Amaral, Ping Chen, Scott Crouter, Wei Ding", "title": "Bag-of-Words Method Applied to Accelerometer Measurements for the\n  Purpose of Classification and Energy Estimation", "comments": "10 pages, 6 tables, 2 figures. This paper has been withdrawn due to a\n  few crucial errors in the early sections of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerometer measurements are the prime type of sensor information most\nthink of when seeking to measure physical activity. On the market, there are\nmany fitness measuring devices which aim to track calories burned and steps\ncounted through the use of accelerometers. These measurements, though good\nenough for the average consumer, are noisy and unreliable in terms of the\nprecision of measurement needed in a scientific setting. The contribution of\nthis paper is an innovative and highly accurate regression method which uses an\nintermediary two-stage classification step to better direct the regression of\nenergy expenditure values from accelerometer counts.\n  We show that through an additional unsupervised layer of intermediate feature\nconstruction, we can leverage latent patterns within accelerometer counts to\nprovide better grounds for activity classification than expert-constructed\ntimeseries features. For this, our approach utilizes a mathematical model\noriginating in natural language processing, the bag-of-words model, that has in\nthe past years been appearing in diverse disciplines outside of the natural\nlanguage processing field such as image processing. Further emphasizing the\nnatural language connection to stochastics, we use a gaussian mixture model to\nlearn the dictionary upon which the bag-of-words model is built. Moreover, we\nshow that with the addition of these features, we're able to improve regression\nroot mean-squared error of energy expenditure by approximately 1.4 units over\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 18:21:26 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 23:01:36 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Amaral", "Kevin M.", ""], ["Chen", "Ping", ""], ["Crouter", "Scott", ""], ["Ding", "Wei", ""]]}, {"id": "1704.01605", "submitter": "Daniel O'Malley", "authors": "Daniel O'Malley, Velimir V. Vesselinov, Boian S. Alexandrov, Ludmil B.\n  Alexandrov", "title": "Nonnegative/binary matrix factorization with a D-Wave quantum annealer", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0206653", "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  D-Wave quantum annealers represent a novel computational architecture and\nhave attracted significant interest, but have been used for few real-world\ncomputations. Machine learning has been identified as an area where quantum\nannealing may be useful. Here, we show that the D-Wave 2X can be effectively\nused as part of an unsupervised machine learning method. This method can be\nused to analyze large datasets. The D-Wave only limits the number of features\nthat can be extracted from the dataset. We apply this method to learn the\nfeatures from a set of facial images.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 18:49:56 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["O'Malley", "Daniel", ""], ["Vesselinov", "Velimir V.", ""], ["Alexandrov", "Boian S.", ""], ["Alexandrov", "Ludmil B.", ""]]}, {"id": "1704.01652", "submitter": "Christopher Harshaw", "authors": "Moran Feldman, Christopher Harshaw, Amin Karbasi", "title": "Greed is Good: Near-Optimal Submodular Maximization via Greedy\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that greedy methods perform well for maximizing monotone\nsubmodular functions. At the same time, such methods perform poorly in the face\nof non-monotonicity. In this paper, we show - arguably, surprisingly - that\ninvoking the classical greedy algorithm $O(\\sqrt{k})$-times leads to the\n(currently) fastest deterministic algorithm, called Repeated Greedy, for\nmaximizing a general submodular function subject to $k$-independent system\nconstraints. Repeated Greedy achieves $(1 + O(1/\\sqrt{k}))k$ approximation\nusing $O(nr\\sqrt{k})$ function evaluations (here, $n$ and $r$ denote the size\nof the ground set and the maximum size of a feasible solution, respectively).\nWe then show that by a careful sampling procedure, we can run the greedy\nalgorithm only once and obtain the (currently) fastest randomized algorithm,\ncalled Sample Greedy, for maximizing a submodular function subject to\n$k$-extendible system constraints (a subclass of $k$-independent system\nconstrains). Sample Greedy achieves $(k + 3)$-approximation with only $O(nr/k)$\nfunction evaluations. Finally, we derive an almost matching lower bound, and\nshow that no polynomial time algorithm can have an approximation ratio smaller\nthan $ k + 1/2 - \\varepsilon$. To further support our theoretical results, we\ncompare the performance of Repeated Greedy and Sample Greedy with prior art in\na concrete application (movie recommendation). We consistently observe that\nwhile Sample Greedy achieves practically the same utility as the best baseline,\nit performs at least two orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 21:03:53 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Feldman", "Moran", ""], ["Harshaw", "Christopher", ""], ["Karbasi", "Amin", ""]]}, {"id": "1704.01664", "submitter": "Cheng Ju", "authors": "Cheng Ju and Aur\\'elien Bibaut and Mark J. van der Laan", "title": "The Relative Performance of Ensemble Methods with Deep Convolutional\n  Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks have been successfully applied to a variety of\nmachine learning tasks, including image recognition, semantic segmentation, and\nmachine translation. However, few studies fully investigated ensembles of\nartificial neural networks. In this work, we investigated multiple widely used\nensemble methods, including unweighted averaging, majority voting, the Bayes\nOptimal Classifier, and the (discrete) Super Learner, for image recognition\ntasks, with deep neural networks as candidate algorithms. We designed several\nexperiments, with the candidate algorithms being the same network structure\nwith different model checkpoints within a single training process, networks\nwith same structure but trained multiple times stochastically, and networks\nwith different structure. In addition, we further studied the over-confidence\nphenomenon of the neural networks, as well as its impact on the ensemble\nmethods. Across all of our experiments, the Super Learner achieved best\nperformance among all the ensemble methods in this study.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 23:04:43 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Ju", "Cheng", ""], ["Bibaut", "Aur\u00e9lien", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1704.01665", "submitter": "Hanjun Dai", "authors": "Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, Le Song", "title": "Learning Combinatorial Optimization Algorithms over Graphs", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of good heuristics or approximation algorithms for NP-hard\ncombinatorial optimization problems often requires significant specialized\nknowledge and trial-and-error. Can we automate this challenging, tedious\nprocess, and learn the algorithms instead? In many real-world applications, it\nis typically the case that the same optimization problem is solved again and\nagain on a regular basis, maintaining the same problem structure but differing\nin the data. This provides an opportunity for learning heuristic algorithms\nthat exploit the structure of such recurring problems. In this paper, we\npropose a unique combination of reinforcement learning and graph embedding to\naddress this challenge. The learned greedy policy behaves like a meta-algorithm\nthat incrementally constructs a solution, and the action is determined by the\noutput of a graph embedding network capturing the current state of the\nsolution. We show that our framework can be applied to a diverse range of\noptimization problems over graphs, and learns effective algorithms for the\nMinimum Vertex Cover, Maximum Cut and Traveling Salesman problems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 23:08:07 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 22:08:37 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 23:12:54 GMT"}, {"version": "v4", "created": "Wed, 21 Feb 2018 19:47:20 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Dai", "Hanjun", ""], ["Khalil", "Elias B.", ""], ["Zhang", "Yuyu", ""], ["Dilkina", "Bistra", ""], ["Song", "Le", ""]]}, {"id": "1704.01691", "submitter": "Chunting Zhou", "authors": "Chunting Zhou and Graham Neubig", "title": "Multi-space Variational Encoder-Decoders for Semi-supervised Labeled\n  Sequence Transduction", "comments": "Accepted by ACL 2017", "journal-ref": "ACL 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeled sequence transduction is a task of transforming one sequence into\nanother sequence that satisfies desiderata specified by a set of labels. In\nthis paper we propose multi-space variational encoder-decoders, a new model for\nlabeled sequence transduction with semi-supervised learning. The generative\nmodel can use neural networks to handle both discrete and continuous latent\nvariables to exploit various features of data. Experiments show that our model\nprovides not only a powerful supervised framework but also can effectively take\nadvantage of the unlabeled data. On the SIGMORPHON morphological inflection\nbenchmark, our model outperforms single-model state-of-art results by a large\nmargin for the majority of languages.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 02:36:56 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 03:22:23 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhou", "Chunting", ""], ["Neubig", "Graham", ""]]}, {"id": "1704.01700", "submitter": "Anirban Roychowdhury", "authors": "Anirban Roychowdhury", "title": "Accelerated Stochastic Quasi-Newton Optimization on Riemann Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an L-BFGS optimization algorithm on Riemannian manifolds using\nminibatched stochastic variance reduction techniques for fast convergence with\nconstant step sizes, without resorting to linesearch methods designed to\nsatisfy Wolfe conditions. We provide a new convergence proof for strongly\nconvex functions without using curvature conditions on the manifold, as well as\na convergence discussion for nonconvex functions. We discuss a couple of ways\nto obtain the correction pairs used to calculate the product of the gradient\nwith the inverse Hessian, and empirically demonstrate their use in synthetic\nexperiments on computation of Karcher means for symmetric positive definite\nmatrices and leading eigenvalues of large scale data matrices. We compare our\nmethod to VR-PCA for the latter experiment, along with Riemannian SVRG for both\ncases, and show strong convergence results for a range of datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 03:34:29 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 22:02:30 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 15:02:02 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Roychowdhury", "Anirban", ""]]}, {"id": "1704.01701", "submitter": "Elaine Angelino", "authors": "Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer,\n  Cynthia Rudin", "title": "Learning Certifiably Optimal Rule Lists for Categorical Data", "comments": "A short version of this work appeared in KDD '17 as \"Learning\n  Certifiably Optimal Rule Lists\"", "journal-ref": "Journal of Machine Learning Research 18(234):1-78, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and implementation of a custom discrete optimization\ntechnique for building rule lists over a categorical feature space. Our\nalgorithm produces rule lists with optimal training performance, according to\nthe regularized empirical risk, with a certificate of optimality. By leveraging\nalgorithmic bounds, efficient data structures, and computational reuse, we\nachieve several orders of magnitude speedup in time and a massive reduction of\nmemory consumption. We demonstrate that our approach produces optimal rule\nlists on practical problems in seconds. Our results indicate that it is\npossible to construct optimal sparse rule lists that are approximately as\naccurate as the COMPAS proprietary risk prediction tool on data from Broward\nCounty, Florida, but that are completely interpretable. This framework is a\nnovel alternative to CART and other decision tree methods for interpretable\nmodeling.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 04:02:35 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 19:00:01 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 17:49:44 GMT"}, {"version": "v4", "created": "Fri, 3 Aug 2018 22:51:24 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Angelino", "Elaine", ""], ["Larus-Stone", "Nicholas", ""], ["Alabi", "Daniel", ""], ["Seltzer", "Margo", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1704.01704", "submitter": "Yi Han", "authors": "Yi Han, Benjamin I. P. Rubinstein", "title": "Adequacy of the Gradient-Descent Method for Classifier Evasion Attacks", "comments": "10 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the wide use of machine learning in adversarial settings including\ncomputer security, recent studies have demonstrated vulnerabilities to evasion\nattacks---carefully crafted adversarial samples that closely resemble\nlegitimate instances, but cause misclassification. In this paper, we examine\nthe adequacy of the leading approach to generating adversarial samples---the\ngradient descent approach. In particular (1) we perform extensive experiments\non three datasets, MNIST, USPS and Spambase, in order to analyse the\neffectiveness of the gradient-descent method against non-linear support vector\nmachines, and conclude that carefully reduced kernel smoothness can\nsignificantly increase robustness to the attack; (2) we demonstrate that\nseparated inter-class support vectors lead to more secure models, and propose a\nquantity similar to margin that can efficiently predict potential\nsusceptibility to gradient-descent attacks, before the attack is launched; and\n(3) we design a new adversarial sample construction algorithm based on\noptimising the multiplicative ratio of class decision functions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 04:35:40 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 04:32:43 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Han", "Yi", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "1704.01770", "submitter": "Diego Garc\\'ia-Gil", "authors": "Diego Garc\\'ia-Gil, Juli\\'an Luengo, Salvador Garc\\'ia and Francisco\n  Herrera", "title": "Enabling Smart Data: Noise filtering in Big Data classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In any knowledge discovery process the value of extracted knowledge is\ndirectly related to the quality of the data used. Big Data problems, generated\nby massive growth in the scale of data observed in recent years, also follow\nthe same dictate. A common problem affecting data quality is the presence of\nnoise, particularly in classification problems, where label noise refers to the\nincorrect labeling of training instances, and is known to be a very disruptive\nfeature of data. However, in this Big Data era, the massive growth in the scale\nof the data poses a challenge to traditional proposals created to tackle noise,\nas they have difficulties coping with such a large amount of data. New\nalgorithms need to be proposed to treat the noise in Big Data problems,\nproviding high quality and clean data, also known as Smart Data. In this paper,\ntwo Big Data preprocessing approaches to remove noisy examples are proposed: an\nhomogeneous ensemble and an heterogeneous ensemble filter, with special\nemphasis in their scalability and performance traits. The obtained results show\nthat these proposals enable the practitioner to efficiently obtain a Smart\nDataset from any Big Data classification problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 10:06:52 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 11:39:16 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Garc\u00eda-Gil", "Diego", ""], ["Luengo", "Juli\u00e1n", ""], ["Garc\u00eda", "Salvador", ""], ["Herrera", "Francisco", ""]]}, {"id": "1704.01815", "submitter": "Terence Fusco B.Sc.(Hons)", "authors": "Terence Fusco, Yaxin Bi, Haiying Wang, Fiona Browne", "title": "Incremental Transductive Learning Approaches to Schistosomiasis Vector\n  Classification", "comments": "8 pages, 5 figures, Dragon 4 Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key issues pertaining to collection of epidemic disease data for our\nanalysis purposes are that it is a labour intensive, time consuming and\nexpensive process resulting in availability of sparse sample data which we use\nto develop prediction models. To address this sparse data issue, we present\nnovel Incremental Transductive methods to circumvent the data collection\nprocess by applying previously acquired data to provide consistent,\nconfidence-based labelling alternatives to field survey research. We\ninvestigated various reasoning approaches for semisupervised machine learning\nincluding Bayesian models for labelling data. The results show that using the\nproposed methods, we can label instances of data with a class of vector density\nat a high level of confidence. By applying the Liberal and Strict Training\nApproaches, we provide a labelling and classification alternative to standalone\nalgorithms. The methods in this paper are components in the process of reducing\nthe proliferation of the Schistosomiasis disease and its effects.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 13:03:05 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Fusco", "Terence", ""], ["Bi", "Yaxin", ""], ["Wang", "Haiying", ""], ["Browne", "Fiona", ""]]}, {"id": "1704.01858", "submitter": "Nicholas Monath", "authors": "Ari Kobren, Nicholas Monath, Akshay Krishnamurthy, Andrew McCallum", "title": "An Online Hierarchical Algorithm for Extreme Clustering", "comments": "20 pages. Code available here: https://github.com/iesl/xcluster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern clustering methods scale well to a large number of data items, N,\nbut not to a large number of clusters, K. This paper introduces PERCH, a new\nnon-greedy algorithm for online hierarchical clustering that scales to both\nmassive N and K--a problem setting we term extreme clustering. Our algorithm\nefficiently routes new data points to the leaves of an incrementally-built\ntree. Motivated by the desire for both accuracy and speed, our approach\nperforms tree rotations for the sake of enhancing subtree purity and\nencouraging balancedness. We prove that, under a natural separability\nassumption, our non-greedy algorithm will produce trees with perfect dendrogram\npurity regardless of online data arrival order. Our experiments demonstrate\nthat PERCH constructs more accurate trees than other tree-building clustering\nalgorithms and scales well with both N and K, achieving a higher quality\nclustering than the strongest flat clustering competitor in nearly half the\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 14:29:10 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Kobren", "Ari", ""], ["Monath", "Nicholas", ""], ["Krishnamurthy", "Akshay", ""], ["McCallum", "Andrew", ""]]}, {"id": "1704.01896", "submitter": "Yixi Xu", "authors": "Yixi Xu, Jean Honorio, Xiao Wang", "title": "On the Statistical Efficiency of Compositional Nonparametric Prediction", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS), 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a compositional nonparametric method in which a\nmodel is expressed as a labeled binary tree of $2k+1$ nodes, where each node is\neither a summation, a multiplication, or the application of one of the $q$\nbasis functions to one of the $p$ covariates. We show that in order to recover\na labeled binary tree from a given dataset, the sufficient number of samples is\n$O(k\\log(pq)+\\log(k!))$, and the necessary number of samples is $\\Omega(k\\log\n(pq)-\\log(k!))$. We further propose a greedy algorithm for regression in order\nto validate our theoretical findings through synthetic experiments.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 15:43:08 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 01:46:35 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 15:28:26 GMT"}, {"version": "v4", "created": "Fri, 20 Oct 2017 00:49:39 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Xu", "Yixi", ""], ["Honorio", "Jean", ""], ["Wang", "Xiao", ""]]}, {"id": "1704.01897", "submitter": "Wei-Shi Zheng", "authors": "Long-Kai Huang, Qiang Yang, Wei-Shi Zheng", "title": "Online Hashing", "comments": "To appear in IEEE Transactions on Neural Networks and Learning\n  Systems (DOI: 10.1109/TNNLS.2017.2689242)", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2689242", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although hash function learning algorithms have achieved great success in\nrecent years, most existing hash models are off-line, which are not suitable\nfor processing sequential or online data. To address this problem, this work\nproposes an online hash model to accommodate data coming in stream for online\nlearning. Specifically, a new loss function is proposed to measure the\nsimilarity loss between a pair of data samples in hamming space. Then, a\nstructured hash model is derived and optimized in a passive-aggressive way.\nTheoretical analysis on the upper bound of the cumulative loss for the proposed\nonline hash model is provided. Furthermore, we extend our online hashing from a\nsingle-model to a multi-model online hashing that trains multiple models so as\nto retain diverse online hashing models in order to avoid biased update. The\ncompetitive efficiency and effectiveness of the proposed online hash models are\nverified through extensive experiments on several large-scale datasets as\ncompared to related hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 15:44:29 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Huang", "Long-Kai", ""], ["Yang", "Qiang", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1704.01985", "submitter": "Yanmin Qian", "authors": "Dong Yu, Xuankai Chang, Yanmin Qian", "title": "Recognizing Multi-talker Speech with Permutation Invariant Training", "comments": "5 pages, 6 figures, InterSpeech2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel technique for direct recognition of\nmultiple speech streams given the single channel of mixed speech, without first\nseparating them. Our technique is based on permutation invariant training (PIT)\nfor automatic speech recognition (ASR). In PIT-ASR, we compute the average\ncross entropy (CE) over all frames in the whole utterance for each possible\noutput-target assignment, pick the one with the minimum CE, and optimize for\nthat assignment. PIT-ASR forces all the frames of the same speaker to be\naligned with the same output layer. This strategy elegantly solves the label\npermutation problem and speaker tracing problem in one shot. Our experiments on\nartificially mixed AMI data showed that the proposed approach is very\npromising.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 08:39:32 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 08:58:26 GMT"}, {"version": "v3", "created": "Fri, 16 Jun 2017 08:29:41 GMT"}, {"version": "v4", "created": "Mon, 19 Jun 2017 10:57:38 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yu", "Dong", ""], ["Chang", "Xuankai", ""], ["Qian", "Yanmin", ""]]}, {"id": "1704.02038", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, Adarsh Subbaswamy, Suchi Saria", "title": "Treatment-Response Models for Counterfactual Reasoning with\n  Continuous-time, Continuous-valued Interventions", "comments": "In Proceedings of the Thirty-Third Conference on Uncertainty in\n  Artificial Intelligence (UAI-2017), Sydney, Australia, August 2017. The first\n  two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment effects can be estimated from observational data as the difference\nin potential outcomes. In this paper, we address the challenge of estimating\nthe potential outcome when treatment-dose levels can vary continuously over\ntime. Further, the outcome variable may not be measured at a regular frequency.\nOur proposed solution represents the treatment response curves using linear\ntime-invariant dynamical systems---this provides a flexible means for modeling\nresponse over time to highly variable dose curves. Moreover, for multivariate\ndata, the proposed method: uncovers shared structure in treatment response and\nthe baseline across multiple markers; and, flexibly models challenging\ncorrelation structure both across and within signals over time. For this, we\nbuild upon the framework of multiple-output Gaussian Processes. On simulated\nand a challenging clinical dataset, we show significant gains in accuracy over\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 22:42:13 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 02:16:25 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Soleimani", "Hossein", ""], ["Subbaswamy", "Adarsh", ""], ["Saria", "Suchi", ""]]}, {"id": "1704.02046", "submitter": "Dan Elbaz", "authors": "Dan Elbaz, Michael Zibulevsky", "title": "End to End Deep Neural Network Frequency Demodulation of Speech Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequency modulation (FM) is a form of radio broadcasting which is widely\nused nowadays and has been for almost a century. We suggest a\nsoftware-defined-radio (SDR) receiver for FM demodulation that adopts an\nend-to-end learning based approach and utilizes the prior information of\ntransmitted speech message in the demodulation process. The receiver detects\nand enhances speech from the in-phase and quadrature components of its base\nband version. The new system yields high performance detection for both\nacoustical disturbances, and communication channel noise and is foreseen to\nout-perform the established methods for low signal to noise ratio (SNR)\nconditions in both mean square error and in perceptual evaluation of speech\nquality score.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 23:16:33 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 14:47:16 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 00:47:36 GMT"}, {"version": "v4", "created": "Sun, 21 May 2017 11:55:14 GMT"}, {"version": "v5", "created": "Sat, 7 Oct 2017 16:34:18 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Elbaz", "Dan", ""], ["Zibulevsky", "Michael", ""]]}, {"id": "1704.02109", "submitter": "Yuantao Gu", "authors": "Gen Li and Yuantao Gu", "title": "Restricted Isometry Property of Gaussian Random Projection for Finite\n  Set of Subspaces", "comments": "37 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TSP.2017.2778685", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction plays an essential role when decreasing the complexity of\nsolving large-scale problems. The well-known Johnson-Lindenstrauss (JL) Lemma\nand Restricted Isometry Property (RIP) admit the use of random projection to\nreduce the dimension while keeping the Euclidean distance, which leads to the\nboom of Compressed Sensing and the field of sparsity related signal processing.\nRecently, successful applications of sparse models in computer vision and\nmachine learning have increasingly hinted that the underlying structure of high\ndimensional data looks more like a union of subspaces (UoS). In this paper,\nmotivated by JL Lemma and an emerging field of Compressed Subspace Clustering\n(CSC), we study for the first time the RIP of Gaussian random matrices for the\ncompression of two subspaces based on the generalized projection $F$-norm\ndistance. We theoretically prove that with high probability the affinity or\ndistance between two projected subspaces are concentrated around their\nestimates. When the ambient dimension after projection is sufficiently large,\nthe affinity and distance between two subspaces almost remain unchanged after\nrandom projection. Numerical experiments verify the theoretical work.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 06:59:31 GMT"}, {"version": "v2", "created": "Sat, 12 Aug 2017 05:11:25 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 15:26:04 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Li", "Gen", ""], ["Gu", "Yuantao", ""]]}, {"id": "1704.02124", "submitter": "Wojciech Fedorko", "authors": "Jannicke Pearkes, Wojciech Fedorko, Alison Lister, Colin Gay", "title": "Jet Constituents for Deep Neural Network Based Top Quark Tagging", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.LG hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature on deep neural networks for tagging of highly energetic\njets resulting from top quark decays has focused on image based techniques or\nmultivariate approaches using high-level jet substructure variables. Here, a\nsequential approach to this task is taken by using an ordered sequence of jet\nconstituents as training inputs. Unlike the majority of previous approaches,\nthis strategy does not result in a loss of information during pixelisation or\nthe calculation of high level features. The jet classification method achieves\na background rejection of 45 at a 50% efficiency operating point for\nreconstruction level jets with transverse momentum range of 600 to 2500 GeV and\nis insensitive to multiple proton-proton interactions at the levels expected\nthroughout Run 2 of the LHC.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 08:16:29 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 11:49:00 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Pearkes", "Jannicke", ""], ["Fedorko", "Wojciech", ""], ["Lister", "Alison", ""], ["Gay", "Colin", ""]]}, {"id": "1704.02146", "submitter": "Maria Schuld", "authors": "Maria Schuld and Francesco Petruccione", "title": "Quantum ensembles of quantum classifiers", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum machine learning witnesses an increasing amount of quantum algorithms\nfor data-driven decision making, a problem with potential applications ranging\nfrom automated image recognition to medical diagnosis. Many of those algorithms\nare implementations of quantum classifiers, or models for the classification of\ndata inputs with a quantum computer. Following the success of collective\ndecision making with ensembles in classical machine learning, this paper\nintroduces the concept of quantum ensembles of quantum classifiers. Creating\nthe ensemble corresponds to a state preparation routine, after which the\nquantum classifiers are evaluated in parallel and their combined decision is\naccessed by a single-qubit measurement. This framework naturally allows for\nexponentially large ensembles in which -- similar to Bayesian learning -- the\nindividual classifiers do not have to be trained. As an example, we analyse an\nexponentially large quantum ensemble in which each classifier is weighed\naccording to its performance in classifying the training data, leading to new\nresults for quantum as well as classical machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 09:12:39 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Schuld", "Maria", ""], ["Petruccione", "Francesco", ""]]}, {"id": "1704.02147", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad and Varun Kanade and Frederik Mallmann-Trenn and\n  Claire Mathieu", "title": "Hierarchical Clustering: Objective Functions and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is a recursive partitioning of a dataset into\nclusters at an increasingly finer granularity. Motivated by the fact that most\nwork on hierarchical clustering was based on providing algorithms, rather than\noptimizing a specific objective, Dasgupta framed similarity-based hierarchical\nclustering as a combinatorial optimization problem, where a `good' hierarchical\nclustering is one that minimizes some cost function. He showed that this cost\nfunction has certain desirable properties.\n  We take an axiomatic approach to defining `good' objective functions for both\nsimilarity and dissimilarity-based hierarchical clustering. We characterize a\nset of \"admissible\" objective functions (that includes Dasgupta's one) that\nhave the property that when the input admits a `natural' hierarchical\nclustering, it has an optimal value.\n  Equipped with a suitable objective function, we analyze the performance of\npractical algorithms, as well as develop better algorithms. For\nsimilarity-based hierarchical clustering, Dasgupta showed that the divisive\nsparsest-cut approach achieves an $O(\\log^{3/2} n)$-approximation. We give a\nrefined analysis of the algorithm and show that it in fact achieves an\n$O(\\sqrt{\\log n})$-approx. (Charikar and Chatziafratis independently proved\nthat it is a $O(\\sqrt{\\log n})$-approx.). This improves upon the LP-based\n$O(\\log n)$-approx. of Roy and Pokutta. For dissimilarity-based hierarchical\nclustering, we show that the classic average-linkage algorithm gives a factor 2\napprox., and provide a simple and better algorithm that gives a factor 3/2\napprox..\n  Finally, we consider `beyond-worst-case' scenario through a generalisation of\nthe stochastic block model for hierarchical clustering. We show that Dasgupta's\ncost function has desirable properties for these inputs and we provide a simple\n1 + o(1)-approximation in this setting.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 09:14:28 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Kanade", "Varun", ""], ["Mallmann-Trenn", "Frederik", ""], ["Mathieu", "Claire", ""]]}, {"id": "1704.02197", "submitter": "Amarjot Singh", "authors": "Vibin Vijay, Raghunath Vp, Amarjot Singh, SN Omar", "title": "Variance Based Moving K-Means Algorithm", "comments": "Accepted at the 7th IEEE International Advance Computing Conference\n  (IACC-2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a useful data exploratory method with its wide applicability in\nmultiple fields. However, data clustering greatly relies on initialization of\ncluster centers that can result in large intra-cluster variance and dead\ncenters, therefore leading to sub-optimal solutions. This paper proposes a\nnovel variance based version of the conventional Moving K-Means (MKM) algorithm\ncalled Variance Based Moving K-Means (VMKM) that can partition data into\noptimal homogeneous clusters, irrespective of cluster initialization. The\nalgorithm utilizes a novel distance metric and a unique data element selection\ncriteria to transfer the selected elements between clusters to achieve low\nintra-cluster variance and subsequently avoid dead centers. Quantitative and\nqualitative comparison with various clustering techniques is performed on four\ndatasets selected from image processing, bioinformatics, remote sensing and the\nstock market respectively. An extensive analysis highlights the superior\nperformance of the proposed method over other techniques.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 12:10:39 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 13:03:54 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Vijay", "Vibin", ""], ["Vp", "Raghunath", ""], ["Singh", "Amarjot", ""], ["Omar", "SN", ""]]}, {"id": "1704.02216", "submitter": "Ashkan Esmaeili", "authors": "Ali Mottaghi, Kayhan Behdin, Ashkan Esmaeili, Mohammadreza Heydari,\n  and Farokh Marvasti", "title": "OBTAIN: Real-Time Beat Tracking in Audio Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a system in order to perform the real-time beat\ntracking for an audio signal. We use Onset Strength Signal (OSS) to detect the\nonsets and estimate the tempos. Then, we form Cumulative Beat Strength Signal\n(CBSS) by taking advantage of OSS and estimated tempos. Next, we perform peak\ndetection by extracting the periodic sequence of beats among all CBSS peaks. In\nsimulations, we can see that our proposed algorithm, Online Beat TrAckINg\n(OBTAIN), outperforms state-of-art results in terms of prediction accuracy\nwhile maintaining comparable and practical computational complexity. The\nreal-time performance is tractable visually as illustrated in the simulations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 13:15:15 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 19:36:55 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Mottaghi", "Ali", ""], ["Behdin", "Kayhan", ""], ["Esmaeili", "Ashkan", ""], ["Heydari", "Mohammadreza", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1704.02227", "submitter": "Maciej Zieba", "authors": "Maciej Zieba, Lei Wang", "title": "Training Triplet Networks with GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triplet networks are widely used models that are characterized by good\nperformance in classification and retrieval tasks. In this work we propose to\ntrain a triplet network by putting it as the discriminator in Generative\nAdversarial Nets (GANs). We make use of the good capability of representation\nlearning of the discriminator to increase the predictive quality of the model.\nWe evaluated our approach on Cifar10 and MNIST datasets and observed\nsignificant improvement on the classification performance using the simple k-nn\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 17:09:20 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Zieba", "Maciej", ""], ["Wang", "Lei", ""]]}, {"id": "1704.02232", "submitter": "Sejun Park", "authors": "Sejun Park, Yunhun Jang, Andreas Galanis, Jinwoo Shin, Daniel\n  Stefankovic, Eric Vigoda", "title": "Rapid Mixing Swendsen-Wang Sampler for Stochastic Partitioned Attractive\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gibbs sampler is a particularly popular Markov chain used for learning\nand inference problems in Graphical Models (GMs). These tasks are\ncomputationally intractable in general, and the Gibbs sampler often suffers\nfrom slow mixing. In this paper, we study the Swendsen-Wang dynamics which is a\nmore sophisticated Markov chain designed to overcome bottlenecks that impede\nthe Gibbs sampler. We prove O(\\log n) mixing time for attractive binary\npairwise GMs (i.e., ferromagnetic Ising models) on stochastic partitioned\ngraphs having n vertices, under some mild conditions, including low temperature\nregions where the Gibbs sampler provably mixes exponentially slow. Our\nexperiments also confirm that the Swendsen-Wang sampler significantly\noutperforms the Gibbs sampler when they are used for learning parameters of\nattractive GMs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 05:12:36 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Park", "Sejun", ""], ["Jang", "Yunhun", ""], ["Galanis", "Andreas", ""], ["Shin", "Jinwoo", ""], ["Stefankovic", "Daniel", ""], ["Vigoda", "Eric", ""]]}, {"id": "1704.02239", "submitter": "Nicolas Tremblay", "authors": "Nicolas Tremblay (1), Simon Barthelme (2), Pierre-Olivier Amblard (1)\n  ((1) CNRS, GIPSA-CICS (2) CNRS, GIPSA-VIBS)", "title": "\\'Echantillonnage de signaux sur graphes via des processus\n  d\\'eterminantaux", "comments": "in French", "journal-ref": "GRETSI, Sep 2017, Juan-les-Pins, France", "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling k-bandlimited graph signals, ie, linear\ncombinations of the first k graph Fourier modes. We know that a set of k nodes\nembedding all k-bandlimited signals always exists, thereby enabling their\nperfect reconstruction after sampling. Unfortunately, to exhibit such a set,\none needs to partially diagonalize the graph Laplacian, which becomes\nprohibitive at large scale. We propose a novel strategy based on determinantal\npoint processes that side-steps partial diagonalisation and enables\nreconstruction with only O(k) samples. While doing so, we exhibit a new general\nalgorithm to sample determinantal process, faster than the state-of-the-art\nalgorithm by an order k.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 14:11:36 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 09:34:13 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Tremblay", "Nicolas", "", "CNRS, GIPSA-CICS"], ["Barthelme", "Simon", "", "CNRS, GIPSA-VIBS"], ["Amblard", "Pierre-Olivier", "", "CNRS, GIPSA-CICS"]]}, {"id": "1704.02254", "submitter": "Silvia Chiappa", "authors": "Silvia Chiappa and S\\'ebastien Racaniere and Daan Wierstra and Shakir\n  Mohamed", "title": "Recurrent Environment Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that can simulate how environments change in response to actions can\nbe used by agents to plan and act efficiently. We improve on previous\nenvironment simulators from high-dimensional pixel observations by introducing\nrecurrent neural networks that are able to make temporally and spatially\ncoherent predictions for hundreds of time-steps into the future. We present an\nin-depth analysis of the factors affecting performance, providing the most\nextensive attempt to advance the understanding of the properties of these\nmodels. We address the issue of computationally inefficiency with a model that\ndoes not need to generate a high-dimensional image at each time-step. We show\nthat our approach can be used to improve exploration and is adaptable to many\ndiverse environments, namely 10 Atari games, a 3D car racing environment, and\ncomplex 3D mazes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 14:53:54 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 15:43:32 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Chiappa", "Silvia", ""], ["Racaniere", "S\u00e9bastien", ""], ["Wierstra", "Daan", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1704.02263", "submitter": "Edilson Anselmo Corr\\^ea J\\'unior", "authors": "Edilson A. Corr\\^ea Jr., Vanessa Queiroz Marinho, Leandro Borges dos\n  Santos", "title": "NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter\n  Sentiment Analysis", "comments": "Published in Proceedings of SemEval-2017, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our multi-view ensemble approach to SemEval-2017 Task 4\non Sentiment Analysis in Twitter, specifically, the Message Polarity\nClassification subtask for English (subtask A). Our system is a voting\nensemble, where each base classifier is trained in a different feature space.\nThe first space is a bag-of-words model and has a Linear SVM as base\nclassifier. The second and third spaces are two different strategies of\ncombining word embeddings to represent sentences and use a Linear SVM and a\nLogistic Regressor as base classifiers. The proposed system was ranked 18th out\nof 38 systems considering F1 score and 20th considering recall.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 15:27:10 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Corr\u00eaa", "Edilson A.", "Jr."], ["Marinho", "Vanessa Queiroz", ""], ["Santos", "Leandro Borges dos", ""]]}, {"id": "1704.02281", "submitter": "Subhojyoti Mukherjee", "authors": "Subhojyoti Mukherjee, K. P. Naveen, Nandan Sudarsanam, Balaraman\n  Ravindran", "title": "Thresholding Bandits with Augmented UCB", "comments": "7 pages, Accepted at Proceedings of the 26th International Joint\n  Conference on Artificial Intelligence, 2017, 2515-2521", "journal-ref": "Proceedings of the 26th International Joint Conference on\n  Artificial Intelligence, 2017, 2515-2521", "doi": "10.24963/ijcai.2017/350", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the Augmented-UCB (AugUCB) algorithm for a\nfixed-budget version of the thresholding bandit problem (TBP), where the\nobjective is to identify a set of arms whose quality is above a threshold. A\nkey feature of AugUCB is that it uses both mean and variance estimates to\neliminate arms that have been sufficiently explored; to the best of our\nknowledge this is the first algorithm to employ such an approach for the\nconsidered TBP. Theoretically, we obtain an upper bound on the loss\n(probability of mis-classification) incurred by AugUCB. Although UCBEV in\nliterature provides a better guarantee, it is important to emphasize that UCBEV\nhas access to problem complexity (whose computation requires arms' mean and\nvariances), and hence is not realistic in practice; this is in contrast to\nAugUCB whose implementation does not require any such complexity inputs. We\nconduct extensive simulation experiments to validate the performance of AugUCB.\nThrough our simulation work, we establish that AugUCB, owing to its utilization\nof variance estimates, performs significantly better than the state-of-the-art\nAPT, CSAR and other non variance-based algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 16:31:13 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 12:19:48 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2019 21:43:30 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Mukherjee", "Subhojyoti", ""], ["Naveen", "K. P.", ""], ["Sudarsanam", "Nandan", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1704.02298", "submitter": "Rose Catherine", "authors": "Rose Catherine, William Cohen", "title": "TransNets: Learning to Transform for Recommendation", "comments": "Accepted for publication in the 11th ACM Conference on Recommender\n  Systems (RecSys 2017)", "journal-ref": null, "doi": "10.1145/3109859.3109878", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning methods have been shown to improve the performance of\nrecommender systems over traditional methods, especially when review text is\navailable. For example, a recent model, DeepCoNN, uses neural nets to learn one\nlatent representation for the text of all reviews written by a target user, and\na second latent representation for the text of all reviews for a target item,\nand then combines these latent representations to obtain state-of-the-art\nperformance on recommendation tasks. We show that (unsurprisingly) much of the\npredictive value of review text comes from reviews of the target user for the\ntarget item. We then introduce a way in which this information can be used in\nrecommendation, even when the target user's review for the target item is not\navailable. Our model, called TransNets, extends the DeepCoNN model by\nintroducing an additional latent layer representing the target user-target item\npair. We then regularize this layer, at training time, to be similar to another\nlatent representation of the target user's review of the target item. We show\nthat TransNets and extensions of it improve substantially over the previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:13:03 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 15:14:22 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Catherine", "Rose", ""], ["Cohen", "William", ""]]}, {"id": "1704.02304", "submitter": "Dmitry Ulyanov", "authors": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky", "title": "It Takes (Only) Two: Adversarial Generator-Encoder Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new autoencoder-type architecture that is trainable in an\nunsupervised mode, sustains both generation and inference, and has the quality\nof conditional and unconditional samples boosted by adversarial learning.\nUnlike previous hybrids of autoencoders and adversarial networks, the\nadversarial game in our approach is set up directly between the encoder and the\ngenerator, and no external mappings are trained in the process of learning. The\ngame objective compares the divergences of each of the real and the generated\ndata distributions with the prior distribution in the latent space. We show\nthat direct generator-vs-encoder game leads to a tight coupling of the two\ncomponents, resulting in samples and reconstructions of a comparable quality to\nsome recently-proposed more complex architectures.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:38:29 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:09:41 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 15:05:03 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ulyanov", "Dmitry", ""], ["Vedaldi", "Andrea", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1704.02345", "submitter": "Ershad Banijamali Mr.", "authors": "Ershad Banijamali, Ali Ghodsi", "title": "Fast Spectral Clustering Using Autoencoders and Landmarks", "comments": "8 Pages- Accepted in 14th International Conference on Image Analysis\n  and Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an algorithm for performing spectral clustering\nefficiently. Spectral clustering is a powerful clustering algorithm that\nsuffers from high computational complexity, due to eigen decomposition. In this\nwork, we first build the adjacency matrix of the corresponding graph of the\ndataset. To build this matrix, we only consider a limited number of points,\ncalled landmarks, and compute the similarity of all data points with the\nlandmarks. Then, we present a definition of the Laplacian matrix of the graph\nthat enable us to perform eigen decomposition efficiently, using a deep\nautoencoder. The overall complexity of the algorithm for eigen decomposition is\n$O(np)$, where $n$ is the number of data points and $p$ is the number of\nlandmarks. At last, we evaluate the performance of the algorithm in different\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 18:40:52 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Banijamali", "Ershad", ""], ["Ghodsi", "Ali", ""]]}, {"id": "1704.02346", "submitter": "Luciana Ferrer", "authors": "Luciana Ferrer", "title": "Joint Probabilistic Linear Discriminant Analysis", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard probabilistic linear discriminant analysis (PLDA) for speaker\nrecognition assumes that the sample's features (usually, i-vectors) are given\nby a sum of three terms: a term that depends on the speaker identity, a term\nthat models the within-speaker variability and is assumed independent across\nsamples, and a final term that models any remaining variability and is also\nindependent across samples. In this work, we propose a generalization of this\nmodel where the within-speaker variability is not necessarily assumed\nindependent across samples but dependent on another discrete variable. This\nvariable, which we call the channel variable as in the standard PLDA approach,\ncould be, for example, a discrete category for the channel characteristics, the\nlanguage spoken by the speaker, the type of speech in the sample\n(conversational, monologue, read), etc. The value of this variable is assumed\nto be known during training but not during testing. Scoring is performed, as in\nstandard PLDA, by computing a likelihood ratio between the null hypothesis that\nthe two sides of a trial belong to the same speaker versus the alternative\nhypothesis that the two sides belong to different speakers. The two likelihoods\nare computed by marginalizing over two hypothesis about the channels in both\nsides of a trial: that they are the same and that they are different. This way,\nwe expect that the new model will be better at coping with same-channel versus\ndifferent-channel trials than standard PLDA, since knowledge about the channel\n(or language, or speech style) is used during training and implicitly\nconsidered during scoring.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 18:42:05 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 15:16:56 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Ferrer", "Luciana", ""]]}, {"id": "1704.02360", "submitter": "Yuki Saito", "authors": "Hiroyuki Miyoshi, Yuki Saito, Shinnosuke Takamichi, and Hiroshi\n  Saruwatari", "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context\n  Posterior Probabilities", "comments": "Accepted to INTERSPEECH 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice conversion (VC) using sequence-to-sequence learning of context\nposterior probabilities is proposed. Conventional VC using shared context\nposterior probabilities predicts target speech parameters from the context\nposterior probabilities estimated from the source speech parameters. Although\nconventional VC can be built from non-parallel data, it is difficult to convert\nspeaker individuality such as phonetic property and speaking rate contained in\nthe posterior probabilities because the source posterior probabilities are\ndirectly used for predicting target speech parameters. In this work, we assume\nthat the training data partly include parallel speech data and propose\nsequence-to-sequence learning between the source and target posterior\nprobabilities. The conversion models perform non-linear and variable-length\ntransformation from the source probability sequence to the target one. Further,\nwe propose a joint training algorithm for the modules. In contrast to\nconventional VC, which separately trains the speech recognition that estimates\nposterior probabilities and the speech synthesis that predicts target speech\nparameters, our proposed method jointly trains these modules along with the\nproposed probability conversion modules. Experimental results demonstrate that\nour approach outperforms the conventional VC.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 12:35:33 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 04:43:37 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 08:11:02 GMT"}, {"version": "v4", "created": "Mon, 7 Aug 2017 02:42:01 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Miyoshi", "Hiroyuki", ""], ["Saito", "Yuki", ""], ["Takamichi", "Shinnosuke", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "1704.02373", "submitter": "Achintya Sarkar", "authors": "Achintya Kr. Sarkar and Zheng-Hua Tan", "title": "Time-Contrastive Learning Based DNN Bottleneck Features for\n  Text-Dependent Speaker Verification", "comments": null, "journal-ref": "NIPS Time Series Workshop 2017, Long Beach, CA, USA", "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a time-contrastive learning (TCL) based bottleneck\n(BN)feature extraction method for speech signals with an application to\ntext-dependent (TD) speaker verification (SV). It is well-known that speech\nsignals exhibit quasi-stationary behavior in and only in a short interval, and\nthe TCL method aims to exploit this temporal structure. More specifically, it\ntrains deep neural networks (DNNs) to discriminate temporal events obtained by\nuniformly segmenting speech signals, in contrast to existing DNN based BN\nfeature extraction methods that train DNNs using labeled data to discriminate\nspeakers or pass-phrases or phones or a combination of them. In the context of\nspeaker verification, speech data of fixed pass-phrases are used for TCL-BN\ntraining, while the pass-phrases used for TCL-BN training are excluded from\nbeing used for SV, so that the learned features can be considered generic. The\nmethod is evaluated on the RedDots Challenge 2016 database. Experimental\nresults show that TCL-BN is superior to the existing speaker and pass-phrase\ndiscriminant BN features and the Mel-frequency cepstral coefficient feature for\ntext-dependent speaker verification.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 09:37:41 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 16:56:31 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 16:19:20 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Sarkar", "Achintya Kr.", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "1704.02378", "submitter": "Amit Dhurandhar", "authors": "Amit Dhurandhar and Margareta Ackerman and Xiang Wang", "title": "Uncovering Group Level Insights with Accordant Clustering", "comments": "accepted to SDM 2017 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a widely-used data mining tool, which aims to discover\npartitions of similar items in data. We introduce a new clustering paradigm,\n\\emph{accordant clustering}, which enables the discovery of (predefined) group\nlevel insights. Unlike previous clustering paradigms that aim to understand\nrelationships amongst the individual members, the goal of accordant clustering\nis to uncover insights at the group level through the analysis of their\nmembers. Group level insight can often support a call to action that cannot be\ninformed through previous clustering techniques. We propose the first accordant\nclustering algorithm, and prove that it finds near-optimal solutions when data\npossesses inherent cluster structure. The insights revealed by accordant\nclusterings enabled experts in the field of medicine to isolate successful\ntreatments for a neurodegenerative disease, and those in finance to discover\npatterns of unnecessary spending.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 21:31:59 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Ackerman", "Margareta", ""], ["Wang", "Xiang", ""]]}, {"id": "1704.02399", "submitter": "Jian Peng", "authors": "Yang Liu, Prajit Ramachandran, Qiang Liu, Jian Peng", "title": "Stein Variational Policy Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Policy gradient methods have been successfully applied to many complex\nreinforcement learning problems. However, policy gradient methods suffer from\nhigh variance, slow convergence, and inefficient exploration. In this work, we\nintroduce a maximum entropy policy optimization framework which explicitly\nencourages parameter exploration, and show that this framework can be reduced\nto a Bayesian inference problem. We then propose a novel Stein variational\npolicy gradient method (SVPG) which combines existing policy gradient methods\nand a repulsive functional to generate a set of diverse but well-behaved\npolicies. SVPG is robust to initialization and can easily be implemented in a\nparallel manner. On continuous control problems, we find that implementing SVPG\non top of REINFORCE and advantage actor-critic algorithms improves both average\nreturn and data efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 23:24:07 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Liu", "Yang", ""], ["Ramachandran", "Prajit", ""], ["Liu", "Qiang", ""], ["Peng", "Jian", ""]]}, {"id": "1704.02532", "submitter": "Senthil Yogamani", "authors": "Ahmad El Sallab, Mohammed Abdou, Etienne Perot and Senthil Yogamani", "title": "Deep Reinforcement Learning framework for Autonomous Driving", "comments": "Reprinted with permission of IS&T: The Society for Imaging Science\n  and Technology, sole copyright owners of Electronic Imaging, Autonomous\n  Vehicles and Machines 2017", "journal-ref": "IS&T Electronic Imaging, Autonomous Vehicles and Machines 2017,\n  AVM-023, pg. 70-76 (2017)", "doi": "10.2352/ISSN.2470-1173.2017.19.AVM-023", "report-no": null, "categories": "stat.ML cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning is considered to be a strong AI paradigm which can be\nused to teach machines through interaction with the environment and learning\nfrom their mistakes. Despite its perceived utility, it has not yet been\nsuccessfully applied in automotive applications. Motivated by the successful\ndemonstrations of learning of Atari games and Go by Google DeepMind, we propose\na framework for autonomous driving using deep reinforcement learning. This is\nof particular relevance as it is difficult to pose autonomous driving as a\nsupervised learning problem due to strong interactions with the environment\nincluding other vehicles, pedestrians and roadworks. As it is a relatively new\narea of research for autonomous driving, we provide a short overview of deep\nreinforcement learning and then describe our proposed framework. It\nincorporates Recurrent Neural Networks for information integration, enabling\nthe car to handle partially observable scenarios. It also integrates the recent\nwork on attention models to focus on relevant information, thereby reducing the\ncomputational complexity for deployment on embedded hardware. The framework was\ntested in an open source 3D car racing simulator called TORCS. Our simulation\nresults demonstrate learning of autonomous maneuvering in a scenario of complex\nroad curvatures and simple interaction of other vehicles.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 20:04:03 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Sallab", "Ahmad El", ""], ["Abdou", "Mohammed", ""], ["Perot", "Etienne", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1704.02592", "submitter": "Keigo Kimura", "authors": "Keigo Kimura and Lu Sun and Mineichi Kudo", "title": "MLC Toolbox: A MATLAB/OCTAVE Library for Multi-Label Classification", "comments": "Instruction pages are now under construction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Label Classification toolbox is a MATLAB/OCTAVE library for Multi-Label\nClassification (MLC). There exists a few Java libraries for MLC, but no\nMATLAB/OCTAVE library that covers various methods. This toolbox offers an\nenvironment for evaluation, comparison and visualization of the MLC results.\nOne attraction of this toolbox is that it enables us to try many combinations\nof feature space dimension reduction, sample clustering, label space dimension\nreduction and ensemble, etc.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 12:36:04 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Kimura", "Keigo", ""], ["Sun", "Lu", ""], ["Kudo", "Mineichi", ""]]}, {"id": "1704.02598", "submitter": "Vasilis Syrgkanis", "authors": "Vasilis Syrgkanis", "title": "A Sample Complexity Measure with Applications to Learning Optimal\n  Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new sample complexity measure, which we refer to as\nsplit-sample growth rate. For any hypothesis $H$ and for any sample $S$ of size\n$m$, the split-sample growth rate $\\hat{\\tau}_H(m)$ counts how many different\nhypotheses can empirical risk minimization output on any sub-sample of $S$ of\nsize $m/2$. We show that the expected generalization error is upper bounded by\n$O\\left(\\sqrt{\\frac{\\log(\\hat{\\tau}_H(2m))}{m}}\\right)$. Our result is enabled\nby a strengthening of the Rademacher complexity analysis of the expected\ngeneralization error. We show that this sample complexity measure, greatly\nsimplifies the analysis of the sample complexity of optimal auction design, for\nmany auction classes studied in the literature. Their sample complexity can be\nderived solely by noticing that in these auction classes, ERM on any sample or\nsub-sample will pick parameters that are equal to one of the points in the\nsample.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 13:17:52 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 12:52:22 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Syrgkanis", "Vasilis", ""]]}, {"id": "1704.02654", "submitter": "Daniel Cullina", "authors": "Arjun Nitin Bhagoji, Daniel Cullina, Chawin Sitawarin, Prateek Mittal", "title": "Enhancing Robustness of Machine Learning Systems via Data\n  Transformations", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of data transformations as a defense against evasion\nattacks on ML classifiers. We present and investigate strategies for\nincorporating a variety of data transformations including dimensionality\nreduction via Principal Component Analysis and data `anti-whitening' to enhance\nthe resilience of machine learning, targeting both the classification and the\ntraining phase. We empirically evaluate and demonstrate the feasibility of\nlinear transformations of data as a defense mechanism against evasion attacks\nusing multiple real-world datasets. Our key findings are that the defense is\n(i) effective against the best known evasion attacks from the literature,\nresulting in a two-fold increase in the resources required by a white-box\nadversary with knowledge of the defense for a successful attack, (ii)\napplicable across a range of ML classifiers, including Support Vector Machines\nand Deep Neural Networks, and (iii) generalizable to multiple application\ndomains, including image classification and human activity classification.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 20:21:43 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 01:53:58 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 03:00:16 GMT"}, {"version": "v4", "created": "Wed, 29 Nov 2017 22:41:25 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Bhagoji", "Arjun Nitin", ""], ["Cullina", "Daniel", ""], ["Sitawarin", "Chawin", ""], ["Mittal", "Prateek", ""]]}, {"id": "1704.02665", "submitter": "Sadegh Eskandari", "authors": "Sadegh Eskandari, Emre Akbas", "title": "Supervised Infinite Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new feature selection method that is suitable for\nboth unsupervised and supervised problems. We build upon the recently proposed\nInfinite Feature Selection (IFS) method where feature subsets of all sizes\n(including infinity) are considered. We extend IFS in two ways. First, we\npropose a supervised version of it. Second, we propose new ways of forming the\nfeature adjacency matrix that perform better for unsupervised problems. We\nextensively evaluate our methods on many benchmark datasets, including large\nimage-classification datasets (PASCAL VOC), and show that our methods\noutperform both the IFS and the widely used \"minimum-redundancy\nmaximum-relevancy (mRMR)\" feature selection algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 21:58:47 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 12:32:57 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 08:33:26 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Eskandari", "Sadegh", ""], ["Akbas", "Emre", ""]]}, {"id": "1704.02681", "submitter": "Vincenzo Liguori", "authors": "Vincenzo Liguori", "title": "Pyramid Vector Quantization for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of Pyramid Vector Quantization (PVQ) to reduce\nthe computational cost for a variety of neural networks (NNs) while, at the\nsame time, compressing the weights that describe them. This is based on the\nfact that the dot product between an N dimensional vector of real numbers and\nan N dimensional PVQ vector can be calculated with only additions and\nsubtractions and one multiplication. This is advantageous since tensor\nproducts, commonly used in NNs, can be re-conduced to a dot product or a set of\ndot products. Finally, it is stressed that any NN architecture that is based on\nan operation that can be re-conduced to a dot product can benefit from the\ntechniques described here.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 01:17:43 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Liguori", "Vincenzo", ""]]}, {"id": "1704.02685", "submitter": "Avanti Shrikumar", "authors": "Avanti Shrikumar, Peyton Greenside, Anshul Kundaje", "title": "Learning Important Features Through Propagating Activation Differences", "comments": "Updated to include changes present in the ICML camera-ready paper,\n  and other small corrections", "journal-ref": "PMLR 70:3145-3153, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purported \"black box\" nature of neural networks is a barrier to adoption\nin applications where interpretability is essential. Here we present DeepLIFT\n(Deep Learning Important FeaTures), a method for decomposing the output\nprediction of a neural network on a specific input by backpropagating the\ncontributions of all neurons in the network to every feature of the input.\nDeepLIFT compares the activation of each neuron to its 'reference activation'\nand assigns contribution scores according to the difference. By optionally\ngiving separate consideration to positive and negative contributions, DeepLIFT\ncan also reveal dependencies which are missed by other approaches. Scores can\nbe computed efficiently in a single backward pass. We apply DeepLIFT to models\ntrained on MNIST and simulated genomic data, and show significant advantages\nover gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides:\nbit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code:\nhttp://goo.gl/RM8jvH.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 02:23:57 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 22:13:28 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Shrikumar", "Avanti", ""], ["Greenside", "Peyton", ""], ["Kundaje", "Anshul", ""]]}, {"id": "1704.02686", "submitter": "Shuchin Aeron", "authors": "Eric Bailey and Shuchin Aeron", "title": "Word Embeddings via Tensor Factorization", "comments": "More simulation results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most popular word embedding techniques involve implicit or explicit\nfactorization of a word co-occurrence based matrix into low rank factors. In\nthis paper, we aim to generalize this trend by using numerical methods to\nfactor higher-order word co-occurrence based arrays, or \\textit{tensors}. We\npresent four word embeddings using tensor factorization and analyze their\nadvantages and disadvantages. One of our main contributions is a novel joint\nsymmetric tensor factorization technique related to the idea of coupled tensor\nfactorization. We show that embeddings based on tensor factorization can be\nused to discern the various meanings of polysemous words without being\nexplicitly trained to do so, and motivate the intuition behind why this works\nin a way that doesn't with existing methods. We also modify an existing word\nembedding evaluation metric known as Outlier Detection [Camacho-Collados and\nNavigli, 2016] to evaluate the quality of the order-$N$ relations that a word\nembedding captures, and show that tensor-based methods outperform existing\nmatrix-based methods at this task. Experimentally, we show that all of our word\nembeddings either outperform or are competitive with state-of-the-art baselines\ncommonly used today on a variety of recent datasets. Suggested applications of\ntensor factorization-based word embeddings are given, and all source code and\npre-trained vectors are publicly available online.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 02:24:37 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 18:56:30 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bailey", "Eric", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1704.02708", "submitter": "Richard Nock", "authors": "Richard Nock, Frank Nielsen", "title": "Evolving a Vector Space with any Generating Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Valiant's model of evolution, a class of representations is evolvable iff\na polynomial-time process of random mutations guided by selection converges\nwith high probability to a representation as $\\epsilon$-close as desired from\nthe optimal one, for any required $\\epsilon>0$. Several previous positive\nresults exist that can be related to evolving a vector space, but each former\nresult imposes disproportionate representations or restrictions on\n(re)initialisations, distributions, performance functions and/or the mutator.\nIn this paper, we show that all it takes to evolve a normed vector space is\nmerely a set that generates the space. Furthermore, it takes only\n$\\tilde{O}(1/\\epsilon^2)$ steps and it is essentially stable, agnostic and\nhandles target drifts that rival some proven in fairly restricted settings. Our\nalgorithm can be viewed as a close relative to a popular fifty-years old\ngradient-free optimization method for which little is still known from the\nconvergence standpoint: Nelder-Mead simplex method.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 04:41:38 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 21:47:47 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Nock", "Richard", ""], ["Nielsen", "Frank", ""]]}, {"id": "1704.02712", "submitter": "Zheng Xu", "authors": "Zheng Xu, Mario A. T. Figueiredo, Xiaoming Yuan, Christoph Studer, and\n  Tom Goldstein", "title": "Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern computer vision and machine learning applications rely on solving\ndifficult optimization problems that involve non-differentiable objective\nfunctions and constraints. The alternating direction method of multipliers\n(ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a\ngeneralization of ADMM that often achieves better performance, but its\nefficiency depends strongly on algorithm parameters that must be chosen by an\nexpert user. We propose an adaptive method that automatically tunes the key\nalgorithm parameters to achieve optimal performance without user oversight.\nInspired by recent work on adaptivity, the proposed adaptive relaxed ADMM\n(ARADMM) is derived by assuming a Barzilai-Borwein style linear gradient. A\ndetailed convergence analysis of ARADMM is provided, and numerical results on\nseveral applications demonstrate fast practical convergence.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 05:07:38 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Xu", "Zheng", ""], ["Figueiredo", "Mario A. T.", ""], ["Yuan", "Xiaoming", ""], ["Studer", "Christoph", ""], ["Goldstein", "Tom", ""]]}, {"id": "1704.02718", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c, Alex Olshevsky and C\\'esar A. Uribe", "title": "Distributed Learning for Cooperative Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of cooperative inference where a group of agents\ninteract over a network and seek to estimate a joint parameter that best\nexplains a set of observations. Agents do not know the network topology or the\nobservations of other agents. We explore a variational interpretation of the\nBayesian posterior density, and its relation to the stochastic mirror descent\nalgorithm, to propose a new distributed learning algorithm. We show that, under\nappropriate assumptions, the beliefs generated by the proposed algorithm\nconcentrate around the true parameter exponentially fast. We provide explicit\nnon-asymptotic bounds for the convergence rate. Moreover, we develop explicit\nand computationally efficient algorithms for observation models belonging to\nexponential families.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 06:04:34 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1704.02771", "submitter": "Luca Martino", "authors": "L. Martino, V. Elvira, G. Camps-Valls", "title": "Group Importance Sampling for Particle Filtering and MCMC", "comments": "To appear in Digital Signal Processing. Related Matlab demos are\n  provided at https://github.com/lukafree/GIS.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods and their implementations by means of sophisticated Monte\nCarlo techniques have become very popular in signal processing over the last\nyears. Importance Sampling (IS) is a well-known Monte Carlo technique that\napproximates integrals involving a posterior distribution by means of weighted\nsamples. In this work, we study the assignation of a single weighted sample\nwhich compresses the information contained in a population of weighted samples.\nPart of the theory that we present as Group Importance Sampling (GIS) has been\nemployed implicitly in different works in the literature. The provided analysis\nyields several theoretical and practical consequences. For instance, we discuss\nthe application of GIS into the Sequential Importance Resampling framework and\nshow that Independent Multiple Try Metropolis schemes can be interpreted as a\nstandard Metropolis-Hastings algorithm, following the GIS approach. We also\nintroduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS.\nThe first one, named Group Metropolis Sampling method, produces a Markov chain\nof sets of weighted samples. All these sets are then employed for obtaining a\nunique global estimator. The second one is the Distributed Particle\nMetropolis-Hastings technique, where different parallel particle filters are\njointly used to drive an MCMC algorithm. Different resampled trajectories are\ncompared and then tested with a proper acceptance probability. The novel\nschemes are tested in different numerical experiments such as learning the\nhyperparameters of Gaussian Processes, two localization problems in a wireless\nsensor network (with synthetic and real data) and the tracking of vegetation\nparameters given satellite observations, where they are compared with several\nbenchmark Monte Carlo techniques. Three illustrative Matlab demos are also\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 09:20:47 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 14:22:22 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 20:51:47 GMT"}, {"version": "v4", "created": "Sat, 4 Aug 2018 09:19:51 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Martino", "L.", ""], ["Elvira", "V.", ""], ["Camps-Valls", "G.", ""]]}, {"id": "1704.02789", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Plamen P. Angelov, Edwin Lughofer", "title": "Parsimonious Random Vector Functional Link Network for Data Streams", "comments": "this paper is submitted for publication in Information Sciences", "journal-ref": null, "doi": "10.1016/j.ins.2017.11.050", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of random vector functional link network (RVFLN) has provided a\nbreakthrough in the design of neural networks (NNs) since it conveys solid\ntheoretical justification of randomized learning. Existing works in RVFLN are\nhardly scalable for data stream analytics because they are inherent to the\nissue of complexity as a result of the absence of structural learning\nscenarios. A novel class of RVLFN, namely parsimonious random vector functional\nlink network (pRVFLN), is proposed in this paper. pRVFLN features an open\nstructure paradigm where its network structure can be built from scratch and\ncan be automatically generated in accordance with degree of nonlinearity and\ntime-varying property of system being modelled. pRVFLN is equipped with\ncomplexity reduction scenarios where inconsequential hidden nodes can be pruned\nand input features can be dynamically selected. pRVFLN puts into perspective an\nonline active learning mechanism which expedites the training process and\nrelieves operator labelling efforts. In addition, pRVFLN introduces a\nnon-parametric type of hidden node, developed using an interval-valued data\ncloud. The hidden node completely reflects the real data distribution and is\nnot constrained by a specific shape of the cluster. All learning procedures of\npRVFLN follow a strictly single-pass learning mode, which is applicable for an\nonline real-time deployment. The efficacy of pRVFLN was rigorously validated\nthrough numerous simulations and comparisons with state-of-the art algorithms\nwhere it produced the most encouraging numerical results. Furthermore, the\nrobustness of pRVFLN was investigated and a new conclusion is made to the scope\nof random parameters where it plays vital role to the success of randomized\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 10:24:34 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 11:59:53 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Angelov", "Plamen P.", ""], ["Lughofer", "Edwin", ""]]}, {"id": "1704.02798", "submitter": "Meire Fortunato", "authors": "Meire Fortunato, Charles Blundell, Oriol Vinyals", "title": "Bayesian Recurrent Neural Networks", "comments": "12th Women in Machine Learning Workshop (WiML 2017), co-located with\n  the 31st Conference on Neural Information Processing Systems (NeurIPS 2017),\n  Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore a straightforward variational Bayes scheme for\nRecurrent Neural Networks. Firstly, we show that a simple adaptation of\ntruncated backpropagation through time can yield good quality uncertainty\nestimates and superior regularisation at only a small extra computational cost\nduring training, also reducing the amount of parameters by 80\\%. Secondly, we\ndemonstrate how a novel kind of posterior approximation yields further\nimprovements to the performance of Bayesian RNNs. We incorporate local gradient\ninformation into the approximate posterior to sharpen it around the current\nbatch statistics. We show how this technique is not exclusive to recurrent\nneural networks and can be applied more widely to train Bayesian neural\nnetworks. We also empirically demonstrate how Bayesian RNNs are superior to\ntraditional RNNs on a language modelling benchmark and an image captioning\ntask, as well as showing how each of these methods improve our model over a\nvariety of other schemes for training them. We also introduce a new benchmark\nfor studying uncertainty for language models so future methods can be easily\ncompared.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 10:59:05 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 17:25:08 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 12:14:16 GMT"}, {"version": "v4", "created": "Thu, 9 May 2019 22:04:45 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Fortunato", "Meire", ""], ["Blundell", "Charles", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1704.02799", "submitter": "Mohammad Azzeh", "authors": "Israa Ahmed Zriqat, Ahmad Mousa Altamimi, Mohammad Azzeh", "title": "A Comparative Study for Predicting Heart Diseases Using Data Mining\n  Classification Methods", "comments": null, "journal-ref": "ISSN 1947-5500", "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the precision of heart diseases detection has been investigated by\nmany researchers in the literature. Such improvement induced by the\noverwhelming health care expenditures and erroneous diagnosis. As a result,\nvarious methodologies have been proposed to analyze the disease factors aiming\nto decrease the physicians practice variation and reduce medical costs and\nerrors. In this paper, our main motivation is to develop an effective\nintelligent medical decision support system based on data mining techniques. In\nthis context, five data mining classifying algorithms, with large datasets,\nhave been utilized to assess and analyze the risk factors statistically related\nto heart diseases in order to compare the performance of the implemented\nclassifiers (e.g., Na\\\"ive Bayes, Decision Tree, Discriminant, Random Forest,\nand Support Vector Machine). To underscore the practical viability of our\napproach, the selected classifiers have been implemented using MATLAB tool with\ntwo datasets. Results of the conducted experiments showed that all\nclassification algorithms are predictive and can give relatively correct\nanswer. However, the decision tree outperforms other classifiers with an\naccuracy rate of 99.0% followed by Random forest. That is the case because both\nof them have relatively same mechanism but the Random forest can build ensemble\nof decision tree. Although ensemble learning has been proved to produce\nsuperior results, but in our case the decision tree has outperformed its\nensemble version.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 11:03:14 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Zriqat", "Israa Ahmed", ""], ["Altamimi", "Ahmad Mousa", ""], ["Azzeh", "Mohammad", ""]]}, {"id": "1704.02801", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa and Mihaela van der Schaar", "title": "Bayesian Inference of Individualized Treatment Effects using Multi-task\n  Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicated on the increasing abundance of electronic health records, we\ninvesti- gate the problem of inferring individualized treatment effects using\nobservational data. Stemming from the potential outcomes model, we propose a\nnovel multi- task learning framework in which factual and counterfactual\noutcomes are mod- eled as the outputs of a function in a vector-valued\nreproducing kernel Hilbert space (vvRKHS). We develop a nonparametric Bayesian\nmethod for learning the treatment effects using a multi-task Gaussian process\n(GP) with a linear coregion- alization kernel as a prior over the vvRKHS. The\nBayesian approach allows us to compute individualized measures of confidence in\nour estimates via pointwise credible intervals, which are crucial for realizing\nthe full potential of precision medicine. The impact of selection bias is\nalleviated via a risk-based empirical Bayes method for adapting the multi-task\nGP prior, which jointly minimizes the empirical error in factual outcomes and\nthe uncertainty in (unobserved) counter- factual outcomes. We conduct\nexperiments on observational datasets for an inter- ventional social program\napplied to premature infants, and a left ventricular assist device applied to\ncardiac patients wait-listed for a heart transplant. In both experi- ments, we\nshow that our method significantly outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 11:03:36 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 13:29:58 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1704.02848", "submitter": "Hai-Jun Zhou", "authors": "Huiling Zhen, Shang-Nan Wang, and Hai-Jun Zhou", "title": "Unsupervised prototype learning in an associative-memory network", "comments": "We found serious inconsistence between the numerical protocol\n  described in the text and the actual numerical code used by the first author\n  to produce the data. Because of this inconsistence, we decide to withdraw the\n  preprint. The corresponding author (Hai-Jun Zhou) deeply apologizes for not\n  being able to detect this inconsistence earlier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning in a generalized Hopfield associative-memory network is\ninvestigated in this work. First, we prove that the (generalized) Hopfield\nmodel is equivalent to a semi-restricted Boltzmann machine with a layer of\nvisible neurons and another layer of hidden binary neurons, so it could serve\nas the building block for a multilayered deep-learning system. We then\ndemonstrate that the Hopfield network can learn to form a faithful internal\nrepresentation of the observed samples, with the learned memory patterns being\nprototypes of the input data. Furthermore, we propose a spectral method to\nextract a small set of concepts (idealized prototypes) as the most concise\nsummary or abstraction of the empirical data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 13:20:23 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 02:45:12 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Zhen", "Huiling", ""], ["Wang", "Shang-Nan", ""], ["Zhou", "Hai-Jun", ""]]}, {"id": "1704.02882", "submitter": "Hadrien Hendrikx", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, Hadrien Hendrikx, Alexandre\n  Maurer", "title": "Dynamic Safe Interruptibility for Decentralized Multi-Agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, agents learn by performing actions and observing\ntheir outcomes. Sometimes, it is desirable for a human operator to\n\\textit{interrupt} an agent in order to prevent dangerous situations from\nhappening. Yet, as part of their learning process, agents may link these\ninterruptions, that impact their reward, to specific states and deliberately\navoid them. The situation is particularly challenging in a multi-agent context\nbecause agents might not only learn from their own past interruptions, but also\nfrom those of other agents. Orseau and Armstrong defined \\emph{safe\ninterruptibility} for one learner, but their work does not naturally extend to\nmulti-agent systems. This paper introduces \\textit{dynamic safe\ninterruptibility}, an alternative definition more suited to decentralized\nlearning problems, and studies this notion in two learning frameworks:\n\\textit{joint action learners} and \\textit{independent learners}. We give\nrealistic sufficient conditions on the learning algorithm to enable dynamic\nsafe interruptibility in the case of joint action learners, yet show that these\nconditions are not sufficient for independent learners. We show however that if\nagents can detect interruptions, it is possible to prune the observations to\nensure dynamic safe interruptibility even for independent learners.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 14:38:37 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 11:01:28 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Hendrikx", "Hadrien", ""], ["Maurer", "Alexandre", ""]]}, {"id": "1704.02890", "submitter": "Sven Banisch", "authors": "Sven Banisch and Eckehard Olbrich", "title": "Opinion Polarization by Learning from Social Feedback", "comments": "Presented at the Social Simulation Conference (Dublin 2017)", "journal-ref": "The Journal of Mathematical Sociology, 2018", "doi": "10.1080/0022250X.2018.1517761", "report-no": null, "categories": "physics.soc-ph cs.LG cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a new mechanism to explain polarization phenomena in opinion\ndynamics in which agents evaluate alternative views on the basis of the social\nfeedback obtained on expressing them. High support of the favored opinion in\nthe social environment, is treated as a positive feedback which reinforces the\nvalue associated to this opinion. In connected networks of sufficiently high\nmodularity, different groups of agents can form strong convictions of competing\nopinions. Linking the social feedback process to standard equilibrium concepts\nwe analytically characterize sufficient conditions for the stability of\nbi-polarization. While previous models have emphasized the polarization effects\nof deliberative argument-based communication, our model highlights an affective\nexperience-based route to polarization, without assumptions about negative\ninfluence or bounded confidence.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 14:12:02 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 21:37:32 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 13:14:38 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Banisch", "Sven", ""], ["Olbrich", "Eckehard", ""]]}, {"id": "1704.02901", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky, Nikos Komodakis", "title": "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on\n  Graphs", "comments": "Accepted to CVPR 2017; extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of problems can be formulated as prediction on graph-structured\ndata. In this work, we generalize the convolution operator from regular grids\nto arbitrary graphs while avoiding the spectral domain, which allows us to\nhandle graphs of varying size and connectivity. To move beyond a simple\ndiffusion, filter weights are conditioned on the specific edge labels in the\nneighborhood of a vertex. Together with the proper choice of graph coarsening,\nwe explore constructing deep neural networks for graph classification. In\nparticular, we demonstrate the generality of our formulation in point cloud\nclassification, where we set the new state of the art, and on a graph\nclassification dataset, where we outperform other deep learning approaches. The\nsource code is available at https://github.com/mys007/ecc\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:18:54 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 18:05:11 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 09:31:17 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Simonovsky", "Martin", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1704.02906", "submitter": "Viveka Kulharia", "authors": "Arnab Ghosh and Viveka Kulharia and Vinay Namboodiri and Philip H. S.\n  Torr and Puneet K. Dokania", "title": "Multi-Agent Diverse Generative Adversarial Networks", "comments": "This is an updated version of our CVPR'18 paper with the same title.\n  In this version, we also introduce MAD-GAN-Sim in Appendix B", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MAD-GAN, an intuitive generalization to the Generative Adversarial\nNetworks (GANs) and its conditional variants to address the well known problem\nof mode collapse. First, MAD-GAN is a multi-agent GAN architecture\nincorporating multiple generators and one discriminator. Second, to enforce\nthat different generators capture diverse high probability modes, the\ndiscriminator of MAD-GAN is designed such that along with finding the real and\nfake samples, it is also required to identify the generator that generated the\ngiven fake sample. Intuitively, to succeed in this task, the discriminator must\nlearn to push different generators towards different identifiable modes. We\nperform extensive experiments on synthetic and real datasets and compare\nMAD-GAN with different variants of GAN. We show high quality diverse sample\ngenerations for challenging tasks such as image-to-image translation and face\ngeneration. In addition, we also show that MAD-GAN is able to disentangle\ndifferent modalities when trained using highly challenging diverse-class\ndataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the\nend, we show its efficacy on the unsupervised feature representation task. In\nAppendix, we introduce a similarity based competing objective (MAD-GAN-Sim)\nwhich encourages different generators to generate diverse samples based on a\nuser defined similarity metric. We show its performance on the image-to-image\ntranslation, and also show its effectiveness on the unsupervised feature\nrepresentation task.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:26:23 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 23:29:16 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 16:21:52 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Ghosh", "Arnab", ""], ["Kulharia", "Viveka", ""], ["Namboodiri", "Vinay", ""], ["Torr", "Philip H. S.", ""], ["Dokania", "Puneet K.", ""]]}, {"id": "1704.02958", "submitter": "Arturs Backurs", "authors": "Arturs Backurs, Piotr Indyk, Ludwig Schmidt", "title": "On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel\n  Methods and Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization (ERM) is ubiquitous in machine learning and\nunderlies most supervised learning methods. While there has been a large body\nof work on algorithms for various ERM problems, the exact computational\ncomplexity of ERM is still not understood. We address this issue for multiple\npopular ERM problems including kernel SVMs, kernel ridge regression, and\ntraining the final layer of a neural network. In particular, we give\nconditional hardness results for these problems based on complexity-theoretic\nassumptions such as the Strong Exponential Time Hypothesis. Under these\nassumptions, we show that there are no algorithms that solve the aforementioned\nERM problems to high accuracy in sub-quadratic time. We also give similar\nhardness results for computing the gradient of the empirical loss, which is the\nmain computational burden in many non-convex learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 17:26:41 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1704.02971", "submitter": "Yao Qin", "authors": "Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and\n  Garrison Cottrell", "title": "A Dual-Stage Attention-Based Recurrent Neural Network for Time Series\n  Prediction", "comments": "International Joint Conference on Artificial Intelligence (IJCAI),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nonlinear autoregressive exogenous (NARX) model, which predicts the\ncurrent value of a time series based upon its previous values as well as the\ncurrent and past values of multiple driving (exogenous) series, has been\nstudied for decades. Despite the fact that various NARX models have been\ndeveloped, few of them can capture the long-term temporal dependencies\nappropriately and select the relevant driving series to make predictions. In\nthis paper, we propose a dual-stage attention-based recurrent neural network\n(DA-RNN) to address these two issues. In the first stage, we introduce an input\nattention mechanism to adaptively extract relevant driving series (a.k.a.,\ninput features) at each time step by referring to the previous encoder hidden\nstate. In the second stage, we use a temporal attention mechanism to select\nrelevant encoder hidden states across all time steps. With this dual-stage\nattention scheme, our model can not only make predictions effectively, but can\nalso be easily interpreted. Thorough empirical studies based upon the SML 2010\ndataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can\noutperform state-of-the-art methods for time series prediction.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 23:50:09 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 19:35:08 GMT"}, {"version": "v3", "created": "Mon, 10 Jul 2017 21:26:23 GMT"}, {"version": "v4", "created": "Mon, 14 Aug 2017 10:15:06 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Qin", "Yao", ""], ["Song", "Dongjin", ""], ["Chen", "Haifeng", ""], ["Cheng", "Wei", ""], ["Jiang", "Guofei", ""], ["Cottrell", "Garrison", ""]]}, {"id": "1704.03012", "submitter": "Carlos Florensa Campo", "authors": "Carlos Florensa, Yan Duan, Pieter Abbeel", "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": "International Conference on Learning Representations 2017", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has achieved many impressive results in recent\nyears. However, tasks with sparse rewards or long horizons continue to pose\nsignificant challenges. To tackle these important problems, we propose a\ngeneral framework that first learns useful skills in a pre-training\nenvironment, and then leverages the acquired skills for learning faster in\ndownstream tasks. Our approach brings together some of the strengths of\nintrinsic motivation and hierarchical methods: the learning of useful skill is\nguided by a single proxy reward, the design of which requires very minimal\ndomain knowledge about the downstream tasks. Then a high-level policy is\ntrained on top of these skills, providing a significant improvement of the\nexploration and allowing to tackle sparse rewards in the downstream tasks. To\nefficiently pre-train a large span of skills, we use Stochastic Neural Networks\ncombined with an information-theoretic regularizer. Our experiments show that\nthis combination is effective in learning a wide span of interpretable skills\nin a sample-efficient way, and can significantly boost the learning performance\nuniformly across a wide range of downstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 18:41:28 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Florensa", "Carlos", ""], ["Duan", "Yan", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1704.03033", "submitter": "Maria Bauza Villalonga", "authors": "Maria Bauza, Alberto Rodriguez", "title": "A probabilistic data-driven model for planar pushing", "comments": "8 pages, 11 figures, ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a data-driven approach to model planar pushing\ninteraction to predict both the most likely outcome of a push and its expected\nvariability. The learned models rely on a variation of Gaussian processes with\ninput-dependent noise called Variational Heteroscedastic Gaussian processes\n(VHGP) that capture the mean and variance of a stochastic function. We show\nthat we can learn accurate models that outperform analytical models after less\nthan 100 samples and saturate in performance with less than 1000 samples. We\nvalidate the results against a collected dataset of repeated trajectories, and\nuse the learned models to study questions such as the nature of the variability\nin pushing, and the validity of the quasi-static assumption.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 19:41:41 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 22:21:13 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Bauza", "Maria", ""], ["Rodriguez", "Alberto", ""]]}, {"id": "1704.03037", "submitter": "Chun-Ta Lu", "authors": "Chun-Ta Lu, Lifang He, Hao Ding, Bokai Cao, Philip S. Yu", "title": "Learning from Multi-View Multi-Way Data via Structural Factorization\n  Machines", "comments": "10 pages", "journal-ref": null, "doi": "10.1145/3178876.3186071", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world relations among entities can often be observed and determined by\ndifferent perspectives/views. For example, the decision made by a user on\nwhether to adopt an item relies on multiple aspects such as the contextual\ninformation of the decision, the item's attributes, the user's profile and the\nreviews given by other users. Different views may exhibit multi-way\ninteractions among entities and provide complementary information. In this\npaper, we introduce a multi-tensor-based approach that can preserve the\nunderlying structure of multi-view data in a generic predictive model.\nSpecifically, we propose structural factorization machines (SFMs) that learn\nthe common latent spaces shared by multi-view tensors and automatically adjust\nthe importance of each view in the predictive model. Furthermore, the\ncomplexity of SFMs is linear in the number of parameters, which make SFMs\nsuitable to large-scale problems. Extensive experiments on real-world datasets\ndemonstrate that the proposed SFMs outperform several state-of-the-art methods\nin terms of prediction accuracy and computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 19:52:29 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 00:29:16 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Lu", "Chun-Ta", ""], ["He", "Lifang", ""], ["Ding", "Hao", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""]]}, {"id": "1704.03039", "submitter": "Pedro Morgado", "authors": "Pedro Morgado, Nuno Vasconcelos", "title": "Semantically Consistent Regularization for Zero-Shot Recognition", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of semantics in zero-shot learning is considered. The effectiveness\nof previous approaches is analyzed according to the form of supervision\nprovided. While some learn semantics independently, others only supervise the\nsemantic subspace explained by training classes. Thus, the former is able to\nconstrain the whole space but lacks the ability to model semantic correlations.\nThe latter addresses this issue but leaves part of the semantic space\nunsupervised. This complementarity is exploited in a new convolutional neural\nnetwork (CNN) framework, which proposes the use of semantics as constraints for\nrecognition.Although a CNN trained for classification has no transfer ability,\nthis can be encouraged by learning an hidden semantic layer together with a\nsemantic code for classification. Two forms of semantic constraints are then\nintroduced. The first is a loss-based regularizer that introduces a\ngeneralization constraint on each semantic predictor. The second is a codeword\nregularizer that favors semantic-to-class mappings consistent with prior\nsemantic knowledge while allowing these to be learned from data. Significant\nimprovements over the state-of-the-art are achieved on several datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 19:59:33 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Morgado", "Pedro", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1704.03058", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, Sinisa Todorovic, Song-Chun Zhu", "title": "CERN: Confidence-Energy Recurrent Network for Group Activity Recognition", "comments": "Accepted to IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is about recognizing human activities occurring in videos at\ndistinct semantic levels, including individual actions, interactions, and group\nactivities. The recognition is realized using a two-level hierarchy of Long\nShort-Term Memory (LSTM) networks, forming a feed-forward deep architecture,\nwhich can be trained end-to-end. In comparison with existing architectures of\nLSTMs, we make two key contributions giving the name to our approach as\nConfidence-Energy Recurrent Network -- CERN. First, instead of using the common\nsoftmax layer for prediction, we specify a novel energy layer (EL) for\nestimating the energy of our predictions. Second, rather than finding the\ncommon minimum-energy class assignment, which may be numerically unstable under\nuncertainty, we specify that the EL additionally computes the p-values of the\nsolutions, and in this way estimates the most confident energy minimum. The\nevaluation on the Collective Activity and Volleyball datasets demonstrates: (i)\nadvantages of our two contributions relative to the common softmax and\nenergy-minimization formulations and (ii) a superior performance relative to\nthe state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 21:08:39 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Shu", "Tianmin", ""], ["Todorovic", "Sinisa", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1704.03073", "submitter": "Timothy Lillicrap", "authors": "Ivaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel\n  Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, Martin\n  Riedmiller", "title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "comments": "12 pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning and reinforcement learning methods have recently been used to\nsolve a variety of problems in continuous control domains. An obvious\napplication of these techniques is dexterous manipulation tasks in robotics\nwhich are difficult to solve using traditional control theory or\nhand-engineered approaches. One example of such a task is to grasp an object\nand precisely stack it on another. Solving this difficult and practically\nrelevant problem in the real world is an important long-term goal for the field\nof robotics. Here we take a step towards this goal by examining the problem in\nsimulation and providing models and techniques aimed at solving it. We\nintroduce two extensions to the Deep Deterministic Policy Gradient algorithm\n(DDPG), a model-free Q-learning based method, which make it significantly more\ndata-efficient and scalable. Our results show that by making extensive use of\noff-policy data and replay, it is possible to find control policies that\nrobustly grasp objects and stack them. Further, our results hint that it may\nsoon be feasible to train successful stacking policies by collecting\ninteractions on real robots.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 22:29:50 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Popov", "Ivaylo", ""], ["Heess", "Nicolas", ""], ["Lillicrap", "Timothy", ""], ["Hafner", "Roland", ""], ["Barth-Maron", "Gabriel", ""], ["Vecerik", "Matej", ""], ["Lampe", "Thomas", ""], ["Tassa", "Yuval", ""], ["Erez", "Tom", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1704.03079", "submitter": "Asit Mishra", "authors": "Asit Mishra, Jeffrey J Cook, Eriko Nurvitadhi and Debbie Marr", "title": "WRPN: Training and Inference using Wide Reduced-Precision Networks", "comments": "Under submission to CVPR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For computer vision applications, prior works have shown the efficacy of\nreducing the numeric precision of model parameters (network weights) in deep\nneural networks but also that reducing the precision of activations hurts model\naccuracy much more than reducing the precision of model parameters. We study\nschemes to train networks from scratch using reduced-precision activations\nwithout hurting the model accuracy. We reduce the precision of activation maps\n(along with model parameters) using a novel quantization scheme and increase\nthe number of filter maps in a layer, and find that this scheme compensates or\nsurpasses the accuracy of the baseline full-precision network. As a result, one\ncan significantly reduce the dynamic memory footprint, memory bandwidth,\ncomputational energy and speed up the training and inference process with\nappropriate hardware support. We call our scheme WRPN - wide reduced-precision\nnetworks. We report results using our proposed schemes and show that our\nresults are better than previously reported accuracies on ILSVRC-12 dataset\nwhile being computationally less expensive compared to previously reported\nreduced-precision networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 22:54:38 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Mishra", "Asit", ""], ["Cook", "Jeffrey J", ""], ["Nurvitadhi", "Eriko", ""], ["Marr", "Debbie", ""]]}, {"id": "1704.03084", "submitter": "Xiujun Li", "authors": "Baolin Peng and Xiujun Li and Lihong Li and Jianfeng Gao and Asli\n  Celikyilmaz and Sungjin Lee and Kam-Fai Wong", "title": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep\n  Reinforcement Learning", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a dialogue agent to fulfill complex tasks, such as travel planning,\nis challenging because the agent has to learn to collectively complete multiple\nsubtasks. For example, the agent needs to reserve a hotel and book a flight so\nthat there leaves enough time for commute between arrival and hotel check-in.\nThis paper addresses this challenge by formulating the task in the mathematical\nframework of options over Markov Decision Processes (MDPs), and proposing a\nhierarchical deep reinforcement learning approach to learning a dialogue\nmanager that operates at different temporal scales. The dialogue manager\nconsists of: (1) a top-level dialogue policy that selects among subtasks or\noptions, (2) a low-level dialogue policy that selects primitive actions to\ncomplete the subtask given by the top-level policy, and (3) a global state\ntracker that helps ensure all cross-subtask constraints be satisfied.\nExperiments on a travel planning task with simulated and real users show that\nour approach leads to significant improvements over three baselines, two based\non handcrafted rules and the other based on flat deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 23:24:46 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 19:36:30 GMT"}, {"version": "v3", "created": "Sat, 22 Jul 2017 22:23:53 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Peng", "Baolin", ""], ["Li", "Xiujun", ""], ["Li", "Lihong", ""], ["Gao", "Jianfeng", ""], ["Celikyilmaz", "Asli", ""], ["Lee", "Sungjin", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "1704.03141", "submitter": "Yejin Kim", "authors": "Yejin Kim, Jimeng Sun, Hwanjo Yu, Xiaoqian Jiang", "title": "Federated Tensor Factorization for Computational Phenotyping", "comments": null, "journal-ref": null, "doi": "10.1145/3097983.3098118", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorization models offer an effective approach to convert massive\nelectronic health records into meaningful clinical concepts (phenotypes) for\ndata analysis. These models need a large amount of diverse samples to avoid\npopulation bias. An open challenge is how to derive phenotypes jointly across\nmultiple hospitals, in which direct patient-level data sharing is not possible\n(e.g., due to institutional policies). In this paper, we developed a novel\nsolution to enable federated tensor factorization for computational phenotyping\nwithout sharing patient-level data. We developed secure data harmonization and\nfederated computation procedures based on alternating direction method of\nmultipliers (ADMM). Using this method, the multiple hospitals iteratively\nupdate tensors and transfer secure summarized information to a central server,\nand the server aggregates the information to generate phenotypes. We\ndemonstrated with real medical datasets that our method resembles the\ncentralized training model (based on combined datasets) in terms of accuracy\nand phenotypes discovery while respecting privacy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 04:28:03 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Kim", "Yejin", ""], ["Sun", "Jimeng", ""], ["Yu", "Hwanjo", ""], ["Jiang", "Xiaoqian", ""]]}, {"id": "1704.03144", "submitter": "Maziar Raissi", "authors": "Maziar Raissi", "title": "Parametric Gaussian Process Regression for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the concept of parametric Gaussian processes (PGPs),\nwhich is built upon the seemingly self-contradictory idea of making Gaussian\nprocesses parametric. Parametric Gaussian processes, by construction, are\ndesigned to operate in \"big data\" regimes where one is interested in\nquantifying the uncertainty associated with noisy data. The proposed\nmethodology circumvents the well-established need for stochastic variational\ninference, a scalable algorithm for approximating posterior distributions. The\neffectiveness of the proposed approach is demonstrated using an illustrative\nexample with simulated data and a benchmark dataset in the airline industry\nwith approximately 6 million records.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 04:57:24 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 20:12:45 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Raissi", "Maziar", ""]]}, {"id": "1704.03165", "submitter": "Leonardo F. R. Ribeiro", "authors": "Leonardo F. R. Ribeiro, Pedro H. P. Savarese, Daniel R. Figueiredo", "title": "struc2vec: Learning Node Representations from Structural Identity", "comments": "10 pages, KDD2017, Research Track", "journal-ref": null, "doi": "10.1145/3097983.3098061", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural identity is a concept of symmetry in which network nodes are\nidentified according to the network structure and their relationship to other\nnodes. Structural identity has been studied in theory and practice over the\npast decades, but only recently has it been addressed with representational\nlearning techniques. This work presents struc2vec, a novel and flexible\nframework for learning latent representations for the structural identity of\nnodes. struc2vec uses a hierarchy to measure node similarity at different\nscales, and constructs a multilayer graph to encode structural similarities and\ngenerate structural context for nodes. Numerical experiments indicate that\nstate-of-the-art techniques for learning node representations fail in capturing\nstronger notions of structural identity, while struc2vec exhibits much superior\nperformance in this task, as it overcomes limitations of prior approaches. As a\nconsequence, numerical experiments indicate that struc2vec improves performance\non classification tasks that depend more on structural identity.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 06:32:36 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 01:35:12 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 20:47:08 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Ribeiro", "Leonardo F. R.", ""], ["Savarese", "Pedro H. P.", ""], ["Figueiredo", "Daniel R.", ""]]}, {"id": "1704.03188", "submitter": "Kimin Lee", "authors": "Kimin Lee, Jaehyung Kim, Song Chong, Jinwoo Shin", "title": "Simplified Stochastic Feedforward Neural Networks", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been believed that stochastic feedforward neural networks (SFNNs) have\nseveral advantages beyond deterministic deep neural networks (DNNs): they have\nmore expressive power allowing multi-modal mappings and regularize better due\nto their stochastic nature. However, training large-scale SFNN is notoriously\nharder. In this paper, we aim at developing efficient training methods for\nSFNN, in particular using known architectures and pre-trained parameters of\nDNN. To this end, we propose a new intermediate stochastic model, called\nSimplified-SFNN, which can be built upon any baseline DNNand approximates\ncertain SFNN by simplifying its upper latent units above stochastic ones. The\nmain novelty of our approach is in establishing the connection between three\nmodels, i.e., DNN->Simplified-SFNN->SFNN, which naturally leads to an efficient\ntraining procedure of the stochastic models utilizing pre-trained parameters of\nDNN. Using several popular DNNs, we show how they can be effectively\ntransferred to the corresponding stochastic models for both multi-modal and\nclassification tasks on MNIST, TFD, CASIA, CIFAR-10, CIFAR-100 and SVHN\ndatasets. In particular, we train a stochastic model of 28 layers and 36\nmillion parameters, where training such a large-scale stochastic network is\nsignificantly challenging without using Simplified-SFNN\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 08:19:00 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Lee", "Kimin", ""], ["Kim", "Jaehyung", ""], ["Chong", "Song", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1704.03205", "submitter": "Luigi Troiano", "authors": "Luigi Troiano and Elena Mejuto and Pravesh Kriplani", "title": "On Feature Reduction using Deep Learning for Trend Prediction in Finance", "comments": "6 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major advantages in using Deep Learning for Finance is to embed a\nlarge collection of information into investment decisions. A way to do that is\nby means of compression, that lead us to consider a smaller feature space.\nSeveral studies are proving that non-linear feature reduction performed by Deep\nLearning tools is effective in price trend prediction. The focus has been put\nmainly on Restricted Boltzmann Machines (RBM) and on output obtained by them.\nFew attention has been payed to Auto-Encoders (AE) as an alternative means to\nperform a feature reduction. In this paper we investigate the application of\nboth RBM and AE in more general terms, attempting to outline how architectural\nand input space characteristics can affect the quality of prediction.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 09:08:50 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Troiano", "Luigi", ""], ["Mejuto", "Elena", ""], ["Kriplani", "Pravesh", ""]]}, {"id": "1704.03223", "submitter": "Zahra Mousavi", "authors": "Zahra Mousavi, Heshaam Faili", "title": "Persian Wordnet Construction using Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated supervised method for Persian wordnet\nconstruction. Using a Persian corpus and a bi-lingual dictionary, the initial\nlinks between Persian words and Princeton WordNet synsets have been generated.\nThese links will be discriminated later as correct or incorrect by employing\nseven features in a trained classification system. The whole method is just a\nclassification system, which has been trained on a train set containing FarsNet\nas a set of correct instances. State of the art results on the automatically\nderived Persian wordnet is achieved. The resulted wordnet with a precision of\n91.18% includes more than 16,000 words and 22,000 synsets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 09:47:28 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Mousavi", "Zahra", ""], ["Faili", "Heshaam", ""]]}, {"id": "1704.03296", "submitter": "Ruth Fong", "authors": "Ruth Fong and Andrea Vedaldi", "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation", "comments": "Final camera-ready paper published at ICCV 2017 (Supplementary\n  materials:\n  http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Fong_Interpretable_Explanations_of_ICCV_2017_supplemental.pdf)", "journal-ref": "Proceedings of the 2017 IEEE International Conference on Computer\n  Vision (ICCV)", "doi": "10.1109/ICCV.2017.371", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning algorithms are increasingly applied to high impact yet\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\ncritical that researchers can explain how such algorithms arrived at their\npredictions. In recent years, a number of image saliency methods have been\ndeveloped to summarize where highly complex neural networks \"look\" in an image\nfor evidence for their predictions. However, these techniques are limited by\ntheir heuristic nature and architectural constraints. In this paper, we make\ntwo main contributions: First, we propose a general framework for learning\ndifferent kinds of explanations for any black box algorithm. Second, we\nspecialise the framework to find the part of an image most responsible for a\nclassifier decision. Unlike previous works, our method is model-agnostic and\ntestable because it is grounded in explicit and interpretable image\nperturbations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 14:15:20 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 13:53:21 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 16:03:33 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Fong", "Ruth", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1704.03298", "submitter": "Ralf Mikut", "authors": "Ralf Mikut, Andreas Bartschat, Wolfgang Doneit, Jorge \\'Angel\n  Gonz\\'alez Ordiano, Benjamin Schott, Johannes Stegmaier, Simon Waczowicz,\n  Markus Reischl", "title": "The MATLAB Toolbox SciXMiner: User's Manual and Programmer's Guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Matlab toolbox SciXMiner is designed for the visualization and analysis\nof time series and features with a special focus to classification problems. It\nwas developed at the Institute of Applied Computer Science of the Karlsruhe\nInstitute of Technology (KIT), a member of the Helmholtz Association of German\nResearch Centres in Germany. The aim was to provide an open platform for the\ndevelopment and improvement of data mining methods and its applications to\nvarious medical and technical problems. SciXMiner bases on Matlab (tested for\nthe version 2017a). Many functions do not require additional standard toolboxes\nbut some parts of Signal, Statistics and Wavelet toolboxes are used for special\ncases. The decision to a Matlab-based solution was made to use the wide\nmathematical functionality of this package provided by The Mathworks Inc.\nSciXMiner is controlled by a graphical user interface (GUI) with menu items and\ncontrol elements like popup lists, checkboxes and edit elements. This makes it\neasier to work with SciXMiner for inexperienced users. Furthermore, an\nautomatization and batch standardization of analyzes is possible using macros.\nThe standard Matlab style using the command line is also available. SciXMiner\nis an open source software. The download page is\nhttp://sourceforge.net/projects/SciXMiner. It is licensed under the conditions\nof the GNU General Public License (GNU-GPL) of The Free Software Foundation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 14:17:47 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Mikut", "Ralf", ""], ["Bartschat", "Andreas", ""], ["Doneit", "Wolfgang", ""], ["Ordiano", "Jorge \u00c1ngel Gonz\u00e1lez", ""], ["Schott", "Benjamin", ""], ["Stegmaier", "Johannes", ""], ["Waczowicz", "Simon", ""], ["Reischl", "Markus", ""]]}, {"id": "1704.03371", "submitter": "Cameron Musco", "authors": "Cameron Musco and David P. Woodruff", "title": "Sublinear Time Low-Rank Approximation of Positive Semidefinite Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to compute a relative-error low-rank approximation to any\npositive semidefinite (PSD) matrix in sublinear time, i.e., for any $n \\times\nn$ PSD matrix $A$, in $\\tilde O(n \\cdot poly(k/\\epsilon))$ time we output a\nrank-$k$ matrix $B$, in factored form, for which $\\|A-B\\|_F^2 \\leq\n(1+\\epsilon)\\|A-A_k\\|_F^2$, where $A_k$ is the best rank-$k$ approximation to\n$A$. When $k$ and $1/\\epsilon$ are not too large compared to the sparsity of\n$A$, our algorithm does not need to read all entries of the matrix. Hence, we\nsignificantly improve upon previous $nnz(A)$ time algorithms based on oblivious\nsubspace embeddings, and bypass an $nnz(A)$ time lower bound for general\nmatrices (where $nnz(A)$ denotes the number of non-zero entries in the matrix).\nWe prove time lower bounds for low-rank approximation of PSD matrices, showing\nthat our algorithm is close to optimal. Finally, we extend our techniques to\ngive sublinear time algorithms for low-rank approximation of $A$ in the (often\nstronger) spectral norm metric $\\|A-B\\|_2^2$ and for ridge regression on PSD\nmatrices.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 15:44:49 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 02:59:15 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2019 15:24:34 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Musco", "Cameron", ""], ["Woodruff", "David P.", ""]]}, {"id": "1704.03404", "submitter": "Santosh K C", "authors": "K C Santosh, Suman Kalyan Maity and Arjun Mukherjee", "title": "ENWalk: Learning Network Features for Spam Detection in Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social medias are increasing their influence with the vast public information\nleading to their active use for marketing by the companies and organizations.\nSuch marketing promotions are difficult to identify unlike the traditional\nmedias like TV and newspaper. So, it is very much important to identify the\npromoters in the social media. Although, there are active ongoing researches,\nexisting approaches are far from solving the problem. To identify such\nimposters, it is very much important to understand their strategies of social\ncircle creation and dynamics of content posting. Are there any specific spammer\ntypes? How successful are each types? We analyze these questions in the light\nof social relationships in Twitter. Our analyses discover two types of spammers\nand their relationships with the dynamics of content posts. Our results\ndiscover novel dynamics of spamming which are intuitive and arguable. We\npropose ENWalk, a framework to detect the spammers by learning the feature\nrepresentations of the users in the social media. We learn the feature\nrepresentations using the random walks biased on the spam dynamics.\nExperimental results on large-scale twitter network and the corresponding\ntweets show the effectiveness of our approach that outperforms the existing\napproaches\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 16:37:37 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Santosh", "K C", ""], ["Maity", "Suman Kalyan", ""], ["Mukherjee", "Arjun", ""]]}, {"id": "1704.03421", "submitter": "Malika Bendechache", "authors": "Malika Bendechache, Nhien-An Le-Khac, M-Tahar Kechadi", "title": "Efficient Large Scale Clustering based on Data Partitioning", "comments": "10 pages", "journal-ref": "Data Science and Advanced Analytics (DSAA), 2016 IEEE\n  International Conference on, 612--621, 2016", "doi": "10.1109/DSAA.2016.70", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering techniques are very attractive for extracting and identifying\npatterns in datasets. However, their application to very large spatial datasets\npresents numerous challenges such as high-dimensionality data, heterogeneity,\nand high complexity of some algorithms. For instance, some algorithms may have\nlinear complexity but they require the domain knowledge in order to determine\ntheir input parameters. Distributed clustering techniques constitute a very\ngood alternative to the big data challenges (e.g.,Volume, Variety, Veracity,\nand Velocity). Usually these techniques consist of two phases. The first phase\ngenerates local models or patterns and the second one tends to aggregate the\nlocal results to obtain global models. While the first phase can be executed in\nparallel on each site and, therefore, efficient, the aggregation phase is\ncomplex, time consuming and may produce incorrect and ambiguous global clusters\nand therefore incorrect models. In this paper we propose a new distributed\nclustering approach to deal efficiently with both phases, generation of local\nresults and generation of global models by aggregation. For the first phase,\nour approach is capable of analysing the datasets located in each site using\ndifferent clustering techniques. The aggregation phase is designed in such a\nway that the final clusters are compact and accurate while the overall process\nis efficient in time and memory allocation. For the evaluation, we use two\nwell-known clustering algorithms, K-Means and DBSCAN. One of the key outputs of\nthis distributed clustering technique is that the number of global clusters is\ndynamic, no need to be fixed in advance. Experimental results show that the\napproach is scalable and produces high quality results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 17:05:01 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 15:23:31 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Bendechache", "Malika", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1704.03453", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick\n  McDaniel", "title": "The Space of Transferable Adversarial Examples", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are maliciously perturbed inputs designed to mislead\nmachine learning (ML) models at test-time. They often transfer: the same\nadversarial example fools more than one model.\n  In this work, we propose novel methods for estimating the previously unknown\ndimensionality of the space of adversarial inputs. We find that adversarial\nexamples span a contiguous subspace of large (~25) dimensionality. Adversarial\nsubspaces with higher dimensionality are more likely to intersect. We find that\nfor two different models, a significant fraction of their subspaces is shared,\nthus enabling transferability.\n  In the first quantitative analysis of the similarity of different models'\ndecision boundaries, we show that these boundaries are actually close in\narbitrary directions, whether adversarial or benign. We conclude by formally\nstudying the limits of transferability. We derive (1) sufficient conditions on\nthe data distribution that imply transferability for simple model classes and\n(2) examples of scenarios in which transfer does not occur. These findings\nindicate that it may be possible to design defenses against transfer-based\nattacks, even for models that are vulnerable to direct attacks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 17:59:12 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 18:14:30 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Papernot", "Nicolas", ""], ["Goodfellow", "Ian", ""], ["Boneh", "Dan", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1704.03458", "submitter": "Jinsung Yoon", "authors": "J. Yoon, W. R. Zame, A. Banerjee, M. Cadeiras, A. M. Alaa, M. van der\n  Schaar", "title": "Personalized Survival Predictions for Cardiac Transplantation via Trees\n  of Predictors", "comments": "Main manuscript: 20 pages, Supplementary materials: 13 pages, 5\n  figures, 3 tables. Submitted to Science Translational Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the limited pool of donor organs, accurate predictions of survival on\nthe wait list and post transplantation are crucial for cardiac transplantation\ndecisions and policy. However, current clinical risk scores do not yield\naccurate predictions. We develop a new methodology (ToPs, Trees of Predictors)\nbuilt on the principle that specific predictors should be used for specific\nclusters within the target population. ToPs discovers these specific clusters\nof patients and the specific predictor that perform best for each cluster. In\ncomparison with current clinical risk scoring systems, our method provides\nsignificant improvements in the prediction of survival time on the wait list\nand post transplantation. For example, in terms of 3 month survival for\npatients who were on the US patient wait list in the period 1985 to 2015, our\nmethod achieves AUC of 0.847, the best commonly used clinical risk score\n(MAGGIC) achieves 0.630. In terms of 3 month survival/mortality predictions (in\ncomparison to MAGGIC), holding specificity at 80.0 percents, our algorithm\ncorrectly predicts survival for 1,228 (26.0 percents more patients out of 4,723\nwho actually survived, holding sensitivity at 80.0 percents, our algorithm\ncorrectly predicts mortality for 839 (33.0 percents) more patients out of 2,542\nwho did not survive. Our method achieves similar improvements for other time\nhorizons and for predictions post transplantation. Therefore, we offer a more\naccurate, personalized approach to survival analysis that can benefit patients,\nclinicians and policymakers in making clinical decisions and setting clinical\npolicy. Because risk prediction is widely used in diagnostic and prognostic\nclinical decision making across diseases and clinical specialties, the\nimplications of our methods are far reaching.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 16:06:08 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Yoon", "J.", ""], ["Zame", "W. R.", ""], ["Banerjee", "A.", ""], ["Cadeiras", "M.", ""], ["Alaa", "A. M.", ""], ["van der Schaar", "M.", ""]]}, {"id": "1704.03477", "submitter": "David Ha", "authors": "David Ha and Douglas Eck", "title": "A Neural Representation of Sketch Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sketch-rnn, a recurrent neural network (RNN) able to construct\nstroke-based drawings of common objects. The model is trained on thousands of\ncrude human-drawn images representing hundreds of classes. We outline a\nframework for conditional and unconditional sketch generation, and describe new\nrobust training methods for generating coherent sketch drawings in a vector\nformat.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:09:01 GMT"}, {"version": "v2", "created": "Sun, 16 Apr 2017 01:26:05 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 16:28:23 GMT"}, {"version": "v4", "created": "Fri, 19 May 2017 16:40:16 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Ha", "David", ""], ["Eck", "Douglas", ""]]}, {"id": "1704.03543", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Leveraging Term Banks for Answering Complex Questions: A Case for Sparse\n  Vectors", "comments": "Related datasets can be found at http://allenai.org/data.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While open-domain question answering (QA) systems have proven effective for\nanswering simple questions, they struggle with more complex questions. Our goal\nis to answer more complex questions reliably, without incurring a significant\ncost in knowledge resource construction to support the QA. One readily\navailable knowledge resource is a term bank, enumerating the key concepts in a\ndomain. We have developed an unsupervised learning approach that leverages a\nterm bank to guide a QA system, by representing the terminological knowledge\nwith thousands of specialized vector spaces. In experiments with complex\nscience questions, we show that this approach significantly outperforms several\nstate-of-the-art QA systems, demonstrating that significant leverage can be\ngained from continuous vector representations of domain terminology.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 21:21:39 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1704.03564", "submitter": "Shay Moran", "authors": "Daniel M. Kane and Shachar Lovett and Shay Moran and Jiapeng Zhang", "title": "Active classification with comparison queries", "comments": "23 pages (not including references), 1 figure. The new version\n  contains a minor fix in the proof of Lemma 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an extension of active learning in which the learning algorithm may\nask the annotator to compare the distances of two examples from the boundary of\ntheir label-class. For example, in a recommendation system application (say for\nrestaurants), the annotator may be asked whether she liked or disliked a\nspecific restaurant (a label query); or which one of two restaurants did she\nlike more (a comparison query).\n  We focus on the class of half spaces, and show that under natural\nassumptions, such as large margin or bounded bit-description of the input\nexamples, it is possible to reveal all the labels of a sample of size $n$ using\napproximately $O(\\log n)$ queries. This implies an exponential improvement over\nclassical active learning, where only label queries are allowed. We complement\nthese results by showing that if any of these assumptions is removed then, in\nthe worst case, $\\Omega(n)$ queries are required.\n  Our results follow from a new general framework of active learning with\nadditional queries. We identify a combinatorial dimension, called the\n\\emph{inference dimension}, that captures the query complexity when each\nadditional query is determined by $O(1)$ examples (such as comparison queries,\neach of which is determined by the two compared examples). Our results for half\nspaces follow by bounding the inference dimension in the cases discussed above.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 22:55:29 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 00:49:37 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Kane", "Daniel M.", ""], ["Lovett", "Shachar", ""], ["Moran", "Shay", ""], ["Zhang", "Jiapeng", ""]]}, {"id": "1704.03617", "submitter": "Matthew Riemer", "authors": "Matthew Riemer, Elham Khabiri, and Richard Goodwin", "title": "Representation Stability as a Regularizer for Improved Text Analytics\n  Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural networks are well suited for sequential transfer learning\ntasks, the catastrophic forgetting problem hinders proper integration of prior\nknowledge. In this work, we propose a solution to this problem by using a\nmulti-task objective based on the idea of distillation and a mechanism that\ndirectly penalizes forgetting at the shared representation layer during the\nknowledge integration phase of training. We demonstrate our approach on a\nTwitter domain sentiment analysis task with sequential knowledge transfer from\nfour related tasks. We show that our technique outperforms networks fine-tuned\nto the target task. Additionally, we show both through empirical evidence and\nexamples that it does not forget useful knowledge from the source task that is\nforgotten during standard fine-tuning. Surprisingly, we find that first\ndistilling a human made rule based sentiment engine into a recurrent neural\nnetwork and then integrating the knowledge with the target task data leads to a\nsubstantial gain in generalization performance. Our experiments demonstrate the\npower of multi-source transfer techniques in practical text analytics problems\nwhen paired with distillation. In particular, for the SemEval 2016 Task 4\nSubtask A (Nakov et al., 2016) dataset we surpass the state of the art\nestablished during the competition with a comparatively simple model\narchitecture that is not even competitive when trained on only the labeled task\nspecific data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 04:38:18 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Riemer", "Matthew", ""], ["Khabiri", "Elham", ""], ["Goodwin", "Richard", ""]]}, {"id": "1704.03626", "submitter": "Shinnosuke Takamichi", "authors": "Shinnosuke Takamichi, Tomoki Koriyama, Hiroshi Saruwatari", "title": "Sampling-based speech parameter generation using moment-matching\n  networks", "comments": "Submitted to INTERSPEECH 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents sampling-based speech parameter generation using\nmoment-matching networks for Deep Neural Network (DNN)-based speech synthesis.\nAlthough people never produce exactly the same speech even if we try to express\nthe same linguistic and para-linguistic information, typical statistical speech\nsynthesis produces completely the same speech, i.e., there is no\ninter-utterance variation in synthetic speech. To give synthetic speech natural\ninter-utterance variation, this paper builds DNN acoustic models that make it\npossible to randomly sample speech parameters. The DNNs are trained so that\nthey make the moments of generated speech parameters close to those of natural\nspeech parameters. Since the variation of speech parameters is compressed into\na low-dimensional simple prior noise vector, our algorithm has lower\ncomputation cost than direct sampling of speech parameters. As the first step\ntowards generating synthetic speech that has natural inter-utterance variation,\nthis paper investigates whether or not the proposed sampling-based generation\ndeteriorates synthetic speech quality. In evaluation, we compare speech quality\nof conventional maximum likelihood-based generation and proposed sampling-based\ngeneration. The result demonstrates the proposed generation causes no\ndegradation in speech quality.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 05:46:44 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Takamichi", "Shinnosuke", ""], ["Koriyama", "Tomoki", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "1704.03636", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Philipp Grohs and Helmut B\\\"olcskei", "title": "Energy Propagation in Deep Convolutional Neural Networks", "comments": "Corrected errors in arguments on the spectral decay of Sobolev\n  functions and on the volume of tubes, IEEE Transactions on Information\n  Theory, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical machine learning tasks employ very deep convolutional neural\nnetworks. Such large depths pose formidable computational challenges in\ntraining and operating the network. It is therefore important to understand how\nfast the energy contained in the propagated signals (a.k.a. feature maps)\ndecays across layers. In addition, it is desirable that the feature extractor\ngenerated by the network be informative in the sense of the only signal mapping\nto the all-zeros feature vector being the zero input signal. This \"trivial\nnull-set\" property can be accomplished by asking for \"energy conservation\" in\nthe sense of the energy in the feature vector being proportional to that of the\ncorresponding input signal. This paper establishes conditions for energy\nconservation (and thus for a trivial null-set) for a wide class of deep\nconvolutional neural network-based feature extractors and characterizes\ncorresponding feature map energy decay rates. Specifically, we consider general\nscattering networks employing the modulus non-linearity and we find that under\nmild analyticity and high-pass conditions on the filters (which encompass,\ninter alia, various constructions of Weyl-Heisenberg filters, wavelets,\nridgelets, ($\\alpha$)-curvelets, and shearlets) the feature map energy decays\nat least polynomially fast. For broad families of wavelets and Weyl-Heisenberg\nfilters, the guaranteed decay rate is shown to be exponential. Moreover, we\nprovide handy estimates of the number of layers needed to have at least\n$((1-\\varepsilon)\\cdot 100)\\%$ of the input signal energy be contained in the\nfeature vector.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 06:27:20 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 07:04:58 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 08:31:13 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Wiatowski", "Thomas", ""], ["Grohs", "Philipp", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1704.03711", "submitter": "Dorian Cazau", "authors": "D. Cazau, G. Nuel", "title": "Investigation on the use of Hidden-Markov Models in automatic\n  transcription of music", "comments": "arXiv admin note: text overlap with arXiv:1703.09772", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov Models (HMMs) are a ubiquitous tool to model time series data,\nand have been widely used in two main tasks of Automatic Music Transcription\n(AMT): note segmentation, i.e. identifying the played notes after a multi-pitch\nestimation, and sequential post-processing, i.e. correcting note segmentation\nusing training data. In this paper, we employ the multi-pitch estimation method\ncalled Probabilistic Latent Component Analysis (PLCA), and develop AMT systems\nby integrating different HMM-based modules in this framework. For note\nsegmentation, we use two different twostate on/o? HMMs, including a\nhigher-order one for duration modeling. For sequential post-processing, we\nfocused on a musicological modeling of polyphonic harmonic transitions, using a\nfirst- and second-order HMMs whose states are defined through candidate note\nmixtures. These different PLCA plus HMM systems have been evaluated\ncomparatively on two different instrument repertoires, namely the piano (using\nthe MAPS database) and the marovany zither. Our results show that the use of\nHMMs could bring noticeable improvements to transcription results, depending on\nthe instrument repertoire.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 11:20:44 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Cazau", "D.", ""], ["Nuel", "G.", ""]]}, {"id": "1704.03718", "submitter": "Wenjie Zhang", "authors": "Wenjie Zhang, Junchi Yan, Xiangfeng Wang and Hongyuan Zha", "title": "Deep Extreme Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label learning (XML) or classification has been a practical and\nimportant problem since the boom of big data. The main challenge lies in the\nexponential label space which involves $2^L$ possible label sets especially\nwhen the label dimension $L$ is huge, e.g., in millions for Wikipedia labels.\nThis paper is motivated to better explore the label space by originally\nestablishing an explicit label graph. In the meanwhile, deep learning has been\nwidely studied and used in various classification problems including\nmulti-label classification, however it has not been properly introduced to XML,\nwhere the label space can be as large as in millions. In this paper, we propose\na practical deep embedding method for extreme multi-label classification, which\nharvests the ideas of non-linear embedding and graph priors-based label space\nmodeling simultaneously. Extensive experiments on public datasets for XML show\nthat our method performs competitive against state-of-the-art result.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 12:09:40 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 07:46:31 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 15:59:32 GMT"}, {"version": "v4", "created": "Fri, 8 Jun 2018 11:39:04 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zhang", "Wenjie", ""], ["Yan", "Junchi", ""], ["Wang", "Xiangfeng", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1704.03732", "submitter": "Todd Hester", "authors": "Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom\n  Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Gabriel\n  Dulac-Arnold, Ian Osband, John Agapiou, Joel Z. Leibo, Audrunas Gruslys", "title": "Deep Q-learning from Demonstrations", "comments": "Published at AAAI 2018. Previously on arxiv as \"Learning from\n  Demonstrations for Real World Reinforcement Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has achieved several high profile successes\nin difficult decision-making problems. However, these algorithms typically\nrequire a huge amount of data before they reach reasonable performance. In\nfact, their performance during learning can be extremely poor. This may be\nacceptable for a simulator, but it severely limits the applicability of deep RL\nto many real-world tasks, where the agent must learn in the real environment.\nIn this paper we study a setting where the agent may access data from previous\ncontrol of the system. We present an algorithm, Deep Q-learning from\nDemonstrations (DQfD), that leverages small sets of demonstration data to\nmassively accelerate the learning process even from relatively small amounts of\ndemonstration data and is able to automatically assess the necessary ratio of\ndemonstration data while learning thanks to a prioritized replay mechanism.\nDQfD works by combining temporal difference updates with supervised\nclassification of the demonstrator's actions. We show that DQfD has better\ninitial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN)\nas it starts with better scores on the first million steps on 41 of 42 games\nand on average it takes PDD DQN 83 million steps to catch up to DQfD's\nperformance. DQfD learns to out-perform the best demonstration given in 14 of\n42 games. In addition, DQfD leverages human demonstrations to achieve\nstate-of-the-art results for 11 games. Finally, we show that DQfD performs\nbetter than three related algorithms for incorporating demonstration data into\nDQN.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 12:44:37 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 13:19:52 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 10:48:28 GMT"}, {"version": "v4", "created": "Wed, 22 Nov 2017 21:18:31 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Hester", "Todd", ""], ["Vecerik", "Matej", ""], ["Pietquin", "Olivier", ""], ["Lanctot", "Marc", ""], ["Schaul", "Tom", ""], ["Piot", "Bilal", ""], ["Horgan", "Dan", ""], ["Quan", "John", ""], ["Sendonaris", "Andrew", ""], ["Dulac-Arnold", "Gabriel", ""], ["Osband", "Ian", ""], ["Agapiou", "John", ""], ["Leibo", "Joel Z.", ""], ["Gruslys", "Audrunas", ""]]}, {"id": "1704.03743", "submitter": "Giles Tetteh", "authors": "Giles Tetteh, Markus Rempfler, Bjoern H. Menze, Claus Zimmer", "title": "Deep-FExt: Deep Feature Extraction for Vessel Segmentation and\n  Centerline Prediction", "comments": "9 pages", "journal-ref": "Wang Q., Shi Y., Suk HI., Suzuki K. (eds) Machine Learning in\n  Medical Imaging. MLMI 2017. Lecture Notes in Computer Science, vol 10541.\n  Springer, Cham", "doi": "10.1007/978-3-319-67389-9_40", "report-no": "pp 344-352", "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction is a very crucial task in image and pixel (voxel)\nclassification and regression in biomedical image modeling. In this work we\npresent a machine learning based feature extraction scheme based on inception\nmodels for pixel classification tasks. We extract features under multi-scale\nand multi-layer schemes through convolutional operators. Layers of Fully\nConvolutional Network are later stacked on this feature extraction layers and\ntrained end-to-end for the purpose of classification. We test our model on the\nDRIVE and STARE public data sets for the purpose of segmentation and centerline\ndetection and it out performs most existing hand crafted or deterministic\nfeature schemes found in literature. We achieve an average maximum Dice of 0.85\non the DRIVE data set which out performs the scores from the second human\nannotator of this data set. We also achieve an average maximum Dice of 0.85 and\nkappa of 0.84 on the STARE data set. Though these datasets are mainly 2-D we\nalso propose ways of extending this feature extraction scheme to handle 3-D\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 13:10:20 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Tetteh", "Giles", ""], ["Rempfler", "Markus", ""], ["Menze", "Bjoern H.", ""], ["Zimmer", "Claus", ""]]}, {"id": "1704.03751", "submitter": "Shaoshan Liu", "authors": "Dawei Sun, Shaoshan Liu, Jean-Luc Gaudiot", "title": "Enabling Embedded Inference Engine with ARM Compute Library: A Case\n  Study", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When you need to enable deep learning on low-cost embedded SoCs, is it better\nto port an existing deep learning framework or should you build one from\nscratch? In this paper, we share our practical experiences of building an\nembedded inference engine using ARM Compute Library (ACL). The results show\nthat, contradictory to conventional wisdoms, for simple models, it takes much\nless development time to build an inference engine from scratch compared to\nporting existing frameworks. In addition, by utilizing ACL, we managed to build\nan inference engine that outperforms TensorFlow by 25%. Our conclusion is that,\non embedded devices, we most likely will use very simple deep learning models\nfor inference, and with well-developed building blocks such as ACL, it may be\nbetter in both performance and development time to build the engine from\nscratch.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 13:31:26 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 15:07:41 GMT"}, {"version": "v3", "created": "Fri, 14 Apr 2017 10:16:50 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Sun", "Dawei", ""], ["Liu", "Shaoshan", ""], ["Gaudiot", "Jean-Luc", ""]]}, {"id": "1704.03754", "submitter": "Vasilis Syrgkanis", "authors": "Vasilis Syrgkanis", "title": "A Proof of Orthogonal Double Machine Learning with $Z$-Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two stage estimation with a non-parametric first stage and a\ngeneralized method of moments second stage, in a simpler setting than\n(Chernozhukov et al. 2016). We give an alternative proof of the theorem given\nin (Chernozhukov et al. 2016) that orthogonal second stage moments, sample\nsplitting and $n^{1/4}$-consistency of the first stage, imply\n$\\sqrt{n}$-consistency and asymptotic normality of second stage estimates. Our\nproof is for a variant of their estimator, which is based on the empirical\nversion of the moment condition (Z-estimator), rather than a minimization of a\nnorm of the empirical vector of moments (M-estimator). This note is meant\nprimarily for expository purposes, rather than as a new technical contribution.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 13:34:56 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 18:37:29 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Syrgkanis", "Vasilis", ""]]}, {"id": "1704.03809", "submitter": "Merlijn Blaauw", "authors": "Merlijn Blaauw, Jordi Bonada", "title": "A Neural Parametric Singing Synthesizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model for singing synthesis based on a modified version of\nthe WaveNet architecture. Instead of modeling raw waveform, we model features\nproduced by a parametric vocoder that separates the influence of pitch and\ntimbre. This allows conveniently modifying pitch to match any target melody,\nfacilitates training on more modest dataset sizes, and significantly reduces\ntraining and generation times. Our model makes frame-wise predictions using\nmixture density outputs rather than categorical outputs in order to reduce the\nrequired parameter count. As we found overfitting to be an issue with the\nrelatively small datasets used in our experiments, we propose a method to\nregularize the model and make the autoregressive generation process more robust\nto prediction errors. Using a simple multi-stream architecture, harmonic,\naperiodic and voiced/unvoiced components can all be predicted in a coherent\nmanner. We compare our method to existing parametric statistical and\nstate-of-the-art concatenative methods using quantitative metrics and a\nlistening test. While naive implementations of the autoregressive generation\nalgorithm tend to be inefficient, using a smart algorithm we can greatly speed\nup the process and obtain a system that's competitive in both speed and\nquality.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 15:57:08 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 10:31:56 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 12:20:01 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Blaauw", "Merlijn", ""], ["Bonada", "Jordi", ""]]}, {"id": "1704.03817", "submitter": "Ruohan Wang", "authors": "Ruohan Wang, Antoine Cully, Hyung Jin Chang, Yiannis Demiris", "title": "MAGAN: Margin Adaptation for Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs)\nalgorithm, a novel training procedure for GANs to improve stability and\nperformance by using an adaptive hinge loss function. We estimate the\nappropriate hinge loss margin with the expected energy of the target\ndistribution, and derive principled criteria for when to update the margin. We\nprove that our method converges to its global optimum under certain\nassumptions. Evaluated on the task of unsupervised image generation, the\nproposed training procedure is simple yet robust on a diverse set of data, and\nachieves qualitative and quantitative improvements compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 16:15:38 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 20:58:28 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 12:37:36 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Wang", "Ruohan", ""], ["Cully", "Antoine", ""], ["Chang", "Hyung Jin", ""], ["Demiris", "Yiannis", ""]]}, {"id": "1704.03834", "submitter": "Sael Lee", "authors": "Jaya Thomas and Lee Sael", "title": "Deep Neural Network Based Precursor microRNA Prediction on Eleven\n  Species", "comments": "6 pages, 2 figures, extended from BigComp2017 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MicroRNA (miRNA) are small non-coding RNAs that regulates the gene expression\nat the post-transcriptional level. Determining whether a sequence segment is\nmiRNA is experimentally challenging. Also, experimental results are sensitive\nto the experimental environment. These limitations inspire the development of\ncomputational methods for predicting the miRNAs. We propose a deep learning\nbased classification model, called DP-miRNA, for predicting precursor miRNA\nsequence that contains the miRNA sequence. The feature set based Restricted\nBoltzmann Machine method, which we call DP-miRNA, uses 58 features that are\ncategorized into four groups: sequence features, folding measures, stem-loop\nfeatures and statistical feature. We evaluate the performance of the DP-miRNA\non eleven twelve data sets of varying species, including the human. The deep\nneural network based classification outperformed support vector machine, neural\nnetwork, naive Baye's classifiers, k-nearest neighbors, random forests, and a\nhybrid system combining support vector machine and genetic algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 13:38:01 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Thomas", "Jaya", ""], ["Sael", "Lee", ""]]}, {"id": "1704.03844", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Renato L. F. Cunha, Evandro Caldeira, Luciana Fujii", "title": "Determining Song Similarity via Machine Learning Techniques and Tagging\n  Information", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of determining item similarity is a crucial one in a recommender\nsystem. This constitutes the base upon which the recommender system will work\nto determine which items are more likely to be enjoyed by a user, resulting in\nmore user engagement. In this paper we tackle the problem of determining song\nsimilarity based solely on song metadata (such as the performer, and song\ntitle) and on tags contributed by users. We evaluate our approach under a\nseries of different machine learning algorithms. We conclude that tf-idf\nachieves better results than Word2Vec to model the dataset to feature vectors.\nWe also conclude that k-NN models have better performance than SVMs and Linear\nRegression for this problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 17:07:51 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Cunha", "Renato L. F.", ""], ["Caldeira", "Evandro", ""], ["Fujii", "Luciana", ""]]}, {"id": "1704.03847", "submitter": "Nikolay Savinov", "authors": "Timo Hackel, Nikolay Savinov, Lubor Ladicky, Jan D. Wegner, Konrad\n  Schindler, Marc Pollefeys", "title": "Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark", "comments": "Accepted to ISPRS Annals. The benchmark website is available at\n  http://www.semantic3d.net/ . The baseline code is available at\n  https://github.com/nsavinov/semantic3dnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new 3D point cloud classification benchmark data set\nwith over four billion manually labelled points, meant as input for data-hungry\n(deep) learning methods. We also discuss first submissions to the benchmark\nthat use deep convolutional neural networks (CNNs) as a work horse, which\nalready show remarkable performance improvements over state-of-the-art. CNNs\nhave become the de-facto standard for many tasks in computer vision and machine\nlearning like semantic segmentation or object detection in images, but have no\nyet led to a true breakthrough for 3D point cloud labelling tasks due to lack\nof training data. With the massive data set presented in this paper, we aim at\nclosing this data gap to help unleash the full potential of deep learning\nmethods for 3D labelling tasks. Our semantic3D.net data set consists of dense\npoint clouds acquired with static terrestrial laser scanners. It contains 8\nsemantic classes and covers a wide range of urban outdoor scenes: churches,\nstreets, railroad tracks, squares, villages, soccer fields and castles. We\ndescribe our labelling interface and show that our data set provides more dense\nand complete point clouds with much higher overall number of labelled points\ncompared to those already available to the research community. We further\nprovide baseline method descriptions and comparison between methods submitted\nto our online system. We hope semantic3D.net will pave the way for deep\nlearning methods in 3D point cloud labelling to learn richer, more general 3D\nrepresentations, and first submissions after only a few months indicate that\nthis might indeed be the case.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 17:12:57 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Hackel", "Timo", ""], ["Savinov", "Nikolay", ""], ["Ladicky", "Lubor", ""], ["Wegner", "Jan D.", ""], ["Schindler", "Konrad", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1704.03866", "submitter": "Gautam Kamath", "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur\n  Moitra, Alistair Stewart", "title": "Robustly Learning a Gaussian: Getting Optimal Error, Efficiently", "comments": "To appear in SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of learning the parameters of a\nhigh-dimensional Gaussian in the presence of noise -- where an\n$\\varepsilon$-fraction of our samples were chosen by an adversary. We give\nrobust estimators that achieve estimation error $O(\\varepsilon)$ in the total\nvariation distance, which is optimal up to a universal constant that is\nindependent of the dimension.\n  In the case where just the mean is unknown, our robustness guarantee is\noptimal up to a factor of $\\sqrt{2}$ and the running time is polynomial in $d$\nand $1/\\epsilon$. When both the mean and covariance are unknown, the running\ntime is polynomial in $d$ and quasipolynomial in $1/\\varepsilon$. Moreover all\nof our algorithms require only a polynomial number of samples. Our work shows\nthat the same sorts of error guarantees that were established over fifty years\nago in the one-dimensional setting can also be achieved by efficient algorithms\nin high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 17:55:05 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 21:52:55 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kamath", "Gautam", ""], ["Kane", "Daniel M.", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""], ["Stewart", "Alistair", ""]]}, {"id": "1704.03926", "submitter": "Bence Cserna", "authors": "Bence Cserna, Marek Petrik, Reazul Hasan Russel, Wheeler Ruml", "title": "Value Directed Exploration in Multi-Armed Bandits with Structured Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandits are a quintessential machine learning problem requiring\nthe balancing of exploration and exploitation. While there has been progress in\ndeveloping algorithms with strong theoretical guarantees, there has been less\nfocus on practical near-optimal finite-time performance. In this paper, we\npropose an algorithm for Bayesian multi-armed bandits that utilizes\nvalue-function-driven online planning techniques. Building on previous work on\nUCB and Gittins index, we introduce linearly-separable value functions that\ntake both the expected return and the benefit of exploration into consideration\nto perform n-step lookahead. The algorithm enjoys a sub-linear performance\nguarantee and we present simulation results that confirm its strength in\nproblems with structured priors. The simplicity and generality of our approach\nmakes it a strong candidate for analyzing more complex multi-armed bandit\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 20:46:50 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 14:02:27 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Cserna", "Bence", ""], ["Petrik", "Marek", ""], ["Russel", "Reazul Hasan", ""], ["Ruml", "Wheeler", ""]]}, {"id": "1704.03969", "submitter": "Jian Du", "authors": "Jian Du, Shaodan Ma, Yik-Chung Wu, Soummya Kar and Jos\\'e M. F. Moura", "title": "Convergence analysis of the information matrix in Gaussian belief\n  propagation", "comments": "arXiv admin note: substantial text overlap with arXiv:1611.02010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian belief propagation (BP) has been widely used for distributed\nestimation in large-scale networks such as the smart grid, communication\nnetworks, and social networks, where local measurements/observations are\nscattered over a wide geographical area. However, the convergence of Gaus- sian\nBP is still an open issue. In this paper, we consider the convergence of\nGaussian BP, focusing in particular on the convergence of the information\nmatrix. We show analytically that the exchanged message information matrix\nconverges for arbitrary positive semidefinite initial value, and its dis- tance\nto the unique positive definite limit matrix decreases exponentially fast.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 02:07:15 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Du", "Jian", ""], ["Ma", "Shaodan", ""], ["Wu", "Yik-Chung", ""], ["Kar", "Soummya", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1704.03971", "submitter": "Sitao Xiang", "authors": "Sitao Xiang, Hao Li", "title": "On the Effects of Batch and Weight Normalization in Generative\n  Adversarial Networks", "comments": "v3 rejected by NIPS 2017, updated and re-submitted to CVPR 2018. v4:\n  add experiments with ResNet and like to new code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are highly effective unsupervised\nlearning frameworks that can generate very sharp data, even for data such as\nimages with complex, highly multimodal distributions. However GANs are known to\nbe very hard to train, suffering from problems such as mode collapse and\ndisturbing visual artifacts. Batch normalization (BN) techniques have been\nintroduced to address the training. Though BN accelerates the training in the\nbeginning, our experiments show that the use of BN can be unstable and\nnegatively impact the quality of the trained model. The evaluation of BN and\nnumerous other recent schemes for improving GAN training is hindered by the\nlack of an effective objective quality measure for GAN models. To address these\nissues, we first introduce a weight normalization (WN) approach for GAN\ntraining that significantly improves the stability, efficiency and the quality\nof the generated samples. To allow a methodical evaluation, we introduce\nsquared Euclidean reconstruction error on a test set as a new objective\nmeasure, to assess training performance in terms of speed, stability, and\nquality of generated samples. Our experiments with a standard DCGAN\narchitecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10)\nindicate that training using WN is generally superior to BN for GANs, achieving\n10% lower mean squared loss for reconstruction and significantly better\nqualitative results than BN. We further demonstrate the stability of WN on a\n21-layer ResNet trained with the CelebA data set. The code for this paper is\navailable at https://github.com/stormraiser/gan-weightnorm-resnet\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 02:15:28 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 02:28:03 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 07:16:59 GMT"}, {"version": "v4", "created": "Mon, 4 Dec 2017 01:56:42 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Xiang", "Sitao", ""], ["Li", "Hao", ""]]}, {"id": "1704.03976", "submitter": "Takeru Miyato", "authors": "Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Shin Ishii", "title": "Virtual Adversarial Training: A Regularization Method for Supervised and\n  Semi-Supervised Learning", "comments": "To be appeared in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new regularization method based on virtual adversarial loss: a\nnew measure of local smoothness of the conditional label distribution given\ninput. Virtual adversarial loss is defined as the robustness of the conditional\nlabel distribution around each input data point against local perturbation.\nUnlike adversarial training, our method defines the adversarial direction\nwithout label information and is hence applicable to semi-supervised learning.\nBecause the directions in which we smooth the model are only \"virtually\"\nadversarial, we call our method virtual adversarial training (VAT). The\ncomputational cost of VAT is relatively low. For neural networks, the\napproximated gradient of virtual adversarial loss can be computed with no more\nthan two pairs of forward- and back-propagations. In our experiments, we\napplied VAT to supervised and semi-supervised learning tasks on multiple\nbenchmark datasets. With a simple enhancement of the algorithm based on the\nentropy minimization principle, our VAT achieves state-of-the-art performance\nfor semi-supervised learning tasks on SVHN and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 02:45:27 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 04:52:47 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Miyato", "Takeru", ""], ["Maeda", "Shin-ichi", ""], ["Koyama", "Masanori", ""], ["Ishii", "Shin", ""]]}, {"id": "1704.03992", "submitter": "Ying Zhang", "authors": "Ying Zhang", "title": "Fully Distributed and Asynchronized Stochastic Gradient Descent for\n  Networked Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a general data-fitting problem over a networked system,\nin which many computing nodes are connected by an undirected graph. This kind\nof problem can find many real-world applications and has been studied\nextensively in the literature. However, existing solutions either need a\ncentral controller for information sharing or requires slot synchronization\namong different nodes, which increases the difficulty of practical\nimplementations, especially for a very large and heterogeneous system.\n  As a contrast, in this paper, we treat the data-fitting problem over the\nnetwork as a stochastic programming problem with many constraints. By adapting\nthe results in a recent paper, we design a fully distributed and asynchronized\nstochastic gradient descent (SGD) algorithm. We show that our algorithm can\nachieve global optimality and consensus asymptotically by only local\ncomputations and communications. Additionally, we provide a sharp lower bound\nfor the convergence speed in the regular graph case. This result fits the\nintuition and provides guidance to design a `good' network topology to speed up\nthe convergence. Also, the merit of our design is validated by experiments on\nboth synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 04:58:54 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Zhang", "Ying", ""]]}, {"id": "1704.04010", "submitter": "Dylan Foster", "authors": "Dylan J. Foster, Alexander Rakhlin, Karthik Sridharan", "title": "ZigZag: A new approach to adaptive online learning", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel family of algorithms for the online learning setting with\nregret against any data sequence bounded by the empirical Rademacher complexity\nof that sequence. To develop a general theory of when this type of adaptive\nregret bound is achievable we establish a connection to the theory of\ndecoupling inequalities for martingales in Banach spaces. When the hypothesis\nclass is a set of linear functions bounded in some norm, such a regret bound is\nachievable if and only if the norm satisfies certain decoupling inequalities\nfor martingales. Donald Burkholder's celebrated geometric characterization of\ndecoupling inequalities (1984) states that such an inequality holds if and only\nif there exists a special function called a Burkholder function satisfying\ncertain restricted concavity properties. Our online learning algorithms are\nefficient in terms of queries to this function.\n  We realize our general theory by giving novel efficient algorithms for\nclasses including lp norms, Schatten p-norms, group norms, and reproducing\nkernel Hilbert spaces. The empirical Rademacher complexity regret bound implies\n--- when used in the i.i.d. setting --- a data-dependent complexity bound for\nexcess risk after online-to-batch conversion. To showcase the power of the\nempirical Rademacher complexity regret bound, we derive improved rates for a\nsupervised learning generalization of the online learning with low rank experts\ntask and for the online matrix prediction task.\n  In addition to obtaining tight data-dependent regret bounds, our algorithms\nenjoy improved efficiency over previous techniques based on Rademacher\ncomplexity, automatically work in the infinite horizon setting, and are\nscale-free. To obtain such adaptive methods, we introduce novel machinery, and\nthe resulting algorithms are not based on the standard tools of online convex\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 06:50:34 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Foster", "Dylan J.", ""], ["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1704.04039", "submitter": "Vladimir Golkov", "authors": "Vladimir Golkov, Marcin J. Skwark, Atanas Mirchev, Georgi Dikov,\n  Alexander R. Geanes, Jeffrey Mendenhall, Jens Meiler and Daniel Cremers", "title": "3D Deep Learning for Biological Function Prediction from Physical Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the biological function of molecules, be it proteins or drug-like\ncompounds, from their atomic structure is an important and long-standing\nproblem. Function is dictated by structure, since it is by spatial interactions\nthat molecules interact with each other, both in terms of steric\ncomplementarity, as well as intermolecular forces. Thus, the electron density\nfield and electrostatic potential field of a molecule contain the \"raw\nfingerprint\" of how this molecule can fit to binding partners. In this paper,\nwe show that deep learning can predict biological function of molecules\ndirectly from their raw 3D approximated electron density and electrostatic\npotential fields. Protein function based on EC numbers is predicted from the\napproximated electron density field. In another experiment, the activity of\nsmall molecules is predicted with quality comparable to state-of-the-art\ndescriptor-based methods. We propose several alternative computational models\nfor the GPU with different memory and runtime requirements for different sizes\nof molecules and of databases. We also propose application-specific\nmulti-channel data representations. With future improvements of training\ndatasets and neural network settings in combination with complementary\ninformation sources (sequence, genomic context, expression level), deep\nlearning can be expected to show its generalization power and revolutionize the\nfield of molecular function prediction.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:11:23 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Golkov", "Vladimir", ""], ["Skwark", "Marcin J.", ""], ["Mirchev", "Atanas", ""], ["Dikov", "Georgi", ""], ["Geanes", "Alexander R.", ""], ["Mendenhall", "Jeffrey", ""], ["Meiler", "Jens", ""], ["Cremers", "Daniel", ""]]}, {"id": "1704.04050", "submitter": "Caifa Zhou", "authors": "Lin Ma, Caifa Zhou, Xi Liu, Yubin Xu", "title": "Adaptive Neighboring Selection Algorithm Based on Curvature Prediction\n  in Manifold Learning", "comments": "3 figures, from Journal of Harbin Institute of Technology", "journal-ref": "Journal of Harbin Institute of Technology, 20(3), pp.119--123\n  (2013)", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently manifold learning algorithm for dimensionality reduction attracts\nmore and more interests, and various linear and nonlinear, global and local\nalgorithms are proposed. The key step of manifold learning algorithm is the\nneighboring region selection. However, so far for the references we know, few\nof which propose a generally accepted algorithm to well select the neighboring\nregion. So in this paper, we propose an adaptive neighboring selection\nalgorithm, which successfully applies the LLE and ISOMAP algorithms in the\ntest. It is an algorithm that can find the optimal K nearest neighbors of the\ndata points on the manifold. And the theoretical basis of the algorithm is the\napproximated curvature of the data point on the manifold. Based on Riemann\nGeometry, Jacob matrix is a proper mathematical concept to predict the\napproximated curvature. By verifying the proposed algorithm on embedding Swiss\nroll from R3 to R2 based on LLE and ISOMAP algorithm, the simulation results\nshow that the proposed adaptive neighboring selection algorithm is feasible and\nable to find the optimal value of K, making the residual variance relatively\nsmall and better visualization of the results. By quantitative analysis, the\nembedding quality measured by residual variance is increased 45.45% after using\nthe proposed algorithm in LLE.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:33:56 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Ma", "Lin", ""], ["Zhou", "Caifa", ""], ["Liu", "Xi", ""], ["Xu", "Yubin", ""]]}, {"id": "1704.04055", "submitter": "Dino Ienco", "authors": "Dino Ienco, Raffaele Gaetano, Claire Dupaquier and Pierre Maurel", "title": "Land Cover Classification via Multi-temporal Spatial Data by Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2017.2728698", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, modern earth observation programs produce huge volumes of satellite\nimages time series (SITS) that can be useful to monitor geographical areas\nthrough time. How to efficiently analyze such kind of information is still an\nopen question in the remote sensing field. Recently, deep learning methods\nproved suitable to deal with remote sensing data mainly for scene\nclassification (i.e. Convolutional Neural Networks - CNNs - on single images)\nwhile only very few studies exist involving temporal deep learning approaches\n(i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series.\nIn this letter we evaluate the ability of Recurrent Neural Networks, in\nparticular the Long-Short Term Memory (LSTM) model, to perform land cover\nclassification considering multi-temporal spatial data derived from a time\nseries of satellite images. We carried out experiments on two different\ndatasets considering both pixel-based and object-based classification. The\nobtained results show that Recurrent Neural Networks are competitive compared\nto state-of-the-art classifiers, and may outperform classical approaches in\npresence of low represented and/or highly mixed classes. We also show that\nusing the alternative feature representation generated by LSTM can improve the\nperformances of standard classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:47:12 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Ienco", "Dino", ""], ["Gaetano", "Raffaele", ""], ["Dupaquier", "Claire", ""], ["Maurel", "Pierre", ""]]}, {"id": "1704.04095", "submitter": "Mohsen Moradi", "authors": "Mohsen Moradi", "title": "Training Neural Networks Based on Imperialist Competitive Algorithm for\n  Predicting Earthquake Intensity", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study we determined neural network weights and biases by Imperialist\nCompetitive Algorithm (ICA) in order to train network for predicting earthquake\nintensity in Richter. For this reason, we used dependent parameters like\nearthquake occurrence time, epicenter's latitude and longitude in degree, focal\ndepth in kilometer, and the seismological center distance from epicenter and\nearthquake focal center in kilometer which has been provided by Berkeley data\nbase. The studied neural network has two hidden layer: its first layer has 16\nneurons and the second layer has 24 neurons. By using ICA algorithm, average\nerror for testing data is 0.0007 with a variance equal to 0.318. The earthquake\nprediction error in Richter by MSE criteria for ICA algorithm is 0.101, but by\nusing GA, the MSE value is 0.115.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 22:42:52 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Moradi", "Mohsen", ""]]}, {"id": "1704.04110", "submitter": "David Salinas", "authors": "David Salinas, Valentin Flunkert, Jan Gasthaus", "title": "DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasting, i.e. estimating the probability distribution of a\ntime series' future given its past, is a key enabler for optimizing business\nprocesses. In retail businesses, for example, forecasting demand is crucial for\nhaving the right inventory available at the right time at the right place. In\nthis paper we propose DeepAR, a methodology for producing accurate\nprobabilistic forecasts, based on training an auto regressive recurrent network\nmodel on a large number of related time series. We demonstrate how by applying\ndeep learning techniques to forecasting, one can overcome many of the\nchallenges faced by widely-used classical approaches to the problem. We show\nthrough extensive empirical evaluation on several real-world forecasting data\nsets accuracy improvements of around 15% compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 13:11:53 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 07:39:14 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 13:43:50 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Salinas", "David", ""], ["Flunkert", "Valentin", ""], ["Gasthaus", "Jan", ""]]}, {"id": "1704.04133", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Alexander Wong, Graham W. Taylor", "title": "Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR)\n  Approach to Understanding Deep Neural Networks", "comments": "Accepted at Computer Vision and Patter Recognition Workshop (CVPR-W)\n  on Explainable Computer Vision, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an\napproach to visualize and understand the decisions made by deep neural networks\n(DNNs) given a specific input. CLEAR facilitates the visualization of attentive\nregions and levels of interest of DNNs during the decision-making process. It\nalso enables the visualization of the most dominant classes associated with\nthese attentive regions of interest. As such, CLEAR can mitigate some of the\nshortcomings of heatmap-based methods associated with decision ambiguity, and\nallows for better insights into the decision-making process of DNNs.\nQuantitative and qualitative experiments across three different datasets\ndemonstrate the efficacy of CLEAR for gaining a better understanding of the\ninner workings of DNNs during the decision-making process.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 13:44:33 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 18:38:06 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Kumar", "Devinder", ""], ["Wong", "Alexander", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1704.04163", "submitter": "Cameron Musco", "authors": "Cameron Musco, Praneeth Netrapalli, Aaron Sidford, Shashanka Ubaru,\n  David P. Woodruff", "title": "Spectrum Approximation Beyond Fast Matrix Multiplication: Algorithms and\n  Hardness", "comments": "ITCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the singular value spectrum of a matrix $A \\in \\mathbb{R}^{n\n\\times n}$ is a fundamental task in countless applications. In matrix\nmultiplication time, it is possible to perform a full SVD and directly compute\nthe singular values $\\sigma_1,...,\\sigma_n$. However, little is known about\nalgorithms that break this runtime barrier.\n  Using tools from stochastic trace estimation, polynomial approximation, and\nfast system solvers, we show how to efficiently isolate different ranges of\n$A$'s spectrum and approximate the number of singular values in these ranges.\nWe thus effectively compute a histogram of the spectrum, which can stand in for\nthe true singular values in many applications.\n  We use this primitive to give the first algorithms for approximating a wide\nclass of symmetric matrix norms in faster than matrix multiplication time. For\nexample, we give a $(1 + \\epsilon)$ approximation algorithm for the\nSchatten-$1$ norm (the nuclear norm) running in just $\\tilde O((nnz(A)n^{1/3} +\nn^2)\\epsilon^{-3})$ time for $A$ with uniform row sparsity or $\\tilde\nO(n^{2.18} \\epsilon^{-3})$ time for dense matrices. The runtime scales smoothly\nfor general Schatten-$p$ norms, notably becoming $\\tilde O (p \\cdot nnz(A)\n\\epsilon^{-3})$ for any $p \\ge 2$.\n  At the same time, we show that the complexity of spectrum approximation is\ninherently tied to fast matrix multiplication in the small $\\epsilon$ regime.\nWe prove that achieving milder $\\epsilon$ dependencies in our algorithms would\nimply faster than matrix multiplication time triangle detection for general\ngraphs. This further implies that highly accurate algorithms running in\nsubcubic time yield subcubic time matrix multiplication. As an application of\nour bounds, we show that precisely computing all effective resistances in a\ngraph in less than matrix multiplication time is likely difficult, barring a\nmajor algorithmic breakthrough.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 14:55:23 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 14:15:31 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2019 15:37:07 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Musco", "Cameron", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""], ["Ubaru", "Shashanka", ""], ["Woodruff", "David P.", ""]]}, {"id": "1704.04222", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang, James Glass", "title": "Learning Latent Representations for Speech Generation and Transformation", "comments": "Accepted to Interspeech 2017", "journal-ref": "Interspeech 2017, pp 1273-1277", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ability to model a generative process and learn a latent representation\nfor speech in an unsupervised fashion will be crucial to process vast\nquantities of unlabelled speech data. Recently, deep probabilistic generative\nmodels such as Variational Autoencoders (VAEs) have achieved tremendous success\nin modeling natural images. In this paper, we apply a convolutional VAE to\nmodel the generative process of natural speech. We derive latent space\narithmetic operations to disentangle learned latent representations. We\ndemonstrate the capability of our model to modify the phonetic content or the\nspeaker identity for speech segments using the derived operations, without the\nneed for parallel supervisory data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 17:41:11 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 16:41:54 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1704.04235", "submitter": "Xiaofang Wang", "authors": "Lingkun Luo, Xiaofang Wang, Shiqiang Hu, Chao Wang, Yuxing Tang,\n  Liming Chen", "title": "Close Yet Distinctive Domain Adaptation", "comments": "11pages, 3 figures, ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is transfer learning which aims to generalize a learning\nmodel across training and testing data with different distributions. Most\nprevious research tackle this problem in seeking a shared feature\nrepresentation between source and target domains while reducing the mismatch of\ntheir data distributions. In this paper, we propose a close yet discriminative\ndomain adaptation method, namely CDDA, which generates a latent feature\nrepresentation with two interesting properties. First, the discrepancy between\nthe source and target domain, measured in terms of both marginal and\nconditional probability distribution via Maximum Mean Discrepancy is minimized\nso as to attract two domains close to each other. More importantly, we also\ndesign a repulsive force term, which maximizes the distances between each label\ndependent sub-domain to all others so as to drag different class dependent\nsub-domains far away from each other and thereby increase the discriminative\npower of the adapted domain. Moreover, given the fact that the underlying data\nmanifold could have complex geometric structure, we further propose the\nconstraints of label smoothness and geometric structure consistency for label\npropagation. Extensive experiments are conducted on 36 cross-domain image\nclassification tasks over four public datasets. The comprehensive results show\nthat the proposed method consistently outperforms the state-of-the-art methods\nwith significant margins.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 08:30:21 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Luo", "Lingkun", ""], ["Wang", "Xiaofang", ""], ["Hu", "Shiqiang", ""], ["Wang", "Chao", ""], ["Tang", "Yuxing", ""], ["Chen", "Liming", ""]]}, {"id": "1704.04238", "submitter": "David Kappel", "authors": "David Kappel, Robert Legenstein, Stefan Habenschuss, Michael Hsieh and\n  Wolfgang Maass", "title": "A dynamic connectome supports the emergence of stable computational\n  function of neural circuits through reward-based learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synaptic connections between neurons in the brain are dynamic because of\ncontinuously ongoing spine dynamics, axonal sprouting, and other processes. In\nfact, it was recently shown that the spontaneous synapse-autonomous component\nof spine dynamics is at least as large as the component that depends on the\nhistory of pre- and postsynaptic neural activity. These data are inconsistent\nwith common models for network plasticity, and raise the questions how neural\ncircuits can maintain a stable computational function in spite of these\ncontinuously ongoing processes, and what functional uses these ongoing\nprocesses might have. Here, we present a rigorous theoretical framework for\nthese seemingly stochastic spine dynamics and rewiring processes in the context\nof reward-based learning tasks. We show that spontaneous synapse-autonomous\nprocesses, in combination with reward signals such as dopamine, can explain the\ncapability of networks of neurons in the brain to configure themselves for\nspecific computational tasks, and to compensate automatically for later changes\nin the network or task. Furthermore we show theoretically and through computer\nsimulations that stable computational performance is compatible with\ncontinuously ongoing synapse-autonomous changes. After reaching good\ncomputational performance it causes primarily a slow drift of network\narchitecture and dynamics in task-irrelevant dimensions, as observed for neural\nactivity in motor cortex and other areas. On the more abstract level of\nreinforcement learning the resulting model gives rise to an understanding of\nreward-driven network plasticity as continuous sampling of network\nconfigurations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 15:52:14 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 10:34:44 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 08:11:34 GMT"}, {"version": "v4", "created": "Fri, 5 Jan 2018 12:56:42 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kappel", "David", ""], ["Legenstein", "Robert", ""], ["Habenschuss", "Stefan", ""], ["Hsieh", "Michael", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1704.04289", "submitter": "Stephan Mandt", "authors": "Stephan Mandt, Matthew D. Hoffman, and David M. Blei", "title": "Stochastic Gradient Descent as Approximate Bayesian Inference", "comments": "35 pages, published version (JMLR 2017)", "journal-ref": "Journal of Machine Learning Research 18 (2017) 1-35", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent with a constant learning rate (constant SGD)\nsimulates a Markov chain with a stationary distribution. With this perspective,\nwe derive several new results. (1) We show that constant SGD can be used as an\napproximate Bayesian posterior inference algorithm. Specifically, we show how\nto adjust the tuning parameters of constant SGD to best match the stationary\ndistribution to a posterior, minimizing the Kullback-Leibler divergence between\nthese two distributions. (2) We demonstrate that constant SGD gives rise to a\nnew variational EM algorithm that optimizes hyperparameters in complex\nprobabilistic models. (3) We also propose SGD with momentum for sampling and\nshow how to adjust the damping coefficient accordingly. (4) We analyze MCMC\nalgorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we\nquantify the approximation errors due to finite learning rates. Finally (5), we\nuse the stochastic process perspective to give a short proof of why Polyak\naveraging is optimal. Based on this idea, we propose a scalable approximate\nMCMC algorithm, the Averaged Stochastic Gradient Sampler.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 22:17:30 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 21:07:09 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Mandt", "Stephan", ""], ["Hoffman", "Matthew D.", ""], ["Blei", "David M.", ""]]}, {"id": "1704.04313", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Philippe Degen, Luca Benini", "title": "CBinfer: Change-Based Inference for Convolutional Neural Networks on\n  Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.PF eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting per-frame features using convolutional neural networks for\nreal-time processing of video data is currently mainly performed on powerful\nGPU-accelerated workstations and compute clusters. However, there are many\napplications such as smart surveillance cameras that require or would benefit\nfrom on-site processing. To this end, we propose and evaluate a novel algorithm\nfor change-based evaluation of CNNs for video data recorded with a static\ncamera setting, exploiting the spatio-temporal sparsity of pixel changes. We\nachieve an average speed-up of 8.6x over a cuDNN baseline on a realistic\nbenchmark with a negligible accuracy loss of less than 0.1% and no retraining\nof the network. The resulting energy efficiency is 10x higher than that of\nper-frame evaluation and reaches an equivalent of 328 GOp/s/W on the Tegra X1\nplatform.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 00:36:55 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 09:27:14 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Degen", "Philippe", ""], ["Benini", "Luca", ""]]}, {"id": "1704.04327", "submitter": "Surya Bhupatiraju", "authors": "Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, Pushmeet Kohli", "title": "Deep API Programmer: Learning to Program with APIs", "comments": "8 pages + 4 pages of supplementary material. Submitted to IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DAPIP, a Programming-By-Example system that learns to program with\nAPIs to perform data transformation tasks. We design a domain-specific language\n(DSL) that allows for arbitrary concatenations of API outputs and constant\nstrings. The DSL consists of three family of APIs: regular expression-based\nAPIs, lookup APIs, and transformation APIs. We then present a novel neural\nsynthesis algorithm to search for programs in the DSL that are consistent with\na given set of examples. The search algorithm uses recently introduced neural\narchitectures to encode input-output examples and to model the program search\nin the DSL. We show that synthesis algorithm outperforms baseline methods for\nsynthesizing programs on both synthetic and real-world benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 02:04:06 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Bhupatiraju", "Surya", ""], ["Singh", "Rishabh", ""], ["Mohamed", "Abdel-rahman", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1704.04333", "submitter": "Yuxin Peng", "authors": "Jinwei Qi, Xin Huang, and Yuxin Peng", "title": "Cross-media Similarity Metric Learning with Unified Deep Networks", "comments": "19 pages, submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a highlighting research topic in the multimedia area, cross-media\nretrieval aims to capture the complex correlations among multiple media types.\nLearning better shared representation and distance metric for multimedia data\nis important to boost the cross-media retrieval. Motivated by the strong\nability of deep neural network in feature representation and comparison\nfunctions learning, we propose the Unified Network for Cross-media Similarity\nMetric (UNCSM) to associate cross-media shared representation learning with\ndistance metric in a unified framework. First, we design a two-pathway deep\nnetwork pretrained with contrastive loss, and employ double triplet similarity\nloss for fine-tuning to learn the shared representation for each media type by\nmodeling the relative semantic similarity. Second, the metric network is\ndesigned for effectively calculating the cross-media similarity of the shared\nrepresentation, by modeling the pairwise similar and dissimilar constraints.\nCompared to the existing methods which mostly ignore the dissimilar constraints\nand only use sample distance metric as Euclidean distance separately, our UNCSM\napproach unifies the representation learning and distance metric to preserve\nthe relative similarity as well as embrace more complex similarity functions\nfor further improving the cross-media retrieval accuracy. The experimental\nresults show that our UNCSM approach outperforms 8 state-of-the-art methods on\n4 widely-used cross-media datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 02:25:50 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Qi", "Jinwei", ""], ["Huang", "Xin", ""], ["Peng", "Yuxin", ""]]}, {"id": "1704.04422", "submitter": "Rui Chen", "authors": "Rui Chen, Huizhu Jia, Xiaodong Xie, Wen Gao", "title": "Learning a collaborative multiscale dictionary based on robust empirical\n  mode decomposition", "comments": "to be published in Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning is a challenge topic in many image processing areas. The\nbasic goal is to learn a sparse representation from an overcomplete basis set.\nDue to combining the advantages of generic multiscale representations with\nlearning based adaptivity, multiscale dictionary representation approaches have\nthe power in capturing structural characteristics of natural images. However,\nexisting multiscale learning approaches still suffer from three main\nweaknesses: inadaptability to diverse scales of image data, sensitivity to\nnoise and outliers, difficulty to determine optimal dictionary structure. In\nthis paper, we present a novel multiscale dictionary learning paradigm for\nsparse image representations based on an improved empirical mode decomposition.\nThis powerful data-driven analysis tool for multi-dimensional signal can fully\nadaptively decompose the image into multiscale oscillating components according\nto intrinsic modes of data self. This treatment can obtain a robust and\neffective sparse representation, and meanwhile generates a raw base dictionary\nat multiple geometric scales and spatial frequency bands. This dictionary is\nrefined by selecting optimal oscillating atoms based on frequency clustering.\nIn order to further enhance sparsity and generalization, a tolerance dictionary\nis learned using a coherence regularized model. A fast proximal scheme is\ndeveloped to optimize this model. The multiscale dictionary is considered as\nthe product of oscillating dictionary and tolerance dictionary. Experimental\nresults demonstrate that the proposed learning approach has the superior\nperformance in sparse image representations as compared with several competing\nmethods. We also show the promising results in image denoising application.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 02:31:20 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Chen", "Rui", ""], ["Jia", "Huizhu", ""], ["Xie", "Xiaodong", ""], ["Gao", "Wen", ""]]}, {"id": "1704.04451", "submitter": "Phong Le", "authors": "Phong Le and Ivan Titov", "title": "Optimizing Differentiable Relaxations of Coreference Evaluation Metrics", "comments": "10 pages. CoNLL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coreference evaluation metrics are hard to optimize directly as they are\nnon-differentiable functions, not easily decomposable into elementary\ndecisions. Consequently, most approaches optimize objectives only indirectly\nrelated to the end goal, resulting in suboptimal performance. Instead, we\npropose a differentiable relaxation that lends itself to gradient-based\noptimisation, thus bypassing the need for reinforcement learning or heuristic\nmodification of cross-entropy. We show that by modifying the training objective\nof a competitive neural coreference system, we obtain a substantial gain in\nperformance. This suggests that our approach can be regarded as a viable\nalternative to using reinforcement learning or more computationally expensive\nimitation learning.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 15:22:51 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 08:30:32 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 07:55:47 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Le", "Phong", ""], ["Titov", "Ivan", ""]]}, {"id": "1704.04456", "submitter": "Kiwon Um", "authors": "Kiwon Um, Xiangyu Hu, Nils Thuerey", "title": "Liquid Splash Modeling with Neural Networks", "comments": "to appear in Computer Graphics Forum, more information:\n  https://ge.in.tum.de/publications/2018-mlflip-um/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new data-driven approach to model detailed splashes for\nliquid simulations with neural networks. Our model learns to generate\nsmall-scale splash detail for the fluid-implicit-particle method using training\ndata acquired from physically parametrized, high resolution simulations. We use\nneural networks to model the regression of splash formation using a classifier\ntogether with a velocity modifier. For the velocity modification, we employ a\nheteroscedastic model. We evaluate our method for different spatial scales,\nsimulation setups, and solvers. Our simulation results demonstrate that our\nmodel significantly improves visual fidelity with a large amount of realistic\ndroplet formation and yields splash detail much more efficiently than finer\ndiscretizations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 15:28:37 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 17:19:52 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Um", "Kiwon", ""], ["Hu", "Xiangyu", ""], ["Thuerey", "Nils", ""]]}, {"id": "1704.04463", "submitter": "Huizhen Yu", "authors": "Huizhen Yu, A. Rupam Mahmood, Richard S. Sutton", "title": "On Generalized Bellman Equations and Temporal-Difference Learning", "comments": "Minor revision; 41 pages; to appear in Journal on Machine Learning\n  Research, 2018", "journal-ref": "Journal of Machine Learning Research 19(48):1-49, 2018", "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider off-policy temporal-difference (TD) learning in discounted Markov\ndecision processes, where the goal is to evaluate a policy in a model-free way\nby using observations of a state process generated without executing the\npolicy. To curb the high variance issue in off-policy TD learning, we propose a\nnew scheme of setting the $\\lambda$-parameters of TD, based on generalized\nBellman equations. Our scheme is to set $\\lambda$ according to the eligibility\ntrace iterates calculated in TD, thereby easily keeping these traces in a\ndesired bounded range. Compared with prior work, this scheme is more direct and\nflexible, and allows much larger $\\lambda$ values for off-policy TD learning\nwith bounded traces. As to its soundness, using Markov chain theory, we prove\nthe ergodicity of the joint state-trace process under nonrestrictive\nconditions, and we show that associated with our scheme is a generalized\nBellman equation (for the policy to be evaluated) that depends on both the\nevolution of $\\lambda$ and the unique invariant probability measure of the\nstate-trace process. These results not only lead immediately to a\ncharacterization of the convergence behavior of least-squares based\nimplementation of our scheme, but also prepare the ground for further analysis\nof gradient-based implementations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 16:01:18 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 20:27:40 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Yu", "Huizhen", ""], ["Mahmood", "A. Rupam", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1704.04470", "submitter": "L. Elisa Celis", "authors": "L. Elisa Celis and Farnood Salehi", "title": "Lean From Thy Neighbor: Stochastic & Adversarial Bandits in a Network", "comments": "This article was first circulated in January 2015 and presented at\n  ISMP 2015 under the title \"Bandit in a Network\"\n  (https://informs.emeetingsonline.com/emeetings/formbuilder/clustersessiondtl.asp?csnno=22329&mmnno=264&ppnno=85856)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An individual's decisions are often guided by those of his or her peers,\ni.e., neighbors in a social network. Presumably, being privy to the experiences\nof others aids in learning and decision making, but how much advantage does an\nindividual gain by observing her neighbors? Such problems make appearances in\nsociology and economics and, in this paper, we present a novel model to capture\nsuch decision-making processes and appeal to the classical multi-armed bandit\nframework to analyze it. Each individual, in addition to her own actions, can\nobserve the actions and rewards obtained by her neighbors, and can use all of\nthis information in order to minimize her own regret. We provide algorithms for\nthis setting, both for stochastic and adversarial bandits, and show that their\nregret smoothly interpolates between the regret in the classical bandit setting\nand that of the full-information setting as a function of the neighbors'\nexploration. In the stochastic setting the additional information must simply\nbe incorporated into the usual estimation of the rewards, while in the\nadversarial setting this is attained by constructing a new unbiased estimator\nfor the rewards and appropriately bounding the amount of additional information\nprovided by the neighbors. These algorithms are optimal up to log factors;\ndespite the fact that the agents act independently and selfishly, this implies\nthat it is an approximate Nash equilibria for all agents to use our algorithms.\nFurther, we show via empirical simulations that our algorithms, often\nsignificantly, outperform existing algorithms that one could apply to this\nsetting.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 16:24:58 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Celis", "L. Elisa", ""], ["Salehi", "Farnood", ""]]}, {"id": "1704.04522", "submitter": "Girish Chowdhary", "authors": "Hossein Mohamadipanah, Mahdi Heydari, Girish Chowdhary", "title": "Hierarchic Kernel Recursive Least-Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new kernel-based algorithm for modeling evenly distributed\nmultidimensional datasets that does not rely on input space sparsification. The\npresented method reorganizes the typical single-layer kernel-based model into a\ndeep hierarchical structure, such that the weights of a kernel model over each\ndimension are modeled over its adjacent dimension. We show that modeling\nweights in the suggested structure leads to significant computational speedup\nand improved modeling accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 19:43:47 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 02:41:38 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Mohamadipanah", "Hossein", ""], ["Heydari", "Mahdi", ""], ["Chowdhary", "Girish", ""]]}, {"id": "1704.04548", "submitter": "Max Simchowitz", "authors": "Max Simchowitz, Ahmed El Alaoui, Benjamin Recht", "title": "On the Gap Between Strict-Saddles and True Convexity: An Omega(log d)\n  Lower Bound for Eigenvector Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.CO math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a \\emph{query complexity} lower bound on rank-one principal\ncomponent analysis (PCA). We consider an oracle model where, given a symmetric\nmatrix $M \\in \\mathbb{R}^{d \\times d}$, an algorithm is allowed to make $T$\n\\emph{exact} queries of the form $w^{(i)} = Mv^{(i)}$ for $i \\in\n\\{1,\\dots,T\\}$, where $v^{(i)}$ is drawn from a distribution which depends\narbitrarily on the past queries and measurements $\\{v^{(j)},w^{(j)}\\}_{1 \\le j\n\\le i-1}$. We show that for a small constant $\\epsilon$, any adaptive,\nrandomized algorithm which can find a unit vector $\\widehat{v}$ for which\n$\\widehat{v}^{\\top}M\\widehat{v} \\ge (1-\\epsilon)\\|M\\|$, with even small\nprobability, must make $T = \\Omega(\\log d)$ queries. In addition to settling a\nwidely-held folk conjecture, this bound demonstrates a fundamental gap between\nconvex optimization and \"strict-saddle\" non-convex optimization of which PCA is\na canonical example: in the former, first-order methods can have dimension-free\niteration complexity, whereas in PCA, the iteration complexity of\ngradient-based methods must necessarily grow with the dimension. Our argument\nproceeds via a reduction to estimating the rank-one spike in a deformed Wigner\nmodel. We establish lower bounds for this model by developing a \"truncated\"\nanalogue of the $\\chi^2$ Bayes-risk lower bound of Chen et al.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 21:56:11 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Simchowitz", "Max", ""], ["Alaoui", "Ahmed El", ""], ["Recht", "Benjamin", ""]]}, {"id": "1704.04567", "submitter": "Jie Zhong", "authors": "Jie Zhong, Yijun Huang, Ji Liu", "title": "Asynchronous Parallel Empirical Variance Guided Algorithms for the\n  Thresholding Bandit Problem", "comments": "added lower bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the multi-armed thresholding bandit problem --\nidentifying all arms whose expected rewards are above a predefined threshold\nvia as few pulls (or rounds) as possible -- proposed by Locatelli et al. [2016]\nrecently. Although the proposed algorithm in Locatelli et al. [2016] achieves\nthe optimal round complexity in a certain sense, there still remain unsolved\nissues. This paper proposes an asynchronous parallel thresholding algorithm and\nits parameter-free version to improve the efficiency and the applicability. On\none hand, the proposed two algorithms use the empirical variance to guide the\npull decision at each round, and significantly improve the round complexity of\nthe \"optimal\" algorithm when all arms have bounded high order moments. The\nproposed algorithms can be proven to be optimal. On the other hand, most bandit\nalgorithms assume that the reward can be observed immediately after the pull or\nthe next decision would not be made before all rewards are observed. Our\nproposed asynchronous parallel algorithms allow making the choice of the next\npull with unobserved rewards from earlier pulls, which avoids such an\nunrealistic assumption and significantly improves the identification process.\nOur theoretical analysis justifies the effectiveness and the efficiency of\nproposed asynchronous parallel algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 02:42:30 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 00:14:13 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zhong", "Jie", ""], ["Huang", "Yijun", ""], ["Liu", "Ji", ""]]}, {"id": "1704.04587", "submitter": "Markus Haltmeier", "authors": "Stephan Antholzer, Markus Haltmeier, and Johannes Schwab", "title": "Deep Learning for Photoacoustic Tomography from Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of fast and accurate image reconstruction algorithms is a\ncentral aspect of computed tomography. In this paper, we investigate this issue\nfor the sparse data problem in photoacoustic tomography (PAT). We develop a\ndirect and highly efficient reconstruction algorithm based on deep learning. In\nour approach image reconstruction is performed with a deep convolutional neural\nnetwork (CNN), whose weights are adjusted prior to the actual image\nreconstruction based on a set of training data. The proposed reconstruction\napproach can be interpreted as a network that uses the PAT filtered\nbackprojection algorithm for the first layer, followed by the U-net\narchitecture for the remaining layers. Actual image reconstruction with deep\nlearning consists in one evaluation of the trained CNN, which does not require\ntime consuming solution of the forward and adjoint problems. At the same time,\nour numerical results demonstrate that the proposed deep learning approach\nreconstructs images with a quality comparable to state of the art iterative\napproaches for PAT from sparse data.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 05:33:32 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 06:22:48 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 13:45:40 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Antholzer", "Stephan", ""], ["Haltmeier", "Markus", ""], ["Schwab", "Johannes", ""]]}, {"id": "1704.04683", "submitter": "Guokun Lai", "authors": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RACE, a new dataset for benchmark evaluation of methods in the\nreading comprehension task. Collected from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, RACE consists\nof near 28,000 passages and near 100,000 questions generated by human experts\n(English instructors), and covers a variety of topics which are carefully\ndesigned for evaluating the students' ability in understanding and reasoning.\nIn particular, the proportion of questions that requires reasoning is much\nlarger in RACE than that in other benchmark datasets for reading comprehension,\nand there is a significant gap between the performance of the state-of-the-art\nmodels (43%) and the ceiling human performance (95%). We hope this new dataset\ncan serve as a valuable resource for research and evaluation in machine\ncomprehension. The dataset is freely available at\nhttp://www.cs.cmu.edu/~glai1/data/race/ and the code is available at\nhttps://github.com/qizhex/RACE_AR_baselines.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 19:31:41 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 15:47:40 GMT"}, {"version": "v3", "created": "Sat, 10 Jun 2017 03:21:55 GMT"}, {"version": "v4", "created": "Sat, 15 Jul 2017 18:48:57 GMT"}, {"version": "v5", "created": "Tue, 5 Dec 2017 19:36:03 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Lai", "Guokun", ""], ["Xie", "Qizhe", ""], ["Liu", "Hanxiao", ""], ["Yang", "Yiming", ""], ["Hovy", "Eduard", ""]]}, {"id": "1704.04688", "submitter": "Giles Hooker", "authors": "Giles Hooker and Cliff Hooker", "title": "Machine Learning and the Future of Realism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The preceding three decades have seen the emergence, rise, and proliferation\nof machine learning (ML). From half-recognised beginnings in perceptrons,\nneural nets, and decision trees, algorithms that extract correlations (that is,\npatterns) from a set of data points have broken free from their origin in\ncomputational cognition to embrace all forms of problem solving, from voice\nrecognition to medical diagnosis to automated scientific research and\ndriverless cars, and it is now widely opined that the real industrial\nrevolution lies less in mobile phone and similar than in the maturation and\nuniversal application of ML. Among the consequences just might be the triumph\nof anti-realism over realism.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 20:49:09 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Hooker", "Giles", ""], ["Hooker", "Cliff", ""]]}, {"id": "1704.04718", "submitter": "Xu Youjun Xu Youjun", "authors": "Youjun Xu, Jianfeng Pei, Luhua Lai", "title": "Deep Learning Based Regression and Multi-class Models for Acute Oral\n  Toxicity Prediction with Automatic Chemical Feature Extraction", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For quantitative structure-property relationship (QSPR) studies in\nchemoinformatics, it is important to get interpretable relationship between\nchemical properties and chemical features. However, the predictive power and\ninterpretability of QSPR models are usually two different objectives that are\ndifficult to achieve simultaneously. A deep learning architecture using\nmolecular graph encoding convolutional neural networks (MGE-CNN) provided a\nuniversal strategy to construct interpretable QSPR models with high predictive\npower. Instead of using application-specific preset molecular descriptors or\nfingerprints, the models can be resolved using raw and pertinent features\nwithout manual intervention or selection. In this study, we developed acute\noral toxicity (AOT) models of compounds using the MGE-CNN architecture as a\ncase study. Three types of high-level predictive models: regression model\n(deepAOT-R), multi-classification model (deepAOT-C) and multi-task model\n(deepAOT-CR) for AOT evaluation were constructed. These models highly\noutperformed previously reported models. For the two external datasets\ncontaining 1673 (test set I) and 375 (test set II) compounds, the R2 and mean\nabsolute error (MAE) of deepAOT-R on the test set I were 0.864 and 0.195, and\nthe prediction accuracy of deepAOT-C was 95.5% and 96.3% on the test set I and\nII, respectively. The two external prediction accuracy of deepAOT-CR is 95.0%\nand 94.1%, while the R2 and MAE are 0.861 and 0.204 for test set I,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 04:17:32 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 02:10:10 GMT"}, {"version": "v3", "created": "Thu, 4 May 2017 09:52:38 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Xu", "Youjun", ""], ["Pei", "Jianfeng", ""], ["Lai", "Luhua", ""]]}, {"id": "1704.04760", "submitter": "David Patterson David Patterson", "authors": "Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav\n  Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers,\n  Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell,\n  Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami,\n  Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug\n  Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek\n  Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve\n  Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle\n  Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran\n  Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas\n  Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt\n  Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew\n  Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory\n  Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter,\n  Walter Wang, Eric Wilcox, and Doe Hyun Yoon", "title": "In-Datacenter Performance Analysis of a Tensor Processing Unit", "comments": "17 pages, 11 figures, 8 tables. To appear at the 44th International\n  Symposium on Computer Architecture (ISCA), Toronto, Canada, June 24-28, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many architects believe that major improvements in cost-energy-performance\nmust now come from domain-specific hardware. This paper evaluates a custom\nASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since\n2015 that accelerates the inference phase of neural networks (NN). The heart of\nthe TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak\nthroughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed\non-chip memory. The TPU's deterministic execution model is a better match to\nthe 99th-percentile response-time requirement of our NN applications than are\nthe time-varying optimizations of CPUs and GPUs (caches, out-of-order\nexecution, multithreading, multiprocessing, prefetching, ...) that help average\nthroughput more than guaranteed latency. The lack of such features helps\nexplain why, despite having myriad MACs and a big memory, the TPU is relatively\nsmall and low power. We compare the TPU to a server-class Intel Haswell CPU and\nan Nvidia K80 GPU, which are contemporaries deployed in the same datacenters.\nOur workload, written in the high-level TensorFlow framework, uses production\nNN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters'\nNN inference demand. Despite low utilization for some applications, the TPU is\non average about 15X - 30X faster than its contemporary GPU or CPU, with\nTOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the\nTPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and\n200X the CPU.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 12:07:54 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Jouppi", "Norman P.", ""], ["Young", "Cliff", ""], ["Patil", "Nishant", ""], ["Patterson", "David", ""], ["Agrawal", "Gaurav", ""], ["Bajwa", "Raminder", ""], ["Bates", "Sarah", ""], ["Bhatia", "Suresh", ""], ["Boden", "Nan", ""], ["Borchers", "Al", ""], ["Boyle", "Rick", ""], ["Cantin", "Pierre-luc", ""], ["Chao", "Clifford", ""], ["Clark", "Chris", ""], ["Coriell", "Jeremy", ""], ["Daley", "Mike", ""], ["Dau", "Matt", ""], ["Dean", "Jeffrey", ""], ["Gelb", "Ben", ""], ["Ghaemmaghami", "Tara Vazir", ""], ["Gottipati", "Rajendra", ""], ["Gulland", "William", ""], ["Hagmann", "Robert", ""], ["Ho", "C. Richard", ""], ["Hogberg", "Doug", ""], ["Hu", "John", ""], ["Hundt", "Robert", ""], ["Hurt", "Dan", ""], ["Ibarz", "Julian", ""], ["Jaffey", "Aaron", ""], ["Jaworski", "Alek", ""], ["Kaplan", "Alexander", ""], ["Khaitan", "Harshit", ""], ["Koch", "Andy", ""], ["Kumar", "Naveen", ""], ["Lacy", "Steve", ""], ["Laudon", "James", ""], ["Law", "James", ""], ["Le", "Diemthu", ""], ["Leary", "Chris", ""], ["Liu", "Zhuyuan", ""], ["Lucke", "Kyle", ""], ["Lundin", "Alan", ""], ["MacKean", "Gordon", ""], ["Maggiore", "Adriana", ""], ["Mahony", "Maire", ""], ["Miller", "Kieran", ""], ["Nagarajan", "Rahul", ""], ["Narayanaswami", "Ravi", ""], ["Ni", "Ray", ""], ["Nix", "Kathy", ""], ["Norrie", "Thomas", ""], ["Omernick", "Mark", ""], ["Penukonda", "Narayana", ""], ["Phelps", "Andy", ""], ["Ross", "Jonathan", ""], ["Ross", "Matt", ""], ["Salek", "Amir", ""], ["Samadiani", "Emad", ""], ["Severn", "Chris", ""], ["Sizikov", "Gregory", ""], ["Snelham", "Matthew", ""], ["Souter", "Jed", ""], ["Steinberg", "Dan", ""], ["Swing", "Andy", ""], ["Tan", "Mercedes", ""], ["Thorson", "Gregory", ""], ["Tian", "Bo", ""], ["Toma", "Horia", ""], ["Tuttle", "Erick", ""], ["Vasudevan", "Vijay", ""], ["Walter", "Richard", ""], ["Wang", "Walter", ""], ["Wilcox", "Eric", ""], ["Yoon", "Doe Hyun", ""]]}, {"id": "1704.04799", "submitter": "Alexander Jung", "authors": "Saeed Basirian and Alexander Jung", "title": "Random Walk Sampling for Big Data over Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown recently that graph signals with small total variation can\nbe accurately recovered from only few samples if the sampling set satisfies a\ncertain condition, referred to as the network nullspace property. Based on this\nrecovery condition, we propose a sampling strategy for smooth graph signals\nbased on random walks. Numerical experiments demonstrate the effectiveness of\nthis approach for graph signals obtained from a synthetic random graph model as\nwell as a real-world dataset.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 17:43:38 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Basirian", "Saeed", ""], ["Jung", "Alexander", ""]]}, {"id": "1704.04810", "submitter": "Nariman Farsad Dr.", "authors": "Nariman Farsad, David Pan, Andrea Goldsmith", "title": "A Novel Experimental Platform for In-Vessel Multi-Chemical Molecular\n  Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new multi-chemical experimental platform for molecular\ncommunication where the transmitter can release different chemicals. This\nplatform is designed to be inexpensive and accessible, and it can be expanded\nto simulate different environments including the cardiovascular system and\ncomplex network of pipes in industrial complexes and city infrastructures. To\ndemonstrate the capabilities of the platform, we implement a time-slotted\nbinary communication system where a bit-0 is represented by an acid pulse, a\nbit-1 by a base pulse, and information is carried via pH signals. The channel\nmodel for this system, which is nonlinear and has long memories, is unknown.\nTherefore, we devise novel detection algorithms that use techniques from\nmachine learning and deep learning to train a maximum-likelihood detector.\nUsing these algorithms the bit error rate improves by an order of magnitude\nrelative to the approach used in previous works. Moreover, our system achieves\na data rate that is an order of magnitude higher than any of the previous\nmolecular communication platforms.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 19:24:30 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Farsad", "Nariman", ""], ["Pan", "David", ""], ["Goldsmith", "Andrea", ""]]}, {"id": "1704.04853", "submitter": "Llewyn Salt Mr.", "authors": "Llewyn Salt, David Howard, Giacomo Indiveri, Yulia Sandamirskaya", "title": "Differential Evolution and Bayesian Optimisation for Hyper-Parameter\n  Selection in Mixed-Signal Neuromorphic Circuits Applied to UAV Obstacle\n  Avoidance", "comments": "Submitted to TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2941506", "report-no": null, "categories": "cs.NE cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lobula Giant Movement Detector (LGMD) is a an identified neuron of the\nlocust that detects looming objects and triggers its escape responses.\nUnderstanding the neural principles and networks that lead to these fast and\nrobust responses can lead to the design of efficient facilitate obstacle\navoidance strategies in robotic applications. Here we present a neuromorphic\nspiking neural network model of the LGMD driven by the output of a neuromorphic\nDynamic Vision Sensor (DVS), which has been optimised to produce robust and\nreliable responses in the face of the constraints and variability of its mixed\nsignal analogue-digital circuits. As this LGMD model has many parameters, we\nuse the Differential Evolution (DE) algorithm to optimise its parameter space.\nWe also investigate the use of Self-Adaptive Differential Evolution (SADE)\nwhich has been shown to ameliorate the difficulties of finding appropriate\ninput parameters for DE. We explore the use of two biological mechanisms:\nsynaptic plasticity and membrane adaptivity in the LGMD. We apply DE and SADE\nto find parameters best suited for an obstacle avoidance system on an unmanned\naerial vehicle (UAV), and show how it outperforms state-of-the-art Bayesian\noptimisation used for comparison.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 03:14:18 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 14:06:32 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 11:31:55 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Salt", "Llewyn", ""], ["Howard", "David", ""], ["Indiveri", "Giacomo", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "1704.04865", "submitter": "Felix Juefei-Xu", "authors": "Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides", "title": "Gang of GANs: Generative Adversarial Networks with Maximum Margin\n  Ranking", "comments": "16 pages. 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional generative adversarial networks (GAN) and many of its variants\nare trained by minimizing the KL or JS-divergence loss that measures how close\nthe generated data distribution is from the true data distribution. A recent\nadvance called the WGAN based on Wasserstein distance can improve on the KL and\nJS-divergence based GANs, and alleviate the gradient vanishing, instability,\nand mode collapse issues that are common in the GAN training. In this work, we\naim at improving on the WGAN by first generalizing its discriminator loss to a\nmargin-based one, which leads to a better discriminator, and in turn a better\ngenerator, and then carrying out a progressive training paradigm involving\nmultiple GANs to contribute to the maximum margin ranking loss so that the GAN\nat later stages will improve upon early stages. We call this method Gang of\nGANs (GoGAN). We have shown theoretically that the proposed GoGAN can reduce\nthe gap between the true data distribution and the generated data distribution\nby at least half in an optimally trained WGAN. We have also proposed a new way\nof measuring GAN quality which is based on image completion tasks. We have\nevaluated our method on four visual datasets: CelebA, LSUN Bedroom, CIFAR-10,\nand 50K-SSFF, and have seen both visual and quantitative improvement over\nbaseline WGAN.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 04:42:56 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Juefei-Xu", "Felix", ""], ["Boddeti", "Vishnu Naresh", ""], ["Savvides", "Marios", ""]]}, {"id": "1704.04866", "submitter": "Feiyun Zhu", "authors": "Feiyun Zhu and Peng Liao", "title": "Effective Warm Start for the Online Actor-Critic Reinforcement Learning\n  based mHealth Intervention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reinforcement learning (RL) is increasingly popular for the\npersonalized mobile health (mHealth) intervention. It is able to personalize\nthe type and dose of interventions according to user's ongoing statuses and\nchanging needs. However, at the beginning of online learning, there are usually\ntoo few samples to support the RL updating, which leads to poor performances. A\ndelay in good performance of the online learning algorithms can be especially\ndetrimental in the mHealth, where users tend to quickly disengage with the\nmHealth app. To address this problem, we propose a new online RL methodology\nthat focuses on an effective warm start. The main idea is to make full use of\nthe data accumulated and the decision rule achieved in a former study. As a\nresult, we can greatly enrich the data size at the beginning of online learning\nin our method. Such case accelerates the online learning process for new users\nto achieve good performances not only at the beginning of online learning but\nalso through the whole online learning process. Besides, we use the decision\nrules achieved in a previous study to initialize the parameter in our online RL\nmodel for new users. It provides a good initialization for the proposed online\nRL algorithm. Experiment results show that promising improvements have been\nachieved by our method compared with the state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 04:43:05 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 04:51:43 GMT"}, {"version": "v3", "created": "Sun, 21 May 2017 21:00:12 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Zhu", "Feiyun", ""], ["Liao", "Peng", ""]]}, {"id": "1704.04932", "submitter": "Pratik Chaudhari", "authors": "Pratik Chaudhari, Adam Oberman, Stanley Osher, Stefano Soatto,\n  Guillaume Carlier", "title": "Deep Relaxation: partial differential equations for optimizing deep\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we establish a connection between non-convex optimization\nmethods for training deep neural networks and nonlinear partial differential\nequations (PDEs). Relaxation techniques arising in statistical physics which\nhave already been used successfully in this context are reinterpreted as\nsolutions of a viscous Hamilton-Jacobi PDE. Using a stochastic control\ninterpretation allows we prove that the modified algorithm performs better in\nexpectation that stochastic gradient descent. Well-known PDE regularity results\nallow us to analyze the geometry of the relaxed energy landscape, confirming\nempirical evidence. The PDE is derived from a stochastic homogenization\nproblem, which arises in the implementation of the algorithm. The algorithms\nscale well in practice and can effectively tackle the high dimensionality of\nmodern neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 11:21:32 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 21:26:45 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Chaudhari", "Pratik", ""], ["Oberman", "Adam", ""], ["Osher", "Stanley", ""], ["Soatto", "Stefano", ""], ["Carlier", "Guillaume", ""]]}, {"id": "1704.04959", "submitter": "Aahitagni Mukherjee", "authors": "Abhishek Sinha, Mausoom Sarkar, Aahitagni Mukherjee, Balaji\n  Krishnamurthy", "title": "Introspection: Accelerating Neural Network Training By Learning Weight\n  Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks are function approximators that have achieved\nstate-of-the-art accuracy in numerous machine learning tasks. In spite of their\ngreat success in terms of accuracy, their large training time makes it\ndifficult to use them for various tasks. In this paper, we explore the idea of\nlearning weight evolution pattern from a simple network for accelerating\ntraining of novel neural networks. We use a neural network to learn the\ntraining pattern from MNIST classification and utilize it to accelerate\ntraining of neural networks used for CIFAR-10 and ImageNet classification. Our\nmethod has a low memory footprint and is computationally efficient. This method\ncan also be used with other optimizers to give faster convergence. The results\nindicate a general trend in the weight evolution during training of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 13:23:36 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Sinha", "Abhishek", ""], ["Sarkar", "Mausoom", ""], ["Mukherjee", "Aahitagni", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1704.04960", "submitter": "Zhitao Gong", "authors": "Zhitao Gong, Wenlu Wang, Wei-Shinn Ku", "title": "Adversarial and Clean Data Are Not Twins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adversarial attack has cast a shadow on the massive success of deep neural\nnetworks. Despite being almost visually identical to the clean data, the\nadversarial images can fool deep neural networks into wrong predictions with\nvery high confidence. In this paper, however, we show that we can build a\nsimple binary classifier separating the adversarial apart from the clean data\nwith accuracy over 99%. We also empirically show that the binary classifier is\nrobust to a second-round adversarial attack. In other words, it is difficult to\ndisguise adversarial samples to bypass the binary classifier. Further more, we\nempirically investigate the generalization limitation which lingers on all\ncurrent defensive methods, including the binary classifier approach. And we\nhypothesize that this is the result of intrinsic property of adversarial\ncrafting algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 13:25:17 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Gong", "Zhitao", ""], ["Wang", "Wenlu", ""], ["Ku", "Wei-Shinn", ""]]}, {"id": "1704.04962", "submitter": "Thomas Brouwer", "authors": "Thomas Brouwer, Pietro Li\\'o", "title": "Bayesian Hybrid Matrix Factorisation for Data Integration", "comments": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2017)", "journal-ref": "PMLR 54:557-566, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel Bayesian hybrid matrix factorisation model (HMF) for\ndata integration, based on combining multiple matrix factorisation methods,\nthat can be used for in- and out-of-matrix prediction of missing values. The\nmodel is very general and can be used to integrate many datasets across\ndifferent entity types, including repeated experiments, similarity matrices,\nand very sparse datasets. We apply our method on two biological applications,\nand extensively compare it to state-of-the-art machine learning and matrix\nfactorisation models. For in-matrix predictions on drug sensitivity datasets we\nobtain consistently better performances than existing methods. This is\nespecially the case when we increase the sparsity of the datasets. Furthermore,\nwe perform out-of-matrix predictions on methylation and gene expression\ndatasets, and obtain the best results on two of the three datasets, especially\nwhen the predictivity of datasets is high.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 13:39:29 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Brouwer", "Thomas", ""], ["Li\u00f3", "Pietro", ""]]}, {"id": "1704.04966", "submitter": "Fanhua Shang", "authors": "Fanhua Shang", "title": "Larger is Better: The Effect of Learning Rates Enjoyed by Stochastic\n  Optimization with Progressive Variance Reduction", "comments": "36 pages, 10 figures. The simple variant of SVRG is much better than\n  the best-known stochastic method, Katyusha", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple variant of the original stochastic\nvariance reduction gradient (SVRG), where hereafter we refer to as the variance\nreduced stochastic gradient descent (VR-SGD). Different from the choices of the\nsnapshot point and starting point in SVRG and its proximal variant, Prox-SVRG,\nthe two vectors of each epoch in VR-SGD are set to the average and last iterate\nof the previous epoch, respectively. This setting allows us to use much larger\nlearning rates or step sizes than SVRG, e.g., 3/(7L) for VR-SGD vs 1/(10L) for\nSVRG, and also makes our convergence analysis more challenging. In fact, a\nlarger learning rate enjoyed by VR-SGD means that the variance of its\nstochastic gradient estimator asymptotically approaches zero more rapidly.\nUnlike common stochastic methods such as SVRG and proximal stochastic methods\nsuch as Prox-SVRG, we design two different update rules for smooth and\nnon-smooth objective functions, respectively. In other words, VR-SGD can tackle\nnon-smooth and/or non-strongly convex problems directly without using any\nreduction techniques such as quadratic regularizers. Moreover, we analyze the\nconvergence properties of VR-SGD for strongly convex problems, which show that\nVR-SGD attains a linear convergence rate. We also provide the convergence\nguarantees of VR-SGD for non-strongly convex problems. Experimental results\nshow that the performance of VR-SGD is significantly better than its\ncounterparts, SVRG and Prox-SVRG, and it is also much better than the best\nknown stochastic method, Katyusha.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 13:50:43 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Shang", "Fanhua", ""]]}, {"id": "1704.04997", "submitter": "Ardavan Saeedi", "authors": "Ardavan Saeedi, Matthew D. Hoffman, Stephen J. DiVerdi, Asma\n  Ghandeharioun, Matthew J. Johnson, Ryan P. Adams", "title": "Multimodal Prediction and Personalization of Photo Edits with Deep\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Professional-grade software applications are powerful but\ncomplicated$-$expert users can achieve impressive results, but novices often\nstruggle to complete even basic tasks. Photo editing is a prime example: after\nloading a photo, the user is confronted with an array of cryptic sliders like\n\"clarity\", \"temp\", and \"highlights\". An automatically generated suggestion\ncould help, but there is no single \"correct\" edit for a given image$-$different\nexperts may make very different aesthetic decisions when faced with the same\nimage, and a single expert may make different choices depending on the intended\nuse of the image (or on a whim). We therefore want a system that can propose\nmultiple diverse, high-quality edits while also learning from and adapting to a\nuser's aesthetic preferences. In this work, we develop a statistical model that\nmeets these objectives. Our model builds on recent advances in neural network\ngenerative modeling and scalable inference, and uses hierarchical structure to\nlearn editing patterns across many diverse users. Empirically, we find that our\nmodel outperforms other approaches on this challenging multimodal prediction\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 15:15:12 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Saeedi", "Ardavan", ""], ["Hoffman", "Matthew D.", ""], ["DiVerdi", "Stephen J.", ""], ["Ghandeharioun", "Asma", ""], ["Johnson", "Matthew J.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1704.05021", "submitter": "Alham Fikri Aji", "authors": "Alham Fikri Aji and Kenneth Heafield", "title": "Sparse Communication for Distributed Gradient Descent", "comments": "EMNLP 2017", "journal-ref": null, "doi": "10.18653/v1/D17-1045", "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make distributed stochastic gradient descent faster by exchanging sparse\nupdates instead of dense updates. Gradient updates are positively skewed as\nmost updates are near zero, so we map the 99% smallest updates (by absolute\nvalue) to zero then exchange sparse matrices. This method can be combined with\nquantization to further improve the compression. We explore different\nconfigurations and apply them to neural machine translation and MNIST image\nclassification tasks. Most configurations work on MNIST, whereas different\nconfigurations reduce convergence rate on the more complex translation task.\nOur experiments show that we can achieve up to 49% speed up on MNIST and 22% on\nNMT without damaging the final accuracy or BLEU.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 16:32:02 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 21:47:51 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Aji", "Alham Fikri", ""], ["Heafield", "Kenneth", ""]]}, {"id": "1704.05041", "submitter": "Youngmin Ha", "authors": "Youngmin Ha", "title": "Fast multi-output relevance vector regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to decrease the time complexity of multi-output relevance\nvector regression from O(VM^3) to O(V^3+M^3), where V is the number of output\ndimensions, M is the number of basis functions, and V<M. The experimental\nresults demonstrate that the proposed method is more competitive than the\nexisting method, with regard to computation time. MATLAB codes are available at\nhttp://www.mathworks.com/matlabcentral/fileexchange/49131.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 17:32:05 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Ha", "Youngmin", ""]]}, {"id": "1704.05051", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Baicen Xiao and Radha Poovendran", "title": "Google's Cloud Vision API Is Not Robust To Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google has recently introduced the Cloud Vision API for image analysis.\nAccording to the demonstration website, the API \"quickly classifies images into\nthousands of categories, detects individual objects and faces within images,\nand finds and reads printed words contained within images.\" It can be also used\nto \"detect different types of inappropriate content from adult to violent\ncontent.\"\n  In this paper, we evaluate the robustness of Google Cloud Vision API to input\nperturbation. In particular, we show that by adding sufficient noise to the\nimage, the API generates completely different outputs for the noisy image,\nwhile a human observer would perceive its original content. We show that the\nattack is consistently successful, by performing extensive experiments on\ndifferent image types, including natural images, images containing faces and\nimages with texts. For instance, using images from ImageNet dataset, we found\nthat adding an average of 14.25% impulse noise is enough to deceive the API.\nOur findings indicate the vulnerability of the API in adversarial environments.\nFor example, an adversary can bypass an image filtering system by adding noise\nto inappropriate images. We then show that when a noise filter is applied on\ninput images, the API generates mostly the same outputs for restored images as\nfor original images. This observation suggests that cloud vision API can\nreadily benefit from noise filtering, without the need for updating image\nanalysis algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 09:47:46 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 05:31:16 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Hosseini", "Hossein", ""], ["Xiao", "Baicen", ""], ["Poovendran", "Radha", ""]]}, {"id": "1704.05119", "submitter": "Sharan Narang", "authors": "Sharan Narang, Erich Elsen, Gregory Diamos, Shubho Sengupta", "title": "Exploring Sparsity in Recurrent Neural Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN) are widely used to solve a variety of\nproblems and as the quantity of data and the amount of available compute have\nincreased, so have model sizes. The number of parameters in recent\nstate-of-the-art networks makes them hard to deploy, especially on mobile\nphones and embedded devices. The challenge is due to both the size of the model\nand the time it takes to evaluate it. In order to deploy these RNNs\nefficiently, we propose a technique to reduce the parameters of a network by\npruning weights during the initial training of the network. At the end of\ntraining, the parameters of the network are sparse while accuracy is still\nclose to the original dense neural network. The network size is reduced by 8x\nand the time required to train the model remains constant. Additionally, we can\nprune a larger dense network to achieve better than baseline performance while\nstill reducing the total number of parameters significantly. Pruning RNNs\nreduces the size of the model and can also help achieve significant inference\ntime speed-up using sparse matrix multiply. Benchmarks show that using our\ntechnique model size can be reduced by 90% and speed-up is around 2x to 7x.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 20:42:05 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 22:10:47 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Narang", "Sharan", ""], ["Elsen", "Erich", ""], ["Diamos", "Gregory", ""], ["Sengupta", "Shubho", ""]]}, {"id": "1704.05120", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt", "title": "Does robustness imply tractability? A lower bound for planted clique in\n  the semi-random model", "comments": "Improved lower bound to give recovery probability tending to zero.\n  Factored out and highlighted perturbed Bernoulli argument", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a robust analog of the planted clique problem. In this analog, a\nset $S$ of vertices is chosen and all edges in $S$ are included; then, edges\nbetween $S$ and the rest of the graph are included with probability\n$\\frac{1}{2}$, while edges not touching $S$ are allowed to vary arbitrarily.\nFor this semi-random model, we show that the information-theoretic threshold\nfor recovery is $\\tilde{\\Theta}(\\sqrt{n})$, in sharp contrast to the classical\ninformation-theoretic threshold of $\\Theta(\\log(n))$. This matches the\nconjectured computational threshold for the classical planted clique problem,\nand thus raises the intriguing possibility that, once we require robustness,\nthere is no computational-statistical gap for planted clique. Our lower bound\ninvolves establishing a result regarding the KL divergence of a family of\nperturbed Bernoulli distributions, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 20:53:18 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 21:21:55 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Steinhardt", "Jacob", ""]]}, {"id": "1704.05135", "submitter": "Stanislas Lauly", "authors": "Sebastien Jean, Stanislas Lauly, Orhan Firat, Kyunghyun Cho", "title": "Does Neural Machine Translation Benefit from Larger Context?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural machine translation architecture that models the\nsurrounding text in addition to the source sentence. These models lead to\nbetter performance, both in terms of general translation quality and pronoun\nprediction, when trained on small corpora, although this improvement largely\ndisappears when trained with a larger corpus. We also discover that\nattention-based neural machine translation is well suited for pronoun\nprediction and compares favorably with other approaches that were specifically\ndesigned for this task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 21:42:19 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Jean", "Sebastien", ""], ["Lauly", "Stanislas", ""], ["Firat", "Orhan", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1704.05147", "submitter": "Daoming Lyu", "authors": "Bo Liu, Daoming Lyu, Wen Dong, Saad Biaz", "title": "O$^2$TD: (Near)-Optimal Off-Policy TD Learning", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal difference learning and Residual Gradient methods are the most\nwidely used temporal difference based learning algorithms; however, it has been\nshown that none of their objective functions is optimal w.r.t approximating the\ntrue value function $V$. Two novel algorithms are proposed to approximate the\ntrue value function $V$. This paper makes the following contributions: (1) A\nbatch algorithm that can help find the approximate optimal off-policy\nprediction of the true value function $V$. (2) A linear computational cost (per\nstep) near-optimal algorithm that can learn from a collection of off-policy\nsamples. (3) A new perspective of the emphatic temporal difference learning\nwhich bridges the gap between off-policy optimality and off-policy stability.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 23:18:48 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 22:22:52 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Liu", "Bo", ""], ["Lyu", "Daoming", ""], ["Dong", "Wen", ""], ["Biaz", "Saad", ""]]}, {"id": "1704.05155", "submitter": "Zhe Gan", "authors": "Yunchen Pu, Zhe Gan, Ricardo Henao, Chunyuan Li, Shaobo Han, Lawrence\n  Carin", "title": "VAE Learning via Stein Variational Gradient Descent", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method for learning variational autoencoders (VAEs) is developed, based\non Stein variational gradient descent. A key advantage of this approach is that\none need not make parametric assumptions about the form of the encoder\ndistribution. Performance is further enhanced by integrating the proposed\nencoder with importance sampling. Excellent performance is demonstrated across\nmultiple unsupervised and semi-supervised problems, including semi-supervised\nanalysis of the ImageNet data, demonstrating the scalability of the model to\nlarge datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 00:08:34 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 03:27:43 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 16:40:15 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Pu", "Yunchen", ""], ["Gan", "Zhe", ""], ["Henao", "Ricardo", ""], ["Li", "Chunyuan", ""], ["Han", "Shaobo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1704.05194", "submitter": "Xiaoqiang Zhu", "authors": "Kun Gai, Xiaoqiang Zhu, Han Li, Kai Liu, Zhe Wang", "title": "Learning Piece-wise Linear Models from Large Scale Data for Ad Click\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CTR prediction in real-world business is a difficult machine learning problem\nwith large scale nonlinear sparse data. In this paper, we introduce an\nindustrial strength solution with model named Large Scale Piece-wise Linear\nModel (LS-PLM). We formulate the learning problem with $L_1$ and $L_{2,1}$\nregularizers, leading to a non-convex and non-smooth optimization problem.\nThen, we propose a novel algorithm to solve it efficiently, based on\ndirectional derivatives and quasi-Newton method. In addition, we design a\ndistributed system which can run on hundreds of machines parallel and provides\nus with the industrial scalability. LS-PLM model can capture nonlinear patterns\nfrom massive sparse data, saving us from heavy feature engineering jobs. Since\n2012, LS-PLM has become the main CTR prediction model in Alibaba's online\ndisplay advertising system, serving hundreds of millions users every day.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 04:03:19 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Gai", "Kun", ""], ["Zhu", "Xiaoqiang", ""], ["Li", "Han", ""], ["Liu", "Kai", ""], ["Wang", "Zhe", ""]]}, {"id": "1704.05204", "submitter": "Shixiang Wan", "authors": "Shixiang Wan and Quan Zou", "title": "HPSLPred: An Ensemble Multi-label Classifier for Human Protein\n  Subcellular Location Prediction with Imbalanced Source", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the subcellular localization of proteins is an important and\nchallenging problem. Traditional experimental approaches are often expensive\nand time-consuming. Consequently, a growing number of research efforts employ a\nseries of machine learning approaches to predict the subcellular location of\nproteins. There are two main challenges among the state-of-the-art prediction\nmethods. First, most of the existing techniques are designed to deal with\nmulti-class rather than multi-label classification, which ignores connections\nbetween multiple labels. In reality, multiple locations of particular proteins\nimplies that there are vital and unique biological significances that deserve\nspecial focus and cannot be ignored. Second, techniques for handling imbalanced\ndata in multi-label classification problems are necessary, but never employed.\nFor solving these two issues, we have developed an ensemble multi-label\nclassifier called HPSLPred, which can be applied for multi-label classification\nwith an imbalanced protein source. For convenience, a user-friendly webserver\nhas been established at http://server.malab.cn/HPSLPred.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 05:18:25 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Wan", "Shixiang", ""], ["Zou", "Quan", ""]]}, {"id": "1704.05223", "submitter": "ByungIl Kwak", "authors": "Byung Il Kwak, JiYoung Woo and Huy Kang Kim", "title": "Know Your Master: Driver Profiling-based Anti-theft Method", "comments": "8 pages, 11 figures, Accepted for PST 2016 : 14th International\n  Conference on Privacy, Security and Trust", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many anti-theft technologies are implemented, auto-theft is still\nincreasing. Also, security vulnerabilities of cars can be used for auto-theft\nby neutralizing anti-theft system. This keyless auto-theft attack will be\nincreased as cars adopt computerized electronic devices more. To detect\nauto-theft efficiently, we propose the driver verification method that analyzes\ndriving patterns using measurements from the sensor in the vehicle. In our\nmodel, we add mechanical features of automotive parts that are excluded in\nprevious works, but can be differentiated by drivers' driving behaviors. We\ndesign the model that uses significant features through feature selection to\nreduce the time cost of feature processing and improve the detection\nperformance. Further, we enrich the feature set by deriving statistical\nfeatures such as mean, median, and standard deviation. This minimizes the\neffect of fluctuation of feature values per driver and finally generates the\nreliable model. We also analyze the effect of the size of sliding window on\nperformance to detect the time point when the detection becomes reliable and to\ninform owners the theft event as soon as possible. We apply our model with real\ndriving and show the contribution of our work to the literature of driver\nidentification.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 08:09:26 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Kwak", "Byung Il", ""], ["Woo", "JiYoung", ""], ["Kim", "Huy Kang", ""]]}, {"id": "1704.05249", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, Ilias Leontiadis, Alexandros Karatzoglou, Konstantina\n  Papagiannaki", "title": "Hot or not? Forecasting cellular network hot spots using sector\n  performance indicators", "comments": "Accepted for publication at ICDE 2017 - Industrial Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To manage and maintain large-scale cellular networks, operators need to know\nwhich sectors underperform at any given time. For this purpose, they use the\nso-called hot spot score, which is the result of a combination of multiple\nnetwork measurements and reflects the instantaneous overall performance of\nindividual sectors. While operators have a good understanding of the current\nperformance of a network and its overall trend, forecasting the performance of\neach sector over time is a challenging task, as it is affected by both regular\nand non-regular events, triggered by human behavior and hardware failures. In\nthis paper, we study the spatio-temporal patterns of the hot spot score and\nuncover its regularities. Based on our observations, we then explore the\npossibility to use recent measurements' history to predict future hot spots. To\nthis end, we consider tree-based machine learning models, and study their\nperformance as a function of time, amount of past data, and prediction horizon.\nOur results indicate that, compared to the best baseline, tree-based models can\ndeliver up to 14% better forecasts for regular hot spots and 153% better\nforecasts for non-regular hot spots. The latter brings strong evidence that,\nfor moderate horizons, forecasts can be made even for sectors exhibiting\nisolated, non-regular behavior. Overall, our work provides insight into the\ndynamics of cellular sectors and their predictability. It also paves the way\nfor more proactive network operations with greater forecasting horizons.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 09:34:48 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Leontiadis", "Ilias", ""], ["Karatzoglou", "Alexandros", ""], ["Papagiannaki", "Konstantina", ""]]}, {"id": "1704.05271", "submitter": "Yannis Papanikolaou", "authors": "Yannis Papanikolaou, Grigorios Tsoumakas, Manos Laliotis, Nikos\n  Markantonatos and Ioannis Vlahavas", "title": "Large-Scale Online Semantic Indexing of Biomedical Articles via an\n  Ensemble of Multi-Label Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: In this paper we present the approaches and methods employed in\norder to deal with a large scale multi-label semantic indexing task of\nbiomedical papers. This work was mainly implemented within the context of the\nBioASQ challenge of 2014. Methods: The main contribution of this work is a\nmulti-label ensemble method that incorporates a McNemar statistical\nsignificance test in order to validate the combination of the constituent\nmachine learning algorithms. Some secondary contributions include a study on\nthe temporal aspects of the BioASQ corpus (observations apply also to the\nBioASQ's super-set, the PubMed articles collection) and the proper adaptation\nof the algorithms used to deal with this challenging classification task.\nResults: The ensemble method we developed is compared to other approaches in\nexperimental scenarios with subsets of the BioASQ corpus giving positive\nresults. During the BioASQ 2014 challenge we obtained the first place during\nthe first batch and the third in the two following batches. Our success in the\nBioASQ challenge proved that a fully automated machine-learning approach, which\ndoes not implement any heuristics and rule-based approaches, can be highly\ncompetitive and outperform other approaches in similar challenging contexts.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 11:17:00 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Papanikolaou", "Yannis", ""], ["Tsoumakas", "Grigorios", ""], ["Laliotis", "Manos", ""], ["Markantonatos", "Nikos", ""], ["Vlahavas", "Ioannis", ""]]}, {"id": "1704.05310", "submitter": "Armand Joulin", "authors": "Piotr Bojanowski, Armand Joulin", "title": "Unsupervised Learning by Predicting Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks provide visual features that perform remarkably\nwell in many computer vision applications. However, training these networks\nrequires significant amounts of supervision. This paper introduces a generic\nframework to train deep networks, end-to-end, with no supervision. We propose\nto fix a set of target representations, called Noise As Targets (NAT), and to\nconstrain the deep features to align to them. This domain agnostic approach\navoids the standard unsupervised learning issues of trivial solutions and\ncollapsing of features. Thanks to a stochastic batch reassignment strategy and\na separable square loss function, it scales to millions of images. The proposed\napproach produces representations that perform on par with state-of-the-art\nunsupervised methods on ImageNet and Pascal VOC.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 12:51:47 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""]]}, {"id": "1704.05396", "submitter": "Fran\\c{c}ois Leduc-Primeau", "authors": "Jean-Charles Vialatte and Fran\\c{c}ois Leduc-Primeau", "title": "A Study of Deep Learning Robustness Against Computation Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many types of integrated circuits, accepting larger failure rates in\ncomputations can be used to improve energy efficiency. We study the performance\nof faulty implementations of certain deep neural networks based on pessimistic\nand optimistic models of the effect of hardware faults. After identifying the\nimpact of hyperparameters such as the number of layers on robustness, we study\nthe ability of the network to compensate for computational failures through an\nincrease of the network size. We show that some networks can achieve equivalent\nperformance under faulty implementations, and quantify the required increase in\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 15:33:10 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Vialatte", "Jean-Charles", ""], ["Leduc-Primeau", "Fran\u00e7ois", ""]]}, {"id": "1704.05409", "submitter": "Giorgio Roffo", "authors": "Giorgio Roffo and Simone Melzi", "title": "Ranking to Learn: Feature Ranking and Selection via Eigenvector\n  Centrality", "comments": "Preprint version - Lecture Notes in Computer Science - Springer 2017", "journal-ref": "New Frontiers in Mining Complex Patterns, Fifth International\n  workshop, nfMCP2016. Lecture Notes in Computer Science - Springer", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era where accumulating data is easy and storing it inexpensive, feature\nselection plays a central role in helping to reduce the high-dimensionality of\nhuge amounts of otherwise meaningless data. In this paper, we propose a\ngraph-based method for feature selection that ranks features by identifying the\nmost important ones into arbitrary set of cues. Mapping the problem on an\naffinity graph-where features are the nodes-the solution is given by assessing\nthe importance of nodes through some indicators of centrality, in particular,\nthe Eigen-vector Centrality (EC). The gist of EC is to estimate the importance\nof a feature as a function of the importance of its neighbors. Ranking central\nnodes individuates candidate features, which turn out to be effective from a\nclassification point of view, as proved by a thoroughly experimental section.\nOur approach has been tested on 7 diverse datasets from recent literature\n(e.g., biological data and object recognition, among others), and compared\nagainst filter, embedded and wrappers methods. The results are remarkable in\nterms of accuracy, stability and low execution time.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 16:21:05 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Roffo", "Giorgio", ""], ["Melzi", "Simone", ""]]}, {"id": "1704.05420", "submitter": "Cem Subakan", "authors": "Y. Cem Subakan, Paris Smaragdis", "title": "Diagonal RNNs in Symbolic Music Modeling", "comments": "Submitted to Waspaa 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new Recurrent Neural Network (RNN) architecture.\nThe novelty is simple: We use diagonal recurrent matrices instead of full. This\nresults in better test likelihood and faster convergence compared to regular\nfull RNNs in most of our experiments. We show the benefits of using diagonal\nrecurrent matrices with popularly used LSTM and GRU architectures as well as\nwith the vanilla RNN architecture, on four standard symbolic music datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 16:47:38 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 23:36:18 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Subakan", "Y. Cem", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1704.05495", "submitter": "Jean Harb", "authors": "Jean Harb and Doina Precup", "title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "comments": "8 pages, 3 figures, NIPS 2016 Deep Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eligibility traces in reinforcement learning are used as a bias-variance\ntrade-off and can often speed up training time by propagating knowledge back\nover time-steps in a single update. We investigate the use of eligibility\ntraces in combination with recurrent networks in the Atari domain. We\nillustrate the benefits of both recurrent nets and eligibility traces in some\nAtari games, and highlight also the importance of the optimization used in the\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 18:46:12 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Harb", "Jean", ""], ["Precup", "Doina", ""]]}, {"id": "1704.05566", "submitter": "Jeremy Morton", "authors": "Jeremy Morton and Mykel J. Kochenderfer", "title": "Simultaneous Policy Learning and Latent State Inference for Imitating\n  Driver Behavior", "comments": "7 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a method for learning driver models that account for\nvariables that cannot be observed directly. When trained on a synthetic\ndataset, our models are able to learn encodings for vehicle trajectories that\ndistinguish between four distinct classes of driver behavior. Such encodings\nare learned without any knowledge of the number of driver classes or any\nobjective that directly requires the models to learn encodings for each class.\nWe show that driving policies trained with knowledge of latent variables are\nmore effective than baseline methods at imitating the driver behavior that they\nare trained to replicate. Furthermore, we demonstrate that the actions chosen\nby our policy are heavily influenced by the latent variable settings that are\nprovided to them.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 00:23:59 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Morton", "Jeremy", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1704.05579", "submitter": "Mikhail Khodak", "authors": "Mikhail Khodak, Nikunj Saunshi and Kiran Vodrahalli", "title": "A Large Self-Annotated Corpus for Sarcasm", "comments": "6 pages, 4 Figures. To Appear in LREC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Self-Annotated Reddit Corpus (SARC), a large corpus for\nsarcasm research and for training and evaluating systems for sarcasm detection.\nThe corpus has 1.3 million sarcastic statements -- 10 times more than any\nprevious dataset -- and many times more instances of non-sarcastic statements,\nallowing for learning in both balanced and unbalanced label regimes. Each\nstatement is furthermore self-annotated -- sarcasm is labeled by the author,\nnot an independent annotator -- and provided with user, topic, and conversation\ncontext. We evaluate the corpus for accuracy, construct benchmarks for sarcasm\ndetection, and evaluate baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 02:01:39 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 01:25:08 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 03:09:01 GMT"}, {"version": "v4", "created": "Thu, 22 Mar 2018 22:23:10 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Khodak", "Mikhail", ""], ["Saunshi", "Nikunj", ""], ["Vodrahalli", "Kiran", ""]]}, {"id": "1704.05588", "submitter": "Dhiraj Gandhi", "authors": "Dhiraj Gandhi, Lerrel Pinto, Abhinav Gupta", "title": "Learning to Fly by Crashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid\nobstacles? One approach is to use a small dataset collected by human experts:\nhowever, high capacity learning algorithms tend to overfit when trained with\nlittle data. An alternative is to use simulation. But the gap between\nsimulation and real world remains large especially for perception problems. The\nreason most research avoids using large-scale real data is the fear of crashes!\nIn this paper, we propose to bite the bullet and collect a dataset of crashes\nitself! We build a drone whose sole purpose is to crash into objects: it\nsamples naive trajectories and crashes into random objects. We crash our drone\n11,500 times to create one of the biggest UAV crash dataset. This dataset\ncaptures the different ways in which a UAV can crash. We use all this negative\nflying data in conjunction with positive data sampled from the same\ntrajectories to learn a simple yet powerful policy for UAV navigation. We show\nthat this simple self-supervised model is quite effective in navigating the UAV\neven in extremely cluttered environments with dynamic obstacles including\nhumans. For supplementary video see: https://youtu.be/u151hJaGKUo\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 02:20:20 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 00:13:19 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Gandhi", "Dhiraj", ""], ["Pinto", "Lerrel", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1704.05596", "submitter": "Zhen Wang", "authors": "Zhen Wang, Yuan-Hai Shao, Lan Bai, Li-Ming Liu, Nai-Yang Deng", "title": "Insensitive Stochastic Gradient Twin Support Vector Machine for Large\n  Scale Problems", "comments": "31 pages, 31 figures", "journal-ref": "Information Sciences, Volume 462, September 2018, Pages 114-131", "doi": "10.1016/j.ins.2018.06.007", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent algorithm has been successfully applied on\nsupport vector machines (called PEGASOS) for many classification problems. In\nthis paper, stochastic gradient descent algorithm is investigated to twin\nsupport vector machines for classification. Compared with PEGASOS, the proposed\nstochastic gradient twin support vector machines (SGTSVM) is insensitive on\nstochastic sampling for stochastic gradient descent algorithm. In theory, we\nprove the convergence of SGTSVM instead of almost sure convergence of PEGASOS.\nFor uniformly sampling, the approximation between SGTSVM and twin support\nvector machines is also given, while PEGASOS only has an opportunity to obtain\nan approximation of support vector machines. In addition, the nonlinear SGTSVM\nis derived directly from its linear case. Experimental results on both\nartificial datasets and large scale problems show the stable performance of\nSGTSVM with a fast learning speed.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 03:08:38 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 06:38:48 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Wang", "Zhen", ""], ["Shao", "Yuan-Hai", ""], ["Bai", "Lan", ""], ["Liu", "Li-Ming", ""], ["Deng", "Nai-Yang", ""]]}, {"id": "1704.05646", "submitter": "Lech Szymanski", "authors": "Lech Szymanski, Brendan McCane, Wei Gao, Zhi-Hua Zhou", "title": "Effects of the optimisation of the margin distribution on generalisation\n  in deep architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being so vital to success of Support Vector Machines, the principle\nof separating margin maximisation is not used in deep learning. We show that\nminimisation of margin variance and not maximisation of the margin is more\nsuitable for improving generalisation in deep architectures. We propose the\nHalfway loss function that minimises the Normalised Margin Variance (NMV) at\nthe output of a deep learning models and evaluate its performance against the\nSoftmax Cross-Entropy loss on the MNIST, smallNORB and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 08:31:20 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Szymanski", "Lech", ""], ["McCane", "Brendan", ""], ["Gao", "Wei", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1704.05665", "submitter": "Qingcai Chen", "authors": "Xin Liu, Qingcai Chen, Xiangping Wu, Yan Liu, Yang Liu", "title": "CNN based music emotion classification", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music emotion recognition (MER) is usually regarded as a multi-label tagging\ntask, and each segment of music can inspire specific emotion tags. Most\nresearchers extract acoustic features from music and explore the relations\nbetween these features and their corresponding emotion tags. Considering the\ninconsistency of emotions inspired by the same music segment for human beings,\nseeking for the key acoustic features that really affect on emotions is really\na challenging task. In this paper, we propose a novel MER method by using deep\nconvolutional neural network (CNN) on the music spectrograms that contains both\nthe original time and frequency domain information. By the proposed method, no\nadditional effort on extracting specific features required, which is left to\nthe training procedure of the CNN model. Experiments are conducted on the\nstandard CAL500 and CAL500exp dataset. Results show that, for both datasets,\nthe proposed method outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 09:28:39 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Liu", "Xin", ""], ["Chen", "Qingcai", ""], ["Wu", "Xiangping", ""], ["Liu", "Yan", ""], ["Liu", "Yang", ""]]}, {"id": "1704.05693", "submitter": "Lior Wolf", "authors": "Lior Wolf, Yaniv Taigman, Adam Polyak", "title": "Unsupervised Creation of Parameterized Avatars", "comments": "v2 -- a change in the references due to a request from authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of mapping an input image to a tied pair consisting of a\nvector of parameters and an image that is created using a graphical engine from\nthe vector of parameters. The mapping's objective is to have the output image\nas similar as possible to the input image. During training, no supervision is\ngiven in the form of matching inputs and outputs.\n  This learning problem extends two literature problems: unsupervised domain\nadaptation and cross domain transfer. We define a generalization bound that is\nbased on discrepancy, and employ a GAN to implement a network solution that\ncorresponds to this bound. Experimentally, our method is shown to solve the\nproblem of automatically creating avatars.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 11:19:45 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 16:10:53 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Wolf", "Lior", ""], ["Taigman", "Yaniv", ""], ["Polyak", "Adam", ""]]}, {"id": "1704.05708", "submitter": "Usman Mahmood Khan Usman Mahmood Khan", "authors": "U. M. Khan, Z. Kabir, S. A. Hassan, S. H. Ahmed", "title": "A Deep Learning Framework using Passive WiFi Sensing for Respiration\n  Monitoring", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end deep learning framework using passive WiFi\nsensing to classify and estimate human respiration activity. A passive radar\ntest-bed is used with two channels where the first channel provides the\nreference WiFi signal, whereas the other channel provides a surveillance signal\nthat contains reflections from the human target. Adaptive filtering is\nperformed to make the surveillance signal source-data invariant by eliminating\nthe echoes of the direct transmitted signal. We propose a novel convolutional\nneural network to classify the complex time series data and determine if it\ncorresponds to a breathing activity, followed by a random forest estimator to\ndetermine breathing rate. We collect an extensive dataset to train the learning\nmodels and develop reference benchmarks for the future studies in the field.\nBased on the results, we conclude that deep learning techniques coupled with\npassive radars offer great potential for end-to-end human activity recognition.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 12:35:17 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Khan", "U. M.", ""], ["Kabir", "Z.", ""], ["Hassan", "S. A.", ""], ["Ahmed", "S. H.", ""]]}, {"id": "1704.05712", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, Volker\n  Fischer", "title": "Universal Adversarial Perturbations Against Semantic Image Segmentation", "comments": "Final version for ICCV including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning is remarkably successful on perceptual tasks, it was also\nshown to be vulnerable to adversarial perturbations of the input. These\nperturbations denote noise added to the input that was generated specifically\nto fool the system while being quasi-imperceptible for humans. More severely,\nthere even exist universal perturbations that are input-agnostic but fool the\nnetwork on the majority of inputs. While recent work has focused on image\nclassification, this work proposes attacks against semantic image segmentation:\nwe present an approach for generating (universal) adversarial perturbations\nthat make the network yield a desired target segmentation as output. We show\nempirically that there exist barely perceptible universal noise patterns which\nresult in nearly the same predicted segmentation for arbitrary inputs.\nFurthermore, we also show the existence of universal noise which removes a\ntarget class (e.g., all pedestrians) from the segmentation while leaving the\nsegmentation mostly unchanged otherwise.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 12:48:52 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 08:35:25 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 18:55:54 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Kumar", "Mummadi Chaithanya", ""], ["Brox", "Thomas", ""], ["Fischer", "Volker", ""]]}, {"id": "1704.05761", "submitter": "Bin Liu", "authors": "Bin Liu, Ke-Jia Chen", "title": "Maximum Likelihood Estimation based on Random Subspace EDA: Application\n  to Extrasolar Planet Detection", "comments": "12 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses maximum likelihood (ML) estimation based model fitting\nin the context of extrasolar planet detection. This problem is featured by the\nfollowing properties: 1) the candidate models under consideration are highly\nnonlinear; 2) the likelihood surface has a huge number of peaks; 3) the\nparameter space ranges in size from a few to dozens of dimensions. These\nproperties make the ML search a very challenging problem, as it lacks any\nanalytical or gradient based searching solution to explore the parameter space.\nA population based searching method, called estimation of distribution\nalgorithm (EDA), is adopted to explore the model parameter space starting from\na batch of random locations. EDA is featured by its ability to reveal and\nutilize problem structures. This property is desirable for characterizing the\ndetections. However, it is well recognized that EDAs can not scale well to\nlarge scale problems, as it consists of iterative random sampling and model\nfitting procedures, which results in the well-known dilemma curse of\ndimensionality. A novel mechanism to perform EDAs in interactive random\nsubspaces spanned by correlated variables is proposed and the hope is to\nalleviate the curse of dimensionality for EDAs by performing the operations of\nsampling and model fitting in lower dimensional subspaces. The effectiveness of\nthe proposed algorithm is verified via both benchmark numerical studies and\nreal data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 05:37:32 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 14:14:00 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Liu", "Bin", ""], ["Chen", "Ke-Jia", ""]]}, {"id": "1704.05907", "submitter": "Hongyu Guo", "authors": "Hongyu Guo and Colin Cherry and Jiang Su", "title": "End-to-End Multi-View Networks for Text Classification", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-view network for text classification. Our method\nautomatically creates various views of its input text, each taking the form of\nsoft attention weights that distribute the classifier's focus among a set of\nbase features. For a bag-of-words representation, each view focuses on a\ndifferent subset of the text's words. Aggregating many such views results in a\nmore discriminative and robust representation. Through a novel architecture\nthat both stacks and concatenates views, we produce a network that emphasizes\nboth depth and width, allowing training to converge quickly. Using our\nmulti-view architecture, we establish new state-of-the-art accuracies on two\nbenchmark tasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 19:33:38 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Guo", "Hongyu", ""], ["Cherry", "Colin", ""], ["Su", "Jiang", ""]]}, {"id": "1704.05908", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy", "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion", "comments": "Accepted by ACL 2017. Minor update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases are important resources for a variety of natural language\nprocessing tasks but suffer from incompleteness. We propose a novel embedding\nmodel, \\emph{ITransF}, to perform knowledge base completion. Equipped with a\nsparse attention mechanism, ITransF discovers hidden concepts of relations and\ntransfer statistical strength through the sharing of concepts. Moreover, the\nlearned associations between relations and concepts, which are represented by\nsparse attention vectors, can be interpreted easily. We evaluate ITransF on two\nbenchmark datasets---WN18 and FB15k for knowledge base completion and obtains\nimprovements on both the mean rank and Hits@10 metrics, over all baselines that\ndo not use additional information.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 19:35:54 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 05:20:09 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Xie", "Qizhe", ""], ["Ma", "Xuezhe", ""], ["Dai", "Zihang", ""], ["Hovy", "Eduard", ""]]}, {"id": "1704.05948", "submitter": "Li Chen", "authors": "Li Chen, Mingwei Zhang, Chih-Yuan Yang, Ravi Sahita", "title": "Semi-supervised classification for dynamic Android malware detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of threats to Android phones creates challenges for malware\ndetection. Manually labeling the samples into benign or different malicious\nfamilies requires tremendous human efforts, while it is comparably easy and\ncheap to obtain a large amount of unlabeled APKs from various sources.\nMoreover, the fast-paced evolution of Android malware continuously generates\nderivative malware families. These families often contain new signatures, which\ncan escape detection when using static analysis. These practical challenges can\nalso cause traditional supervised machine learning algorithms to degrade in\nperformance.\n  In this paper, we propose a framework that uses model-based semi-supervised\n(MBSS) classification scheme on the dynamic Android API call logs. The\nsemi-supervised approach efficiently uses the labeled and unlabeled APKs to\nestimate a finite mixture model of Gaussian distributions via conditional\nexpectation-maximization and efficiently detects malwares during out-of-sample\ntesting. We compare MBSS with the popular malware detection classifiers such as\nsupport vector machine (SVM), $k$-nearest neighbor (kNN) and linear\ndiscriminant analysis (LDA). Under the ideal classification setting, MBSS has\ncompetitive performance with 98\\% accuracy and very low false positive rate for\nin-sample classification. For out-of-sample testing, the out-of-sample test\ndata exhibit similar behavior of retrieving phone information and sending to\nthe network, compared with in-sample training set. When this similarity is\nstrong, MBSS and SVM with linear kernel maintain 90\\% detection rate while\n$k$NN and LDA suffer great performance degradation. When this similarity is\nslightly weaker, all classifiers degrade in performance, but MBSS still\nperforms significantly better than other classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 22:29:04 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Chen", "Li", ""], ["Zhang", "Mingwei", ""], ["Yang", "Chih-Yuan", ""], ["Sahita", "Ravi", ""]]}, {"id": "1704.05960", "submitter": "Milad Zafar Nezhad", "authors": "Milad Zafar Nezhad, Dongxiao Zhu, Xiangrui Li, Kai Yang, Phillip Levy", "title": "SAFS: A Deep Feature Selection Approach for Precision Medicine", "comments": null, "journal-ref": null, "doi": "10.1109/BIBM.2016.7822569", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep feature selection method based on deep\narchitecture. Our method uses stacked auto-encoders for feature representation\nin higher-level abstraction. We developed and applied a novel feature learning\napproach to a specific precision medicine problem, which focuses on assessing\nand prioritizing risk factors for hypertension (HTN) in a vulnerable\ndemographic subgroup (African-American). Our approach is to use deep learning\nto identify significant risk factors affecting left ventricular mass indexed to\nbody surface area (LVMI) as an indicator of heart damage risk. The results show\nthat our feature learning and representation approach leads to better results\nin comparison with others.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 00:01:28 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Nezhad", "Milad Zafar", ""], ["Zhu", "Dongxiao", ""], ["Li", "Xiangrui", ""], ["Yang", "Kai", ""], ["Levy", "Phillip", ""]]}, {"id": "1704.05963", "submitter": "Daniel R. Jiang", "authors": "Daniel R. Jiang, Lina Al-Kanj, Warren B. Powell", "title": "Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo Tree Search (MCTS), most famously used in game-play artificial\nintelligence (e.g., the game of Go), is a well-known strategy for constructing\napproximate solutions to sequential decision problems. Its primary innovation\nis the use of a heuristic, known as a default policy, to obtain Monte Carlo\nestimates of downstream values for states in a decision tree. This information\nis used to iteratively expand the tree towards regions of states and actions\nthat an optimal policy might visit. However, to guarantee convergence to the\noptimal action, MCTS requires the entire tree to be expanded asymptotically. In\nthis paper, we propose a new technique called Primal-Dual MCTS that utilizes\nsampled information relaxation upper bounds on potential actions, creating the\npossibility of \"ignoring\" parts of the tree that stem from highly suboptimal\nchoices. This allows us to prove that despite converging to a partial decision\ntree in the limit, the recommended action from Primal-Dual MCTS is optimal. The\nnew approach shows significant promise when used to optimize the behavior of a\nsingle driver navigating a graph while operating on a ride-sharing platform.\nNumerical experiments on a real dataset of 7,000 trips in New Jersey suggest\nthat Primal-Dual MCTS improves upon standard MCTS by producing deeper decision\ntrees and exhibits a reduced sensitivity to the size of the action space.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 00:16:01 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Jiang", "Daniel R.", ""], ["Al-Kanj", "Lina", ""], ["Powell", "Warren B.", ""]]}, {"id": "1704.05982", "submitter": "Tao Wu", "authors": "Tao Wu and David Gleich", "title": "Retrospective Higher-Order Markov Processes for User Trails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users form information trails as they browse the web, checkin with a\ngeolocation, rate items, or consume media. A common problem is to predict what\na user might do next for the purposes of guidance, recommendation, or\nprefetching. First-order and higher-order Markov chains have been widely used\nmethods to study such sequences of data. First-order Markov chains are easy to\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\nin contrast, have too many parameters and suffer from overfitting the training\ndata. Fitting these parameters with regularization and smoothing only offers\nmild improvements. In this paper we propose the retrospective higher-order\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\nis a special case of a higher-order Markov chain where the transitions depend\nretrospectively on a single history state instead of an arbitrary combination\nof history states. There are two immediate computational advantages: the number\nof parameters is linear in the order of the Markov chain and the model can be\nfit to large state spaces. Furthermore, by providing a specific structure to\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\nutilizing history states without risks of overfitting the data. We demonstrate\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\nmethod on various real application datasets spanning geolocation data, review\nsequences, and business locations. The RHOMP model uniformly outperforms\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\nfactorizations in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 02:14:17 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Wu", "Tao", ""], ["Gleich", "David", ""]]}, {"id": "1704.06001", "submitter": "Pooya Khorrami", "authors": "Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad\n  Babaeizadeh, Shiyu Chang, Yang Zhang, Mark A. Hasegawa-Johnson, Roy H.\n  Campbell, Thomas S. Huang", "title": "Fast Generation for Convolutional Autoregressive Models", "comments": "Accepted at ICLR 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional autoregressive models have recently demonstrated\nstate-of-the-art performance on a number of generation tasks. While fast,\nparallel training methods have been crucial for their success, generation is\ntypically implemented in a na\\\"{i}ve fashion where redundant computations are\nunnecessarily repeated. This results in slow generation, making such models\ninfeasible for production environments. In this work, we describe a method to\nspeed up generation in convolutional autoregressive models. The key idea is to\ncache hidden states to avoid redundant computation. We apply our fast\ngeneration method to the Wavenet and PixelCNN++ models and achieve up to\n$21\\times$ and $183\\times$ speedups respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 04:13:21 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Ramachandran", "Prajit", ""], ["Paine", "Tom Le", ""], ["Khorrami", "Pooya", ""], ["Babaeizadeh", "Mohammad", ""], ["Chang", "Shiyu", ""], ["Zhang", "Yang", ""], ["Hasegawa-Johnson", "Mark A.", ""], ["Campbell", "Roy H.", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1704.06036", "submitter": "Jack Valmadre", "authors": "Jack Valmadre, Luca Bertinetto, Jo\\~ao F. Henriques, Andrea Vedaldi,\n  Philip H. S. Torr", "title": "End-to-end representation learning for Correlation Filter based tracking", "comments": "To appear at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Correlation Filter is an algorithm that trains a linear template to\ndiscriminate between images and their translations. It is well suited to object\ntracking because its formulation in the Fourier domain provides a fast\nsolution, enabling the detector to be re-trained once per frame. Previous works\nthat use the Correlation Filter, however, have adopted features that were\neither manually designed or trained for a different task. This work is the\nfirst to overcome this limitation by interpreting the Correlation Filter\nlearner, which has a closed-form solution, as a differentiable layer in a deep\nneural network. This enables learning deep features that are tightly coupled to\nthe Correlation Filter. Experiments illustrate that our method has the\nimportant practical benefit of allowing lightweight architectures to achieve\nstate-of-the-art performance at high framerates.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 07:51:27 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Valmadre", "Jack", ""], ["Bertinetto", "Luca", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Vedaldi", "Andrea", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1704.06061", "submitter": "Ziqiang Shi", "authors": "Ziqiang Shi and Liu Liu and Mengjiao Wang and Rujie Liu", "title": "Multi-view (Joint) Probability Linear Discrimination Analysis for\n  Multi-view Feature Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view feature has been proved to be very effective in many multimedia\napplications. However, the current back-end classifiers cannot make full use of\nsuch features. In this paper, we propose a method to model the multi-faceted\ninformation in the multi-view features explicitly and jointly. In our approach,\nthe feature was modeled as a result derived by a generative multi-view\n(joint\\footnotemark[1]) Probability Linear Discriminant Analysis (PLDA) model,\nwhich contains multiple kinds of latent variables. The usual PLDA model only\nconsiders one single label. However, in practical use, when using multi-task\nlearned network as feature extractor, the extracted feature are always attached\nto several labels. This type of feature is called multi-view feature. With\nmulti-view (joint) PLDA, we are able to explicitly build a model that can\ncombine multiple heterogeneous information from the multi-view features. In\nverification step, we calculated the likelihood to describe whether the two\nfeatures having consistent labels or not. This likelihood are used in the\nfollowing decision-making. Experiments have been conducted on large scale\nverification task. On the public RSR2015 data corpus, the results showed that\nour approach can achieve 0.02\\% EER and 0.09\\% EER for impostor wrong and\nimpostor correct cases respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 09:15:32 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 00:59:04 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 09:24:16 GMT"}, {"version": "v4", "created": "Fri, 7 Jul 2017 00:44:27 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Shi", "Ziqiang", ""], ["Liu", "Liu", ""], ["Wang", "Mengjiao", ""], ["Liu", "Rujie", ""]]}, {"id": "1704.06062", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff, Amit Mandelbaum, Daphna Weinshall", "title": "Every Untrue Label is Untrue in its Own Way: Controlling Error Type with\n  the Log Bilinear Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the method of choice in many application domains of\nmachine learning in recent years, especially for multi-class classification\ntasks. The most common loss function used in this context is the cross-entropy\nloss, which reduces to the log loss in the typical case when there is a single\ncorrect response label. While this loss is insensitive to the identity of the\nassigned class in the case of misclassification, in practice it is often the\ncase that some errors may be more detrimental than others. Here we present the\nbilinear-loss (and related log-bilinear-loss) which differentially penalizes\nthe different wrong assignments of the model. We thoroughly test this method\nusing standard models and benchmark image datasets. As one application, we show\nthe ability of this method to better contain error within the correct\nsuper-class, in the hierarchically labeled CIFAR100 dataset, without affecting\nthe overall performance of the classifier.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 09:29:09 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Resheff", "Yehezkel S.", ""], ["Mandelbaum", "Amit", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1704.06084", "submitter": "Steffen Thoma", "authors": "Steffen Thoma, Achim Rettinger, Fabian Both", "title": "Knowledge Fusion via Embeddings from Text, Knowledge Graphs, and Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a baseline approach for cross-modal knowledge fusion. Different\nbasic fusion methods are evaluated on existing embedding approaches to show the\npotential of joining knowledge about certain concepts across modalities in a\nfused concept representation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 10:49:51 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Thoma", "Steffen", ""], ["Rettinger", "Achim", ""], ["Both", "Fabian", ""]]}, {"id": "1704.06131", "submitter": "Yewen Pu", "authors": "Yewen Pu, Leslie P Kaelbling, Armando Solar-Lezama", "title": "Learning to Acquire Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of diagnosis where a set of simple observations are\nused to infer a potentially complex hidden hypothesis. Finding the optimal\nsubset of observations is intractable in general, thus we focus on the problem\nof active diagnosis, where the agent selects the next most-informative\nobservation based on the results of previous observations. We show that under\nthe assumption of uniform observation entropy, one can build an implication\nmodel which directly predicts the outcome of the potential next observation\nconditioned on the results of past observations, and selects the observation\nwith the maximum entropy. This approach enjoys reduced computation complexity\nby bypassing the complicated hypothesis space, and can be trained on\nobservation data alone, learning how to query without knowledge of the hidden\nhypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 13:28:02 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 12:58:45 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Pu", "Yewen", ""], ["Kaelbling", "Leslie P", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "1704.06176", "submitter": "Cem Deniz", "authors": "Cem M. Deniz, Siyuan Xiang, Spencer Hallyburton, Arakua Welbeck, James\n  S. Babb, Stephen Honig, Kyunghyun Cho, and Gregory Chang", "title": "Segmentation of the Proximal Femur from MR Images using Deep\n  Convolutional Neural Networks", "comments": "This is a pre-print of an article published in Scientific Reports.\n  The final authenticated version is available online at:\n  https://doi.org/10.1038/s41598-018-34817-6", "journal-ref": "Scientific Reports, volume 8, Article number: 16485 (2018)", "doi": "10.1038/s41598-018-34817-6", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) has been proposed as a complimentary method\nto measure bone quality and assess fracture risk. However, manual segmentation\nof MR images of bone is time-consuming, limiting the use of MRI measurements in\nthe clinical practice. The purpose of this paper is to present an automatic\nproximal femur segmentation method that is based on deep convolutional neural\nnetworks (CNNs). This study had institutional review board approval and written\ninformed consent was obtained from all subjects. A dataset of volumetric\nstructural MR images of the proximal femur from 86 subject were\nmanually-segmented by an expert. We performed experiments by training two\ndifferent CNN architectures with multiple number of initial feature maps and\nlayers, and tested their segmentation performance against the gold standard of\nmanual segmentations using four-fold cross-validation. Automatic segmentation\nof the proximal femur achieved a high dice similarity score of 0.94$\\pm$0.05\nwith precision = 0.95$\\pm$0.02, and recall = 0.94$\\pm$0.08 using a CNN\narchitecture based on 3D convolution exceeding the performance of 2D CNNs. The\nhigh segmentation accuracy provided by CNNs has the potential to help bring the\nuse of structural MRI measurements of bone quality into clinical practice for\nmanagement of osteoporosis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 14:54:29 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 21:15:40 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 20:36:28 GMT"}, {"version": "v4", "created": "Tue, 20 Mar 2018 18:32:16 GMT"}, {"version": "v5", "created": "Tue, 5 Feb 2019 14:46:00 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Deniz", "Cem M.", ""], ["Xiang", "Siyuan", ""], ["Hallyburton", "Spencer", ""], ["Welbeck", "Arakua", ""], ["Babb", "James S.", ""], ["Honig", "Stephen", ""], ["Cho", "Kyunghyun", ""], ["Chang", "Gregory", ""]]}, {"id": "1704.06191", "submitter": "Min Lin", "authors": "Min Lin", "title": "Softmax GAN", "comments": "NIPS 2017 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The\nkey idea of Softmax GAN is to replace the classification loss in the original\nGAN with a softmax cross-entropy loss in the sample space of one single batch.\nIn the adversarial learning of $N$ real training samples and $M$ generated\nsamples, the target of discriminator training is to distribute all the\nprobability mass to the real samples, each with probability $\\frac{1}{M}$, and\ndistribute zero probability to generated data. In the generator training phase,\nthe target is to assign equal probability to all data points in the batch, each\nwith probability $\\frac{1}{M+N}$. While the original GAN is closely related to\nNoise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance\nSampling version of GAN. We futher demonstrate with experiments that this\nsimple change stabilizes GAN training.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 15:35:14 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 15:09:00 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Lin", "Min", ""]]}, {"id": "1704.06199", "submitter": "Alessandro Rozza", "authors": "Franco Manessi and Alessandro Rozza and Mario Manzo", "title": "Dynamic Graph Convolutional Networks", "comments": null, "journal-ref": "The final version has been published in Elsevier Pattern\n  Recognition, August 2019", "doi": "10.1016/j.patcog.2019.107000", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different classification tasks need to manage structured data, which are\nusually modeled as graphs. Moreover, these graphs can be dynamic, meaning that\nthe vertices/edges of each graph may change during time. Our goal is to jointly\nexploit structured data and temporal information through the use of a neural\nnetwork model. To the best of our knowledge, this task has not been addressed\nusing these kind of architectures. For this reason, we propose two novel\napproaches, which combine Long Short-Term Memory networks and Graph\nConvolutional Networks to learn long short-term dependencies together with\ngraph structure. The quality of our methods is confirmed by the promising\nresults achieved.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 15:54:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Manessi", "Franco", ""], ["Rozza", "Alessandro", ""], ["Manzo", "Mario", ""]]}, {"id": "1704.06209", "submitter": "Brendt Wohlberg", "authors": "Brendt Wohlberg", "title": "ADMM Penalty Parameter Selection by Residual Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriate selection of the penalty parameter is crucial to obtaining good\nperformance from the Alternating Direction Method of Multipliers (ADMM). While\nanalytic results for optimal selection of this parameter are very limited,\nthere is a heuristic method that appears to be relatively successful in a\nnumber of different problems. The contribution of this paper is to demonstrate\nthat their is a potentially serious flaw in this heuristic approach, and to\npropose a modification that at least partially addresses it.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 16:15:40 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Wohlberg", "Brendt", ""]]}, {"id": "1704.06256", "submitter": "Quanquan Gu", "authors": "Jinghui Chen and Lingxiao Wang and Xiao Zhang and Quanquan Gu", "title": "Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption", "comments": "29 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the robust phase retrieval problem of recovering the unknown\nsignal from the magnitude-only measurements, where the measurements can be\ncontaminated by both sparse arbitrary corruption and bounded random noise. We\npropose a new nonconvex algorithm for robust phase retrieval, namely Robust\nWirtinger Flow to jointly estimate the unknown signal and the sparse\ncorruption. We show that our proposed algorithm is guaranteed to converge\nlinearly to the unknown true signal up to a minimax optimal statistical\nprecision in such a challenging setting. Compared with existing robust phase\nretrieval methods, we achieve an optimal sample complexity of $O(n)$ in both\nnoisy and noise-free settings. Thorough experiments on both synthetic and real\ndatasets corroborate our theory.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 17:59:05 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 22:35:25 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Chen", "Jinghui", ""], ["Wang", "Lingxiao", ""], ["Zhang", "Xiao", ""], ["Gu", "Quanquan", ""]]}, {"id": "1704.06279", "submitter": "Maciej Koch-Janusz", "authors": "Maciej Koch-Janusz and Zohar Ringel", "title": "Mutual Information, Neural Networks and the Renormalization Group", "comments": "The accepted (substantially extended) version", "journal-ref": "Nature Physics 14, 578--582 (2018)", "doi": "10.1038/s41567-018-0081-4", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical systems differring in their microscopic details often display\nstrikingly similar behaviour when probed at macroscopic scales. Those universal\nproperties, largely determining their physical characteristics, are revealed by\nthe powerful renormalization group (RG) procedure, which systematically retains\n\"slow\" degrees of freedom and integrates out the rest. However, the important\ndegrees of freedom may be difficult to identify. Here we demonstrate a machine\nlearning algorithm capable of identifying the relevant degrees of freedom and\nexecuting RG steps iteratively without any prior knowledge about the system. We\nintroduce an artificial neural network based on a model-independent,\ninformation-theoretic characterization of a real-space RG procedure, performing\nthis task. We apply the algorithm to classical statistical physics problems in\none and two dimensions. We demonstrate RG flow and extract the Ising critical\nexponent. Our results demonstrate that machine learning techniques can extract\nabstract physical concepts and consequently become an integral part of theory-\nand model-building.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 18:02:50 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 15:06:30 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Koch-Janusz", "Maciej", ""], ["Ringel", "Zohar", ""]]}, {"id": "1704.06327", "submitter": "Kamran Ghasedi Dizaji", "authors": "Kamran Ghasedi Dizaji, Amirhossein Herandi, Cheng Deng, Weidong Cai,\n  Heng Huang", "title": "Deep Clustering via Joint Convolutional Autoencoder Embedding and\n  Relative Entropy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image clustering is one of the most important computer vision applications,\nwhich has been extensively studied in literature. However, current clustering\nmethods mostly suffer from lack of efficiency and scalability when dealing with\nlarge-scale and high-dimensional data. In this paper, we propose a new\nclustering model, called DEeP Embedded RegularIzed ClusTering (DEPICT), which\nefficiently maps data into a discriminative embedding subspace and precisely\npredicts cluster assignments. DEPICT generally consists of a multinomial\nlogistic regression function stacked on top of a multi-layer convolutional\nautoencoder. We define a clustering objective function using relative entropy\n(KL divergence) minimization, regularized by a prior for the frequency of\ncluster assignments. An alternating strategy is then derived to optimize the\nobjective by updating parameters and estimating cluster assignments.\nFurthermore, we employ the reconstruction loss functions in our autoencoder, as\na data-dependent regularization term, to prevent the deep embedding function\nfrom overfitting. In order to benefit from end-to-end optimization and\neliminate the necessity for layer-wise pretraining, we introduce a joint\nlearning framework to minimize the unified clustering and reconstruction loss\nfunctions together and train all network layers simultaneously. Experimental\nresults indicate the superiority and faster running time of DEPICT in\nreal-world clustering tasks, where no labeled data is available for\nhyper-parameter tuning.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 20:29:46 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 23:08:45 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 00:07:22 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Dizaji", "Kamran Ghasedi", ""], ["Herandi", "Amirhossein", ""], ["Deng", "Cheng", ""], ["Cai", "Weidong", ""], ["Huang", "Heng", ""]]}, {"id": "1704.06440", "submitter": "John Schulman", "authors": "John Schulman and Xi Chen and Pieter Abbeel", "title": "Equivalence Between Policy Gradients and Soft Q-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two of the leading approaches for model-free reinforcement learning are\npolicy gradient methods and $Q$-learning methods. $Q$-learning methods can be\neffective and sample-efficient when they work, however, it is not\nwell-understood why they work, since empirically, the $Q$-values they estimate\nare very inaccurate. A partial explanation may be that $Q$-learning methods are\nsecretly implementing policy gradient updates: we show that there is a precise\nequivalence between $Q$-learning and policy gradient methods in the setting of\nentropy-regularized reinforcement learning, that \"soft\" (entropy-regularized)\n$Q$-learning is exactly equivalent to a policy gradient method. We also point\nout a connection between $Q$-learning methods and natural policy gradient\nmethods. Experimentally, we explore the entropy-regularized versions of\n$Q$-learning and policy gradients, and we find them to perform as well as (or\nslightly better than) the standard variants on the Atari benchmark. We also\nshow that the equivalence holds in practical settings by constructing a\n$Q$-learning method that closely matches the learning dynamics of A3C without\nusing a target network or $\\epsilon$-greedy exploration schedule.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 08:33:59 GMT"}, {"version": "v2", "created": "Sun, 27 Aug 2017 23:43:20 GMT"}, {"version": "v3", "created": "Sat, 18 Nov 2017 19:19:07 GMT"}, {"version": "v4", "created": "Sun, 14 Oct 2018 22:54:38 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Schulman", "John", ""], ["Chen", "Xi", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1704.06497", "submitter": "Julia Kreutzer", "authors": "Julia Kreutzer, Artem Sokolov, Stefan Riezler", "title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit structured prediction describes a stochastic optimization framework\nwhere learning is performed from partial feedback. This feedback is received in\nthe form of a task loss evaluation to a predicted output structure, without\nhaving access to gold standard structures. We advance this framework by lifting\nlinear bandit learning to neural sequence-to-sequence learning problems using\nattention-based recurrent neural networks. Furthermore, we show how to\nincorporate control variates into our learning algorithms for variance\nreduction and improved generalization. We present an evaluation on a neural\nmachine translation task that shows improvements of up to 5.89 BLEU points for\ndomain adaptation from simulated bandit feedback.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 11:56:00 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 17:00:18 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Kreutzer", "Julia", ""], ["Sokolov", "Artem", ""], ["Riezler", "Stefan", ""]]}, {"id": "1704.06498", "submitter": "Benjamin Paassen", "authors": "Benjamin Paa{\\ss}en and Christina G\\\"opfert and Barbara Hammer", "title": "Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces", "comments": "preprint of a submission to 'Neural Processing Letters' (Special\n  issue 'Off the mainstream')", "journal-ref": "Neural Processing Letters 48 (2018) 669-689", "doi": "10.1007/s11063-017-9684-5", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph models are relevant in many fields, such as distributed computing,\nintelligent tutoring systems or social network analysis. In many cases, such\nmodels need to take changes in the graph structure into account, i.e. a varying\nnumber of nodes or edges. Predicting such changes within graphs can be expected\nto yield important insight with respect to the underlying dynamics, e.g. with\nrespect to user behaviour. However, predictive techniques in the past have\nalmost exclusively focused on single edges or nodes. In this contribution, we\nattempt to predict the future state of a graph as a whole. We propose to phrase\ntime series prediction as a regression problem and apply dissimilarity- or\nkernel-based regression techniques, such as 1-nearest neighbor, kernel\nregression and Gaussian process regression, which can be applied to graphs via\ngraph kernels. The output of the regression is a point embedded in a\npseudo-Euclidean space, which can be analyzed using subsequent dissimilarity-\nor kernel-based processing methods. We discuss strategies to speed up Gaussian\nProcesses regression from cubic to linear time and evaluate our approach on two\nwell-established theoretical models of graph evolution as well as two real data\nsets from the domain of intelligent tutoring systems. We find that simple\nregression methods, such as kernel regression, are sufficient to capture the\ndynamics in the theoretical models, but that Gaussian process regression\nsignificantly improves the prediction error for real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 12:08:30 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 10:21:36 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 11:47:42 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Paa\u00dfen", "Benjamin", ""], ["G\u00f6pfert", "Christina", ""], ["Hammer", "Barbara", ""]]}, {"id": "1704.06611", "submitter": "Jonathon Cai", "authors": "Jonathon Cai, Richard Shin, Dawn Song", "title": "Making Neural Programming Architectures Generalize via Recursion", "comments": "Published in ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirically, neural networks that attempt to learn programs from data have\nexhibited poor generalizability. Moreover, it has traditionally been difficult\nto reason about the behavior of these models beyond a certain level of input\ncomplexity. In order to address these issues, we propose augmenting neural\narchitectures with a key abstraction: recursion. As an application, we\nimplement recursion in the Neural Programmer-Interpreter framework on four\ntasks: grade-school addition, bubble sort, topological sort, and quicksort. We\ndemonstrate superior generalizability and interpretability with small amounts\nof training data. Recursion divides the problem into smaller pieces and\ndrastically reduces the domain of each neural network component, making it\ntractable to prove guarantees about the overall system's behavior. Our\nexperience suggests that in order for neural architectures to robustly learn\nprogram semantics, it is necessary to incorporate a concept like recursion.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:02:26 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Cai", "Jonathon", ""], ["Shin", "Richard", ""], ["Song", "Dawn", ""]]}, {"id": "1704.06625", "submitter": "Christopher Metzler", "authors": "Christopher A. Metzler, Ali Mousavi, Richard G. Baraniuk", "title": "Learned D-AMP: Principled Neural Network based Compressive Image\n  Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive image recovery is a challenging problem that requires fast and\naccurate algorithms. Recently, neural networks have been applied to this\nproblem with promising results. By exploiting massively parallel GPU processing\narchitectures and oodles of training data, they can run orders of magnitude\nfaster than existing techniques. However, these methods are largely\nunprincipled black boxes that are difficult to train and often-times specific\nto a single measurement matrix.\n  It was recently demonstrated that iterative sparse-signal-recovery algorithms\ncan be \"unrolled\" to form interpretable deep networks. Taking inspiration from\nthis work, we develop a novel neural network architecture that mimics the\nbehavior of the denoising-based approximate message passing (D-AMP) algorithm.\nWe call this new network Learned D-AMP (LDAMP).\n  The LDAMP network is easy to train, can be applied to a variety of different\nmeasurement matrices, and comes with a state-evolution heuristic that\naccurately predicts its performance. Most importantly, it outperforms the\nstate-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and\nrun time. At high resolutions, and when used with sensing matrices that have\nfast implementations, LDAMP runs over $50\\times$ faster than BM3D-AMP and\nhundreds of times faster than NLR-CS.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:40:29 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 02:46:02 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 21:49:15 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 21:36:00 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Metzler", "Christopher A.", ""], ["Mousavi", "Ali", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1704.06656", "submitter": "Mahdi Zarei", "authors": "Mahdi Zarei", "title": "Feature selection algorithm based on Catastrophe model to improve the\n  performance of regression analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce a new feature selection algorithm to remove the\nirrelevant or redundant features in the data sets. In this algorithm the\nimportance of a feature is based on its fitting to the Catastrophe model.\nAkaike information crite- rion value is used for ranking the features in the\ndata set. The proposed algorithm is compared with well-known RELIEF feature\nselection algorithm. Breast Cancer, Parkinson Telemonitoring data and Slice\nlocality data sets are used to evaluate the model.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 17:32:23 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Zarei", "Mahdi", ""]]}, {"id": "1704.06731", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski and Dhruv Mahajan and S. Sathiya Keerthi and S.\n  V. N. Vishwanathan and Markus Weimer", "title": "Batch-Expansion Training: An Efficient Optimization Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Batch-Expansion Training (BET), a framework for running a batch\noptimizer on a gradually expanding dataset. As opposed to stochastic\napproaches, batches do not need to be resampled i.i.d. at every iteration, thus\nmaking BET more resource efficient in a distributed setting, and when\ndisk-access is constrained. Moreover, BET can be easily paired with most batch\noptimizers, does not require any parameter-tuning, and compares favorably to\nexisting stochastic and batch methods. We show that when the batch size grows\nexponentially with the number of outer iterations, BET achieves optimal\n$O(1/\\epsilon)$ data-access convergence rate for strongly convex objectives.\nExperiments in parallel and distributed settings show that BET performs better\nthan standard batch and stochastic approaches.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 01:26:11 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 22:19:28 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 17:56:43 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""], ["Mahajan", "Dhruv", ""], ["Keerthi", "S. Sathiya", ""], ["Vishwanathan", "S. V. N.", ""], ["Weimer", "Markus", ""]]}, {"id": "1704.06743", "submitter": "Raghav Chalapathy", "authors": "Raghavendra Chalapathy (University of Sydney and Capital Markets\n  Cooperative Research Centre (CMCRC)), Aditya Krishna Menon (Data61/CSIRO and\n  the Australian National University), and Sanjay Chawla (Qatar Computing\n  Research Institute (QCRI), HBKU)", "title": "Robust, Deep and Inductive Anomaly Detection", "comments": "Accepted ECML PKDD 2017 Skopje, Macedonia 18-22 September the\n  European Conference On Machine Learning & Principles and Practice of\n  Knowledge Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  PCA is a classical statistical technique whose simplicity and maturity has\nseen it find widespread use as an anomaly detection technique. However, it is\nlimited in this regard by being sensitive to gross perturbations of the input,\nand by seeking a linear subspace that captures normal behaviour. The first\nissue has been dealt with by robust PCA, a variant of PCA that explicitly\nallows for some data points to be arbitrarily corrupted, however, this does not\nresolve the second issue, and indeed introduces the new issue that one can no\nlonger inductively find anomalies on a test set. This paper addresses both\nissues in a single model, the robust autoencoder. This method learns a\nnonlinear subspace that captures the majority of data points, while allowing\nfor some data to have arbitrary corruption. The model is simple to train and\nleverages recent advances in the optimisation of deep neural networks.\nExperiments on a range of real-world datasets highlight the model's\neffectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 04:12:24 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 18:46:25 GMT"}, {"version": "v3", "created": "Sun, 30 Jul 2017 08:47:45 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chalapathy", "Raghavendra", "", "University of Sydney and Capital Markets\n  Cooperative Research Centre"], ["Menon", "Aditya Krishna", "", "Data61/CSIRO and\n  the Australian National University"], ["Chawla", "Sanjay", "", "Qatar Computing\n  Research Institute"]]}, {"id": "1704.06767", "submitter": "Han Bao", "authors": "Han Bao, Tomoya Sakai, Issei Sato, Masashi Sugiyama", "title": "Convex Formulation of Multiple Instance Learning from Positive and\n  Unlabeled Bags", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) is a variation of traditional supervised\nlearning problems where data (referred to as bags) are composed of sub-elements\n(referred to as instances) and only bag labels are available. MIL has a variety\nof applications such as content-based image retrieval, text categorization and\nmedical diagnosis. Most of the previous work for MIL assume that the training\nbags are fully labeled. However, it is often difficult to obtain an enough\nnumber of labeled bags in practical situations, while many unlabeled bags are\navailable. A learning framework called PU learning (positive and unlabeled\nlearning) can address this problem. In this paper, we propose a convex PU\nlearning method to solve an MIL problem. We experimentally show that the\nproposed method achieves better performance with significantly lower\ncomputational costs than an existing method for PU-MIL.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 08:50:19 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 03:04:53 GMT"}, {"version": "v3", "created": "Tue, 1 May 2018 12:16:34 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Bao", "Han", ""], ["Sakai", "Tomoya", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1704.06803", "submitter": "Federico Monti", "authors": "Federico Monti, Michael M. Bronstein, Xavier Bresson", "title": "Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion models are among the most common formulations of\nrecommender systems. Recent works have showed a boost of performance of these\ntechniques when introducing the pairwise relationships between users/items in\nthe form of graphs, and imposing smoothness priors on these graphs. However,\nsuch techniques do not fully exploit the local stationarity structures of\nuser/item graphs, and the number of parameters to learn is linear w.r.t. the\nnumber of users and items. We propose a novel approach to overcome these\nlimitations by using geometric deep learning on graphs. Our matrix completion\narchitecture combines graph convolutional neural networks and recurrent neural\nnetworks to learn meaningful statistical graph-structured patterns and the\nnon-linear diffusion process that generates the known ratings. This neural\nnetwork system requires a constant number of parameters independent of the\nmatrix size. We apply our method on both synthetic and real datasets, showing\nthat it outperforms state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 14:02:01 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Monti", "Federico", ""], ["Bronstein", "Michael M.", ""], ["Bresson", "Xavier", ""]]}, {"id": "1704.06850", "submitter": "Nishanth Dikkala", "authors": "Constantinos Daskalakis, Nishanth Dikkala, Nick Gravin", "title": "Testing Symmetric Markov Chains from a Single Trajectory", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical distribution testing assumes access to i.i.d. samples from the\ndistribution that is being tested. We initiate the study of Markov chain\ntesting, assuming access to a single trajectory of a Markov Chain. In\nparticular, we observe a single trajectory X0,...,Xt,... of an unknown,\nsymmetric, and finite state Markov Chain M. We do not control the starting\nstate X0, and we cannot restart the chain. Given our single trajectory, the\ngoal is to test whether M is identical to a model Markov Chain M0 , or far from\nit under an appropriate notion of difference. We propose a measure of\ndifference between two Markov chains, motivated by the early work of Kazakos\n[Kaz78], which captures the scaling behavior of the total variation distance\nbetween trajectories sampled from the Markov chains as the length of these\ntrajectories grows. We provide efficient testers and information-theoretic\nlower bounds for testing identity of symmetric Markov chains under our proposed\nmeasure of difference, which are tight up to logarithmic factors if the hitting\ntimes of the model chain M0 is O(n) in the size of the state space n.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 21:02:31 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 03:28:50 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Dikkala", "Nishanth", ""], ["Gravin", "Nick", ""]]}, {"id": "1704.06877", "submitter": "Adams Wei Yu", "authors": "Adams Wei Yu, Hongrae Lee, Quoc V. Le", "title": "Learning to Skim Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks are showing much promise in many sub-areas of\nnatural language processing, ranging from document classification to machine\ntranslation to automatic question answering. Despite their promise, many\nrecurrent models have to read the whole text word by word, making it slow to\nhandle long documents. For example, it is difficult to use a recurrent network\nto read a book and answer questions about it. In this paper, we present an\napproach of reading text while skipping irrelevant information if needed. The\nunderlying model is a recurrent network that learns how far to jump after\nreading a few words of the input text. We employ a standard policy gradient\nmethod to train the model to make discrete jumping decisions. In our benchmarks\non four different tasks, including number prediction, sentiment analysis, news\narticle classification and automatic Q\\&A, our proposed model, a modified LSTM\nwith jumping, is up to 6 times faster than the standard sequential LSTM, while\nmaintaining the same or even better accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 03:54:22 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 19:58:31 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Yu", "Adams Wei", ""], ["Lee", "Hongrae", ""], ["Le", "Quoc V.", ""]]}, {"id": "1704.06880", "submitter": "Avishek Ghosh", "authors": "Avishek Ghosh, Sayak Ray Chowdhury, Aditya Gopalan", "title": "Misspecified Linear Bandits", "comments": "Thirty-First AAAI Conference on Artificial Intelligence, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online learning in misspecified linear stochastic\nmulti-armed bandit problems. Regret guarantees for state-of-the-art linear\nbandit algorithms such as Optimism in the Face of Uncertainty Linear bandit\n(OFUL) hold under the assumption that the arms expected rewards are perfectly\nlinear in their features. It is, however, of interest to investigate the impact\nof potential misspecification in linear bandit models, where the expected\nrewards are perturbed away from the linear subspace determined by the arms\nfeatures. Although OFUL has recently been shown to be robust to relatively\nsmall deviations from linearity, we show that any linear bandit algorithm that\nenjoys optimal regret performance in the perfectly linear setting (e.g., OFUL)\nmust suffer linear regret under a sparse additive perturbation of the linear\nmodel. In an attempt to overcome this negative result, we define a natural\nclass of bandit models characterized by a non-sparse deviation from linearity.\nWe argue that the OFUL algorithm can fail to achieve sublinear regret even\nunder models that have non-sparse deviation.We finally develop a novel bandit\nalgorithm, comprising a hypothesis test for linearity followed by a decision to\nuse either the OFUL or Upper Confidence Bound (UCB) algorithm. For perfectly\nlinear bandit models, the algorithm provably exhibits OFULs favorable regret\nperformance, while for misspecified models satisfying the non-sparse deviation\nproperty, the algorithm avoids the linear regret phenomenon and falls back on\nUCBs sublinear regret scaling. Numerical experiments on synthetic data, and on\nrecommendation data from the public Yahoo! Learning to Rank Challenge dataset,\nempirically support our findings.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 04:37:57 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Ghosh", "Avishek", ""], ["Chowdhury", "Sayak Ray", ""], ["Gopalan", "Aditya", ""]]}, {"id": "1704.06885", "submitter": "Hong Zhao", "authors": "Hong Zhao", "title": "A General Theory for Training Learning Machine", "comments": "55 pages, 18 figures. arXiv admin note: substantial text overlap with\n  arXiv:1602.03950", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though the deep learning is pushing the machine learning to a new stage,\nbasic theories of machine learning are still limited. The principle of\nlearning, the role of the a prior knowledge, the role of neuron bias, and the\nbasis for choosing neural transfer function and cost function, etc., are still\nfar from clear. In this paper, we present a general theoretical framework for\nmachine learning. We classify the prior knowledge into common and\nproblem-dependent parts, and consider that the aim of learning is to maximally\nincorporate them. The principle we suggested for maximizing the former is the\ndesign risk minimization principle, while the neural transfer function, the\ncost function, as well as pretreatment of samples, are endowed with the role\nfor maximizing the latter. The role of the neuron bias is explained from a\ndifferent angle. We develop a Monte Carlo algorithm to establish the\ninput-output responses, and we control the input-output sensitivity of a\nlearning machine by controlling that of individual neurons. Applications of\nfunction approaching and smoothing, pattern recognition and classification, are\nprovided to illustrate how to train general learning machines based on our\ntheory and algorithm. Our method may in addition induce new applications, such\nas the transductive inference.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 05:48:18 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Zhao", "Hong", ""]]}, {"id": "1704.06913", "submitter": "Rahma Chaabouni", "authors": "Rahma Chaabouni, Ewan Dunbar, Neil Zeghidour, Emmanuel Dupoux", "title": "Learning weakly supervised multimodal phoneme embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works have explored deep architectures for learning multimodal speech\nrepresentation (e.g. audio and images, articulation and audio) in a supervised\nway. Here we investigate the role of combining different speech modalities,\ni.e. audio and visual information representing the lips movements, in a weakly\nsupervised way using Siamese networks and lexical same-different side\ninformation. In particular, we ask whether one modality can benefit from the\nother to provide a richer representation for phone recognition in a weakly\nsupervised setting. We introduce mono-task and multi-task methods for merging\nspeech and visual modalities for phone recognition. The mono-task learning\nconsists in applying a Siamese network on the concatenation of the two\nmodalities, while the multi-task learning receives several different\ncombinations of modalities at train time. We show that multi-task learning\nenhances discriminability for visual and multimodal inputs while minimally\nimpacting auditory inputs. Furthermore, we present a qualitative analysis of\nthe obtained phone embeddings, and show that cross-modal visual input can\nimprove the discriminability of phonological features which are visually\ndiscernable (rounding, open/close, labial place of articulation), resulting in\nrepresentations that are closer to abstract linguistic features than those\nbased on audio only.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 11:27:53 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 12:21:22 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Chaabouni", "Rahma", ""], ["Dunbar", "Ewan", ""], ["Zeghidour", "Neil", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "1704.06933", "submitter": "Lijun Wu", "authors": "Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai,\n  Tie-Yan Liu", "title": "Adversarial Neural Machine Translation", "comments": "ACML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a new learning paradigm for Neural Machine\nTranslation (NMT). Instead of maximizing the likelihood of the human\ntranslation as in previous works, we minimize the distinction between human\ntranslation and the translation given by an NMT model. To achieve this goal,\ninspired by the recent success of generative adversarial networks (GANs), we\nemploy an adversarial training architecture and name it as Adversarial-NMT. In\nAdversarial-NMT, the training of the NMT model is assisted by an adversary,\nwhich is an elaborately designed Convolutional Neural Network (CNN). The goal\nof the adversary is to differentiate the translation result generated by the\nNMT model from that by human. The goal of the NMT model is to produce high\nquality translations so as to cheat the adversary. A policy gradient method is\nleveraged to co-train the NMT model and the adversary. Experimental results on\nEnglish$\\rightarrow$French and German$\\rightarrow$English translation tasks\nshow that Adversarial-NMT can achieve significantly better translation quality\nthan several strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 05:08:47 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 13:35:31 GMT"}, {"version": "v3", "created": "Sat, 24 Jun 2017 03:29:54 GMT"}, {"version": "v4", "created": "Sun, 30 Sep 2018 14:04:21 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Wu", "Lijun", ""], ["Xia", "Yingce", ""], ["Zhao", "Li", ""], ["Tian", "Fei", ""], ["Qin", "Tao", ""], ["Lai", "Jianhuang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1704.06956", "submitter": "Sida Wang", "authors": "Sida I. Wang and Samuel Ginn and Percy Liang and Christoper D. Manning", "title": "Naturalizing a Programming Language via Interactive Learning", "comments": "10 pages, ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to create a convenient natural language interface for performing\nwell-specified but complex actions such as analyzing data, manipulating text,\nand querying databases. However, existing natural language interfaces for such\ntasks are quite primitive compared to the power one wields with a programming\nlanguage. To bridge this gap, we start with a core programming language and\nallow users to \"naturalize\" the core language incrementally by defining\nalternative, more natural syntax and increasingly complex concepts in terms of\ncompositions of simpler ones. In a voxel world, we show that a community of\nusers can simultaneously teach a common system a diverse language and use it to\nbuild hundreds of complex voxel structures. Over the course of three days,\nthese users went from using only the core language to using the naturalized\nlanguage in 85.9\\% of the last 10K utterances.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 18:13:10 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Wang", "Sida I.", ""], ["Ginn", "Samuel", ""], ["Liang", "Percy", ""], ["Manning", "Christoper D.", ""]]}, {"id": "1704.06970", "submitter": "Kartik Goyal", "authors": "Kartik Goyal, Chris Dyer and Taylor Berg-Kirkpatrick", "title": "Differentiable Scheduled Sampling for Credit Assignment", "comments": "Accepted at ACL2017 (http://bit.ly/2oj1muX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a continuous relaxation of the argmax operation can be\nused to create a differentiable approximation to greedy decoding for\nsequence-to-sequence (seq2seq) models. By incorporating this approximation into\nthe scheduled sampling training procedure (Bengio et al., 2015)--a well-known\ntechnique for correcting exposure bias--we introduce a new training objective\nthat is continuous and differentiable everywhere and that can provide\ninformative gradients near points where previous decoding decisions change\ntheir value. In addition, by using a related approximation, we demonstrate a\nsimilar approach to sampled-based training. Finally, we show that our approach\noutperforms cross-entropy training and scheduled sampling procedures in two\nsequence prediction tasks: named entity recognition and machine translation.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 20:05:36 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Goyal", "Kartik", ""], ["Dyer", "Chris", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1704.07049", "submitter": "ByeoungDo Kim", "authors": "ByeoungDo Kim, Chang Mook Kang, Seung Hi Lee, Hyunmin Chae, Jaekyum\n  Kim, Chung Choo Chung, and Jun Won Choi", "title": "Probabilistic Vehicle Trajectory Prediction over Occupancy Grid Map via\n  Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient vehicle trajectory prediction\nframework based on recurrent neural network. Basically, the characteristic of\nthe vehicle's trajectory is different from that of regular moving objects since\nit is affected by various latent factors including road structure, traffic\nrules, and driver's intention. Previous state of the art approaches use\nsophisticated vehicle behavior model describing these factors and derive the\ncomplex trajectory prediction algorithm, which requires a system designer to\nconduct intensive model optimization for practical use. Our approach is\ndata-driven and simple to use in that it learns complex behavior of the\nvehicles from the massive amount of trajectory data through deep neural network\nmodel. The proposed trajectory prediction method employs the recurrent neural\nnetwork called long short-term memory (LSTM) to analyze the temporal behavior\nand predict the future coordinate of the surrounding vehicles. The proposed\nscheme feeds the sequence of vehicles' coordinates obtained from sensor\nmeasurements to the LSTM and produces the probabilistic information on the\nfuture location of the vehicles over occupancy grid map. The experiments\nconducted using the data collected from highway driving show that the proposed\nmethod can produce reasonably good estimate of future trajectory.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 05:58:33 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 03:04:26 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Kim", "ByeoungDo", ""], ["Kang", "Chang Mook", ""], ["Lee", "Seung Hi", ""], ["Chae", "Hyunmin", ""], ["Kim", "Jaekyum", ""], ["Chung", "Chung Choo", ""], ["Choi", "Jun Won", ""]]}, {"id": "1704.07050", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Benjamin Strauss", "title": "Using Global Constraints and Reranking to Improve Cognates Detection", "comments": "10 pages, 6 figures, 6 tables; published in the Proceedings of the\n  55th Annual Meeting of the Association for Computational Linguistics, pages\n  1983-1992, Vancouver, Canada, July 2017", "journal-ref": "In Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics, pages 1983-1992, Vancouver, Canada, July 2017.\n  Association for Computational Linguistics", "doi": "10.18653/v1/P17-1181", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global constraints and reranking have not been used in cognates detection\nresearch to date. We propose methods for using global constraints by performing\nrescoring of the score matrices produced by state of the art cognates detection\nsystems. Using global constraints to perform rescoring is complementary to\nstate of the art methods for performing cognates detection and results in\nsignificant performance improvements beyond current state of the art\nperformance on publicly available datasets with different language pairs and\nvarious conditions such as different levels of baseline state of the art\nperformance and different data size conditions, including with more realistic\nlarge data size conditions than have been evaluated with in the past.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 06:04:50 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 20:19:58 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Bloodgood", "Michael", ""], ["Strauss", "Benjamin", ""]]}, {"id": "1704.07055", "submitter": "Rupayan Chakraborty", "authors": "Sri Harsha Dumpala, Rupayan Chakraborty, Sunil Kumar Kopparapu", "title": "k-FFNN: A priori knowledge infused Feed-forward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network (RNN) are being extensively used over feed-forward\nneural networks (FFNN) because of their inherent capability to capture temporal\nrelationships that exist in the sequential data such as speech. This aspect of\nRNN is advantageous especially when there is no a priori knowledge about the\ntemporal correlations within the data. However, RNNs require large amount of\ndata to learn these temporal correlations, limiting their advantage in low\nresource scenarios. It is not immediately clear (a) how a priori temporal\nknowledge can be used in a FFNN architecture (b) how a FFNN performs when\nprovided with this knowledge about temporal correlations (assuming available)\nduring training. The objective of this paper is to explore k-FFNN, namely a\nFFNN architecture that can incorporate the a priori knowledge of the temporal\nrelationships within the data sequence during training and compare k-FFNN\nperformance with RNN in a low resource scenario. We evaluate the performance of\nk-FFNN and RNN by extensive experimentation on MediaEval 2016 audio data\n(\"Emotional Impact of Movies\" task). Experimental results show that the\nperformance of k-FFNN is comparable to RNN, and in some scenarios k-FFNN\nperforms better than RNN when temporal knowledge is injected into FFNN\narchitecture. The main contributions of this paper are (a) fusing a priori\nknowledge into FFNN architecture to construct a k-FFNN and (b) analyzing the\nperformance of k-FFNN with respect to RNN for different size of training data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 06:54:23 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Dumpala", "Sri Harsha", ""], ["Chakraborty", "Rupayan", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1704.07068", "submitter": "Manlio De Domenico", "authors": "Manlio De Domenico", "title": "Diffusion geometry unravels the emergence of functional clusters in\n  collective phenomena", "comments": "9 pages, 7 figures", "journal-ref": "Phys. Rev. Lett. 118, 168301 (2017)", "doi": "10.1103/PhysRevLett.118.168301", "report-no": null, "categories": "physics.soc-ph cond-mat.dis-nn cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective phenomena emerge from the interaction of natural or artificial\nunits with a complex organization. The interplay between structural patterns\nand dynamics might induce functional clusters that, in general, are different\nfrom topological ones. In biological systems, like the human brain, the overall\nfunctionality is often favored by the interplay between connectivity and\nsynchronization dynamics, with functional clusters that do not coincide with\nanatomical modules in most cases. In social, socio-technical and engineering\nsystems, the quest for consensus favors the emergence of clusters.\n  Despite the unquestionable evidence for mesoscale organization of many\ncomplex systems and the heterogeneity of their inter-connectivity, a way to\npredict and identify the emergence of functional modules in collective\nphenomena continues to elude us. Here, we propose an approach based on random\nwalk dynamics to define the diffusion distance between any pair of units in a\nnetworked system. Such a metric allows to exploit the underlying diffusion\ngeometry to provide a unifying framework for the intimate relationship between\nmetastable synchronization, consensus and random search dynamics in complex\nnetworks, pinpointing the functional mesoscale organization of synthetic and\nbiological systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 07:39:47 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["De Domenico", "Manlio", ""]]}, {"id": "1704.07121", "submitter": "Hexiang Hu", "authors": "Wei-Lun Chao, Hexiang Hu, Fei Sha", "title": "Being Negative but Constructively: Lessons Learnt from Creating Better\n  Visual Question Answering Datasets", "comments": "Accepted for Oral Presentation at NAACL-HLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (Visual QA) has attracted a lot of attention\nlately, seen essentially as a form of (visual) Turing test that artificial\nintelligence should strive to achieve. In this paper, we study a crucial\ncomponent of this task: how can we design good datasets for the task? We focus\non the design of multiple-choice based datasets where the learner has to select\nthe right answer from a set of candidate ones including the target (\\ie the\ncorrect one) and the decoys (\\ie the incorrect ones). Through careful analysis\nof the results attained by state-of-the-art learning models and human\nannotators on existing datasets, we show that the design of the decoy answers\nhas a significant impact on how and what the learning models learn from the\ndatasets. In particular, the resulting learner can ignore the visual\ninformation, the question, or both while still doing well on the task. Inspired\nby this, we propose automatic procedures to remedy such design deficiencies. We\napply the procedures to re-construct decoy answers for two popular Visual QA\ndatasets as well as to create a new Visual QA dataset from the Visual Genome\nproject, resulting in the largest dataset for this task. Extensive empirical\nstudies show that the design deficiencies have been alleviated in the remedied\ndatasets and the performance on them is likely a more faithful indicator of the\ndifference among learning models. The datasets are released and publicly\navailable via http://www.teds.usc.edu/website_vqa/.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:05:19 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 20:34:21 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chao", "Wei-Lun", ""], ["Hu", "Hexiang", ""], ["Sha", "Fei", ""]]}, {"id": "1704.07139", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "An Aposteriorical Clusterability Criterion for $k$-Means++ and\n  Simplicity of Clustering", "comments": "58 pages", "journal-ref": "SN Computer Science 1(2): 80 (2020), ISSN: 2662-995X (Print)\n  2661-8907", "doi": "10.1007/s42979-020-0079-8", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the notion of a well-clusterable data set combining the point of\nview of the objective of $k$-means clustering algorithm (minimising the centric\nspread of data elements) and common sense (clusters shall be separated by\ngaps). We identify conditions under which the optimum of $k$-means objective\ncoincides with a clustering under which the data is separated by predefined\ngaps.\n  We investigate two cases: when the whole clusters are separated by some gap\nand when only the cores of the clusters meet some separation condition.\n  We overcome a major obstacle in using clusterability criteria due to the fact\nthat known approaches to clusterability checking had the disadvantage that they\nare related to the optimal clustering which is NP hard to identify.\n  Compared to other approaches to clusterability, the novelty consists in the\npossibility of an a posteriori (after running $k$-means) check if the data set\nis well-clusterable or not. As the $k$-means algorithm applied for this purpose\nhas polynomial complexity so does therefore the appropriate check.\nAdditionally, if $k$-means++ fails to identify a clustering that meets\nclusterability criteria, with high probability the data is not\nwell-clusterable.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:55:41 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 20:05:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1704.07147", "submitter": "Yuki Fujimoto", "authors": "Yuki Fujimoto and Toru Ohira", "title": "A Neural Network model with Bidirectional Whitening", "comments": "16pages", "journal-ref": "In: Rutkowski L., Scherer R., Korytkowski M., Pedrycz W.,\n  Tadeusiewicz R., Zurada J. (eds) Artificial Intelligence and Soft Computing.\n  ICAISC 2018. Lecture Notes in Computer Science, vol 10841. Springer, Cham", "doi": "10.1007/978-3-319-91253-0_5", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here a new model and algorithm which performs an efficient Natural\ngradient descent for Multilayer Perceptrons. Natural gradient descent was\noriginally proposed from a point of view of information geometry, and it\nperforms the steepest descent updates on manifolds in a Riemannian space. In\nparticular, we extend an approach taken by the \"Whitened neural networks\"\nmodel. We make the whitening process not only in feed-forward direction as in\nthe original model, but also in the back-propagation phase. Its efficacy is\nshown by an application of this \"Bidirectional whitened neural networks\" model\nto a handwritten character recognition data (MNIST data).\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 11:18:58 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Fujimoto", "Yuki", ""], ["Ohira", "Toru", ""]]}, {"id": "1704.07156", "submitter": "Marek Rei", "authors": "Marek Rei", "title": "Semi-supervised Multitask Learning for Sequence Labeling", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sequence labeling framework with a secondary training objective,\nlearning to predict surrounding words for every word in the dataset. This\nlanguage modeling objective incentivises the system to learn general-purpose\npatterns of semantic and syntactic composition, which are also useful for\nimproving accuracy on different sequence labeling tasks. The architecture was\nevaluated on a range of datasets, covering the tasks of error detection in\nlearner texts, named entity recognition, chunking and POS-tagging. The novel\nlanguage modeling objective provided consistent performance improvements on\nevery benchmark, without requiring any additional annotated or unannotated\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 11:47:06 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Rei", "Marek", ""]]}, {"id": "1704.07187", "submitter": "Irina Petrova", "authors": "Irina Petrova, Arina Buzdalova", "title": "Reinforcement Learning Based Dynamic Selection of Auxiliary Objectives\n  with Preserving of the Best Found Solution", "comments": "this is a full version of a paper which has been accepted as a\n  student workshop paper to GECCO conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiency of single-objective optimization can be improved by introducing\nsome auxiliary objectives. Ideally, auxiliary objectives should be helpful.\nHowever, in practice, objectives may be efficient on some optimization stages\nbut obstructive on others. In this paper we propose a modification of the EA+RL\nmethod which dynamically selects optimized objectives using reinforcement\nlearning. The proposed modification prevents from losing the best found\nsolution. We analysed the proposed modification and compared it with the EA+RL\nmethod and Random Local Search on XdivK, Generalized OneMax and LeadingOnes\nproblems. The proposed modification outperforms the EA+RL method on all problem\ninstances. It also outperforms the single objective approach on the most\nproblem instances. We also provide detailed analysis of how different\ncomponents of the considered algorithms influence efficiency of optimization.\nIn addition, we present theoretical analysis of the proposed modification on\nthe XdivK problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 12:52:51 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Petrova", "Irina", ""], ["Buzdalova", "Arina", ""]]}, {"id": "1704.07207", "submitter": "Jinbo Xu", "authors": "Zhen Li, Sheng Wang, Yizhou Yu and Jinbo Xu", "title": "Predicting membrane protein contacts from non-membrane proteins by deep\n  transfer learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational prediction of membrane protein (MP) structures is very\nchallenging partially due to lack of sufficient solved structures for homology\nmodeling. Recently direct evolutionary coupling analysis (DCA) sheds some light\non protein contact prediction and accordingly, contact-assisted folding, but\nDCA is effective only on some very large-sized families since it uses\ninformation only in a single protein family. This paper presents a deep\ntransfer learning method that can significantly improve MP contact prediction\nby learning contact patterns and complex sequence-contact relationship from\nthousands of non-membrane proteins (non-MPs). Tested on 510 non-redundant MPs,\nour deep model (learned from only non-MPs) has top L/10 long-range contact\nprediction accuracy 0.69, better than our deep model trained by only MPs (0.63)\nand much better than a representative DCA method CCMpred (0.47) and the CASP11\nwinner MetaPSICOV (0.55). The accuracy of our deep model can be further\nimproved to 0.72 when trained by a mix of non-MPs and MPs. When only contacts\nin transmembrane regions are evaluated, our method has top L/10 long-range\naccuracy 0.62, 0.57, and 0.53 when trained by a mix of non-MPs and MPs, by\nnon-MPs only, and by MPs only, respectively, still much better than MetaPSICOV\n(0.45) and CCMpred (0.40). All these results suggest that sequence-structure\nrelationship learned by our deep model from non-MPs generalizes well to MP\ncontact prediction. Improved contact prediction also leads to better\ncontact-assisted folding. Using only top predicted contacts as restraints, our\ndeep learning method can fold 160 and 200 of 510 MPs with TMscore>0.6 when\ntrained by non-MPs only and by a mix of non-MPs and MPs, respectively, while\nCCMpred and MetaPSICOV can do so for only 56 and 77 MPs, respectively. Our\ncontact-assisted folding also greatly outperforms homology modeling.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 13:27:22 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Li", "Zhen", ""], ["Wang", "Sheng", ""], ["Yu", "Yizhou", ""], ["Xu", "Jinbo", ""]]}, {"id": "1704.07228", "submitter": "Kiran Koshy Thekumparampil", "authors": "Sahand Negahban and Sewoong Oh and Kiran K. Thekumparampil and Jiaming\n  Xu", "title": "Learning from Comparisons and Choices", "comments": "77 pages, 12 figures; added new experiments and references. arXiv\n  admin note: substantial text overlap with arXiv:1506.07947", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When tracking user-specific online activities, each user's preference is\nrevealed in the form of choices and comparisons. For example, a user's purchase\nhistory is a record of her choices, i.e. which item was chosen among a subset\nof offerings. A user's preferences can be observed either explicitly as in\nmovie ratings or implicitly as in viewing times of news articles. Given such\nindividualized ordinal data in the form of comparisons and choices, we address\nthe problem of collaboratively learning representations of the users and the\nitems. The learned features can be used to predict a user's preference of an\nunseen item to be used in recommendation systems. This also allows one to\ncompute similarities among users and items to be used for categorization and\nsearch. Motivated by the empirical successes of the MultiNomial Logit (MNL)\nmodel in marketing and transportation, and also more recent successes in word\nembedding and crowdsourced image embedding, we pose this problem as learning\nthe MNL model parameters that best explain the data. We propose a convex\nrelaxation for learning the MNL model, and show that it is minimax optimal up\nto a logarithmic factor by comparing its performance to a fundamental lower\nbound. This characterizes the minimax sample complexity of the problem, and\nproves that the proposed estimator cannot be improved upon other than by a\nlogarithmic factor. Further, the analysis identifies how the accuracy depends\non the topology of sampling via the spectrum of the sampling graph. This\nprovides a guideline for designing surveys when one can choose which items are\nto be compared. This is accompanied by numerical simulations on synthetic and\nreal data sets, confirming our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 13:49:14 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 10:26:16 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Negahban", "Sahand", ""], ["Oh", "Sewoong", ""], ["Thekumparampil", "Kiran K.", ""], ["Xu", "Jiaming", ""]]}, {"id": "1704.07287", "submitter": "Shubham Toshniwal", "authors": "Trang Tran, Shubham Toshniwal, Mohit Bansal, Kevin Gimpel, Karen\n  Livescu, Mari Ostendorf", "title": "Parsing Speech: A Neural Approach to Integrating Lexical and\n  Acoustic-Prosodic Information", "comments": "Accepted in NAACL HLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conversational speech, the acoustic signal provides cues that help\nlisteners disambiguate difficult parses. For automatically parsing spoken\nutterances, we introduce a model that integrates transcribed text and\nacoustic-prosodic features using a convolutional neural network over energy and\npitch trajectories coupled with an attention-based recurrent neural network\nthat accepts text and prosodic features. We find that different types of\nacoustic-prosodic features are individually helpful, and together give\nstatistically significant improvements in parse and disfluency detection F1\nscores over a strong text-only baseline. For this study with known sentence\nboundaries, error analyses show that the main benefit of acoustic-prosodic\nfeatures is in sentences with disfluencies, attachment decisions are most\nimproved, and transcription errors obscure gains from prosody.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 15:33:26 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 23:02:57 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Tran", "Trang", ""], ["Toshniwal", "Shubham", ""], ["Bansal", "Mohit", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1704.07352", "submitter": "Pratik Jawanpuria", "authors": "Pratik Jawanpuria, Bamdev Mishra", "title": "Structured low-rank matrix learning: algorithms and applications", "comments": "Accepted in ICML'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a low-rank matrix, constrained to lie in\na linear subspace, and introduce a novel factorization for modeling such\nmatrices. A salient feature of the proposed factorization scheme is it\ndecouples the low-rank and the structural constraints onto separate factors. We\nformulate the optimization problem on the Riemannian spectrahedron manifold,\nwhere the Riemannian framework allows to develop computationally efficient\nconjugate gradient and trust-region algorithms. Experiments on problems such as\nstandard/robust/non-negative matrix completion, Hankel matrix learning and\nmulti-task learning demonstrate the efficacy of our approach. A shorter version\nof this work has been published in ICML'18.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:47:29 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 04:10:24 GMT"}, {"version": "v3", "created": "Sat, 23 Dec 2017 07:50:44 GMT"}, {"version": "v4", "created": "Mon, 12 Feb 2018 18:07:36 GMT"}, {"version": "v5", "created": "Fri, 15 Jun 2018 10:27:07 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Jawanpuria", "Pratik", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1704.07433", "submitter": "Haw-Shiuan Chang", "authors": "Haw-Shiuan Chang and Erik Learned-Miller and Andrew McCallum", "title": "Active Bias: Training More Accurate Neural Networks by Emphasizing High\n  Variance Samples", "comments": "camera-ready version for NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-paced learning and hard example mining re-weight training instances to\nimprove learning accuracy. This paper presents two improved alternatives based\non lightweight estimates of sample uncertainty in stochastic gradient descent\n(SGD): the variance in predicted probability of the correct class across\niterations of mini-batch SGD, and the proximity of the correct class\nprobability to the decision threshold. Extensive experimental results on six\ndatasets show that our methods reliably improve accuracy in various network\narchitectures, including additional gains on top of other popular training\ntechniques, such as residual learning, momentum, ADAM, batch normalization,\ndropout, and distillation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 19:48:49 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 18:04:36 GMT"}, {"version": "v3", "created": "Wed, 3 Jan 2018 03:18:57 GMT"}, {"version": "v4", "created": "Sat, 6 Jan 2018 20:33:39 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Chang", "Haw-Shiuan", ""], ["Learned-Miller", "Erik", ""], ["McCallum", "Andrew", ""]]}, {"id": "1704.07468", "submitter": "Yanjun  Qi Dr.", "authors": "Ritambhara Singh, Arshdeep Sekhon, Kamran Kowsari, Jack Lanchantin,\n  Beilun Wang and Yanjun Qi", "title": "GaKCo: a Fast GApped k-mer string Kernel using COunting", "comments": "@ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String Kernel (SK) techniques, especially those using gapped $k$-mers as\nfeatures (gk), have obtained great success in classifying sequences like DNA,\nprotein, and text. However, the state-of-the-art gk-SK runs extremely slow when\nwe increase the dictionary size ($\\Sigma$) or allow more mismatches ($M$). This\nis because current gk-SK uses a trie-based algorithm to calculate co-occurrence\nof mismatched substrings resulting in a time cost proportional to\n$O(\\Sigma^{M})$. We propose a \\textbf{fast} algorithm for calculating\n\\underline{Ga}pped $k$-mer \\underline{K}ernel using \\underline{Co}unting\n(GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of\nsubstrings using cumulative counting. This algorithm is fast, scalable to\nlarger $\\Sigma$ and $M$, and naturally parallelizable. We provide a rigorous\nasymptotic analysis that compares GaKCo with the state-of-the-art gk-SK.\nTheoretically, the time cost of GaKCo is independent of the $\\Sigma^{M}$ term\nthat slows down the trie-based approach. Experimentally, we observe that GaKCo\nachieves the same accuracy as the state-of-the-art and outperforms its speed by\nfactors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein\n(12 datasets), and character-based English text (2 datasets), respectively.\n  GaKCo is shared as an open source tool at\n\\url{https://github.com/QData/GaKCo-SVM}\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 21:43:21 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 20:12:01 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 17:25:17 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Singh", "Ritambhara", ""], ["Sekhon", "Arshdeep", ""], ["Kowsari", "Kamran", ""], ["Lanchantin", "Jack", ""], ["Wang", "Beilun", ""], ["Qi", "Yanjun", ""]]}, {"id": "1704.07483", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron", "title": "Continuously Differentiable Exponential Linear Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential Linear Units (ELUs) are a useful rectifier for constructing deep\nlearning architectures, as they may speed up and otherwise improve learning by\nvirtue of not have vanishing gradients and by having mean activations near\nzero. However, the ELU activation as parametrized in [1] is not continuously\ndifferentiable with respect to its input when the shape parameter alpha is not\nequal to 1. We present an alternative parametrization which is C1 continuous\nfor all values of alpha, making the rectifier easier to reason about and making\nalpha easier to tune. This alternative parametrization has several other useful\nproperties that the original parametrization of ELU does not: 1) its derivative\nwith respect to x is bounded, 2) it contains both the linear transfer function\nand ReLU as special cases, and 3) it is scale-similar with respect to alpha.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 22:37:08 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Barron", "Jonathan T.", ""]]}, {"id": "1704.07487", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan", "title": "Bootstrapping Graph Convolutional Neural Networks for Autism Spectrum\n  Disorder Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using predictive models to identify patterns that can act as biomarkers for\ndifferent neuropathoglogical conditions is becoming highly prevalent. In this\npaper, we consider the problem of Autism Spectrum Disorder (ASD) classification\nwhere previous work has shown that it can be beneficial to incorporate a wide\nvariety of meta features, such as socio-cultural traits, into predictive\nmodeling. A graph-based approach naturally suits these scenarios, where a\ncontextual graph captures traits that characterize a population, while the\nspecific brain activity patterns are utilized as a multivariate signal at the\nnodes. Graph neural networks have shown improvements in inferencing with\ngraph-structured data. Though the underlying graph strongly dictates the\noverall performance, there exists no systematic way of choosing an appropriate\ngraph in practice, thus making predictive models non-robust. To address this,\nwe propose a bootstrapped version of graph convolutional neural networks\n(G-CNNs) that utilizes an ensemble of weakly trained G-CNNs, and reduce the\nsensitivity of models on the choice of graph construction. We demonstrate its\neffectiveness on the challenging Autism Brain Imaging Data Exchange (ABIDE)\ndataset and show that our approach improves upon recently proposed graph-based\nneural networks. We also show that our method remains more robust to noisy\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 22:52:32 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 02:42:31 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "1704.07498", "submitter": "Samir Abdelrahman", "authors": "Mohammad Amin Morid, Olivia R. Liu Sheng, Samir Abdelrahman", "title": "Leveraging Patient Similarity and Time Series Data in Healthcare\n  Predictive Models", "comments": "To appear:Twenty-third Americas Conference on Information Systems,\n  Boston, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patient time series classification faces challenges in high degrees of\ndimensionality and missingness. In light of patient similarity theory, this\nstudy explores effective temporal feature engineering and reduction, missing\nvalue imputation, and change point detection methods that can afford\nsimilarity-based classification models with desirable accuracy enhancement. We\nselect a piecewise aggregation approximation method to extract fine-grain\ntemporal features and propose a minimalist method to impute missing values in\ntemporal features. For dimensionality reduction, we adopt a gradient descent\nsearch method for feature weight assignment. We propose new patient status and\ndirectional change definitions based on medical knowledge or clinical\nguidelines about the value ranges for different patient status levels, and\ndevelop a method to detect change points indicating positive or negative\npatient status changes. We evaluate the effectiveness of the proposed methods\nin the context of early Intensive Care Unit mortality prediction. The\nevaluation results show that the k-Nearest Neighbor algorithm that incorporates\nmethods we select and propose significantly outperform the relevant benchmarks\nfor early ICU mortality prediction. This study makes contributions to time\nseries classification and early ICU mortality prediction via identifying and\nenhancing temporal feature engineering and reduction methods for\nsimilarity-based time series classification.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 00:25:06 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 00:27:53 GMT"}, {"version": "v3", "created": "Sun, 30 Apr 2017 15:33:25 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Morid", "Mohammad Amin", ""], ["Sheng", "Olivia R. Liu", ""], ["Abdelrahman", "Samir", ""]]}, {"id": "1704.07499", "submitter": "Samir Abdelrahman", "authors": "Mohammad Amin Morid, Olivia R. Liu Sheng, Samir Abdelrahman", "title": "PPMF: A Patient-based Predictive Modeling Framework for Early ICU\n  Mortality Prediction", "comments": "10 pages, Healthcare Analytics and Medical Decision Making, INFORMS\n  Workshop. Nashville, Tennessee, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, developing a good model for early intensive care unit (ICU)\nmortality prediction is still challenging. This paper presents a patient based\npredictive modeling framework (PPMF) to improve the performance of ICU\nmortality prediction using data collected during the first 48 hours of ICU\nadmission. PPMF consists of three main components verifying three related\nresearch hypotheses. The first component captures dynamic changes of patients\nstatus in the ICU using their time series data (e.g., vital signs and\nlaboratory tests). The second component is a local approximation algorithm that\nclassifies patients based on their similarities. The third component is a\nGradient Decent wrapper that updates feature weights according to the\nclassification feedback. Experiments using data from MIMICIII show that PPMF\nsignificantly outperforms: (1) the severity score systems, namely SASP III,\nAPACHE IV, and MPM0III, (2) the aggregation based classifiers that utilize\nsummarized time series, and (3) baseline feature selection methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 00:27:00 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Morid", "Mohammad Amin", ""], ["Sheng", "Olivia R. Liu", ""], ["Abdelrahman", "Samir", ""]]}, {"id": "1704.07503", "submitter": "Cheng-Hao Cai", "authors": "Cheng-Hao Cai, Dengfeng Ke, Yanyan Xu, Kaile Su", "title": "Learning of Human-like Algebraic Reasoning Using Deep Feedforward Neural\n  Networks", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.bica.2018.07.004", "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a wide gap between symbolic reasoning and deep learning. In this\nresearch, we explore the possibility of using deep learning to improve symbolic\nreasoning. Briefly, in a reasoning system, a deep feedforward neural network is\nused to guide rewriting processes after learning from algebraic reasoning\nexamples produced by humans. To enable the neural network to recognise patterns\nof algebraic expressions with non-deterministic sizes, reduced partial trees\nare used to represent the expressions. Also, to represent both top-down and\nbottom-up information of the expressions, a centralisation technique is used to\nimprove the reduced partial trees. Besides, symbolic association vectors and\nrule application records are used to improve the rewriting processes.\nExperimental results reveal that the algebraic reasoning examples can be\naccurately learnt only if the feedforward neural network has enough hidden\nlayers. Also, the centralisation technique, the symbolic association vectors\nand the rule application records can reduce error rates of reasoning. In\nparticular, the above approaches have led to 4.6% error rate of reasoning on a\ndataset of linear equations, differentials and integrals.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 01:10:09 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Cai", "Cheng-Hao", ""], ["Ke", "Dengfeng", ""], ["Xu", "Yanyan", ""], ["Su", "Kaile", ""]]}, {"id": "1704.07505", "submitter": "Feng Nan", "authors": "Feng Nan and Venkatesh Saligrama", "title": "Dynamic Model Selection for Prediction Under a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic model selection approach for resource-constrained\nprediction. Given an input instance at test-time, a gating function identifies\na prediction model for the input among a collection of models. Our objective is\nto minimize overall average cost without sacrificing accuracy. We learn gating\nand prediction models on fully labeled training data by means of a bottom-up\nstrategy. Our novel bottom-up method is a recursive scheme whereby a\nhigh-accuracy complex model is first trained. Then a low-complexity gating and\nprediction model are subsequently learnt to adaptively approximate the\nhigh-accuracy model in regions where low-cost models are capable of making\nhighly accurate predictions. We pose an empirical loss minimization problem\nwith cost constraints to jointly train gating and prediction models. On a\nnumber of benchmark datasets our method outperforms state-of-the-art achieving\nhigher accuracy for the same cost.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 01:17:22 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Nan", "Feng", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1704.07506", "submitter": "Luca de Alfaro", "authors": "Eugenio Tacchini, Gabriele Ballarin, Marco L. Della Vedova, Stefano\n  Moret, Luca de Alfaro", "title": "Some Like it Hoax: Automated Fake News Detection in Social Networks", "comments": null, "journal-ref": "Proceedings of the Second Workshop on Data Science for Social Good\n  (SoGood), Skopje, Macedonia, 2017. CEUR Workshop Proceedings Volume 1960,\n  2017", "doi": null, "report-no": "Technical Report UCSC-SOE-17-05, School of Engineering, University\n  of California, Santa Cruz", "categories": "cs.LG cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the reliability of information on the Internet has emerged\nas a crucial issue of modern society. Social network sites (SNSs) have\nrevolutionized the way in which information is spread by allowing users to\nfreely share content. As a consequence, SNSs are also increasingly used as\nvectors for the diffusion of misinformation and hoaxes. The amount of\ndisseminated information and the rapidity of its diffusion make it practically\nimpossible to assess reliability in a timely manner, highlighting the need for\nautomatic hoax detection systems.\n  As a contribution towards this objective, we show that Facebook posts can be\nclassified with high accuracy as hoaxes or non-hoaxes on the basis of the users\nwho \"liked\" them. We present two classification techniques, one based on\nlogistic regression, the other on a novel adaptation of boolean crowdsourcing\nalgorithms. On a dataset consisting of 15,500 Facebook posts and 909,236 users,\nwe obtain classification accuracies exceeding 99% even when the training set\ncontains less than 1% of the posts. We further show that our techniques are\nrobust: they work even when we restrict our attention to the users who like\nboth hoax and non-hoax posts. These results suggest that mapping the diffusion\npattern of information can be a useful component of automatic hoax detection\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 01:20:40 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Tacchini", "Eugenio", ""], ["Ballarin", "Gabriele", ""], ["Della Vedova", "Marco L.", ""], ["Moret", "Stefano", ""], ["de Alfaro", "Luca", ""]]}, {"id": "1704.07511", "submitter": "Ga Wu", "authors": "Ga Wu, Buser Say, Scott Sanner", "title": "Scalable Planning with Tensorflow for Hybrid Nonlinear Domains", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given recent deep learning results that demonstrate the ability to\neffectively optimize high-dimensional non-convex functions with gradient\ndescent optimization on GPUs, we ask in this paper whether symbolic gradient\noptimization tools such as Tensorflow can be effective for planning in hybrid\n(mixed discrete and continuous) nonlinear domains with high dimensional state\nand action spaces? To this end, we demonstrate that hybrid planning with\nTensorflow and RMSProp gradient descent is competitive with mixed integer\nlinear program (MILP) based optimization on piecewise linear planning domains\n(where we can compute optimal solutions) and substantially outperforms\nstate-of-the-art interior point methods for nonlinear planning domains.\nFurthermore, we remark that Tensorflow is highly scalable, converging to a\nstrong plan on a large-scale concurrent domain with a total of 576,000\ncontinuous action parameters distributed over a horizon of 96 time steps and\n100 parallel instances in only 4 minutes. We provide a number of insights that\nclarify such strong performance including observations that despite long\nhorizons, RMSProp avoids both the vanishing and exploding gradient problems.\nTogether these results suggest a new frontier for highly scalable planning in\nnonlinear hybrid domains by leveraging GPUs and the power of recent advances in\ngradient descent with highly optimized toolkits like Tensorflow.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 01:52:45 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 17:58:42 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 23:32:02 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Wu", "Ga", ""], ["Say", "Buser", ""], ["Sanner", "Scott", ""]]}, {"id": "1704.07515", "submitter": "Shin Ando Ph. D.", "authors": "Shin Ando and Chun-Yuan Huang", "title": "Deep Over-sampling Framework for Classifying Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance is a challenging issue in practical classification problems\nfor deep learning models as well as traditional models. Traditionally\nsuccessful countermeasures such as synthetic over-sampling have had limited\nsuccess with complex, structured data handled by deep learning models. In this\npaper, we propose Deep Over-sampling (DOS), a framework for extending the\nsynthetic over-sampling method to exploit the deep feature space acquired by a\nconvolutional neural network (CNN). Its key feature is an explicit, supervised\nrepresentation learning, for which the training data presents each raw input\nsample with a synthetic embedding target in the deep feature space, which is\nsampled from the linear subspace of in-class neighbors. We implement an\niterative process of training the CNN and updating the targets, which induces\nsmaller in-class variance among the embeddings, to increase the discriminative\npower of the deep representation. We present an empirical study using public\nbenchmarks, which shows that the DOS framework not only counteracts class\nimbalance better than the existing method, but also improves the performance of\nthe CNN in the standard, balanced settings.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 02:12:00 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 06:40:40 GMT"}, {"version": "v3", "created": "Wed, 12 Jul 2017 21:02:37 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Ando", "Shin", ""], ["Huang", "Chun-Yuan", ""]]}, {"id": "1704.07535", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Mitchell Stern, Dan Klein", "title": "Abstract Syntax Networks for Code Generation and Semantic Parsing", "comments": "ACL 2017. MR and MS contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasks like code generation and semantic parsing require mapping unstructured\n(or partially structured) inputs to well-formed, executable outputs. We\nintroduce abstract syntax networks, a modeling framework for these problems.\nThe outputs are represented as abstract syntax trees (ASTs) and constructed by\na decoder with a dynamically-determined modular structure paralleling the\nstructure of the output tree. On the benchmark Hearthstone dataset for code\ngeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,\ncompared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we\nperform competitively on the Atis, Jobs, and Geo semantic parsing datasets with\nno task-specific engineering.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 04:37:35 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Stern", "Mitchell", ""], ["Klein", "Dan", ""]]}, {"id": "1704.07548", "submitter": "Huiguang He", "authors": "Changde Du, Changying Du, Jinpeng Li, Wei-long Zheng, Bao-liang Lu,\n  Huiguang He", "title": "Semi-supervised Bayesian Deep Multi-modal Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In emotion recognition, it is difficult to recognize human's emotional states\nusing just a single modality. Besides, the annotation of physiological\nemotional data is particularly expensive. These two aspects make the building\nof effective emotion recognition model challenging. In this paper, we first\nbuild a multi-view deep generative model to simulate the generative process of\nmulti-modality emotional data. By imposing a mixture of Gaussians assumption on\nthe posterior approximation of the latent variables, our model can learn the\nshared deep representation from multiple modalities. To solve the\nlabeled-data-scarcity problem, we further extend our multi-view model to\nsemi-supervised learning scenario by casting the semi-supervised classification\nproblem as a specialized missing data imputation task. Our semi-supervised\nmulti-view deep generative framework can leverage both labeled and unlabeled\ndata from multiple modalities, where the weight factor for each modality can be\nlearned automatically. Compared with previous emotion recognition methods, our\nmethod is more robust and flexible. The experiments conducted on two real\nmulti-modal emotion datasets have demonstrated the superiority of our framework\nover a number of competitors.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 06:29:59 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Du", "Changde", ""], ["Du", "Changying", ""], ["Li", "Jinpeng", ""], ["Zheng", "Wei-long", ""], ["Lu", "Bao-liang", ""], ["He", "Huiguang", ""]]}, {"id": "1704.07597", "submitter": "Tushar S. Vaidya", "authors": "Tushar Vaidya and Carlos Murguia and Georgios Piliouras", "title": "Learning Agents in Black-Scholes Financial Markets: Consensus Dynamics\n  and Volatility Smiles", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-Scholes (BS) is the standard mathematical model for option pricing in\nfinancial markets. Option prices are calculated using an analytical formula\nwhose main inputs are strike (at which price to exercise) and volatility. The\nBS framework assumes that volatility remains constant across all strikes,\nhowever, in practice it varies. How do traders come to learn these parameters?\nWe introduce natural models of learning agents, in which they update their\nbeliefs about the true implied volatility based on the opinions of other\ntraders. We prove convergence of these opinion dynamics using techniques from\ncontrol theory and leader-follower models, thus providing a resolution between\ntheory and market practices. We allow for two different models, one with\nfeedback and one with an unknown leader.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 09:13:37 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 14:43:32 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 03:41:29 GMT"}, {"version": "v4", "created": "Sat, 11 Jul 2020 02:11:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Vaidya", "Tushar", ""], ["Murguia", "Carlos", ""], ["Piliouras", "Georgios", ""]]}, {"id": "1704.07657", "submitter": "Andrey Ignatov", "authors": "Dmitry Ignatov and Andrey Ignatov", "title": "Decision Stream: Cultivating Deep Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various modifications of decision trees have been extensively used during the\npast years due to their high efficiency and interpretability. Tree node\nsplitting based on relevant feature selection is a key step of decision tree\nlearning, at the same time being their major shortcoming: the recursive nodes\npartitioning leads to geometric reduction of data quantity in the leaf nodes,\nwhich causes an excessive model complexity and data overfitting. In this paper,\nwe present a novel architecture - a Decision Stream, - aimed to overcome this\nproblem. Instead of building a tree structure during the learning process, we\npropose merging nodes from different branches based on their similarity that is\nestimated with two-sample test statistics, which leads to generation of a deep\ndirected acyclic graph of decision rules that can consist of hundreds of\nlevels. To evaluate the proposed solution, we test it on several common machine\nlearning problems - credit scoring, twitter sentiment analysis, aircraft flight\ncontrol, MNIST and CIFAR image classification, synthetic data classification\nand regression. Our experimental results reveal that the proposed approach\nsignificantly outperforms the standard decision tree learning methods on both\nregression and classification tasks, yielding a prediction error decrease up to\n35%.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 12:20:33 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 10:22:29 GMT"}, {"version": "v3", "created": "Sun, 3 Sep 2017 18:01:09 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Ignatov", "Dmitry", ""], ["Ignatov", "Andrey", ""]]}, {"id": "1704.07664", "submitter": "Ashish Mani Dr.", "authors": "Arit Kumar Bishwas, Ashish Mani, Vasile Palade", "title": "An All-Pair Quantum SVM Approach for Big Data Multiclass Classification", "comments": null, "journal-ref": "Quantum Information Processing October 2018, 17:282 Springer US", "doi": "10.1007/s11128-018-2046-z", "report-no": null, "categories": "cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have discussed a quantum approach for the all-pair\nmulticlass classification problem. We have shown that the multiclass support\nvector machine for big data classification with a quantum all-pair approach can\nbe implemented in logarithm runtime complexity on a quantum computer. In an\nall-pair approach, there is one binary classification problem for each pair of\nclasses, and so there are k (k-1)/2 classifiers for a k-class problem. As\ncompared to the classical multiclass support vector machine that can be\nimplemented with polynomial run time complexity, our approach exhibits\nexponential speed up in the quantum version. The quantum all-pair algorithm can\nbe used with other classification algorithms, and a speed up gain can be\nachieved as compared to their classical counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 12:33:57 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 04:51:40 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Bishwas", "Arit Kumar", ""], ["Mani", "Ashish", ""], ["Palade", "Vasile", ""]]}, {"id": "1704.07669", "submitter": "Wenjian Yu Prof.", "authors": "Wenjian Yu, Yu Gu, Jian Li, Shenghua Liu, and Yaohang Li", "title": "Single-Pass PCA of Large High-Dimensional Data", "comments": "IJCAI 2017, 16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a fundamental dimension reduction tool\nin statistics and machine learning. For large and high-dimensional data,\ncomputing the PCA (i.e., the singular vectors corresponding to a number of\ndominant singular values of the data matrix) becomes a challenging task. In\nthis work, a single-pass randomized algorithm is proposed to compute PCA with\nonly one pass over the data. It is suitable for processing extremely large and\nhigh-dimensional data stored in slow memory (hard disk) or the data generated\nin a streaming fashion. Experiments with synthetic and real data validate the\nalgorithm's accuracy, which has orders of magnitude smaller error than an\nexisting single-pass algorithm. For a set of high-dimensional data stored as a\n150 GB file, the proposed algorithm is able to compute the first 50 principal\ncomponents in just 24 minutes on a typical 24-core computer, with less than 1\nGB memory cost.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 12:55:20 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Yu", "Wenjian", ""], ["Gu", "Yu", ""], ["Li", "Jian", ""], ["Liu", "Shenghua", ""], ["Li", "Yaohang", ""]]}, {"id": "1704.07706", "submitter": "Owen Vallis Ph.D.", "authors": "Jordan Hochenbaum, Owen S. Vallis, Arun Kejariwal", "title": "Automatic Anomaly Detection in the Cloud Via Statistical Learning", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance and high availability have become increasingly important drivers,\namongst other drivers, for user retention in the context of web services such\nas social networks, and web search. Exogenic and/or endogenic factors often\ngive rise to anomalies, making it very challenging to maintain high\navailability, while also delivering high performance. Given that\nservice-oriented architectures (SOA) typically have a large number of services,\nwith each service having a large set of metrics, automatic detection of\nanomalies is non-trivial.\n  Although there exists a large body of prior research in anomaly detection,\nexisting techniques are not applicable in the context of social network data,\nowing to the inherent seasonal and trend components in the time series data.\n  To this end, we developed two novel statistical techniques for automatically\ndetecting anomalies in cloud infrastructure data. Specifically, the techniques\nemploy statistical learning to detect anomalies in both application, and system\nmetrics. Seasonal decomposition is employed to filter the trend and seasonal\ncomponents of the time series, followed by the use of robust statistical\nmetrics -- median and median absolute deviation (MAD) -- to accurately detect\nanomalies, even in the presence of seasonal spikes.\n  We demonstrate the efficacy of the proposed techniques from three different\nperspectives, viz., capacity planning, user behavior, and supervised learning.\nIn particular, we used production data for evaluation, and we report Precision,\nRecall, and F-measure in each case.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 06:09:48 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Hochenbaum", "Jordan", ""], ["Vallis", "Owen S.", ""], ["Kejariwal", "Arun", ""]]}, {"id": "1704.07751", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Dan Klein", "title": "Fine-Grained Entity Typing with High-Multiplicity Assignments", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As entity type systems become richer and more fine-grained, we expect the\nnumber of types assigned to a given entity to increase. However, most\nfine-grained typing work has focused on datasets that exhibit a low degree of\ntype multiplicity. In this paper, we consider the high-multiplicity regime\ninherent in data sources such as Wikipedia that have semi-open type systems. We\nintroduce a set-prediction approach to this problem and show that our model\noutperforms unstructured baselines on a new Wikipedia-based fine-grained typing\ncorpus.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 15:52:52 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Klein", "Dan", ""]]}, {"id": "1704.07790", "submitter": "Haoyi Xiong", "authors": "Haoyi Xiong, Wei Cheng, Wenqing Hu, Jiang Bian, and Zhishan Guo", "title": "FWDA: a Fast Wishart Discriminant Analysis with its Application to\n  Electronic Health Records Data Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Discriminant Analysis (LDA) on Electronic Health Records (EHR) data is\nwidely-used for early detection of diseases. Classical LDA for EHR data\nclassification, however, suffers from two handicaps: the ill-posed estimation\nof LDA parameters (e.g., covariance matrix), and the \"linear inseparability\" of\nEHR data. To handle these two issues, in this paper, we propose a novel\nclassifier FWDA -- Fast Wishart Discriminant Analysis, that makes predictions\nin an ensemble way. Specifically, FWDA first surrogates the distribution of\ninverse covariance matrices using a Wishart distribution estimated from the\ntraining data, then \"weighted-averages\" the classification results of multiple\nLDA classifiers parameterized by the sampled inverse covariance matrices via a\nBayesian Voting scheme. The weights for voting are optimally updated to adapt\neach new input data, so as to enable the nonlinear classification. Theoretical\nanalysis indicates that FWDA possesses a fast convergence rate and a robust\nperformance on high dimensional data. Extensive experiments on large-scale EHR\ndataset show that our approach outperforms state-of-the-art algorithms by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:11:57 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Xiong", "Haoyi", ""], ["Cheng", "Wei", ""], ["Hu", "Wenqing", ""], ["Bian", "Jiang", ""], ["Guo", "Zhishan", ""]]}, {"id": "1704.07807", "submitter": "Ming Yan", "authors": "Zhi Li and Wei Shi and Ming Yan", "title": "A decentralized proximal-gradient method with network independent\n  step-sizes and separated convergence rates", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, 67 (2019), 4494-4506", "doi": "10.1109/TSP.2019.2926022", "report-no": null, "categories": "math.OC cs.DC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel proximal-gradient algorithm for a decentralized\noptimization problem with a composite objective containing smooth and\nnon-smooth terms. Specifically, the smooth and nonsmooth terms are dealt with\nby gradient and proximal updates, respectively. The proposed algorithm is\nclosely related to a previous algorithm, PG-EXTRA \\cite{shi2015proximal}, but\nhas a few advantages. First of all, agents use uncoordinated step-sizes, and\nthe stable upper bounds on step-sizes are independent of network topologies.\nThe step-sizes depend on local objective functions, and they can be as large as\nthose of the gradient descent. Secondly, for the special case without\nnon-smooth terms, linear convergence can be achieved under the strong convexity\nassumption. The dependence of the convergence rate on the objective functions\nand the network are separated, and the convergence rate of the new algorithm is\nas good as one of the two convergence rates that match the typical rates for\nthe general gradient descent and the consensus averaging. We provide numerical\nexperiments to demonstrate the efficacy of the introduced algorithm and\nvalidate our theoretical discoveries.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:36:15 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 02:26:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Li", "Zhi", ""], ["Shi", "Wei", ""], ["Yan", "Ming", ""]]}, {"id": "1704.07816", "submitter": "Long Jin", "authors": "Long Jin, Justin Lazarow, Zhuowen Tu", "title": "Introspective Classification with Convolutional Nets", "comments": "12 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose introspective convolutional networks (ICN) that emphasize the\nimportance of having convolutional neural networks empowered with generative\ncapabilities. We employ a reclassification-by-synthesis algorithm to perform\ntraining using a formulation stemmed from the Bayes theory. Our ICN tries to\niteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by\nimproving the classification. The single CNN classifier learned is at the same\ntime generative --- being able to directly synthesize new samples within its\nown discriminative model. We conduct experiments on benchmark datasets\nincluding MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures,\nand observe improved classification results.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:49:03 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 05:09:48 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Jin", "Long", ""], ["Lazarow", "Justin", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1704.07820", "submitter": "Long Jin", "authors": "Justin Lazarow, Long Jin, Zhuowen Tu", "title": "Introspective Generative Modeling: Decide Discriminatively", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unsupervised learning by developing introspective generative\nmodeling (IGM) that attains a generator using progressively learned deep\nconvolutional neural networks. The generator is itself a discriminator, capable\nof introspection: being able to self-evaluate the difference between its\ngenerated samples and the given training data. When followed by repeated\ndiscriminative learning, desirable properties of modern discriminative\nclassifiers are directly inherited by the generator. IGM learns a cascade of\nCNN classifiers using a synthesis-by-classification algorithm. In the\nexperiments, we observe encouraging results on a number of applications\nincluding texture modeling, artistic style transferring, face modeling, and\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:57:33 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lazarow", "Justin", ""], ["Jin", "Long", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1704.07854", "submitter": "Lukas Prantl", "authors": "Lukas Prantl, Boris Bonev, Nils Thuerey", "title": "Generating Liquid Simulations with Deformation-aware Neural Networks", "comments": "ICLR 2019, further information and videos at\n  https://ge.in.tum.de/publications/2017-prantl-defonn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for deformation-aware neural networks that learn\nthe weighting and synthesis of dense volumetric deformation fields. Our method\nspecifically targets the space-time representation of physical surfaces from\nliquid simulations. Liquids exhibit highly complex, non-linear behavior under\nchanging simulation conditions such as different initial conditions. Our\nalgorithm captures these complex phenomena in two stages: a first neural\nnetwork computes a weighting function for a set of pre-computed deformations,\nwhile a second network directly generates a deformation field for refining the\nsurface. Key for successful training runs in this setting is a suitable loss\nfunction that encodes the effect of the deformations, and a robust calculation\nof the corresponding gradients. To demonstrate the effectiveness of our\napproach, we showcase our method with several complex examples of flowing\nliquids with topology changes. Our representation makes it possible to rapidly\ngenerate the desired implicit surfaces. We have implemented a mobile\napplication to demonstrate that real-time interactions with complex liquid\neffects are possible with our approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 18:21:42 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 14:54:16 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 15:05:01 GMT"}, {"version": "v4", "created": "Wed, 20 Feb 2019 13:27:28 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Prantl", "Lukas", ""], ["Bonev", "Boris", ""], ["Thuerey", "Nils", ""]]}, {"id": "1704.07888", "submitter": "Waheed Bajwa", "authors": "Matthew Nokleby and Waheed U. Bajwa", "title": "Stochastic Optimization from Distributed, Streaming Data in Rate-limited\n  Networks", "comments": "16 pages, 6 figures; Accepted for publication in IEEE Transactions on\n  Signal and Information Processing over Networks", "journal-ref": "Published in IEEE Trans. Signal Inform. Proc. over Netw., vol. 5,\n  no. 1, pp. 152-167, Mar. 2019", "doi": "10.1109/TSIPN.2018.2866320", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by machine learning applications in networks of sensors,\ninternet-of-things (IoT) devices, and autonomous agents, we propose techniques\nfor distributed stochastic convex learning from high-rate data streams. The\nsetup involves a network of nodes---each one of which has a stream of data\narriving at a constant rate---that solve a stochastic convex optimization\nproblem by collaborating with each other over rate-limited communication links.\nTo this end, we present and analyze two algorithms---termed distributed\nstochastic approximation mirror descent (D-SAMD) and accelerated distributed\nstochastic approximation mirror descent (AD-SAMD)---that are based on two\nstochastic variants of mirror descent and in which nodes collaborate via\napproximate averaging of the local, noisy subgradients using distributed\nconsensus. Our main contributions are (i) bounds on the convergence rates of\nD-SAMD and AD-SAMD in terms of the number of nodes, network topology, and ratio\nof the data streaming and communication rates, and (ii) sufficient conditions\nfor order-optimum convergence of these algorithms. In particular, we show that\nfor sufficiently well-connected networks, distributed learning schemes can\nobtain order-optimum convergence even if the communications rate is small.\nFurther we find that the use of accelerated methods significantly enlarges the\nregime in which order-optimum convergence is achieved; this is in contrast to\nthe centralized setting, where accelerated methods usually offer only a modest\nimprovement. Finally, we demonstrate the effectiveness of the proposed\nalgorithms using numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 19:52:52 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 18:41:03 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 10:31:27 GMT"}, {"version": "v4", "created": "Mon, 6 Aug 2018 08:42:35 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Nokleby", "Matthew", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1704.07911", "submitter": "Urs Muller", "authors": "Mariusz Bojarski, Philip Yeres, Anna Choromanska, Krzysztof\n  Choromanski, Bernhard Firner, Lawrence Jackel, Urs Muller", "title": "Explaining How a Deep Neural Network Trained with End-to-End Learning\n  Steers a Car", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of a complete software stack for autonomous driving, NVIDIA has\ncreated a neural-network-based system, known as PilotNet, which outputs\nsteering angles given images of the road ahead. PilotNet is trained using road\nimages paired with the steering angles generated by a human driving a\ndata-collection car. It derives the necessary domain knowledge by observing\nhuman drivers. This eliminates the need for human engineers to anticipate what\nis important in an image and foresee all the necessary rules for safe driving.\nRoad tests demonstrated that PilotNet can successfully perform lane keeping in\na wide variety of driving conditions, regardless of whether lane markings are\npresent or not.\n  The goal of the work described here is to explain what PilotNet learns and\nhow it makes its decisions. To this end we developed a method for determining\nwhich elements in the road image most influence PilotNet's steering decision.\nResults show that PilotNet indeed learns to recognize relevant objects on the\nroad.\n  In addition to learning the obvious features such as lane markings, edges of\nroads, and other cars, PilotNet learns more subtle features that would be hard\nto anticipate and program by engineers, for example, bushes lining the edge of\nthe road and atypical vehicle classes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 21:25:41 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Bojarski", "Mariusz", ""], ["Yeres", "Philip", ""], ["Choromanska", "Anna", ""], ["Choromanski", "Krzysztof", ""], ["Firner", "Bernhard", ""], ["Jackel", "Lawrence", ""], ["Muller", "Urs", ""]]}, {"id": "1704.07926", "submitter": "Kelvin Guu", "authors": "Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, Percy Liang", "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum\n  Marginal Likelihood", "comments": "Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to learn a semantic parser that maps natural language utterances\ninto executable programs when only indirect supervision is available: examples\nare labeled with the correct execution result, but not the program itself.\nConsequently, we must search the space of programs for those that output the\ncorrect result, while not being misled by spurious programs: incorrect programs\nthat coincidentally output the correct result. We connect two common learning\nparadigms, reinforcement learning (RL) and maximum marginal likelihood (MML),\nand then present a new learning algorithm that combines the strengths of both.\nThe new algorithm guards against spurious programs by combining the systematic\nsearch traditionally employed in MML with the randomized exploration of RL, and\nby updating parameters such that probability is spread more evenly across\nconsistent programs. We apply our learning algorithm to a new neural semantic\nparser and show significant gains over existing state-of-the-art results on a\nrecent context-dependent semantic parsing task.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 22:51:12 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Guu", "Kelvin", ""], ["Pasupat", "Panupong", ""], ["Liu", "Evan Zheran", ""], ["Liang", "Percy", ""]]}, {"id": "1704.07938", "submitter": "Tien Thanh Nguyen", "authors": "Tien Thanh Nguyen, Thi Thu Thuy Nguyen, Xuan Cuong Pham, Alan\n  Wee-Chung Liew, James C. Bezdek", "title": "An ensemble-based online learning algorithm for streaming data", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce an ensemble-based approach for online machine\nlearning. The ensemble of base classifiers in our approach is obtained by\nlearning Naive Bayes classifiers on different training sets which are generated\nby projecting the original training set to lower dimensional space. We propose\na mechanism to learn sequences of data using data chunks paradigm. The\nexperiments conducted on a number of UCI datasets and one synthetic dataset\ndemonstrate that the proposed approach performs significantly better than some\nwell-known online learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 00:33:36 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Nguyen", "Tien Thanh", ""], ["Nguyen", "Thi Thu Thuy", ""], ["Pham", "Xuan Cuong", ""], ["Liew", "Alan Wee-Chung", ""], ["Bezdek", "James C.", ""]]}, {"id": "1704.07943", "submitter": "Fang Liu", "authors": "Swapna Buccapatnam, Fang Liu, Atilla Eryilmaz, Ness B. Shroff", "title": "Reward Maximization Under Uncertainty: Leveraging Side-Observations on\n  Networks", "comments": "minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic multi-armed bandit (MAB) problem in the presence of\nside-observations across actions that occur as a result of an underlying\nnetwork structure. In our model, a bipartite graph captures the relationship\nbetween actions and a common set of unknowns such that choosing an action\nreveals observations for the unknowns that it is connected to. This models a\ncommon scenario in online social networks where users respond to their friends'\nactivity, thus providing side information about each other's preferences. Our\ncontributions are as follows: 1) We derive an asymptotic lower bound (with\nrespect to time) as a function of the bi-partite network structure on the\nregret of any uniformly good policy that achieves the maximum long-term average\nreward. 2) We propose two policies - a randomized policy; and a policy based on\nthe well-known upper confidence bound (UCB) policies - both of which explore\neach action at a rate that is a function of its network position. We show,\nunder mild assumptions, that these policies achieve the asymptotic lower bound\non the regret up to a multiplicative factor, independent of the network\nstructure. Finally, we use numerical examples on a real-world social network\nand a routing example network to demonstrate the benefits obtained by our\npolicies over other existing policies.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 01:53:09 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 20:39:01 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Buccapatnam", "Swapna", ""], ["Liu", "Fang", ""], ["Eryilmaz", "Atilla", ""], ["Shroff", "Ness B.", ""]]}, {"id": "1704.07953", "submitter": "Feihu Huang", "authors": "Feihu Huang and Songcan Chen", "title": "Linear Convergence of Accelerated Stochastic Gradient Descent for\n  Nonconvex Nonsmooth Optimization", "comments": "This paper has been withdrawn by the author due to some errors in the\n  proof of the convergence analysis. They will modify these errors as soon as\n  possible", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the stochastic gradient descent (SGD) method for the\nnonconvex nonsmooth optimization, and propose an accelerated SGD method by\ncombining the variance reduction technique with Nesterov's extrapolation\ntechnique. Moreover, based on the local error bound condition, we establish the\nlinear convergence of our method to obtain a stationary point of the nonconvex\noptimization. In particular, we prove that not only the sequence generated\nlinearly converges to a stationary point of the problem, but also the\ncorresponding sequence of objective values is linearly convergent. Finally,\nsome numerical experiments demonstrate the effectiveness of our method. To the\nbest of our knowledge, it is first proved that the accelerated SGD method\nconverges linearly to the local minimum of the nonconvex optimization.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 02:43:24 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 23:05:21 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Huang", "Feihu", ""], ["Chen", "Songcan", ""]]}, {"id": "1704.07971", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Jason D. Lee", "title": "A Flexible Framework for Hypothesis Testing in High-dimensions", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing in the linear regression model is a fundamental\nstatistical problem. We consider linear regression in the high-dimensional\nregime where the number of parameters exceeds the number of samples ($p> n$).\nIn order to make informative inference, we assume that the model is\napproximately sparse, that is the effect of covariates on the response can be\nwell approximated by conditioning on a relatively small number of covariates\nwhose identities are unknown. We develop a framework for testing very general\nhypotheses regarding the model parameters. Our framework encompasses testing\nwhether the parameter lies in a convex cone, testing the signal strength, and\ntesting arbitrary functionals of the parameter. We show that the proposed\nprocedure controls the type I error, and also analyze the power of the\nprocedure. Our numerical experiments confirm our theoretical findings and\ndemonstrate that we control false positive rate (type I error) near the nominal\nlevel, and have high power. By duality between hypotheses testing and\nconfidence intervals, the proposed framework can be used to obtain valid\nconfidence intervals for various functionals of the model parameters. For\nlinear functionals, the length of confidence intervals is shown to be minimax\nrate optimal.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 05:01:16 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 07:35:33 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 08:10:47 GMT"}, {"version": "v4", "created": "Sat, 21 Sep 2019 06:11:57 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Javanmard", "Adel", ""], ["Lee", "Jason D.", ""]]}, {"id": "1704.07978", "submitter": "Xin Li", "authors": "Pengfei Zhu, Xin Li, Pascal Poupart, Guanghui Miao", "title": "On Improving Deep Reinforcement Learning for POMDPs", "comments": "7 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Reinforcement Learning (RL) recently emerged as one of the most\ncompetitive approaches for learning in sequential decision making problems with\nfully observable environments, e.g., computer Go. However, very little work has\nbeen done in deep RL to handle partially observable environments. We propose a\nnew architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to\nenhance learning performance in partially observable domains. Actions are\nencoded by a fully connected layer and coupled with a convolutional observation\nto form an action-observation pair. The time series of action-observation pairs\nare then integrated by an LSTM layer that learns latent states based on which a\nfully connected layer computes Q-values as in conventional Deep Q-Networks\n(DQNs). We demonstrate the effectiveness of our new architecture in several\npartially observable domains, including flickering Atari games.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 05:55:07 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 11:03:37 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 04:36:40 GMT"}, {"version": "v4", "created": "Mon, 22 Jan 2018 14:17:12 GMT"}, {"version": "v5", "created": "Thu, 17 May 2018 06:10:57 GMT"}, {"version": "v6", "created": "Thu, 24 May 2018 14:13:20 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Zhu", "Pengfei", ""], ["Li", "Xin", ""], ["Poupart", "Pascal", ""], ["Miao", "Guanghui", ""]]}, {"id": "1704.07987", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni", "title": "Training L1-Regularized Models with Orthant-Wise Passive Descent\n  Algorithms", "comments": "Accepted to The Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18). Feb 2018, New Orleans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $L_1$-regularized models are widely used for sparse regression or\nclassification tasks. In this paper, we propose the orthant-wise passive\ndescent algorithm (OPDA) for optimizing $L_1$-regularized models, as an\nimproved substitute of proximal algorithms, which are the standard tools for\noptimizing the models nowadays. OPDA uses a stochastic variance-reduced\ngradient (SVRG) to initialize the descent direction, then apply a novel\nalignment operator to encourage each element keeping the same sign after one\niteration of update, so the parameter remains in the same orthant as before. It\nalso explicitly suppresses the magnitude of each element to impose sparsity.\nThe quasi-Newton update can be utilized to incorporate curvature information\nand accelerate the speed. We prove a linear convergence rate for OPDA on\ngeneral smooth and strongly-convex loss functions. By conducting experiments on\n$L_1$-regularized logistic regression and convolutional neural networks, we\nshow that OPDA outperforms state-of-the-art stochastic proximal algorithms,\nimplying a wide range of applications in training sparse models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 07:07:13 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 15:20:53 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 08:57:23 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Wangni", "Jianqiao", ""]]}, {"id": "1704.08006", "submitter": "Hongcheng Li", "authors": "Bin Liang and Hongcheng Li and Miaoqiang Su and Pan Bian and Xirong Li\n  and Wenchang Shi", "title": "Deep Text Classification Can be Fooled", "comments": "8 pages", "journal-ref": "https://www.ijcai.org/proceedings/2018/585", "doi": "10.24963/ijcai.2018/585", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an effective method to craft text adversarial\nsamples, revealing one important yet underestimated fact that DNN-based text\nclassifiers are also prone to adversarial sample attack. Specifically,\nconfronted with different adversarial scenarios, the text items that are\nimportant for classification are identified by computing the cost gradients of\nthe input (white-box attack) or generating a series of occluded test samples\n(black-box attack). Based on these items, we design three perturbation\nstrategies, namely insertion, modification, and removal, to generate\nadversarial samples. The experiment results show that the adversarial samples\ngenerated by our method can successfully fool both state-of-the-art\ncharacter-level and word-level DNN-based text classifiers. The adversarial\nsamples can be perturbed to any desirable classes without compromising their\nutilities. At the same time, the introduced perturbation is difficult to be\nperceived.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 08:17:34 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 08:07:04 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Liang", "Bin", ""], ["Li", "Hongcheng", ""], ["Su", "Miaoqiang", ""], ["Bian", "Pan", ""], ["Li", "Xirong", ""], ["Shi", "Wenchang", ""]]}, {"id": "1704.08045", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen and Matthias Hein", "title": "The loss surface of deep and wide neural networks", "comments": "ICML 2017. Main results now hold for larger classes of loss functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the optimization problem behind deep neural networks is highly\nnon-convex, it is frequently observed in practice that training deep networks\nseems possible without getting stuck in suboptimal points. It has been argued\nthat this is the case as all local minima are close to being globally optimal.\nWe show that this is (almost) true, in fact almost all local minima are\nglobally optimal, for a fully connected network with squared loss and analytic\nactivation function given that the number of hidden units of one layer of the\nnetwork is larger than the number of training points and the network structure\nfrom this layer on is pyramidal.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 10:24:54 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 19:43:39 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Nguyen", "Quynh", ""], ["Hein", "Matthias", ""]]}, {"id": "1704.08067", "submitter": "Arnaud Joly", "authors": "Arnaud Joly", "title": "Exploiting random projections and sparsity with random forests and\n  gradient boosting methods -- Application to multi-label and multi-output\n  learning, random forest model compression and leveraging input sparsity", "comments": "PhD Thesis, Liege, Dec 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within machine learning, the supervised learning field aims at modeling the\ninput-output relationship of a system, from past observations of its behavior.\nDecision trees characterize the input-output relationship through a series of\nnested $if-then-else$ questions, the testing nodes, leading to a set of\npredictions, the leaf nodes. Several of such trees are often combined together\nfor state-of-the-art performance: random forest ensembles average the\npredictions of randomized decision trees trained independently in parallel,\nwhile tree boosting ensembles train decision trees sequentially to refine the\npredictions made by the previous ones. The emergence of new applications\nrequires scalable supervised learning algorithms in terms of computational\npower and memory space with respect to the number of inputs, outputs, and\nobservations without sacrificing accuracy. In this thesis, we identify three\nmain areas where decision tree methods could be improved for which we provide\nand evaluate original algorithmic solutions: (i) learning over high dimensional\noutput spaces, (ii) learning with large sample datasets and stringent memory\nconstraints at prediction time and (iii) learning over high dimensional sparse\ninput spaces.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 11:45:04 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Joly", "Arnaud", ""]]}, {"id": "1704.08068", "submitter": "Dawei Dai", "authors": "Dawei Dai and Weimin Tan and Hong Zhan", "title": "Understanding the Feedforward Artificial Neural Network Model From the\n  Perspective of Network Flow", "comments": "arXiv admin note: text overlap with arXiv:1702.04595 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recent years, deep learning based on artificial neural network (ANN) has\nachieved great success in pattern recognition. However, there is no clear\nunderstanding of such neural computational models. In this paper, we try to\nunravel \"black-box\" structure of Ann model from network flow. Specifically, we\nconsider the feed forward Ann as a network flow model, which consists of many\ndirectional class-pathways. Each class-pathway encodes one class. The\nclass-pathway of a class is obtained by connecting the activated neural nodes\nin each layer from input to output, where activation value of neural node\n(node-value) is defined by the weights of each layer in a trained\nANN-classifier. From the perspective of the class-pathway, training an\nANN-classifier can be regarded as the formulation process of class-pathways of\ndifferent classes. By analyzing the the distances of each two class-pathways in\na trained ANN-classifiers, we try to answer the questions, why the classifier\nperforms so? At last, from the neural encodes view, we define the importance of\neach neural node through the class-pathways, which is helpful to optimize the\nstructure of a classifier. Experiments for two types of ANN model including\nmulti-layer MLP and CNN verify that the network flow based on class-pathway is\na reasonable explanation for ANN models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 11:45:05 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Dai", "Dawei", ""], ["Tan", "Weimin", ""], ["Zhan", "Hong", ""]]}, {"id": "1704.08092", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist, Niko Schenk, Christian Chiarcos", "title": "A Recurrent Neural Model with Attention for the Recognition of Chinese\n  Implicit Discourse Relations", "comments": "To appear at ACL2017, code available at\n  https://github.com/sronnqvist/discourse-ablstm", "journal-ref": null, "doi": "10.18653/v1/P17-2040", "report-no": "Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics (ACL'17)", "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an attention-based Bi-LSTM for Chinese implicit discourse\nrelations and demonstrate that modeling argument pairs as a joint sequence can\noutperform word order-agnostic approaches. Our model benefits from a partial\nsampling scheme and is conceptually simple, yet achieves state-of-the-art\nperformance on the Chinese Discourse Treebank. We also visualize its attention\nactivity to illustrate the model's ability to selectively focus on the relevant\nparts of an input sequence.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 13:10:12 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Schenk", "Niko", ""], ["Chiarcos", "Christian", ""]]}, {"id": "1704.08134", "submitter": "Mohammadreza Soltaninejad", "authors": "Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou, Nigel Allinson,\n  Xujiong Ye", "title": "Multimodal MRI brain tumor segmentation using random forests with\n  features learned from fully convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel learning based method for automated\nsegmenta-tion of brain tumor in multimodal MRI images. The machine learned\nfeatures from fully convolutional neural network (FCN) and hand-designed texton\nfea-tures are used to classify the MRI image voxels. The score map with\npixel-wise predictions is used as a feature map which is learned from\nmultimodal MRI train-ing dataset using the FCN. The learned features are then\napplied to random for-ests to classify each MRI image voxel into normal brain\ntissues and different parts of tumor. The method was evaluated on BRATS 2013\nchallenge dataset. The results show that the application of the random forest\nclassifier to multimodal MRI images using machine-learned features based on FCN\nand hand-designed features based on textons provides promising segmentations.\nThe Dice overlap measure for automatic brain tumor segmentation against ground\ntruth is 0.88, 080 and 0.73 for complete tumor, core and enhancing tumor,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 14:22:02 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Soltaninejad", "Mohammadreza", ""], ["Zhang", "Lei", ""], ["Lambrou", "Tryphon", ""], ["Allinson", "Nigel", ""], ["Ye", "Xujiong", ""]]}, {"id": "1704.08165", "submitter": "Yotam Hechtlinger", "authors": "Yotam Hechtlinger, Purvasha Chakravarti and Jining Qin", "title": "A Generalization of Convolutional Neural Networks to Graph-Structured\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a generalization of Convolutional Neural Networks\n(CNNs) from low-dimensional grid data, such as images, to graph-structured\ndata. We propose a novel spatial convolution utilizing a random walk to uncover\nthe relations within the input, analogous to the way the standard convolution\nuses the spatial neighborhood of a pixel on the grid. The convolution has an\nintuitive interpretation, is efficient and scalable and can also be used on\ndata with varying graph structure. Furthermore, this generalization can be\napplied to many standard regression or classification problems, by learning the\nthe underlying graph. We empirically demonstrate the performance of the\nproposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular\nactivity data set.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 15:37:50 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Hechtlinger", "Yotam", ""], ["Chakravarti", "Purvasha", ""], ["Qin", "Jining", ""]]}, {"id": "1704.08227", "submitter": "Rahul Kidambi", "authors": "Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli and\n  Aaron Sidford", "title": "Accelerating Stochastic Gradient Descent For Least Squares Regression", "comments": "54 pages, 3 figures, 1 table; updated acknowledgements, minor title\n  change. Paper appeared in the proceedings of the Conference on Learning\n  Theory (COLT), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is widespread sentiment that it is not possible to effectively utilize\nfast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy\nball) for the purposes of stochastic optimization due to their instability and\nerror accumulation, a notion made precise in d'Aspremont 2008 and Devolder,\nGlineur, and Nesterov 2014. This work considers these issues for the special\ncase of stochastic approximation for the least squares regression problem, and\nour main result refutes the conventional wisdom by showing that acceleration\ncan be made robust to statistical errors. In particular, this work introduces\nan accelerated stochastic gradient method that provably achieves the minimax\noptimal statistical risk faster than stochastic gradient descent. Critical to\nthe analysis is a sharp characterization of accelerated stochastic gradient\ndescent as a stochastic process. We hope this characterization gives insights\ntowards the broader question of designing simple and effective accelerated\nstochastic methods for more general convex and non-convex optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:30:27 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 18:11:32 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Jain", "Prateek", ""], ["Kakade", "Sham M.", ""], ["Kidambi", "Rahul", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1704.08243", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, Devi Parikh", "title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) has received a lot of attention over the past\ncouple of years. A number of deep learning models have been proposed for this\ntask. However, it has been shown that these models are heavily driven by\nsuperficial correlations in the training data and lack compositionality -- the\nability to answer questions about unseen compositions of seen concepts. This\ncompositionality is desirable and central to intelligence. In this paper, we\npropose a new setting for Visual Question Answering where the test\nquestion-answer pairs are compositionally novel compared to training\nquestion-answer pairs. To facilitate developing models under this setting, we\npresent a new compositional split of the VQA v1.0 dataset, which we call\nCompositional VQA (C-VQA). We analyze the distribution of questions and answers\nin the C-VQA splits. Finally, we evaluate several existing VQA models under\nthis new setting and show that the performances of these models degrade by a\nsignificant amount compared to the original VQA setting.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:57:59 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Kembhavi", "Aniruddha", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1704.08246", "submitter": "Zhao Song", "authors": "Zhao Song, David P. Woodruff, Peilin Zhong", "title": "Relative Error Tensor Low Rank Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider relative error low rank approximation of $tensors$ with respect\nto the Frobenius norm: given an order-$q$ tensor $A \\in\n\\mathbb{R}^{\\prod_{i=1}^q n_i}$, output a rank-$k$ tensor $B$ for which\n$\\|A-B\\|_F^2 \\leq (1+\\epsilon)$OPT, where OPT $= \\inf_{\\textrm{rank-}k~A'}\n\\|A-A'\\|_F^2$. Despite the success on obtaining relative error low rank\napproximations for matrices, no such results were known for tensors. One\nstructural issue is that there may be no rank-$k$ tensor $A_k$ achieving the\nabove infinum. Another, computational issue, is that an efficient relative\nerror low rank approximation algorithm for tensors would allow one to compute\nthe rank of a tensor, which is NP-hard. We bypass these issues via (1)\nbicriteria and (2) parameterized complexity solutions:\n  (1) We give an algorithm which outputs a rank $k' = O((k/\\epsilon)^{q-1})$\ntensor $B$ for which $\\|A-B\\|_F^2 \\leq (1+\\epsilon)$OPT in $nnz(A) + n \\cdot\n\\textrm{poly}(k/\\epsilon)$ time in the real RAM model. Here $nnz(A)$ is the\nnumber of non-zero entries in $A$.\n  (2) We give an algorithm for any $\\delta >0$ which outputs a rank $k$ tensor\n$B$ for which $\\|A-B\\|_F^2 \\leq (1+\\epsilon)$OPT and runs in $ ( nnz(A) + n\n\\cdot \\textrm{poly}(k/\\epsilon) + \\exp(k^2/\\epsilon) ) \\cdot n^\\delta$ time in\nthe unit cost RAM model.\n  For outputting a rank-$k$ tensor, or even a bicriteria solution with\nrank-$Ck$ for a certain constant $C > 1$, we show a $2^{\\Omega(k^{1-o(1)})}$\ntime lower bound under the Exponential Time Hypothesis.\n  Our results give the first relative error low rank approximations for tensors\nfor a large number of robust error measures for which nothing was known, as\nwell as column row and tube subset selection. We also obtain new results for\nmatrices, such as $nnz(A)$-time CUR decompositions, improving previous\n$nnz(A)\\log n$-time algorithms, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:59:11 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 20:25:01 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Song", "Zhao", ""], ["Woodruff", "David P.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1704.08265", "submitter": "Chunxia Zhang", "authors": "Chunxia Zhang, Yilei Wu and Mu Zhu", "title": "Pruning variable selection ensembles", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of variable selection, ensemble learning has gained increasing\ninterest due to its great potential to improve selection accuracy and to reduce\nfalse discovery rate. A novel ordering-based selective ensemble learning\nstrategy is designed in this paper to obtain smaller but more accurate\nensembles. In particular, a greedy sorting strategy is proposed to rearrange\nthe order by which the members are included into the integration process.\nThrough stopping the fusion process early, a smaller subensemble with higher\nselection accuracy can be obtained. More importantly, the sequential inclusion\ncriterion reveals the fundamental strength-diversity trade-off among ensemble\nmembers. By taking stability selection (abbreviated as StabSel) as an example,\nsome experiments are conducted with both simulated and real-world data to\nexamine the performance of the novel algorithm. Experimental results\ndemonstrate that pruned StabSel generally achieves higher selection accuracy\nand lower false discovery rates than StabSel and several other benchmark\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 18:01:10 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Zhang", "Chunxia", ""], ["Wu", "Yilei", ""], ["Zhu", "Mu", ""]]}, {"id": "1704.08303", "submitter": "Mehmet A. S\\\"uzen PhD", "authors": "Mehmet S\\\"uzen, Cornelius Weber and Joan J. Cerd\\`a", "title": "Spectral Ergodicity in Deep Learning Architectures via Surrogate Random\n  Matrices", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": "10.5281/zenodo.822411 and 10.5281/zenodo.579642", "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a novel method to quantify spectral ergodicity for random\nmatrices is presented. The new methodology combines approaches rooted in the\nmetrics of Thirumalai-Mountain (TM) and Kullbach-Leibler (KL) divergence. The\nmethod is applied to a general study of deep and recurrent neural networks via\nthe analysis of random matrix ensembles mimicking typical weight matrices of\nthose systems. In particular, we examine circular random matrix ensembles:\ncircular unitary ensemble (CUE), circular orthogonal ensemble (COE), and\ncircular symplectic ensemble (CSE). Eigenvalue spectra and spectral ergodicity\nare computed for those ensembles as a function of network size. It is observed\nthat as the matrix size increases the level of spectral ergodicity of the\nensemble rises, i.e., the eigenvalue spectra obtained for a single realisation\nat random from the ensemble is closer to the spectra obtained averaging over\nthe whole ensemble. Based on previous results we conjecture that success of\ndeep learning architectures is strongly bound to the concept of spectral\nergodicity. The method to compute spectral ergodicity proposed in this work\ncould be used to optimise the size and architecture of deep as well as\nrecurrent neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:26:08 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 21:47:38 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 09:57:03 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["S\u00fczen", "Mehmet", ""], ["Weber", "Cornelius", ""], ["Cerd\u00e0", "Joan J.", ""]]}, {"id": "1704.08305", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers", "title": "Limits of End-to-End Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end learning refers to training a possibly complex learning system by\napplying gradient-based learning to the system as a whole. End-to-end learning\nsystem is specifically designed so that all modules are differentiable. In\neffect, not only a central learning machine, but also all \"peripheral\" modules\nlike representation learning and memory formation are covered by a holistic\nlearning process. The power of end-to-end learning has been demonstrated on\nmany tasks, like playing a whole array of Atari video games with a single\narchitecture. While pushing for solutions to more challenging tasks, network\narchitectures keep growing more and more complex.\n  In this paper we ask the question whether and to what extent end-to-end\nlearning is a future-proof technique in the sense of scaling to complex and\ndiverse data processing architectures. We point out potential inefficiencies,\nand we argue in particular that end-to-end learning does not make optimal use\nof the modular design of present neural networks. Our surprisingly simple\nexperiments demonstrate these inefficiencies, up to the complete breakdown of\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 19:12:37 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Glasmachers", "Tobias", ""]]}, {"id": "1704.08361", "submitter": "David Von Dollen", "authors": "David Von Dollen", "title": "Identifying Similarities in Epileptic Patients for Drug Resistance\n  Prediction", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, approximately 30% of epileptic patients treated with antiepileptic\ndrugs (AEDs) remain resistant to treatment (known as refractory patients). This\nproject seeks to understand the underlying similarities in refractory patients\nvs. other epileptic patients, identify features contributing to drug resistance\nacross underlying phenotypes for refractory patients, and develop predictive\nmodels for drug resistance in epileptic patients. In this study, epileptic\npatient data was examined to attempt to observe discernable similarities or\ndifferences in refractory patients (case) and other non-refractory patients\n(control) to map underlying mechanisms in causality. For the first part of the\nstudy, unsupervised algorithms such as Kmeans, Spectral Clustering, and\nGaussian Mixture Models were used to examine patient features projected into a\nlower dimensional space. Results from this study showed a high degree of\nnon-linearity in the underlying feature space. For the second part of this\nstudy, classification algorithms such as Logistic Regression, Gradient Boosted\nDecision Trees, and SVMs, were tested on the reduced-dimensionality features,\nwith accuracy results of 0.83(+/-0.3) testing using 7 fold cross validation.\nObservations of test results indicate using a radial basis function kernel PCA\nto reduce features ingested by a Gradient Boosted Decision Tree Ensemble lead\nto gains in improved accuracy in mapping a binary decision to highly non-linear\nfeatures collected from epileptic patients.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 21:53:16 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Von Dollen", "David", ""]]}, {"id": "1704.08362", "submitter": "Ge Wang", "authors": "Fenglei Fan, Wenxiang Cong, Ge Wang", "title": "A New Type of Neurons for Machine Learning", "comments": "5 pages, 8 figures, 11 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the use of an artificial neural network is the\nmainstream approach. Such a network consists of layers of neurons. These\nneurons are of the same type characterized by the two features: (1) an inner\nproduct of an input vector and a matching weighting vector of trainable\nparameters and (2) a nonlinear excitation function. Here we investigate the\npossibility of replacing the inner product with a quadratic function of the\ninput vector, thereby upgrading the 1st order neuron to the 2nd order neuron,\nempowering individual neurons, and facilitating the optimization of neural\nnetworks. Also, numerical examples are provided to illustrate the feasibility\nand merits of the 2nd order neurons. Finally, further topics are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 22:02:25 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Fan", "Fenglei", ""], ["Cong", "Wenxiang", ""], ["Wang", "Ge", ""]]}, {"id": "1704.08383", "submitter": "Qingyang Li", "authors": "Qingyang Li, Dajiang Zhu, Jie Zhang, Derrek Paul Hibar, Neda\n  Jahanshad, Yalin Wang, Jieping Ye, Paul M. Thompson, Jie Wang", "title": "Large-scale Feature Selection of Risk Genetic Factors for Alzheimer's\n  Disease via Distributed Group Lasso Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWAS) have achieved great success in the\ngenetic study of Alzheimer's disease (AD). Collaborative imaging genetics\nstudies across different research institutions show the effectiveness of\ndetecting genetic risk factors. However, the high dimensionality of GWAS data\nposes significant challenges in detecting risk SNPs for AD. Selecting relevant\nfeatures is crucial in predicting the response variable. In this study, we\npropose a novel Distributed Feature Selection Framework (DFSF) to conduct the\nlarge-scale imaging genetics studies across multiple institutions. To speed up\nthe learning process, we propose a family of distributed group Lasso screening\nrules to identify irrelevant features and remove them from the optimization.\nThen we select the relevant group features by performing the group Lasso\nfeature selection process in a sequence of parameters. Finally, we employ the\nstability selection to rank the top risk SNPs that might help detect the early\nstage of AD. To the best of our knowledge, this is the first distributed\nfeature selection model integrated with group Lasso feature selection as well\nas detecting the risk genetic factors across multiple research institutions\nsystem. Empirical studies are conducted on 809 subjects with 5.9 million SNPs\nwhich are distributed across several individual institutions, demonstrating the\nefficiency and effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 00:02:34 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Li", "Qingyang", ""], ["Zhu", "Dajiang", ""], ["Zhang", "Jie", ""], ["Hibar", "Derrek Paul", ""], ["Jahanshad", "Neda", ""], ["Wang", "Yalin", ""], ["Ye", "Jieping", ""], ["Thompson", "Paul M.", ""], ["Wang", "Jie", ""]]}, {"id": "1704.08424", "submitter": "Ben Athiwaratkun", "authors": "Ben Athiwaratkun, Andrew Gordon Wilson", "title": "Multimodal Word Distributions", "comments": "This paper also appears at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings provide point representations of words containing useful\nsemantic information. We introduce multimodal word distributions formed from\nGaussian mixtures, for multiple word meanings, entailment, and rich uncertainty\ninformation. To learn these distributions, we propose an energy-based\nmax-margin objective. We show that the resulting approach captures uniquely\nexpressive semantic information, and outperforms alternatives, such as word2vec\nskip-grams, and Gaussian embeddings, on benchmark datasets such as word\nsimilarity and entailment.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 03:59:54 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 17:56:33 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Athiwaratkun", "Ben", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1704.08432", "submitter": "Sunyoung Kwon", "authors": "Sunyoung Kwon, Sungroh Yoon", "title": "DeepCCI: End-to-end Deep Learning for Chemical-Chemical Interaction\n  Prediction", "comments": "ACM-BCB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical-chemical interaction (CCI) plays a key role in predicting candidate\ndrugs, toxicity, therapeutic effects, and biological functions. In various\ntypes of chemical analyses, computational approaches are often required due to\nthe amount of data that needs to be handled. The recent remarkable growth and\noutstanding performance of deep learning have attracted considerable research\nattention. However,even in state-of-the-art drug analysis methods, deep\nlearning continues to be used only as a classifier, although deep learning is\ncapable of not only simple classification but also automated feature\nextraction. In this paper, we propose the first end-to-end learning method for\nCCI, named DeepCCI. Hidden features are derived from a simplified molecular\ninput line entry system (SMILES), which is a string notation representing the\nchemical structure, instead of learning from crafted features. To discover\nhidden representations for the SMILES strings, we use convolutional neural\nnetworks (CNNs). To guarantee the commutative property for homogeneous\ninteraction, we apply model sharing and hidden representation merging\ntechniques. The performance of DeepCCI was compared with a plain deep\nclassifier and conventional machine learning methods. The proposed DeepCCI\nshowed the best performance in all seven evaluation metrics used. In addition,\nthe commutative property was experimentally validated. The automatically\nextracted features through end-to-end SMILES learning alleviates the\nsignificant efforts required for manual feature engineering. It is expected to\nimprove prediction performance, in drug analyses.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 05:03:08 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 04:28:17 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 08:19:08 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Kwon", "Sunyoung", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1704.08443", "submitter": "Ho Bae", "authors": "Ho Bae, Byunghan Lee, Sunyoung Kwon, Sungroh Yoon", "title": "DNA Steganalysis Using Deep Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in next-generation sequencing technologies have facilitated\nthe use of deoxyribonucleic acid (DNA) as a novel covert channels in\nsteganography. There are various methods that exist in other domains to detect\nhidden messages in conventional covert channels. However, they have not been\napplied to DNA steganography. The current most common detection approaches,\nnamely frequency analysis-based methods, often overlook important signals when\ndirectly applied to DNA steganography because those methods depend on the\ndistribution of the number of sequence characters. To address this limitation,\nwe propose a general sequence learning-based DNA steganalysis framework. The\nproposed approach learns the intrinsic distribution of coding and non-coding\nsequences and detects hidden messages by exploiting distribution variations\nafter hiding these messages. Using deep recurrent neural networks (RNNs), our\nframework identifies the distribution variations by using the classification\nscore to predict whether a sequence is to be a coding or non-coding sequence.\nWe compare our proposed method to various existing methods and biological\nsequence analysis methods implemented on top of our framework. According to our\nexperimental results, our approach delivers a robust detection performance\ncompared to other tools.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 06:16:30 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 14:20:46 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 04:04:58 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Bae", "Ho", ""], ["Lee", "Byunghan", ""], ["Kwon", "Sunyoung", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1704.08488", "submitter": "Dieter Hendricks", "authors": "Dieter Hendricks and Stephen J. Roberts", "title": "Optimal client recommendation for market makers in illiquid financial\n  products", "comments": "12 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of liquidity provision in financial markets can result in\nprolonged exposure to illiquid instruments for market makers. In this case,\nwhere a proprietary position is not desired, pro-actively targeting the right\nclient who is likely to be interested can be an effective means to offset this\nposition, rather than relying on commensurate interest arising through natural\ndemand. In this paper, we consider the inference of a client profile for the\npurpose of corporate bond recommendation, based on typical recorded information\navailable to the market maker. Given a historical record of corporate bond\ntransactions and bond meta-data, we use a topic-modelling analogy to develop a\nprobabilistic technique for compiling a curated list of client recommendations\nfor a particular bond that needs to be traded, ranked by probability of\ninterest. We show that a model based on Latent Dirichlet Allocation offers\npromising performance to deliver relevant recommendations for sales traders.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 09:28:50 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Hendricks", "Dieter", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1704.08504", "submitter": "Szu-Wei Fu", "authors": "Szu-Wei Fu, Ting-yao Hu, Yu Tsao, and Xugang Lu", "title": "Complex spectrogram enhancement by convolutional neural network with\n  multi-metrics learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to address two issues existing in the current speech\nenhancement methods: 1) the difficulty of phase estimations; 2) a single\nobjective function cannot consider multiple metrics simultaneously. To solve\nthe first problem, we propose a novel convolutional neural network (CNN) model\nfor complex spectrogram enhancement, namely estimating clean real and imaginary\n(RI) spectrograms from noisy ones. The reconstructed RI spectrograms are\ndirectly used to synthesize enhanced speech waveforms. In addition, since\nlog-power spectrogram (LPS) can be represented as a function of RI\nspectrograms, its reconstruction is also considered as another target. Thus a\nunified objective function, which combines these two targets (reconstruction of\nRI spectrograms and LPS), is equivalent to simultaneously optimizing two\ncommonly used objective metrics: segmental signal-to-noise ratio (SSNR) and\nlogspectral distortion (LSD). Therefore, the learning process is called\nmulti-metrics learning (MML). Experimental results confirm the effectiveness of\nthe proposed CNN with RI spectrograms and MML in terms of improved standardized\nevaluation metrics on a speech enhancement task.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 11:01:33 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 14:49:27 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Fu", "Szu-Wei", ""], ["Hu", "Ting-yao", ""], ["Tsao", "Yu", ""], ["Lu", "Xugang", ""]]}, {"id": "1704.08533", "submitter": "Dongrui Wu", "authors": "Dongrui Wu and Brent J. Lance and Vernon J. Lawhern and Stephen Gordon\n  and Tzyy-Ping Jung and Chin-Teng Lin", "title": "EEG-Based User Reaction Time Estimation Using Riemannian Geometry\n  Features", "comments": "arXiv admin note: text overlap with arXiv:1702.02914", "journal-ref": "IEEE Trans. on Neural Systems and Rehabilitation Engineering,\n  25(11), pp. 2157-2168, 2017", "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Riemannian geometry has been successfully used in many brain-computer\ninterface (BCI) classification problems and demonstrated superior performance.\nIn this paper, for the first time, it is applied to BCI regression problems, an\nimportant category of BCI applications. More specifically, we propose a new\nfeature extraction approach for Electroencephalogram (EEG) based BCI regression\nproblems: a spatial filter is first used to increase the signal quality of the\nEEG trials and also to reduce the dimensionality of the covariance matrices,\nand then Riemannian tangent space features are extracted. We validate the\nperformance of the proposed approach in reaction time estimation from EEG\nsignals measured in a large-scale sustained-attention psychomotor vigilance\ntask, and show that compared with the traditional powerband features, the\ntangent space features can reduce the root mean square estimation error by\n4.30-8.30%, and increase the estimation correlation coefficient by 6.59-11.13%.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 12:30:05 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Wu", "Dongrui", ""], ["Lance", "Brent J.", ""], ["Lawhern", "Vernon J.", ""], ["Gordon", "Stephen", ""], ["Jung", "Tzyy-Ping", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "1704.08676", "submitter": "Daniele Ramazzotti", "authors": "Stefano Beretta and Mauro Castelli and Ivo Goncalves and Roberto\n  Henriques and Daniele Ramazzotti", "title": "Learning the structure of Bayesian Networks: A quantitative assessment\n  of the effect of different algorithmic schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging tasks when adopting Bayesian Networks (BNs) is\nthe one of learning their structure from data. This task is complicated by the\nhuge search space of possible solutions, and by the fact that the problem is\nNP-hard. Hence, full enumeration of all the possible solutions is not always\nfeasible and approximations are often required. However, to the best of our\nknowledge, a quantitative analysis of the performance and characteristics of\nthe different heuristics to solve this problem has never been done before.\n  For this reason, in this work, we provide a detailed comparison of many\ndifferent state-of-the-arts methods for structural learning on simulated data\nconsidering both BNs with discrete and continuous variables, and with different\nrates of noise in the data. In particular, we investigate the performance of\ndifferent widespread scores and algorithmic approaches proposed for the\ninference and the statistical pitfalls within them.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 17:40:22 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 20:16:37 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Beretta", "Stefano", ""], ["Castelli", "Mauro", ""], ["Goncalves", "Ivo", ""], ["Henriques", "Roberto", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1704.08683", "submitter": "Hongyang Zhang", "authors": "Maria-Florina Balcan and Yingyu Liang and David P. Woodruff and\n  Hongyang Zhang", "title": "Matrix Completion and Related Problems via Strong Duality", "comments": "37 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the strong duality of non-convex matrix factorization\nproblems: we show that under certain dual conditions, these problems and its\ndual have the same optimum. This has been well understood for convex\noptimization, but little was known for non-convex problems. We propose a novel\nanalytical framework and show that under certain dual conditions, the optimal\nsolution of the matrix factorization program is the same as its bi-dual and\nthus the global optimality of the non-convex program can be achieved by solving\nits bi-dual which is convex. These dual conditions are satisfied by a wide\nclass of matrix factorization problems, although matrix factorization problems\nare hard to solve in full generality. This analytical framework may be of\nindependent interest to non-convex optimization more broadly.\n  We apply our framework to two prototypical matrix factorization problems:\nmatrix completion and robust Principal Component Analysis (PCA). These are\nexamples of efficiently recovering a hidden matrix given limited reliable\nobservations of it. Our framework shows that exact recoverability and strong\nduality hold with nearly-optimal sample complexity guarantees for matrix\ncompletion and robust PCA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 17:54:46 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 17:51:47 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 11:39:03 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 14:48:14 GMT"}, {"version": "v5", "created": "Wed, 25 Apr 2018 14:14:39 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Liang", "Yingyu", ""], ["Woodruff", "David P.", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1704.08715", "submitter": "Lev Utkin", "authors": "Lev V. Utkin and Mikhail A. Ryabinin", "title": "A Siamese Deep Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Siamese Deep Forest (SDF) is proposed in the paper. It is based on the Deep\nForest or gcForest proposed by Zhou and Feng and can be viewed as a gcForest\nmodification. It can be also regarded as an alternative to the well-known\nSiamese neural networks. The SDF uses a modified training set consisting of\nconcatenated pairs of vectors. Moreover, it defines the class distributions in\nthe deep forest as the weighted sum of the tree class probabilities such that\nthe weights are determined in order to reduce distances between similar pairs\nand to increase them between dissimilar points. We show that the weights can be\nobtained by solving a quadratic optimization problem. The SDF aims to prevent\noverfitting which takes place in neural networks when only limited training\ndata are available. The numerical experiments illustrate the proposed distance\nmetric method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 18:51:41 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Utkin", "Lev V.", ""], ["Ryabinin", "Mikhail A.", ""]]}, {"id": "1704.08756", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Tomasz Kajdanowicz", "title": "A Network Perspective on Stratification of Multi-Label Data", "comments": "submitted for ECML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, we have witnessed the development of multi-label\nclassification methods which utilize the structure of the label space in a\ndivide and conquer approach to improve classification performance and allow\nlarge data sets to be classified efficiently. Yet most of the available data\nsets have been provided in train/test splits that did not account for\nmaintaining a distribution of higher-order relationships between labels among\nsplits or folds. We present a new approach to stratifying multi-label data for\nclassification purposes based on the iterative stratification approach proposed\nby Sechidis et. al. in an ECML PKDD 2011 paper. Our method extends the\niterative approach to take into account second-order relationships between\nlabels. Obtained results are evaluated using statistical properties of obtained\nstrata as presented by Sechidis. We also propose new statistical measures\nrelevant to second-order quality: label pairs distribution, the percentage of\nlabel pairs without positive evidence in folds and label pair - fold pairs that\nhave no positive evidence for the label pair. We verify the impact of new\nmethods on classification performance of Binary Relevance, Label Powerset and a\nfast greedy community detection based label space partitioning classifier.\nRandom Forests serve as base classifiers. We check the variation of the number\nof communities obtained per fold, and the stability of their modularity score.\nSecond-Order Iterative Stratification is compared to standard k-fold, label\nset, and iterative stratification. The proposed approach lowers the variance of\nclassification quality, improves label pair oriented measures and example\ndistribution while maintaining a competitive quality in label-oriented\nmeasures. We also witness an increase in stability of network characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 21:43:53 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Kajdanowicz", "Tomasz", ""]]}, {"id": "1704.08772", "submitter": "Grigorios Chrysos", "authors": "Grigorios G. Chrysos, Stefanos Zafeiriou", "title": "Deep Face Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind deblurring consists a long studied task, however the outcomes of\ngeneric methods are not effective in real world blurred images. Domain-specific\nmethods for deblurring targeted object categories, e.g. text or faces,\nfrequently outperform their generic counterparts, hence they are attracting an\nincreasing amount of attention. In this work, we develop such a domain-specific\nmethod to tackle deblurring of human faces, henceforth referred to as face\ndeblurring. Studying faces is of tremendous significance in computer vision,\nhowever face deblurring has yet to demonstrate some convincing results. This\ncan be partly attributed to the combination of i) poor texture and ii) highly\nstructure shape that yield the contour/gradient priors (that are typically\nused) sub-optimal. In our work instead of making assumptions over the prior, we\nadopt a learning approach by inserting weak supervision that exploits the\nwell-documented structure of the face. Namely, we utilise a deep network to\nperform the deblurring and employ a face alignment technique to pre-process\neach face. We additionally surpass the requirement of the deep network for\nthousands training samples, by introducing an efficient framework that allows\nthe generation of a large dataset. We utilised this framework to create 2MF2, a\ndataset of over two million frames. We conducted experiments with real world\nblurred facial images and report that our method returns a result close to the\nsharp natural latent image.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 23:01:45 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 07:45:36 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Chrysos", "Grigorios G.", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1704.08783", "submitter": "Gunwoong Park Gunwoong Park", "authors": "Gunwoong Park, Garvesh Raskutti", "title": "Learning Quadratic Variance Function (QVF) DAG models via OverDispersion\n  Scoring (ODS)", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning DAG or Bayesian network models is an important problem in\nmulti-variate causal inference. However, a number of challenges arises in\nlearning large-scale DAG models including model identifiability and\ncomputational complexity since the space of directed graphs is huge. In this\npaper, we address these issues in a number of steps for a broad class of DAG\nmodels where the noise or variance is signal-dependent. Firstly we introduce a\nnew class of identifiable DAG models, where each node has a distribution where\nthe variance is a quadratic function of the mean (QVF DAG models). Our QVF DAG\nmodels include many interesting classes of distributions such as Poisson,\nBinomial, Geometric, Exponential, Gamma and many other distributions in which\nthe noise variance depends on the mean. We prove that this class of QVF DAG\nmodels is identifiable, and introduce a new algorithm, the OverDispersion\nScoring (ODS) algorithm, for learning large-scale QVF DAG models. Our algorithm\nis based on firstly learning the moralized or undirected graphical model\nrepresentation of the DAG to reduce the DAG search-space, and then exploiting\nthe quadratic variance property to learn the causal ordering. We show through\ntheoretical results and simulations that our algorithm is statistically\nconsistent in the high-dimensional p>n setting provided that the degree of the\nmoralized graph is bounded and performs well compared to state-of-the-art\nDAG-learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 01:36:53 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Park", "Gunwoong", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "1704.08792", "submitter": "Renato Negrinho", "authors": "Renato Negrinho, Geoff Gordon", "title": "DeepArchitect: Automatically Designing and Training Deep Architectures", "comments": "12 pages, 10 figures. Code available at\n  https://github.com/negrinho/deep_architect. See\n  http://www.cs.cmu.edu/~negrinho/ for more info. In submission to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning, performance is strongly affected by the choice of\narchitecture and hyperparameters. While there has been extensive work on\nautomatic hyperparameter optimization for simple spaces, complex spaces such as\nthe space of deep architectures remain largely unexplored. As a result, the\nchoice of architecture is done manually by the human expert through a slow\ntrial and error process guided mainly by intuition. In this paper we describe a\nframework for automatically designing and training deep models. We propose an\nextensible and modular language that allows the human expert to compactly\nrepresent complex search spaces over architectures and their hyperparameters.\nThe resulting search spaces are tree-structured and therefore easy to traverse.\nModels can be automatically compiled to computational graphs once values for\nall hyperparameters have been chosen. We can leverage the structure of the\nsearch space to introduce different model search algorithms, such as random\nsearch, Monte Carlo tree search (MCTS), and sequential model-based optimization\n(SMBO). We present experiments comparing the different algorithms on CIFAR-10\nand show that MCTS and SMBO outperform random search. In addition, these\nexperiments show that our framework can be used effectively for model\ndiscovery, as it is possible to describe expressive search spaces and discover\ncompetitive models without much effort from the human expert. Code for our\nframework and experiments has been made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 02:48:38 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Negrinho", "Renato", ""], ["Gordon", "Geoff", ""]]}, {"id": "1704.08797", "submitter": "Sarfaraz Hussein", "authors": "Sarfaraz Hussein, Kunlin Cao, Qi Song, Ulas Bagci", "title": "Risk Stratification of Lung Nodules Using 3D CNN-Based Multi-task\n  Learning", "comments": "Accepted for publication at Information Processing in Medical Imaging\n  (IPMI) 2017", "journal-ref": "Information Processing in Medical Imaging. IPMI 2017. Lecture\n  Notes in Computer Science, vol 10265. Springer, Cham", "doi": "10.1007/978-3-319-59050-9_20", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk stratification of lung nodules is a task of primary importance in lung\ncancer diagnosis. Any improvement in robust and accurate nodule\ncharacterization can assist in identifying cancer stage, prognosis, and\nimproving treatment planning. In this study, we propose a 3D Convolutional\nNeural Network (CNN) based nodule characterization strategy. With a completely\n3D approach, we utilize the volumetric information from a CT scan which would\nbe otherwise lost in the conventional 2D CNN based approaches. In order to\naddress the need for a large amount for training data for CNN, we resort to\ntransfer learning to obtain highly discriminative features. Moreover, we also\nacquire the task dependent feature representation for six high-level nodule\nattributes and fuse this complementary information via a Multi-task learning\n(MTL) framework. Finally, we propose to incorporate potential disagreement\namong radiologists while scoring different nodule attributes in a graph\nregularized sparse multi-task learning. We evaluated our proposed approach on\none of the largest publicly available lung nodule datasets comprising 1018\nscans and obtained state-of-the-art results in regressing the malignancy\nscores.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 03:32:54 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Hussein", "Sarfaraz", ""], ["Cao", "Kunlin", ""], ["Song", "Qi", ""], ["Bagci", "Ulas", ""]]}, {"id": "1704.08803", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, W. Bruce\n  Croft", "title": "Neural Ranking Models with Weak Supervision", "comments": "In proceedings of The 40th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the impressive improvements achieved by unsupervised deep neural\nnetworks in computer vision and NLP tasks, such improvements have not yet been\nobserved in ranking for information retrieval. The reason may be the complexity\nof the ranking problem, as it is not obvious how to learn from queries and\ndocuments when no supervised signal is available. Hence, in this paper, we\npropose to train a neural ranking model using weak supervision, where labels\nare obtained automatically without human annotators or any external resources\n(e.g., click data). To this aim, we use the output of an unsupervised ranking\nmodel, such as BM25, as a weak supervision signal. We further train a set of\nsimple yet effective ranking models based on feed-forward neural networks. We\nstudy their effectiveness under various learning scenarios (point-wise and\npair-wise models) and using different input representations (i.e., from\nencoding query-document pairs into dense/sparse vectors to using word embedding\nrepresentation). We train our networks using tens of millions of training\ninstances and evaluate it on two standard collections: a homogeneous news\ncollection(Robust) and a heterogeneous large-scale web collection (ClueWeb).\nOur experiments indicate that employing proper objective functions and letting\nthe networks to learn the input representation based on weakly supervised data\nleads to impressive performance, with over 13% and 35% MAP improvements over\nthe BM25 model on the Robust and the ClueWeb collections. Our findings also\nsuggest that supervised neural ranking models can greatly benefit from\npre-training on large amounts of weakly labeled data that can be easily\nobtained from unsupervised IR models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 04:08:47 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 11:58:34 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Zamani", "Hamed", ""], ["Severyn", "Aliaksei", ""], ["Kamps", "Jaap", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1704.08818", "submitter": "Yong Xia", "authors": "Benteng Ma, Yong Xia", "title": "A Tribe Competition-Based Genetic Algorithm for Feature Selection in\n  Pattern Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2017.04.042", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection has always been a critical step in pattern recognition, in\nwhich evolutionary algorithms, such as the genetic algorithm (GA), are most\ncommonly used. However, the individual encoding scheme used in various GAs\nwould either pose a bias on the solution or require a pre-specified number of\nfeatures, and hence may lead to less accurate results. In this paper, a tribe\ncompetition-based genetic algorithm (TCbGA) is proposed for feature selection\nin pattern classification. The population of individuals is divided into\nmultiple tribes, and the initialization and evolutionary operations are\nmodified to ensure that the number of selected features in each tribe follows a\nGaussian distribution. Thus each tribe focuses on exploring a specific part of\nthe solution space. Meanwhile, tribe competition is introduced to the evolution\nprocess, which allows the winning tribes, which produce better individuals, to\nenlarge their sizes, i.e. having more individuals to search their parts of the\nsolution space. This algorithm, therefore, avoids the bias on solutions and\nrequirement of a pre-specified number of features. We have evaluated our\nalgorithm against several state-of-the-art feature selection approaches on 20\nbenchmark datasets. Our results suggest that the proposed TCbGA algorithm can\nidentify the optimal feature subset more effectively and produce more accurate\npattern classification.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 06:25:50 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Ma", "Benteng", ""], ["Xia", "Yong", ""]]}, {"id": "1704.08829", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Rong Zhou, and Nesreen K. Ahmed", "title": "Deep Feature Learning for Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general graph representation learning framework called\nDeepGL for learning deep node and edge representations from large (attributed)\ngraphs. In particular, DeepGL begins by deriving a set of base features (e.g.,\ngraphlet features) and automatically learns a multi-layered hierarchical graph\nrepresentation where each successive layer leverages the output from the\nprevious layer to learn features of a higher-order. Contrary to previous work,\nDeepGL learns relational functions (each representing a feature) that\ngeneralize across-networks and therefore useful for graph-based transfer\nlearning tasks. Moreover, DeepGL naturally supports attributed graphs, learns\ninterpretable features, and is space-efficient (by learning sparse feature\nvectors). In addition, DeepGL is expressive, flexible with many interchangeable\ncomponents, efficient with a time complexity of $\\mathcal{O}(|E|)$, and\nscalable for large networks via an efficient parallel implementation. Compared\nwith the state-of-the-art method, DeepGL is (1) effective for across-network\ntransfer learning tasks and attributed graph representation learning, (2)\nspace-efficient requiring up to 6x less memory, (3) fast with up to 182x\nspeedup in runtime performance, and (4) accurate with an average improvement of\n20% or more on many learning tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 07:31:11 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 10:31:05 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""], ["Ahmed", "Nesreen K.", ""]]}, {"id": "1704.08847", "submitter": "Moustapha Cisse", "authors": "Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin,\n  Nicolas Usunier", "title": "Parseval Networks: Improving Robustness to Adversarial Examples", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Parseval networks, a form of deep neural networks in which the\nLipschitz constant of linear, convolutional and aggregation layers is\nconstrained to be smaller than 1. Parseval networks are empirically and\ntheoretically motivated by an analysis of the robustness of the predictions\nmade by deep neural networks when their input is subject to an adversarial\nperturbation. The most important feature of Parseval networks is to maintain\nweight matrices of linear and convolutional layers to be (approximately)\nParseval tight frames, which are extensions of orthogonal matrices to\nnon-square matrices. We describe how these constraints can be maintained\nefficiently during SGD. We show that Parseval networks match the\nstate-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House\nNumbers (SVHN) while being more robust than their vanilla counterpart against\nadversarial examples. Incidentally, Parseval networks also tend to train faster\nand make a better usage of the full capacity of the networks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 08:43:55 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 01:11:21 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Cisse", "Moustapha", ""], ["Bojanowski", "Piotr", ""], ["Grave", "Edouard", ""], ["Dauphin", "Yann", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1704.08863", "submitter": "Siddharth Krishna Kumar", "authors": "Siddharth Krishna Kumar", "title": "On weight initialization in deep neural networks", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A proper initialization of the weights in a neural network is critical to its\nconvergence. Current insights into weight initialization come primarily from\nlinear activation functions. In this paper, I develop a theory for weight\ninitializations with non-linear activations. First, I derive a general weight\ninitialization strategy for any neural network using activation functions\ndifferentiable at 0. Next, I derive the weight initialization strategy for the\nRectified Linear Unit (RELU), and provide theoretical insights into why the\nXavier initialization is a poor choice with RELU activations. My analysis\nprovides a clear demonstration of the role of non-linearities in determining\nthe proper weight initializations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 09:57:52 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 22:43:10 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Kumar", "Siddharth Krishna", ""]]}, {"id": "1704.08883", "submitter": "Seyed Sajad Mousavi", "authors": "Seyed Sajad Mousavi, Michael Schukat, Enda Howley", "title": "Traffic Light Control Using Deep Policy-Gradient and Value-Function\n  Based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in combining deep neural network architectures with\nreinforcement learning techniques have shown promising potential results in\nsolving complex control problems with high dimensional state and action spaces.\nInspired by these successes, in this paper, we build two kinds of reinforcement\nlearning algorithms: deep policy-gradient and value-function based agents which\ncan predict the best possible traffic signal for a traffic intersection. At\neach time step, these adaptive traffic light control agents receive a snapshot\nof the current state of a graphical traffic simulator and produce control\nsignals. The policy-gradient based agent maps its observation directly to the\ncontrol signal, however the value-function based agent first estimates values\nfor all legal control signals. The agent then selects the optimal control\naction with the highest value. Our methods show promising results in a traffic\nnetwork simulated in the SUMO traffic simulator, without suffering from\ninstability issues during the training process.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 11:44:42 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 14:45:56 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Mousavi", "Seyed Sajad", ""], ["Schukat", "Michael", ""], ["Howley", "Enda", ""]]}, {"id": "1704.08913", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Jie Chen, C\\'edric Richard", "title": "Adaptation and learning over networks for nonlinear system modeling", "comments": "To be published as a chapter in `Adaptive Learning Methods for\n  Nonlinear System Modeling', Elsevier Publishing, Eds. D. Comminiello and J.C.\n  Principe (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we analyze nonlinear filtering problems in distributed\nenvironments, e.g., sensor networks or peer-to-peer protocols. In these\nscenarios, the agents in the environment receive measurements in a streaming\nfashion, and they are required to estimate a common (nonlinear) model by\nalternating local computations and communications with their neighbors. We\nfocus on the important distinction between single-task problems, where the\nunderlying model is common to all agents, and multitask problems, where each\nagent might converge to a different model due to, e.g., spatial dependencies or\nother factors. Currently, most of the literature on distributed learning in the\nnonlinear case has focused on the single-task case, which may be a strong\nlimitation in real-world scenarios. After introducing the problem and reviewing\nthe existing approaches, we describe a simple kernel-based algorithm tailored\nfor the multitask case. We evaluate the proposal on a simulated benchmark task,\nand we conclude by detailing currently open problems and lines of research.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 13:08:51 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Scardapane", "Simone", ""], ["Chen", "Jie", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1704.08914", "submitter": "Ehsaneddin Asgari", "authors": "Ehsaneddin Asgari and Hinrich Sch\\\"utze", "title": "Past, Present, Future: A Computational Investigation of the Typology of\n  Tense in 1000 Languages", "comments": null, "journal-ref": "Extended version of EMNLP 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SuperPivot, an analysis method for low-resource languages that\noccur in a superparallel corpus, i.e., in a corpus that contains an order of\nmagnitude more languages than parallel corpora currently in use. We show that\nSuperPivot performs well for the crosslingual analysis of the linguistic\nphenomenon of tense. We produce analysis results for more than 1000 languages,\nconducting - to the best of our knowledge - the largest crosslingual\ncomputational study performed to date. We extend existing methodology for\nleveraging parallel corpora for typological analysis by overcoming a limiting\nassumption of earlier work: We only require that a linguistic feature is\novertly marked in a few of thousands of languages as opposed to requiring that\nit be marked in all languages under investigation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 13:11:09 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 17:20:41 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Asgari", "Ehsaneddin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1704.09011", "submitter": "Khashayar Khosravi", "authors": "Hamsa Bastani and Mohsen Bayati and Khashayar Khosravi", "title": "Mostly Exploration-Free Algorithms for Contextual Bandits", "comments": "62 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contextual bandit literature has traditionally focused on algorithms that\naddress the exploration-exploitation tradeoff. In particular, greedy algorithms\nthat exploit current estimates without any exploration may be sub-optimal in\ngeneral. However, exploration-free greedy algorithms are desirable in practical\nsettings where exploration may be costly or unethical (e.g., clinical trials).\nSurprisingly, we find that a simple greedy algorithm can be rate optimal\n(achieves asymptotically optimal regret) if there is sufficient randomness in\nthe observed contexts (covariates). We prove that this is always the case for a\ntwo-armed bandit under a general class of context distributions that satisfy a\ncondition we term covariate diversity. Furthermore, even absent this condition,\nwe show that a greedy algorithm can be rate optimal with positive probability.\nThus, standard bandit algorithms may unnecessarily explore. Motivated by these\nresults, we introduce Greedy-First, a new algorithm that uses only observed\ncontexts and rewards to determine whether to follow a greedy algorithm or to\nexplore. We prove that this algorithm is rate optimal without any additional\nassumptions on the context distribution or the number of arms. Extensive\nsimulations demonstrate that Greedy-First successfully reduces exploration and\noutperforms existing (exploration-based) contextual bandit algorithms such as\nThompson sampling or upper confidence bound (UCB).\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 17:15:56 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 17:10:41 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 22:54:32 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 16:08:16 GMT"}, {"version": "v5", "created": "Tue, 2 Oct 2018 23:27:05 GMT"}, {"version": "v6", "created": "Fri, 30 Aug 2019 21:41:36 GMT"}, {"version": "v7", "created": "Mon, 25 Nov 2019 19:50:43 GMT"}, {"version": "v8", "created": "Sun, 19 Apr 2020 02:21:25 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bastani", "Hamsa", ""], ["Bayati", "Mohsen", ""], ["Khosravi", "Khashayar", ""]]}, {"id": "1704.09028", "submitter": "Benjamin Van Roy", "authors": "Daniel Russo and David Tse and Benjamin Van Roy", "title": "Time-Sensitive Bandit Learning and Satisficing Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on bandit learning and regret analysis has focused on contexts\nwhere the goal is to converge on an optimal action in a manner that limits\nexploration costs. One shortcoming imposed by this orientation is that it does\nnot treat time preference in a coherent manner. Time preference plays an\nimportant role when the optimal action is costly to learn relative to\nnear-optimal actions. This limitation has not only restricted the relevance of\ntheoretical results but has also influenced the design of algorithms. Indeed,\npopular approaches such as Thompson sampling and UCB can fare poorly in such\nsituations. In this paper, we consider discounted rather than cumulative\nregret, where a discount factor encodes time preference. We propose satisficing\nThompson sampling -- a variation of Thompson sampling -- and establish a strong\ndiscounted regret bound for this new algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 17:54:59 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Russo", "Daniel", ""], ["Tse", "David", ""], ["Van Roy", "Benjamin", ""]]}]