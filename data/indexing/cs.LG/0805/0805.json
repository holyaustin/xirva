[{"id": "0805.0149", "submitter": "Guangwu Xu", "authors": "T. Tony Cai, Guangwu Xu, and Jun Zhang", "title": "On Recovery of Sparse Signals via $\\ell_1$ Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers constrained $\\ell_1$ minimization methods for the\nrecovery of high dimensional sparse signals in three settings: noiseless,\nbounded error and Gaussian noise. A unified and elementary treatment is given\nin these noise settings for two $\\ell_1$ minimization methods: the Dantzig\nselector and $\\ell_1$ minimization with an $\\ell_2$ constraint. The results of\nthis paper improve the existing results in the literature by weakening the\nconditions and tightening the error bounds. The improvement on the conditions\nshows that signals with larger support can be recovered accurately. This paper\nalso establishes connections between restricted isometry property and the\nmutual incoherence property. Some results of Candes, Romberg and Tao (2006) and\nDonoho, Elad, and Temlyakov (2006) are extended.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2008 20:25:27 GMT"}], "update_date": "2008-05-05", "authors_parsed": [["Cai", "T. Tony", ""], ["Xu", "Guangwu", ""], ["Zhang", "Jun", ""]]}, {"id": "0805.1480", "submitter": "Koji Hukushima", "authors": "Takeshi Hirama and Koji Hukushima", "title": "On-line Learning of an Unlearnable True Teacher through Mobile Ensemble\n  Teachers", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": "10.1143/JPSJ.77.094801", "report-no": null, "categories": "cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-line learning of a hierarchical learning model is studied by a method from\nstatistical mechanics. In our model a student of a simple perceptron learns\nfrom not a true teacher directly, but ensemble teachers who learn from the true\nteacher with a perceptron learning rule. Since the true teacher and the\nensemble teachers are expressed as non-monotonic perceptron and simple ones,\nrespectively, the ensemble teachers go around the unlearnable true teacher with\nthe distance between them fixed in an asymptotic steady state. The\ngeneralization performance of the student is shown to exceed that of the\nensemble teachers in a transient state, as was shown in similar\nensemble-teachers models. Further, it is found that moving the ensemble\nteachers even in the steady state, in contrast to the fixed ensemble teachers,\nis efficient for the performance of the student.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2008 15:40:24 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Hirama", "Takeshi", ""], ["Hukushima", "Koji", ""]]}, {"id": "0805.2027", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis and Michail G. Lagoudakis", "title": "Rollout Sampling Approximate Policy Iteration", "comments": "18 pages, 2 figures, to appear in Machine Learning 72(3). Presented\n  at EWRL08, to be presented at ECML 2008", "journal-ref": null, "doi": "10.1007/s10994-008-5069-3", "report-no": null, "categories": "cs.LG cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researchers have recently investigated the connection between\nreinforcement learning and classification. We are motivated by proposals of\napproximate policy iteration schemes without value functions which focus on\npolicy representation using classifiers and address policy learning as a\nsupervised learning problem. This paper proposes variants of an improved policy\niteration scheme which addresses the core sampling problem in evaluating a\npolicy through simulation as a multi-armed bandit machine. The resulting\nalgorithm offers comparable performance to the previous algorithm achieved,\nhowever, with significantly less computational effort. An order of magnitude\nimprovement is demonstrated experimentally in two standard reinforcement\nlearning domains: inverted pendulum and mountain-car.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2008 11:19:19 GMT"}, {"version": "v2", "created": "Sun, 6 Jul 2008 17:36:33 GMT"}], "update_date": "2008-07-06", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Lagoudakis", "Michail G.", ""]]}, {"id": "0805.2362", "submitter": "Andreas Maurer", "authors": "Andreas Maurer", "title": "An optimization problem on the sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove existence and uniqueness of the minimizer for the average geodesic\ndistance to the points of a geodesically convex set on the sphere. This implies\na corresponding existence and uniqueness result for an optimal algorithm for\nhalfspace learning, when data and target functions are drawn from the uniform\ndistribution.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2008 17:25:03 GMT"}], "update_date": "2008-05-16", "authors_parsed": [["Maurer", "Andreas", ""]]}, {"id": "0805.2368", "submitter": "Alex Smola J", "authors": "Arthur Gretton, Karsten Borgwardt, Malte J. Rasch, Bernhard Scholkopf,\n  Alexander J. Smola", "title": "A Kernel Method for the Two-Sample Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for analyzing and comparing distributions, allowing us\nto design statistical tests to determine if two samples are drawn from\ndifferent distributions. Our test statistic is the largest difference in\nexpectations over functions in the unit ball of a reproducing kernel Hilbert\nspace (RKHS). We present two tests based on large deviation bounds for the test\nstatistic, while a third is based on the asymptotic distribution of this\nstatistic. The test statistic can be computed in quadratic time, although\nefficient linear time approximations are available. Several classical metrics\non distributions are recovered when the function space used to compute the\ndifference in expectations is allowed to be more general (eg. a Banach space).\nWe apply our two-sample tests to a variety of problems, including attribute\nmatching for databases using the Hungarian marriage method, where they perform\nstrongly. Excellent performance is also obtained when comparing distributions\nover graphs, for which these are the first such tests.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2008 17:46:53 GMT"}], "update_date": "2008-05-16", "authors_parsed": [["Gretton", "Arthur", ""], ["Borgwardt", "Karsten", ""], ["Rasch", "Malte J.", ""], ["Scholkopf", "Bernhard", ""], ["Smola", "Alexander J.", ""]]}, {"id": "0805.2752", "submitter": "Constantinos Panagiotakopoulos", "authors": "Constantinos Panagiotakopoulos and Petroula Tsampouka", "title": "The Margitron: A Generalised Perceptron with Margin", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify the classical Perceptron algorithm with margin as a member of a\nbroader family of large margin classifiers which we collectively call the\nMargitron. The Margitron, (despite its) sharing the same update rule with the\nPerceptron, is shown in an incremental setting to converge in a finite number\nof updates to solutions possessing any desirable fraction of the maximum\nmargin. Experiments comparing the Margitron with decomposition SVMs on tasks\ninvolving linear kernels and 2-norm soft margin are also reported.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2008 20:07:22 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Panagiotakopoulos", "Constantinos", ""], ["Tsampouka", "Petroula", ""]]}, {"id": "0805.2775", "submitter": "Afshin Rostamizadeh", "authors": "Corinna Cortes, Mehryar Mohri, Michael Riley, Afshin Rostamizadeh", "title": "Sample Selection Bias Correction Theory", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a theoretical analysis of sample selection bias\ncorrection. The sample bias correction technique commonly used in machine\nlearning consists of reweighting the cost of an error on each training point of\na biased sample to more closely reflect the unbiased distribution. This relies\non weights derived by various estimation techniques based on finite samples. We\nanalyze the effect of an error in that estimation on the accuracy of the\nhypothesis returned by the learning algorithm for two estimation techniques: a\ncluster-based estimation technique and kernel mean matching. We also report the\nresults of sample bias correction experiments with several data sets using\nthese techniques. Our analysis is based on the novel concept of distributional\nstability which generalizes the existing concept of point-based stability. Much\nof our work and proof techniques can be used to analyze other importance\nweighting techniques and their effect on accuracy when using a distributionally\nstable algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2008 02:55:08 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Cortes", "Corinna", ""], ["Mohri", "Mehryar", ""], ["Riley", "Michael", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "0805.2891", "submitter": "Miroslava Sotakova", "authors": "Shai Ben-David, Tyler Lu, David Pal, Miroslava Sotakova", "title": "Learning Low-Density Separators", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a novel, basic, unsupervised learning problem - learning the lowest\ndensity homogeneous hyperplane separator of an unknown probability\ndistribution. This task is relevant to several problems in machine learning,\nsuch as semi-supervised learning and clustering stability. We investigate the\nquestion of existence of a universally consistent algorithm for this problem.\nWe propose two natural learning paradigms and prove that, on input unlabeled\nrandom samples generated by any member of a rich family of distributions, they\nare guaranteed to converge to the optimal separator for that distribution. We\ncomplement this result by showing that no learning algorithm for our task can\nachieve uniform learning rates (that are independent of the data generating\ndistribution).\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2008 17:55:08 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2009 18:25:33 GMT"}], "update_date": "2009-01-22", "authors_parsed": [["Ben-David", "Shai", ""], ["Lu", "Tyler", ""], ["Pal", "David", ""], ["Sotakova", "Miroslava", ""]]}, {"id": "0805.4290", "submitter": "Abdel Ennaji", "authors": "Abdel Ennaji (LITIS), Arnaud Ribert (LITIS), Yves Lecourtier (LITIS)", "title": "From Data Topology to a Modular Classifier", "comments": null, "journal-ref": "International Journal On Document Analysis and Recognition 6, 1\n  (2003) 1-9", "doi": "10.1007/s10032-002-0095-3", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes an approach to designing a distributed and modular\nneural classifier. This approach introduces a new hierarchical clustering that\nenables one to determine reliable regions in the representation space by\nexploiting supervised information. A multilayer perceptron is then associated\nwith each of these detected clusters and charged with recognizing elements of\nthe associated cluster while rejecting all others. The obtained global\nclassifier is comprised of a set of cooperating neural networks and completed\nby a K-nearest neighbor classifier charged with treating elements rejected by\nall the neural networks. Experimental results for the handwritten digit\nrecognition problem and comparison with neural and statistical nonmodular\nclassifiers are given.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2008 09:16:44 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Ennaji", "Abdel", "", "LITIS"], ["Ribert", "Arnaud", "", "LITIS"], ["Lecourtier", "Yves", "", "LITIS"]]}]