[{"id": "1003.0024", "submitter": "Joshua Dillon", "authors": "Joshua V Dillon, Krishnakumar Balasubramanian and Guy Lebanon", "title": "Asymptotic Analysis of Generative Semi-Supervised Learning", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semisupervised learning has emerged as a popular framework for improving\nmodeling accuracy while controlling labeling cost. Based on an extension of\nstochastic composite likelihood we quantify the asymptotic accuracy of\ngenerative semi-supervised learning. In doing so, we complement\ndistribution-free analysis by providing an alternative framework to measure the\nvalue associated with different labeling policies and resolve the fundamental\nquestion of how much data to label and in what manner. We demonstrate our\napproach with both simulation studies and real world experiments using naive\nBayes for text classification and MRFs and CRFs for structured prediction in\nNLP.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2010 21:59:02 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Dillon", "Joshua V", ""], ["Balasubramanian", "Krishnakumar", ""], ["Lebanon", "Guy", ""]]}, {"id": "1003.0034", "submitter": "Jennifer Vaughan", "authors": "Yiling Chen and Jennifer Wortman Vaughan", "title": "A New Understanding of Prediction Markets Via No-Regret Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the striking mathematical connections that exist between market\nscoring rules, cost function based prediction markets, and no-regret learning.\nWe show that any cost function based prediction market can be interpreted as an\nalgorithm for the commonly studied problem of learning from expert advice by\nequating trades made in the market with losses observed by the learning\nalgorithm. If the loss of the market organizer is bounded, this bound can be\nused to derive an O(sqrt(T)) regret bound for the corresponding learning\nalgorithm. We then show that the class of markets with convex cost functions\nexactly corresponds to the class of Follow the Regularized Leader learning\nalgorithms, with the choice of a cost function in the market corresponding to\nthe choice of a regularizer in the learning problem. Finally, we show an\nequivalence between market scoring rules and prediction markets with convex\ncost functions. This implies that market scoring rules can also be interpreted\nnaturally as Follow the Regularized Leader algorithms, and may be of\nindependent interest. These connections provide new insight into how it is that\ncommonly studied markets, such as the Logarithmic Market Scoring Rule, can\naggregate opinions into accurate estimates of the likelihood of future events.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2010 23:27:22 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Chen", "Yiling", ""], ["Vaughan", "Jennifer Wortman", ""]]}, {"id": "1003.0079", "submitter": "Marius Kloft", "authors": "Marius Kloft, Ulf Brefeld, Soeren Sonnenburg, Alexander Zien", "title": "Non-Sparse Regularization for Multiple Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCB/EECS-2010-21", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning linear combinations of multiple kernels is an appealing strategy\nwhen the right choice of features is unknown. Previous approaches to multiple\nkernel learning (MKL) promote sparse kernel combinations to support\ninterpretability and scalability. Unfortunately, this 1-norm MKL is rarely\nobserved to outperform trivial baselines in practical applications. To allow\nfor robust kernel mixtures, we generalize MKL to arbitrary norms. We devise new\ninsights on the connection between several existing MKL formulations and\ndevelop two efficient interleaved optimization strategies for arbitrary norms,\nlike p-norms with p>1. Empirically, we demonstrate that the interleaved\noptimization strategies are much faster compared to the commonly used wrapper\napproaches. A theoretical analysis and an experiment on controlled artificial\ndata experiment sheds light on the appropriateness of sparse, non-sparse and\n$\\ell_\\infty$-norm MKL in various scenarios. Empirical applications of p-norm\nMKL to three real-world problems from computational biology show that\nnon-sparse MKL achieves accuracies that go beyond the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2010 08:54:29 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2010 06:18:11 GMT"}, {"version": "v3", "created": "Tue, 26 Oct 2010 20:21:35 GMT"}], "update_date": "2010-10-28", "authors_parsed": [["Kloft", "Marius", ""], ["Brefeld", "Ulf", ""], ["Sonnenburg", "Soeren", ""], ["Zien", "Alexander", ""]]}, {"id": "1003.0120", "submitter": "John Langford", "authors": "Alex Strehl, John Langford, Sham Kakade, Lihong Li", "title": "Learning from Logged Implicit Exploration Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a sound and consistent foundation for the use of \\emph{nonrandom}\nexploration data in \"contextual bandit\" or \"partially labeled\" settings where\nonly the value of a chosen action is learned.\n  The primary challenge in a variety of settings is that the exploration\npolicy, in which \"offline\" data is logged, is not explicitly known. Prior\nsolutions here require either control of the actions during the learning\nprocess, recorded random exploration, or actions chosen obliviously in a\nrepeated manner. The techniques reported here lift these restrictions, allowing\nthe learning of a policy for choosing actions given features from historical\ndata where no randomization occurred or was logged.\n  We empirically verify our solution on two reasonably sized sets of real-world\ndata obtained from Yahoo!.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2010 17:53:46 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2010 16:06:16 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Strehl", "Alex", ""], ["Langford", "John", ""], ["Kakade", "Sham", ""], ["Li", "Lihong", ""]]}, {"id": "1003.0146", "submitter": "Lihong Li", "authors": "Lihong Li, Wei Chu, John Langford, Robert E. Schapire", "title": "A Contextual-Bandit Approach to Personalized News Article Recommendation", "comments": "10 pages, 5 figures", "journal-ref": "Presented at the Nineteenth International Conference on World Wide\n  Web (WWW 2010), Raleigh, NC, USA, 2010", "doi": "10.1145/1772690.1772758", "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized web services strive to adapt their services (advertisements,\nnews articles, etc) to individual users by making use of both content and user\ninformation. Despite a few recent advances, this problem remains challenging\nfor at least two reasons. First, web service is featured with dynamically\nchanging pools of content, rendering traditional collaborative filtering\nmethods inapplicable. Second, the scale of most web services of practical\ninterest calls for solutions that are both fast in learning and computation.\n  In this work, we model personalized recommendation of news articles as a\ncontextual bandit problem, a principled approach in which a learning algorithm\nsequentially selects articles to serve users based on contextual information\nabout the users and articles, while simultaneously adapting its\narticle-selection strategy based on user-click feedback to maximize total user\nclicks.\n  The contributions of this work are three-fold. First, we propose a new,\ngeneral contextual bandit algorithm that is computationally efficient and well\nmotivated from learning theory. Second, we argue that any bandit algorithm can\nbe reliably evaluated offline using previously recorded random traffic.\nFinally, using this offline evaluation method, we successfully applied our new\nalgorithm to a Yahoo! Front Page Today Module dataset containing over 33\nmillion events. Results showed a 12.5% click lift compared to a standard\ncontext-free bandit algorithm, and the advantage becomes even greater when data\ngets more scarce.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2010 02:18:59 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 23:49:42 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Li", "Lihong", ""], ["Chu", "Wei", ""], ["Langford", "John", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1003.0205", "submitter": "Aarti Singh", "authors": "Aarti Singh, Robert D. Nowak and Robert Calderbank", "title": "Detecting Weak but Hierarchically-Structured Patterns in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect weak distributed activation patterns in networks is\ncritical to several applications, such as identifying the onset of anomalous\nactivity or incipient congestion in the Internet, or faint traces of a\nbiochemical spread by a sensor network. This is a challenging problem since\nweak distributed patterns can be invisible in per node statistics as well as a\nglobal network-wide aggregate. Most prior work considers situations in which\nthe activation/non-activation of each node is statistically independent, but\nthis is unrealistic in many problems. In this paper, we consider structured\npatterns arising from statistical dependencies in the activation process. Our\ncontributions are three-fold. First, we propose a sparsifying transform that\nsuccinctly represents structured activation patterns that conform to a\nhierarchical dependency graph. Second, we establish that the proposed transform\nfacilitates detection of very weak activation patterns that cannot be detected\nwith existing methods. Third, we show that the structure of the hierarchical\ndependency graph governing the activation process, and hence the network\ntransform, can be learnt from very few (logarithmic in network size)\nindependent snapshots of network activity.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2010 18:23:11 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Singh", "Aarti", ""], ["Nowak", "Robert D.", ""], ["Calderbank", "Robert", ""]]}, {"id": "1003.0470", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian, Pinar Donmez, Guy Lebanon", "title": "Unsupervised Supervised Learning II: Training Margin Based Classifiers\n  without Labels", "comments": "22 pages, 43 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular linear classifiers, such as logistic regression, boosting, or\nSVM, are trained by optimizing a margin-based risk function. Traditionally,\nthese risk functions are computed based on a labeled dataset. We develop a\nnovel technique for estimating such risks using only unlabeled data and the\nmarginal label distribution. We prove that the proposed risk estimator is\nconsistent on high-dimensional datasets and demonstrate it on synthetic and\nreal-world data. In particular, we show how the estimate is used for evaluating\nclassifiers in transfer learning, and for training classifiers with no labeled\ndata whatsoever.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 22:32:18 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2010 21:19:35 GMT"}], "update_date": "2010-07-23", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Donmez", "Pinar", ""], ["Lebanon", "Guy", ""]]}, {"id": "1003.0516", "submitter": "Marcus Hutter", "authors": "Marcus Hutter and Minh-Ngoc Tran", "title": "Model Selection with the Loss Rank Principle", "comments": "31 LaTeX pages, 1 figure", "journal-ref": "Computational Statistics and Data Analysis, 54 (2010) pages\n  1288-1306", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key issue in statistics and machine learning is to automatically select the\n\"right\" model complexity, e.g., the number of neighbors to be averaged over in\nk nearest neighbor (kNN) regression or the polynomial degree in regression with\npolynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) -\nfor model selection in regression and classification. It is based on the loss\nrank, which counts how many other (fictitious) data would be fitted better.\nLoRP selects the model that has minimal loss rank. Unlike most penalized\nmaximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the\nregression functions and the loss function. It works without a stochastic noise\nmodel, and is directly applicable to any non-parametric regressor, like kNN.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 08:21:07 GMT"}], "update_date": "2010-10-04", "authors_parsed": [["Hutter", "Marcus", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "1003.0529", "submitter": "Suresh Venkatasubramanian", "authors": "Arvind Agarwal, Jeff M. Phillips, Suresh Venkatasubramanian", "title": "A Unified Algorithmic Framework for Multi-Dimensional Scaling", "comments": "18 pages, 7 figures. This version fixes a bug in the proof of Theorem\n  6.1 (dimensionality reduction for spherical data). The statement of the\n  result remains the same.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified algorithmic framework for solving many\nknown variants of \\mds. Our algorithm is a simple iterative scheme with\nguaranteed convergence, and is \\emph{modular}; by changing the internals of a\nsingle subroutine in the algorithm, we can switch cost functions and target\nspaces easily. In addition to the formal guarantees of convergence, our\nalgorithms are accurate; in most cases, they converge to better quality\nsolutions than existing methods, in comparable time. We expect that this\nframework will be useful for a number of \\mds variants that have not yet been\nstudied.\n  Our framework extends to embedding high-dimensional points lying on a sphere\nto points on a lower dimensional sphere, preserving geodesic distances. As a\ncompliment to this result, we also extend the Johnson-Lindenstrauss Lemma to\nthis spherical setting, where projecting to a random $O((1/\\eps^2) \\log\nn)$-dimensional sphere causes $\\eps$-distortion.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 09:11:44 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2010 17:21:53 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Agarwal", "Arvind", ""], ["Phillips", "Jeff M.", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1003.0691", "submitter": "Joshua Dillon", "authors": "Joshua V Dillon and Guy Lebanon", "title": "Statistical and Computational Tradeoffs in Stochastic Composite\n  Likelihood", "comments": "30 pages, 97 figures, 2 authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimators are often of limited practical use due to the\nintensive computation they require. We propose a family of alternative\nestimators that maximize a stochastic variation of the composite likelihood\nfunction. Each of the estimators resolve the computation-accuracy tradeoff\ndifferently, and taken together they span a continuous spectrum of\ncomputation-accuracy tradeoff resolutions. We prove the consistency of the\nestimators, provide formulas for their asymptotic variance, statistical\nrobustness, and computational complexity. We discuss experimental results in\nthe context of Boltzmann machines and conditional random fields. The\ntheoretical and experimental studies demonstrate the effectiveness of the\nestimators when the computational resources are insufficient. They also\ndemonstrate that in some cases reduced computational complexity is associated\nwith robustness thereby increasing statistical accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 21:54:16 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Dillon", "Joshua V", ""], ["Lebanon", "Guy", ""]]}, {"id": "1003.0696", "submitter": "Arvind Agarwal", "authors": "Arvind Agarwal, Hal Daume III", "title": "Exponential Family Hybrid Semi-Supervised Learning", "comments": "6 pages, 3 figures", "journal-ref": "Twenty-First International Joint Conference on Artificial\n  Intelligence 2009, pg 974-979", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to semi-supervised learning based on an exponential\nfamily characterization. Our approach generalizes previous work on coupled\npriors for hybrid generative/discriminative models. Our model is more flexible\nand natural than previous approaches. Experimental results on several data sets\nshow that our approach also performs better in practice.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 22:27:31 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Agarwal", "Arvind", ""], ["Daume", "Hal", "III"]]}, {"id": "1003.1020", "submitter": "Haiping Huang", "authors": "Haiping Huang and Haijun Zhou", "title": "Learning by random walks in the weight space of the Ising perceptron", "comments": "12 pages, 4 figures, An extensively revised version", "journal-ref": "J. Stat. Mech.: Theory Exp. P08014 (2010)", "doi": "10.1088/1742-5468/2010/08/P08014", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several variants of a stochastic local search process for constructing the\nsynaptic weights of an Ising perceptron are studied. In this process, binary\npatterns are sequentially presented to the Ising perceptron and are then\nlearned as the synaptic weight configuration is modified through a chain of\nsingle- or double-weight flips within the compatible weight configuration space\nof the earlier learned patterns. This process is able to reach a storage\ncapacity of $\\alpha \\approx 0.63$ for pattern length N = 101 and $\\alpha\n\\approx 0.41$ for N = 1001. If in addition a relearning process is exploited,\nthe learning performance is further improved to a storage capacity of $\\alpha\n\\approx 0.80$ for N = 101 and $\\alpha \\approx 0.42$ for N=1001. We found that,\nfor a given learning task, the solutions constructed by the random walk\nlearning process are separated by a typical Hamming distance, which decreases\nwith the constraint density $\\alpha$ of the learning task; at a fixed value of\n$\\alpha$, the width of the Hamming distance distributions decreases with $N$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 11:38:33 GMT"}, {"version": "v2", "created": "Sun, 30 May 2010 03:44:54 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Huang", "Haiping", ""], ["Zhou", "Haijun", ""]]}, {"id": "1003.1141", "submitter": "Peter Turney", "authors": "Peter D. Turney and Patrick Pantel", "title": "From Frequency to Meaning: Vector Space Models of Semantics", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research, (2010), 37, 141-188", "doi": "10.1613/jair.2934", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computers understand very little of the meaning of human language. This\nprofoundly limits our ability to give instructions to computers, the ability of\ncomputers to explain their actions to us, and the ability of computers to\nanalyse and process text. Vector space models (VSMs) of semantics are beginning\nto address these limits. This paper surveys the use of VSMs for semantic\nprocessing of text. We organize the literature on VSMs according to the\nstructure of the matrix in a VSM. There are currently three broad classes of\nVSMs, based on term-document, word-context, and pair-pattern matrices, yielding\nthree classes of applications. We survey a broad range of applications in these\nthree categories and we take a detailed look at a specific open source project\nin each category. Our goal in this survey is to show the breadth of\napplications of VSMs for semantics, to provide a new perspective on VSMs for\nthose who are already familiar with the area, and to provide pointers into the\nliterature for those who are less familiar with the field.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 21:07:18 GMT"}], "update_date": "2010-03-08", "authors_parsed": [["Turney", "Peter D.", ""], ["Pantel", "Patrick", ""]]}, {"id": "1003.1266", "submitter": "Ulrike von Luxburg", "authors": "Ulrike von Luxburg, Agnes Radl, Matthias Hein", "title": "Hitting and commute times in large graphs are often misleading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next to the shortest path distance, the second most popular distance function\nbetween vertices in a graph is the commute distance (resistance distance). For\ntwo vertices u and v, the hitting time H_{uv} is the expected time it takes a\nrandom walk to travel from u to v. The commute time is its symmetrized version\nC_{uv} = H_{uv} + H_{vu}. In our paper we study the behavior of hitting times\nand commute distances when the number n of vertices in the graph is very large.\nWe prove that as n converges to infinty, hitting times and commute distances\nconverge to expressions that do not take into account the global structure of\nthe graph at all. Namely, the hitting time H_{uv} converges to 1/d_v and the\ncommute time to 1/d_u + 1/d_v where d_u and d_v denote the degrees of vertices\nu and v. In these cases, the hitting and commute times are misleading in the\nsense that they do not provide information about the structure of the graph. We\nfocus on two major classes of random graphs: random geometric graphs (k-nearest\nneighbor graphs, epsilon-graphs, Gaussian similarity graphs) and random graphs\nwith given expected degrees (in particular, Erdos-Renyi graphs with and without\nplanted partitions)\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2010 13:54:11 GMT"}, {"version": "v2", "created": "Thu, 26 May 2011 08:07:41 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["von Luxburg", "Ulrike", ""], ["Radl", "Agnes", ""], ["Hein", "Matthias", ""]]}, {"id": "1003.1354", "submitter": "Ankan Saha", "authors": "Xinhua Zhang (1), Ankan Saha (2), S.V.N. Vishwanathan (1)((1) Purdue\n  University, (2) University of Chicago)", "title": "Faster Rates for training Max-Margin Markov Networks", "comments": "14 pages Submitted to COLT 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured output prediction is an important machine learning problem both in\ntheory and practice, and the max-margin Markov network (\\mcn) is an effective\napproach. All state-of-the-art algorithms for optimizing \\mcn\\ objectives take\nat least $O(1/\\epsilon)$ number of iterations to find an $\\epsilon$ accurate\nsolution. Recent results in structured optimization suggest that faster rates\nare possible by exploiting the structure of the objective function. Towards\nthis end \\citet{Nesterov05} proposed an excessive gap reduction technique based\non Euclidean projections which converges in $O(1/\\sqrt{\\epsilon})$ iterations\non strongly convex functions. Unfortunately when applied to \\mcn s, this\napproach does not admit graphical model factorization which, as in many\nexisting algorithms, is crucial for keeping the cost per iteration tractable.\nIn this paper, we present a new excessive gap reduction technique based on\nBregman projections which admits graphical model factorization naturally, and\nconverges in $O(1/\\sqrt{\\epsilon})$ iterations. Compared with existing\nalgorithms, the convergence rate of our method has better dependence on\n$\\epsilon$ and other parameters of the problem, and can be easily kernelized.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2010 05:49:19 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Zhang", "Xinhua", ""], ["Saha", "Ankan", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1003.1410", "submitter": "Seungyeon Kim", "authors": "Seungyeon Kim, Guy Lebanon", "title": "Local Space-Time Smoothing for Version Controlled Documents", "comments": "9 pages, 6 figures", "journal-ref": "Proceedings of the 23rd International Conference on Computational\n  Linguistics (Coling 2010); 2010 Aug 23-27; Beijing, CN", "doi": null, "report-no": null, "categories": "cs.GR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike static documents, version controlled documents are continuously edited\nby one or more authors. Such collaborative revision process makes traditional\nmodeling and visualization techniques inappropriate. In this paper we propose a\nnew representation based on local space-time smoothing that captures important\nrevision patterns. We demonstrate the applicability of our framework using\nexperiments on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2010 18:08:12 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 18:05:00 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Kim", "Seungyeon", ""], ["Lebanon", "Guy", ""]]}, {"id": "1003.1450", "submitter": "Rdv Ijcsis", "authors": "Heidar Mamosian, Amir Masoud Rahmani, Mashalla Abbasi Dezfouli", "title": "A New Clustering Approach based on Page's Path Similarity for Navigation\n  Patterns Mining", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS February 2010, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 7, No. 2, pp. 009-014, February 2010, USA", "doi": null, "report-no": "Computer Science ISSN 19475500", "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In recent years, predicting the user's next request in web navigation has\nreceived much attention. An information source to be used for dealing with such\nproblem is the left information by the previous web users stored at the web\naccess log on the web servers. Purposed systems for this problem work based on\nthis idea that if a large number of web users request specific pages of a\nwebsite on a given session, it can be concluded that these pages are satisfying\nsimilar information needs, and therefore they are conceptually related. In this\nstudy, a new clustering approach is introduced that employs logical path\nstoring of a website pages as another parameter which is regarded as a\nsimilarity parameter and conceptual relation between web pages. The results of\nsimulation have shown that the proposed approach is more than others precise in\ndetermining the clusters.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 11:08:33 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Mamosian", "Heidar", ""], ["Rahmani", "Amir Masoud", ""], ["Dezfouli", "Mashalla Abbasi", ""]]}, {"id": "1003.1499", "submitter": "Rdv Ijcsis", "authors": "Mofreh A. Hogo", "title": "Evaluation of E-Learners Behaviour using Different Fuzzy Clustering\n  Models: A Comparative Study", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper introduces an evaluation methodologies for the e-learners'\nbehaviour that will be a feedback to the decision makers in e-learning system.\nLearner's profile plays a crucial role in the evaluation process to improve the\ne-learning process performance. The work focuses on the clustering of the\ne-learners based on their behaviour into specific categories that represent the\nlearner's profiles. The learners' classes named as regular, workers, casual,\nbad, and absent. The work may answer the question of how to return bad students\nto be regular ones. The work presented the use of different fuzzy clustering\ntechniques as fuzzy c-means and kernelized fuzzy c-means to find the learners'\ncategories and predict their profiles. The paper presents the main phases as\ndata description, preparation, features selection, and the experiments design\nusing different fuzzy clustering models. Analysis of the obtained results and\ncomparison with the real world behavior of those learners proved that there is\na match with percentage of 78%. Fuzzy clustering reflects the learners'\nbehavior more than crisp clustering. Comparison between FCM and KFCM proved\nthat the KFCM is much better than FCM in predicting the learners' behaviour.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 17:36:26 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Hogo", "Mofreh A.", ""]]}, {"id": "1003.1510", "submitter": "Rdv Ijcsis", "authors": "Wongkot Sriurai, Phayung Meesad, Choochart Haruechaiyasak", "title": "Hierarchical Web Page Classification Based on a Topic Model and\n  Neighboring Pages Integration", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Most Web page classification models typically apply the bag of words (BOW)\nmodel to represent the feature space. The original BOW representation, however,\nis unable to recognize semantic relationships between terms. One possible\nsolution is to apply the topic model approach based on the Latent Dirichlet\nAllocation algorithm to cluster the term features into a set of latent topics.\nTerms assigned into the same topic are semantically related. In this paper, we\npropose a novel hierarchical classification method based on a topic model and\nby integrating additional term features from neighboring pages. Our\nhierarchical classification method consists of two phases: (1) feature\nrepresentation by using a topic model and integrating neighboring pages, and\n(2) hierarchical Support Vector Machines (SVM) classification model constructed\nfrom a confusion matrix. From the experimental results, the approach of using\nthe proposed hierarchical SVM model by integrating current page with\nneighboring pages via the topic model yielded the best performance with the\naccuracy equal to 90.33% and the F1 measure of 90.14%; an improvement of 5.12%\nand 5.13% over the original SVM model, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 18:32:47 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Sriurai", "Wongkot", ""], ["Meesad", "Phayung", ""], ["Haruechaiyasak", "Choochart", ""]]}, {"id": "1003.1795", "submitter": "Rdv Ijcsis", "authors": "Vidhya. K. A, G. Aghila", "title": "A Survey of Na\\\"ive Bayes Machine Learning approach in Text Document\n  Classification", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Text Document classification aims in associating one or more predefined\ncategories based on the likelihood suggested by the training set of labeled\ndocuments. Many machine learning algorithms play a vital role in training the\nsystem with predefined categories among which Na\\\"ive Bayes has some intriguing\nfacts that it is simple, easy to implement and draws better accuracy in large\ndatasets in spite of the na\\\"ive dependence. The importance of Na\\\"ive Bayes\nMachine learning approach has felt hence the study has been taken up for text\ndocument classification and the statistical event models available. This survey\nthe various feature selection methods has been discussed and compared along\nwith the metrics related to text document classification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 06:41:49 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["A", "Vidhya. K.", ""], ["Aghila", "G.", ""]]}, {"id": "1003.2218", "submitter": "Alexey Chernov", "authors": "Alexey Chernov, Yuri Kalnishkan, Fedor Zhdanov, Vladimir Vovk", "title": "Supermartingales in Prediction with Expert Advice", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the method of defensive forecasting, based on the use of\ngame-theoretic supermartingales, to prediction with expert advice. In the\ntraditional setting of a countable number of experts and a finite number of\noutcomes, the Defensive Forecasting Algorithm is very close to the well-known\nAggregating Algorithm. Not only the performance guarantees but also the\npredictions are the same for these two methods of fundamentally different\nnature. We discuss also a new setting where the experts can give advice\nconditional on the learner's future decision. Both the algorithms can be\nadapted to the new setting and give the same performance guarantees as in the\ntraditional setting. Finally, we outline an application of defensive\nforecasting to a setting with several loss functions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2010 21:53:56 GMT"}], "update_date": "2010-03-12", "authors_parsed": [["Chernov", "Alexey", ""], ["Kalnishkan", "Yuri", ""], ["Zhdanov", "Fedor", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1003.2471", "submitter": "Fangwen Fu", "authors": "Fangwen Fu and Mihaela van der Schaar", "title": "Structure-Aware Stochastic Control for Transmission Scheduling", "comments": "41pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of real-time transmission scheduling\nover time-varying channels. We first formulate the transmission scheduling\nproblem as a Markov decision process (MDP) and systematically unravel the\nstructural properties (e.g. concavity in the state-value function and\nmonotonicity in the optimal scheduling policy) exhibited by the optimal\nsolutions. We then propose an online learning algorithm which preserves these\nstructural properties and achieves -optimal solutions for an arbitrarily small\n. The advantages of the proposed online method are that: (i) it does not\nrequire a priori knowledge of the traffic arrival and channel statistics and\n(ii) it adaptively approximates the state-value functions using piece-wise\nlinear functions and has low storage and computation complexity. We also extend\nthe proposed low-complexity online learning solution to the prioritized data\ntransmission. The simulation results demonstrate that the proposed method\nachieves significantly better utility (or delay)-energy trade-offs when\ncomparing to existing state-of-art online optimization methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 04:07:41 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Fu", "Fangwen", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1003.2586", "submitter": "Francesca A. Lisi", "authors": "Francesca A. Lisi", "title": "Inductive Logic Programming in Databases: from Datalog to DL+log", "comments": "30 pages, 3 figures, 2 tables.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address an issue that has been brought to the attention of\nthe database community with the advent of the Semantic Web, i.e. the issue of\nhow ontologies (and semantics conveyed by them) can help solving typical\ndatabase problems, through a better understanding of KR aspects related to\ndatabases. In particular, we investigate this issue from the ILP perspective by\nconsidering two database problems, (i) the definition of views and (ii) the\ndefinition of constraints, for a database whose schema is represented also by\nmeans of an ontology. Both can be reformulated as ILP problems and can benefit\nfrom the expressive and deductive power of the KR framework DL+log. We\nillustrate the application scenarios by means of examples. Keywords: Inductive\nLogic Programming, Relational Databases, Ontologies, Description Logics, Hybrid\nKnowledge Representation and Reasoning Systems. Note: To appear in Theory and\nPractice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 17:40:43 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Lisi", "Francesca A.", ""]]}, {"id": "1003.2751", "submitter": "Benjamin Rubinstein", "authors": "Blaine Nelson and Benjamin I. P. Rubinstein and Ling Huang and Anthony\n  D. Joseph and Shing-hon Lau and Steven J. Lee and Satish Rao and Anthony Tran\n  and J. D. Tygar", "title": "Near-Optimal Evasion of Convex-Inducing Classifiers", "comments": "8 pages; to appear at AISTATS'2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers are often used to detect miscreant activities. We study how an\nadversary can efficiently query a classifier to elicit information that allows\nthe adversary to evade detection at near-minimal cost. We generalize results of\nLowd and Meek (2005) to convex-inducing classifiers. We present algorithms that\nconstruct undetected instances of near-minimal cost using only polynomially\nmany queries in the dimension of the space and without reverse engineering the\ndecision boundary.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2010 01:25:06 GMT"}], "update_date": "2010-03-16", "authors_parsed": [["Nelson", "Blaine", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Huang", "Ling", ""], ["Joseph", "Anthony D.", ""], ["Lau", "Shing-hon", ""], ["Lee", "Steven J.", ""], ["Rao", "Satish", ""], ["Tran", "Anthony", ""], ["Tygar", "J. D.", ""]]}, {"id": "1003.3279", "submitter": "Antonio Mucherino", "authors": "Antonio Mucherino, Sonia Cafieri", "title": "A New Heuristic for Feature Selection by Consistent Biclustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of data, biclustering aims at finding simultaneous partitions in\nbiclusters of its samples and of the features which are used for representing\nthe samples. Consistent biclusterings allow to obtain correct classifications\nof the samples from the known classification of the features, and vice versa,\nand they are very useful for performing supervised classifications. The problem\nof finding consistent biclusterings can be seen as a feature selection problem,\nwhere the features that are not relevant for classification purposes are\nremoved from the set of data, while the total number of features is maximized\nin order to preserve information. This feature selection problem can be\nformulated as a linear fractional 0-1 optimization problem. We propose a\nreformulation of this problem as a bilevel optimization problem, and we present\na heuristic algorithm for an efficient solution of the reformulated problem.\nComputational experiments show that the presented algorithm is able to find\nbetter solutions with respect to the ones obtained by employing previously\npresented heuristic algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2010 01:48:56 GMT"}], "update_date": "2010-03-18", "authors_parsed": [["Mucherino", "Antonio", ""], ["Cafieri", "Sonia", ""]]}, {"id": "1003.3821", "submitter": "Dan Guralnik", "authors": "Dan Guralnik", "title": "A Formal Approach to Modeling the Memory of a Living Organism", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a living organism as an observer of the evolution of its\nenvironment recording sensory information about the state space X of the\nenvironment in real time. Sensory information is sampled and then processed on\ntwo levels. On the biological level, the organism serves as an evaluation\nmechanism of the subjective relevance of the incoming data to the observer: the\nobserver assigns excitation values to events in X it could recognize using its\nsensory equipment. On the algorithmic level, sensory input is used for updating\na database, the memory of the observer whose purpose is to serve as a\ngeometric/combinatorial model of X, whose nodes are weighted by the excitation\nvalues produced by the evaluation mechanism. These values serve as a guidance\nsystem for deciding how the database should transform as observation data\nmounts. We define a searching problem for the proposed model and discuss the\nmodel's flexibility and its computational efficiency, as well as the\npossibility of implementing it as a dynamic network of neuron-like units. We\nshow how various easily observable properties of the human memory and thought\nprocess can be explained within the framework of this model. These include:\nreasoning (with efficiency bounds), errors, temporary and permanent loss of\ninformation. We are also able to define general learning problems in terms of\nthe new model, such as the language acquisition problem.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 15:56:37 GMT"}], "update_date": "2010-03-22", "authors_parsed": [["Guralnik", "Dan", ""]]}, {"id": "1003.3967", "submitter": "Andreas Krause", "authors": "Daniel Golovin and Andreas Krause", "title": "Adaptive Submodularity: Theory and Applications in Active Learning and\n  Stochastic Optimization", "comments": "60 pages, 6 figures. Version 5 addresses a flaw in the proof of\n  Theorem 13 identified by Nan and Saligrama (2017). The revision includes a\n  weaker version of Theorem 13, guaranteeing squared logarithmic approximation\n  under an additional strong adaptive submodularity condition. This condition\n  is met by all applications considered in the paper, as discussed in the\n  revised Sections 7, 8 and 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving stochastic optimization problems under partial observability, where\none needs to adaptively make decisions with uncertain outcomes, is a\nfundamental but notoriously difficult challenge. In this paper, we introduce\nthe concept of adaptive submodularity, generalizing submodular set functions to\nadaptive policies. We prove that if a problem satisfies this property, a simple\nadaptive greedy algorithm is guaranteed to be competitive with the optimal\npolicy. In addition to providing performance guarantees for both stochastic\nmaximization and coverage, adaptive submodularity can be exploited to\ndrastically speed up the greedy algorithm by using lazy evaluations. We\nillustrate the usefulness of the concept by giving several examples of adaptive\nsubmodular objectives arising in diverse applications including sensor\nplacement, viral marketing and active learning. Proving adaptive submodularity\nfor these problems allows us to recover existing results in these applications\nas special cases, improve approximation guarantees and handle natural\ngeneralizations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 04:06:22 GMT"}, {"version": "v2", "created": "Mon, 24 May 2010 02:25:04 GMT"}, {"version": "v3", "created": "Mon, 30 Aug 2010 23:00:54 GMT"}, {"version": "v4", "created": "Wed, 17 Oct 2012 03:04:19 GMT"}, {"version": "v5", "created": "Wed, 6 Dec 2017 08:21:07 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Golovin", "Daniel", ""], ["Krause", "Andreas", ""]]}, {"id": "1003.4079", "submitter": "William Jackson", "authors": "Swathi. H", "title": "Gene Expression Data Knowledge Discovery using Global and Local\n  Clustering", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 3, March 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.CE cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand complex biological systems, the research community has produced\nhuge corpus of gene expression data. A large number of clustering approaches\nhave been proposed for the analysis of gene expression data. However,\nextracting important biological knowledge is still harder. To address this\ntask, clustering techniques are used. In this paper, hybrid Hierarchical\nk-Means algorithm is used for clustering and biclustering gene expression data\nis used. To discover both local and global clustering structure biclustering\nand clustering algorithms are utilized. A validation technique, Figure of Merit\nis used to determine the quality of clustering results. Appropriate knowledge\nis mined from the clusters by embedding a BLAST similarity search program into\nthe clustering and biclustering process. To discover both local and global\nclustering structure biclustering and clustering algorithms are utilized. To\ndetermine the quality of clustering results, a validation technique, Figure of\nMerit is used. Appropriate knowledge is mined from the clusters by embedding a\nBLAST similarity search program into the clustering and biclustering process.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 06:31:36 GMT"}], "update_date": "2010-03-28", "authors_parsed": [["H", "Swathi.", ""]]}, {"id": "1003.4274", "submitter": "Burkhard Schipper", "authors": "Peter Duersch, Joerg Oechssler, Burkhard C. Schipper", "title": "Unbeatable Imitation", "comments": null, "journal-ref": null, "doi": "10.1016/j.geb.2012.05.002", "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for many classes of symmetric two-player games, the simple\ndecision rule \"imitate-the-best\" can hardly be beaten by any other decision\nrule. We provide necessary and sufficient conditions for imitation to be\nunbeatable and show that it can only be beaten by much in games that are of the\nrock-scissors-paper variety. Thus, in many interesting examples, like 2x2\ngames, Cournot duopoly, price competition, rent seeking, public goods games,\ncommon pool resource games, minimum effort coordination games, arms race,\nsearch, bargaining, etc., imitation cannot be beaten by much even by a very\nclever opponent.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 20:40:28 GMT"}], "update_date": "2013-01-25", "authors_parsed": [["Duersch", "Peter", ""], ["Oechssler", "Joerg", ""], ["Schipper", "Burkhard C.", ""]]}, {"id": "1003.4781", "submitter": "Xu Miao", "authors": "Xu Miao, Rajesh P.N. Rao", "title": "Large Margin Boltzmann Machines and Large Margin Sigmoid Belief Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "UW-CSE-09-04-01", "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current statistical models for structured prediction make simplifying\nassumptions about the underlying output graph structure, such as assuming a\nlow-order Markov chain, because exact inference becomes intractable as the\ntree-width of the underlying graph increases. Approximate inference algorithms,\non the other hand, force one to trade off representational power with\ncomputational efficiency. In this paper, we propose two new types of\nprobabilistic graphical models, large margin Boltzmann machines (LMBMs) and\nlarge margin sigmoid belief networks (LMSBNs), for structured prediction.\nLMSBNs in particular allow a very fast inference algorithm for arbitrary graph\nstructures that runs in polynomial time with a high probability. This\nprobability is data-distribution dependent and is maximized in learning. The\nnew approach overcomes the representation-efficiency trade-off in previous\nmodels and allows fast structured prediction with complicated graph structures.\nWe present results from applying a fully connected model to multi-label scene\nclassification and demonstrate that the proposed approach can yield significant\nperformance gains over current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 02:21:11 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Miao", "Xu", ""], ["Rao", "Rajesh P. N.", ""]]}, {"id": "1003.4944", "submitter": "Ryan Adams", "authors": "Ryan Prescott Adams, George E. Dahl, Iain Murray", "title": "Incorporating Side Information in Probabilistic Matrix Factorization\n  with Gaussian Processes", "comments": "18 pages, 4 figures, Submitted to UAI 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic matrix factorization (PMF) is a powerful method for modeling\ndata associated with pairwise relationships, finding use in collaborative\nfiltering, computational biology, and document analysis, among other areas. In\nmany domains, there is additional information that can assist in prediction.\nFor example, when modeling movie ratings, we might know when the rating\noccurred, where the user lives, or what actors appear in the movie. It is\ndifficult, however, to incorporate this side information into the PMF model. We\npropose a framework for incorporating side information by coupling together\nmultiple PMF problems via Gaussian process priors. We replace scalar latent\nfeatures with functions that vary over the space of side information. The GP\npriors on these functions require them to vary smoothly and share information.\nWe successfully use this new method to predict the scores of professional\nbasketball games, where side information about the venue and date of the game\nare relevant for the outcome.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 16:12:48 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["Dahl", "George E.", ""], ["Murray", "Iain", ""]]}, {"id": "1003.5623", "submitter": "Ashley Smith", "authors": "Pawan Kumar, Astik Biswas, A .N. Mishra and Mahesh Chandra", "title": "Spoken Language Identification Using Hybrid Feature Extraction Methods", "comments": null, "journal-ref": "Journal of Telecommunications, Volume 1, Issue 2, pp11-15, March\n  2010", "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and motivates the use of hybrid robust feature\nextraction technique for spoken language identification (LID) system. The\nspeech recognizers use a parametric form of a signal to get the most important\ndistinguishable features of speech signal for recognition task. In this paper\nMel-frequency cepstral coefficients (MFCC), Perceptual linear prediction\ncoefficients (PLP) along with two hybrid features are used for language\nIdentification. Two hybrid features, Bark Frequency Cepstral Coefficients\n(BFCC) and Revised Perceptual Linear Prediction Coefficients (RPLP) were\nobtained from combination of MFCC and PLP. Two different classifiers, Vector\nQuantization (VQ) with Dynamic Time Warping (DTW) and Gaussian Mixture Model\n(GMM) were used for classification. The experiment shows better identification\nrate using hybrid feature extraction techniques compared to conventional\nfeature extraction methods.BFCC has shown better performance than MFCC with\nboth classifiers. RPLP along with GMM has shown best identification performance\namong all feature extraction techniques.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2010 17:48:22 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Kumar", "Pawan", ""], ["Biswas", "Astik", ""], ["Mishra", "A . N.", ""], ["Chandra", "Mahesh", ""]]}, {"id": "1003.5627", "submitter": "Ashley Smith", "authors": "Mahmoud I. Abdalla and Hanaa S. Ali", "title": "Wavelet-Based Mel-Frequency Cepstral Coefficients for Speaker\n  Identification using Hidden Markov Models", "comments": null, "journal-ref": "Journal of Telecommunications, Volume 1, Issue 2, pp16-21, March\n  2010", "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the performance of speaker identification systems, an effective\nand robust method is proposed to extract speech features, capable of operating\nin noisy environment. Based on the time-frequency multi-resolution property of\nwavelet transform, the input speech signal is decomposed into various frequency\nchannels. For capturing the characteristic of the signal, the Mel-Frequency\nCepstral Coefficients (MFCCs) of the wavelet channels are calculated. Hidden\nMarkov Models (HMMs) were used for the recognition stage as they give better\nrecognition for the speaker's features than Dynamic Time Warping (DTW).\nComparison of the proposed approach with the MFCCs conventional feature\nextraction method shows that the proposed method not only effectively reduces\nthe influence of noise, but also improves recognition. A recognition rate of\n99.3% was obtained using the proposed feature extraction technique compared to\n98.7% using the MFCCs. When the test patterns were corrupted by additive white\nGaussian noise with 20 dB S/N ratio, the recognition rate was 97.3% using the\nproposed method compared to 93.3% using the MFCCs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2010 17:54:55 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Abdalla", "Mahmoud I.", ""], ["Ali", "Hanaa S.", ""]]}, {"id": "1003.5749", "submitter": "Sylvie Billot", "authors": "Iris Eshkol (CORAL), Isabelle Tellier (LIFO), Taalab Samer (LIFO),\n  Sylvie Billot (LIFO)", "title": "Etiqueter un corpus oral par apprentissage automatique \\`a l'aide de\n  connaissances linguistiques", "comments": null, "journal-ref": "10\\`emes Journ\\'ees Internationales d'Analyse statistique des\n  Donn\\'ees Textuelles JADT'2010, Rome : Italie (2010)", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the Eslo1 (\"Enqu\\^ete sociolinguistique d'Orl\\'eans\", i.e.\n\"Sociolinguistic Inquiery of Orl\\'eans\") campain, a large oral corpus has been\ngathered and transcribed in a textual format. The purpose of the work presented\nhere is to associate a morpho-syntactic label to each unit of this corpus. To\nthis aim, we have first studied the specificities of the necessary labels, and\ntheir various possible levels of description. This study has led to a new\noriginal hierarchical structuration of labels. Then, considering that our new\nset of labels was different from the one used in every available software, and\nthat these softwares usually do not fit for oral data, we have built a new\nlabeling tool by a Machine Learning approach, from data labeled by Cordial and\ncorrected by hand. We have applied linear CRF (Conditional Random Fields)\ntrying to take the best possible advantage of the linguistic knowledge that was\nused to define the set of labels. We obtain an accuracy between 85 and 90%,\ndepending of the parameters used.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 07:04:46 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Eshkol", "Iris", "", "CORAL"], ["Tellier", "Isabelle", "", "LIFO"], ["Samer", "Taalab", "", "LIFO"], ["Billot", "Sylvie", "", "LIFO"]]}, {"id": "1003.5865", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Phalguni Gupta, Jamuna Kanta Sing", "title": "Offline Signature Identification by Fusion of Multiple Classifiers using\n  Statistical Learning Theory", "comments": "11 pages, 3 figures, IJSIA 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper uses Support Vector Machines (SVM) to fuse multiple classifiers\nfor an offline signature system. From the signature images, global and local\nfeatures are extracted and the signatures are verified with the help of\nGaussian empirical rule, Euclidean and Mahalanobis distance based classifiers.\nSVM is used to fuse matching scores of these matchers. Finally, recognition of\nquery signatures is done by comparing it with all signatures of the database.\nThe proposed system is tested on a signature database contains 5400 offline\nsignatures of 600 individuals and the results are found to be promising.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 16:36:36 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Gupta", "Phalguni", ""], ["Sing", "Jamuna Kanta", ""]]}, {"id": "1003.5956", "submitter": "Lihong Li", "authors": "Lihong Li and Wei Chu and John Langford and Xuanhui Wang", "title": "Unbiased Offline Evaluation of Contextual-bandit-based News Article\n  Recommendation Algorithms", "comments": "10 pages, 7 figures, revised from the published version at the WSDM\n  2011 conference", "journal-ref": null, "doi": "10.1145/1935826.1935878", "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms have become popular for online recommendation\nsystems such as Digg, Yahoo! Buzz, and news recommendation in general.\n\\emph{Offline} evaluation of the effectiveness of new algorithms in these\napplications is critical for protecting online user experiences but very\nchallenging due to their \"partial-label\" nature. Common practice is to create a\nsimulator which simulates the online environment for the problem at hand and\nthen run an algorithm against this simulator. However, creating simulator\nitself is often difficult and modeling bias is usually unavoidably introduced.\nIn this paper, we introduce a \\emph{replay} methodology for contextual bandit\nalgorithm evaluation. Different from simulator-based approaches, our method is\ncompletely data-driven and very easy to adapt to different applications. More\nimportantly, our method can provide provably unbiased evaluations. Our\nempirical results on a large-scale news article recommendation dataset\ncollected from Yahoo! Front Page conform well with our theoretical results.\nFurthermore, comparisons between our offline replay and online bucket\nevaluation of several contextual bandit algorithms show accuracy and\neffectiveness of our offline evaluation method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2010 01:20:07 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 23:33:07 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Li", "Lihong", ""], ["Chu", "Wei", ""], ["Langford", "John", ""], ["Wang", "Xuanhui", ""]]}]