[{"id": "1606.00068", "submitter": "Marco Cusumano-Towner", "authors": "Marco F Cusumano-Towner, Vikash K Mansinghka", "title": "Quantifying the probable approximation error of probabilistic inference\n  programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new technique for quantifying the approximation error\nof a broad class of probabilistic inference programs, including ones based on\nboth variational and Monte Carlo approaches. The key idea is to derive a\nsubjective bound on the symmetrized KL divergence between the distribution\nachieved by an approximate inference program and its true target distribution.\nThe bound's validity (and subjectivity) rests on the accuracy of two auxiliary\nprobabilistic programs: (i) a \"reference\" inference program that defines a gold\nstandard of accuracy and (ii) a \"meta-inference\" program that answers the\nquestion \"what internal random choices did the original approximate inference\nprogram probably make given that it produced a particular result?\" The paper\nincludes empirical results on inference problems drawn from linear regression,\nDirichlet process mixture modeling, HMMs, and Bayesian networks. The\nexperiments show that the technique is robust to the quality of the reference\ninference program and that it can detect implementation bugs that are not\napparent from predictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 22:37:43 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Cusumano-Towner", "Marco F", ""], ["Mansinghka", "Vikash K", ""]]}, {"id": "1606.00119", "submitter": "Rajat Sen", "authors": "Rajat Sen, Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G.\n  Dimakis, and Sanjay Shakkottai", "title": "Contextual Bandits with Latent Confounders: An NMF Approach", "comments": "37 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by online recommendation and advertising systems, we consider a\ncausal model for stochastic contextual bandits with a latent low-dimensional\nconfounder. In our model, there are $L$ observed contexts and $K$ arms of the\nbandit. The observed context influences the reward obtained through a latent\nconfounder variable with cardinality $m$ ($m \\ll L,K$). The arm choice and the\nlatent confounder causally determines the reward while the observed context is\ncorrelated with the confounder. Under this model, the $L \\times K$ mean reward\nmatrix $\\mathbf{U}$ (for each context in $[L]$ and each arm in $[K]$)\nfactorizes into non-negative factors $\\mathbf{A}$ ($L \\times m$) and\n$\\mathbf{W}$ ($m \\times K$). This insight enables us to propose an\n$\\epsilon$-greedy NMF-Bandit algorithm that designs a sequence of interventions\n(selecting specific arms), that achieves a balance between learning this\nlow-dimensional structure and selecting the best arm to minimize regret. Our\nalgorithm achieves a regret of $\\mathcal{O}\\left(L\\mathrm{poly}(m, \\log K) \\log\nT \\right)$ at time $T$, as compared to $\\mathcal{O}(LK\\log T)$ for conventional\ncontextual bandits, assuming a constant gap between the best arm and the rest\nfor each context. These guarantees are obtained under mild sufficiency\nconditions on the factors that are weaker versions of the well-known\nStatistical RIP condition. We further propose a class of generative models that\nsatisfy our sufficient conditions, and derive a lower bound of\n$\\mathcal{O}\\left(Km\\log T\\right)$. These are the first regret guarantees for\nonline matrix completion with bandit feedback, when the rank is greater than\none. We further compare the performance of our algorithm with the state of the\nart, on synthetic and real world data-sets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 05:21:40 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 05:31:47 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 15:59:58 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Sen", "Rajat", ""], ["Shanmugam", "Karthikeyan", ""], ["Kocaoglu", "Murat", ""], ["Dimakis", "Alexandros G.", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1606.00128", "submitter": "Yanbo Fan", "authors": "Yanbo Fan, Ran He, Jian Liang, Bao-Gang Hu", "title": "Self-Paced Learning: an Implicit Regularization Perspective", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-paced learning (SPL) mimics the cognitive mechanism of humans and\nanimals that gradually learns from easy to hard samples. One key issue in SPL\nis to obtain better weighting strategy that is determined by minimizer\nfunction. Existing methods usually pursue this by artificially designing the\nexplicit form of SPL regularizer. In this paper, we focus on the minimizer\nfunction, and study a group of new regularizer, named self-paced implicit\nregularizer that is deduced from robust loss function. Based on the convex\nconjugacy theory, the minimizer function for self-paced implicit regularizer\ncan be directly learned from the latent loss function, while the analytic form\nof the regularizer can be even known. A general framework (named SPL-IR) for\nSPL is developed accordingly. We demonstrate that the learning procedure of\nSPL-IR is associated with latent robust loss functions, thus can provide some\ntheoretical inspirations for its working mechanism. We further analyze the\nrelation between SPL-IR and half-quadratic optimization. Finally, we implement\nSPL-IR to both supervised and unsupervised tasks, and experimental results\ncorroborate our ideas and demonstrate the correctness and effectiveness of\nimplicit regularizers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 06:18:29 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 04:17:56 GMT"}, {"version": "v3", "created": "Sun, 18 Sep 2016 15:32:47 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Fan", "Yanbo", ""], ["He", "Ran", ""], ["Liang", "Jian", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1606.00136", "submitter": "Ichiro Takeuchi Prof.", "authors": "Hiroyuki Hanada, Atsushi Shibagaki, Jun Sakuma, Ichiro Takeuchi", "title": "Efficiently Bounding Optimal Solutions after Small Data Modification in\n  Large-Scale Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale classification problems in changing environments where a\nsmall part of the dataset is modified, and the effect of the data modification\nmust be quickly incorporated into the classifier. When the entire dataset is\nlarge, even if the amount of the data modification is fairly small, the\ncomputational cost of re-training the classifier would be prohibitively large.\nIn this paper, we propose a novel method for efficiently incorporating such a\ndata modification effect into the classifier without actually re-training it.\nThe proposed method provides bounds on the unknown optimal classifier with the\ncost only proportional to the size of the data modification. We demonstrate\nthrough numerical experiments that the proposed method provides sufficiently\ntight bounds with negligible computational costs, especially when a small part\nof the dataset is modified in a large-scale classification problem.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 06:56:17 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Hanada", "Hiroyuki", ""], ["Shibagaki", "Atsushi", ""], ["Sakuma", "Jun", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1606.00182", "submitter": "G\\'eraud Le Falher", "authors": "G\\'eraud Le Falher, Nicol\\`o Cesa-Bianchi, Claudio Gentile, Fabio\n  Vitale", "title": "On the Troll-Trust Model for Edge Sign Prediction in Social Networks", "comments": "v5: accepted to AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of edge sign prediction, we are given a directed graph\n(representing a social network), and our task is to predict the binary labels\nof the edges (i.e., the positive or negative nature of the social\nrelationships). Many successful heuristics for this problem are based on the\ntroll-trust features, estimating at each node the fraction of outgoing and\nincoming positive/negative edges. We show that these heuristics can be\nunderstood, and rigorously analyzed, as approximators to the Bayes optimal\nclassifier for a simple probabilistic model of the edge labels. We then show\nthat the maximum likelihood estimator for this model approximately corresponds\nto the predictions of a Label Propagation algorithm run on a transformed\nversion of the original social graph. Extensive experiments on a number of\nreal-world datasets show that this algorithm is competitive against\nstate-of-the-art classifiers in terms of both accuracy and scalability.\nFinally, we show that troll-trust features can also be used to derive online\nlearning algorithms which have theoretical guarantees even when edges are\nadversarially labeled.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 09:16:46 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 13:39:36 GMT"}, {"version": "v3", "created": "Fri, 17 Jun 2016 16:47:46 GMT"}, {"version": "v4", "created": "Fri, 14 Oct 2016 09:39:59 GMT"}, {"version": "v5", "created": "Tue, 28 Feb 2017 21:33:41 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Falher", "G\u00e9raud Le", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Gentile", "Claudio", ""], ["Vitale", "Fabio", ""]]}, {"id": "1606.00226", "submitter": "Richard Combes", "authors": "Thomas Bonald and Richard Combes", "title": "A Minimax Optimal Algorithm for Crowdsourcing", "comments": "19 pages, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of accurately estimating the reliability of workers\nbased on noisy labels they provide, which is a fundamental question in\ncrowdsourcing. We propose a novel lower bound on the minimax estimation error\nwhich applies to any estimation procedure. We further propose Triangular\nEstimation (TE), an algorithm for estimating the reliability of workers. TE has\nlow complexity, may be implemented in a streaming setting when labels are\nprovided by workers in real time, and does not rely on an iterative procedure.\nWe further prove that TE is minimax optimal and matches our lower bound. We\nconclude by assessing the performance of TE and other state-of-the-art\nalgorithms on both synthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 11:18:21 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 16:19:38 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Bonald", "Thomas", ""], ["Combes", "Richard", ""]]}, {"id": "1606.00253", "submitter": "Georgios Balikas", "authors": "Georgios Balikas, Massih-Reza Amini, Marianne Clausel", "title": "On a Topic Model for Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic topic models are generative models that describe the content of\ndocuments by discovering the latent topics underlying them. However, the\nstructure of the textual input, and for instance the grouping of words in\ncoherent text spans such as sentences, contains much information which is\ngenerally lost with these models. In this paper, we propose sentenceLDA, an\nextension of LDA whose goal is to overcome this limitation by incorporating the\nstructure of the text in the generative and inference processes. We illustrate\nthe advantages of sentenceLDA by comparing it with LDA using both intrinsic\n(perplexity) and extrinsic (text classification) evaluation tasks on different\ntext collections.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:34:50 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Balikas", "Georgios", ""], ["Amini", "Massih-Reza", ""], ["Clausel", "Marianne", ""]]}, {"id": "1606.00282", "submitter": "Ubai Sandouk", "authors": "Ubai Sandouk and Ke Chen", "title": "Multi-Label Zero-Shot Learning via Concept Embedding", "comments": "15 pages. Technical Report 2016-06-01. School of Computer Science.\n  The University of Manchester. (Submitted to a Journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero Shot Learning (ZSL) enables a learning model to classify instances of an\nunseen class during training. While most research in ZSL focuses on\nsingle-label classification, few studies have been done in multi-label ZSL,\nwhere an instance is associated with a set of labels simultaneously, due to the\ndifficulty in modeling complex semantics conveyed by a set of labels. In this\npaper, we propose a novel approach to multi-label ZSL via concept embedding\nlearned from collections of public users' annotations of multimedia. Thanks to\nconcept embedding, multi-label ZSL can be done by efficiently mapping an\ninstance input features onto the concept embedding space in a similar manner\nused in single-label ZSL. Moreover, our semantic learning model is capable of\nembedding an out-of-vocabulary label by inferring its meaning from its\nco-occurring labels. Thus, our approach allows both seen and unseen labels\nduring the concept embedding learning to be used in the aforementioned instance\nmapping, which makes multi-label ZSL more flexible and suitable for real\napplications. Experimental results of multi-label ZSL on images and music\ntracks suggest that our approach outperforms a state-of-the-art multi-label ZSL\nmodel and can deal with a scenario involving out-of-vocabulary labels without\nre-training the semantics learning model.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 13:38:04 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Sandouk", "Ubai", ""], ["Chen", "Ke", ""]]}, {"id": "1606.00298", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, George Fazekas, Mark Sandler", "title": "Automatic tagging using deep convolutional neural networks", "comments": "Accepted to ISMIR (International Society of Music Information\n  Retrieval) Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a content-based automatic music tagging algorithm using fully\nconvolutional neural networks (FCNs). We evaluate different architectures\nconsisting of 2D convolutional layers and subsampling layers only. In the\nexperiments, we measure the AUC-ROC scores of the architectures with different\ncomplexities and input types using the MagnaTagATune dataset, where a 4-layer\narchitecture shows state-of-the-art performance with mel-spectrogram input.\nFurthermore, we evaluated the performances of the architectures with varying\nthe number of layers on a larger dataset (Million Song Dataset), and found that\ndeeper models outperformed the 4-layer architecture. The experiments show that\nmel-spectrogram is an effective time-frequency representation for automatic\ntagging and that more complex models benefit from more training data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 14:18:08 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""]]}, {"id": "1606.00313", "submitter": "Vasilis Syrgkanis", "authors": "Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, Robert E.\n  Schapire", "title": "Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an oracle-based algorithm for the adversarial contextual bandit\nproblem, where either contexts are drawn i.i.d. or the sequence of contexts is\nknown a priori, but where the losses are picked adversarially. Our algorithm is\ncomputationally efficient, assuming access to an offline optimization oracle,\nand enjoys a regret of order $O((KT)^{\\frac{2}{3}}(\\log N)^{\\frac{1}{3}})$,\nwhere $K$ is the number of actions, $T$ is the number of iterations and $N$ is\nthe number of baseline policies. Our result is the first to break the\n$O(T^{\\frac{3}{4}})$ barrier that is achieved by recently introduced\nalgorithms. Breaking this barrier was left as a major open problem. Our\nanalysis is based on the recent relaxation based approach of (Rakhlin and\nSridharan, 2016).\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 14:47:19 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Syrgkanis", "Vasilis", ""], ["Luo", "Haipeng", ""], ["Krishnamurthy", "Akshay", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1606.00370", "submitter": "Sarah Ostadabbas", "authors": "Maria S. Perez-Rosero, Behnaz Rezaei, Murat Akcakaya, and Sarah\n  Ostadabbas", "title": "Decoding Emotional Experience through Physiological Signal Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing consensus among re- searchers that making a computer\nemotionally intelligent with the ability to decode human affective states would\nallow a more meaningful and natural way of human-computer interactions (HCIs).\nOne unobtrusive and non-invasive way of recognizing human affective states\nentails the exploration of how physiological signals vary under different\nemotional experiences. In particular, this paper explores the correlation\nbetween autonomically-mediated changes in multimodal body signals and discrete\nemotional states. In order to fully exploit the information in each modality,\nwe have provided an innovative classification approach for three specific\nphysiological signals including Electromyogram (EMG), Blood Volume Pressure\n(BVP) and Galvanic Skin Response (GSR). These signals are analyzed as inputs to\nan emotion recognition paradigm based on fusion of a series of weak learners.\nOur proposed classification approach showed 88.1% recognition accuracy, which\noutperformed the conventional Support Vector Machine (SVM) classifier with 17%\naccuracy improvement. Furthermore, in order to avoid information redundancy and\nthe resultant over-fitting, a feature reduction method is proposed based on a\ncorrelation analysis to optimize the number of features required for training\nand validating each weak learner. Results showed that despite the feature space\ndimensionality reduction from 27 to 18 features, our methodology preserved the\nrecognition accuracy of about 85.0%. This reduction in complexity will get us\none step closer towards embedding this human emotion encoder in the wireless\nand wearable HCI platforms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 17:52:30 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Perez-Rosero", "Maria S.", ""], ["Rezaei", "Behnaz", ""], ["Akcakaya", "Murat", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1606.00372", "submitter": "Rami Al-Rfou", "authors": "Rami Al-Rfou and Marc Pickett and Javier Snaider and Yun-hsuan Sung\n  and Brian Strope and Ray Kurzweil", "title": "Conversational Contextual Cues: The Case of Personalization and History\n  for Response Ranking", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the task of modeling open-domain, multi-turn, unstructured,\nmulti-participant, conversational dialogue. We specifically study the effect of\nincorporating different elements of the conversation. Unlike previous efforts,\nwhich focused on modeling messages and responses, we extend the modeling to\nlong context and participant's history. Our system does not rely on handwritten\nrules or engineered features; instead, we train deep neural networks on a large\nconversational dataset. In particular, we exploit the structure of Reddit\ncomments and posts to extract 2.1 billion messages and 133 million\nconversations. We evaluate our models on the task of predicting the next\nresponse in a conversation, and we find that modeling both context and\nparticipants improves prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:01:14 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Al-Rfou", "Rami", ""], ["Pickett", "Marc", ""], ["Snaider", "Javier", ""], ["Sung", "Yun-hsuan", ""], ["Strope", "Brian", ""], ["Kurzweil", "Ray", ""]]}, {"id": "1606.00389", "submitter": "Tianyi Zhou", "authors": "Tianyi Zhou and Jeff Bilmes", "title": "Stream Clipper: Scalable Submodular Maximization on Stream", "comments": "17 pages, 12 figures, submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a streaming submodular maximization algorithm \"stream clipper\"\nthat performs as well as the offline greedy algorithm on document/video\nsummarization in practice. It adds elements from a stream either to a solution\nset $S$ or to an extra buffer $B$ based on two adaptive thresholds, and\nimproves $S$ by a final greedy step that starts from $S$ adding elements from\n$B$. During this process, swapping elements out of $S$ can occur if doing so\nyields improvements. The thresholds adapt based on if current memory\nutilization exceeds a budget, e.g., it increases the lower threshold, and\nremoves from the buffer $B$ elements below the new lower threshold. We show\nthat, while our approximation factor in the worst case is $1/2$ (like in\nprevious work, and corresponding to the tight bound), we show that there are\ndata-dependent conditions where our bound falls within the range $[1/2,\n1-1/e]$. In news and video summarization experiments, the algorithm\nconsistently outperforms other streaming methods, and, while using\nsignificantly less computation and memory, performs similarly to the offline\ngreedy algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:43:13 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 22:36:44 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 01:50:38 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Zhou", "Tianyi", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1606.00398", "submitter": "Sherenaz Al-Haj Baddar", "authors": "Sherenaz W. Al-Haj Baddar", "title": "Short Communication on QUIST: A Quick Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short communication we introduce the quick clustering algorithm\n(QUIST), an efficient hierarchical clustering algorithm based on sorting. QUIST\nis a poly-logarithmic divisive clustering algorithm that does not assume the\nnumber of clusters, and/or the cluster size to be known ahead of time. It is\nalso insensitive to the original ordering of the input.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:57:54 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Baddar", "Sherenaz W. Al-Haj", ""]]}, {"id": "1606.00399", "submitter": "Tianyi Zhou", "authors": "Tianyi Zhou, Hua Ouyang, Yi Chang, Jeff Bilmes, Carlos Guestrin", "title": "Scaling Submodular Maximization via Pruned Submodularity Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new random pruning method (called \"submodular sparsification\n(SS)\") to reduce the cost of submodular maximization. The pruning is applied\nvia a \"submodularity graph\" over the $n$ ground elements, where each directed\nedge is associated with a pairwise dependency defined by the submodular\nfunction. In each step, SS prunes a $1-1/\\sqrt{c}$ (for $c>1$) fraction of the\nnodes using weights on edges computed based on only a small number ($O(\\log\nn)$) of randomly sampled nodes. The algorithm requires $\\log_{\\sqrt{c}}n$ steps\nwith a small and highly parallelizable per-step computation. An accuracy-speed\ntradeoff parameter $c$, set as $c = 8$, leads to a fast shrink rate\n$\\sqrt{2}/4$ and small iteration complexity $\\log_{2\\sqrt{2}}n$. Analysis shows\nthat w.h.p., the greedy algorithm on the pruned set of size $O(\\log^2 n)$ can\nachieve a guarantee similar to that of processing the original dataset. In news\nand video summarization tasks, SS is able to substantially reduce both\ncomputational costs and memory usage, while maintaining (or even slightly\nexceeding) the quality of the original (and much more costly) greedy algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:58:36 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Zhou", "Tianyi", ""], ["Ouyang", "Hua", ""], ["Chang", "Yi", ""], ["Bilmes", "Jeff", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1606.00511", "submitter": "Xi He", "authors": "Xi He and Dheevatsa Mudigere and Mikhail Smelyanskiy and Martin\n  Tak\\'a\\v{c}", "title": "Distributed Hessian-Free Optimization for Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural network is a high dimensional and a highly non-convex\noptimization problem. Stochastic gradient descent (SGD) algorithm and it's\nvariations are the current state-of-the-art solvers for this task. However, due\nto non-covexity nature of the problem, it was observed that SGD slows down near\nsaddle point. Recent empirical work claim that by detecting and escaping saddle\npoint efficiently, it's more likely to improve training performance. With this\nobjective, we revisit Hessian-free optimization method for deep networks. We\nalso develop its distributed variant and demonstrate superior scaling potential\nto SGD, which allows more efficiently utilizing larger computing resources thus\nenabling large models and faster time to obtain desired solution. Furthermore,\nunlike truncated Newton method (Marten's HF) that ignores negative curvature\ninformation by using na\\\"ive conjugate gradient method and Gauss-Newton Hessian\napproximation information - we propose a novel algorithm to explore negative\ncurvature direction by solving the sub-problem with stabilized bi-conjugate\nmethod involving possible indefinite stochastic Hessian information. We show\nthat these techniques accelerate the training process for both the standard\nMNIST dataset and also the TIMIT speech recognition problem, demonstrating\nrobust performance with upto an order of magnitude larger batch sizes. This\nincreased scaling potential is illustrated with near linear speed-up on upto 16\nCPU nodes for a simple 4-layer network.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 00:39:03 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 13:51:26 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["He", "Xi", ""], ["Mudigere", "Dheevatsa", ""], ["Smelyanskiy", "Mikhail", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1606.00540", "submitter": "Zhen Hu", "authors": "Zhen Hu, Zhuyin Xue, Tong Cui, Shiqiang Zong, Chenglong He", "title": "Multi-pretrained Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining is widely used in deep neutral network and one of the most famous\npretraining models is Deep Belief Network (DBN). The optimization formulas are\ndifferent during the pretraining process for different pretraining models. In\nthis paper, we pretrained deep neutral network by different pretraining models\nand hence investigated the difference between DBN and Stacked Denoising\nAutoencoder (SDA) when used as pretraining model. The experimental results show\nthat DBN get a better initial model. However the model converges to a\nrelatively worse model after the finetuning process. Yet after pretrained by\nSDA for the second time the model converges to a better model if finetuned.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 05:39:54 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Hu", "Zhen", ""], ["Xue", "Zhuyin", ""], ["Cui", "Tong", ""], ["Zong", "Shiqiang", ""], ["He", "Chenglong", ""]]}, {"id": "1606.00575", "submitter": "Shizhao Sun", "authors": "Shizhao Sun, Wei Chen, Jiang Bian, Xiaoguang Liu, Tie-Yan Liu", "title": "Ensemble-Compression: A New Method for Parallel Training of Deep Neural\n  Networks", "comments": "ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallelization framework has become a necessity to speed up the training of\ndeep neural networks (DNN) recently. Such framework typically employs the Model\nAverage approach, denoted as MA-DNN, in which parallel workers conduct\nrespective training based on their own local data while the parameters of local\nmodels are periodically communicated and averaged to obtain a global model\nwhich serves as the new start of local models. However, since DNN is a highly\nnon-convex model, averaging parameters cannot ensure that such global model can\nperform better than those local models. To tackle this problem, we introduce a\nnew parallel training framework called Ensemble-Compression, denoted as EC-DNN.\nIn this framework, we propose to aggregate the local models by ensemble, i.e.,\naveraging the outputs of local models instead of the parameters. As most of\nprevalent loss functions are convex to the output of DNN, the performance of\nensemble-based global model is guaranteed to be at least as good as the average\nperformance of local models. However, a big challenge lies in the explosion of\nmodel size since each round of ensemble can give rise to multiple times size\nincrement. Thus, we carry out model compression after each ensemble,\nspecialized by a distillation based method in this paper, to reduce the size of\nthe global model to be the same as the local ones. Our experimental results\ndemonstrate the prominent advantage of EC-DNN over MA-DNN in terms of both\naccuracy and speedup.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 08:10:10 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 08:50:05 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Sun", "Shizhao", ""], ["Chen", "Wei", ""], ["Bian", "Jiang", ""], ["Liu", "Xiaoguang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1606.00577", "submitter": "Justin Wood", "authors": "Justin Wood, Patrick Tan, Wei Wang, Corey Arnold", "title": "Source-LDA: Enhancing probabilistic topic models using prior knowledge\n  sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to topic modeling involves extracting co-occurring n-grams\nof a corpus into semantic themes. The set of n-grams in a theme represents an\nunderlying topic, but most topic modeling approaches are not able to label\nthese sets of words with a single n-gram. Such labels are useful for topic\nidentification in summarization systems. This paper introduces a novel approach\nto labeling a group of n-grams comprising an individual topic. The approach\ntaken is to complement the existing topic distributions over words with a known\ndistribution based on a predefined set of topics. This is done by integrating\nexisting labeled knowledge sources representing known potential topics into the\nprobabilistic topic model. These knowledge sources are translated into a\ndistribution and used to set the hyperparameters of the Dirichlet generated\ndistribution over words. In the inference these modified distributions guide\nthe convergence of the latent topics to conform with the complementary\ndistributions. This approach ensures that the topic inference process is\nconsistent with existing knowledge. The label assignment from the complementary\nknowledge sources are then transferred to the latent topics of the corpus. The\nresults show both accurate label assignment to topics as well as improved topic\ngeneration than those obtained using various labeling approaches based off\nLatent Dirichlet allocation (LDA).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 08:15:15 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 05:15:36 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 21:03:06 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Wood", "Justin", ""], ["Tan", "Patrick", ""], ["Wang", "Wei", ""], ["Arnold", "Corey", ""]]}, {"id": "1606.00602", "submitter": "Xiyu Yu PhD", "authors": "Xiyu Yu, Dacheng Tao", "title": "Variance-Reduced Proximal Stochastic Gradient Descent for Non-convex\n  Composite optimization", "comments": "This paper has been withdrawn by the author due to an error in the\n  proof of the convergence rate. They will modify this proof as soon as\n  possible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we study non-convex composite optimization: first, a finite-sum of\nsmooth but non-convex functions, and second, a general function that admits a\nsimple proximal mapping. Most research on stochastic methods for composite\noptimization assumes convexity or strong convexity of each function. In this\npaper, we extend this problem into the non-convex setting using variance\nreduction techniques, such as prox-SVRG and prox-SAGA. We prove that, with a\nconstant step size, both prox-SVRG and prox-SAGA are suitable for non-convex\ncomposite optimization, and help the problem converge to a stationary point\nwithin $O(1/\\epsilon)$ iterations. That is similar to the convergence rate seen\nwith the state-of-the-art RSAG method and faster than stochastic gradient\ndescent. Our analysis is also extended into the min-batch setting, which\nlinearly accelerates the convergence. To the best of our knowledge, this is the\nfirst analysis of convergence rate of variance-reduced proximal stochastic\ngradient for non-convex composite optimization.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 09:59:16 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 04:15:04 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Yu", "Xiyu", ""], ["Tao", "Dacheng", ""]]}, {"id": "1606.00611", "submitter": "Boris Knyazev", "authors": "Boris Knyazev, Erhardt Barth, Thomas Martinetz", "title": "Recursive Autoconvolution for Unsupervised Learning of Convolutional\n  Neural Networks", "comments": "8 pages, accepted to International Joint Conference on Neural\n  Networks (IJCNN 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual recognition tasks, such as image classification, unsupervised\nlearning exploits cheap unlabeled data and can help to solve these tasks more\nefficiently. We show that the recursive autoconvolution operator, adopted from\nphysics, boosts existing unsupervised methods by learning more discriminative\nfilters. We take well established convolutional neural networks and train their\nfilters layer-wise. In addition, based on previous works we design a network\nwhich extracts more than 600k features per sample, but with the total number of\ntrainable parameters greatly reduced by introducing shared filters in higher\nlayers. We evaluate our networks on the MNIST, CIFAR-10, CIFAR-100 and STL-10\nimage classification benchmarks and report several state of the art results\namong other unsupervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 10:37:46 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 18:31:05 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Knyazev", "Boris", ""], ["Barth", "Erhardt", ""], ["Martinetz", "Thomas", ""]]}, {"id": "1606.00704", "submitter": "Vincent Dumoulin", "authors": "Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro,\n  Alex Lamb, Martin Arjovsky, Aaron Courville", "title": "Adversarially Learned Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the adversarially learned inference (ALI) model, which jointly\nlearns a generation network and an inference network using an adversarial\nprocess. The generation network maps samples from stochastic latent variables\nto the data space while the inference network maps training examples in data\nspace to the space of latent variables. An adversarial game is cast between\nthese two networks and a discriminative network is trained to distinguish\nbetween joint latent/data-space samples from the generative network and joint\nsamples from the inference network. We illustrate the ability of the model to\nlearn mutually coherent inference and generation networks through the\ninspections of model samples and reconstructions and confirm the usefulness of\nthe learned representations by obtaining a performance competitive with\nstate-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 14:43:37 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 18:05:10 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 18:28:22 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Belghazi", "Ishmael", ""], ["Poole", "Ben", ""], ["Mastropietro", "Olivier", ""], ["Lamb", "Alex", ""], ["Arjovsky", "Martin", ""], ["Courville", "Aaron", ""]]}, {"id": "1606.00709", "submitter": "Sebastian Nowozin", "authors": "Sebastian Nowozin, Botond Cseke, Ryota Tomioka", "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence\n  Minimization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural samplers are probabilistic models that implement sampling\nusing feedforward neural networks: they take a random input vector and produce\na sample from a probability distribution defined by the network weights. These\nmodels are expressive and allow efficient computation of samples and\nderivatives, but cannot be used for computing likelihoods or for\nmarginalization. The generative-adversarial training method allows to train\nsuch models through the use of an auxiliary discriminative neural network. We\nshow that the generative-adversarial approach is a special case of an existing\nmore general variational divergence estimation approach. We show that any\nf-divergence can be used for training generative neural samplers. We discuss\nthe benefits of various choices of divergence functions on training complexity\nand the quality of the obtained generative models.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 14:53:33 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Nowozin", "Sebastian", ""], ["Cseke", "Botond", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1606.00720", "submitter": "Michael Smith", "authors": "Michael Thomas Smith, Max Zwiessele, Neil D. Lawrence", "title": "Differentially Private Gaussian Processes", "comments": "9 pages + 4 supplementary material pages, 6 plots grouped into 5\n  figures, accepted at AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for machine learning is increasing the availability of data\nwhile respecting the privacy of individuals. Here we combine the provable\nprivacy guarantees of the differential privacy framework with the flexibility\nof Gaussian processes (GPs). We propose a method using GPs to provide\ndifferentially private (DP) regression. We then improve this method by crafting\nthe DP noise covariance structure to efficiently protect the training data,\nwhile minimising the scale of the added noise. We find that this cloaking\nmethod achieves the greatest accuracy, while still providing privacy\nguarantees, and offers practical DP for regression over multi-dimensional\ninputs. Together these methods provide a starter toolkit for combining\ndifferential privacy and GPs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 15:26:53 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 11:56:29 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 16:34:53 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Smith", "Michael Thomas", ""], ["Zwiessele", "Max", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1606.00739", "submitter": "Stefan Riezler", "authors": "Artem Sokolov and Julia Kreutzer and Christopher Lo and Stefan Riezler", "title": "Stochastic Structured Prediction under Bandit Feedback", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic structured prediction under bandit feedback follows a learning\nprotocol where on each of a sequence of iterations, the learner receives an\ninput, predicts an output structure, and receives partial feedback in form of a\ntask loss evaluation of the predicted structure. We present applications of\nthis learning scenario to convex and non-convex objectives for structured\nprediction and analyze them as stochastic first-order methods. We present an\nexperimental evaluation on problems of natural language processing over\nexponential output spaces, and compare convergence speed across different\nobjectives under the practical criterion of optimal task performance on\ndevelopment data and the optimization-theoretic criterion of minimal squared\ngradient norm. Best results under both criteria are obtained for a non-convex\nobjective for pairwise preference learning under bandit feedback.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 16:06:29 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 16:29:42 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Sokolov", "Artem", ""], ["Kreutzer", "Julia", ""], ["Lo", "Christopher", ""], ["Riezler", "Stefan", ""]]}, {"id": "1606.00776", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula,\n  Bowen Zhou, Yoshua Bengio, Aaron Courville", "title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue\n  Response Generation", "comments": "21 pages, 2 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the multiresolution recurrent neural network, which extends the\nsequence-to-sequence framework to model natural language generation as two\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\nand a sequence of natural language tokens. There are many ways to estimate or\nlearn the high-level coarse tokens, but we argue that a simple extraction\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\nSuch procedure allows training the multiresolution recurrent neural network by\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\nthe standard log- likelihood objective w.r.t. natural language tokens (word\nperplexity), optimizing the joint log-likelihood biases the model towards\nmodeling high-level abstractions. We apply the proposed model to the task of\ndialogue response generation in two challenging domains: the Ubuntu technical\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\ncompeting approaches by a substantial margin, achieving state-of-the-art\nresults according to both automatic evaluation metrics and a human evaluation\nstudy. On Twitter, the model appears to generate more relevant and on-topic\nresponses according to automatic evaluation metrics. Finally, our experiments\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\nnatural language and is better able to capture long-term structure.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 17:37:31 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 02:01:16 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Klinger", "Tim", ""], ["Tesauro", "Gerald", ""], ["Talamadupula", "Kartik", ""], ["Zhou", "Bowen", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "1606.00787", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Eric Xing", "title": "Post-Inference Prior Swapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Bayesian methods are praised for their ability to incorporate useful\nprior knowledge, in practice, convenient priors that allow for computationally\ncheap or tractable inference are commonly used. In this paper, we investigate\nthe following question: for a given model, is it possible to compute an\ninference result with any convenient false prior, and afterwards, given any\ntarget prior of interest, quickly transform this result into the target\nposterior? A potential solution is to use importance sampling (IS). However, we\ndemonstrate that IS will fail for many choices of the target prior, depending\non its parametric form and similarity to the false prior. Instead, we propose\nprior swapping, a method that leverages the pre-inferred false posterior to\nefficiently generate accurate posterior samples under arbitrary target priors.\nPrior swapping lets us apply less-costly inference algorithms to certain\nmodels, and incorporate new or updated prior information \"post-inference\". We\ngive theoretical guarantees about our method, and demonstrate it empirically on\na number of models and priors.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 18:20:35 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 18:01:17 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Neiswanger", "Willie", ""], ["Xing", "Eric", ""]]}, {"id": "1606.00856", "submitter": "Jesus Malo", "authors": "Valero Laparra and Jesus Malo", "title": "Sequential Principal Curves Analysis", "comments": "17 pages, 14 figs., 72 refs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work includes all the technical details of the Sequential Principal\nCurves Analysis (SPCA) in a single document. SPCA is an unsupervised nonlinear\nand invertible feature extraction technique. The identified curvilinear\nfeatures can be interpreted as a set of nonlinear sensors: the response of each\nsensor is the projection onto the corresponding feature. Moreover, it can be\neasily tuned for different optimization criteria; e.g. infomax, error\nminimization, decorrelation; by choosing the right way to measure distances\nalong each curvilinear feature. Even though proposed in [Laparra et al. Neural\nComp. 12] and shown to work in multiple modalities in [Laparra and Malo\nFrontiers Hum. Neuro. 15], the SPCA framework has its original roots in the\nnonlinear ICA algorithm in [Malo and Gutierrez Network 06]. Later on, the SPCA\nphilosophy for nonlinear generalization of PCA originated substantially faster\nalternatives at the cost of introducing different constraints in the model.\nNamely, the Principal Polynomial Analysis (PPA) [Laparra et al. IJNS 14], and\nthe Dimensionality Reduction via Regression (DRR) [Laparra et al. IEEE TGRS\n15]. This report illustrates the reasons why we developed such family and is\nthe appropriate technical companion for the missing details in [Laparra et al.,\nNeCo 12, Laparra and Malo, Front.Hum.Neuro. 15]. See also the data, code and\nexamples in the dedicated sites http://isp.uv.es/spca.html and\nhttp://isp.uv.es/after effects.html\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 20:23:00 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Laparra", "Valero", ""], ["Malo", "Jesus", ""]]}, {"id": "1606.00868", "submitter": "Aykut Firat", "authors": "Aykut Firat", "title": "Unified Framework for Quantification", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification is the machine learning task of estimating test-data class\nproportions that are not necessarily similar to those in training. Apart from\nits intrinsic value as an aggregate statistic, quantification output can also\nbe used to optimize classifier probabilities, thereby increasing classification\naccuracy. We unify major quantification approaches under a constrained\nmulti-variate regression framework, and use mathematical programming to\nestimate class proportions for different loss functions. With this modeling\napproach, we extend existing binary-only quantification approaches to\nmulti-class settings as well. We empirically verify our unified framework by\nexperimenting with several multi-class datasets including the Stanford\nSentiment Treebank and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 20:42:31 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Firat", "Aykut", ""]]}, {"id": "1606.00897", "submitter": "Stefan Bauer", "authors": "Stefan Bauer and Nicolas Carion and Peter Sch\\\"uffler and Thomas Fuchs\n  and Peter Wild and Joachim M. Buhmann", "title": "Multi-Organ Cancer Classification and Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.TO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust cell nuclei classification is the cornerstone for a wider\nrange of tasks in digital and Computational Pathology. However, most machine\nlearning systems require extensive labeling from expert pathologists for each\nindividual problem at hand, with no or limited abilities for knowledge transfer\nbetween datasets and organ sites. In this paper we implement and evaluate a\nvariety of deep neural network models and model ensembles for nuclei\nclassification in renal cell cancer (RCC) and prostate cancer (PCa). We propose\na convolutional neural network system based on residual learning which\nsignificantly improves over the state-of-the-art in cell nuclei classification.\nFinally, we show that the combination of tissue types during training increases\nnot only classification accuracy but also overall survival analysis.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 21:09:00 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 20:06:14 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Bauer", "Stefan", ""], ["Carion", "Nicolas", ""], ["Sch\u00fcffler", "Peter", ""], ["Fuchs", "Thomas", ""], ["Wild", "Peter", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1606.00911", "submitter": "Peter Landgren", "authors": "Peter Landgren, Vaibhav Srivastava, and Naomi Ehrich Leonard", "title": "Distributed Cooperative Decision-Making in Multiarmed Bandits:\n  Frequentist and Bayesian Algorithms", "comments": "This revision provides a correction to the original paper, which\n  appeared in the Proceedings of the 2016 IEEE Conference on Decision and\n  Control (CDC). The second statement of Proposition 1 and Theorem 1 are new\n  from arXiv:1512.06888v3 and Lemma 1 is new. These are used to prove regret\n  bounds in Theorems 2 and 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed cooperative decision-making under the explore-exploit\ntradeoff in the multiarmed bandit (MAB) problem. We extend the state-of-the-art\nfrequentist and Bayesian algorithms for single-agent MAB problems to\ncooperative distributed algorithms for multi-agent MAB problems in which agents\ncommunicate according to a fixed network graph. We rely on a running consensus\nalgorithm for each agent's estimation of mean rewards from its own rewards and\nthe estimated rewards of its neighbors. We prove the performance of these\nalgorithms and show that they asymptotically recover the performance of a\ncentralized agent. Further, we rigorously characterize the influence of the\ncommunication graph structure on the decision-making performance of the group.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 21:49:25 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 01:14:13 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 04:49:44 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Landgren", "Peter", ""], ["Srivastava", "Vaibhav", ""], ["Leonard", "Naomi Ehrich", ""]]}, {"id": "1606.00917", "submitter": "Faizan Javed", "authors": "Faizan Javed, Matt McNair, Ferosh Jacob, Meng Zhao", "title": "Towards a Job Title Classification System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document classification for text, images and other applicable entities has\nlong been a focus of research in academia and also finds application in many\nindustrial settings. Amidst a plethora of approaches to solve such problems,\nmachine-learning techniques have found success in a variety of scenarios. In\nthis paper we discuss the design of a machine learning-based semi-supervised\njob title classification system for the online job recruitment domain currently\nin production at CareerBuilder.com and propose enhancements to it. The system\nleverages a varied collection of classification as well clustering algorithms.\nThese algorithms are encompassed in an architecture that facilitates leveraging\nexisting off-the-shelf machine learning tools and techniques while keeping into\nconsideration the challenges of constructing a scalable classification system\nfor a large taxonomy of categories. As a continuously evolving system that is\nstill under development we first discuss the existing semi-supervised\nclassification system which is composed of both clustering and classification\ncomponents in a proximity-based classifier setup and results of which are\nalready used across numerous products at CareerBuilder. We then elucidate our\nlong-term goals for job title classification and propose enhancements to the\nexisting system in the form of a two-stage coarse and fine level classifier\naugmentation to construct a cascade of hierarchical vertical classifiers.\nPreliminary results are presented using experimental evaluation on real world\nindustrial data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 22:01:50 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Javed", "Faizan", ""], ["McNair", "Matt", ""], ["Jacob", "Ferosh", ""], ["Zhao", "Meng", ""]]}, {"id": "1606.00925", "submitter": "Qingyun Sun", "authors": "Qingyun Sun, Mengyuan Yan David Donoho and Stephen Boyd", "title": "Convolutional Imputation of Matrix Networks", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matrix network is a family of matrices, with relatedness modeled by a\nweighted graph. We consider the task of completing a partially observed matrix\nnetwork. We assume a novel sampling scheme where a fraction of matrices might\nbe completely unobserved. How can we recover the entire matrix network from\nincomplete observations? This mathematical problem arises in many applications\nincluding medical imaging and social networks.\n  To recover the matrix network, we propose a structural assumption that the\nmatrices have a graph Fourier transform which is low-rank. We formulate a\nconvex optimization problem and prove an exact recovery guarantee for the\noptimization problem. Furthermore, we numerically characterize the exact\nrecovery regime for varying rank and sampling rate and discover a new phase\ntransition phenomenon. Then we give an iterative imputation algorithm to\nefficiently solve the optimization problem and complete large scale matrix\nnetworks. We demonstrate the algorithm with a variety of applications such as\nMRI and Facebook user network.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 22:40:59 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 22:37:05 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 21:01:10 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Sun", "Qingyun", ""], ["Donoho", "Mengyuan Yan David", ""], ["Boyd", "Stephen", ""]]}, {"id": "1606.00930", "submitter": "Jacques Wainer", "authors": "Jacques Wainer", "title": "Comparison of 14 different families of classification algorithms on 115\n  binary datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tested 14 very different classification algorithms (random forest,\ngradient boosting machines, SVM - linear, polynomial, and RBF - 1-hidden-layer\nneural nets, extreme learning machines, k-nearest neighbors and a bagging of\nknn, naive Bayes, learning vector quantization, elastic net logistic\nregression, sparse linear discriminant analysis, and a boosting of linear\nclassifiers) on 115 real life binary datasets. We followed the Demsar analysis\nand found that the three best classifiers (random forest, gbm and RBF SVM) are\nnot significantly different from each other. We also discuss that a change of\nless then 0.0112 in the error rate should be considered as an irrelevant\nchange, and used a Bayesian ANOVA analysis to conclude that with high\nprobability the differences between these three classifiers is not of practical\nconsequence. We also verified the execution time of \"standard implementations\"\nof these algorithms and concluded that RBF SVM is the fastest (significantly\nso) both in training time and in training plus testing time.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 23:01:25 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Wainer", "Jacques", ""]]}, {"id": "1606.00968", "submitter": "Hoang M. Le", "authors": "Hoang M. Le, Andrew Kang, Yisong Yue and Peter Carr", "title": "Smooth Imitation Learning for Online Sequence Prediction", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of smooth imitation learning for online sequence\nprediction, where the goal is to train a policy that can smoothly imitate\ndemonstrated behavior in a dynamic and continuous environment in response to\nonline, sequential context input. Since the mapping from context to behavior is\noften complex, we take a learning reduction approach to reduce smooth imitation\nlearning to a regression problem using complex function classes that are\nregularized to ensure smoothness. We present a learning meta-algorithm that\nachieves fast and stable convergence to a good policy. Our approach enjoys\nseveral attractive properties, including being fully deterministic, employing\nan adaptive learning rate that can provably yield larger policy improvements\ncompared to previous approaches, and the ability to ensure stable convergence.\nOur empirical results demonstrate significant performance gains over previous\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 05:25:27 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Le", "Hoang M.", ""], ["Kang", "Andrew", ""], ["Yue", "Yisong", ""], ["Carr", "Peter", ""]]}, {"id": "1606.00972", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Song-Chun Zhu, Ying Nian Wu", "title": "Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video sequences contain rich dynamic patterns, such as dynamic texture\npatterns that exhibit stationarity in the temporal domain, and action patterns\nthat are non-stationary in either spatial or temporal domain. We show that a\nspatial-temporal generative ConvNet can be used to model and synthesize dynamic\npatterns. The model defines a probability distribution on the video sequence,\nand the log probability is defined by a spatial-temporal ConvNet that consists\nof multiple layers of spatial-temporal filters to capture spatial-temporal\npatterns of different scales. The model can be learned from the training video\nsequences by an \"analysis by synthesis\" learning algorithm that iterates the\nfollowing two steps. Step 1 synthesizes video sequences from the currently\nlearned model. Step 2 then updates the model parameters based on the difference\nbetween the synthesized video sequences and the observed training sequences. We\nshow that the learning algorithm can synthesize realistic dynamic patterns.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 05:36:06 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 23:26:38 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Xie", "Jianwen", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1606.00985", "submitter": "Enmei Tu", "authors": "Enmei Tu, Yaqian Zhang, Lin Zhu, Jie Yang and Nikola Kasabov", "title": "A Graph-Based Semi-Supervised k Nearest-Neighbor Method for Nonlinear\n  Manifold Distributed Data Classification", "comments": "32 pages, 12 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $k$ Nearest Neighbors ($k$NN) is one of the most widely used supervised\nlearning algorithms to classify Gaussian distributed data, but it does not\nachieve good results when it is applied to nonlinear manifold distributed data,\nespecially when a very limited amount of labeled samples are available. In this\npaper, we propose a new graph-based $k$NN algorithm which can effectively\nhandle both Gaussian distributed data and nonlinear manifold distributed data.\nTo achieve this goal, we first propose a constrained Tired Random Walk (TRW) by\nconstructing an $R$-level nearest-neighbor strengthened tree over the graph,\nand then compute a TRW matrix for similarity measurement purposes. After this,\nthe nearest neighbors are identified according to the TRW matrix and the class\nlabel of a query point is determined by the sum of all the TRW weights of its\nnearest neighbors. To deal with online situations, we also propose a new\nalgorithm to handle sequential samples based a local neighborhood\nreconstruction. Comparison experiments are conducted on both synthetic data\nsets and real-world data sets to demonstrate the validity of the proposed new\n$k$NN algorithm and its improvements to other version of $k$NN algorithms.\nGiven the widespread appearance of manifold structures in real-world problems\nand the popularity of the traditional $k$NN algorithm, the proposed manifold\nversion $k$NN shows promising potential for classifying manifold-distributed\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 07:09:26 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Tu", "Enmei", ""], ["Zhang", "Yaqian", ""], ["Zhu", "Lin", ""], ["Yang", "Jie", ""], ["Kasabov", "Nikola", ""]]}, {"id": "1606.01042", "submitter": "Alexy Bhowmick Mr", "authors": "Alexy Bhowmick, Shyamanta M. Hazarika", "title": "Machine Learning for E-mail Spam Filtering: Review,Techniques and Trends", "comments": "Journal. 27 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive review of the most effective content-based e-mail\nspam filtering techniques. We focus primarily on Machine Learning-based spam\nfilters and their variants, and report on a broad review ranging from surveying\nthe relevant ideas, efforts, effectiveness, and the current progress. The\ninitial exposition of the background examines the basics of e-mail spam\nfiltering, the evolving nature of spam, spammers playing cat-and-mouse with\ne-mail service providers (ESPs), and the Machine Learning front in fighting\nspam. We conclude by measuring the impact of Machine Learning-based filters and\nexplore the promising offshoots of latest developments.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 10:58:37 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Bhowmick", "Alexy", ""], ["Hazarika", "Shyamanta M.", ""]]}, {"id": "1606.01128", "submitter": "Bilal Piot", "authors": "Bilal Piot, Matthieu Geist, Olivier Pietquin", "title": "Difference of Convex Functions Programming Applied to Control with\n  Expert Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports applications of Difference of Convex functions (DC)\nprogramming to Learning from Demonstrations (LfD) and Reinforcement Learning\n(RL) with expert data. This is made possible because the norm of the Optimal\nBellman Residual (OBR), which is at the heart of many RL and LfD algorithms, is\nDC. Improvement in performance is demonstrated on two specific algorithms,\nnamely Reward-regularized Classification for Apprenticeship Learning (RCAL) and\nReinforcement Learning with Expert Demonstrations (RLED), through experiments\non generic Markov Decision Processes (MDP), called Garnets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 15:07:52 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 08:12:15 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Piot", "Bilal", ""], ["Geist", "Matthieu", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1606.01141", "submitter": "Nils Kriege", "authors": "Nils M. Kriege, Pierre-Louis Giscard, Richard C. Wilson", "title": "On Valid Optimal Assignment Kernels and Applications to Graph\n  Classification", "comments": "9 pages, 4 figures, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of kernel methods has initiated the design of novel positive\nsemidefinite functions, in particular for structured data. A leading design\nparadigm for this is the convolution kernel, which decomposes structured\nobjects into their parts and sums over all pairs of parts. Assignment kernels,\nin contrast, are obtained from an optimal bijection between parts, which can\nprovide a more valid notion of similarity. In general however, optimal\nassignments yield indefinite functions, which complicates their use in kernel\nmethods. We characterize a class of base kernels used to compare parts that\nguarantees positive semidefinite optimal assignment kernels. These base kernels\ngive rise to hierarchies from which the optimal assignment kernels are computed\nin linear time by histogram intersection. We apply these results by developing\nthe Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high\nclassification accuracy on widely-used benchmark data sets improving over the\noriginal Weisfeiler-Lehman kernel.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 15:32:27 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 12:25:27 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 14:56:20 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Kriege", "Nils M.", ""], ["Giscard", "Pierre-Louis", ""], ["Wilson", "Richard C.", ""]]}, {"id": "1606.01160", "submitter": "Dong Huang", "authors": "Dong Huang and Jian-Huang Lai and Chang-Dong Wang", "title": "Robust Ensemble Clustering Using Probability Trajectories", "comments": "The MATLAB code and experimental data of this work are available at:\n  https://www.researchgate.net/publication/284259332", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 2016, vol.28,\n  no.5, pp.1312-1326", "doi": "10.1109/TKDE.2015.2503753", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many successful ensemble clustering approaches have been developed\nin recent years, there are still two limitations to most of the existing\napproaches. First, they mostly overlook the issue of uncertain links, which may\nmislead the overall consensus process. Second, they generally lack the ability\nto incorporate global information to refine the local links. To address these\ntwo limitations, in this paper, we propose a novel ensemble clustering approach\nbased on sparse graph representation and probability trajectory analysis. In\nparticular, we present the elite neighbor selection strategy to identify the\nuncertain links by locally adaptive thresholds and build a sparse graph with a\nsmall number of probably reliable links. We argue that a small number of\nprobably reliable links can lead to significantly better consensus results than\nusing all graph links regardless of their reliability. The random walk process\ndriven by a new transition probability matrix is utilized to explore the global\ninformation in the graph. We derive a novel and dense similarity measure from\nthe sparse graph by analyzing the probability trajectories of the random\nwalkers, based on which two consensus functions are further proposed.\nExperimental results on multiple real-world datasets demonstrate the\neffectiveness and efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 16:09:32 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Huang", "Dong", ""], ["Lai", "Jian-Huang", ""], ["Wang", "Chang-Dong", ""]]}, {"id": "1606.01164", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John J Hopfield", "title": "Dense Associative Memory for Pattern Recognition", "comments": "Accepted for publication at NIPS 2016", "journal-ref": "Advances in Neural Information Processing Systems 29 (2016),\n  1172-1180", "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of associative memory is studied, which stores and reliably retrieves\nmany more patterns than the number of neurons in the network. We propose a\nsimple duality between this dense associative memory and neural networks\ncommonly used in deep learning. On the associative memory side of this duality,\na family of models that smoothly interpolates between two limiting cases can be\nconstructed. One limit is referred to as the feature-matching mode of pattern\nrecognition, and the other one as the prototype regime. On the deep learning\nside of the duality, this family corresponds to feedforward neural networks\nwith one hidden layer and various activation functions, which transmit the\nactivities of the visible neurons to the hidden layer. This family of\nactivation functions includes logistics, rectified linear units, and rectified\npolynomials of higher degrees. The proposed duality makes it possible to apply\nenergy-based intuition from associative memory to analyze computational\nproperties of neural networks with unusual activation functions - the higher\nrectified polynomials which until now have not been used in deep learning. The\nutility of the dense memories is illustrated for two test cases: the logical\ngate XOR and the recognition of handwritten digits from the MNIST data set.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 16:17:01 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 16:05:36 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John J", ""]]}, {"id": "1606.01166", "submitter": "Jean-Charles Vialatte", "authors": "Jean-Charles Vialatte, Vincent Gripon, Gr\\'egoire Mercier", "title": "Generalizing the Convolution Operator to extend CNNs to Irregular\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have become the state-of-the-art in\nsupervised learning vision tasks. Their convolutional filters are of paramount\nimportance for they allow to learn patterns while disregarding their locations\nin input images. When facing highly irregular domains, generalized\nconvolutional operators based on an underlying graph structure have been\nproposed. However, these operators do not exactly match standard ones on grid\ngraphs, and introduce unwanted additional invariance (e.g. with regards to\nrotations). We propose a novel approach to generalize CNNs to irregular domains\nusing weight sharing and graph-based operators. Using experiments, we show that\nthese models resemble CNNs on regular domains and offer better performance than\nmultilayer perceptrons on distorded ones.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 16:18:22 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 09:41:55 GMT"}, {"version": "v3", "created": "Tue, 4 Jul 2017 11:46:46 GMT"}, {"version": "v4", "created": "Wed, 25 Oct 2017 12:28:26 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Vialatte", "Jean-Charles", ""], ["Gripon", "Vincent", ""], ["Mercier", "Gr\u00e9goire", ""]]}, {"id": "1606.01175", "submitter": "Patrick Shafto", "authors": "Baxter S. Eaves Jr., Naomi H. Feldman, Thomas L. Griffiths, Patrick\n  Shafto", "title": "Infant directed speech is consistent with teaching", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infant-directed speech (IDS) has distinctive properties that differ from\nadult-directed speech (ADS). Why it has these properties -- and whether they\nare intended to facilitate language learning -- is matter of contention. We\nargue that much of this disagreement stems from lack of a formal, guiding\ntheory of how phonetic categories should best be taught to infant-like\nlearners. In the absence of such a theory, researchers have relied on\nintuitions about learning to guide the argument. We use a formal theory of\nteaching, validated through experiments in other domains, as the basis for a\ndetailed analysis of whether IDS is well-designed for teaching phonetic\ncategories. Using the theory, we generate ideal data for teaching phonetic\ncategories in English. We qualitatively compare the simulated teaching data\nwith human IDS, finding that the teaching data exhibit many features of IDS,\nincluding some that have been taken as evidence IDS is not for teaching. The\nsimulated data reveal potential pitfalls for experimentalists exploring the\nrole of IDS in language learning. Focusing on different formants and phoneme\nsets leads to different conclusions, and the benefit of the teaching data to\nlearners is not apparent until a sufficient number of examples have been\nprovided. Finally, we investigate transfer of IDS to learning ADS. The teaching\ndata improves classification of ADS data, but only for the learner they were\ngenerated to teach; not universally across all classes of learner. This\nresearch offers a theoretically-grounded framework that empowers\nexperimentalists to systematically evaluate whether IDS is for teaching.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 00:59:41 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Eaves", "Baxter S.", "Jr."], ["Feldman", "Naomi H.", ""], ["Griffiths", "Thomas L.", ""], ["Shafto", "Patrick", ""]]}, {"id": "1606.01190", "submitter": "Panayotis Mertikopoulos", "authors": "Panayotis Mertikopoulos and E. Veronica Belmega and Romain Negrel and\n  Luca Sanguinetti", "title": "Distributed stochastic optimization via matrix exponential learning", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": "10.1109/TSP.2017.2656847", "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a distributed learning scheme for a broad class\nof stochastic optimization problems and games that arise in signal processing\nand wireless communications. The proposed algorithm relies on the method of\nmatrix exponential learning (MXL) and only requires locally computable gradient\nobservations that are possibly imperfect and/or obsolete. To analyze it, we\nintroduce the notion of a stable Nash equilibrium and we show that the\nalgorithm is globally convergent to such equilibria - or locally convergent\nwhen an equilibrium is only locally stable. We also derive an explicit linear\nbound for the algorithm's convergence speed, which remains valid under\nmeasurement errors and uncertainty of arbitrarily high variance. To validate\nour theoretical analysis, we test the algorithm in realistic\nmulti-carrier/multiple-antenna wireless scenarios where several users seek to\nmaximize their energy efficiency. Our results show that learning allows users\nto attain a net increase between 100% and 500% in energy efficiency, even under\nvery high uncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 17:13:31 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Mertikopoulos", "Panayotis", ""], ["Belmega", "E. Veronica", ""], ["Negrel", "Romain", ""], ["Sanguinetti", "Luca", ""]]}, {"id": "1606.01261", "submitter": "Maximilian Balandat", "authors": "Maximilian Balandat, Walid Krichene, Claire Tomlin and Alexandre Bayen", "title": "Minimizing Regret on Reflexive Banach Spaces and Learning Nash\n  Equilibria in Continuous Zero-Sum Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general version of the adversarial online learning problem. We are\ngiven a decision set $\\mathcal{X}$ in a reflexive Banach space $X$ and a\nsequence of reward vectors in the dual space of $X$. At each iteration, we\nchoose an action from $\\mathcal{X}$, based on the observed sequence of previous\nrewards. Our goal is to minimize regret, defined as the gap between the\nrealized reward and the reward of the best fixed action in hindsight. Using\nresults from infinite dimensional convex analysis, we generalize the method of\nDual Averaging (or Follow the Regularized Leader) to our setting and obtain\ngeneral upper bounds on the worst-case regret that subsume a wide range of\nresults from the literature. Under the assumption of uniformly continuous\nrewards, we obtain explicit anytime regret bounds in a setting where the\ndecision set is the set of probability distributions on a compact metric space\n$S$ whose Radon-Nikodym derivatives are elements of $L^p(S)$ for some $p > 1$.\nImportantly, we make no convexity assumptions on either the set $S$ or the\nreward functions. We also prove a general lower bound on the worst-case regret\nfor any online algorithm. We then apply these results to the problem of\nlearning in repeated continuous two-player zero-sum games, in which players'\nstrategy sets are compact metric spaces. In doing so, we first prove that if\nboth players play a Hannan-consistent strategy, then with probability 1 the\nempirical distributions of play weakly converge to the set of Nash equilibria\nof the game. We then show that, under mild assumptions, Dual Averaging on the\n(infinite-dimensional) space of probability distributions indeed achieves\nHannan-consistency. Finally, we illustrate our results through numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 20:07:41 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Balandat", "Maximilian", ""], ["Krichene", "Walid", ""], ["Tomlin", "Claire", ""], ["Bayen", "Alexandre", ""]]}, {"id": "1606.01269", "submitter": "Jason Williams", "authors": "Jason D. Williams and Geoffrey Zweig", "title": "End-to-end LSTM-based dialog control optimized with supervised and\n  reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a model for end-to-end learning of task-oriented dialog\nsystems. The main component of the model is a recurrent neural network (an\nLSTM), which maps from raw dialog history directly to a distribution over\nsystem actions. The LSTM automatically infers a representation of dialog\nhistory, which relieves the system developer of much of the manual feature\nengineering of dialog state. In addition, the developer can provide software\nthat expresses business rules and provides access to programmatic APIs,\nenabling the LSTM to take actions in the real world on behalf of the user. The\nLSTM can be optimized using supervised learning (SL), where a domain expert\nprovides example dialogs which the LSTM should imitate; or using reinforcement\nlearning (RL), where the system improves by interacting directly with end\nusers. Experiments show that SL and RL are complementary: SL alone can derive a\nreasonable initial policy from a small number of training dialogs; and starting\nRL optimization with a policy trained with SL substantially accelerates the\nlearning rate of RL.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 20:32:52 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Williams", "Jason D.", ""], ["Zweig", "Geoffrey", ""]]}, {"id": "1606.01275", "submitter": "Zhiwei Steven Wu", "authors": "Michael Kearns, Zhiwei Steven Wu", "title": "Predicting with Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new learning model in which a joint distribution over vector\npairs $(x,y)$ is determined by an unknown function $c(x)$ that maps input\nvectors $x$ not to individual outputs, but to entire {\\em distributions\\/} over\noutput vectors $y$. Our main results take the form of rather general reductions\nfrom our model to algorithms for PAC learning the function class and the\ndistribution class separately, and show that virtually every such combination\nyields an efficient algorithm in our model. Our methods include a randomized\nreduction to classification noise and an application of Le Cam's method to\nobtain robust learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 20:56:51 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 01:02:06 GMT"}, {"version": "v3", "created": "Fri, 9 Jun 2017 15:06:49 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Kearns", "Michael", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1606.01280", "submitter": "Xingxing Zhang", "authors": "Xingxing Zhang, Jianpeng Cheng and Mirella Lapata", "title": "Dependency Parsing as Head Selection", "comments": "to appear in EACL 2017; Our code is available at\n  http://github.com/XingxingZhang/dense_parser", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional graph-based dependency parsers guarantee a tree structure both\nduring training and inference. Instead, we formalize dependency parsing as the\nproblem of independently selecting the head of each word in a sentence. Our\nmodel which we call \\textsc{DeNSe} (as shorthand for {\\bf De}pendency {\\bf\nN}eural {\\bf Se}lection) produces a distribution over possible heads for each\nword using features obtained from a bidirectional recurrent neural network.\nWithout enforcing structural constraints during training, \\textsc{DeNSe}\ngenerates (at inference time) trees for the overwhelming majority of sentences,\nwhile non-tree outputs can be adjusted with a maximum spanning tree algorithm.\nWe evaluate \\textsc{DeNSe} on four languages (English, Chinese, Czech, and\nGerman) with varying degrees of non-projectivity. Despite the simplicity of the\napproach, our parsers are on par with the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 21:27:03 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 20:25:02 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 22:22:10 GMT"}, {"version": "v4", "created": "Thu, 22 Dec 2016 15:28:34 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Zhang", "Xingxing", ""], ["Cheng", "Jianpeng", ""], ["Lapata", "Mirella", ""]]}, {"id": "1606.01305", "submitter": "David Krueger", "authors": "David Krueger, Tegan Maharaj, J\\'anos Kram\\'ar, Mohammad Pezeshki,\n  Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron\n  Courville, Chris Pal", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "comments": "David Krueger and Tegan Maharaj contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose zoneout, a novel method for regularizing RNNs. At each timestep,\nzoneout stochastically forces some hidden units to maintain their previous\nvalues. Like dropout, zoneout uses random noise to train a pseudo-ensemble,\nimproving generalization. But by preserving instead of dropping hidden units,\ngradient information and state information are more readily propagated through\ntime, as in feedforward stochastic depth networks. We perform an empirical\ninvestigation of various RNN regularizers, and find that zoneout gives\nsignificant performance improvements across tasks. We achieve competitive\nresults with relatively simple models in character- and word-level language\nmodelling on the Penn Treebank and Text8 datasets, and combining with recurrent\nbatch normalization yields state-of-the-art results on permuted sequential\nMNIST.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 23:31:47 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 18:59:48 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 03:12:03 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 20:43:09 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Krueger", "David", ""], ["Maharaj", "Tegan", ""], ["Kram\u00e1r", "J\u00e1nos", ""], ["Pezeshki", "Mohammad", ""], ["Ballas", "Nicolas", ""], ["Ke", "Nan Rosemary", ""], ["Goyal", "Anirudh", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""], ["Pal", "Chris", ""]]}, {"id": "1606.01412", "submitter": "Krzysztof Krawiec", "authors": "Krzysztof Krawiec and Jerry Swan", "title": "Distance Metric Ensemble Learning and the Andrews-Curtis Conjecture", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the search for a counterexample to the Poincar\\'e conjecture in\nthree and four dimensions, the Andrews-Curtis conjecture was proposed in 1965.\nIt is now generally suspected that the Andrews-Curtis conjecture is false, but\nsmall potential counterexamples are not so numerous, and previous work has\nattempted to eliminate some via combinatorial search. Progress has however been\nlimited, with the most successful approach (breadth-first-search using\nsecondary storage) being neither scalable nor heuristically-informed. A\nprevious empirical analysis of problem structure examined several heuristic\nmeasures of search progress and determined that none of them provided any\nuseful guidance for search. In this article, we induce new quality measures\ndirectly from the problem structure and combine them to produce a more\neffective search driver via ensemble machine learning. By this means, we\neliminate 19 potential counterexamples, the status of which had been unknown\nfor some years.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 20:06:03 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Krawiec", "Krzysztof", ""], ["Swan", "Jerry", ""]]}, {"id": "1606.01467", "submitter": "Jie Fu", "authors": "Jie Fu", "title": "Deep Q-Networks for Accelerating the Training of Deep Neural Networks", "comments": "We choose to withdraw this paper. The DQN itself has too many\n  hyperparameters, which makes it almost impossible to be applied to reasonably\n  large datasets. In the later versions (from v4) with SGDR experiments, it\n  seems that the agent only performs random actions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a principled deep reinforcement learning (RL)\napproach that is able to accelerate the convergence rate of general deep neural\nnetworks (DNNs). With our approach, a deep RL agent (synonym for optimizer in\nthis work) is used to automatically learn policies about how to schedule\nlearning rates during the optimization of a DNN. The state features of the\nagent are learned from the weight statistics of the optimizee during training.\nThe reward function of this agent is designed to learn policies that minimize\nthe optimizee's training time given a certain performance goal. The actions of\nthe agent correspond to changing the learning rate for the optimizee during\ntraining. As far as we know, this is the first attempt to use deep RL to learn\nhow to optimize a large-sized DNN. We perform extensive experiments on a\nstandard benchmark dataset and demonstrate the effectiveness of the policies\nlearned by our approach.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 06:42:56 GMT"}, {"version": "v10", "created": "Thu, 13 Jul 2017 08:49:36 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 17:02:49 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 14:17:18 GMT"}, {"version": "v4", "created": "Sun, 16 Oct 2016 04:20:25 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 05:27:02 GMT"}, {"version": "v6", "created": "Fri, 11 Nov 2016 06:22:25 GMT"}, {"version": "v7", "created": "Thu, 17 Nov 2016 06:16:24 GMT"}, {"version": "v8", "created": "Tue, 20 Jun 2017 10:28:34 GMT"}, {"version": "v9", "created": "Wed, 12 Jul 2017 12:29:29 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Fu", "Jie", ""]]}, {"id": "1606.01487", "submitter": "Massimiliano Pontil", "authors": "Andreas Maurer and Massimiliano Pontil", "title": "Bounds for Vector-Valued Function Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to derive risk bounds for vector-valued learning with\na broad class of feature maps and loss functions. Multi-task learning and\none-vs-all multi-category learning are treated as examples. We discuss in\ndetail vector-valued functions with one hidden layer, and demonstrate that the\nconditions under which shared representations are beneficial for multi- task\nlearning are equally applicable to multi-category learning.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 09:59:32 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1606.01530", "submitter": "Fatemeh Navidi", "authors": "Fatemeh Navidi, Prabhanjan Kambadur, Viswanath Nagarajan", "title": "Adaptive Submodular Ranking and Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general stochastic ranking problem where an algorithm needs to\nadaptively select a sequence of elements so as to \"cover\" a random scenario\n(drawn from a known distribution) at minimum expected cost. The coverage of\neach scenario is captured by an individual submodular function, where the\nscenario is said to be covered when its function value goes above a given\nthreshold. We obtain a logarithmic factor approximation algorithm for this\nadaptive ranking problem, which is the best possible (unless P=NP). This\nproblem unifies and generalizes many previously studied problems with\napplications in search ranking and active learning. The approximation ratio of\nour algorithm either matches or improves the best result known in each of these\nspecial cases. Furthermore, we extend our results to an adaptive vehicle\nrouting problem, where costs are determined by an underlying metric. This\nrouting problem is a significant generalization of the previously-studied\nadaptive traveling salesman and traveling repairman problems. Our approximation\nratio nearly matches the best bound known for these special cases. Finally, we\npresent experimental results for some applications of adaptive ranking.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 16:19:58 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 18:53:02 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Navidi", "Fatemeh", ""], ["Kambadur", "Prabhanjan", ""], ["Nagarajan", "Viswanath", ""]]}, {"id": "1606.01540", "submitter": "John Schulman", "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John\n  Schulman, Jie Tang, Wojciech Zaremba", "title": "OpenAI Gym", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenAI Gym is a toolkit for reinforcement learning research. It includes a\ngrowing collection of benchmark problems that expose a common interface, and a\nwebsite where people can share their results and compare the performance of\nalgorithms. This whitepaper discusses the components of OpenAI Gym and the\ndesign decisions that went into the software.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 17:54:48 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Brockman", "Greg", ""], ["Cheung", "Vicki", ""], ["Pettersson", "Ludwig", ""], ["Schneider", "Jonas", ""], ["Schulman", "John", ""], ["Tang", "Jie", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1606.01549", "submitter": "Bhuwan Dhingra", "authors": "Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W. Cohen, Ruslan\n  Salakhutdinov", "title": "Gated-Attention Readers for Text Comprehension", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of answering cloze-style questions over\ndocuments. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop\narchitecture with a novel attention mechanism, which is based on multiplicative\ninteractions between the query embedding and the intermediate states of a\nrecurrent neural network document reader. This enables the reader to build\nquery-specific representations of tokens in the document for accurate answer\nselection. The GA Reader obtains state-of-the-art results on three benchmarks\nfor this task--the CNN \\& Daily Mail news stories and the Who Did What dataset.\nThe effectiveness of multiplicative interaction is demonstrated by an ablation\nstudy, and by comparing to alternative compositional operators for implementing\nthe gated-attention. The code is available at\nhttps://github.com/bdhingra/ga-reader.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 19:30:39 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 19:27:42 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 18:50:05 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Dhingra", "Bhuwan", ""], ["Liu", "Hanxiao", ""], ["Yang", "Zhilin", ""], ["Cohen", "William W.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1606.01568", "submitter": "Jacopo Cavazza", "authors": "Jacopo Cavazza and Vittorio Murino", "title": "Active Regression with Adaptive Huber Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the scalar regression problem through a novel solution\nto exactly optimize the Huber loss in a general semi-supervised setting, which\ncombines multi-view learning and manifold regularization. We propose a\nprincipled algorithm to 1) avoid computationally expensive iterative schemes\nwhile 2) adapting the Huber loss threshold in a data-driven fashion and 3)\nactively balancing the use of labelled data to remove noisy or inconsistent\nannotations at the training stage. In a wide experimental evaluation, dealing\nwith diverse applications, we assess the superiority of our paradigm which is\nable to combine robustness towards noise with both strong performance and low\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 21:59:34 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 07:24:43 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Cavazza", "Jacopo", ""], ["Murino", "Vittorio", ""]]}, {"id": "1606.01583", "submitter": "Augustus Odena", "authors": "Augustus Odena", "title": "Semi-Supervised Learning with Generative Adversarial Networks", "comments": "Appearing in the Data Efficient Machine Learning workshop at ICML\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Generative Adversarial Networks (GANs) to the semi-supervised\ncontext by forcing the discriminator network to output class labels. We train a\ngenerative model G and a discriminator D on a dataset with inputs belonging to\none of N classes. At training time, D is made to predict which of N+1 classes\nthe input belongs to, where an extra class is added to correspond to the\noutputs of G. We show that this method can be used to create a more\ndata-efficient classifier and that it allows for generating higher quality\nsamples than a regular GAN.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 23:42:19 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 01:07:38 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Odena", "Augustus", ""]]}, {"id": "1606.01584", "submitter": "Ricky Laishram", "authors": "Ricky Laishram, Vir Virander Phoha", "title": "Curie: A method for protecting SVM Classifier from Poisoning Attack", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is used in a number of security related applications such as\nbiometric user authentication, speaker identification etc. A type of causative\nintegrity attack against machine learning called Poisoning attack works by\ninjecting specially crafted data points in the training data so as to increase\nthe false positive rate of the classifier. In the context of the biometric\nauthentication, this means that more intruders will be classified as valid\nuser, and in case of speaker identification system, user A will be classified\nuser B. In this paper, we examine poisoning attack against SVM and introduce -\nCurie - a method to protect the SVM classifier from the poisoning attack. The\nbasic idea of our method is to identify the poisoned data points injected by\nthe adversary and filter them out. Our method is light weight and can be easily\nintegrated into existing systems. Experimental results show that it works very\nwell in filtering out the poisoned data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 23:42:56 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 00:41:08 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Laishram", "Ricky", ""], ["Phoha", "Vir Virander", ""]]}, {"id": "1606.01587", "submitter": "Takayuki Muranushi", "authors": "Yuko Hada-Muranushi, Takayuki Muranushi, Ayumi Asai, Daisuke\n  Okanohara, Rudy Raymond, Gentaro Watanabe, Shigeru Nemoto, Kazunari Shibata", "title": "A Deep-Learning Approach for Operation of an Automated Realtime Flare\n  Forecast", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated forecasts serve important role in space weather science, by\nproviding statistical insights to flare-trigger mechanisms, and by enabling\ntailor-made forecasts and high-frequency forecasts. Only by realtime forecast\nwe can experimentally measure the performance of flare-forecasting methods\nwhile confidently avoiding overlearning.\n  We have been operating unmanned flare forecast service since August, 2015\nthat provides 24-hour-ahead forecast of solar flares, every 12 minutes. We\nreport the method and prediction results of the system.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 00:02:20 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Hada-Muranushi", "Yuko", ""], ["Muranushi", "Takayuki", ""], ["Asai", "Ayumi", ""], ["Okanohara", "Daisuke", ""], ["Raymond", "Rudy", ""], ["Watanabe", "Gentaro", ""], ["Nemoto", "Shigeru", ""], ["Shibata", "Kazunari", ""]]}, {"id": "1606.01651", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio, Benjamin Scellier, Olexa Bilaniuk, Joao Sacramento and\n  Walter Senn", "title": "Feedforward Initialization for Fast Inference of Deep Generative\n  Networks is biologically plausible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider deep multi-layered generative models such as Boltzmann machines\nor Hopfield nets in which computation (which implements inference) is both\nrecurrent and stochastic, but where the recurrence is not to model sequential\nstructure, only to perform computation. We find conditions under which a simple\nfeedforward computation is a very good initialization for inference, after the\ninput units are clamped to observed values. It means that after the feedforward\ninitialization, the recurrent network is very close to a fixed point of the\nnetwork dynamics, where the energy gradient is 0. The main condition is that\nconsecutive layers form a good auto-encoder, or more generally that different\ngroups of inputs into the unit (in particular, bottom-up inputs on one hand,\ntop-down inputs on the other hand) are consistent with each other, producing\nthe same contribution into the total weighted sum of inputs. In biological\nterms, this would correspond to having each dendritic branch correctly\npredicting the aggregate input from all the dendritic branches, i.e., the soma\npotential. This is consistent with the prediction that the synaptic weights\ninto dendritic branches such as those of the apical and basal dendrites of\npyramidal cells are trained to minimize the prediction error made by the\ndendritic branch when the target is the somatic activity. Whereas previous work\nhas shown how to achieve fast negative phase inference (when the model is\nunclamped) in a predictive recurrent model, this contribution helps to achieve\nfast positive phase inference (when the target output is clamped) in such\nrecurrent neural models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 08:09:19 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 00:10:22 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Bengio", "Yoshua", ""], ["Scellier", "Benjamin", ""], ["Bilaniuk", "Olexa", ""], ["Sacramento", "Joao", ""], ["Senn", "Walter", ""]]}, {"id": "1606.01735", "submitter": "Hakan Bilen", "authors": "Hakan Bilen and Andrea Vedaldi", "title": "Integrated perception with recurrent multi-task neural networks", "comments": "9 pages, 3 figures, 2 tables", "journal-ref": "Advances in Neural Information Processing (NIPS) 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern discriminative predictors have been shown to match natural\nintelligences in specific perceptual tasks in image classification, object and\npart detection, boundary extraction, etc. However, a major advantage that\nnatural intelligences still have is that they work well for \"all\" perceptual\nproblems together, solving them efficiently and coherently in an \"integrated\nmanner\". In order to capture some of these advantages in machine perception, we\nask two questions: whether deep neural networks can learn universal image\nrepresentations, useful not only for a single task but for all of them, and how\nthe solutions to the different tasks can be integrated in this framework. We\nanswer by proposing a new architecture, which we call \"MultiNet\", in which not\nonly deep image features are shared between tasks, but where tasks can interact\nin a recurrent manner by encoding the results of their analysis in a common\nshared representation of the data. In this manner, we show that the performance\nof individual tasks in standard benchmarks can be improved first by sharing\nfeatures between them and then, more significantly, by integrating their\nsolutions in the common representation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 13:27:25 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 14:38:00 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1606.01781", "submitter": "Alexis Conneau", "authors": "Alexis Conneau, Holger Schwenk, Lo\\\"ic Barrault, Yann Lecun", "title": "Very Deep Convolutional Networks for Text Classification", "comments": "10 pages, EACL 2017, camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant approach for many NLP tasks are recurrent neural networks, in\nparticular LSTMs, and convolutional neural networks. However, these\narchitectures are rather shallow in comparison to the deep convolutional\nnetworks which have pushed the state-of-the-art in computer vision. We present\na new architecture (VDCNN) for text processing which operates directly at the\ncharacter level and uses only small convolutions and pooling operations. We are\nable to show that the performance of this model increases with depth: using up\nto 29 convolutional layers, we report improvements over the state-of-the-art on\nseveral public text classification tasks. To the best of our knowledge, this is\nthe first time that very deep convolutional nets have been applied to text\nprocessing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 15:14:50 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 12:49:11 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Conneau", "Alexis", ""], ["Schwenk", "Holger", ""], ["Barrault", "Lo\u00efc", ""], ["Lecun", "Yann", ""]]}, {"id": "1606.01793", "submitter": "Christian Grussler", "authors": "Christian Grussler, Anders Rantzer and Pontus Giselsson", "title": "Low-rank Optimization with Convex Constraints", "comments": "Accepted for publication in IEEE Transactions on Automatic Control", "journal-ref": "IEEE Trans. Automat. Control, 63(11), 4000-4007, 2018", "doi": "10.1109/TAC.2018.2813009", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of low-rank approximation with convex constraints, which appears\nin data analysis, system identification, model order reduction, low-order\ncontroller design and low-complexity modelling is considered. Given a matrix,\nthe objective is to find a low-rank approximation that meets rank and convex\nconstraints, while minimizing the distance to the matrix in the squared\nFrobenius norm. In many situations, this non-convex problem is convexified by\nnuclear norm regularization. However, we will see that the approximations\nobtained by this method may be far from optimal. In this paper, we propose an\nalternative convex relaxation that uses the convex envelope of the squared\nFrobenius norm and the rank constraint. With this approach, easily verifiable\nconditions are obtained under which the solutions to the convex relaxation and\nthe original non-convex problem coincide. An SDP representation of the convex\nenvelope is derived, which allows us to apply this approach to several known\nproblems. Our example on optimal low-rank Hankel approximation/model reduction\nillustrates that the proposed convex relaxation performs consistently better\nthan nuclear norm regularization and may outperform balanced truncation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 15:46:08 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 15:14:26 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 12:47:51 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Grussler", "Christian", ""], ["Rantzer", "Anders", ""], ["Giselsson", "Pontus", ""]]}, {"id": "1606.01855", "submitter": "Aaron Schein", "authors": "Aaron Schein, Mingyuan Zhou, David M. Blei, Hanna Wallach", "title": "Bayesian Poisson Tucker Decomposition for Learning the Structure of\n  International Relations", "comments": "To appear in Proceedings of the 33rd International Conference on\n  Machine Learning (ICML 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling\ncountry--country interaction event data. These data consist of interaction\nevents of the form \"country $i$ took action $a$ toward country $j$ at time\n$t$.\" BPTD discovers overlapping country--community memberships, including the\nnumber of latent communities. In addition, it discovers directed\ncommunity--community interaction networks that are specific to \"topics\" of\naction types and temporal \"regimes.\" We show that BPTD yields an efficient MCMC\ninference algorithm and achieves better predictive performance than related\nmodels. We also demonstrate that it discovers interpretable latent structure\nthat agrees with our knowledge of international relations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 18:34:56 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Schein", "Aaron", ""], ["Zhou", "Mingyuan", ""], ["Blei", "David M.", ""], ["Wallach", "Hanna", ""]]}, {"id": "1606.01865", "submitter": "Zhengping Che", "authors": "Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, Yan\n  Liu", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing\n  Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series data in practical applications, such as health care,\ngeoscience, and biology, are characterized by a variety of missing values. In\ntime series prediction and other related tasks, it has been noted that missing\nvalues and their missing patterns are often correlated with the target labels,\na.k.a., informative missingness. There is very limited work on exploiting the\nmissing patterns for effective imputation and improving prediction performance.\nIn this paper, we develop novel deep learning models, namely GRU-D, as one of\nthe early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a\nstate-of-the-art recurrent neural network. It takes two representations of\nmissing patterns, i.e., masking and time interval, and effectively incorporates\nthem into a deep model architecture so that it not only captures the long-term\ntemporal dependencies in time series, but also utilizes the missing patterns to\nachieve better prediction results. Experiments of time series classification\ntasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic\ndatasets demonstrate that our models achieve state-of-the-art performance and\nprovides useful insights for better understanding and utilization of missing\nvalues in time series analysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 19:08:41 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 20:51:29 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Che", "Zhengping", ""], ["Purushotham", "Sanjay", ""], ["Cho", "Kyunghyun", ""], ["Sontag", "David", ""], ["Liu", "Yan", ""]]}, {"id": "1606.01868", "submitter": "Marc G. Bellemare", "authors": "Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul,\n  David Saxton, Remi Munos", "title": "Unifying Count-Based Exploration and Intrinsic Motivation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an agent's uncertainty about its environment and the problem of\ngeneralizing this uncertainty across observations. Specifically, we focus on\nthe problem of exploration in non-tabular reinforcement learning. Drawing\ninspiration from the intrinsic motivation literature, we use density models to\nmeasure uncertainty, and propose a novel algorithm for deriving a pseudo-count\nfrom an arbitrary density model. This technique enables us to generalize\ncount-based exploration algorithms to the non-tabular case. We apply our ideas\nto Atari 2600 games, providing sensible pseudo-counts from raw pixels. We\ntransform these pseudo-counts into intrinsic rewards and obtain significantly\nimproved exploration in a number of hard games, including the infamously\ndifficult Montezuma's Revenge.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 19:21:32 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 21:16:21 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Bellemare", "Marc G.", ""], ["Srinivasan", "Sriram", ""], ["Ostrovski", "Georg", ""], ["Schaul", "Tom", ""], ["Saxton", "David", ""], ["Munos", "Remi", ""]]}, {"id": "1606.01885", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Learning to Optimize", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm design is a laborious process and often requires many iterations of\nideation and validation. In this paper, we explore automating algorithm design\nand present a method to learn an optimization algorithm, which we believe to be\nthe first method that can automatically discover a better algorithm. We\napproach this problem from a reinforcement learning perspective and represent\nany particular optimization algorithm as a policy. We learn an optimization\nalgorithm using guided policy search and demonstrate that the resulting\nalgorithm outperforms existing hand-engineered algorithms in terms of\nconvergence speed and/or the final objective value.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 19:50:47 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1606.01981", "submitter": "Paul Merolla", "authors": "Paul Merolla, Rathinakumar Appuswamy, John Arthur, Steve K. Esser,\n  Dharmendra Modha", "title": "Deep neural networks are robust to weight binarization and other\n  non-linear distortions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results show that deep neural networks achieve excellent performance\neven when, during training, weights are quantized and projected to a binary\nrepresentation. Here, we show that this is just the tip of the iceberg: these\nsame networks, during testing, also exhibit a remarkable robustness to\ndistortions beyond quantization, including additive and multiplicative noise,\nand a class of non-linear projections where binarization is just a special\ncase. To quantify this robustness, we show that one such network achieves 11%\ntest error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore,\nwe find that a common training heuristic--namely, projecting quantized weights\nduring backpropagation--can be altered (or even removed) and networks still\nachieve a base level of robustness during testing. Specifically, training with\nweight projections other than quantization also works, as does simply clipping\nthe weights, both of which have never been reported before. We confirm our\nresults for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas,\nwe propose a stochastic projection rule that leads to a new state of the art\nnetwork with 7.64% test error on CIFAR-10 using no data augmentation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 00:28:42 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Merolla", "Paul", ""], ["Appuswamy", "Rathinakumar", ""], ["Arthur", "John", ""], ["Esser", "Steve K.", ""], ["Modha", "Dharmendra", ""]]}, {"id": "1606.02077", "submitter": "Nagarajan Natarajan", "authors": "Prateek Jain and Nagarajan Natarajan", "title": "Regret Bounds for Non-decomposable Metrics with Missing Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recommending relevant labels (items) for a given\ndata point (user). In particular, we are interested in the practically\nimportant setting where the evaluation is with respect to non-decomposable\n(over labels) performance metrics like the $F_1$ measure, and the training data\nhas missing labels. To this end, we propose a generic framework that given a\nperformance metric $\\Psi$, can devise a regularized objective function and a\nthreshold such that all the values in the predicted score vector above and only\nabove the threshold are selected to be positive. We show that the regret or\ngeneralization error in the given metric $\\Psi$ is bounded ultimately by\nestimation error of certain underlying parameters. In particular, we derive\nregret bounds under three popular settings: a) collaborative filtering, b)\nmultilabel classification, and c) PU (positive-unlabeled) learning. For each of\nthe above problems, we can obtain precise non-asymptotic regret bound which is\nsmall even when a large fraction of labels is missing. Our empirical results on\nsynthetic and benchmark datasets demonstrate that by explicitly modeling for\nmissing labels and optimizing the desired performance metric, our algorithm\nindeed achieves significantly better performance (like $F_1$ score) when\ncompared to methods that do not model missing label information carefully.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 10:00:30 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Jain", "Prateek", ""], ["Natarajan", "Nagarajan", ""]]}, {"id": "1606.02109", "submitter": "Onur Dikmen", "authors": "Antti Honkela, Mrinal Das, Arttu Nieminen, Onur Dikmen and Samuel\n  Kaski", "title": "Efficient differentially private learning improves drug sensitivity\n  prediction", "comments": "14 pages + 13 pages supplementary information, 3 + 3 figures", "journal-ref": "Biology Direct (2018) 13:1", "doi": "10.1186/s13062-017-0203-4", "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of a personalised recommendation system face a dilemma: recommendations\ncan be improved by learning from data, but only if the other users are willing\nto share their private information. Good personalised predictions are vitally\nimportant in precision medicine, but genomic information on which the\npredictions are based is also particularly sensitive, as it directly identifies\nthe patients and hence cannot easily be anonymised. Differential privacy has\nemerged as a potentially promising solution: privacy is considered sufficient\nif presence of individual patients cannot be distinguished. However,\ndifferentially private learning with current methods does not improve\npredictions with feasible data sizes and dimensionalities. Here we show that\nuseful predictors can be learned under powerful differential privacy\nguarantees, and even from moderately-sized data sets, by demonstrating\nsignificant improvements with a new robust private regression method in the\naccuracy of private drug sensitivity prediction. The method combines two key\nproperties not present even in recent proposals, which can be generalised to\nother predictors: we prove it is asymptotically consistently and efficiently\nprivate, and demonstrate that it performs well on finite data. Good finite data\nperformance is achieved by limiting the sharing of private information by\ndecreasing the dimensionality and by projecting outliers to fit tighter bounds,\ntherefore needing to add less noise for equal privacy. As already the\nsimple-to-implement method shows promise on the challenging genomic data, we\nanticipate rapid progress towards practical applications in many fields, such\nas mobile sensing and social media, in addition to the badly needed precision\nmedicine solutions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 11:52:28 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 11:38:00 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Honkela", "Antti", ""], ["Das", "Mrinal", ""], ["Nieminen", "Arttu", ""], ["Dikmen", "Onur", ""], ["Kaski", "Samuel", ""]]}, {"id": "1606.02185", "submitter": "Harrison Edwards", "authors": "Harrison Edwards, Amos Storkey", "title": "Towards a Neural Statistician", "comments": "Updated to camera ready version for ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient learner is one who reuses what they already know to tackle a new\nproblem. For a machine learner, this means understanding the similarities\namongst datasets. In order to do this, one must take seriously the idea of\nworking with datasets, rather than datapoints, as the key objects to model.\nTowards this goal, we demonstrate an extension of a variational autoencoder\nthat can learn a method for computing representations, or statistics, of\ndatasets in an unsupervised fashion. The network is trained to produce\nstatistics that encapsulate a generative model for each dataset. Hence the\nnetwork enables efficient learning from new datasets for both unsupervised and\nsupervised tasks. We show that we are able to learn statistics that can be used\nfor: clustering datasets, transferring generative models to new datasets,\nselecting representative samples of datasets and classifying previously unseen\nclasses. We refer to our model as a neural statistician, and by this we mean a\nneural network that can learn to compute summary statistics of datasets without\nsupervision.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 15:36:39 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 17:18:16 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Edwards", "Harrison", ""], ["Storkey", "Amos", ""]]}, {"id": "1606.02193", "submitter": "Gabriel Martins Dias", "authors": "Gabriel Martins Dias, Maddalena Nurchis and Boris Bellalta", "title": "Adapting Sampling Interval of Sensor Networks Using On-Line\n  Reinforcement Learning", "comments": "6 pages, 2 figures, submitted to the IEEE World Forum on Internet of\n  Things 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring Wireless Sensor Networks (WSNs) are composed of sensor nodes that\nreport temperature, relative humidity, and other environmental parameters. The\ntime between two successive measurements is a critical parameter to set during\nthe WSN configuration because it can impact the WSN's lifetime, the wireless\nmedium contention and the quality of the reported data. As trends in monitored\nparameters can significantly vary between scenarios and within time,\nidentifying a sampling interval suitable for several cases is also challenging.\nIn this work, we propose a dynamic sampling rate adaptation scheme based on\nreinforcement learning, able to tune sensors' sampling interval on-the-fly,\naccording to environmental conditions and application requirements. The primary\ngoal is to set the sampling interval to the best value possible so as to avoid\noversampling and save energy, while not missing environmental changes that can\nbe relevant for the application. In simulations, our mechanism could reduce up\nto 73% the total number of transmissions compared to a fixed strategy and,\nsimultaneously, keep the average quality of information provided by the WSN.\nThe inherent flexibility of the reinforcement learning algorithm facilitates\nits use in several scenarios, so as to exploit the broad scope of the Internet\nof Things.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 16:01:16 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 16:11:58 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Dias", "Gabriel Martins", ""], ["Nurchis", "Maddalena", ""], ["Bellalta", "Boris", ""]]}, {"id": "1606.02206", "submitter": "Farzan Farnia", "authors": "Farzan Farnia, David Tse", "title": "A Minimax Approach to Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a task of predicting $Y$ from $X$, a loss function $L$, and a set of\nprobability distributions $\\Gamma$ on $(X,Y)$, what is the optimal decision\nrule minimizing the worst-case expected loss over $\\Gamma$? In this paper, we\naddress this question by introducing a generalization of the principle of\nmaximum entropy. Applying this principle to sets of distributions with marginal\non $X$ constrained to be the empirical marginal from the data, we develop a\ngeneral minimax approach for supervised learning problems. While for some loss\nfunctions such as squared-error and log loss, the minimax approach rederives\nwell-knwon regression models, for the 0-1 loss it results in a new linear\nclassifier which we call the maximum entropy machine. The maximum entropy\nmachine minimizes the worst-case 0-1 loss over the structured set of\ndistribution, and by our numerical experiments can outperform other well-known\nlinear classifiers such as SVM. We also prove a bound on the generalization\nworst-case error in the minimax approach.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 16:39:09 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 01:20:30 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 23:03:43 GMT"}, {"version": "v4", "created": "Sun, 6 Nov 2016 23:19:58 GMT"}, {"version": "v5", "created": "Tue, 4 Jul 2017 01:56:04 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Farnia", "Farzan", ""], ["Tse", "David", ""]]}, {"id": "1606.02228", "submitter": "Dmytro Mishkin", "authors": "Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas", "title": "Systematic evaluation of CNN advances on the ImageNet", "comments": "Submitted to CVIU Special Issue on Deep Learning. Updated dataset\n  quality experiment", "journal-ref": null, "doi": "10.1016/j.cviu.2017.05.007", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper systematically studies the impact of a range of recent advances in\nCNN architectures and learning methods on the object categorization (ILSVRC)\nproblem. The evalution tests the influence of the following choices of the\narchitecture: non-linearity (ReLU, ELU, maxout, compatibility with batch\nnormalization), pooling variants (stochastic, max, average, mixed), network\nwidth, classifier design (convolutional, fully-connected, SPP), image\npre-processing, and of learning parameters: learning rate, batch size,\ncleanliness of the data, etc.\n  The performance gains of the proposed modifications are first tested\nindividually and then in combination. The sum of individual gains is bigger\nthan the observed improvement when all modifications are introduced, but the\n\"deficit\" is small suggesting independence of their benefits. We show that the\nuse of 128x128 pixel images is sufficient to make qualitative conclusions about\noptimal network structure that hold for the full size Caffe and VGG nets. The\nresults are obtained an order of magnitude faster than with the standard 224\npixel images.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 17:38:06 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 13:48:39 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Mishkin", "Dmytro", ""], ["Sergievskiy", "Nikolay", ""], ["Matas", "Jiri", ""]]}, {"id": "1606.02275", "submitter": "Roger Grosse", "authors": "Roger B. Grosse and Siddharth Ancha and Daniel M. Roy", "title": "Measuring the reliability of MCMC inference with bidirectional Monte\n  Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is one of the main workhorses of\nprobabilistic inference, but it is notoriously hard to measure the quality of\napproximate posterior samples. This challenge is particularly salient in black\nbox inference methods, which can hide details and obscure inference failures.\nIn this work, we extend the recently introduced bidirectional Monte Carlo\ntechnique to evaluate MCMC-based posterior inference algorithms. By running\nannealed importance sampling (AIS) chains both from prior to posterior and vice\nversa on simulated data, we upper bound in expectation the symmetrized KL\ndivergence between the true posterior distribution and the distribution of\napproximate samples. We present Bounding Divergences with REverse Annealing\n(BREAD), a protocol for validating the relevance of simulated data experiments\nto real datasets, and integrate it into two probabilistic programming\nlanguages: WebPPL and Stan. As an example of how BREAD can be used to guide the\ndesign of inference algorithms, we apply it to study the effectiveness of\ndifferent model representations in both WebPPL and Stan.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:39:02 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Grosse", "Roger B.", ""], ["Ancha", "Siddharth", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1606.02279", "submitter": "Jingbin Wang", "authors": "Ru-Ze Liang, Wei Xie, Weizhi Li, Xin Du, Jim Jing-Yan Wang, Jingbin\n  Wang", "title": "Semi-supervised structured output prediction by local linear regression\n  and sub-gradient descent", "comments": "arXiv admin note: substantial text overlap with arXiv:1604.03010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel semi-supervised structured output prediction method based\non local linear regression in this paper. The existing semi-supervise\nstructured output prediction methods learn a global predictor for all the data\npoints in a data set, which ignores the differences of local distributions of\nthe data set, and the effects to the structured output prediction. To solve\nthis problem, we propose to learn the missing structured outputs and local\npredictors for neighborhoods of different data points jointly. Using the local\nlinear regression strategy, in the neighborhood of each data point, we propose\nto learn a local linear predictor by minimizing both the complexity of the\npredictor and the upper bound of the structured prediction loss. The\nminimization problem is solved by sub-gradient descent algorithms. We conduct\nexperiments over two benchmark data sets, and the results show the advantages\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:52:02 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 14:34:24 GMT"}, {"version": "v3", "created": "Tue, 16 Aug 2016 13:10:04 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Liang", "Ru-Ze", ""], ["Xie", "Wei", ""], ["Li", "Weizhi", ""], ["Du", "Xin", ""], ["Wang", "Jim Jing-Yan", ""], ["Wang", "Jingbin", ""]]}, {"id": "1606.02346", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Tomasz Kajdanowicz, Kristian Kersting", "title": "How is a data-driven approach better than random choice in label space\n  division for multi-label classification?", "comments": null, "journal-ref": null, "doi": "10.3390/e18080282", "report-no": null, "categories": "cs.LG cs.PF cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose using five data-driven community detection approaches from social\nnetworks to partition the label space for the task of multi-label\nclassification as an alternative to random partitioning into equal subsets as\nperformed by RAkELd: modularity-maximizing fastgreedy and leading eigenvector,\ninfomap, walktrap and label propagation algorithms. We construct a label\nco-occurence graph (both weighted an unweighted versions) based on training\ndata and perform community detection to partition the label set. We include\nBinary Relevance and Label Powerset classification methods for comparison. We\nuse gini-index based Decision Trees as the base classifier. We compare educated\napproaches to label space divisions against random baselines on 12 benchmark\ndata sets over five evaluation measures. We show that in almost all cases seven\neducated guess approaches are more likely to outperform RAkELd than otherwise\nin all measures, but Hamming Loss. We show that fastgreedy and walktrap\ncommunity detection methods on weighted label co-occurence graphs are 85-92%\nmore likely to yield better F1 scores than random partitioning. Infomap on the\nunweighted label co-occurence graphs is on average 90% of the times better than\nrandom paritioning in terms of Subset Accuracy and 89% when it comes to Jaccard\nsimilarity. Weighted fastgreedy is better on average than RAkELd when it comes\nto Hamming Loss.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 22:17:30 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Kajdanowicz", "Tomasz", ""], ["Kersting", "Kristian", ""]]}, {"id": "1606.02355", "submitter": "Tommaso Furlanello", "authors": "Tommaso Furlanello, Jiaping Zhao, Andrew M. Saxe, Laurent Itti, Bosco\n  S. Tjan", "title": "Active Long Term Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual Learning in artificial neural networks suffers from interference\nand forgetting when different tasks are learned sequentially. This paper\nintroduces the Active Long Term Memory Networks (A-LTM), a model of sequential\nmulti-task deep learning that is able to maintain previously learned\nassociation between sensory input and behavioral output while acquiring knew\nknowledge. A-LTM exploits the non-convex nature of deep neural networks and\nactively maintains knowledge of previously learned, inactive tasks using a\ndistillation loss. Distortions of the learned input-output map are penalized\nbut hidden layers are free to transverse towards new local optima that are more\nfavorable for the multi-task objective. We re-frame the McClelland's seminal\nHippocampal theory with respect to Catastrophic Inference (CI) behavior\nexhibited by modern deep architectures trained with back-propagation and\ninhomogeneous sampling of latent factors across epochs. We present empirical\nresults of non-trivial CI during continual learning in Deep Linear Networks\ntrained on the same task, in Convolutional Neural Networks when the task shifts\nfrom predicting semantic to graphical factors and during domain adaptation from\nsimple to complex environments. We present results of the A-LTM model's ability\nto maintain viewpoint recognition learned in the highly controlled iLab-20M\ndataset with 10 object categories and 88 camera viewpoints, while adapting to\nthe unstructured domain of Imagenet with 1,000 object categories.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 23:43:42 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Furlanello", "Tommaso", ""], ["Zhao", "Jiaping", ""], ["Saxe", "Andrew M.", ""], ["Itti", "Laurent", ""], ["Tjan", "Bosco S.", ""]]}, {"id": "1606.02378", "submitter": "Arunkumar Byravan", "authors": "Arunkumar Byravan and Dieter Fox", "title": "SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks", "comments": "8 pages. To appear at the IEEE International Conference on Robotics\n  and Automation (ICRA), 2017. V2 Update: Final version submitted to ICRA with\n  experiments testing the robustness of the system to noise and preliminary\n  results on real world data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SE3-Nets, which are deep neural networks designed to model and\nlearn rigid body motion from raw point cloud data. Based only on sequences of\ndepth images along with action vectors and point wise data associations,\nSE3-Nets learn to segment effected object parts and predict their motion\nresulting from the applied force. Rather than learning point wise flow vectors,\nSE3-Nets predict SE3 transformations for different parts of the scene. Using\nsimulated depth data of a table top scene and a robot manipulator, we show that\nthe structure underlying SE3-Nets enables them to generate a far more\nconsistent prediction of object motion than traditional flow based networks.\nAdditional experiments with a depth camera observing a Baxter robot pushing\nobjects on a table show that SE3-Nets also work well on real data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 02:36:11 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 17:32:22 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 22:41:40 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Byravan", "Arunkumar", ""], ["Fox", "Dieter", ""]]}, {"id": "1606.02396", "submitter": "Ardavan Saeedi", "authors": "Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, Samuel J. Gershman", "title": "Deep Successor Reinforcement Learning", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning robust value functions given raw observations and rewards is now\npossible with model-free and model-based deep reinforcement learning\nalgorithms. There is a third alternative, called Successor Representations\n(SR), which decomposes the value function into two components -- a reward\npredictor and a successor map. The successor map represents the expected future\nstate occupancy from any given state and the reward predictor maps states to\nscalar rewards. The value function of a state can be computed as the inner\nproduct between the successor map and the reward weights. In this paper, we\npresent DSR, which generalizes SR within an end-to-end deep reinforcement\nlearning framework. DSR has several appealing properties including: increased\nsensitivity to distal reward changes due to factorization of reward and world\ndynamics, and the ability to extract bottleneck states (subgoals) given\nsuccessor maps trained under a random policy. We show the efficacy of our\napproach on two diverse environments given raw pixel observations -- simple\ngrid-world domains (MazeBase) and the Doom game engine.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 04:48:49 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Saeedi", "Ardavan", ""], ["Gautam", "Simanta", ""], ["Gershman", "Samuel J.", ""]]}, {"id": "1606.02404", "submitter": "Hassan Ashtiani", "authors": "Hassan Ashtiani, Shrinu Kushagra and Shai Ben-David", "title": "Clustering with Same-Cluster Queries", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for Semi-Supervised Active Clustering framework\n(SSAC), where the learner is allowed to interact with a domain expert, asking\nwhether two given instances belong to the same cluster or not. We study the\nquery and computational complexity of clustering in this framework. We consider\na setting where the expert conforms to a center-based clustering with a notion\nof margin. We show that there is a trade off between computational complexity\nand query complexity; We prove that for the case of $k$-means clustering (i.e.,\nwhen the expert conforms to a solution of $k$-means), having access to\nrelatively few such queries allows efficient solutions to otherwise NP hard\nproblems.\n  In particular, we provide a probabilistic polynomial-time (BPP) algorithm for\nclustering in this setting that asks $O\\big(k^2\\log k + k\\log n)$ same-cluster\nqueries and runs with time complexity $O\\big(kn\\log n)$ (where $k$ is the\nnumber of clusters and $n$ is the number of instances). The algorithm succeeds\nwith high probability for data satisfying margin conditions under which,\nwithout queries, we show that the problem is NP hard. We also prove a lower\nbound on the number of queries needed to have a computationally efficient\nclustering algorithm in this setting.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 05:28:14 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 18:16:44 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Ashtiani", "Hassan", ""], ["Kushagra", "Shrinu", ""], ["Ben-David", "Shai", ""]]}, {"id": "1606.02407", "submitter": "Rathinakumar Appuswamy", "authors": "Rathinakumar Appuswamy, Tapan Nayak, John Arthur, Steven Esser, Paul\n  Merolla, Jeffrey Mckinstry, Timothy Melano, Myron Flickner, Dharmendra Modha", "title": "Structured Convolution Matrices for Energy-efficient Deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a relationship between network representation in energy-efficient\nneuromorphic architectures and block Toplitz convolutional matrices. Inspired\nby this connection, we develop deep convolutional networks using a family of\nstructured convolutional matrices and achieve state-of-the-art trade-off\nbetween energy efficiency and classification accuracy for well-known image\nrecognition tasks. We also put forward a novel method to train binary\nconvolutional networks by utilising an existing connection between\nnoisy-rectified linear units and binary activations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 05:31:43 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Appuswamy", "Rathinakumar", ""], ["Nayak", "Tapan", ""], ["Arthur", "John", ""], ["Esser", "Steven", ""], ["Merolla", "Paul", ""], ["Mckinstry", "Jeffrey", ""], ["Melano", "Timothy", ""], ["Flickner", "Myron", ""], ["Modha", "Dharmendra", ""]]}, {"id": "1606.02421", "submitter": "Igor Colin", "authors": "Igor Colin, Aur\\'elien Bellet, Joseph Salmon, St\\'ephan\n  Cl\\'emen\\c{c}on", "title": "Gossip Dual Averaging for Decentralized Optimization of Pairwise\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In decentralized networks (of sensors, connected objects, etc.), there is an\nimportant need for efficient algorithms to optimize a global cost function, for\ninstance to learn a global model from the local data collected by each\ncomputing unit. In this paper, we address the problem of decentralized\nminimization of pairwise functions of the data points, where these points are\ndistributed over the nodes of a graph defining the communication topology of\nthe network. This general problem finds applications in ranking, distance\nmetric learning and graph inference, among others. We propose new gossip\nalgorithms based on dual averaging which aims at solving such problems both in\nsynchronous and asynchronous settings. The proposed framework is flexible\nenough to deal with constrained and regularized variants of the optimization\nproblem. Our theoretical analysis reveals that the proposed algorithms preserve\nthe convergence rate of centralized dual averaging up to an additive bias term.\nWe present numerical simulations on Area Under the ROC Curve (AUC) maximization\nand metric learning problems which illustrate the practical interest of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 07:01:47 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Colin", "Igor", ""], ["Bellet", "Aur\u00e9lien", ""], ["Salmon", "Joseph", ""], ["Cl\u00e9men\u00e7on", "St\u00e9phan", ""]]}, {"id": "1606.02448", "submitter": "Claire Vernade", "authors": "Paul Lagr\\'ee (UP11, LRI), Claire Vernade (LTCI), Olivier Capp\\'e\n  (LTCI)", "title": "Multiple-Play Bandits in the Position-Based Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequentially learning to place items in multi-position displays or lists is a\ntask that can be cast into the multiple-play semi-bandit setting. However, a\nmajor concern in this context is when the system cannot decide whether the user\nfeedback for each item is actually exploitable. Indeed, much of the content may\nhave been simply ignored by the user. The present work proposes to exploit\navailable information regarding the display position bias under the so-called\nPosition-based click model (PBM). We first discuss how this model differs from\nthe Cascade model and its variants considered in several recent works on\nmultiple-play bandits. We then provide a novel regret lower bound for this\nmodel as well as computationally efficient algorithms that display good\nempirical and theoretical performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 08:31:46 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Lagr\u00e9e", "Paul", "", "UP11, LRI"], ["Vernade", "Claire", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"]]}, {"id": "1606.02492", "submitter": "Shreyas Saxena", "authors": "Shreyas Saxena and Jakob Verbeek", "title": "Convolutional Neural Fabrics", "comments": "Corrected typos (In proceedings of NIPS16 )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of CNNs, selecting the optimal architecture for a given\ntask remains an open problem. Instead of aiming to select a single optimal\narchitecture, we propose a \"fabric\" that embeds an exponentially large number\nof architectures. The fabric consists of a 3D trellis that connects response\nmaps at different layers, scales, and channels with a sparse homogeneous local\nconnectivity pattern. The only hyper-parameters of a fabric are the number of\nchannels and layers. While individual architectures can be recovered as paths,\nthe fabric can in addition ensemble all embedded architectures together,\nsharing their weights where their paths overlap. Parameters can be learned\nusing standard methods based on back-propagation, at a cost that scales\nlinearly in the fabric size. We present benchmark results competitive with the\nstate of the art for image classification on MNIST and CIFAR10, and for\nsemantic segmentation on the Part Labels dataset.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 10:17:51 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 16:21:57 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 13:10:05 GMT"}, {"version": "v4", "created": "Mon, 30 Jan 2017 12:28:29 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Saxena", "Shreyas", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1606.02555", "submitter": "Marco Dinarelli", "authors": "Marco Dinarelli and Isabelle Tellier", "title": "Improving Recurrent Neural Networks For Sequence Labelling", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study different types of Recurrent Neural Networks (RNN) for\nsequence labeling tasks. We propose two new variants of RNNs integrating\nimprovements for sequence labeling, and we compare them to the more traditional\nElman and Jordan RNNs. We compare all models, either traditional or new, on\nfour distinct tasks of sequence labeling: two on Spoken Language Understanding\n(ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the\nPenn Treebank (PTB) corpora. The results show that our new variants of RNNs are\nalways more effective than the others.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 13:47:18 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Dinarelli", "Marco", ""], ["Tellier", "Isabelle", ""]]}, {"id": "1606.02560", "submitter": "Tiancheng Zhao", "authors": "Tiancheng Zhao and Maxine Eskenazi", "title": "Towards End-to-End Learning for Dialog State Tracking and Management\n  using Deep Reinforcement Learning", "comments": "In proceeding of SIGDIAL 2016. Added changes based-on peer review,\n  including: 1. Added references, 2. fixed typos in text and figures, 3. added\n  minor change to introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an end-to-end framework for task-oriented dialog systems\nusing a variant of Deep Recurrent Q-Networks (DRQN). The model is able to\ninterface with a relational database and jointly learn policies for both\nlanguage understanding and dialog strategy. Moreover, we propose a hybrid\nalgorithm that combines the strength of reinforcement learning and supervised\nlearning to achieve faster learning speed. We evaluated the proposed model on a\n20 Question Game conversational game simulator. Results show that the proposed\nmethod outperforms the modular-based baseline and learns a distributed\nrepresentation of the latent dialog state.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:03:25 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 21:50:30 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Zhao", "Tiancheng", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1606.02580", "submitter": "Chrisantha Fernando Dr", "authors": "Chrisantha Fernando, Dylan Banarse, Malcolm Reynolds, Frederic Besse,\n  David Pfau, Max Jaderberg, Marc Lanctot, Daan Wierstra", "title": "Convolution by Evolution: Differentiable Pattern Producing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a differentiable version of the Compositional\nPattern Producing Network, called the DPPN. Unlike a standard CPPN, the\ntopology of a DPPN is evolved but the weights are learned. A Lamarckian\nalgorithm, that combines evolution and learning, produces DPPNs to reconstruct\nan image. Our main result is that DPPNs can be evolved/trained to compress the\nweights of a denoising autoencoder from 157684 to roughly 200 parameters, while\nachieving a reconstruction accuracy comparable to a fully connected network\nwith more than two orders of magnitude more parameters. The regularization\nability of the DPPN allows it to rediscover (approximate) convolutional network\narchitectures embedded within a fully connected architecture. Such\nconvolutional architectures are the current state of the art for many computer\nvision applications, so it is satisfying that DPPNs are capable of discovering\nthis structure rather than having to build it in by design. DPPNs exhibit\nbetter generalization when tested on the Omniglot dataset after being trained\non MNIST, than directly encoded fully connected autoencoders. DPPNs are\ntherefore a new framework for integrating learning and evolution.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:37:39 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Fernando", "Chrisantha", ""], ["Banarse", "Dylan", ""], ["Reynolds", "Malcolm", ""], ["Besse", "Frederic", ""], ["Pfau", "David", ""], ["Jaderberg", "Max", ""], ["Lanctot", "Marc", ""], ["Wierstra", "Daan", ""]]}, {"id": "1606.02608", "submitter": "David Martins de Matos", "authors": "Jaime Ferreira and David Martins de Matos and Ricardo Ribeiro", "title": "Fast and Extensible Online Multivariate Kernel Density Estimation", "comments": "17 pages, 1 figure, 7 tables, submission to Pattern Recognition\n  Letters, review version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present xokde++, a state-of-the-art online kernel density estimation\napproach that maintains Gaussian mixture models input data streams. The\napproach follows state-of-the-art work on online density estimation, but was\nredesigned with computational efficiency, numerical robustness, and\nextensibility in mind. Our approach produces comparable or better results than\nthe current state-of-the-art, while achieving significant computational\nperformance gains and improved numerical stability. The use of diagonal\ncovariance Gaussian kernels, which further improve performance and stability,\nat a small loss of modelling quality, is also explored. Our approach is up to\n40 times faster, while requiring 90\\% less memory than the closest\nstate-of-the-art counterpart.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 15:39:17 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Ferreira", "Jaime", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1606.02615", "submitter": "David Darmon", "authors": "David Darmon", "title": "Specific Differential Entropy Rate Estimation for Continuous-Valued Time\n  Series", "comments": null, "journal-ref": "Entropy 18.5 (2016): 190", "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for quantifying the inherent unpredictability of a\ncontinuous-valued time series via an extension of the differential Shannon\nentropy rate. Our extension, the specific entropy rate, quantifies the amount\nof predictive uncertainty associated with a specific state, rather than\naveraged over all states. We relate the specific entropy rate to popular\n`complexity' measures such as Approximate and Sample Entropies. We provide a\ndata-driven approach for estimating the specific entropy rate of an observed\ntime series. Finally, we consider three case studies of estimating specific\nentropy rate from synthetic and physiological data relevant to the analysis of\nheart rate variability.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 15:57:35 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Darmon", "David", ""]]}, {"id": "1606.02617", "submitter": "Aleksander Lodwich", "authors": "Aleksander Lodwich, Faisal Shafait and Thomas Breuel", "title": "Efficient Estimation of k for the Nearest Neighbors Class of Methods", "comments": "Technical Report, 16p, alternative source:\n  http://lodwich.net/Science.html", "journal-ref": null, "doi": "10.13140/RG.2.1.5045.4649", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k Nearest Neighbors (kNN) method has received much attention in the past\ndecades, where some theoretical bounds on its performance were identified and\nwhere practical optimizations were proposed for making it work fairly well in\nhigh dimensional spaces and on large datasets. From countless experiments of\nthe past it became widely accepted that the value of k has a significant impact\non the performance of this method. However, the efficient optimization of this\nparameter has not received so much attention in literature. Today, the most\ncommon approach is to cross-validate or bootstrap this value for all values in\nquestion. This approach forces distances to be recomputed many times, even if\nefficient methods are used. Hence, estimating the optimal k can become\nexpensive even on modern systems. Frequently, this circumstance leads to a\nsparse manual search of k. In this paper we want to point out that a systematic\nand thorough estimation of the parameter k can be performed efficiently. The\ndiscussed approach relies on large matrices, but we want to argue, that in\npractice a higher space complexity is often much less of a problem than\nrepetitive distance computations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 16:11:53 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 11:34:59 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Lodwich", "Aleksander", ""], ["Shafait", "Faisal", ""], ["Breuel", "Thomas", ""]]}, {"id": "1606.02647", "submitter": "Marc G. Bellemare", "authors": "R\\'emi Munos, Tom Stepleton, Anna Harutyunyan, Marc G. Bellemare", "title": "Safe and Efficient Off-Policy Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we take a fresh look at some old and new algorithms for\noff-policy, return-based reinforcement learning. Expressing these in a common\nform, we derive a novel algorithm, Retrace($\\lambda$), with three desired\nproperties: (1) it has low variance; (2) it safely uses samples collected from\nany behaviour policy, whatever its degree of \"off-policyness\"; and (3) it is\nefficient as it makes the best use of samples collected from near on-policy\nbehaviour policies. We analyze the contractive nature of the related operator\nunder both off-policy policy evaluation and control settings and derive online\nsample-based algorithms. We believe this is the first return-based off-policy\ncontrol algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy\nin the Limit with Infinite Exploration). As a corollary, we prove the\nconvergence of Watkins' Q($\\lambda$), which was an open problem since 1989. We\nillustrate the benefits of Retrace($\\lambda$) on a standard suite of Atari 2600\ngames.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 17:34:13 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 21:26:31 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Munos", "R\u00e9mi", ""], ["Stepleton", "Tom", ""], ["Harutyunyan", "Anna", ""], ["Bellemare", "Marc G.", ""]]}, {"id": "1606.02679", "submitter": "Daniel Romero", "authors": "Daniel Romero, Seung-Jun Kim, Georgios B. Giannakis, Roberto\n  Lopez-Valcarce", "title": "Learning Power Spectrum Maps from Quantized Power Measurements", "comments": "Submitted Jun. 2016", "journal-ref": null, "doi": "10.1109/TSP.2017.2666775", "report-no": null, "categories": "cs.IT cs.LG math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power spectral density (PSD) maps providing the distribution of RF power\nacross space and frequency are constructed using power measurements collected\nby a network of low-cost sensors. By introducing linear compression and\nquantization to a small number of bits, sensor measurements can be communicated\nto the fusion center with minimal bandwidth requirements. Strengths of data-\nand model-driven approaches are combined to develop estimators capable of\nincorporating multiple forms of spectral and propagation prior information\nwhile fitting the rapid variations of shadow fading across space. To this end,\nnovel nonparametric and semiparametric formulations are investigated. It is\nshown that PSD maps can be obtained using support vector machine-type solvers.\nIn addition to batch approaches, an online algorithm attuned to real-time\noperation is developed. Numerical tests assess the performance of the novel\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 01:30:19 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 15:59:29 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Romero", "Daniel", ""], ["Kim", "Seung-Jun", ""], ["Giannakis", "Georgios B.", ""], ["Lopez-Valcarce", "Roberto", ""]]}, {"id": "1606.02689", "submitter": "Pei-Hao Su", "authors": "Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan\n  Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young", "title": "Continuously Learning Neural Dialogue Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a two-step approach for dialogue management in task-oriented\nspoken dialogue systems. A unified neural network framework is proposed to\nenable the system to first learn by supervision from a set of dialogue data and\nthen continuously improve its behaviour via reinforcement learning, all using\ngradient-based algorithms on one single model. The experiments demonstrate the\nsupervised model's effectiveness in the corpus-based evaluation, with user\nsimulation, and with paid human subjects. The use of reinforcement learning\nfurther improves the model's performance in both interactive settings,\nespecially under higher-noise conditions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 19:03:06 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Su", "Pei-Hao", ""], ["Gasic", "Milica", ""], ["Mrksic", "Nikola", ""], ["Rojas-Barahona", "Lina", ""], ["Ultes", "Stefan", ""], ["Vandyke", "David", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1606.02702", "submitter": "Joseph  Salmon", "authors": "Eugene Ndiaye and Olivier Fercoq and Alexandre Gramfort and Vincent\n  Lecl\\`ere and Joseph Salmon", "title": "Efficient Smoothed Concomitant Lasso Estimation for High Dimensional\n  Regression", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/904/1/012006", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional settings, sparse structures are crucial for efficiency,\nboth in term of memory, computation and performance. It is customary to\nconsider $\\ell_1$ penalty to enforce sparsity in such scenarios. Sparsity\nenforcing methods, the Lasso being a canonical example, are popular candidates\nto address high dimension. For efficiency, they rely on tuning a parameter\ntrading data fitting versus sparsity. For the Lasso theory to hold this tuning\nparameter should be proportional to the noise level, yet the latter is often\nunknown in practice. A possible remedy is to jointly optimize over the\nregression parameter as well as over the noise level. This has been considered\nunder several names in the literature: Scaled-Lasso, Square-root Lasso,\nConcomitant Lasso estimation for instance, and could be of interest for\nconfidence sets or uncertainty quantification. In this work, after illustrating\nnumerical difficulties for the Smoothed Concomitant Lasso formulation, we\npropose a modification we coined Smoothed Concomitant Lasso, aimed at\nincreasing numerical stability. We propose an efficient and accurate solver\nleading to a computational cost no more expansive than the one for the Lasso.\nWe leverage on standard ingredients behind the success of fast Lasso solvers: a\ncoordinate descent algorithm, combined with safe screening rules to achieve\nspeed efficiency, by eliminating early irrelevant features.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 19:51:47 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Lecl\u00e8re", "Vincent", ""], ["Salmon", "Joseph", ""]]}, {"id": "1606.02718", "submitter": "Giacomo Torlai", "authors": "Giacomo Torlai and Roger G. Melko", "title": "Learning Thermodynamics with Boltzmann Machines", "comments": "8 pages, 5 figures", "journal-ref": "Phys. Rev. B 94, 165134 (2016)", "doi": "10.1103/PhysRevB.94.165134", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Boltzmann machine is a stochastic neural network that has been extensively\nused in the layers of deep architectures for modern machine learning\napplications. In this paper, we develop a Boltzmann machine that is capable of\nmodelling thermodynamic observables for physical systems in thermal\nequilibrium. Through unsupervised learning, we train the Boltzmann machine on\ndata sets constructed with spin configurations importance-sampled from the\npartition function of an Ising Hamiltonian at different temperatures using\nMonte Carlo (MC) methods. The trained Boltzmann machine is then used to\ngenerate spin states, for which we compare thermodynamic observables to those\ncomputed by direct MC sampling. We demonstrate that the Boltzmann machine can\nfaithfully reproduce the observables of the physical system. Further, we\nobserve that the number of neurons required to obtain accurate results\nincreases as the system is brought close to criticality.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 20:00:01 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Torlai", "Giacomo", ""], ["Melko", "Roger G.", ""]]}, {"id": "1606.02827", "submitter": "Shuyang Gao", "authors": "Shuyang Gao, Greg Ver Steeg, Aram Galstyan", "title": "Variational Information Maximization for Feature Selection", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is one of the most fundamental problems in machine\nlearning. An extensive body of work on information-theoretic feature selection\nexists which is based on maximizing mutual information between subsets of\nfeatures and class labels. Practical methods are forced to rely on\napproximations due to the difficulty of estimating mutual information. We\ndemonstrate that approximations made by existing methods are based on\nunrealistic assumptions. We formulate a more flexible and general class of\nassumptions based on variational distributions and use them to tractably\ngenerate lower bounds for mutual information. These bounds define a novel\ninformation-theoretic framework for feature selection, which we prove to be\noptimal under tree graphical models with proper choice of variational\ndistributions. Our experiments demonstrate that the proposed method strongly\noutperforms existing information-theoretic feature selection approaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 05:19:23 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Gao", "Shuyang", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1606.02838", "submitter": "Nicolas Keriven", "authors": "Nicolas Keriven (UR1, PANAMA), Anthony Bourrier (GIPSA-lab), R\\'emi\n  Gribonval (PANAMA), Patrick P\\'erez", "title": "Sketching for Large-Scale Learning of Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning parameters from voluminous data can be prohibitive in terms of\nmemory and computational requirements. We propose a \"compressive learning\"\nframework where we estimate model parameters from a sketch of the training\ndata. This sketch is a collection of generalized moments of the underlying\nprobability distribution of the data. It can be computed in a single pass on\nthe training set, and is easily computable on streams or distributed datasets.\nThe proposed framework shares similarities with compressive sensing, which aims\nat drastically reducing the dimension of high-dimensional signals while\npreserving the ability to reconstruct them. To perform the estimation task, we\nderive an iterative algorithm analogous to sparse reconstruction algorithms in\nthe context of linear inverse problems. We exemplify our framework with the\ncompressive estimation of a Gaussian Mixture Model (GMM), providing heuristics\non the choice of the sketching procedure and theoretical guarantees of\nreconstruction. We experimentally show on synthetic data that the proposed\nalgorithm yields results comparable to the classical Expectation-Maximization\n(EM) technique while requiring significantly less memory and fewer computations\nwhen the number of database elements is large. We further demonstrate the\npotential of the approach on real large-scale data (over 10 8 training samples)\nfor the task of model-based speaker verification. Finally, we draw some\nconnections between the proposed framework and approximate Hilbert space\nembedding of probability distributions using random features. We show that the\nproposed sketching operator can be seen as an innovative method to design\ntranslation-invariant kernels adapted to the analysis of GMMs. We also use this\ntheoretical framework to derive information preservation guarantees, in the\nspirit of infinite-dimensional compressive sensing.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 06:59:19 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 11:22:44 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Keriven", "Nicolas", "", "UR1, PANAMA"], ["Bourrier", "Anthony", "", "GIPSA-lab"], ["Gribonval", "R\u00e9mi", "", "PANAMA"], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1606.02854", "submitter": "Ioannis Partalas", "authors": "Ioannis Partalas, Georgios Balikas", "title": "e-Commerce product classification: our participation at cDiscount 2015\n  challenge", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes our participation in the cDiscount 2015 challenge where\nthe goal was to classify product items in a predefined taxonomy of products.\nOur best submission yielded an accuracy score of 64.20\\% in the private part of\nthe leaderboard and we were ranked 10th out of 175 participating teams. We\nfollowed a text classification approach employing mainly linear models. The\nfinal solution was a weighted voting system which combined a variety of trained\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 08:06:00 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Partalas", "Ioannis", ""], ["Balikas", "Georgios", ""]]}, {"id": "1606.02960", "submitter": "Sam Wiseman", "authors": "Sam Wiseman and Alexander M. Rush", "title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "comments": "EMNLP 2016 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-Sequence (seq2seq) modeling has rapidly become an important\ngeneral-purpose NLP tool that has proven effective for many text-generation and\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\ninherits its remarkable accuracy in estimating local, next-word distributions.\nIn this work, we introduce a model and beam-search training scheme, based on\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\nsequence scores. This structured approach avoids classical biases associated\nwith local training and unifies the training loss with the test-time usage,\nwhile preserving the proven model architecture of seq2seq and its efficient\ntraining approach. We show that our system outperforms a highly-optimized\nattention-based seq2seq system and other baselines on three different sequence\nto sequence tasks: word ordering, parsing, and machine translation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 13:29:34 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 03:45:30 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Wiseman", "Sam", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1606.02979", "submitter": "Shaohua Li", "authors": "Shaohua Li, Tat-Seng Chua, Jun Zhu, Chunyan Miao", "title": "Generative Topic Embedding: a Continuous Representation of Documents\n  (Extended Version with Proofs)", "comments": "13 pages. The original version has been accepted in ACL 2016 as a\n  long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding maps words into a low-dimensional continuous embedding space\nby exploiting the local word collocation patterns in a small context window. On\nthe other hand, topic modeling maps documents onto a low-dimensional topic\nspace, by utilizing the global word collocation patterns in the same document.\nThese two types of patterns are complementary. In this paper, we propose a\ngenerative topic embedding model to combine the two types of patterns. In our\nmodel, topics are represented by embedding vectors, and are shared across\ndocuments. The probability of each word is influenced by both its local context\nand its topic. A variational inference method yields the topic embeddings as\nwell as the topic mixing proportions for each document. Jointly they represent\nthe document in a low-dimensional continuous space. In two document\nclassification tasks, our method performs better than eight existing methods,\nwith fewer features. In addition, we illustrate with an example that our method\ncan generate coherent topics even based on only one document.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 14:45:39 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 14:49:07 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Li", "Shaohua", ""], ["Chua", "Tat-Seng", ""], ["Zhu", "Jun", ""], ["Miao", "Chunyan", ""]]}, {"id": "1606.03000", "submitter": "Kobi Cohen", "authors": "Kobi Cohen and Angelia Nedic and R. Srikant", "title": "On Projected Stochastic Gradient Descent Algorithm with Weighted\n  Averaging for Least Squares Regression", "comments": "24 pages, 4 figures, part of this work was presented at the The 41st\n  International Conference on Acoustics, Speech, and Signal Processing\n  (ICASSP), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of least squares regression of a $d$-dimensional unknown\nparameter is considered. A stochastic gradient descent based algorithm with\nweighted iterate-averaging that uses a single pass over the data is studied and\nits convergence rate is analyzed. We first consider a bounded constraint set of\nthe unknown parameter. Under some standard regularity assumptions, we provide\nan explicit $O(1/k)$ upper bound on the convergence rate, depending on the\nvariance (due to the additive noise in the measurements) and the size of the\nconstraint set. We show that the variance term dominates the error and\ndecreases with rate $1/k$, while the term which is related to the size of the\nconstraint set decreases with rate $\\log k/k^2$. We then compare the asymptotic\nratio $\\rho$ between the convergence rate of the proposed scheme and the\nempirical risk minimizer (ERM) as the number of iterations approaches infinity.\nWe show that $\\rho\\leq 4$ under some mild conditions for all $d\\geq 1$. We\nfurther improve the upper bound by showing that $\\rho\\leq 4/3$ for the case of\n$d=1$ and unbounded parameter set. Simulation results demonstrate strong\nperformance of the algorithm as compared to existing methods, and coincide with\n$\\rho\\leq 4/3$ even for large $d$ in practice.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 15:35:08 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Cohen", "Kobi", ""], ["Nedic", "Angelia", ""], ["Srikant", "R.", ""]]}, {"id": "1606.03077", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Alistair Stewart", "title": "Efficient Robust Proper Learning of Log-concave Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the {\\em robust proper learning} of univariate log-concave\ndistributions (over continuous and discrete domains). Given a set of samples\ndrawn from an unknown target distribution, we want to compute a log-concave\nhypothesis distribution that is as close as possible to the target, in total\nvariation distance. In this work, we give the first computationally efficient\nalgorithm for this learning problem. Our algorithm achieves the\ninformation-theoretically optimal sample size (up to a constant factor), runs\nin polynomial time, and is robust to model misspecification with nearly-optimal\nerror guarantees.\n  Specifically, we give an algorithm that, on input $n=O(1/\\eps^{5/2})$ samples\nfrom an unknown distribution $f$, runs in time $\\widetilde{O}(n^{8/5})$, and\noutputs a log-concave hypothesis $h$ that (with high probability) satisfies\n$\\dtv(h, f) = O(\\opt)+\\eps$, where $\\opt$ is the minimum total variation\ndistance between $f$ and the class of log-concave distributions. Our approach\nto the robust proper learning problem is quite flexible and may be applicable\nto many other univariate distribution families.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 19:32:20 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1606.03141", "submitter": "Mehdi Sajjadi", "authors": "Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen", "title": "Mutual Exclusivity Loss for Semi-Supervised Deep Learning", "comments": "5 pages, 1 figures, ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of semi-supervised learning with deep\nConvolutional Neural Networks (ConvNets). Semi-supervised learning is motivated\non the observation that unlabeled data is cheap and can be used to improve the\naccuracy of classifiers. In this paper we propose an unsupervised\nregularization term that explicitly forces the classifier's prediction for\nmultiple classes to be mutually-exclusive and effectively guides the decision\nboundary to lie on the low density space between the manifolds corresponding to\ndifferent classes of data. Our proposed approach is general and can be used\nwith any backpropagation-based learning method. We show through different\nexperiments that our method can improve the object recognition performance of\nConvNets using unlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 23:15:16 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Sajjadi", "Mehdi", ""], ["Javanmardi", "Mehran", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1606.03144", "submitter": "Marek Rei", "authors": "Marek Rei and Ronan Cummins", "title": "Sentence Similarity Measures for Fine-Grained Estimation of Topical\n  Relevance in Learner Essays", "comments": "Accepted for publication at BEA-2016", "journal-ref": null, "doi": "10.18653/v1/W16-0533", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the task of assessing sentence-level prompt relevance in\nlearner essays. Various systems using word overlap, neural embeddings and\nneural compositional models are evaluated on two datasets of learner writing.\nWe propose a new method for sentence-level similarity calculation, which learns\nto adjust the weights of pre-trained word embeddings for a specific task,\nachieving substantially higher accuracy compared to other relevant baselines.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 23:42:45 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""], ["Cummins", "Ronan", ""]]}, {"id": "1606.03153", "submitter": "Furong Huang", "authors": "Furong Huang, Animashree Anandkumar", "title": "Unsupervised Learning of Word-Sequence Representations from Scratch via\n  Convolutional Tensor Decomposition", "comments": "There was an error in section 3, there is a bug in the experiment\n  section. We would like to take it down", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised text embeddings extraction is crucial for text understanding in\nmachine learning. Word2Vec and its variants have received substantial success\nin mapping words with similar syntactic or semantic meaning to vectors close to\neach other. However, extracting context-aware word-sequence embedding remains a\nchallenging task. Training over large corpus is difficult as labels are\ndifficult to get. More importantly, it is challenging for pre-trained models to\nobtain word-sequence embeddings that are universally good for all downstream\ntasks or for any new datasets. We propose a two-phased ConvDic+DeconvDec\nframework to solve the problem by combining a word-sequence dictionary learning\nmodel with a word-sequence embedding decode model. We propose a convolutional\ntensor decomposition mechanism to learn good word-sequence phrase dictionary in\nthe learning phase. It is proved to be more accurate and much more efficient\nthan the popular alternating minimization method. In the decode phase, we\nintroduce a deconvolution framework that is immune to the problem of varying\nsentence lengths. The word-sequence embeddings we extracted using\nConvDic+DeconvDec are universally good for a few downstream tasks we test on.\nThe framework requires neither pre-training nor prior/outside information.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 01:22:32 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 22:32:17 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 19:22:09 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Huang", "Furong", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1606.03168", "submitter": "Anastasios Kyrillidis", "authors": "Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, Sujay\n  Sanghavi", "title": "Finding Low-Rank Solutions via Non-Convex Matrix Factorization,\n  Efficiently and Provably", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.IT cs.LG cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rank-$r$ matrix $X \\in \\mathbb{R}^{m \\times n}$ can be written as a product\n$U V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n\n\\times r}$. One could exploit this observation in optimization: e.g., consider\nthe minimization of a convex function $f(X)$ over rank-$r$ matrices, where the\nset of rank-$r$ matrices is modeled via the factorization $UV^\\top$. Though\nsuch parameterization reduces the number of variables, and is more\ncomputationally efficient (of particular interest is the case $r \\ll \\min\\{m,\nn\\}$), it comes at a cost: $f(UV^\\top)$ becomes a non-convex function w.r.t.\n$U$ and $V$.\n  We study such parameterization for optimization of generic convex objectives\n$f$, and focus on first-order, gradient descent algorithmic solutions. We\npropose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient\nfirst-order method that operates on the $U, V$ factors. We show that when $f$\nis (restricted) smooth, BFGD has local sublinear convergence, and linear\nconvergence when $f$ is both (restricted) smooth and (restricted) strongly\nconvex. For several key applications, we provide simple and efficient\ninitialization schemes that provide approximate solutions good enough for the\nabove convergence results to hold.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 03:18:01 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 20:55:56 GMT"}, {"version": "v3", "created": "Sat, 29 Oct 2016 21:03:47 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Park", "Dohyung", ""], ["Kyrillidis", "Anastasios", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1606.03196", "submitter": "Ritesh Kolte", "authors": "Ritesh Kolte and Ayfer \\\"Ozg\\\"ur", "title": "Phase Retrieval via Incremental Truncated Wirtinger Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the phase retrieval problem, an unknown vector is to be recovered given\nquadratic measurements. This problem has received considerable attention in\nrecent times. In this paper, we present an algorithm to solve a nonconvex\nformulation of the phase retrieval problem, that we call $\\textit{Incremental\nTruncated Wirtinger Flow}$. Given random Gaussian sensing vectors, we prove\nthat it converges linearly to the solution, with an optimal sample complexity.\nWe also provide stability guarantees of the algorithm under noisy measurements.\nPerformance and comparisons with existing algorithms are illustrated via\nnumerical experiments on simulated and real data, with both random and\nstructured sensing vectors.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 06:07:08 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Kolte", "Ritesh", ""], ["\u00d6zg\u00fcr", "Ayfer", ""]]}, {"id": "1606.03203", "submitter": "Finnian Lattimore", "authors": "Finnian Lattimore and Tor Lattimore and Mark D. Reid", "title": "Causal Bandits: Learning Good Interventions via Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of using causal models to improve the rate at which good\ninterventions can be learned online in a stochastic environment. Our formalism\ncombines multi-arm bandits and causal inference to model a novel type of bandit\nfeedback that is not exploited by existing approaches. We propose a new\nalgorithm that exploits the causal feedback and prove a bound on its simple\nregret that is strictly better (in all quantities) than algorithms that do not\nuse the additional causal information.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 06:19:32 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Lattimore", "Finnian", ""], ["Lattimore", "Tor", ""], ["Reid", "Mark D.", ""]]}, {"id": "1606.03207", "submitter": "Hwaran Lee", "authors": "Hwaran Lee, Geonmin Kim, Ho-Gyeong Kim, Sang-Hoon Oh, and Soo-Young\n  Lee", "title": "Deep CNNs along the Time Axis with Intermap Pooling for Robustness to\n  Spectral Variations", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2016.2589962", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) with convolutional and pooling\noperations along the frequency axis have been proposed to attain invariance to\nfrequency shifts of features. However, this is inappropriate with regard to the\nfact that acoustic features vary in frequency. In this paper, we contend that\nconvolution along the time axis is more effective. We also propose the addition\nof an intermap pooling (IMP) layer to deep CNNs. In this layer, filters in each\ngroup extract common but spectrally variant features, then the layer pools the\nfeature maps of each group. As a result, the proposed IMP CNN can achieve\ninsensitivity to spectral variations characteristic of different speakers and\nutterances. The effectiveness of the IMP CNN architecture is demonstrated on\nseveral LVCSR tasks. Even without speaker adaptation techniques, the\narchitecture achieved a WER of 12.7% on the SWB part of the Hub5'2000\nevaluation test set, which is competitive with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 06:44:21 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 07:23:53 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Lee", "Hwaran", ""], ["Kim", "Geonmin", ""], ["Kim", "Ho-Gyeong", ""], ["Oh", "Sang-Hoon", ""], ["Lee", "Soo-Young", ""]]}, {"id": "1606.03212", "submitter": "Furong Huang", "authors": "Furong Huang", "title": "Discovery of Latent Factors in High-dimensional Data Using Tensor\n  Methods", "comments": "Ph.D. Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning aims at the discovery of hidden structure that drives\nthe observations in the real world. It is essential for success in modern\nmachine learning. Latent variable models are versatile in unsupervised learning\nand have applications in almost every domain. Training latent variable models\nis challenging due to the non-convexity of the likelihood objective. An\nalternative method is based on the spectral decomposition of low order moment\ntensors. This versatile framework is guaranteed to estimate the correct model\nconsistently. My thesis spans both theoretical analysis of tensor decomposition\nframework and practical implementation of various applications. This thesis\npresents theoretical results on convergence to globally optimal solution of\ntensor decomposition using the stochastic gradient descent, despite\nnon-convexity of the objective. This is the first work that gives global\nconvergence guarantees for the stochastic gradient descent on non-convex\nfunctions with exponentially many local minima and saddle points. This thesis\nalso presents large-scale deployment of spectral methods carried out on various\nplatforms. Dimensionality reduction techniques such as random projection are\nincorporated for a highly parallel and scalable tensor decomposition algorithm.\nWe obtain a gain in both accuracies and in running times by several orders of\nmagnitude compared to the state-of-art variational methods. To solve real world\nproblems, more advanced models and learning algorithms are proposed. This\nthesis discusses generalization of LDA model to mixed membership stochastic\nblock model for learning user communities in social network, convolutional\ndictionary model for learning word-sequence embeddings, hierarchical tensor\ndecomposition and latent tree structure model for learning disease hierarchy,\nand spatial point process mixture model for detecting cell types in\nneuroscience.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 07:17:00 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Huang", "Furong", ""]]}, {"id": "1606.03238", "submitter": "Matteo Gadaleta", "authors": "Matteo Gadaleta and Michele Rossi", "title": "IDNet: Smartphone-based Gait Recognition with Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.09.005", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we present IDNet, a user authentication framework from\nsmartphone-acquired motion signals. Its goal is to recognize a target user from\ntheir way of walking, using the accelerometer and gyroscope (inertial) signals\nprovided by a commercial smartphone worn in the front pocket of the user's\ntrousers. IDNet features several innovations including: i) a robust and\nsmartphone-orientation-independent walking cycle extraction block, ii) a novel\nfeature extractor based on convolutional neural networks, iii) a one-class\nsupport vector machine to classify walking cycles, and the coherent integration\nof these into iv) a multi-stage authentication technique. IDNet is the first\nsystem that exploits a deep learning approach as universal feature extractors\nfor gait recognition, and that combines classification results from subsequent\nwalking cycles into a multi-stage decision making framework. Experimental\nresults show the superiority of our approach against state-of-the-art\ntechniques, leading to misclassification rates (either false negatives or\npositives) smaller than 0.15% with fewer than five walking cycles. Design\nchoices are discussed and motivated throughout, assessing their impact on the\nuser authentication performance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 09:14:28 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 13:15:53 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 12:11:57 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Gadaleta", "Matteo", ""], ["Rossi", "Michele", ""]]}, {"id": "1606.03276", "submitter": "Shaona Ghosh", "authors": "Shaona Ghosh, Kevin Page and David De Roure", "title": "An Application of Network Lasso Optimization For Ride Sharing Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ride sharing has important implications in terms of environmental, social and\nindividual goals by reducing carbon footprints, fostering social interactions\nand economizing commuter costs. The ride sharing systems that are commonly\navailable lack adaptive and scalable techniques that can simultaneously learn\nfrom the large scale data and predict in real-time dynamic fashion. In this\npaper, we study such a problem towards a smart city initiative, where a generic\nride sharing system is conceived capable of making predictions about ride share\nopportunities based on the historically recorded data while satisfying\nreal-time ride requests. Underpinning the system is an application of a\npowerful machine learning convex optimization framework called Network Lasso\nthat uses the Alternate Direction Method of Multipliers (ADMM) optimization for\nlearning and dynamic prediction. We propose an application of a robust and\nscalable unified optimization framework within the ride sharing case-study. The\napplication of Network Lasso framework is capable of jointly optimizing and\nclustering different rides based on their spatial and model similarity. The\nprediction from the framework clusters new ride requests, making accurate price\nprediction based on the clusters, detecting hidden correlations in the data and\nallowing fast convergence due to the network topology. We provide an empirical\nevaluation of the application of ADMM network Lasso on real trip record and\nsimulated data, proving their effectiveness since the mean squared error of the\nalgorithm's prediction is minimized on the test rides.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 11:29:28 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 20:42:29 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 15:09:56 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Ghosh", "Shaona", ""], ["Page", "Kevin", ""], ["De Roure", "David", ""]]}, {"id": "1606.03401", "submitter": "Audrunas Gruslys", "authors": "Audr\\=unas Gruslys, Remi Munos, Ivo Danihelka, Marc Lanctot, Alex\n  Graves", "title": "Memory-Efficient Backpropagation Through Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to reduce memory consumption of the\nbackpropagation through time (BPTT) algorithm when training recurrent neural\nnetworks (RNNs). Our approach uses dynamic programming to balance a trade-off\nbetween caching of intermediate results and recomputation. The algorithm is\ncapable of tightly fitting within almost any user-set memory budget while\nfinding an optimal execution policy minimizing the computational cost.\nComputational devices have limited memory capacity and maximizing a\ncomputational performance given a fixed memory budget is a practical use-case.\nWe provide asymptotic computational upper bounds for various regimes. The\nalgorithm is particularly effective for long sequences. For sequences of length\n1000, our algorithm saves 95\\% of memory usage while using only one third more\ntime per iteration than the standard BPTT.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 17:20:39 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Gruslys", "Audr\u016bnas", ""], ["Munos", "Remi", ""], ["Danihelka", "Ivo", ""], ["Lanctot", "Marc", ""], ["Graves", "Alex", ""]]}, {"id": "1606.03432", "submitter": "Bryan He", "authors": "Bryan He, Christopher De Sa, Ioannis Mitliagkas, Christopher R\\'e", "title": "Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on\n  How Much", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling is a Markov Chain Monte Carlo sampling technique that\niteratively samples variables from their conditional distributions. There are\ntwo common scan orders for the variables: random scan and systematic scan. Due\nto the benefits of locality in hardware, systematic scan is commonly used, even\nthough most statistical guarantees are only for random scan. While it has been\nconjectured that the mixing times of random scan and systematic scan do not\ndiffer by more than a logarithmic factor, we show by counterexample that this\nis not the case, and we prove that that the mixing times do not differ by more\nthan a polynomial factor under mild conditions. To prove these relative bounds,\nwe introduce a method of augmenting the state space to study systematic scan\nusing conductance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 19:24:10 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["He", "Bryan", ""], ["De Sa", "Christopher", ""], ["Mitliagkas", "Ioannis", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1606.03439", "submitter": "Taesup Kim", "authors": "Taesup Kim, Yoshua Bengio", "title": "Deep Directed Generative Models with Energy-Based Probability Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training energy-based probabilistic models is confronted with apparently\nintractable sums, whose Monte Carlo estimation requires sampling from the\nestimated probability distribution in the inner loop of training. This can be\napproximately achieved by Markov chain Monte Carlo methods, but may still face\na formidable obstacle that is the difficulty of mixing between modes with sharp\nconcentrations of probability. Whereas an MCMC process is usually derived from\na given energy function based on mathematical considerations and requires an\narbitrarily long time to obtain good and varied samples, we propose to train a\ndeep directed generative model (not a Markov chain) so that its sampling\ndistribution approximately matches the energy function that is being trained.\nInspired by generative adversarial networks, the proposed framework involves\ntraining of two models that represent dual views of the estimated probability\ndistribution: the energy function (mapping an input configuration to a scalar\nenergy value) and the generator (mapping a noise vector to a generated\nconfiguration), both represented by deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 19:42:57 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Kim", "Taesup", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1606.03474", "submitter": "Jesse Livezey", "authors": "Jesse A. Livezey and Alejandro F. Bujan and Friedrich T. Sommer", "title": "Learning overcomplete, low coherence dictionaries with linear inference", "comments": "27 pages, 11 figures", "journal-ref": "JMLR 20(174) 1-42 (2019)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding overcomplete latent representations of data has applications in data\nanalysis, signal processing, machine learning, theoretical neuroscience and\nmany other fields. In an overcomplete representation, the number of latent\nfeatures exceeds the data dimensionality, which is useful when the data is\nundersampled by the measurements (compressed sensing, information bottlenecks\nin neural systems) or composed from multiple complete sets of linear features,\neach spanning the data space. Independent Components Analysis (ICA) is a linear\ntechnique for learning sparse latent representations, which typically has a\nlower computational cost than sparse coding, its nonlinear, recurrent\ncounterpart. While well suited for finding complete representations, we show\nthat overcompleteness poses a challenge to existing ICA algorithms.\nSpecifically, the coherence control in existing ICA algorithms, necessary to\nprevent the formation of duplicate dictionary features, is ill-suited in the\novercomplete case. We show that in this case several existing ICA algorithms\nhave undesirable global minima that maximize coherence. Further, by comparing\nICA algorithms on synthetic data and natural images to the computationally more\nexpensive sparse coding solution, we show that the coherence control biases the\nexploration of the data manifold, sometimes yielding suboptimal solutions. We\nprovide a theoretical explanation of these failures and, based on the theory,\npropose improved overcomplete ICA algorithms. All told, this study contributes\nnew insights into and methods for coherence control for linear ICA, some of\nwhich are applicable to many other, potentially nonlinear, unsupervised\nlearning methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 20:34:54 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 00:21:47 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 19:13:28 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2018 03:54:19 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Livezey", "Jesse A.", ""], ["Bujan", "Alejandro F.", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "1606.03476", "submitter": "Jonathan Ho", "authors": "Jonathan Ho, Stefano Ermon", "title": "Generative Adversarial Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider learning a policy from example expert behavior, without interaction\nwith the expert or access to reinforcement signal. One approach is to recover\nthe expert's cost function with inverse reinforcement learning, then extract a\npolicy from that cost function with reinforcement learning. This approach is\nindirect and can be slow. We propose a new general framework for directly\nextracting a policy from data, as if it were obtained by reinforcement learning\nfollowing inverse reinforcement learning. We show that a certain instantiation\nof our framework draws an analogy between imitation learning and generative\nadversarial networks, from which we derive a model-free imitation learning\nalgorithm that obtains significant performance gains over existing model-free\nmethods in imitating complex behaviors in large, high-dimensional environments.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 20:51:29 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Ho", "Jonathan", ""], ["Ermon", "Stefano", ""]]}, {"id": "1606.03490", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton", "title": "The Mythos of Model Interpretability", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning models boast remarkable predictive capabilities.\nBut can you trust your model? Will it work in deployment? What else can it tell\nyou about the world? We want models to be not only good, but interpretable. And\nyet the task of interpretation appears underspecified. Papers provide diverse\nand sometimes non-overlapping motivations for interpretability, and offer\nmyriad notions of what attributes render models interpretable. Despite this\nambiguity, many papers proclaim interpretability axiomatically, absent further\nexplanation. In this paper, we seek to refine the discourse on\ninterpretability. First, we examine the motivations underlying interest in\ninterpretability, finding them to be diverse and occasionally discordant. Then,\nwe address model properties and techniques thought to confer interpretability,\nidentifying transparency to humans and post-hoc explanations as competing\nnotions. Throughout, we discuss the feasibility and desirability of different\nnotions, and question the oft-made assertions that linear models are\ninterpretable and that deep neural networks are not.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 21:28:47 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 21:21:04 GMT"}, {"version": "v3", "created": "Mon, 6 Mar 2017 08:51:10 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Lipton", "Zachary C.", ""]]}, {"id": "1606.03498", "submitter": "Ian Goodfellow", "authors": "Tim Salimans and Ian Goodfellow and Wojciech Zaremba and Vicki Cheung\n  and Alec Radford and Xi Chen", "title": "Improved Techniques for Training GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 22:53:35 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Salimans", "Tim", ""], ["Goodfellow", "Ian", ""], ["Zaremba", "Wojciech", ""], ["Cheung", "Vicki", ""], ["Radford", "Alec", ""], ["Chen", "Xi", ""]]}, {"id": "1606.03508", "submitter": "Dana Hughes", "authors": "Dana Hughes and Nikolaus Correll", "title": "Distributed Machine Learning in Materials that Couple Sensing,\n  Actuation, Computation and Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews machine learning applications and approaches to detection,\nclassification and control of intelligent materials and structures with\nembedded distributed computation elements. The purpose of this survey is to\nidentify desired tasks to be performed in each type of material or structure\n(e.g., damage detection in composites), identify and compare common approaches\nto learning such tasks, and investigate models and training paradigms used.\nMachine learning approaches and common temporal features used in the domains of\nstructural health monitoring, morphable aircraft, wearable computing and\nrobotic skins are explored. As the ultimate goal of this research is to\nincorporate the approaches described in this survey into a robotic material\nparadigm, the potential for adapting the computational models used in these\napplications, and corresponding training algorithms, to an amorphous network of\ncomputing nodes is considered. Distributed versions of support vector machines,\ngraphical models and mixture models developed in the field of wireless sensor\nnetworks are reviewed. Potential areas of investigation, including possible\narchitectures for incorporating machine learning into robotic nodes, training\napproaches, and the possibility of using deep learning approaches for automatic\nfeature extraction, are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 00:18:35 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Hughes", "Dana", ""], ["Correll", "Nikolaus", ""]]}, {"id": "1606.03601", "submitter": "Mohamed Aly", "authors": "Mohamed Aly, Guangming Zang, Wolfgang Heidrich, Peter Wonka", "title": "TRex: A Tomography Reconstruction Proximal Framework for Robust Sparse\n  View X-Ray Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TRex, a flexible and robust Tomographic Reconstruction framework\nusing proximal algorithms. We provide an overview and perform an experimental\ncomparison between the famous iterative reconstruction methods in terms of\nreconstruction quality in sparse view situations. We then derive the proximal\noperators for the four best methods. We show the flexibility of our framework\nby deriving solvers for two noise models: Gaussian and Poisson; and by plugging\nin three powerful regularizers. We compare our framework to state of the art\nmethods, and show superior quality on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 14:19:28 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Aly", "Mohamed", ""], ["Zang", "Guangming", ""], ["Heidrich", "Wolfgang", ""], ["Wonka", "Peter", ""]]}, {"id": "1606.03623", "submitter": "Muhammad Ammad-ud-din Mr.", "authors": "Muhammad Ammad-ud-din, Suleiman A.Khan, Disha Malani, Astrid\n  Murum\\\"agi, Olli Kallioniemi, Tero Aittokallio and Samuel Kaski", "title": "Drug response prediction by inferring pathway-response associations with\n  Kernelized Bayesian Matrix Factorization", "comments": "Accepted in European Conference in Computational Biology, to be\n  published in Bioinformatics 2016", "journal-ref": "2016 Bioinformatics Published by Oxford University Press", "doi": "10.1093/bioinformatics/btw433.", "report-no": "32(17):i455-i463", "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key goal of computational personalized medicine is to systematically\nutilize genomic and other molecular features of samples to predict drug\nresponses for a previously unseen sample. Such predictions are valuable for\ndeveloping hypotheses for selecting therapies tailored for individual patients.\nThis is especially valuable in oncology, where molecular and genetic\nheterogeneity of the cells has a major impact on the response. However, the\nprediction task is extremely challenging, raising the need for methods that can\neffectively model and predict drug responses. In this study, we propose a novel\nformulation of multi-task matrix factorization that allows selective data\nintegration for predicting drug responses. To solve the modeling task, we\nextend the state-of-the-art kernelized Bayesian matrix factorization (KBMF)\nmethod with component-wise multiple kernel learning. In addition, our approach\nexploits the known pathway information in a novel and biologically meaningful\nfashion to learn the drug response associations. Our method quantitatively\noutperforms the state of the art on predicting drug responses in two publicly\navailable cancer data sets as well as on a synthetic data set. In addition, we\nvalidated our model predictions with lab experiments using an in-house cancer\ncell line panel. We finally show the practical applicability of the proposed\nmethod by utilizing prior knowledge to infer pathway-drug response\nassociations, opening up the opportunity for elucidating drug action\nmechanisms. We demonstrate that pathway-response associations can be learned by\nthe proposed model for the well known EGFR and MEK inhibitors.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 20:50:53 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Ammad-ud-din", "Muhammad", ""], ["Khan", "Suleiman A.", ""], ["Malani", "Disha", ""], ["Murum\u00e4gi", "Astrid", ""], ["Kallioniemi", "Olli", ""], ["Aittokallio", "Tero", ""], ["Kaski", "Samuel", ""]]}, {"id": "1606.03628", "submitter": "Jiaping Zhao", "authors": "Jiaping Zhao, Zerong Xi and Laurent Itti", "title": "metricDTW: local distance metric learning in Dynamic Time Warping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn multiple local Mahalanobis distance metrics to perform\nk-nearest neighbor (kNN) classification of temporal sequences. Temporal\nsequences are first aligned by dynamic time warping (DTW); given the alignment\npath, similarity between two sequences is measured by the DTW distance, which\nis computed as the accumulated distance between matched temporal point pairs\nalong the alignment path. Traditionally, Euclidean metric is used for distance\ncomputation between matched pairs, which ignores the data regularities and\nmight not be optimal for applications at hand. Here we propose to learn\nmultiple Mahalanobis metrics, such that DTW distance becomes the sum of\nMahalanobis distances. We adapt the large margin nearest neighbor (LMNN)\nframework to our case, and formulate multiple metric learning as a linear\nprogramming problem. Extensive sequence classification results show that our\nproposed multiple metrics learning approach is effective, insensitive to the\npreceding alignment qualities, and reaches the state-of-the-art performances on\nUCR time series datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 21:14:08 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Zhao", "Jiaping", ""], ["Xi", "Zerong", ""], ["Itti", "Laurent", ""]]}, {"id": "1606.03657", "submitter": "Xi Chen", "authors": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever,\n  Pieter Abbeel", "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing\n  Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes InfoGAN, an information-theoretic extension to the\nGenerative Adversarial Network that is able to learn disentangled\nrepresentations in a completely unsupervised manner. InfoGAN is a generative\nadversarial network that also maximizes the mutual information between a small\nsubset of the latent variables and the observation. We derive a lower bound to\nthe mutual information objective that can be optimized efficiently, and show\nthat our training procedure can be interpreted as a variation of the Wake-Sleep\nalgorithm. Specifically, InfoGAN successfully disentangles writing styles from\ndigit shapes on the MNIST dataset, pose from lighting of 3D rendered images,\nand background digits from the central digit on the SVHN dataset. It also\ndiscovers visual concepts that include hair styles, presence/absence of\neyeglasses, and emotions on the CelebA face dataset. Experiments show that\nInfoGAN learns interpretable representations that are competitive with\nrepresentations learned by existing fully supervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 02:14:31 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Chen", "Xi", ""], ["Duan", "Yan", ""], ["Houthooft", "Rein", ""], ["Schulman", "John", ""], ["Sutskever", "Ilya", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1606.03664", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Weakly Supervised Scalable Audio Content Analysis", "comments": "ICME 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Event Detection is an important task for content analysis of multimedia\ndata. Most of the current works on detection of audio events is driven through\nsupervised learning approaches. We propose a weakly supervised learning\nframework which can make use of the tremendous amount of web multimedia data\nwith significantly reduced annotation effort and expense. Specifically, we use\nseveral multiple instance learning algorithms to show that audio event\ndetection through weak labels is feasible. We also propose a novel scalable\nmultiple instance learning algorithm and show that its competitive with other\nmultiple instance learning algorithms for audio event detection tasks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 04:07:45 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1606.03667", "submitter": "Ji He", "authors": "Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen, Jianfeng Gao, Lihong\n  Li, Li Deng", "title": "Deep Reinforcement Learning with a Combinatorial Action Space for\n  Predicting Popular Reddit Threads", "comments": "To be published in EMNLP 2016, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an online popularity prediction and tracking task as a benchmark\ntask for reinforcement learning with a combinatorial, natural language action\nspace. A specified number of discussion threads predicted to be popular are\nrecommended, chosen from a fixed window of recent comments to track. Novel deep\nreinforcement learning architectures are studied for effective modeling of the\nvalue function associated with actions comprised of interdependent sub-actions.\nThe proposed model, which represents dependence between sub-actions through a\nbi-directional LSTM, gives the best performance across different experimental\nconfigurations and domains, and it also generalizes well with varying numbers\nof recommendation requests.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 05:38:20 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 22:31:36 GMT"}, {"version": "v3", "created": "Thu, 8 Sep 2016 06:38:20 GMT"}, {"version": "v4", "created": "Sat, 17 Sep 2016 00:52:43 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["He", "Ji", ""], ["Ostendorf", "Mari", ""], ["He", "Xiaodong", ""], ["Chen", "Jianshu", ""], ["Gao", "Jianfeng", ""], ["Li", "Lihong", ""], ["Deng", "Li", ""]]}, {"id": "1606.03672", "submitter": "Ashkan Esmaeili", "authors": "Ashkan Esmaeili and Farokh Marvasti", "title": "Comparison of Several Sparse Recovery Methods for Low Rank Matrices with\n  Random Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will investigate the efficacy of IMAT (Iterative Method of\nAdaptive Thresholding) in recovering the sparse signal (parameters) for linear\nmodels with missing data. Sparse recovery rises in compressed sensing and\nmachine learning problems and has various applications necessitating viable\nreconstruction methods specifically when we work with big data. This paper will\nfocus on comparing the power of IMAT in reconstruction of the desired sparse\nsignal with LASSO. Additionally, we will assume the model has random missing\ninformation. Missing data has been recently of interest in big data and machine\nlearning problems since they appear in many cases including but not limited to\nmedical imaging datasets, hospital datasets, and massive MIMO. The dominance of\nIMAT over the well-known LASSO will be taken into account in different\nscenarios. Simulations and numerical results are also provided to verify the\narguments.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 07:05:22 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Esmaeili", "Ashkan", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1606.03685", "submitter": "Pantelis Bouboulis", "authors": "Pantelis Bouboulis and Spyridon Pougkakiotis, Sergios Theodoridis", "title": "Efficient KLMS and KRLS Algorithms: A Random Fourier Feature Perspective", "comments": "presented in the 2016 IEEE Workshop on Statistical Signal Processing\n  (SSP 16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for online Least Squares algorithms for nonlinear\nmodeling in RKH spaces (RKHS). Instead of implicitly mapping the data to a RKHS\n(e.g., kernel trick), we map the data to a finite dimensional Euclidean space,\nusing random features of the kernel's Fourier transform. The advantage is that,\nthe inner product of the mapped data approximates the kernel function. The\nresulting \"linear\" algorithm does not require any form of sparsification,\nsince, in contrast to all existing algorithms, the solution's size remains\nfixed and does not increase with the iteration steps. As a result, the obtained\nalgorithms are computationally significantly more efficient compared to\npreviously derived variants, while, at the same time, they converge at similar\nspeeds and to similar error floors.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 08:59:45 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Bouboulis", "Pantelis", ""], ["Pougkakiotis", "Spyridon", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1606.03777", "submitter": "Nikola Mrk\\v{s}i\\'c", "authors": "Nikola Mrk\\v{s}i\\'c and Diarmuid \\'O S\\'eaghdha and Tsung-Hsien Wen\n  and Blaise Thomson and Steve Young", "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking", "comments": "Accepted as a long paper for the 55th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core components of modern spoken dialogue systems is the belief\ntracker, which estimates the user's goal at every step of the dialogue.\nHowever, most current approaches have difficulty scaling to larger, more\ncomplex dialogue domains. This is due to their dependency on either: a) Spoken\nLanguage Understanding models that require large amounts of annotated training\ndata; or b) hand-crafted lexicons for capturing some of the linguistic\nvariation in users' language. We propose a novel Neural Belief Tracking (NBT)\nframework which overcomes these problems by building on recent advances in\nrepresentation learning. NBT models reason over pre-trained word vectors,\nlearning to compose them into distributed representations of user utterances\nand dialogue context. Our evaluation on two datasets shows that this approach\nsurpasses past limitations, matching the performance of state-of-the-art models\nwhich rely on hand-crafted semantic lexicons and outperforming them when such\nlexicons are not provided.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 22:59:14 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 15:15:03 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Mrk\u0161i\u0107", "Nikola", ""], ["S\u00e9aghdha", "Diarmuid \u00d3", ""], ["Wen", "Tsung-Hsien", ""], ["Thomson", "Blaise", ""], ["Young", "Steve", ""]]}, {"id": "1606.03783", "submitter": "Thomas Lampert", "authors": "Pedro Chahuara, Thomas Lampert, Pierre Gancarski", "title": "Retrieving and Ranking Similar Questions from Question-Answer Archives\n  Using Topic Modelling and Topic Distribution Regression", "comments": "International Conference on Theory and Practice of Digital Libraries\n  2016 (accepted)", "journal-ref": null, "doi": "10.1007/978-3-319-43997-6_4", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presented herein is a novel model for similar question ranking within\ncollaborative question answer platforms. The presented approach integrates a\nregression stage to relate topics derived from questions to those derived from\nquestion-answer pairs. This helps to avoid problems caused by the differences\nin vocabulary used within questions and answers, and the tendency for questions\nto be shorter than answers. The performance of the model is shown to outperform\ntranslation methods and topic modelling (without regression) on several\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 23:50:19 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chahuara", "Pedro", ""], ["Lampert", "Thomas", ""], ["Gancarski", "Pierre", ""]]}, {"id": "1606.03802", "submitter": "Pedro Ribeiro Mendes J\\'unior", "authors": "Pedro Ribeiro Mendes J\\'unior, Terrance E. Boult, Jacques Wainer, and\n  Anderson Rocha", "title": "Specialized Support Vector Machines for Open-set Recognition", "comments": "Some additional information were added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Often, when dealing with real-world recognition problems, we do not need, and\noften cannot have, knowledge of the entire set of possible classes that might\nappear during operational testing. In such cases, we need to think of robust\nclassification methods able to deal with the \"unknown\" and properly reject\nsamples belonging to classes never seen during training. Notwithstanding,\nalmost all existing classifiers to date were mostly developed for the\nclosed-set scenario, i.e., the classification setup in which it is assumed that\nall test samples belong to one of the classes with which the classifier was\ntrained. In the open-set scenario, however, a test sample can belong to none of\nthe known classes and the classifier must properly reject it by classifying it\nas unknown. In this work, we extend upon the well-known Support Vector Machines\n(SVM) classifier and introduce the Specialized Support Vector Machines (SSVM),\nwhich is suitable for recognition in open-set setups. SSVM balances the\nempirical risk and the risk of the unknown and ensures that the region of the\nfeature space in which a test sample would be classified as known (one of the\nknown classes) is always bounded, ensuring a finite risk of the unknown. In\nthis work, we also highlight the properties of the SVM classifier related to\nthe open-set scenario, and provide necessary and sufficient conditions for an\nRBF SVM to have bounded open-space risk.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 03:46:17 GMT"}, {"version": "v10", "created": "Tue, 21 Apr 2020 22:45:04 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 22:54:27 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 16:30:28 GMT"}, {"version": "v4", "created": "Thu, 1 Mar 2018 22:47:18 GMT"}, {"version": "v5", "created": "Tue, 26 Jun 2018 17:33:55 GMT"}, {"version": "v6", "created": "Mon, 5 Nov 2018 13:30:56 GMT"}, {"version": "v7", "created": "Wed, 14 Nov 2018 15:09:14 GMT"}, {"version": "v8", "created": "Thu, 2 May 2019 13:20:51 GMT"}, {"version": "v9", "created": "Wed, 13 Nov 2019 18:44:31 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["J\u00fanior", "Pedro Ribeiro Mendes", ""], ["Boult", "Terrance E.", ""], ["Wainer", "Jacques", ""], ["Rocha", "Anderson", ""]]}, {"id": "1606.03841", "submitter": "Quanming Yao", "authors": "Quanming Yao and James.T Kwok", "title": "Efficient Learning with a Family of Nonconvex Regularizers by\n  Redistributing Nonconvexity", "comments": "Journal version of previous conference paper appeared at ICML-2016\n  with same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of convex regularizers allows for easy optimization, though they\noften produce biased estimation and inferior prediction performance. Recently,\nnonconvex regularizers have attracted a lot of attention and outperformed\nconvex ones. However, the resultant optimization problem is much harder. In\nthis paper, for a large class of nonconvex regularizers, we propose to move the\nnonconvexity from the regularizer to the loss. The nonconvex regularizer is\nthen transformed to a familiar convex regularizer, while the resultant loss\nfunction can still be guaranteed to be smooth. Learning with the convexified\nregularizer can be performed by existing efficient algorithms originally\ndesigned for convex regularizers (such as the proximal algorithm, Frank-Wolfe\nalgorithm, alternating direction method of multipliers and stochastic gradient\ndescent). Extensions are made when the convexified regularizer does not have\nclosed-form proximal step, and when the loss function is nonconvex, nonsmooth.\nExtensive experiments on a variety of machine learning application scenarios\nshow that optimizing the transformed problem is much faster than running the\nstate-of-the-art on the original problem.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 07:21:31 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 05:57:58 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 01:27:29 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James. T", ""]]}, {"id": "1606.03858", "submitter": "Edouard Pauwels", "authors": "Jean-Bernard Lasserre, Edouard Pauwels", "title": "Sorting out typicality with the inverse moment matrix SOS polynomial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a surprising phenomenon related to the representation of a cloud of\ndata points using polynomials. We start with the previously unnoticed empirical\nobservation that, given a collection (a cloud) of data points, the sublevel\nsets of a certain distinguished polynomial capture the shape of the cloud very\naccurately. This distinguished polynomial is a sum-of-squares (SOS) derived in\na simple manner from the inverse of the empirical moment matrix. In fact, this\nSOS polynomial is directly related to orthogonal polynomials and the\nChristoffel function. This allows to generalize and interpret extremality\nproperties of orthogonal polynomials and to provide a mathematical rationale\nfor the observed phenomenon. Among diverse potential applications, we\nillustrate the relevance of our results on a network intrusion detection task\nfor which we obtain performances similar to existing dedicated methods reported\nin the literature.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 08:55:20 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 08:02:03 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Lasserre", "Jean-Bernard", ""], ["Pauwels", "Edouard", ""]]}, {"id": "1606.03860", "submitter": "Yixin Wang", "authors": "Yixin Wang, Alp Kucukelbir, David M. Blei", "title": "Robust Probabilistic Modeling with Bayesian Data Reweighting", "comments": "In ICML 2017. Updated related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models analyze data by relying on a set of assumptions. Data\nthat exhibit deviations from these assumptions can undermine inference and\nprediction quality. Robust models offer protection against mismatch between a\nmodel's assumptions and reality. We propose a way to systematically detect and\nmitigate mismatch of a large class of probabilistic models. The idea is to\nraise the likelihood of each observation to a weight and then to infer both the\nlatent variables and the weights from data. Inferring the weights allows a\nmodel to identify observations that match its assumptions and down-weight\nothers. This enables robust inference and improves predictive accuracy. We\nstudy four different forms of mismatch with reality, ranging from missing\nlatent groups to structure misspecification. A Poisson factorization analysis\nof the Movielens 1M dataset shows the benefits of this approach in a practical\nscenario.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 08:56:35 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 21:13:37 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 16:44:54 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Wang", "Yixin", ""], ["Kucukelbir", "Alp", ""], ["Blei", "David M.", ""]]}, {"id": "1606.03864", "submitter": "Dirk Weissenborn", "authors": "Dirk Weissenborn", "title": "Neural Associative Memory for Dual-Sequence Modeling", "comments": "To appear in RepL4NLP at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important NLP problems can be posed as dual-sequence or\nsequence-to-sequence modeling tasks. Recent advances in building end-to-end\nneural architectures have been highly successful in solving such tasks. In this\nwork we propose a new architecture for dual-sequence modeling that is based on\nassociative memory. We derive AM-RNNs, a recurrent associative memory (AM)\nwhich augments generic recurrent neural networks (RNN). This architecture is\nextended to the Dual AM-RNN which operates on two AMs at once. Our models\nachieve very competitive results on textual entailment. A qualitative analysis\ndemonstrates that long range dependencies between source and target-sequence\ncan be bridged effectively using Dual AM-RNNs. However, an initial experiment\non auto-encoding reveals that these benefits are not exploited by the system\nwhen learning to solve sequence-to-sequence tasks which indicates that\nadditional supervision or regularization is needed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 09:08:04 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 07:59:18 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Weissenborn", "Dirk", ""]]}, {"id": "1606.03956", "submitter": "Eric Tramel", "authors": "Eric W. Tramel and Andre Manoel and Francesco Caltagirone and Marylou\n  Gabri\\'e and Florent Krzakala", "title": "Inferring Sparsity: Compressed Sensing using Generalized Restricted\n  Boltzmann Machines", "comments": "IEEE Information Theory Workshop, 2016", "journal-ref": "2016 IEEE Information Theory Workshop (ITW), Pages: 265 - 269", "doi": "10.1109/ITW.2016.7606837", "report-no": null, "categories": "cs.IT cond-mat.dis-nn cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider compressed sensing reconstruction from $M$\nmeasurements of $K$-sparse structured signals which do not possess a writable\ncorrelation model. Assuming that a generative statistical model, such as a\nBoltzmann machine, can be trained in an unsupervised manner on example signals,\nwe demonstrate how this signal model can be used within a Bayesian framework of\nsignal reconstruction. By deriving a message-passing inference for general\ndistribution restricted Boltzmann machines, we are able to integrate these\ninferred signal models into approximate message passing for compressed sensing\nreconstruction. Finally, we show for the MNIST dataset that this approach can\nbe very effective, even for $M < K$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 14:03:50 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Tramel", "Eric W.", ""], ["Manoel", "Andre", ""], ["Caltagirone", "Francesco", ""], ["Gabri\u00e9", "Marylou", ""], ["Krzakala", "Florent", ""]]}, {"id": "1606.03966", "submitter": "Siddhartha Sen", "authors": "Alekh Agarwal, Sarah Bird, Markus Cozowicz, Luong Hoang, John\n  Langford, Stephen Lee, Jiaji Li, Dan Melamed, Gal Oshri, Oswaldo Ribas,\n  Siddhartha Sen, Alex Slivkins", "title": "Making Contextual Decisions with Low Technical Debt", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications and systems are constantly faced with decisions that require\npicking from a set of actions based on contextual information.\nReinforcement-based learning algorithms such as contextual bandits can be very\neffective in these settings, but applying them in practice is fraught with\ntechnical debt, and no general system exists that supports them completely. We\naddress this and create the first general system for contextual learning,\ncalled the Decision Service.\n  Existing systems often suffer from technical debt that arises from issues\nlike incorrect data collection and weak debuggability, issues we systematically\naddress through our ML methodology and system abstractions. The Decision\nService enables all aspects of contextual bandit learning using four system\nabstractions which connect together in a loop: explore (the decision space),\nlog, learn, and deploy. Notably, our new explore and log abstractions ensure\nthe system produces correct, unbiased data, which our learner uses for online\nlearning and to enable real-time safeguards, all in a fully reproducible\nmanner.\n  The Decision Service has a simple user interface and works with a variety of\napplications: we present two live production deployments for content\nrecommendation that achieved click-through improvements of 25-30%, another with\n18% revenue lift in the landing page, and ongoing applications in tech support\nand machine failure handling. The service makes real-time decisions and learns\ncontinuously and scalably, while significantly lowering technical debt.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 14:17:00 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 14:41:15 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Agarwal", "Alekh", ""], ["Bird", "Sarah", ""], ["Cozowicz", "Markus", ""], ["Hoang", "Luong", ""], ["Langford", "John", ""], ["Lee", "Stephen", ""], ["Li", "Jiaji", ""], ["Melamed", "Dan", ""], ["Oshri", "Gal", ""], ["Ribas", "Oswaldo", ""], ["Sen", "Siddhartha", ""], ["Slivkins", "Alex", ""]]}, {"id": "1606.03976", "submitter": "Fredrik D. Johansson", "authors": "Uri Shalit, Fredrik D. Johansson, David Sontag", "title": "Estimating individual treatment effect: generalization bounds and\n  algorithms", "comments": "Added name \"TARNet\" to refer to version with alpha = 0. Removed supp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is intense interest in applying machine learning to problems of causal\ninference in fields such as healthcare, economics and education. In particular,\nindividual-level causal inference has important applications such as precision\nmedicine. We give a new theoretical analysis and family of algorithms for\npredicting individual treatment effect (ITE) from observational data, under the\nassumption known as strong ignorability. The algorithms learn a \"balanced\"\nrepresentation such that the induced treated and control distributions look\nsimilar. We give a novel, simple and intuitive generalization-error bound\nshowing that the expected ITE estimation error of a representation is bounded\nby a sum of the standard generalization-error of that representation and the\ndistance between the treated and control distributions induced by the\nrepresentation. We use Integral Probability Metrics to measure distances\nbetween distributions, deriving explicit bounds for the Wasserstein and Maximum\nMean Discrepancy (MMD) distances. Experiments on real and simulated data show\nthe new algorithms match or outperform the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 14:40:57 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 13:13:05 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 18:17:38 GMT"}, {"version": "v4", "created": "Wed, 1 Mar 2017 15:44:15 GMT"}, {"version": "v5", "created": "Tue, 16 May 2017 15:11:15 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Shalit", "Uri", ""], ["Johansson", "Fredrik D.", ""], ["Sontag", "David", ""]]}, {"id": "1606.04038", "submitter": "Yongxin Yang", "authors": "Yongxin Yang, Timothy M. Hospedales", "title": "Trace Norm Regularised Deep Multi-Task Learning", "comments": "Submission to Workshop track - ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for training multiple neural networks simultaneously.\nThe parameters from all models are regularised by the tensor trace norm, so\nthat each neural network is encouraged to reuse others' parameters if possible\n-- this is the main motivation behind multi-task learning. In contrast to many\ndeep multi-task learning models, we do not predefine a parameter sharing\nstrategy by specifying which layers have tied parameters. Instead, our\nframework considers sharing for all shareable layers, and the sharing strategy\nis learned in a data-driven way.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 17:15:43 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 01:33:17 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1606.04056", "submitter": "Nadia Labai", "authors": "Nadia Labai and Johann A. Makowsky", "title": "On the exact learnability of graph parameters: The case of partition\n  functions", "comments": "14 pages, full version of the MFCS 2016 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the exact learnability of real valued graph parameters $f$ which are\nknown to be representable as partition functions which count the number of\nweighted homomorphisms into a graph $H$ with vertex weights $\\alpha$ and edge\nweights $\\beta$. M. Freedman, L. Lov\\'asz and A. Schrijver have given a\ncharacterization of these graph parameters in terms of the $k$-connection\nmatrices $C(f,k)$ of $f$. Our model of learnability is based on D. Angluin's\nmodel of exact learning using membership and equivalence queries. Given such a\ngraph parameter $f$, the learner can ask for the values of $f$ for graphs of\ntheir choice, and they can formulate hypotheses in terms of the connection\nmatrices $C(f,k)$ of $f$. The teacher can accept the hypothesis as correct, or\nprovide a counterexample consisting of a graph. Our main result shows that in\nthis scenario, a very large class of partition functions, the rigid partition\nfunctions, can be learned in time polynomial in the size of $H$ and the size of\nthe largest counterexample in the Blum-Shub-Smale model of computation over the\nreals with unit cost.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 18:18:08 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Labai", "Nadia", ""], ["Makowsky", "Johann A.", ""]]}, {"id": "1606.04080", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals and Charles Blundell and Timothy Lillicrap and Koray\n  Kavukcuoglu and Daan Wierstra", "title": "Matching Networks for One Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from a few examples remains a key challenge in machine learning.\nDespite recent advances in important domains such as vision and language, the\nstandard supervised deep learning paradigm does not offer a satisfactory\nsolution for learning new concepts rapidly from little data. In this work, we\nemploy ideas from metric learning based on deep neural features and from recent\nadvances that augment neural networks with external memories. Our framework\nlearns a network that maps a small labelled support set and an unlabelled\nexample to its label, obviating the need for fine-tuning to adapt to new class\ntypes. We then define one-shot learning problems on vision (using Omniglot,\nImageNet) and language tasks. Our algorithm improves one-shot accuracy on\nImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to\ncompeting approaches. We also demonstrate the usefulness of the same model on\nlanguage modeling by introducing a one-shot task on the Penn Treebank.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 19:34:22 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 17:45:19 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Vinyals", "Oriol", ""], ["Blundell", "Charles", ""], ["Lillicrap", "Timothy", ""], ["Kavukcuoglu", "Koray", ""], ["Wierstra", "Daan", ""]]}, {"id": "1606.04130", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, David C. Kale, Randall Wetzel", "title": "Modeling Missing Data in Clinical Time Series with RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a simple strategy to cope with missing data in sequential\ninputs, addressing the task of multilabel classification of diagnoses given\nclinical time series. Collected from the pediatric intensive care unit (PICU)\nat Children's Hospital Los Angeles, our data consists of multivariate time\nseries of observations. The measurements are irregularly spaced, leading to\nmissingness patterns in temporally discretized sequences. While these artifacts\nare typically handled by imputation, we achieve superior predictive performance\nby treating the artifacts as features. Unlike linear models, recurrent neural\nnetworks can realize this improvement using only simple binary indicators of\nmissingness. For linear models, we show an alternative strategy to capture this\nsignal. Training models on missingness patterns only, we show that for some\ndiseases, what tests are run can be as predictive as the results themselves.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 20:34:35 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 09:04:23 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 08:51:35 GMT"}, {"version": "v4", "created": "Tue, 20 Sep 2016 01:10:14 GMT"}, {"version": "v5", "created": "Fri, 11 Nov 2016 12:46:53 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Kale", "David C.", ""], ["Wetzel", "Randall", ""]]}, {"id": "1606.04142", "submitter": "Jean Barbier", "authors": "Jean Barbier, Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault\n  Lesieur, Lenka Zdeborova", "title": "Mutual information for symmetric rank-one matrix estimation: A proof of\n  the replica formula", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 29 (NIPS 2016)\n  pp 424-432", "doi": null, "report-no": null, "categories": "cs.IT cond-mat.dis-nn cs.LG math-ph math.IT math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorizing low-rank matrices has many applications in machine learning and\nstatistics. For probabilistic models in the Bayes optimal setting, a general\nexpression for the mutual information has been proposed using heuristic\nstatistical physics computations, and proven in few specific cases. Here, we\nshow how to rigorously prove the conjectured formula for the symmetric rank-one\ncase. This allows to express the minimal mean-square-error and to characterize\nthe detectability phase transitions in a large set of estimation problems\nranging from community detection to sparse PCA. We also show that for a large\nset of parameters, an iterative algorithm called approximate message-passing is\nBayes optimal. There exists, however, a gap between what currently known\npolynomial algorithms can do and what is expected information theoretically.\nAdditionally, the proof technique has an interest of its own and exploits three\nessential ingredients: the interpolation method introduced in statistical\nphysics by Guerra, the analysis of the approximate message-passing algorithm\nand the theory of spatial coupling and threshold saturation in coding. Our\napproach is generic and applicable to other open problems in statistical\nestimation where heuristic statistical physics predictions are available.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 21:17:15 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Barbier", "Jean", ""], ["Dia", "Mohamad", ""], ["Macris", "Nicolas", ""], ["Krzakala", "Florent", ""], ["Lesieur", "Thibault", ""], ["Zdeborova", "Lenka", ""]]}, {"id": "1606.04145", "submitter": "Ellen Vitercik", "authors": "Maria-Florina Balcan, Tuomas Sandholm, Ellen Vitercik", "title": "Sample Complexity of Automated Mechanism Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of revenue-maximizing combinatorial auctions, i.e. multi-item\nauctions over bundles of goods, is one of the most fundamental problems in\ncomputational economics, unsolved even for two bidders and two items for sale.\nIn the traditional economic models, it is assumed that the bidders' valuations\nare drawn from an underlying distribution and that the auction designer has\nperfect knowledge of this distribution. Despite this strong and oftentimes\nunrealistic assumption, it is remarkable that the revenue-maximizing\ncombinatorial auction remains unknown. In recent years, automated mechanism\ndesign has emerged as one of the most practical and promising approaches to\ndesigning high-revenue combinatorial auctions. The most scalable automated\nmechanism design algorithms take as input samples from the bidders' valuation\ndistribution and then search for a high-revenue auction in a rich auction\nclass. In this work, we provide the first sample complexity analysis for the\nstandard hierarchy of deterministic combinatorial auction classes used in\nautomated mechanism design. In particular, we provide tight sample complexity\nbounds on the number of samples needed to guarantee that the empirical revenue\nof the designed mechanism on the samples is close to its expected revenue on\nthe underlying, unknown distribution over bidder valuations, for each of the\nauction classes in the hierarchy. In addition to helping set automated\nmechanism design on firm foundations, our results also push the boundaries of\nlearning theory. In particular, the hypothesis functions used in our contexts\nare defined through multi-stage combinatorial optimization procedures, rather\nthan simple decision boundaries, as are common in machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 21:25:22 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Sandholm", "Tuomas", ""], ["Vitercik", "Ellen", ""]]}, {"id": "1606.04160", "submitter": "Richard Nock", "authors": "Richard Nock, Giorgio Patrini, Finnian Lattimore, Tiberio Caetano", "title": "The Crossover Process: Learnability and Data Protection from Inference\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is usual to consider data protection and learnability as conflicting\nobjectives. This is not always the case: we show how to jointly control\ninference --- seen as the attack --- and learnability by a noise-free process\nthat mixes training examples, the Crossover Process (cp). One key point is that\nthe cp~is typically able to alter joint distributions without touching on\nmarginals, nor altering the sufficient statistic for the class. In other words,\nit saves (and sometimes improves) generalization for supervised learning, but\ncan alter the relationship between covariates --- and therefore fool measures\nof nonlinear independence and causal inference into misleading ad-hoc\nconclusions. For example, a cp~can increase / decrease odds ratios, bring\nfairness or break fairness, tamper with disparate impact, strengthen, weaken or\nreverse causal directions, change observed statistical measures of dependence.\nFor each of these, we quantify changes brought by a cp, as well as its\nstatistical impact on generalization abilities via a new complexity measure\nthat we call the Rademacher cp~complexity. Experiments on a dozen readily\navailable domains validate the theory.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 22:27:36 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 21:41:50 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Nock", "Richard", ""], ["Patrini", "Giorgio", ""], ["Lattimore", "Finnian", ""], ["Caetano", "Tiberio", ""]]}, {"id": "1606.04166", "submitter": "Heinrich Jiang", "authors": "Heinrich Jiang, Samory Kpotufe", "title": "Modal-set estimation with an application to clustering", "comments": null, "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics, in PMLR 54:1197-1206, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a first procedure that can estimate -- with statistical\nconsistency guarantees -- any local-maxima of a density, under benign\ndistributional conditions. The procedure estimates all such local maxima, or\n$\\textit{modal-sets}$, of any bounded shape or dimension, including usual\npoint-modes. In practice, modal-sets can arise as dense low-dimensional\nstructures in noisy data, and more generally serve to better model the rich\nvariety of locally-high-density structures in data.\n  The procedure is then shown to be competitive on clustering applications, and\nmoreover is quite stable to a wide range of settings of its tuning parameter.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 22:44:39 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Jiang", "Heinrich", ""], ["Kpotufe", "Samory", ""]]}, {"id": "1606.04189", "submitter": "Andrey Zhmoginov", "authors": "Andrey Zhmoginov and Mark Sandler", "title": "Inverting face embeddings with convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have dramatically advanced the state of the art for many\nareas of machine learning. Recently they have been shown to have a remarkable\nability to generate highly complex visual artifacts such as images and text\nrather than simply recognize them.\n  In this work we use neural networks to effectively invert low-dimensional\nface embeddings while producing realistically looking consistent images. Our\ncontribution is twofold, first we show that a gradient ascent style approaches\ncan be used to reproduce consistent images, with a help of a guiding image.\nSecond, we demonstrate that we can train a separate neural network to\neffectively solve the minimization problem in one pass, and generate images in\nreal-time. We then evaluate the loss imposed by using a neural network instead\nof the gradient descent by comparing the final values of the minimized loss\nfunction.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 01:35:12 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 18:52:57 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Zhmoginov", "Andrey", ""], ["Sandler", "Mark", ""]]}, {"id": "1606.04199", "submitter": "Jie Zhou", "authors": "Jie Zhou and Ying Cao and Xuguang Wang and Peng Li and Wei Xu", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine\n  Translation", "comments": "TACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) aims at solving machine translation (MT)\nproblems using neural networks and has exhibited promising results in recent\nyears. However, most of the existing NMT models are shallow and there is still\na performance gap between a single NMT model and the best conventional MT\nsystem. In this work, we introduce a new type of linear connections, named\nfast-forward connections, based on deep Long Short-Term Memory (LSTM) networks,\nand an interleaved bi-directional architecture for stacking the LSTM layers.\nFast-forward connections play an essential role in propagating the gradients\nand building a deep topology of depth 16. On the WMT'14 English-to-French task,\nwe achieve BLEU=37.7 with a single attention model, which outperforms the\ncorresponding single shallow model by 6.2 BLEU points. This is the first time\nthat a single NMT model achieves state-of-the-art performance and outperforms\nthe best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3\neven without using an attention mechanism. After special handling of unknown\nwords and model ensembling, we obtain the best score reported to date on this\ntask with BLEU=40.4. Our models are also validated on the more difficult WMT'14\nEnglish-to-German task.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 03:53:00 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 04:21:03 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2016 13:14:17 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Zhou", "Jie", ""], ["Cao", "Ying", ""], ["Wang", "Xuguang", ""], ["Li", "Peng", ""], ["Xu", "Wei", ""]]}, {"id": "1606.04218", "submitter": "Yong Ren", "authors": "Yong Ren, Jialian Li, Yucen Luo, Jun Zhu", "title": "Conditional Generative Moment-Matching Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum mean discrepancy (MMD) has been successfully applied to learn deep\ngenerative models for characterizing a joint distribution of variables via\nkernel mean embedding. In this paper, we present conditional generative moment-\nmatching networks (CGMMN), which learn a conditional distribution given some\ninput variables based on a conditional maximum mean discrepancy (CMMD)\ncriterion. The learning is performed by stochastic gradient descent with the\ngradient calculated by back-propagation. We evaluate CGMMN on a wide range of\ntasks, including predictive modeling, contextual generation, and Bayesian dark\nknowledge, which distills knowledge from a Bayesian model by learning a\nrelatively small CGMMN student network. Our results demonstrate competitive\nperformance in all the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 07:11:29 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Ren", "Yong", ""], ["Li", "Jialian", ""], ["Luo", "Yucen", ""], ["Zhu", "Jun", ""]]}, {"id": "1606.04232", "submitter": "Maya Kabkab", "authors": "Maya Kabkab, Azadeh Alavi, Rama Chellappa", "title": "DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale supervised classification algorithms, especially those based on\ndeep convolutional neural networks (DCNNs), require vast amounts of training\ndata to achieve state-of-the-art performance. Decreasing this data requirement\nwould significantly speed up the training process and possibly improve\ngeneralization. Motivated by this objective, we consider the task of adaptively\nfinding concise training subsets which will be iteratively presented to the\nlearner. We use convex optimization methods, based on an objective criterion\nand feedback from the current performance of the classifier, to efficiently\nidentify informative samples to train on. We propose an algorithm to decompose\nthe optimization problem into smaller per-class problems, which can be solved\nin parallel. We test our approach on standard classification tasks and\ndemonstrate its effectiveness in decreasing the training set size without\ncompromising performance. We also show that our approach can make the\nclassifier more robust in the presence of label noise and class imbalance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 07:38:13 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Kabkab", "Maya", ""], ["Alavi", "Azadeh", ""], ["Chellappa", "Rama", ""]]}, {"id": "1606.04236", "submitter": "Sabrina M\\\"uller", "authors": "Sabrina M\\\"uller, Onur Atan, Mihaela van der Schaar, Anja Klein", "title": "Context-Aware Proactive Content Caching with Service Differentiation in\n  Wireless Networks", "comments": "32 pages, 9 figures, to appear in IEEE Transactions on Wireless\n  Communications, see http://doi.org/10.1109/TWC.2016.2636139", "journal-ref": "IEEE Transactions on Wireless Communications, vol. 16, no. 2, pp.\n  1024-1036, Feb. 2017", "doi": "10.1109/TWC.2016.2636139", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content caching in small base stations or wireless infostations is considered\nto be a suitable approach to improve the efficiency in wireless content\ndelivery. Placing the optimal content into local caches is crucial due to\nstorage limitations, but it requires knowledge about the content popularity\ndistribution, which is often not available in advance. Moreover, local content\npopularity is subject to fluctuations since mobile users with different\ninterests connect to the caching entity over time. Which content a user prefers\nmay depend on the user's context. In this paper, we propose a novel algorithm\nfor context-aware proactive caching. The algorithm learns context-specific\ncontent popularity online by regularly observing context information of\nconnected users, updating the cache content and observing cache hits\nsubsequently. We derive a sublinear regret bound, which characterizes the\nlearning speed and proves that our algorithm converges to the optimal cache\ncontent placement strategy in terms of maximizing the number of cache hits.\nFurthermore, our algorithm supports service differentiation by allowing\noperators of caching entities to prioritize customer groups. Our numerical\nresults confirm that our algorithm outperforms state-of-the-art algorithms in a\nreal world data set, with an increase in the number of cache hits of at least\n14%.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 07:53:47 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 15:03:53 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["M\u00fcller", "Sabrina", ""], ["Atan", "Onur", ""], ["van der Schaar", "Mihaela", ""], ["Klein", "Anja", ""]]}, {"id": "1606.04268", "submitter": "Or Yair", "authors": "Or Yair, Ronen Talmon", "title": "Local Canonical Correlation Analysis for Nonlinear Common Variables\n  Discovery", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2628348", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of hidden common variables discovery\nfrom multimodal data sets of nonlinear high-dimensional observations. We\npresent a metric based on local applications of canonical correlation analysis\n(CCA) and incorporate it in a kernel-based manifold learning technique.We show\nthat this metric discovers the hidden common variables underlying the\nmultimodal observations by estimating the Euclidean distance between them. Our\napproach can be viewed both as an extension of CCA to a nonlinear setting as\nwell as an extension of manifold learning to multiple data sets. Experimental\nresults show that our method indeed discovers the common variables underlying\nhigh-dimensional nonlinear observations without assuming prior rigid model\nassumptions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:18:46 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Yair", "Or", ""], ["Talmon", "Ronen", ""]]}, {"id": "1606.04269", "submitter": "Alasdair Thomason", "authors": "Alasdair Thomason, Nathan Griffiths, Victor Sanchez", "title": "Context Trees: Augmenting Geospatial Trajectories with Context", "comments": null, "journal-ref": "ACM Transactions on Information Systems 2016 35(2) 14:1-14:37", "doi": "10.1145/2978578", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposing latent knowledge in geospatial trajectories has the potential to\nprovide a better understanding of the movements of individuals and groups.\nMotivated by such a desire, this work presents the context tree, a new\nhierarchical data structure that summarises the context behind user actions in\na single model. We propose a method for context tree construction that augments\ngeospatial trajectories with land usage data to identify such contexts. Through\nevaluation of the construction method and analysis of the properties of\ngenerated context trees, we demonstrate the foundation for understanding and\nmodelling behaviour afforded. Summarising user contexts into a single data\nstructure gives easy access to information that would otherwise remain latent,\nproviding the basis for better understanding and predicting the actions and\nbehaviours of individuals and groups. Finally, we also present a method for\npruning context trees, for use in applications where it is desirable to reduce\nthe size of the tree while retaining useful information.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:27:29 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Thomason", "Alasdair", ""], ["Griffiths", "Nathan", ""], ["Sanchez", "Victor", ""]]}, {"id": "1606.04275", "submitter": "Michiel Stock", "authors": "Michiel Stock and Tapio Pahikkala and Antti Airola and Bernard De\n  Baets and Willem Waegeman", "title": "Efficient Pairwise Learning Using Kernel Ridge Regression: an Exact\n  Two-Step Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise learning or dyadic prediction concerns the prediction of properties\nfor pairs of objects. It can be seen as an umbrella covering various machine\nlearning problems such as matrix completion, collaborative filtering,\nmulti-task learning, transfer learning, network prediction and zero-shot\nlearning. In this work we analyze kernel-based methods for pairwise learning,\nwith a particular focus on a recently-suggested two-step method. We show that\nthis method offers an appealing alternative for commonly-applied\nKronecker-based methods that model dyads by means of pairwise feature\nrepresentations and pairwise kernels. In a series of theoretical results, we\nestablish correspondences between the two types of methods in terms of linear\nalgebra and spectral filtering, and we analyze their statistical consistency.\nIn addition, the two-step method allows us to establish novel algorithmic\nshortcuts for efficient training and validation on very large datasets. Putting\nthose properties together, we believe that this simple, yet powerful method can\nbecome a standard tool for many problems. Extensive experimental results for a\nrange of practical settings are reported.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:38:18 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Stock", "Michiel", ""], ["Pahikkala", "Tapio", ""], ["Airola", "Antti", ""], ["De Baets", "Bernard", ""], ["Waegeman", "Willem", ""]]}, {"id": "1606.04278", "submitter": "Michiel Stock", "authors": "Michiel Stock and Krzysztof Dembczynski and Bernard De Baets and\n  Willem Waegeman", "title": "Exact and efficient top-K inference for multi-target prediction by\n  querying separable linear relational models", "comments": null, "journal-ref": "Data Min Knowl Disc (2016) 30:1370-1394", "doi": "10.1007/s10618-016-0456-z", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex multi-target prediction problems that concern large target\nspaces are characterised by a need for efficient prediction strategies that\navoid the computation of predictions for all targets explicitly. Examples of\nsuch problems emerge in several subfields of machine learning, such as\ncollaborative filtering, multi-label classification, dyadic prediction and\nbiological network inference. In this article we analyse efficient and exact\nalgorithms for computing the top-$K$ predictions in the above problem settings,\nusing a general class of models that we refer to as separable linear relational\nmodels. We show how to use those inference algorithms, which are modifications\nof well-known information retrieval methods, in a variety of machine learning\nsettings. Furthermore, we study the possibility of scoring items incompletely,\nwhile still retaining an exact top-K retrieval. Experimental results in several\napplication domains reveal that the so-called threshold algorithm is very\nscalable, performing often many orders of magnitude more efficiently than the\nnaive approach.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:41:27 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Stock", "Michiel", ""], ["Dembczynski", "Krzysztof", ""], ["De Baets", "Bernard", ""], ["Waegeman", "Willem", ""]]}, {"id": "1606.04289", "submitter": "Dimitrios Alikaniotis", "authors": "Dimitrios Alikaniotis and Helen Yannakoudakis and Marek Rei", "title": "Automatic Text Scoring Using Neural Networks", "comments": "11 pages, 3 figures, 2 tables, ACL-2016", "journal-ref": null, "doi": "10.18653/v1/P16-1068", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Text Scoring (ATS) provides a cost-effective and consistent\nalternative to human marking. However, in order to achieve good performance,\nthe predictive features of the system need to be manually engineered by human\nexperts. We introduce a model that forms word representations by learning the\nextent to which specific words contribute to the text's score. Using Long-Short\nTerm Memory networks to represent the meaning of texts, we demonstrate that a\nfully automated framework is able to achieve excellent results over similar\napproaches. In an attempt to make our results more interpretable, and inspired\nby recent advances in visualizing neural networks, we introduce a novel method\nfor identifying the regions of the text that the model has found more\ndiscriminative.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 10:17:27 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 16:30:33 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Alikaniotis", "Dimitrios", ""], ["Yannakoudakis", "Helen", ""], ["Rei", "Marek", ""]]}, {"id": "1606.04316", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli, Giorgio Corani, Janez Demsar, Marco Zaffalon", "title": "Time for a change: a tutorial for comparing multiple classifiers through\n  Bayesian analysis", "comments": "This paper has been published in the Journal of Machine Learning\n  Research (JMLR) vol.18, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning community adopted the use of null hypothesis\nsignificance testing (NHST) in order to ensure the statistical validity of\nresults. Many scientific fields however realized the shortcomings of\nfrequentist reasoning and in the most radical cases even banned its use in\npublications. We should do the same: just as we have embraced the Bayesian\nparadigm in the development of new machine learning methods, so we should also\nuse it in the analysis of our own results. We argue for abandonment of NHST by\nexposing its fallacies and, more importantly, offer better - more sound and\nuseful - alternatives for it.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 11:35:35 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 11:16:35 GMT"}, {"version": "v3", "created": "Sat, 15 Jul 2017 15:16:48 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Benavoli", "Alessio", ""], ["Corani", "Giorgio", ""], ["Demsar", "Janez", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1606.04317", "submitter": "David van Leeuwen", "authors": "David A. van Leeuwen and Joost van Doremalen", "title": "Calibration of Phone Likelihoods in Automatic Speech Recognition", "comments": "Rejected by Interspeech 2016. I would love to include the reviews,\n  but there is no space for that here (400 characters)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the probabilistic properties of the posteriors in a\nspeech recognition system that uses a deep neural network (DNN) for acoustic\nmodeling. We do this by reducing Kaldi's DNN shared pdf-id posteriors to phone\nlikelihoods, and using test set forced alignments to evaluate these using a\ncalibration sensitive metric. Individual frame posteriors are in principle\nwell-calibrated, because the DNN is trained using cross entropy as the\nobjective function, which is a proper scoring rule. When entire phones are\nassessed, we observe that it is best to average the log likelihoods over the\nduration of the phone. Further scaling of the average log likelihoods by the\nlogarithm of the duration slightly improves the calibration, and this\nimprovement is retained when tested on independent test data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 11:44:31 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["van Leeuwen", "David A.", ""], ["van Doremalen", "Joost", ""]]}, {"id": "1606.04351", "submitter": "Georgios Balikas", "authors": "Georgios Balikas, Massih-Reza Amini", "title": "TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the participation of the team \"TwiSE\" in the SemEval\n2016 challenge. Specifically, we participated in Task 4, namely \"Sentiment\nAnalysis in Twitter\" for which we implemented sentiment classification systems\nfor subtasks A, B, C and D. Our approach consists of two steps. In the first\nstep, we generate and validate diverse feature sets for twitter sentiment\nevaluation, inspired by the work of participants of previous editions of such\nchallenges. In the second step, we focus on the optimization of the evaluation\nmeasures of the different subtasks. To this end, we examine different learning\nstrategies by validating them on the data provided by the task organisers. For\nour final submissions we used an ensemble learning approach (stacked\ngeneralization) for Subtask A and single linear models for the rest of the\nsubtasks. In the official leaderboard we were ranked 9/35, 8/19, 1/11 and 2/14\nfor subtasks A, B, C and D respectively.\\footnote{We make the code available\nfor research purposes at\n\\url{https://github.com/balikasg/SemEval2016-Twitter\\_Sentiment\\_Evaluation}.}\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 13:36:00 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Balikas", "Georgios", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1606.04393", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Akshaya Mishra, and Alexander Wong", "title": "Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural\n  Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking inspiration from biological evolution, we explore the idea of \"Can\ndeep neural networks evolve naturally over successive generations into highly\nefficient deep neural networks?\" by introducing the notion of synthesizing new\nhighly efficient, yet powerful deep neural networks over successive generations\nvia an evolutionary process from ancestor deep neural networks. The\narchitectural traits of ancestor deep neural networks are encoded using\nsynaptic probability models, which can be viewed as the `DNA' of these\nnetworks. New descendant networks with differing network architectures are\nsynthesized based on these synaptic probability models from the ancestor\nnetworks and computational environmental factor models, in a random manner to\nmimic heredity, natural selection, and random mutation. These offspring\nnetworks are then trained into fully functional networks, like one would train\na newborn, and have more efficient, more diverse network architectures than\ntheir ancestor networks, while achieving powerful modeling capabilities.\nExperimental results for the task of visual saliency demonstrated that the\nsynthesized `evolved' offspring networks can achieve state-of-the-art\nperformance while having network architectures that are significantly more\nefficient (with a staggering $\\sim$48-fold decrease in synapses by the fourth\ngeneration) compared to the original ancestor network.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 14:36:55 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 16:15:40 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 22:51:33 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Mishra", "Akshaya", ""], ["Wong", "Alexander", ""]]}, {"id": "1606.04414", "submitter": "Jian Wu", "authors": "Jian Wu, Peter I. Frazier", "title": "The Parallel Knowledge Gradient Method for Batch Bayesian Optimization", "comments": "Minor edits and typo fixes. Please cite \"J. Wu and P. Frazier. The\n  parallel knowledge gradient method for batch bayesian optimization. In\n  Advances In Neural Information Processing Systems, pp. 3126-3134. 2016\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of black-box optimization, one can evaluate multiple\npoints simultaneously, e.g. when evaluating the performances of several\ndifferent neural network architectures in a parallel computing environment. In\nthis paper, we develop a novel batch Bayesian optimization algorithm --- the\nparallel knowledge gradient method. By construction, this method provides the\none-step Bayes-optimal batch of points to sample. We provide an efficient\nstrategy for computing this Bayes-optimal batch of points, and we demonstrate\nthat the parallel knowledge gradient method finds global optima significantly\nfaster than previous batch Bayesian optimization algorithms on both synthetic\ntest functions and when tuning hyperparameters of practical machine learning\nalgorithms, especially when function evaluations are noisy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 15:12:01 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 05:47:09 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2017 20:49:18 GMT"}, {"version": "v4", "created": "Sun, 22 Apr 2018 23:44:15 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wu", "Jian", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1606.04422", "submitter": "Artur Garcez", "authors": "Luciano Serafini and Artur d'Avila Garcez", "title": "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and\n  Knowledge", "comments": "12 pages, 2 figs, 1 table, 27 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Logic Tensor Networks: a uniform framework for integrating\nautomatic learning and reasoning. A logic formalism called Real Logic is\ndefined on a first-order language whereby formulas have truth-value in the\ninterval [0,1] and semantics defined concretely on the domain of real numbers.\nLogical constants are interpreted as feature vectors of real numbers. Real\nLogic promotes a well-founded integration of deductive reasoning on a\nknowledge-base and efficient data-driven relational machine learning. We show\nhow Real Logic can be implemented in deep Tensor Neural Networks with the use\nof Google's tensorflow primitives. The paper concludes with experiments\napplying Logic Tensor Networks on a simple but representative example of\nknowledge completion.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 15:25:28 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 12:28:57 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Serafini", "Luciano", ""], ["Garcez", "Artur d'Avila", ""]]}, {"id": "1606.04435", "submitter": "Kathrin Grosse", "authors": "Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes,\n  Patrick McDaniel", "title": "Adversarial Perturbations Against Deep Neural Networks for Malware\n  Classification", "comments": "version update: correcting typos, incorporating external feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks, like many other machine learning models, have recently\nbeen shown to lack robustness against adversarially crafted inputs. These\ninputs are derived from regular inputs by minor yet carefully selected\nperturbations that deceive machine learning models into desired\nmisclassifications. Existing work in this emerging field was largely specific\nto the domain of image classification, since the high-entropy of images can be\nconveniently manipulated without changing the images' overall visual\nappearance. Yet, it remains unclear how such attacks translate to more\nsecurity-sensitive applications such as malware detection - which may pose\nsignificant challenges in sample generation and arguably grave consequences for\nfailure.\n  In this paper, we show how to construct highly-effective adversarial sample\ncrafting attacks for neural networks used as malware classifiers. The\napplication domain of malware classification introduces additional constraints\nin the adversarial sample crafting problem when compared to the computer vision\ndomain: (i) continuous, differentiable input domains are replaced by discrete,\noften binary inputs; and (ii) the loose condition of leaving visual appearance\nunchanged is replaced by requiring equivalent functional behavior. We\ndemonstrate the feasibility of these attacks on many different instances of\nmalware classifiers that we trained using the DREBIN Android malware data set.\nWe furthermore evaluate to which extent potential defensive mechanisms against\nadversarial crafting can be leveraged to the setting of malware classification.\nWhile feature reduction did not prove to have a positive impact, distillation\nand re-training on adversarially crafted samples show promising results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:01:52 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 08:14:12 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Grosse", "Kathrin", ""], ["Papernot", "Nicolas", ""], ["Manoharan", "Praveen", ""], ["Backes", "Michael", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1606.04442", "submitter": "Christian Szegedy", "authors": "Alex A. Alemi, Francois Chollet, Niklas Een, Geoffrey Irving,\n  Christian Szegedy and Josef Urban", "title": "DeepMath - Deep Sequence Models for Premise Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effectiveness of neural sequence models for premise selection in\nautomated theorem proving, one of the main bottlenecks in the formalization of\nmathematics. We propose a two stage approach for this task that yields good\nresults for the premise selection task on the Mizar corpus while avoiding the\nhand-engineered features of existing state-of-the-art models. To our knowledge,\nthis is the first time deep learning has been applied to theorem proving on a\nlarge scale.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:27:41 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 19:35:16 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Alemi", "Alex A.", ""], ["Chollet", "Francois", ""], ["Een", "Niklas", ""], ["Irving", "Geoffrey", ""], ["Szegedy", "Christian", ""], ["Urban", "Josef", ""]]}, {"id": "1606.04443", "submitter": "Steve Li", "authors": "Steven Cheng-Xian Li, Benjamin Marlin", "title": "A scalable end-to-end Gaussian process adapter for irregularly sampled\n  time series classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for classification of sparse and\nirregularly-sampled time series. The properties of such time series can result\nin substantial uncertainty about the values of the underlying temporal\nprocesses, while making the data difficult to deal with using standard\nclassification methods that assume fixed-dimensional feature spaces. To address\nthese challenges, we propose an uncertainty-aware classification framework\nbased on a special computational layer we refer to as the Gaussian process\nadapter that can connect irregularly sampled time series data to any black-box\nclassifier learnable using gradient descent. We show how to scale up the\nrequired computations based on combining the structured kernel interpolation\nframework and the Lanczos approximation method, and how to discriminatively\ntrain the Gaussian process adapter in combination with a number of classifiers\nend-to-end using backpropagation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:31:14 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 20:05:02 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Li", "Steven Cheng-Xian", ""], ["Marlin", "Benjamin", ""]]}, {"id": "1606.04449", "submitter": "Xi-Lin Li", "authors": "Xi-Lin Li", "title": "Recurrent neural network training with preconditioned stochastic\n  gradient descent", "comments": "Supplemental materials including Matlab code are put at\n  https://sites.google.com/site/lixilinx/home/psgd", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the performance of a recently proposed preconditioned\nstochastic gradient descent (PSGD) algorithm on recurrent neural network (RNN)\ntraining. PSGD adaptively estimates a preconditioner to accelerate gradient\ndescent, and is designed to be simple, general and easy to use, as stochastic\ngradient descent (SGD). RNNs, especially the ones requiring extremely long term\nmemories, are difficult to train. We have tested PSGD on a set of synthetic\npathological RNN learning problems and the real world MNIST handwritten digit\nrecognition task. Experimental results suggest that PSGD is able to achieve\nhighly competitive performance without using any trick like preprocessing,\npretraining or parameter tweaking.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:40:38 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 18:56:38 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Li", "Xi-Lin", ""]]}, {"id": "1606.04460", "submitter": "Charles Blundell", "authors": "Charles Blundell and Benigno Uria and Alexander Pritzel and Yazhe Li\n  and Avraham Ruderman and Joel Z Leibo and Jack Rae and Daan Wierstra and\n  Demis Hassabis", "title": "Model-Free Episodic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art deep reinforcement learning algorithms take many millions of\ninteractions to attain human-level performance. Humans, on the other hand, can\nvery quickly exploit highly rewarding nuances of an environment upon first\ndiscovery. In the brain, such rapid learning is thought to depend on the\nhippocampus and its capacity for episodic memory. Here we investigate whether a\nsimple model of hippocampal episodic control can learn to solve difficult\nsequential decision-making tasks. We demonstrate that it not only attains a\nhighly rewarding strategy significantly faster than state-of-the-art deep\nreinforcement learning algorithms, but also achieves a higher overall reward on\nsome of the more challenging domains.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 17:03:46 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Blundell", "Charles", ""], ["Uria", "Benigno", ""], ["Pritzel", "Alexander", ""], ["Li", "Yazhe", ""], ["Ruderman", "Avraham", ""], ["Leibo", "Joel Z", ""], ["Rae", "Jack", ""], ["Wierstra", "Daan", ""], ["Hassabis", "Demis", ""]]}, {"id": "1606.04474", "submitter": "Matthew W. Hoffman", "authors": "Marcin Andrychowicz and Misha Denil and Sergio Gomez and Matthew W.\n  Hoffman and David Pfau and Tom Schaul and Brendan Shillingford and Nando de\n  Freitas", "title": "Learning to learn by gradient descent by gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The move from hand-designed features to learned features in machine learning\nhas been wildly successful. In spite of this, optimization algorithms are still\ndesigned by hand. In this paper we show how the design of an optimization\nalgorithm can be cast as a learning problem, allowing the algorithm to learn to\nexploit structure in the problems of interest in an automatic way. Our learned\nalgorithms, implemented by LSTMs, outperform generic, hand-designed competitors\non the tasks for which they are trained, and also generalize well to new tasks\nwith similar structure. We demonstrate this on a number of tasks, including\nsimple convex problems, training neural networks, and styling images with\nneural art.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 17:49:32 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 16:45:45 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Andrychowicz", "Marcin", ""], ["Denil", "Misha", ""], ["Gomez", "Sergio", ""], ["Hoffman", "Matthew W.", ""], ["Pfau", "David", ""], ["Schaul", "Tom", ""], ["Shillingford", "Brendan", ""], ["de Freitas", "Nando", ""]]}, {"id": "1606.04487", "submitter": "Stefan Hadjis", "authors": "Stefan Hadjis, Ce Zhang, Ioannis Mitliagkas, Dan Iter, Christopher\n  R\\'e", "title": "Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the factors affecting training time in multi-device deep learning\nsystems. Given a specification of a convolutional neural network, our goal is\nto minimize the time to train this model on a cluster of commodity CPUs and\nGPUs. We first focus on the single-node setting and show that by using standard\nbatching and data-parallel techniques, throughput can be improved by at least\n5.5x over state-of-the-art systems on CPUs. This ensures an end-to-end training\nspeed directly proportional to the throughput of a device regardless of its\nunderlying hardware, allowing each node in the cluster to be treated as a black\nbox. Our second contribution is a theoretical and empirical study of the\ntradeoffs affecting end-to-end training time in a multiple-device setting. We\nidentify the degree of asynchronous parallelization as a key factor affecting\nboth hardware and statistical efficiency. We see that asynchrony can be viewed\nas introducing a momentum term. Our results imply that tuning momentum is\ncritical in asynchronous parallel configurations, and suggest that published\nresults that have not been fully tuned might report suboptimal performance for\nsome configurations. For our third contribution, we use our novel understanding\nof the interaction between system and optimization dynamics to provide an\nefficient hyperparameter optimizer. Our optimizer involves a predictive model\nfor the total time to convergence and selects an allocation of resources to\nminimize that time. We demonstrate that the most popular distributed deep\nlearning systems fall within our tradeoff space, but do not optimize within the\nspace. By doing this optimization, our prototype runs 1.9x to 12x faster than\nthe fastest state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 18:21:04 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 00:05:49 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 13:04:00 GMT"}, {"version": "v4", "created": "Wed, 19 Oct 2016 04:26:03 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Hadjis", "Stefan", ""], ["Zhang", "Ce", ""], ["Mitliagkas", "Ioannis", ""], ["Iter", "Dan", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1606.04506", "submitter": "Yamuna Prasad", "authors": "Yamuna Prasad, Dinesh Khandelwal, K. K. Biswas", "title": "Max-Margin Feature Selection", "comments": "submitted to PR Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications such as in vision, biology and social\nnetworking deal with data in high dimensions. Feature selection is typically\nemployed to select a subset of features which im- proves generalization\naccuracy as well as reduces the computational cost of learning the model. One\nof the criteria used for feature selection is to jointly minimize the\nredundancy and maximize the rele- vance of the selected features. In this\npaper, we formulate the task of feature selection as a one class SVM problem in\na space where features correspond to the data points and instances correspond\nto the dimensions. The goal is to look for a representative subset of the\nfeatures (support vectors) which describes the boundary for the region where\nthe set of the features (data points) exists. This leads to a joint\noptimization of relevance and redundancy in a principled max-margin framework.\nAdditionally, our formulation enables us to leverage existing techniques for\noptimizing the SVM objective resulting in highly computationally efficient\nsolutions for the task of feature selection. Specifically, we employ the dual\ncoordinate descent algorithm (Hsieh et al., 2008), originally proposed for\nSVMs, for our formulation. We use a sparse representation to deal with data in\nvery high dimensions. Experiments on seven publicly available benchmark\ndatasets from a variety of domains show that our approach results in orders of\nmagnitude faster solutions even while retaining the same level of accuracy\ncompared to the state of the art feature selection techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 19:05:01 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Prasad", "Yamuna", ""], ["Khandelwal", "Dinesh", ""], ["Biswas", "K. K.", ""]]}, {"id": "1606.04518", "submitter": "Haoqi Li", "authors": "Haoqi Li, Brian Baucom, Panayiotis Georgiou", "title": "Sparsely Connected and Disjointly Trained Deep Neural Networks for Low\n  Resource Behavioral Annotation: Acoustic Classification in Couples' Therapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational studies are based on accurate assessment of human state. A\nbehavior recognition system that models interlocutors' state in real-time can\nsignificantly aid the mental health domain. However, behavior recognition from\nspeech remains a challenging task since it is difficult to find generalizable\nand representative features because of noisy and high-dimensional data,\nespecially when data is limited and annotated coarsely and subjectively. Deep\nNeural Networks (DNN) have shown promise in a wide range of machine learning\ntasks, but for Behavioral Signal Processing (BSP) tasks their application has\nbeen constrained due to limited quantity of data. We propose a\nSparsely-Connected and Disjointly-Trained DNN (SD-DNN) framework to deal with\nlimited data. First, we break the acoustic feature set into subsets and train\nmultiple distinct classifiers. Then, the hidden layers of these classifiers\nbecome parts of a deeper network that integrates all feature streams. The\noverall system allows for full connectivity while limiting the number of\nparameters trained at any time and allows convergence possible with even\nlimited data. We present results on multiple behavior codes in the couples'\ntherapy domain and demonstrate the benefits in behavior classification\naccuracy. We also show the viability of this system towards live behavior\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 19:32:34 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Li", "Haoqi", ""], ["Baucom", "Brian", ""], ["Georgiou", "Panayiotis", ""]]}, {"id": "1606.04521", "submitter": "Ewout van den Berg", "authors": "Ewout van den Berg, Bhuvana Ramabhadran, Michael Picheny", "title": "Training variance and performance evaluation of neural networks in\n  speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study variance in the results of neural network training on a\nwide variety of configurations in automatic speech recognition. Although this\nvariance itself is well known, this is, to the best of our knowledge, the first\npaper that performs an extensive empirical study on its effects in speech\nrecognition. We view training as sampling from a distribution and show that\nthese distributions can have a substantial variance. These results show the\nurgent need to rethink the way in which results in the literature are reported\nand interpreted.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 19:39:41 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Berg", "Ewout van den", ""], ["Ramabhadran", "Bhuvana", ""], ["Picheny", "Michael", ""]]}, {"id": "1606.04552", "submitter": "Harish Sethu", "authors": "Tingshan Huang, Harish Sethu and Nagarajan Kandasamy", "title": "A New Approach to Dimensionality Reduction for Anomaly Detection in Data\n  Traffic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monitoring and management of high-volume feature-rich traffic in large\nnetworks offers significant challenges in storage, transmission and\ncomputational costs. The predominant approach to reducing these costs is based\non performing a linear mapping of the data to a low-dimensional subspace such\nthat a certain large percentage of the variance in the data is preserved in the\nlow-dimensional representation. This variance-based subspace approach to\ndimensionality reduction forces a fixed choice of the number of dimensions, is\nnot responsive to real-time shifts in observed traffic patterns, and is\nvulnerable to normal traffic spoofing. Based on theoretical insights proved in\nthis paper, we propose a new distance-based approach to dimensionality\nreduction motivated by the fact that the real-time structural differences\nbetween the covariance matrices of the observed and the normal traffic is more\nrelevant to anomaly detection than the structure of the training data alone.\nOur approach, called the distance-based subspace method, allows a different\nnumber of reduced dimensions in different time windows and arrives at only the\nnumber of dimensions necessary for effective anomaly detection. We present\ncentralized and distributed versions of our algorithm and, using simulation on\nreal traffic traces, demonstrate the qualitative and quantitative advantages of\nthe distance-based subspace approach.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 20:29:50 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Huang", "Tingshan", ""], ["Sethu", "Harish", ""], ["Kandasamy", "Nagarajan", ""]]}, {"id": "1606.04561", "submitter": "Amir Ahooye Atashin", "authors": "Amir Ahooye Atashin, Parsa Bagherzadeh, Kamaledin Ghiasi-Shirazi", "title": "A two-stage learning method for protein-protein interaction prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new method for PPI (proteinprotein interaction) prediction\nis proposed. In PPI prediction, a reliable and sufficient number of training\nsamples is not available, but a large number of unlabeled samples is in hand.\nIn the proposed method, the denoising auto encoders are employed for learning\nrobust features. The obtained robust features are used in order to train a\nclassifier with a better performance. The experimental results demonstrate the\ncapabilities of the proposed method.\n  Protein-protein interaction; Denoising auto encoder;Robust features;\nUnlabelled data;\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 20:49:22 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 16:04:09 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Atashin", "Amir Ahooye", ""], ["Bagherzadeh", "Parsa", ""], ["Ghiasi-Shirazi", "Kamaledin", ""]]}, {"id": "1606.04615", "submitter": "Ishan Durugkar", "authors": "Ishan P. Durugkar, Clemens Rosenbaum, Stefan Dernbach, Sridhar\n  Mahadevan", "title": "Deep Reinforcement Learning With Macro-Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has been shown to be a powerful framework for\nlearning policies from complex high-dimensional sensory inputs to actions in\ncomplex tasks, such as the Atari domain. In this paper, we explore output\nrepresentation modeling in the form of temporal abstraction to improve\nconvergence and reliability of deep reinforcement learning approaches. We\nconcentrate on macro-actions, and evaluate these on different Atari 2600 games,\nwhere we show that they yield significant improvements in learning speed.\nAdditionally, we show that they can even achieve better scores than DQN. We\noffer analysis and explanation for both convergence and final results,\nrevealing a problem deep RL approaches have with sparse reward signals.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 01:57:40 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Durugkar", "Ishan P.", ""], ["Rosenbaum", "Clemens", ""], ["Dernbach", "Stefan", ""], ["Mahadevan", "Sridhar", ""]]}, {"id": "1606.04618", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi and Marco F. Duarte", "title": "Masking Strategies for Image Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting an optimal mask for an image manifold,\ni.e., choosing a subset of the pixels of the image that preserves the\nmanifold's geometric structure present in the original data. Such masking\nimplements a form of compressive sensing through emerging imaging sensor\nplatforms for which the power expense grows with the number of pixels acquired.\nOur goal is for the manifold learned from masked images to resemble its full\nimage counterpart as closely as possible. More precisely, we show that one can\nindeed accurately learn an image manifold without having to consider a large\nmajority of the image pixels. In doing so, we consider two masking methods that\npreserve the local and global geometric structure of the manifold,\nrespectively. In each case, the process of finding the optimal masking pattern\ncan be cast as a binary integer program, which is computationally expensive but\ncan be approximated by a fast greedy algorithm. Numerical experiments show that\nthe relevant manifold structure is preserved through the data-dependent masking\nprocess, even for modest mask sizes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 02:03:05 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Duarte", "Marco F.", ""]]}, {"id": "1606.04624", "submitter": "Yingfei Wang", "authors": "Yingfei Wang and Warren Powell", "title": "Finite-time Analysis for the Knowledge-Gradient Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sequential decision problems in which we adaptively choose one of\nfinitely many alternatives and observe a stochastic reward. We offer a new\nperspective of interpreting Bayesian ranking and selection problems as adaptive\nstochastic multi-set maximization problems and derive the first finite-time\nbound of the knowledge-gradient policy for adaptive submodular objective\nfunctions. In addition, we introduce the concept of prior-optimality and\nprovide another insight into the performance of the knowledge gradient policy\nbased on the submodular assumption on the value of information. We demonstrate\nsubmodularity for the two-alternative case and provide other conditions for\nmore general problems, bringing out the issue and importance of submodularity\nin learning problems. Empirical experiments are conducted to further illustrate\nthe finite time behavior of the knowledge gradient policy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 02:44:02 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Wang", "Yingfei", ""], ["Powell", "Warren", ""]]}, {"id": "1606.04646", "submitter": "Jianshu Chen", "authors": "Jianshu Chen, Po-Sen Huang, Xiaodong He, Jianfeng Gao and Li Deng", "title": "Unsupervised Learning of Predictors from Unpaired Input-Output Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning is the most challenging problem in machine learning and\nespecially in deep learning. Among many scenarios, we study an unsupervised\nlearning problem of high economic value --- learning to predict without costly\npairing of input data and corresponding labels. Part of the difficulty in this\nproblem is a lack of solid evaluation measures. In this paper, we take a\npractical approach to grounding unsupervised learning by using the same success\ncriterion as for supervised learning in prediction tasks but we do not require\nthe presence of paired input-output training data. In particular, we propose an\nobjective function that aims to make the predicted outputs fit well the\nstructure of the output while preserving the correlation between the input and\nthe predicted output. We experiment with a synthetic structural prediction\nproblem and show that even with simple linear classifiers, the objective\nfunction is already highly non-convex. We further demonstrate the nature of\nthis non-convex optimization problem as well as potential solutions. In\nparticular, we show that with regularization via a generative model, learning\nwith the proposed unsupervised objective function converges to an optimal\nsolution.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 05:26:29 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Chen", "Jianshu", ""], ["Huang", "Po-Sen", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1606.04671", "submitter": "Andrei Rusu", "authors": "Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert\n  Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell", "title": "Progressive Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to solve complex sequences of tasks--while both leveraging transfer\nand avoiding catastrophic forgetting--remains a key obstacle to achieving\nhuman-level intelligence. The progressive networks approach represents a step\nforward in this direction: they are immune to forgetting and can leverage prior\nknowledge via lateral connections to previously learned features. We evaluate\nthis architecture extensively on a wide variety of reinforcement learning tasks\n(Atari and 3D maze games), and show that it outperforms common baselines based\non pretraining and finetuning. Using a novel sensitivity measure, we\ndemonstrate that transfer occurs at both low-level sensory and high-level\ncontrol layers of the learned policy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 08:20:51 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 22:03:05 GMT"}, {"version": "v3", "created": "Wed, 7 Sep 2016 10:59:12 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Rusu", "Andrei A.", ""], ["Rabinowitz", "Neil C.", ""], ["Desjardins", "Guillaume", ""], ["Soyer", "Hubert", ""], ["Kirkpatrick", "James", ""], ["Kavukcuoglu", "Koray", ""], ["Pascanu", "Razvan", ""], ["Hadsell", "Raia", ""]]}, {"id": "1606.04695", "submitter": "Alexander Vezhnevets", "authors": "Alexander (Sasha) Vezhnevets, Volodymyr Mnih, John Agapiou, Simon\n  Osindero, Alex Graves, Oriol Vinyals, Koray Kavukcuoglu", "title": "Strategic Attentive Writer for Learning Macro-Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep recurrent neural network architecture that learns to\nbuild implicit plans in an end-to-end manner by purely interacting with an\nenvironment in reinforcement learning setting. The network builds an internal\nplan, which is continuously updated upon observation of the next input from the\nenvironment. It can also partition this internal representation into contiguous\nsub- sequences by learning for how long the plan can be committed to - i.e.\nfollowed without re-planing. Combining these properties, the proposed model,\ndubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally\nabstracted macro- actions of varying lengths that are solely learnt from data\nwithout any prior information. These macro-actions enable both structured\nexploration and economic computation. We experimentally demonstrate that STRAW\ndelivers strong improvements on several ATARI games by employing temporally\nextended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same\ntime a general algorithm that can be applied on any sequence data. To that end,\nwe also show that when trained on text prediction task, STRAW naturally\npredicts frequent n-grams (instead of macro-actions), demonstrating the\ngenerality of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 09:28:52 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Alexander", "", "", "Sasha"], ["Vezhnevets", "", ""], ["Mnih", "Volodymyr", ""], ["Agapiou", "John", ""], ["Osindero", "Simon", ""], ["Graves", "Alex", ""], ["Vinyals", "Oriol", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1606.04722", "submitter": "Xi Wu", "authors": "Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, Jeffrey\n  F. Naughton", "title": "Bolt-on Differential Privacy for Scalable Stochastic Gradient\n  Descent-based Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant progress has been made separately on analytics systems for\nscalable stochastic gradient descent (SGD) and private SGD, none of the major\nscalable analytics frameworks have incorporated differentially private SGD.\nThere are two inter-related issues for this disconnect between research and\npractice: (1) low model accuracy due to added noise to guarantee privacy, and\n(2) high development and runtime overhead of the private algorithms. This paper\ntakes a first step to remedy this disconnect and proposes a private SGD\nalgorithm to address \\emph{both} issues in an integrated manner. In contrast to\nthe white-box approach adopted by previous work, we revisit and use the\nclassical technique of {\\em output perturbation} to devise a novel \"bolt-on\"\napproach to private SGD. While our approach trivially addresses (2), it makes\n(1) even more challenging. We address this challenge by providing a novel\nanalysis of the $L_2$-sensitivity of SGD, which allows, under the same privacy\nguarantees, better convergence of SGD when only a constant number of passes can\nbe made over the data. We integrate our algorithm, as well as other\nstate-of-the-art differentially private SGD, into Bismarck, a popular scalable\nSGD-based analytics system on top of an RDBMS. Extensive experiments show that\nour algorithm can be easily integrated, incurs virtually no overhead, scales\nwell, and most importantly, yields substantially better (up to 4X) test\naccuracy than the state-of-the-art algorithms on many real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 11:14:29 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 16:26:59 GMT"}, {"version": "v3", "created": "Thu, 23 Mar 2017 17:35:09 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Wu", "Xi", ""], ["Li", "Fengan", ""], ["Kumar", "Arun", ""], ["Chaudhuri", "Kamalika", ""], ["Jha", "Somesh", ""], ["Naughton", "Jeffrey F.", ""]]}, {"id": "1606.04750", "submitter": "Zhenzhou Wu", "authors": "Zhenzhou Wu, Sunil Sivadas, Yong Kiam Tan, Ma Bin, Rick Siow Mong Goh", "title": "Multi-Modal Hybrid Deep Neural Network for Speech Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNN) have been successful in en- hancing noisy speech\nsignals. Enhancement is achieved by learning a nonlinear mapping function from\nthe features of the corrupted speech signal to that of the reference clean\nspeech signal. The quality of predicted features can be improved by providing\nadditional side channel information that is robust to noise, such as visual\ncues. In this paper we propose a novel deep learning model inspired by insights\nfrom human audio visual perception. In the proposed unified hybrid\narchitecture, features from a Convolution Neural Network (CNN) that processes\nthe visual cues and features from a fully connected DNN that processes the\naudio signal are integrated using a Bidirectional Long Short-Term Memory\n(BiLSTM) network. The parameters of the hybrid model are jointly learned using\nbackpropagation. We compare the quality of enhanced speech from the hybrid\nmodels with those from traditional DNN and BiLSTM models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 13:14:05 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Wu", "Zhenzhou", ""], ["Sivadas", "Sunil", ""], ["Tan", "Yong Kiam", ""], ["Bin", "Ma", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "1606.04753", "submitter": "Matteo Turchetta", "authors": "Matteo Turchetta, Felix Berkenkamp, Andreas Krause", "title": "Safe Exploration in Finite Markov Decision Processes with Gaussian\n  Processes", "comments": "15 pages, extended version with proofs", "journal-ref": "Proc. of Advances in Neural Information Processing Systems (NIPS),\n  2016, pp. 4305-4313", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classical reinforcement learning, when exploring an environment, agents\naccept arbitrary short term loss for long term gain. This is infeasible for\nsafety critical applications, such as robotics, where even a single unsafe\naction may cause system failure. In this paper, we address the problem of\nsafely exploring finite Markov decision processes (MDP). We define safety in\nterms of an, a priori unknown, safety constraint that depends on states and\nactions. We aim to explore the MDP under this constraint, assuming that the\nunknown function satisfies regularity conditions expressed via a Gaussian\nprocess prior. We develop a novel algorithm for this task and prove that it is\nable to completely explore the safely reachable part of the MDP without\nviolating the safety constraint. To achieve this, it cautiously explores safe\nstates and actions in order to gain statistical confidence about the safety of\nunvisited state-action pairs from noisy observations collected while navigating\nthe environment. Moreover, the algorithm explicitly considers reachability when\nexploring the MDP, ensuring that it does not get stuck in any state with no\nsafe way out. We demonstrate our method on digital terrain models for the task\nof exploring an unknown map with a rover.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 13:18:30 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 14:00:11 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Turchetta", "Matteo", ""], ["Berkenkamp", "Felix", ""], ["Krause", "Andreas", ""]]}, {"id": "1606.04778", "submitter": "Rongpeng Li", "authors": "Rongpeng Li, Zhifeng Zhao, Jianchao Zheng, Chengli Mei, Yueming Cai,\n  and Honggang Zhang", "title": "The Learning and Prediction of Application-level Traffic Data in\n  Cellular Networks", "comments": "Accepted by IEEE Transactions on Wireless Communications on March 26,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traffic learning and prediction is at the heart of the evaluation of the\nperformance of telecommunications networks and attracts a lot of attention in\nwired broadband networks. Now, benefiting from the big data in cellular\nnetworks, it becomes possible to make the analyses one step further into the\napplication level. In this paper, we firstly collect a significant amount of\napplication-level traffic data from cellular network operators. Afterwards,\nwith the aid of the traffic \"big data\", we make a comprehensive study over the\nmodeling and prediction framework of cellular network traffic. Our results\nsolidly demonstrate that there universally exist some traffic statistical\nmodeling characteristics, including ALPHA-stable modeled property in the\ntemporal domain and the sparsity in the spatial domain. Meanwhile, the results\nalso demonstrate the distinctions originated from the uniqueness of different\nservice types of applications. Furthermore, we propose a new traffic prediction\nframework to encompass and explore these aforementioned characteristics and\nthen develop a dictionary learning-based alternating direction method to solve\nit. Besides, we validate the prediction accuracy improvement and the robustness\nof the proposed framework through extensive simulation results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 14:13:57 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 03:52:23 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Li", "Rongpeng", ""], ["Zhao", "Zhifeng", ""], ["Zheng", "Jianchao", ""], ["Mei", "Chengli", ""], ["Cai", "Yueming", ""], ["Zhang", "Honggang", ""]]}, {"id": "1606.04801", "submitter": "Kun He Prof.", "authors": "Kun He and Yan Wang and John Hopcroft", "title": "A Powerful Generative Model Using Random Weights for the Deep Image\n  Representation", "comments": "10 pages, 10 figures, submited to NIPS 2016 conference. Computer\n  Vision and Pattern Recognition, Neurons and Cognition, Neural and\n  Evolutionary Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To what extent is the success of deep visualization due to the training?\nCould we do deep visualization using untrained, random weight networks? To\naddress this issue, we explore new and powerful generative models for three\npopular deep visualization tasks using untrained, random weight convolutional\nneural networks. First we invert representations in feature spaces and\nreconstruct images from white noise inputs. The reconstruction quality is\nstatistically higher than that of the same method applied on well trained\nnetworks with the same architecture. Next we synthesize textures using scaled\ncorrelations of representations in multiple layers and our results are almost\nindistinguishable with the original natural texture and the synthesized\ntextures based on the trained network. Third, by recasting the content of an\nimage in the style of various artworks, we create artistic images with high\nperceptual quality, highly competitive to the prior work of Gatys et al. on\npretrained networks. To our knowledge this is the first demonstration of image\nrepresentations using untrained deep neural networks. Our work provides a new\nand fascinating tool to study the representation of deep network architecture\nand sheds light on new understandings on deep visualization.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 14:56:42 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 06:53:55 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["He", "Kun", ""], ["Wang", "Yan", ""], ["Hopcroft", "John", ""]]}, {"id": "1606.04809", "submitter": "R\\'emi Leblond", "authors": "R\\'emi Leblond, Fabian Pedregosa and Simon Lacoste-Julien", "title": "ASAGA: Asynchronous Parallel SAGA", "comments": "Appears in: Proceedings of the 20th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2017), 37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe ASAGA, an asynchronous parallel version of the incremental\ngradient algorithm SAGA that enjoys fast linear convergence rates. Through a\nnovel perspective, we revisit and clarify a subtle but important technical\nissue present in a large fraction of the recent convergence rate proofs for\nasynchronous parallel optimization algorithms, and propose a simplification of\nthe recently introduced \"perturbed iterate\" framework that resolves it. We\nthereby prove that ASAGA can obtain a theoretical linear speedup on multi-core\nsystems even without sparsity assumptions. We present results of an\nimplementation on a 40-core architecture illustrating the practical speedup as\nwell as the hardware overhead.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 15:12:01 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 21:32:53 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 12:38:31 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Leblond", "R\u00e9mi", ""], ["Pedregosa", "Fabian", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1606.04838", "submitter": "Frank E. Curtis", "authors": "L\\'eon Bottou, Frank E. Curtis, Jorge Nocedal", "title": "Optimization Methods for Large-Scale Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a review and commentary on the past, present, and future\nof numerical optimization algorithms in the context of machine learning\napplications. Through case studies on text classification and the training of\ndeep neural networks, we discuss how optimization problems arise in machine\nlearning and what makes them challenging. A major theme of our study is that\nlarge-scale machine learning represents a distinctive setting in which the\nstochastic gradient (SG) method has traditionally played a central role while\nconventional gradient-based nonlinear optimization techniques typically falter.\nBased on this viewpoint, we present a comprehensive theory of a\nstraightforward, yet versatile SG algorithm, discuss its practical behavior,\nand highlight opportunities for designing algorithms with improved performance.\nThis leads to a discussion about the next generation of optimization methods\nfor large-scale machine learning, including an investigation of two main\nstreams of research on techniques that diminish noise in the stochastic\ndirections and methods that make use of second-order derivative approximations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 16:15:53 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 20:08:27 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 20:40:22 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Bottou", "L\u00e9on", ""], ["Curtis", "Frank E.", ""], ["Nocedal", "Jorge", ""]]}, {"id": "1606.04930", "submitter": "Allen Huang", "authors": "Allen Huang, Raymond Wu", "title": "Deep Learning for Music", "comments": "8 pages, Stanford CS224D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to be able to build a generative model from a deep neural network\narchitecture to try to create music that has both harmony and melody and is\npassable as music composed by humans. Previous work in music generation has\nmainly been focused on creating a single melody. More recent work on polyphonic\nmusic modeling, centered around time series probability density estimation, has\nmet some partial success. In particular, there has been a lot of work based off\nof Recurrent Neural Networks combined with Restricted Boltzmann Machines\n(RNN-RBM) and other similar recurrent energy based models. Our approach,\nhowever, is to perform end-to-end learning and generation with deep neural nets\nalone.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 19:38:14 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Huang", "Allen", ""], ["Wu", "Raymond", ""]]}, {"id": "1606.04934", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya\n  Sutskever and Max Welling", "title": "Improving Variational Inference with Inverse Autoregressive Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of normalizing flows provides a general strategy for flexible\nvariational inference of posteriors over latent variables. We propose a new\ntype of normalizing flow, inverse autoregressive flow (IAF), that, in contrast\nto earlier published flows, scales well to high-dimensional latent spaces. The\nproposed flow consists of a chain of invertible transformations, where each\ntransformation is based on an autoregressive neural network. In experiments, we\nshow that IAF significantly improves upon diagonal Gaussian approximate\nposteriors. In addition, we demonstrate that a novel type of variational\nautoencoder, coupled with IAF, is competitive with neural autoregressive models\nin terms of attained log-likelihood on natural images, while allowing\nsignificantly faster synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 19:46:36 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 20:36:01 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Kingma", "Diederik P.", ""], ["Salimans", "Tim", ""], ["Jozefowicz", "Rafal", ""], ["Chen", "Xi", ""], ["Sutskever", "Ilya", ""], ["Welling", "Max", ""]]}, {"id": "1606.04985", "submitter": "Yanwei Cui", "authors": "Yanwei Cui, Laetitia Chapel, S\\'ebastien Lef\\`evre", "title": "Combining multiscale features for classification of hyperspectral\n  images: a sequence based kernel approach", "comments": "8th IEEE GRSS Workshop on Hyperspectral Image and Signal Processing:\n  Evolution in Remote Sensing (WHISPERS 2016), UCLA in Los Angeles, California,\n  U.S", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, hyperspectral image classification widely copes with spatial\ninformation to improve accuracy. One of the most popular way to integrate such\ninformation is to extract hierarchical features from a multiscale segmentation.\nIn the classification context, the extracted features are commonly concatenated\ninto a long vector (also called stacked vector), on which is applied a\nconventional vector-based machine learning technique (e.g. SVM with Gaussian\nkernel). In this paper, we rather propose to use a sequence structured kernel:\nthe spectrum kernel. We show that the conventional stacked vector-based kernel\nis actually a special case of this kernel. Experiments conducted on various\npublicly available hyperspectral datasets illustrate the improvement of the\nproposed kernel w.r.t. conventional ones using the same hierarchical spatial\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 21:19:54 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Cui", "Yanwei", ""], ["Chapel", "Laetitia", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""]]}, {"id": "1606.04988", "submitter": "Paul Mineiro", "authors": "Hal Daume III, Nikos Karampatziakis, John Langford, Paul Mineiro", "title": "Logarithmic Time One-Against-Some", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create a new online reduction of multiclass classification to binary\nclassification for which training and prediction time scale logarithmically\nwith the number of classes. Compared to previous approaches, we obtain\nsubstantially better statistical performance for two reasons: First, we prove a\ntighter and more complete boosting theorem, and second we translate the results\nmore directly into an algorithm. We show that several simple techniques give\nrise to an algorithm that can compete with one-against-all in both space and\npredictive power while offering exponential improvements in speed when the\nnumber of classes is large.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 21:27:43 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 02:09:04 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Daume", "Hal", "III"], ["Karampatziakis", "Nikos", ""], ["Langford", "John", ""], ["Mineiro", "Paul", ""]]}, {"id": "1606.04991", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Alec Koppel and Alejandro Ribeiro", "title": "A Class of Parallel Doubly Stochastic Algorithms for Large-Scale\n  Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1603.06782", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning problems over training sets in which both, the number of\ntraining examples and the dimension of the feature vectors, are large. To solve\nthese problems we propose the random parallel stochastic algorithm (RAPSA). We\ncall the algorithm random parallel because it utilizes multiple parallel\nprocessors to operate on a randomly chosen subset of blocks of the feature\nvector. We call the algorithm stochastic because processors choose training\nsubsets uniformly at random. Algorithms that are parallel in either of these\ndimensions exist, but RAPSA is the first attempt at a methodology that is\nparallel in both the selection of blocks and the selection of elements of the\ntraining set. In RAPSA, processors utilize the randomly chosen functions to\ncompute the stochastic gradient component associated with a randomly chosen\nblock. The technical contribution of this paper is to show that this minimally\ncoordinated algorithm converges to the optimal classifier when the training\nobjective is convex. Moreover, we present an accelerated version of RAPSA\n(ARAPSA) that incorporates the objective function curvature information by\npremultiplying the descent direction by a Hessian approximation matrix. We\nfurther extend the results for asynchronous settings and show that if the\nprocessors perform their updates without any coordination the algorithms are\nstill convergent to the optimal argument. RAPSA and its extensions are then\nnumerically evaluated on a linear estimation problem and a binary image\nclassification task using the MNIST handwritten digit dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 21:34:46 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Koppel", "Alec", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1606.05007", "submitter": "Naoya Takahashi", "authors": "Naoya Takahashi, Tofigh Naghibi, Beat Pfister", "title": "Automatic Pronunciation Generation by Utilizing a Semi-supervised Deep\n  Neural Networks", "comments": "Proc. of 17th Interspeech (2016), San Francisco, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phonemic or phonetic sub-word units are the most commonly used atomic\nelements to represent speech signals in modern ASRs. However they are not the\noptimal choice due to several reasons such as: large amount of effort required\nto handcraft a pronunciation dictionary, pronunciation variations, human\nmistakes and under-resourced dialects and languages. Here, we propose a\ndata-driven pronunciation estimation and acoustic modeling method which only\ntakes the orthographic transcription to jointly estimate a set of sub-word\nunits and a reliable dictionary. Experimental results show that the proposed\nmethod which is based on semi-supervised training of a deep neural network\nlargely outperforms phoneme based continuous speech recognition on the TIMIT\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 23:45:33 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Takahashi", "Naoya", ""], ["Naghibi", "Tofigh", ""], ["Pfister", "Beat", ""]]}, {"id": "1606.05018", "submitter": "Stefan Hosein", "authors": "Stefan Hosein and Patrick Hosein", "title": "Improving Power Generation Efficiency using Deep Neural Networks", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been significant research on power generation,\ndistribution and transmission efficiency especially in the case of renewable\nresources. The main objective is reduction of energy losses and this requires\nimprovements on data acquisition and analysis. In this paper we address these\nconcerns by using consumers' electrical smart meter readings to estimate\nnetwork loading and this information can then be used for better capacity\nplanning. We compare Deep Neural Network (DNN) methods with traditional methods\nfor load forecasting. Our results indicate that DNN methods outperform most\ntraditional methods. This comes at the cost of additional computational\ncomplexity but this can be addressed with the use of cloud resources. We also\nillustrate how these results can be used to better support dynamic pricing.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 00:53:56 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Hosein", "Stefan", ""], ["Hosein", "Patrick", ""]]}, {"id": "1606.05027", "submitter": "Jonas Mueller", "authors": "Jonas Mueller, David N. Reshef, George Du, Tommi Jaakkola", "title": "Learning Optimal Interventions", "comments": "AISTATS 2017", "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics, PMLR 54:1039-1047, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to identify beneficial interventions from observational data. We\nconsider interventions that are narrowly focused (impacting few covariates) and\nmay be tailored to each individual or globally enacted over a population. For\napplications where harmful intervention is drastically worse than proposing no\nchange, we propose a conservative definition of the optimal intervention.\nAssuming the underlying relationship remains invariant under intervention, we\ndevelop efficient algorithms to identify the optimal intervention policy from\nlimited data and provide theoretical guarantees for our approach in a Gaussian\nProcess setting. Although our methods assume covariates can be precisely\nadjusted, they remain capable of improving outcomes in misspecified settings\nwhere interventions incur unintentional downstream effects. Empirically, our\napproach identifies good interventions in two practical applications: gene\nperturbation and writing improvement.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 01:55:33 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 06:12:56 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Mueller", "Jonas", ""], ["Reshef", "David N.", ""], ["Du", "George", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1606.05060", "submitter": "Feng Nan", "authors": "Feng Nan, Joseph Wang, Venkatesh Saligrama", "title": "Pruning Random Forests for Prediction on a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to prune a random forest (RF) for resource-constrained prediction.\nWe first construct a RF and then prune it to optimize expected feature cost &\naccuracy. We pose pruning RFs as a novel 0-1 integer program with linear\nconstraints that encourages feature re-use. We establish total unimodularity of\nthe constraint set to prove that the corresponding LP relaxation solves the\noriginal integer program. We then exploit connections to combinatorial\noptimization and develop an efficient primal-dual algorithm, scalable to large\ndatasets. In contrast to our bottom-up approach, which benefits from good RF\ninitialization, conventional methods are top-down acquiring features based on\ntheir utility value and is generally intractable, requiring heuristics.\nEmpirically, our pruning algorithm outperforms existing state-of-the-art\nresource-constrained algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 05:56:36 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Nan", "Feng", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1606.05228", "submitter": "Charles Zheng", "authors": "Charles Y. Zheng, Rakesh Achanta, and Yuval Benjamini", "title": "How many faces can be recognized? Performance extrapolation for\n  multi-class classification", "comments": "Submitted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of multi-class classification generally increases with the\nnumber of classes. Using data from a subset of the classes, can we predict how\nwell a classifier will scale with an increased number of classes? Under the\nassumption that the classes are sampled exchangeably, and under the assumption\nthat the classifier is generative (e.g. QDA or Naive Bayes), we show that the\nexpected accuracy when the classifier is trained on $k$ classes is the $k-1$st\nmoment of a \\emph{conditional accuracy distribution}, which can be estimated\nfrom data. This provides the theoretical foundation for performance\nextrapolation based on pseudolikelihood, unbiased estimation, and\nhigh-dimensional asymptotics. We investigate the robustness of our methods to\nnon-generative classifiers in simulations and one optical character recognition\nexample.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 15:38:20 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Zheng", "Charles Y.", ""], ["Achanta", "Rakesh", ""], ["Benjamini", "Yuval", ""]]}, {"id": "1606.05233", "submitter": "Jo\\~ao F. Henriques", "authors": "Luca Bertinetto, Jo\\~ao F. Henriques, Jack Valmadre, Philip H. S.\n  Torr, Andrea Vedaldi", "title": "Learning feed-forward one-shot learners", "comments": "The first three authors contributed equally, and are listed in\n  alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot learning is usually tackled by using generative models or\ndiscriminative embeddings. Discriminative methods based on deep learning, which\nare very effective in other learning scenarios, are ill-suited for one-shot\nlearning as they need large amounts of training data. In this paper, we propose\na method to learn the parameters of a deep model in one shot. We construct the\nlearner as a second deep network, called a learnet, which predicts the\nparameters of a pupil network from a single exemplar. In this manner we obtain\nan efficient feed-forward one-shot learner, trained end-to-end by minimizing a\none-shot classification objective in a learning to learn formulation. In order\nto make the construction feasible, we propose a number of factorizations of the\nparameters of the pupil network. We demonstrate encouraging results by learning\ncharacters from single exemplars in Omniglot, and by tracking visual objects\nfrom a single initial exemplar in the Visual Object Tracking benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 15:49:26 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Bertinetto", "Luca", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Valmadre", "Jack", ""], ["Torr", "Philip H. S.", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1606.05302", "submitter": "Farideh Fazayeli", "authors": "Farideh Fazayeli and Arindam Banerjee", "title": "Generalized Direct Change Estimation in Ising Model Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating change in the dependency structure\nbetween two $p$-dimensional Ising models, based on respectively $n_1$ and $n_2$\nsamples drawn from the models. The change is assumed to be structured, e.g.,\nsparse, block sparse, node-perturbed sparse, etc., such that it can be\ncharacterized by a suitable (atomic) norm. We present and analyze a\nnorm-regularized estimator for directly estimating the change in structure,\nwithout having to estimate the structures of the individual Ising models. The\nestimator can work with any norm, and can be generalized to other graphical\nmodels under mild assumptions. We show that only one set of samples, say $n_2$,\nneeds to satisfy the sample complexity requirement for the estimator to work,\nand the estimation error decreases as $\\frac{c}{\\sqrt{\\min(n_1,n_2)}}$, where\n$c$ depends on the Gaussian width of the unit norm ball. For example, for\n$\\ell_1$ norm applied to $s$-sparse change, the change can be accurately\nestimated with $\\min(n_1,n_2)=O(s \\log p)$ which is sharper than an existing\nresult $n_1= O(s^2 \\log p)$ and $n_2 = O(n_1^2)$. Experimental results\nillustrating the effectiveness of the proposed estimator are presented.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 18:21:45 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Fazayeli", "Farideh", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1606.05313", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt and Percy Liang", "title": "Unsupervised Risk Estimation Using Only Conditional Independence\n  Structure", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to estimate a model's test error from unlabeled data, on\ndistributions very different from the training distribution, while assuming\nonly that certain conditional independencies are preserved between train and\ntest. We do not need to assume that the optimal predictor is the same between\ntrain and test, or that the true distribution lies in any parametric family. We\ncan also efficiently differentiate the error estimate to perform unsupervised\ndiscriminative learning. Our technical tool is the method of moments, which\nallows us to exploit conditional independencies in the absence of a\nfully-specified model. Our framework encompasses a large family of losses\nincluding the log and exponential loss, and extends to structured output\nsettings such as hidden Markov models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 18:48:51 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Liang", "Percy", ""]]}, {"id": "1606.05316", "submitter": "Roi Livni", "authors": "Roi Livni and Daniel Carmon and Amir Globerson", "title": "Learning Infinite-Layer Networks: Without the Kernel Trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infinite--Layer Networks (ILN) have recently been proposed as an architecture\nthat mimics neural networks while enjoying some of the advantages of kernel\nmethods. ILN are networks that integrate over infinitely many nodes within a\nsingle hidden layer. It has been demonstrated by several authors that the\nproblem of learning ILN can be reduced to the kernel trick, implying that\nwhenever a certain integral can be computed analytically they are efficiently\nlearnable.\n  In this work we give an online algorithm for ILN, which avoids the kernel\ntrick assumption. More generally and of independent interest, we show that\nkernel methods in general can be exploited even when the kernel cannot be\nefficiently computed but can only be estimated via sampling.\n  We provide a regret analysis for our algorithm, showing that it matches the\nsample complexity of methods which have access to kernel values. Thus, our\nmethod is the first to demonstrate that the kernel trick is not necessary as\nsuch, and random features suffice to obtain comparable performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:02:14 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 04:13:58 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Livni", "Roi", ""], ["Carmon", "Daniel", ""], ["Globerson", "Amir", ""]]}, {"id": "1606.05320", "submitter": "Viktoriya Krakovna", "authors": "Viktoriya Krakovna, Finale Doshi-Velez", "title": "Increasing the Interpretability of Recurrent Neural Networks Using\n  Hidden Markov Models", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks continue to revolutionize various application\ndomains, there is increasing interest in making these powerful models more\nunderstandable and interpretable, and narrowing down the causes of good and bad\npredictions. We focus on recurrent neural networks (RNNs), state of the art\nmodels in speech recognition and translation. Our approach to increasing\ninterpretability is by combining an RNN with a hidden Markov model (HMM), a\nsimpler and more transparent model. We explore various combinations of RNNs and\nHMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained\nfirst, then a small LSTM is given HMM state distributions and trained to fill\nin gaps in the HMM's performance; and a jointly trained hybrid model. We find\nthat the LSTM and HMM learn complementary information about the features in the\ntext.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:13:52 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 22:20:39 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Krakovna", "Viktoriya", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1606.05325", "submitter": "Yubin Park", "authors": "Yubin Park and Joyce Ho and Joydeep Ghosh", "title": "ACDC: $\\alpha$-Carving Decision Chain for Risk Stratification", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many healthcare settings, intuitive decision rules for risk stratification\ncan help effective hospital resource allocation. This paper introduces a novel\nvariant of decision tree algorithms that produces a chain of decisions, not a\ngeneral tree. Our algorithm, $\\alpha$-Carving Decision Chain (ACDC),\nsequentially carves out \"pure\" subsets of the majority class examples. The\nresulting chain of decision rules yields a pure subset of the minority class\nexamples. Our approach is particularly effective in exploring large and\nclass-imbalanced health datasets. Moreover, ACDC provides an interactive\ninterpretation in conjunction with visual performance metrics such as Receiver\nOperating Characteristics curve and Lift chart.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:36:51 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Park", "Yubin", ""], ["Ho", "Joyce", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1606.05328", "submitter": "A\\\"aron van den Oord", "authors": "Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt,\n  Alex Graves, Koray Kavukcuoglu", "title": "Conditional Image Generation with PixelCNN Decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores conditional image generation with a new image density\nmodel based on the PixelCNN architecture. The model can be conditioned on any\nvector, including descriptive labels or tags, or latent embeddings created by\nother networks. When conditioned on class labels from the ImageNet database,\nthe model is able to generate diverse, realistic scenes representing distinct\nanimals, objects, landscapes and structures. When conditioned on an embedding\nproduced by a convolutional network given a single image of an unseen face, it\ngenerates a variety of new portraits of the same person with different facial\nexpressions, poses and lighting conditions. We also show that conditional\nPixelCNN can serve as a powerful decoder in an image autoencoder. Additionally,\nthe gated convolutional layers in the proposed model improve the log-likelihood\nof PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet,\nwith greatly reduced computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:40:56 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 15:44:24 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Oord", "Aaron van den", ""], ["Kalchbrenner", "Nal", ""], ["Vinyals", "Oriol", ""], ["Espeholt", "Lasse", ""], ["Graves", "Alex", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1606.05336", "submitter": "Maithra Raghu", "authors": "Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, Jascha\n  Sohl-Dickstein", "title": "On the Expressive Power of Deep Neural Networks", "comments": "Accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to the problem of neural network expressivity,\nwhich seeks to characterize how structural properties of a neural network\nfamily affect the functions it is able to compute. Our approach is based on an\ninterrelated set of measures of expressivity, unified by the novel notion of\ntrajectory length, which measures how the output of a network changes as the\ninput sweeps along a one-dimensional path. Our findings can be summarized as\nfollows:\n  (1) The complexity of the computed function grows exponentially with depth.\n  (2) All weights are not equal: trained networks are more sensitive to their\nlower (initial) layer weights.\n  (3) Regularizing on trajectory length (trajectory regularization) is a\nsimpler alternative to batch normalization, with the same performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:55:29 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 20:26:47 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2016 22:21:25 GMT"}, {"version": "v4", "created": "Mon, 3 Oct 2016 15:44:39 GMT"}, {"version": "v5", "created": "Wed, 1 Mar 2017 03:00:26 GMT"}, {"version": "v6", "created": "Sun, 18 Jun 2017 13:24:34 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Raghu", "Maithra", ""], ["Poole", "Ben", ""], ["Kleinberg", "Jon", ""], ["Ganguli", "Surya", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1606.05340", "submitter": "Subhaneil Lahiri", "authors": "Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein,\n  Surya Ganguli", "title": "Exponential expressivity in deep neural networks through transient chaos", "comments": "Fixed equation references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine Riemannian geometry with the mean field theory of high dimensional\nchaos to study the nature of signal propagation in generic, deep neural\nnetworks with random weights. Our results reveal an order-to-chaos expressivity\nphase transition, with networks in the chaotic phase computing nonlinear\nfunctions whose global curvature grows exponentially with depth but not width.\nWe prove this generic class of deep random functions cannot be efficiently\ncomputed by any shallow network, going beyond prior work restricted to the\nanalysis of single functions. Moreover, we formalize and quantitatively\ndemonstrate the long conjectured idea that deep networks can disentangle highly\ncurved manifolds in input space into flat manifolds in hidden space. Our\ntheoretical analysis of the expressive power of deep networks broadly applies\nto arbitrary nonlinearities, and provides a quantitative underpinning for\npreviously abstract notions about the geometry of deep functions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:59:57 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 18:13:20 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Poole", "Ben", ""], ["Lahiri", "Subhaneil", ""], ["Raghu", "Maithra", ""], ["Sohl-Dickstein", "Jascha", ""], ["Ganguli", "Surya", ""]]}, {"id": "1606.05374", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt and Gregory Valiant and Moses Charikar", "title": "Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer\n  Prediction", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a crowdsourcing model in which $n$ workers are asked to rate the\nquality of $n$ items previously generated by other workers. An unknown set of\n$\\alpha n$ workers generate reliable ratings, while the remaining workers may\nbehave arbitrarily and possibly adversarially. The manager of the experiment\ncan also manually evaluate the quality of a small number of items, and wishes\nto curate together almost all of the high-quality items with at most an\n$\\epsilon$ fraction of low-quality items. Perhaps surprisingly, we show that\nthis is possible with an amount of work required of the manager, and each\nworker, that does not scale with $n$: the dataset can be curated with\n$\\tilde{O}\\Big(\\frac{1}{\\beta\\alpha^3\\epsilon^4}\\Big)$ ratings per worker, and\n$\\tilde{O}\\Big(\\frac{1}{\\beta\\epsilon^2}\\Big)$ ratings by the manager, where\n$\\beta$ is the fraction of high-quality items. Our results extend to the more\ngeneral setting of peer prediction, including peer grading in online\nclassrooms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 21:45:14 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Valiant", "Gregory", ""], ["Charikar", "Moses", ""]]}, {"id": "1606.05382", "submitter": "Arin Chaudhuri", "authors": "Arin Chaudhuri, Deovrat Kakde, Maria Jahja, Wei Xiao, Hansi Jiang,\n  Seunghyun Kong, Sergiy Peredriy", "title": "Sampling Method for Fast Training of Support Vector Data Description", "comments": null, "journal-ref": null, "doi": "10.1109/RAM.2018.8463127", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) is a popular outlier detection\ntechnique which constructs a flexible description of the input data. SVDD\ncomputation time is high for large training datasets which limits its use in\nbig-data process-monitoring applications. We propose a new iterative\nsampling-based method for SVDD training. The method incrementally learns the\ntraining data description at each iteration by computing SVDD on an independent\nrandom sample selected with replacement from the training data set. The\nexperimental results indicate that the proposed method is extremely fast and\nprovides a good data description .\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 23:18:23 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 17:30:57 GMT"}, {"version": "v3", "created": "Sun, 25 Sep 2016 22:15:38 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Chaudhuri", "Arin", ""], ["Kakde", "Deovrat", ""], ["Jahja", "Maria", ""], ["Xiao", "Wei", ""], ["Jiang", "Hansi", ""], ["Kong", "Seunghyun", ""], ["Peredriy", "Sergiy", ""]]}, {"id": "1606.05386", "submitter": "Marco Tulio Ribeiro", "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin", "title": "Model-Agnostic Interpretability of Machine Learning", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why machine learning models behave the way they do empowers\nboth system designers and end-users in many ways: in model selection, feature\nengineering, in order to trust and act upon the predictions, and in more\nintuitive user interfaces. Thus, interpretability has become a vital concern in\nmachine learning, and work in the area of interpretable models has found\nrenewed interest. In some applications, such models are as accurate as\nnon-interpretable ones, and thus are preferred for their transparency. Even\nwhen they are not accurate, they may still be preferred when interpretability\nis of paramount importance. However, restricting machine learning to\ninterpretable models is often a severe limitation. In this paper we argue for\nexplaining machine learning predictions using model-agnostic approaches. By\ntreating the machine learning models as black-box functions, these approaches\nprovide crucial flexibility in the choice of models, explanations, and\nrepresentations, improving debugging, comparison, and interfaces for a variety\nof users and models. We also outline the main challenges for such methods, and\nreview a recently-introduced model-agnostic explanation approach (LIME) that\naddresses these challenges.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 23:39:41 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Ribeiro", "Marco Tulio", ""], ["Singh", "Sameer", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1606.05427", "submitter": "EPTCS", "authors": "Jasmin Christian Blanchette, Cezary Kaliszyk", "title": "Proceedings First International Workshop on Hammers for Type Theories", "comments": null, "journal-ref": "EPTCS 210, 2016", "doi": "10.4204/EPTCS.210", "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume of EPTCS contains the proceedings of the First Workshop on\nHammers for Type Theories (HaTT 2016), held on 1 July 2016 as part of the\nInternational Joint Conference on Automated Reasoning (IJCAR 2016) in Coimbra,\nPortugal. The proceedings contain four regular papers, as well as abstracts of\nthe two invited talks by Pierre Corbineau (Verimag, France) and Aleksy Schubert\n(University of Warsaw, Poland).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 06:52:32 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Blanchette", "Jasmin Christian", ""], ["Kaliszyk", "Cezary", ""]]}, {"id": "1606.05464", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein and Tim Rockt\\\"aschel and Andreas Vlachos and\n  Kalina Bontcheva", "title": "Stance Detection with Bidirectional Conditional Encoding", "comments": "10 pages", "journal-ref": "EMNLP 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stance detection is the task of classifying the attitude expressed in a text\ntowards a target such as Hillary Clinton to be \"positive\", negative\" or\n\"neutral\". Previous work has assumed that either the target is mentioned in the\ntext or that training data for every target is given. This paper considers the\nmore challenging version of this task, where targets are not always mentioned\nand no training data is available for the test targets. We experiment with\nconditional LSTM encoding, which builds a representation of the tweet that is\ndependent on the target, and demonstrate that it outperforms encoding the tweet\nand the target independently. Performance is improved further when the\nconditional model is augmented with bidirectional encoding. We evaluate our\napproach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving\nperformance second best only to a system trained on semi-automatically labelled\ntweets for the test target. When such weak supervision is added, our approach\nachieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 09:39:47 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 20:49:16 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Augenstein", "Isabelle", ""], ["Rockt\u00e4schel", "Tim", ""], ["Vlachos", "Andreas", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "1606.05554", "submitter": "Noura Al Moubayed", "authors": "Noura Al Moubayed, Toby Breckon, Peter Matthews, and A. Stephen\n  McGough", "title": "SMS Spam Filtering using Probabilistic Topic Modelling and Stacked\n  Denoising Autoencoder", "comments": "Paper was accepted to the 25th International Conference on Artificial\n  Neural Networks (ICANN 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In This paper we present a novel approach to spam filtering and demonstrate\nits applicability with respect to SMS messages. Our approach requires minimum\nfeatures engineering and a small set of la- belled data samples. Features are\nextracted using topic modelling based on latent Dirichlet allocation, and then\na comprehensive data model is created using a Stacked Denoising Autoencoder\n(SDA). Topic modelling summarises the data providing ease of use and high\ninterpretability by visualising the topics using word clouds. Given that the\nSMS messages can be regarded as either spam (unwanted) or ham (wanted), the SDA\nis able to model the messages and accurately discriminate between the two\nclasses without the need for a pre-labelled training set. The results are\ncompared against the state-of-the-art spam detection algorithms with our\nproposed approach achieving over 97% accuracy which compares favourably to the\nbest reported algorithms presented in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 15:15:18 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Moubayed", "Noura Al", ""], ["Breckon", "Toby", ""], ["Matthews", "Peter", ""], ["McGough", "A. Stephen", ""]]}, {"id": "1606.05572", "submitter": "Haizi Yu", "authors": "Haizi Yu, Lav R. Varshney, Guy E. Garnett, Ranjitha Kumar", "title": "Learning Interpretable Musical Compositional Rules and Traces", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout music history, theorists have identified and documented\ninterpretable rules that capture the decisions of composers. This paper asks,\n\"Can a machine behave like a music theorist?\" It presents MUS-ROVER, a\nself-learning system for automatically discovering rules from symbolic music.\nMUS-ROVER performs feature learning via $n$-gram models to extract\ncompositional rules --- statistical patterns over the resulting features. We\nevaluate MUS-ROVER on Bach's (SATB) chorales, demonstrating that it can recover\nknown rules, as well as identify new, characteristic patterns for further\nstudy. We discuss how the extracted rules can be used in both machine and human\ncomposition.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 15:58:24 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Yu", "Haizi", ""], ["Varshney", "Lav R.", ""], ["Garnett", "Guy E.", ""], ["Kumar", "Ranjitha", ""]]}, {"id": "1606.05579", "submitter": "Irina Higgins", "authors": "Irina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria,\n  Charles Blundell, Shakir Mohamed, Alexander Lerchner", "title": "Early Visual Concept Learning with Unsupervised Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated discovery of early visual concepts from raw image data is a major\nopen challenge in AI research. Addressing this problem, we propose an\nunsupervised approach for learning disentangled representations of the\nunderlying factors of variation. We draw inspiration from neuroscience, and\nshow how this can be achieved in an unsupervised generative model by applying\nthe same learning pressures as have been suggested to act in the ventral visual\nstream in the brain. By enforcing redundancy reduction, encouraging statistical\nindependence, and exposure to data with transform continuities analogous to\nthose to which human infants are exposed, we obtain a variational autoencoder\n(VAE) framework capable of learning disentangled factors. Our approach makes\nfew assumptions and works well across a wide variety of datasets. Furthermore,\nour solution has useful emergent properties, such as zero-shot inference and an\nintuitive understanding of \"objectness\".\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 16:19:46 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 19:50:49 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 09:30:26 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Higgins", "Irina", ""], ["Matthey", "Loic", ""], ["Glorot", "Xavier", ""], ["Pal", "Arka", ""], ["Uria", "Benigno", ""], ["Blundell", "Charles", ""], ["Mohamed", "Shakir", ""], ["Lerchner", "Alexander", ""]]}, {"id": "1606.05596", "submitter": "Yang Lei", "authors": "Yang Lei, James C. Bezdek, Simone Romano, Nguyen Xuan Vinh, Jeffrey\n  Chan and James Bailey", "title": "Ground Truth Bias in External Cluster Validity Indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been noticed that some external CVIs exhibit a preferential bias\ntowards a larger or smaller number of clusters which is monotonic (directly or\ninversely) in the number of clusters in candidate partitions. This type of bias\nis caused by the functional form of the CVI model. For example, the popular\nRand index (RI) exhibits a monotone increasing (NCinc) bias, while the Jaccard\nIndex (JI) index suffers from a monotone decreasing (NCdec) bias. This type of\nbias has been previously recognized in the literature. In this work, we\nidentify a new type of bias arising from the distribution of the ground truth\n(reference) partition against which candidate partitions are compared. We call\nthis new type of bias ground truth (GT) bias. This type of bias occurs if a\nchange in the reference partition causes a change in the bias status (e.g.,\nNCinc, NCdec) of a CVI. For example, NCinc bias in the RI can be changed to\nNCdec bias by skewing the distribution of clusters in the ground truth\npartition. It is important for users to be aware of this new type of biased\nbehaviour, since it may affect the interpretations of CVI results. The\nobjective of this article is to study the empirical and theoretical\nimplications of GT bias. To the best of our knowledge, this is the first\nextensive study of such a property for external cluster validity indices.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 17:31:51 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Lei", "Yang", ""], ["Bezdek", "James C.", ""], ["Romano", "Simone", ""], ["Vinh", "Nguyen Xuan", ""], ["Chan", "Jeffrey", ""], ["Bailey", "James", ""]]}, {"id": "1606.05615", "submitter": "Yatao A. Bian", "authors": "Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, Andreas\n  Krause", "title": "Guaranteed Non-convex Optimization: Submodular Maximization over\n  Continuous Domains", "comments": "Appears in the 20th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular continuous functions are a category of (generally)\nnon-convex/non-concave functions with a wide spectrum of applications. We\ncharacterize these functions and demonstrate that they can be maximized\nefficiently with approximation guarantees. Specifically, i) We introduce the\nweak DR property that gives a unified characterization of submodularity for all\nset, integer-lattice and continuous functions; ii) for maximizing monotone\nDR-submodular continuous functions under general down-closed convex\nconstraints, we propose a Frank-Wolfe variant with $(1-1/e)$ approximation\nguarantee, and sub-linear convergence rate; iii) for maximizing general\nnon-monotone submodular continuous functions subject to box constraints, we\npropose a DoubleGreedy algorithm with $1/3$ approximation guarantee. Submodular\ncontinuous functions naturally find applications in various real-world\nsettings, including influence and revenue maximization with continuous\nassignments, sensor energy management, multi-resolution data summarization,\nfacility location, etc. Experimental results show that the proposed algorithms\nefficiently generate superior solutions compared to baseline algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 18:15:52 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 16:21:08 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 15:53:40 GMT"}, {"version": "v4", "created": "Wed, 1 Mar 2017 16:30:15 GMT"}, {"version": "v5", "created": "Mon, 6 May 2019 16:06:35 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Bian", "Andrew An", ""], ["Mirzasoleiman", "Baharan", ""], ["Buhmann", "Joachim M.", ""], ["Krause", "Andreas", ""]]}, {"id": "1606.05642", "submitter": "Mohammad Javad Faraji", "authors": "Mohammadjavad Faraji, Kerstin Preuschoff, Wulfram Gerstner", "title": "Balancing New Against Old Information: The Role of Surprise in Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surprise describes a range of phenomena from unexpected events to behavioral\nresponses. We propose a measure of surprise and use it for surprise-driven\nlearning. Our surprise measure takes into account data likelihood as well as\nthe degree of commitment to a belief via the entropy of the belief\ndistribution. We find that surprise-minimizing learning dynamically adjusts the\nbalance between new and old information without the need of knowledge about the\ntemporal statistics of the environment. We apply our framework to a dynamic\ndecision-making task and a maze exploration task. Our surprise minimizing\nframework is suitable for learning in complex environments, even if the\nenvironment undergoes gradual or sudden changes and could eventually provide a\nframework to study the behavior of humans and animals encountering surprising\nevents.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 19:54:43 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 20:31:24 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Faraji", "Mohammadjavad", ""], ["Preuschoff", "Kerstin", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "1606.05664", "submitter": "Talat Nazir", "authors": "Xiaomin Qi, Sergei Silvestrov and Talat Nazir", "title": "Linear Classification of data with Support Vector Machines and\n  Generalized Support Vector Machines", "comments": "submitted", "journal-ref": null, "doi": "10.1063/1.4972718", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the support vector machine and introduced the notion\nof generalized support vector machine for classification of data. We show that\nthe problem of generalized support vector machine is equivalent to the problem\nof generalized variational inequality and establish various results for the\nexistence of solutions. Moreover, we provide various examples to support our\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 09:54:46 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Qi", "Xiaomin", ""], ["Silvestrov", "Sergei", ""], ["Nazir", "Talat", ""]]}, {"id": "1606.05685", "submitter": "Josua Krause", "authors": "Josua Krause, Adam Perer, Enrico Bertini", "title": "Using Visual Analytics to Interpret Predictive Machine Learning Models", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly believed that increasing the interpretability of a machine\nlearning model may decrease its predictive power. However, inspecting\ninput-output relationships of those models using visual analytics, while\ntreating them as black-box, can help to understand the reasoning behind\noutcomes without sacrificing predictive quality. We identify a space of\npossible solutions and provide two examples of where such techniques have been\nsuccessfully used in practice.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 21:56:43 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 18:06:13 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Krause", "Josua", ""], ["Perer", "Adam", ""], ["Bertini", "Enrico", ""]]}, {"id": "1606.05688", "submitter": "Aleksandar Zlateski", "authors": "Aleksandar Zlateski, Kisuk Lee and H. Sebastian Seung", "title": "ZNNi - Maximizing the Inference Throughput of 3D Convolutional Networks\n  on Multi-Core CPUs and GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding window convolutional networks (ConvNets) have become a popular\napproach to computer vision problems such as image segmentation, and object\ndetection and localization. Here we consider the problem of inference, the\napplication of a previously trained ConvNet, with emphasis on 3D images. Our\ngoal is to maximize throughput, defined as average number of output voxels\ncomputed per unit time. Other things being equal, processing a larger image\ntends to increase throughput, because fractionally less computation is wasted\non the borders of the image. It follows that an apparently slower algorithm may\nend up having higher throughput if it can process a larger image within the\nconstraint of the available RAM. We introduce novel CPU and GPU primitives for\nconvolutional and pooling layers, which are designed to minimize memory\noverhead. The primitives include convolution based on highly efficient pruned\nFFTs. Our theoretical analyses and empirical tests reveal a number of\ninteresting findings. For some ConvNet architectures, cuDNN is outperformed by\nour FFT-based GPU primitives, and these in turn can be outperformed by our CPU\nprimitives. The CPU manages to achieve higher throughput because of its fast\naccess to more RAM. A novel primitive in which the GPU accesses host RAM can\nsignificantly increase GPU throughput. Finally, a CPU-GPU algorithm achieves\nthe greatest throughput of all, 10x or more than other publicly available\nimplementations of sliding window 3D ConvNets. All of our code has been made\navailable as open source project.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 22:16:39 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zlateski", "Aleksandar", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1606.05693", "submitter": "Nicholas Johnson", "authors": "Nicholas Johnson, Vidyashankar Sivakumar, Arindam Banerjee", "title": "Structured Stochastic Linear Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic linear bandit problem proceeds in rounds where at each round\nthe algorithm selects a vector from a decision set after which it receives a\nnoisy linear loss parameterized by an unknown vector. The goal in such a\nproblem is to minimize the (pseudo) regret which is the difference between the\ntotal expected loss of the algorithm and the total expected loss of the best\nfixed vector in hindsight. In this paper, we consider settings where the\nunknown parameter has structure, e.g., sparse, group sparse, low-rank, which\ncan be captured by a norm, e.g., $L_1$, $L_{(1,2)}$, nuclear norm. We focus on\nconstructing confidence ellipsoids which contain the unknown parameter across\nall rounds with high-probability. We show the radius of such ellipsoids depend\non the Gaussian width of sets associated with the norm capturing the structure.\nSuch characterization leads to tighter confidence ellipsoids and, therefore,\nsharper regret bounds compared to bounds in the existing literature which are\nbased on the ambient dimensionality.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 22:31:01 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Johnson", "Nicholas", ""], ["Sivakumar", "Vidyashankar", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1606.05725", "submitter": "Amirhossein Akbarnejad", "authors": "Amirhossein Akbarnejad, Mahdieh Soleymani Baghshah", "title": "An Efficient Large-scale Semi-supervised Multi-label Classifier Capable\n  of Handling Missing labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification has received considerable interest in recent\nyears. Multi-label classifiers have to address many problems including:\nhandling large-scale datasets with many instances and a large set of labels,\ncompensating missing label assignments in the training set, considering\ncorrelations between labels, as well as exploiting unlabeled data to improve\nprediction performance. To tackle datasets with a large set of labels,\nembedding-based methods have been proposed which seek to represent the label\nassignments in a low-dimensional space. Many state-of-the-art embedding-based\nmethods use a linear dimensionality reduction to represent the label\nassignments in a low-dimensional space. However, by doing so, these methods\nactually neglect the tail labels - labels that are infrequently assigned to\ninstances. We propose an embedding-based method that non-linearly embeds the\nlabel vectors using an stochastic approach, thereby predicting the tail labels\nmore accurately. Moreover, the proposed method have excellent mechanisms for\nhandling missing labels, dealing with large-scale datasets, as well as\nexploiting unlabeled data. With the best of our knowledge, our proposed method\nis the first multi-label classifier that simultaneously addresses all of the\nmentioned challenges. Experiments on real-world datasets show that our method\noutperforms stateof-the-art multi-label classifiers by a large margin, in terms\nof prediction performance, as well as training time.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 07:49:13 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Akbarnejad", "Amirhossein", ""], ["Baghshah", "Mahdieh Soleymani", ""]]}, {"id": "1606.05735", "submitter": "Muhammed Salman Shamsi", "authors": "Muhammed Salman Shamsi, Jhansi Lakshmi", "title": "A Comparative Analysis of classification data mining techniques :\n  Deriving key factors useful for predicting students performance", "comments": "6 pages, 6 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Students opting for Engineering as their discipline is increasing rapidly.\nBut due to various factors and inappropriate primary education in India,\nfailure rates are high. Students are unable to excel in core engineering\nbecause of complex and mathematical subjects. Hence, they fail in such\nsubjects. With the help of data mining techniques, we can predict the\nperformance of students in terms of grades and failure in subjects. This paper\nperforms a comparative analysis of various classification techniques, such as\nNa\\\"ive Bayes, LibSVM, J48, Random Forest, and JRip and tries to choose best\namong these. Based on the results obtained, we found that Na\\\"ive Bayes is the\nmost accurate method in terms of students failure prediction and JRip is most\naccurate in terms of students grade prediction. We also found that JRip\nmarginally differs from Na\\\"ive Bayes in terms of accuracy for students failure\nprediction and gives us a set of rules from which we derive the key factors\ninfluencing students performance. Finally, we suggest various ways to mitigate\nthese factors. This study is limited to Indian Education system scenarios.\nHowever, the factors found can be helpful in other scenarios as well.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 10:06:44 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 14:47:28 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Shamsi", "Muhammed Salman", ""], ["Lakshmi", "Jhansi", ""]]}, {"id": "1606.05798", "submitter": "Guolong Su", "authors": "Guolong Su, Dennis Wei, Kush R. Varshney, Dmitry M. Malioutov", "title": "Interpretable Two-level Boolean Rule Learning for Classification", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": "WHI 2016 submission", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a contribution to interpretable machine learning research, we develop a\nnovel optimization framework for learning accurate and sparse two-level Boolean\nrules. We consider rules in both conjunctive normal form (AND-of-ORs) and\ndisjunctive normal form (OR-of-ANDs). A principled objective function is\nproposed to trade classification accuracy and interpretability, where we use\nHamming loss to characterize accuracy and sparsity to characterize\ninterpretability. We propose efficient procedures to optimize these objectives\nbased on linear programming (LP) relaxation, block coordinate descent, and\nalternating minimization. Experiments show that our new algorithms provide very\ngood tradeoffs between accuracy and interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 19:37:26 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Su", "Guolong", ""], ["Wei", "Dennis", ""], ["Varshney", "Kush R.", ""], ["Malioutov", "Dmitry M.", ""]]}, {"id": "1606.05819", "submitter": "Sechan Oh", "authors": "Amit Dhurandhar, Sechan Oh, Marek Petrik", "title": "Building an Interpretable Recommender via Loss-Preserving Transformation", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for building an interpretable recommender system for\npersonalizing online content and promotions. Historical data available for the\nsystem consists of customer features, provided content (promotions), and user\nresponses. Unlike in a standard multi-class classification setting,\nmisclassification costs depend on both recommended actions and customers. Our\nmethod transforms such a data set to a new set which can be used with standard\ninterpretable multi-class classification algorithms. The transformation has the\ndesirable property that minimizing the standard misclassification penalty in\nthis new space is equivalent to minimizing the custom cost function.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 01:37:01 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Oh", "Sechan", ""], ["Petrik", "Marek", ""]]}, {"id": "1606.05844", "submitter": "Sivanand Achanta", "authors": "Sivanand Achanta, KNRK Raju Alluri, Suryakanth V Gangashetty", "title": "Statistical Parametric Speech Synthesis Using Bottleneck Representation\n  From Sequence Auto-encoder", "comments": "5 pages (with references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a statistical parametric speech synthesis approach\nwith unit-level acoustic representation. In conventional deep neural network\nbased speech synthesis, the input text features are repeated for the entire\nduration of phoneme for mapping text and speech parameters. This mapping is\nlearnt at the frame-level which is the de-facto acoustic representation.\nHowever much of this computational requirement can be drastically reduced if\nevery unit can be represented with a fixed-dimensional representation. Using\nrecurrent neural network based auto-encoder, we show that it is indeed possible\nto map units of varying duration to a single vector. We then use this acoustic\nrepresentation at unit-level to synthesize speech using deep neural network\nbased statistical parametric speech synthesis technique. Results show that the\nproposed approach is able to synthesize at the same quality as the conventional\nframe based approach at a highly reduced computational cost.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 08:38:26 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Achanta", "Sivanand", ""], ["Alluri", "KNRK Raju", ""], ["Gangashetty", "Suryakanth V", ""]]}, {"id": "1606.05850", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun", "title": "Guaranteed bounds on the Kullback-Leibler divergence of univariate\n  mixtures using piecewise log-sum-exp inequalities", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": "10.3390/e18120442", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-theoretic measures such as the entropy, cross-entropy and the\nKullback-Leibler divergence between two mixture models is a core primitive in\nmany signal processing tasks. Since the Kullback-Leibler divergence of mixtures\nprovably does not admit a closed-form formula, it is in practice either\nestimated using costly Monte-Carlo stochastic integration, approximated, or\nbounded using various techniques. We present a fast and generic method that\nbuilds algorithmically closed-form lower and upper bounds on the entropy, the\ncross-entropy and the Kullback-Leibler divergence of mixtures. We illustrate\nthe versatile method by reporting on our experiments for approximating the\nKullback-Leibler divergence between univariate exponential mixtures, Gaussian\nmixtures, Rayleigh mixtures, and Gamma mixtures.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 09:39:30 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 00:24:48 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""]]}, {"id": "1606.05896", "submitter": "Akash Srivastava", "authors": "Akash Srivastava, James Zou, Ryan P. Adams and Charles Sutton", "title": "Clustering with a Reject Option: Interactive Clustering as Bayesian\n  Prior Elicitation", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good clustering can help a data analyst to explore and understand a data\nset, but what constitutes a good clustering may depend on domain-specific and\napplication-specific criteria. These criteria can be difficult to formalize,\neven when it is easy for an analyst to know a good clustering when they see\none. We present a new approach to interactive clustering for data exploration\ncalled TINDER, based on a particularly simple feedback mechanism, in which an\nanalyst can reject a given clustering and request a new one, which is chosen to\nbe different from the previous clustering while fitting the data well. We\nformalize this interaction in a Bayesian framework as a method for prior\nelicitation, in which each different clustering is produced by a prior\ndistribution that is modified to discourage previously rejected clusterings. We\nshow that TINDER successfully produces a diverse set of clusterings, each of\nequivalent quality, that are much more diverse than would be obtained by\nrandomized restarts.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 18:07:15 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Srivastava", "Akash", ""], ["Zou", "James", ""], ["Adams", "Ryan P.", ""], ["Sutton", "Charles", ""]]}, {"id": "1606.05908", "submitter": "Carl Doersch", "authors": "Carl Doersch", "title": "Tutorial on Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In just three years, Variational Autoencoders (VAEs) have emerged as one of\nthe most popular approaches to unsupervised learning of complicated\ndistributions. VAEs are appealing because they are built on top of standard\nfunction approximators (neural networks), and can be trained with stochastic\ngradient descent. VAEs have already shown promise in generating many kinds of\ncomplicated data, including handwritten digits, faces, house numbers, CIFAR\nimages, physical models of scenes, segmentation, and predicting the future from\nstatic images. This tutorial introduces the intuitions behind VAEs, explains\nthe mathematics behind them, and describes some empirical behavior. No prior\nknowledge of variational Bayesian methods is assumed.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 21:02:30 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 12:33:43 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 16:56:46 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Doersch", "Carl", ""]]}, {"id": "1606.05918", "submitter": "Matthew Blaschko", "authors": "Matthew B. Blaschko", "title": "Slack and Margin Rescaling as Convex Extensions of Supermodular\n  Functions", "comments": "v2 corrects a bug in the maths of v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slack and margin rescaling are variants of the structured output SVM, which\nis frequently applied to problems in computer vision such as image\nsegmentation, object localization, and learning parts based object models. They\ndefine convex surrogates to task specific loss functions, which, when\nspecialized to non-additive loss functions for multi-label problems, yield\nextensions to increasing set functions. We demonstrate in this paper that we\nmay use these concepts to define polynomial time convex extensions of arbitrary\nsupermodular functions, providing an analysis framework for the tightness of\nthese surrogates. This analysis framework shows that, while neither margin nor\nslack rescaling dominate the other, known bounds on supermodular functions can\nbe used to derive extensions that dominate both of these, indicating possible\ndirections for defining novel structured output prediction surrogates. In\naddition to the analysis of structured prediction loss functions, these results\nimply an approach to supermodular minimization in which margin rescaling is\ncombined with non-polynomial time convex extensions to compute a sequence of LP\nrelaxations reminiscent of a cutting plane method. This approach is applied to\nthe problem of selecting representative exemplars from a set of images,\nvalidating our theoretical contributions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 22:17:05 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 08:46:02 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Blaschko", "Matthew B.", ""]]}, {"id": "1606.05925", "submitter": "Vikrant Singh Tomar", "authors": "Vikrant Singh Tomar and Richard C. Rose", "title": "Graph based manifold regularized deep neural networks for automatic\n  speech recognition", "comments": "12 pages including citations, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been successfully applied to a wide variety\nof acoustic modeling tasks in recent years. These include the applications of\nDNNs either in a discriminative feature extraction or in a hybrid acoustic\nmodeling scenario. Despite the rapid progress in this area, a number of\nchallenges remain in training DNNs. This paper presents an effective way of\ntraining DNNs using a manifold learning based regularization framework. In this\nframework, the parameters of the network are optimized to preserve underlying\nmanifold based relationships between speech feature vectors while minimizing a\nmeasure of loss between network outputs and targets. This is achieved by\nincorporating manifold based locality constraints in the objective criterion of\nDNNs. Empirical evidence is provided to demonstrate that training a network\nwith manifold constraints preserves structural compactness in the hidden layers\nof the network. Manifold regularization is applied to train bottleneck DNNs for\nfeature extraction in hidden Markov model (HMM) based speech recognition. The\nexperiments in this work are conducted on the Aurora-2 spoken digits and the\nAurora-4 read news large vocabulary continuous speech recognition tasks. The\nperformance is measured in terms of word error rate (WER) on these tasks. It is\nshown that the manifold regularized DNNs result in up to 37% reduction in WER\nrelative to standard DNNs.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 23:40:51 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Tomar", "Vikrant Singh", ""], ["Rose", "Richard C.", ""]]}, {"id": "1606.05934", "submitter": "Qiuyan Yan", "authors": "Qiuyan Yan and Qifa Sun and Xinming Yan", "title": "Adapting ELM to Time Series Classification: A Novel Diversified Top-k\n  Shapelets Extraction Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  ELM (Extreme Learning Machine) is a single hidden layer feed-forward network,\nwhere the weights between input and hidden layer are initialized randomly. ELM\nis efficient due to its utilization of the analytical approach to compute\nweights between hidden and output layer. However, ELM still fails to output the\nsemantic classification outcome. To address such limitation, in this paper, we\npropose a diversified top-k shapelets transform framework, where the shapelets\nare the subsequences i.e., the best representative and interpretative features\nof each class. As we identified, the most challenge problems are how to extract\nthe best k shapelets in original candidate sets and how to automatically\ndetermine the k value. Specifically, we first define the similar shapelets and\ndiversified top-k shapelets to construct diversity shapelets graph. Then, a\nnovel diversity graph based top-k shapelets extraction algorithm named as\n\\textbf{DivTopkshapelets}\\ is proposed to search top-k diversified shapelets.\nFinally, we propose a shapelets transformed ELM algorithm named as\n\\textbf{DivShapELM} to automatically determine the k value, which is further\nutilized for time series classification. The experimental results over public\ndata sets demonstrate that the proposed approach significantly outperforms\ntraditional ELM algorithm in terms of effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 00:59:11 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Yan", "Qiuyan", ""], ["Sun", "Qifa", ""], ["Yan", "Xinming", ""]]}, {"id": "1606.05988", "submitter": "Sungkyu Jung", "authors": "Sungkyu Jung", "title": "Continuum directions for supervised dimension reduction", "comments": null, "journal-ref": "Comput. Stat. Data Anal. 125 (2018) 27-43", "doi": "10.1016/j.csda.2018.03.015", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction of multivariate data supervised by auxiliary information\nis considered. A series of basis for dimension reduction is obtained as\nminimizers of a novel criterion. The proposed method is akin to continuum\nregression, and the resulting basis is called continuum directions. With a\npresence of binary supervision data, these directions continuously bridge the\nprincipal component, mean difference and linear discriminant directions, thus\nranging from unsupervised to fully supervised dimension reduction.\nHigh-dimensional asymptotic studies of continuum directions for binary\nsupervision reveal several interesting facts. The conditions under which the\nsample continuum directions are inconsistent, but their classification\nperformance is good, are specified. While the proposed method can be directly\nused for binary and multi-category classification, its generalizations to\nincorporate any form of auxiliary data are also presented. The proposed method\nenjoys fast computation, and the performance is better or on par with more\ncomputer-intensive alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 06:52:41 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 19:43:32 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 17:04:18 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Jung", "Sungkyu", ""]]}, {"id": "1606.05990", "submitter": "Petre Birtea", "authors": "Petre Birtea, Cosmin Cernazanu-Glavan, Alexandru Sisu", "title": "A New Training Method for Feedforward Neural Networks Based on Geometric\n  Contraction Property of Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new training method for a feedforward neural network having the\nactivation functions with the geometric contraction property. The method\nconsists of constructing a new functional that is less nonlinear in comparison\nwith the classical functional by removing the nonlinearity of the activation\nfunction from the output layer. We validate this new method by a series of\nexperiments that show an improved learning speed and better classification\nerror.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 07:05:14 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 19:52:59 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Birtea", "Petre", ""], ["Cernazanu-Glavan", "Cosmin", ""], ["Sisu", "Alexandru", ""]]}, {"id": "1606.06031", "submitter": "Sandro Pezzelle", "authors": "Denis Paperno (1), Germ\\'an Kruszewski (1), Angeliki Lazaridou (1),\n  Quan Ngoc Pham (1), Raffaella Bernardi (1), Sandro Pezzelle (1), Marco Baroni\n  (1), Gemma Boleda (1), Raquel Fern\\'andez (2) ((1) CIMeC - Center for\n  Mind/Brain Sciences, University of Trento, (2) Institute for Logic, Language\n  & Computation, University of Amsterdam)", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "comments": "10 pages, Accepted as a long paper for ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LAMBADA, a dataset to evaluate the capabilities of computational\nmodels for text understanding by means of a word prediction task. LAMBADA is a\ncollection of narrative passages sharing the characteristic that human subjects\nare able to guess their last word if they are exposed to the whole passage, but\nnot if they only see the last sentence preceding the target word. To succeed on\nLAMBADA, computational models cannot simply rely on local context, but must be\nable to keep track of information in the broader discourse. We show that\nLAMBADA exemplifies a wide range of linguistic phenomena, and that none of\nseveral state-of-the-art language models reaches accuracy above 1% on this\nnovel benchmark. We thus propose LAMBADA as a challenging test set, meant to\nencourage the development of new models capable of genuine understanding of\nbroad context in natural language text.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 09:37:17 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Paperno", "Denis", ""], ["Kruszewski", "Germ\u00e1n", ""], ["Lazaridou", "Angeliki", ""], ["Pham", "Quan Ngoc", ""], ["Bernardi", "Raffaella", ""], ["Pezzelle", "Sandro", ""], ["Baroni", "Marco", ""], ["Boleda", "Gemma", ""], ["Fern\u00e1ndez", "Raquel", ""]]}, {"id": "1606.06066", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Reinder Haakma, Wil M. P. van der Aalst", "title": "Mining Local Process Models", "comments": "Published in Elsevier's Journal of Innovation in Digital Ecosystems,\n  Special Issue on Data Mining", "journal-ref": "Journal of Innovation in Digital Ecosystems volume 3 issue 2\n  (2016), pages 183-196", "doi": "10.1016/j.jides.2016.11.001", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a method to discover frequent behavioral patterns\nin event logs. We express these patterns as \\emph{local process models}. Local\nprocess model mining can be positioned in-between process discovery and episode\n/ sequential pattern mining. The technique presented in this paper is able to\nlearn behavioral patterns involving sequential composition, concurrency, choice\nand loop, like in process mining. However, we do not look at start-to-end\nmodels, which distinguishes our approach from process discovery and creates a\nlink to episode / sequential pattern mining. We propose an incremental\nprocedure for building local process models capturing frequent patterns based\non so-called process trees. We propose five quality dimensions and\ncorresponding metrics for local process models, given an event log. We show\nmonotonicity properties for some quality dimensions, enabling a speedup of\nlocal process model discovery through pruning. We demonstrate through a real\nlife case study that mining local patterns allows us to get insights in\nprocesses where regular start-to-end process discovery techniques are only able\nto learn unstructured, flower-like, models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 11:28:26 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 17:23:10 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["Haakma", "Reinder", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1606.06069", "submitter": "Frank Nielsen", "authors": "Ke Sun and Frank Nielsen", "title": "Relative Natural Gradient for Learning Large Complex Models", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher information and natural gradient provided deep insights and powerful\ntools to artificial neural networks. However related analysis becomes more and\nmore difficult as the learner's structure turns large and complex. This paper\nmakes a preliminary step towards a new direction. We extract a local component\nof a large neuron system, and defines its relative Fisher information metric\nthat describes accurately this small component, and is invariant to the other\nparts of the system. This concept is important because the geometry structure\nis much simplified and it can be easily applied to guide the learning of neural\nnetworks. We provide an analysis on a list of commonly used components, and\ndemonstrate how to use this concept to further improve optimization.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 11:36:40 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Sun", "Ke", ""], ["Nielsen", "Frank", ""]]}, {"id": "1606.06121", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam\n  Kalai", "title": "Quantifying and Reducing Stereotypes in Word Embeddings", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are optimized to model statistical properties of\nthe training data. If the input data reflects stereotypes and biases of the\nbroader society, then the output of the learning algorithm also captures these\nstereotypes. In this paper, we initiate the study of gender stereotypes in {\\em\nword embedding}, a popular framework to represent text data. As their use\nbecomes increasingly common, applications can inadvertently amplify unwanted\nstereotypes. We show across multiple datasets that the embeddings contain\nsignificant gender stereotypes, especially with regard to professions. We\ncreated a novel gender analogy task and combined it with crowdsourcing to\nsystematically quantify the gender bias in a given embedding. We developed an\nefficient algorithm that reduces gender stereotype using just a handful of\ntraining examples while preserving the useful geometric properties of the\nembedding. We evaluated our algorithm on several metrics. While we focus on\nmale/female stereotypes, our framework may be applicable to other types of\nembedding biases.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 13:58:45 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Zou", "James", ""], ["Saligrama", "Venkatesh", ""], ["Kalai", "Adam", ""]]}, {"id": "1606.06126", "submitter": "Josiah Hanna", "authors": "Josiah P. Hanna, Peter Stone, Scott Niekum", "title": "Bootstrapping with Models: Confidence Intervals for Off-Policy\n  Evaluation", "comments": "Published in proceedings of the 16th International Conference on\n  Autonomous Agents and Multi-agent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an autonomous agent, executing a poor policy may be costly or even\ndangerous. For such agents, it is desirable to determine confidence interval\nlower bounds on the performance of any given policy without executing said\npolicy. Current methods for exact high confidence off-policy evaluation that\nuse importance sampling require a substantial amount of data to achieve a tight\nlower bound. Existing model-based methods only address the problem in discrete\nstate spaces. Since exact bounds are intractable for many domains we trade off\nstrict guarantees of safety for more data-efficient approximate bounds. In this\ncontext, we propose two bootstrapping off-policy evaluation methods which use\nlearned MDP transition models in order to estimate lower confidence bounds on\npolicy performance with limited data in both continuous and discrete state\nspaces. Since direct use of a model may introduce bias, we derive a theoretical\nupper bound on model bias for when the model transition function is estimated\nwith i.i.d. trajectories. This bound broadens our understanding of the\nconditions under which model-based methods have high bias. Finally, we\nempirically evaluate our proposed methods and analyze the settings in which\ndifferent bootstrapping off-policy confidence interval methods succeed and\nfail.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:06:22 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 23:26:07 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 17:13:08 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Hanna", "Josiah P.", ""], ["Stone", "Peter", ""], ["Niekum", "Scott", ""]]}, {"id": "1606.06160", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low\n  Bitwidth Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DoReFa-Net, a method to train convolutional neural networks that\nhave low bitwidth weights and activations using low bitwidth parameter\ngradients. In particular, during backward pass, parameter gradients are\nstochastically quantized to low bitwidth numbers before being propagated to\nconvolutional layers. As convolutions during forward/backward passes can now\noperate on low bitwidth weights and activations/gradients respectively,\nDoReFa-Net can use bit convolution kernels to accelerate both training and\ninference. Moreover, as bit convolutions can be efficiently implemented on CPU,\nFPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low\nbitwidth neural network on these hardware. Our experiments on SVHN and ImageNet\ndatasets prove that DoReFa-Net can achieve comparable prediction accuracy as\n32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has\n1-bit weights, 2-bit activations, can be trained from scratch using 6-bit\ngradients to get 46.1\\% top-1 accuracy on ImageNet validation set. The\nDoReFa-Net AlexNet model is released publicly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 15:02:31 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 14:21:03 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 01:43:54 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Zhou", "Shuchang", ""], ["Wu", "Yuxin", ""], ["Ni", "Zekun", ""], ["Zhou", "Xinyu", ""], ["Wen", "He", ""], ["Zou", "Yuheng", ""]]}, {"id": "1606.06234", "submitter": "Chao Wang", "authors": "Maohua Zhu, Liu Liu, Chao Wang, Yuan Xie", "title": "CNNLab: a Novel Parallel Framework for Neural Networks using GPU and\n  FPGA-a Practical Study with Trade-off Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and implementing efficient, provably correct parallel neural\nnetwork processing is challenging. Existing high-level parallel abstractions\nlike MapReduce are insufficiently expressive while low-level tools like MPI and\nPthreads leave ML experts repeatedly solving the same design challenges.\nHowever, the diversity and large-scale data size have posed a significant\nchallenge to construct a flexible and high-performance implementation of deep\nlearning neural networks. To improve the performance and maintain the\nscalability, we present CNNLab, a novel deep learning framework using GPU and\nFPGA-based accelerators. CNNLab provides a uniform programming model to users\nso that the hardware implementation and the scheduling are invisible to the\nprogrammers. At runtime, CNNLab leverages the trade-offs between GPU and FPGA\nbefore offloading the tasks to the accelerators. Experimental results on the\nstate-of-the-art Nvidia K40 GPU and Altera DE5 FPGA board demonstrate that the\nCNNLab can provide a universal framework with efficient support for diverse\napplications without increasing the burden of the programmers. Moreover, we\nanalyze the detailed quantitative performance, throughput, power, energy, and\nperformance density for both approaches. Experimental results leverage the\ntrade-offs between GPU and FPGA and provide useful practical experiences for\nthe deep learning research community.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 18:22:09 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zhu", "Maohua", ""], ["Liu", "Liu", ""], ["Wang", "Chao", ""], ["Xie", "Yuan", ""]]}, {"id": "1606.06237", "submitter": "Yining Wang", "authors": "Yining Wang, Animashree Anandkumar", "title": "Online and Differentially-Private Tensor Decomposition", "comments": "19 pages, 9 figures. To appear at the 30th Annual Conference on\n  Advances in Neural Information Processing Systems (NIPS 2016), to be held at\n  Barcelona, Spain. Fix small typos in proofs of Lemmas C.5 and C.6", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we resolve many of the key algorithmic questions regarding\nrobustness, memory efficiency, and differential privacy of tensor\ndecomposition. We propose simple variants of the tensor power method which\nenjoy these strong properties. We present the first guarantees for online\ntensor power method which has a linear memory requirement. Moreover, we present\na noise calibrated tensor power method with efficient privacy guarantees. At\nthe heart of all these guarantees lies a careful perturbation analysis derived\nin this paper which improves up on the existing results significantly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 18:30:10 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 22:30:01 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 21:56:58 GMT"}, {"version": "v4", "created": "Thu, 15 Dec 2016 13:35:22 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Wang", "Yining", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1606.06244", "submitter": "Dylan Foster", "authors": "Dylan J. Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, Eva\n  Tardos", "title": "Learning in Games: Robustness of Fast Convergence", "comments": "27 pages. NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that learning algorithms satisfying a $\\textit{low approximate\nregret}$ property experience fast convergence to approximate optimality in a\nlarge class of repeated games. Our property, which simply requires that each\nlearner has small regret compared to a $(1+\\epsilon)$-multiplicative\napproximation to the best action in hindsight, is ubiquitous among learning\nalgorithms; it is satisfied even by the vanilla Hedge forecaster. Our results\nimprove upon recent work of Syrgkanis et al. [SALS15] in a number of ways. We\nrequire only that players observe payoffs under other players' realized\nactions, as opposed to expected payoffs. We further show that convergence\noccurs with high probability, and show convergence under bandit feedback.\nFinally, we improve upon the speed of convergence by a factor of $n$, the\nnumber of players. Both the scope of settings and the class of algorithms for\nwhich our analysis provides fast convergence are considerably broader than in\nprevious work.\n  Our framework applies to dynamic population games via a low approximate\nregret property for shifting experts. Here we strengthen the results of\nLykouris et al. [LST16] in two ways: We allow players to select learning\nalgorithms from a larger class, which includes a minor variant of the basic\nHedge algorithm, and we increase the maximum churn in players for which\napproximate optimality is achieved.\n  In the bandit setting we present a new algorithm which provides a \"small\nloss\"-type bound with improved dependence on the number of actions in utility\nsettings, and is both simple and efficient. This result may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 18:54:19 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 13:09:10 GMT"}, {"version": "v3", "created": "Wed, 16 Nov 2016 14:55:13 GMT"}, {"version": "v4", "created": "Fri, 16 Dec 2016 20:44:36 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Foster", "Dylan J.", ""], ["Li", "Zhiyuan", ""], ["Lykouris", "Thodoris", ""], ["Sridharan", "Karthik", ""], ["Tardos", "Eva", ""]]}, {"id": "1606.06250", "submitter": "Weiwei Pan", "authors": "Arjumand Masood and Weiwei Pan and Finale Doshi-Velez", "title": "An Empirical Comparison of Sampling Quality Metrics: A Case Study for\n  Bayesian Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we empirically explore the question: how can we assess the\nquality of samples from some target distribution? We assume that the samples\nare provided by some valid Monte Carlo procedure, so we are guaranteed that the\ncollection of samples will asymptotically approximate the true distribution.\nMost current evaluation approaches focus on two questions: (1) Has the chain\nmixed, that is, is it sampling from the distribution? and (2) How independent\nare the samples (as MCMC procedures produce correlated samples)? Focusing on\nthe case of Bayesian nonnegative matrix factorization, we empirically evaluate\nstandard metrics of sampler quality as well as propose new metrics to capture\naspects that these measures fail to expose. The aspect of sampling that is of\nparticular interest to us is the ability (or inability) of sampling methods to\nmove between multiple optima in NMF problems. As a proxy, we propose and study\na number of metrics that might quantify the diversity of a set of NMF\nfactorizations obtained by a sampler through quantifying the coverage of the\nposterior distribution. We compare the performance of a number of standard\nsampling methods for NMF in terms of these new metrics.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 18:59:34 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Masood", "Arjumand", ""], ["Pan", "Weiwei", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1606.06352", "submitter": "Brendan O'Connor", "authors": "Abram Handler, Su Lin Blodgett, Brendan O'Connor", "title": "Visualizing textual models with in-text and word-as-pixel highlighting", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore two techniques which use color to make sense of statistical text\nmodels. One method uses in-text annotations to illustrate a model's view of\nparticular tokens in particular documents. Another uses a high-level,\n\"words-as-pixels\" graphic to display an entire corpus. Together, these methods\noffer both zoomed-in and zoomed-out perspectives into a model's understanding\nof text. We show how these interconnected methods help diagnose a classifier's\npoor performance on Twitter slang, and make sense of a topic model on\nhistorical political texts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 22:30:19 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Handler", "Abram", ""], ["Blodgett", "Su Lin", ""], ["O'Connor", "Brendan", ""]]}, {"id": "1606.06357", "submitter": "Th\\'eo Trouillon", "authors": "Th\\'eo Trouillon, Johannes Welbl, Sebastian Riedel, \\'Eric Gaussier,\n  Guillaume Bouchard", "title": "Complex Embeddings for Simple Link Prediction", "comments": "10+2 pages, accepted at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical relational learning, the link prediction problem is key to\nautomatically understand the structure of large knowledge bases. As in previous\nstudies, we propose to solve this problem through latent factorization.\nHowever, here we make use of complex valued embeddings. The composition of\ncomplex embeddings can handle a large variety of binary relations, among them\nsymmetric and antisymmetric relations. Compared to state-of-the-art models such\nas Neural Tensor Network and Holographic Embeddings, our approach based on\ncomplex embeddings is arguably simpler, as it only uses the Hermitian dot\nproduct, the complex counterpart of the standard dot product between real\nvectors. Our approach is scalable to large datasets as it remains linear in\nboth space and time, while consistently outperforming alternative approaches on\nstandard link prediction benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 22:52:48 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Trouillon", "Th\u00e9o", ""], ["Welbl", "Johannes", ""], ["Riedel", "Sebastian", ""], ["Gaussier", "\u00c9ric", ""], ["Bouchard", "Guillaume", ""]]}, {"id": "1606.06361", "submitter": "Abulhair Saparov", "authors": "Abulhair Saparov, Tom M. Mitchell", "title": "A Probabilistic Generative Grammar for Semantic Parsing", "comments": "[manuscript draft]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a framework that couples the syntax and semantics of natural\nlanguage sentences in a generative model, in order to develop a semantic parser\nthat jointly infers the syntactic, morphological, and semantic representations\nof a given sentence under the guidance of background knowledge. To generate a\nsentence in our framework, a semantic statement is first sampled from a prior,\nsuch as from a set of beliefs in a knowledge base. Given this semantic\nstatement, a grammar probabilistically generates the output sentence. A joint\nsemantic-syntactic parser is derived that returns the $k$-best semantic and\nsyntactic parses for a given sentence. The semantic prior is flexible, and can\nbe used to incorporate background knowledge during parsing, in ways unlike\nprevious semantic parsing approaches. For example, semantic statements\ncorresponding to beliefs in a knowledge base can be given higher prior\nprobability, type-correct statements can be given somewhat lower probability,\nand beliefs outside the knowledge base can be given lower probability. The\nconstruction of our grammar invokes a novel application of hierarchical\nDirichlet processes (HDPs), which in turn, requires a novel and efficient\ninference approach. We present experimental results showing, for a simple\ngrammar, that our parser outperforms a state-of-the-art CCG semantic parser and\nscales to knowledge bases with millions of beliefs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 23:29:55 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Saparov", "Abulhair", ""], ["Mitchell", "Tom M.", ""]]}, {"id": "1606.06366", "submitter": "Bo Tang", "authors": "Bo Tang, Haibo He", "title": "FSMJ: Feature Selection with Maximum Jensen-Shannon Divergence for Text\n  Categorization", "comments": "8 pages, 6 figures, World Congress on Intelligent Control and\n  Automation, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new wrapper feature selection approach based on\nJensen-Shannon (JS) divergence, termed feature selection with maximum\nJS-divergence (FSMJ), for text categorization. Unlike most existing feature\nselection approaches, the proposed FSMJ approach is based on real-valued\nfeatures which provide more information for discrimination than binary-valued\nfeatures used in conventional approaches. We show that the FSMJ is a greedy\napproach and the JS-divergence monotonically increases when more features are\nselected. We conduct several experiments on real-life data sets, compared with\nthe state-of-the-art feature selection approaches for text categorization. The\nsuperior performance of the proposed FSMJ approach demonstrates its\neffectiveness and further indicates its wide potential applications on data\nmining.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 23:58:13 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Tang", "Bo", ""], ["He", "Haibo", ""]]}, {"id": "1606.06368", "submitter": "Fereshte Khani", "authors": "Fereshte Khani, Martin Rinard, Percy Liang", "title": "Unanimous Prediction for 100% Precision with Application to Learning\n  Semantic Mappings", "comments": "ACL 2016, Removed the duplicate author name of the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we train a system that, on any new input, either says \"don't know\" or\nmakes a prediction that is guaranteed to be correct? We answer the question in\nthe affirmative provided our model family is well-specified. Specifically, we\nintroduce the unanimity principle: only predict when all models consistent with\nthe training data predict the same output. We operationalize this principle for\nsemantic parsing, the task of mapping utterances to logical forms. We develop a\nsimple, efficient method that reasons over the infinite set of all consistent\nmodels by only checking two of the models. We prove that our method obtains\n100% precision even with a modest amount of training data from a possibly\nadversarial distribution. Empirically, we demonstrate the effectiveness of our\napproach on the standard GeoQuery dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 23:59:25 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 07:33:01 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Khani", "Fereshte", ""], ["Rinard", "Martin", ""], ["Liang", "Percy", ""]]}, {"id": "1606.06369", "submitter": "Annamalai Narayanan", "authors": "Annamalai Narayanan, Guozhu Meng, Liu Yang, Jinliang Liu and Lihui\n  Chen", "title": "Contextual Weisfeiler-Lehman Graph Kernel For Malware Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel graph kernel specifically to address a\nchallenging problem in the field of cyber-security, namely, malware detection.\nPrevious research has revealed the following: (1) Graph representations of\nprograms are ideally suited for malware detection as they are robust against\nseveral attacks, (2) Besides capturing topological neighbourhoods (i.e.,\nstructural information) from these graphs it is important to capture the\ncontext under which the neighbourhoods are reachable to accurately detect\nmalicious neighbourhoods.\n  We observe that state-of-the-art graph kernels, such as Weisfeiler-Lehman\nkernel (WLK) capture the structural information well but fail to capture\ncontextual information. To address this, we develop the Contextual\nWeisfeiler-Lehman kernel (CWLK) which is capable of capturing both these types\nof information. We show that for the malware detection problem, CWLK is more\nexpressive and hence more accurate than WLK while maintaining comparable\nefficiency. Through our large-scale experiments with more than 50,000\nreal-world Android apps, we demonstrate that CWLK outperforms two\nstate-of-the-art graph kernels (including WLK) and three malware detection\ntechniques by more than 5.27% and 4.87% F-measure, respectively, while\nmaintaining high efficiency. This high accuracy and efficiency make CWLK\nsuitable for large-scale real-world malware detection.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 00:02:45 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Narayanan", "Annamalai", ""], ["Meng", "Guozhu", ""], ["Yang", "Liu", ""], ["Liu", "Jinliang", ""], ["Chen", "Lihui", ""]]}, {"id": "1606.06377", "submitter": "Bo Tang", "authors": "Bo Tang, Paul M. Baggenstoss, Haibo He", "title": "Kernel-based Generative Learning in Distortion Feature Space", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel kernel-based generative classifier which is\ndefined in a distortion subspace using polynomial series expansion, named\nKernel-Distortion (KD) classifier. An iterative kernel selection algorithm is\ndeveloped to steadily improve classification performance by repeatedly removing\nand adding kernels. The experimental results on character recognition\napplication not only show that the proposed generative classifier performs\nbetter than many existing classifiers, but also illustrate that it has\ndifferent recognition capability compared to the state-of-the-art\ndiscriminative classifier - deep belief network. The recognition diversity\nindicates that a hybrid combination of the proposed generative classifier and\nthe discriminative classifier could further improve the classification\nperformance. Two hybrid combination methods, cascading and stacking, have been\nimplemented to verify the diversity and the improvement of the proposed\nclassifier.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 00:45:35 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Tang", "Bo", ""], ["Baggenstoss", "Paul M.", ""], ["He", "Haibo", ""]]}, {"id": "1606.06424", "submitter": "Tanmay Basu", "authors": "Tanmay Basu, Shraman Kumar, Abhishek Kalyan, Priyanka Jayaswal, Pawan\n  Goyal, Stephen Pettifer and Siddhartha R. Jonnalagadda", "title": "A Novel Framework to Expedite Systematic Reviews by Automatically\n  Building Information Extraction Training Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A systematic review identifies and collates various clinical studies and\ncompares data elements and results in order to provide an evidence based answer\nfor a particular clinical question. The process is manual and involves lot of\ntime. A tool to automate this process is lacking. The aim of this work is to\ndevelop a framework using natural language processing and machine learning to\nbuild information extraction algorithms to identify data elements in a new\nprimary publication, without having to go through the expensive task of manual\nannotation to build gold standards for each data element type. The system is\ndeveloped in two stages. Initially, it uses information contained in existing\nsystematic reviews to identify the sentences from the PDF files of the included\nreferences that contain specific data elements of interest using a modified\nJaccard similarity measure. These sentences have been treated as labeled data.A\nSupport Vector Machine (SVM) classifier is trained on this labeled data to\nextract data elements of interests from a new article. We conducted experiments\non Cochrane Database systematic reviews related to congestive heart failure\nusing inclusion criteria as an example data element. The empirical results show\nthat the proposed system automatically identifies sentences containing the data\nelement of interest with a high recall (93.75%) and reasonable precision\n(27.05% - which means the reviewers have to read only 3.7 sentences on\naverage). The empirical results suggest that the tool is retrieving valuable\ninformation from the reference articles, even when it is time-consuming to\nidentify them manually. Thus we hope that the tool will be useful for automatic\ndata extraction from biomedical research publications. The future scope of this\nwork is to generalize this information framework for all types of systematic\nreviews.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 04:56:33 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Basu", "Tanmay", ""], ["Kumar", "Shraman", ""], ["Kalyan", "Abhishek", ""], ["Jayaswal", "Priyanka", ""], ["Goyal", "Pawan", ""], ["Pettifer", "Stephen", ""], ["Jonnalagadda", "Siddhartha R.", ""]]}, {"id": "1606.06564", "submitter": "Alessandro Fontana", "authors": "Alessandro Fontana", "title": "An artificial neural network to find correlation patterns in an\n  arbitrary number of variables", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods to find correlation among variables are of interest to many\ndisciplines, including statistics, machine learning, (big) data mining and\nneurosciences. Parameters that measure correlation between two variables are of\nlimited utility when used with multiple variables. In this work, I propose a\nsimple criterion to measure correlation among an arbitrary number of variables,\nbased on a data set. The central idea is to i) design a function of the\nvariables that can take different forms depending on a set of parameters, ii)\ncalculate the difference between a statistics associated to the function\ncomputed on the data set and the same statistics computed on a randomised\nversion of the data set, called \"scrambled\" data set, and iii) optimise the\nparameters to maximise this difference. Many such functions can be organised in\nlayers, which can in turn be stacked one on top of the other, forming a neural\nnetwork. The function parameters are searched with an enhanced genetic\nalgortihm called POET and the resulting method is tested on a cancer gene data\nset. The method may have potential implications for some issues that affect the\nfield of neural networks, such as overfitting, the need to process huge amounts\nof data for training and the presence of \"adversarial examples\".\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 13:35:43 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 17:52:10 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Fontana", "Alessandro", ""]]}, {"id": "1606.06565", "submitter": "Jacob Steinhardt", "authors": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John\n  Schulman, Dan Man\\'e", "title": "Concrete Problems in AI Safety", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid progress in machine learning and artificial intelligence (AI) has\nbrought increasing attention to the potential impacts of AI technologies on\nsociety. In this paper we discuss one such potential impact: the problem of\naccidents in machine learning systems, defined as unintended and harmful\nbehavior that may emerge from poor design of real-world AI systems. We present\na list of five practical research problems related to accident risk,\ncategorized according to whether the problem originates from having the wrong\nobjective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an\nobjective function that is too expensive to evaluate frequently (\"scalable\nsupervision\"), or undesirable behavior during the learning process (\"safe\nexploration\" and \"distributional shift\"). We review previous work in these\nareas as well as suggesting research directions with a focus on relevance to\ncutting-edge AI systems. Finally, we consider the high-level question of how to\nthink most productively about the safety of forward-looking applications of AI.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 13:37:05 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 17:23:29 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Amodei", "Dario", ""], ["Olah", "Chris", ""], ["Steinhardt", "Jacob", ""], ["Christiano", "Paul", ""], ["Schulman", "John", ""], ["Man\u00e9", "Dan", ""]]}, {"id": "1606.06582", "submitter": "Yuting Zhang", "authors": "Yuting Zhang, Kibok Lee, Honglak Lee", "title": "Augmenting Supervised Neural Networks with Unsupervised Objectives for\n  Large-scale Image Classification", "comments": "International Conference on Machine Learning (ICML), 2016", "journal-ref": "PMLR 48:612-621, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning and supervised learning are key research topics in deep\nlearning. However, as high-capacity supervised neural networks trained with a\nlarge amount of labels have achieved remarkable success in many computer vision\ntasks, the availability of large-scale labeled images reduced the significance\nof unsupervised learning. Inspired by the recent trend toward revisiting the\nimportance of unsupervised learning, we investigate joint supervised and\nunsupervised learning in a large-scale setting by augmenting existing neural\nnetworks with decoding pathways for reconstruction. First, we demonstrate that\nthe intermediate activations of pretrained large-scale classification networks\npreserve almost all the information of input images except a portion of local\nspatial details. Then, by end-to-end training of the entire augmented\narchitecture with the reconstructive objective, we show improvement of the\nnetwork performance for supervised tasks. We evaluate several variants of\nautoencoders, including the recently proposed \"what-where\" autoencoder that\nuses the encoder pooling switches, to study the importance of the architecture\ndesign. Taking the 16-layer VGGNet trained under the ImageNet ILSVRC 2012\nprotocol as a strong baseline for image classification, our methods improve the\nvalidation-set accuracy by a noticeable margin.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 14:12:52 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Zhang", "Yuting", ""], ["Lee", "Kibok", ""], ["Lee", "Honglak", ""]]}, {"id": "1606.06588", "submitter": "Maximilian Karl", "authors": "Maximilian Karl, Artur Lohrer, Dhananjay Shah, Frederik Diehl, Max\n  Fiedler, Saahil Ognawala, Justin Bayer, Patrick van der Smagt", "title": "ML-based tactile sensor calibration: A universal approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the responses of two tactile sensors, the fingertip sensor from the\niCub and the BioTac under different external stimuli. The question of interest\nis to which degree both sensors i) allow the estimation of force exerted on the\nsensor and ii) enable the recognition of differing degrees of curvature. Making\nuse of a force controlled linear motor affecting the tactile sensors we acquire\nseveral high-quality data sets allowing the study of both sensors under exactly\nthe same conditions. We also examined the structure of the representation of\ntactile stimuli in the recorded tactile sensor data using t-SNE embeddings. The\nexperiments show that both the iCub and the BioTac excel in different settings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 14:23:20 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Karl", "Maximilian", ""], ["Lohrer", "Artur", ""], ["Shah", "Dhananjay", ""], ["Diehl", "Frederik", ""], ["Fiedler", "Max", ""], ["Ognawala", "Saahil", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1606.06622", "submitter": "Gordon Christie", "authors": "Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Batra, Devi Parikh", "title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise\n  Questions", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is the task of answering natural-language\nquestions about images. We introduce the novel problem of determining the\nrelevance of questions to images in VQA. Current VQA models do not reason about\nwhether a question is even related to the given image (e.g. What is the capital\nof Argentina?) or if it requires information from external resources to answer\ncorrectly. This can break the continuity of a dialogue in human-machine\ninteraction. Our approaches for determining relevance are composed of two\nstages. Given an image and a question, (1) we first determine whether the\nquestion is visual or not, (2) if visual, we determine whether the question is\nrelevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA\nmodel uncertainty, and caption-question similarity, are able to outperform\nstrong baselines on both relevance tasks. We also present human studies showing\nthat VQA models augmented with such question relevance reasoning are perceived\nas more intelligent, reasonable, and human-like.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:38:27 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 02:56:00 GMT"}, {"version": "v3", "created": "Mon, 26 Sep 2016 15:24:28 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Ray", "Arijit", ""], ["Christie", "Gordon", ""], ["Bansal", "Mohit", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1606.06630", "submitter": "Yuhuai(Tony) Wu", "authors": "Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio and Ruslan\n  Salakhutdinov", "title": "On Multiplicative Integration with Recurrent Neural Networks", "comments": "10 pages, 2 figures; To appear in NIPS2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general and simple structural design called Multiplicative\nIntegration (MI) to improve recurrent neural networks (RNNs). MI changes the\nway in which information from difference sources flows and is integrated in the\ncomputational building block of an RNN, while introducing almost no extra\nparameters. The new structure can be easily embedded into many popular RNN\nmodels, including LSTMs and GRUs. We empirically analyze its learning behaviour\nand conduct evaluations on several tasks using different RNN models. Our\nexperimental results demonstrate that Multiplicative Integration can provide a\nsubstantial performance boost over many of the existing RNN models.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:55:29 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 19:47:10 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wu", "Yuhuai", ""], ["Zhang", "Saizheng", ""], ["Zhang", "Ying", ""], ["Bengio", "Yoshua", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1606.06653", "submitter": "Francesco Grassi", "authors": "Francesco Grassi, Nathanael Perraudin, Benjamin Ricaud", "title": "Tracking Time-Vertex Propagation using Dynamic Graph Wavelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Signal Processing generalizes classical signal processing to signal or\ndata indexed by the vertices of a weighted graph. So far, the research efforts\nhave been focused on static graph signals. However numerous applications\ninvolve graph signals evolving in time, such as spreading or propagation of\nwaves on a network. The analysis of this type of data requires a new set of\nmethods that fully takes into account the time and graph dimensions. We propose\na novel class of wavelet frames named Dynamic Graph Wavelets, whose time-vertex\nevolution follows a dynamic process. We demonstrate that this set of functions\ncan be combined with sparsity based approaches such as compressive sensing to\nreveal information on the dynamic processes occurring on a graph. Experiments\non real seismological data show the efficiency of the technique, allowing to\nestimate the epicenter of earthquake events recorded by a seismic network.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 16:48:25 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Grassi", "Francesco", ""], ["Perraudin", "Nathanael", ""], ["Ricaud", "Benjamin", ""]]}, {"id": "1606.06771", "submitter": "Jeffrey Pawlick", "authors": "Jeffrey Pawlick and Quanyan Zhu", "title": "A Stackelberg Game Perspective on the Conflict Between Machine Learning\n  and Data Obfuscation", "comments": "This conference paper was not accepted. It has been withdrawn because\n  it was subsequently revised and appears here arXiv:1608.02546", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is the new oil; this refrain is repeated extensively in the age of\ninternet tracking, machine learning, and data analytics. Social network\nanalysis, cookie-based advertising, and government surveillance are all\nevidence of the use of data for commercial and national interests. Public\npressure, however, is mounting for the protection of privacy. Frameworks such\nas differential privacy offer machine learning algorithms methods to guarantee\nlimits to information disclosure, but they are seldom implemented. Recently,\nhowever, developers have made significant efforts to undermine tracking through\nobfuscation tools that hide user characteristics in a sea of noise. These\nservices highlight an emerging clash between tracking and data obfuscation. In\nthis paper, we conceptualize this conflict through a dynamic game between users\nand a machine learning algorithm that uses empirical risk minimization. First,\na machine learner declares a privacy protection level, and then users respond\nby choosing their own perturbation amounts. We study the interaction between\nthe users and the learner using a Stackelberg game. The utility functions\nquantify accuracy using expected loss and privacy in terms of the bounds of\ndifferential privacy. In equilibrium, we find selfish users tend to cause\nsignificant utility loss to trackers by perturbing heavily, in a phenomenon\nreminiscent of public good games. Trackers, however, can improve the balance by\nproactively perturbing the data themselves. While other work in this area has\nstudied privacy markets and mechanism design for truthful reporting of user\ninformation, we take a different viewpoint by considering both user and learner\nperturbation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 21:14:48 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 17:11:49 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Pawlick", "Jeffrey", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1606.06793", "submitter": "Vu Nguyen", "authors": "Trung Le, Khanh Nguyen, Van Nguyen, Vu Nguyen, Dinh Phung", "title": "Scalable Semi-supervised Learning with Graph-based Kernel Machine", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring labels are often costly, whereas unlabeled data are usually easy to\nobtain in modern machine learning applications. Semi-supervised learning\nprovides a principled machine learning framework to address such situations,\nand has been applied successfully in many real-word applications and\nindustries. Nonetheless, most of existing semi-supervised learning methods\nencounter two serious limitations when applied to modern and large-scale\ndatasets: computational burden and memory usage demand. To this end, we present\nin this paper the Graph-based semi-supervised Kernel Machine (GKM), a method\nthat leverages the generalization ability of kernel-based method with the\ngeometrical and distributive information formulated through a spectral graph\ninduced from data for semi-supervised learning purpose. Our proposed GKM can be\nsolved directly in the primal form using the Stochastic Gradient Descent method\nwith the ideal convergence rate $O(\\frac{1}{T})$. Besides, our formulation is\nsuitable for a wide spectrum of important loss functions in the literature of\nmachine learning (e.g., Hinge, smooth Hinge, Logistic, L1, and\n{\\epsilon}-insensitive) and smoothness functions (i.e., $l_p(t) = |t|^p$ with\n$p\\ge1$). We further show that the well-known Laplacian Support Vector Machine\nis a special case of our formulation. We validate our proposed method on\nseveral benchmark datasets to demonstrate that GKM is appropriate for the\nlarge-scale datasets since it is optimal in memory usage and yields superior\nclassification accuracy whilst simultaneously achieving a significant\ncomputation speed-up in comparison with the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 00:26:59 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 02:09:35 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 02:40:23 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Le", "Trung", ""], ["Nguyen", "Khanh", ""], ["Nguyen", "Van", ""], ["Nguyen", "Vu", ""], ["Phung", "Dinh", ""]]}, {"id": "1606.06812", "submitter": "Dong Hao", "authors": "Ratha Pech, Dong Hao, Liming Pan, Hong Cheng and Tao Zhou", "title": "Link Prediction via Matrix Completion", "comments": null, "journal-ref": null, "doi": "10.1209/0295-5075/117/38002", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by practical importance of social networks, economic networks,\nbiological networks and so on, studies on large and complex networks have\nattracted a surge of attentions in the recent years. Link prediction is a\nfundamental issue to understand the mechanisms by which new links are added to\nthe networks. We introduce the method of robust principal component analysis\n(robust PCA) into link prediction, and estimate the missing entries of the\nadjacency matrix. On one hand, our algorithm is based on the sparsity and low\nrank property of the matrix, on the other hand, it also performs very well when\nthe network is dense. This is because a relatively dense real network is also\nsparse in comparison to the complete graph. According to extensive experiments\non real networks from disparate fields, when the target network is connected\nand sufficiently dense, whatever it is weighted or unweighted, our method is\ndemonstrated to be very effective and with prediction accuracy being\nconsiderably improved comparing with many state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 03:55:38 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 02:58:40 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Pech", "Ratha", ""], ["Hao", "Dong", ""], ["Pan", "Liming", ""], ["Cheng", "Hong", ""], ["Zhou", "Tao", ""]]}, {"id": "1606.06864", "submitter": "Stefan Braun", "authors": "Stefan Braun, Daniel Neil, Shih-Chii Liu", "title": "A Curriculum Learning Method for Improved Noise Robustness in Automatic\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of automatic speech recognition systems under noisy\nenvironments still leaves room for improvement. Speech enhancement or feature\nenhancement techniques for increasing noise robustness of these systems usually\nadd components to the recognition system that need careful optimization. In\nthis work, we propose the use of a relatively simple curriculum training\nstrategy called accordion annealing (ACCAN). It uses a multi-stage training\nschedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are\nfirst added and samples at increasing higher SNR values are gradually added up\nto an SNR value of 50dB. We also use a method called per-epoch noise mixing\n(PEM) that generates noisy training samples online during training and thus\nenables dynamically changing the SNR of our training data. Both the ACCAN and\nthe PEM methods are evaluated on a end-to-end speech recognition pipeline on\nthe Wall Street Journal corpus. ACCAN decreases the average word error rate\n(WER) on the 20dB to -10dB SNR range by up to 31.4% when compared to a\nconventional multi-condition training method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 09:29:40 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 15:20:39 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Braun", "Stefan", ""], ["Neil", "Daniel", ""], ["Liu", "Shih-Chii", ""]]}, {"id": "1606.06871", "submitter": "Albert Zeyer", "authors": "Albert Zeyer, Patrick Doetsch, Paul Voigtlaender, Ralf Schl\\\"uter,\n  Hermann Ney", "title": "A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic\n  Modeling in Speech Recognition", "comments": "published on ICASSP 2017 conference, New Orleans, USA", "journal-ref": null, "doi": "10.1109/ICASSP.2017.7952599", "report-no": null, "categories": "cs.NE cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study of deep bidirectional long short-term memory\n(LSTM) recurrent neural network (RNN) based acoustic models for automatic\nspeech recognition (ASR). We study the effect of size and depth and train\nmodels of up to 8 layers. We investigate the training aspect and study\ndifferent variants of optimization methods, batching, truncated\nbackpropagation, different regularization techniques such as dropout and $L_2$\nregularization, and different gradient clipping variants.\n  The major part of the experimental analysis was performed on the Quaero\ncorpus. Additional experiments also were performed on the Switchboard corpus.\nOur best LSTM model has a relative improvement in word error rate of over 14\\%\ncompared to our best feed-forward neural network (FFNN) baseline on the Quaero\ntask. On this task, we get our best result with an 8 layer bidirectional LSTM\nand we show that a pretraining scheme with layer-wise construction helps for\ndeep LSTMs.\n  Finally we compare the training calculation time of many of the presented\nexperiments in relation with recognition performance.\n  All the experiments were done with RETURNN, the RWTH extensible training\nframework for universal recurrent neural networks in combination with RASR, the\nRWTH ASR toolkit.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 10:00:14 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 08:08:29 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zeyer", "Albert", ""], ["Doetsch", "Patrick", ""], ["Voigtlaender", "Paul", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "1606.06950", "submitter": "Herman Kamper", "authors": "Herman Kamper, Aren Jansen, Sharon Goldwater", "title": "A segmental framework for fully-unsupervised large-vocabulary speech\n  recognition", "comments": "15 pages, 6 figures, 8 tables", "journal-ref": "Comput. Speech Lang. 46 (2017) 154-174", "doi": "10.1016/j.csl.2017.04.008", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-resource speech technology is a growing research area that aims to\ndevelop methods for speech processing in the absence of transcriptions,\nlexicons, or language modelling text. Early term discovery systems focused on\nidentifying isolated recurring patterns in a corpus, while more recent\nfull-coverage systems attempt to completely segment and cluster the audio into\nword-like units---effectively performing unsupervised speech recognition. This\narticle presents the first attempt we are aware of to apply such a system to\nlarge-vocabulary multi-speaker data. Our system uses a Bayesian modelling\nframework with segmental word representations: each word segment is represented\nas a fixed-dimensional acoustic embedding obtained by mapping the sequence of\nfeature frames to a single embedding vector. We compare our system on English\nand Xitsonga datasets to state-of-the-art baselines, using a variety of\nmeasures including word error rate (obtained by mapping the unsupervised output\nto ground truth transcriptions). Very high word error rates are reported---in\nthe order of 70--80% for speaker-dependent and 80--95% for speaker-independent\nsystems---highlighting the difficulty of this task. Nevertheless, in terms of\ncluster quality and word segmentation metrics, we show that by imposing a\nconsistent top-down segmentation while also using bottom-up knowledge from\ndetected syllable boundaries, both single-speaker and multi-speaker versions of\nour system outperform a purely bottom-up single-speaker syllable-based\napproach. We also show that the discovered clusters can be made less speaker-\nand gender-specific by using an unsupervised autoencoder-like feature extractor\nto learn better frame-level features (prior to embedding). Our system's\ndiscovered clusters are still less pure than those of unsupervised term\ndiscovery systems, but provide far greater coverage.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 13:51:57 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 09:36:02 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kamper", "Herman", ""], ["Jansen", "Aren", ""], ["Goldwater", "Sharon", ""]]}, {"id": "1606.06962", "submitter": "Andreas Loukas", "authors": "Nathanael Perraudin and Andreas Loukas and Francesco Grassi and Pierre\n  Vandergheynst", "title": "Towards stationary time-vertex signal processing", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods for signal processing have shown promise for the analysis\nof data exhibiting irregular structure, such as those found in social,\ntransportation, and sensor networks. Yet, though these systems are often\ndynamic, state-of-the-art methods for signal processing on graphs ignore the\ndimension of time, treating successive graph signals independently or taking a\nglobal average. To address this shortcoming, this paper considers the\nstatistical analysis of time-varying graph signals. We introduce a novel\ndefinition of joint (time-vertex) stationarity, which generalizes the classical\ndefinition of time stationarity and the more recent definition appropriate for\ngraphs. Joint stationarity gives rise to a scalable Wiener optimization\nframework for joint denoising, semi-supervised learning, or more generally\ninversing a linear operator, that is provably optimal. Experimental results on\nreal weather data demonstrate that taking into account graph and time\ndimensions jointly can yield significant accuracy improvements in the\nreconstruction effort.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 14:33:15 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Perraudin", "Nathanael", ""], ["Loukas", "Andreas", ""], ["Grassi", "Francesco", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1606.07035", "submitter": "Sara Magliacane", "authors": "Sara Magliacane, Tom Claassen, Joris M. Mooij", "title": "Ancestral Causal Inference", "comments": "In Proceedings of Advances in Neural Information Processing Systems\n  29 (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-based causal discovery from limited data is a notoriously\ndifficult challenge due to the many borderline independence test decisions.\nSeveral approaches to improve the reliability of the predictions by exploiting\nredundancy in the independence information have been proposed recently. Though\npromising, existing approaches can still be greatly improved in terms of\naccuracy and scalability. We present a novel method that reduces the\ncombinatorial explosion of the search space by using a more coarse-grained\nrepresentation of causal information, drastically reducing computation time.\nAdditionally, we propose a method to score causal predictions based on their\nconfidence. Crucially, our implementation also allows one to easily combine\nobservational and interventional data and to incorporate various types of\navailable background knowledge. We prove soundness and asymptotic consistency\nof our method and demonstrate that it can outperform the state-of-the-art on\nsynthetic data, achieving a speedup of several orders of magnitude. We\nillustrate its practical feasibility by applying it on a challenging protein\ndata set.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 18:26:27 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 22:23:32 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2017 14:26:27 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Magliacane", "Sara", ""], ["Claassen", "Tom", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1606.07043", "submitter": "David Kale", "authors": "Kyle Reing, David C. Kale, Greg Ver Steeg, Aram Galstyan", "title": "Toward Interpretable Topic Discovery via Anchored Correlation\n  Explanation", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many predictive tasks, such as diagnosing a patient based on their medical\nchart, are ultimately defined by the decisions of human experts. Unfortunately,\nencoding experts' knowledge is often time consuming and expensive. We propose a\nsimple way to use fuzzy and informal knowledge from experts to guide discovery\nof interpretable latent topics in text. The underlying intuition of our\napproach is that latent factors should be informative about both correlations\nin the data and a set of relevance variables specified by an expert.\nMathematically, this approach is a combination of the information bottleneck\nand Total Correlation Explanation (CorEx). We give a preliminary evaluation of\nAnchored CorEx, showing that it produces more coherent and interpretable topics\non two distinct corpora.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 19:00:38 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Reing", "Kyle", ""], ["Kale", "David C.", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1606.07081", "submitter": "Lalit Jain", "authors": "Lalit Jain, Kevin Jamieson, Robert Nowak", "title": "Finite Sample Prediction and Recovery Bounds for Ordinal Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of ordinal embedding is to represent items as points in a\nlow-dimensional Euclidean space given a set of constraints in the form of\ndistance comparisons like \"item $i$ is closer to item $j$ than item $k$\".\nOrdinal constraints like this often come from human judgments. To account for\nerrors and variation in judgments, we consider the noisy situation in which the\ngiven constraints are independently corrupted by reversing the correct\nconstraint with some probability. This paper makes several new contributions to\nthis problem. First, we derive prediction error bounds for ordinal embedding\nwith noise by exploiting the fact that the rank of a distance matrix of points\nin $\\mathbb{R}^d$ is at most $d+2$. These bounds characterize how well a\nlearned embedding predicts new comparative judgments. Second, we investigate\nthe special case of a known noise model and study the Maximum Likelihood\nestimator. Third, knowledge of the noise model enables us to relate prediction\nerrors to embedding accuracy. This relationship is highly non-trivial since we\nshow that the linear map corresponding to distance comparisons is\nnon-invertible, but there exists a nonlinear map that is invertible. Fourth,\ntwo new algorithms for ordinal embedding are proposed and evaluated in\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 20:06:10 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Jain", "Lalit", ""], ["Jamieson", "Kevin", ""], ["Nowak", "Robert", ""]]}, {"id": "1606.07104", "submitter": "Barak Sober", "authors": "Barak Sober and David Levin", "title": "Manifold Approximation by Moving Least-Squares Projection (MMLS)", "comments": null, "journal-ref": null, "doi": "10.1007/s00365-019-09489-8", "report-no": null, "categories": "cs.GR cs.LG math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to avoid the curse of dimensionality, frequently encountered in Big\nData analysis, there was a vast development in the field of linear and\nnonlinear dimension reduction techniques in recent years. These techniques\n(sometimes referred to as manifold learning) assume that the scattered input\ndata is lying on a lower dimensional manifold, thus the high dimensionality\nproblem can be overcome by learning the lower dimensionality behavior. However,\nin real life applications, data is often very noisy. In this work, we propose a\nmethod to approximate $\\mathcal{M}$ a $d$-dimensional $C^{m+1}$ smooth\nsubmanifold of $\\mathbb{R}^n$ ($d \\ll n$) based upon noisy scattered data\npoints (i.e., a data cloud). We assume that the data points are located \"near\"\nthe lower dimensional manifold and suggest a non-linear moving least-squares\nprojection on an approximating $d$-dimensional manifold. Under some mild\nassumptions, the resulting approximant is shown to be infinitely smooth and of\nhigh approximation order (i.e., $O(h^{m+1})$, where $h$ is the fill distance\nand $m$ is the degree of the local polynomial approximation). The method\npresented here assumes no analytic knowledge of the approximated manifold and\nthe approximation algorithm is linear in the large dimension $n$. Furthermore,\nthe approximating manifold can serve as a framework to perform operations\ndirectly on the high dimensional data in a computationally efficient manner.\nThis way, the preparatory step of dimension reduction, which induces\ndistortions to the data, can be avoided altogether.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 20:59:12 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 19:21:46 GMT"}, {"version": "v3", "created": "Thu, 2 Nov 2017 21:18:02 GMT"}, {"version": "v4", "created": "Tue, 16 Jan 2018 16:33:45 GMT"}, {"version": "v5", "created": "Thu, 7 Mar 2019 07:58:12 GMT"}, {"version": "v6", "created": "Thu, 16 Jan 2020 00:52:19 GMT"}, {"version": "v7", "created": "Wed, 26 Feb 2020 16:06:19 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Sober", "Barak", ""], ["Levin", "David", ""]]}, {"id": "1606.07112", "submitter": "Tom Zahavy", "authors": "Nir Ben Zrihem, Tom Zahavy, Shie Mannor", "title": "Visualizing Dynamics: from t-SNE to SEMI-MDPs", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) is a trending field of research, showing\ngreat promise in many challenging problems such as playing Atari, solving Go\nand controlling robots. While DRL agents perform well in practice we are still\nmissing the tools to analayze their performance and visualize the temporal\nabstractions that they learn. In this paper, we present a novel method that\nautomatically discovers an internal Semi Markov Decision Process (SMDP) model\nin the Deep Q Network's (DQN) learned representation. We suggest a novel\nvisualization method that represents the SMDP model by a directed graph and\nvisualize it above a t-SNE map. We show how can we interpret the agent's policy\nand give evidence for the hierarchical state aggregation that DQNs are learning\nautomatically. Our algorithm is fully automatic, does not require any domain\nspecific knowledge and is evaluated by a novel likelihood based evaluation\ncriteria.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 21:18:50 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Zrihem", "Nir Ben", ""], ["Zahavy", "Tom", ""], ["Mannor", "Shie", ""]]}, {"id": "1606.07129", "submitter": "Behnoush Abdollahi", "authors": "Behnoush Abdollahi, Olfa Nasraoui", "title": "Explainable Restricted Boltzmann Machines for Collaborative Filtering", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most accurate recommender systems are black-box models, hiding the reasoning\nbehind their recommendations. Yet explanations have been shown to increase the\nuser's trust in the system in addition to providing other benefits such as\nscrutability, meaning the ability to verify the validity of recommendations.\nThis gap between accuracy and transparency or explainability has generated an\ninterest in automated explanation generation methods. Restricted Boltzmann\nMachines (RBM) are accurate models for CF that also lack interpretability. In\nthis paper, we focus on RBM based collaborative filtering recommendations, and\nfurther assume the absence of any additional data source, such as item content\nor user attributes. We thus propose a new Explainable RBM technique that\ncomputes the top-n recommendation list from items that are explainable.\nExperimental results show that our method is effective in generating accurate\nand explainable recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 22:24:30 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Abdollahi", "Behnoush", ""], ["Nasraoui", "Olfa", ""]]}, {"id": "1606.07149", "submitter": "Ivo Bukovsky Ph.D.", "authors": "Ivo Bukovsky and Noriyasu Homma", "title": "An Approach to Stable Gradient Descent Adaptation of Higher-Order Neural\n  Units", "comments": "2016, 13 pages", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems,ISSN:\n  2162-237X,2016", "doi": "10.1109/TNNLS.2016.2572310", "report-no": null, "categories": "cs.NE cs.AI cs.CE cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability evaluation of a weight-update system of higher-order neural units\n(HONUs) with polynomial aggregation of neural inputs (also known as classes of\npolynomial neural networks) for adaptation of both feedforward and recurrent\nHONUs by a gradient descent method is introduced. An essential core of the\napproach is based on spectral radius of a weight-update system, and it allows\nstability monitoring and its maintenance at every adaptation step individually.\nAssuring stability of the weight-update system (at every single adaptation\nstep) naturally results in adaptation stability of the whole neural\narchitecture that adapts to target data. As an aside, the used approach\nhighlights the fact that the weight optimization of HONU is a linear problem,\nso the proposed approach can be generally extended to any neural architecture\nthat is linear in its adaptable parameters.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 01:07:27 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Bukovsky", "Ivo", ""], ["Homma", "Noriyasu", ""]]}, {"id": "1606.07150", "submitter": "Annamalai Narayanan", "authors": "Annamalai Narayanan, Liu Yang, Lihui Chen and Liu Jinliang", "title": "Adaptive and Scalable Android Malware Detection through Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that malware constantly evolves so as to evade detection and\nthis causes the entire malware population to be non-stationary. Contrary to\nthis fact, prior works on machine learning based Android malware detection have\nassumed that the distribution of the observed malware characteristics (i.e.,\nfeatures) do not change over time. In this work, we address the problem of\nmalware population drift and propose a novel online machine learning based\nframework, named DroidOL to handle it and effectively detect malware. In order\nto perform accurate detection, security-sensitive behaviors are captured from\napps in the form of inter-procedural control-flow sub-graph features using a\nstate-of-the-art graph kernel. In order to perform scalable detection and to\nadapt to the drift and evolution in malware population, an online\npassive-aggressive classifier is used.\n  In a large-scale comparative analysis with more than 87,000 apps, DroidOL\nachieves 84.29% accuracy outperforming two state-of-the-art malware techniques\nby more than 20% in their typical batch learning setting and more than 3% when\nthey are continuously re-trained. Our experimental findings strongly indicate\nthat online learning based approaches are highly suitable for real-world\nmalware detection.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 01:08:10 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 10:07:11 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Narayanan", "Annamalai", ""], ["Yang", "Liu", ""], ["Chen", "Lihui", ""], ["Jinliang", "Liu", ""]]}, {"id": "1606.07163", "submitter": "William Souillard-Mandar", "authors": "William Souillard-Mandar, Randall Davis, Cynthia Rudin, Rhoda Au, Dana\n  Penney", "title": "Interpretable Machine Learning Models for the Digital Clock Drawing Test", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular\nneuropsychological screening tool for cognitive conditions. The Digital Clock\nDrawing Test (dCDT) uses novel software to analyze data from a digitizing\nballpoint pen that reports its position with considerable spatial and temporal\nprecision, making possible the analysis of both the drawing process and final\nproduct. We developed methodology to analyze pen stroke data from these\ndrawings, and computed a large collection of features which were then analyzed\nwith a variety of machine learning techniques. The resulting scoring systems\nwere designed to be more accurate than the systems currently used by\nclinicians, but just as interpretable and easy to use. The systems also allow\nus to quantify the tradeoff between accuracy and interpretability. We created\nautomated versions of the CDT scoring systems currently used by clinicians,\nallowing us to benchmark our models, which indicated that our machine learning\nmodels substantially outperformed the existing scoring systems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 02:08:58 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Souillard-Mandar", "William", ""], ["Davis", "Randall", ""], ["Rudin", "Cynthia", ""], ["Au", "Rhoda", ""], ["Penney", "Dana", ""]]}, {"id": "1606.07219", "submitter": "Nattiya Kanhabua Dr.", "authors": "Nattiya Kanhabua and Huamin Ren and Thomas B. Moeslund", "title": "Learning Dynamic Classes of Events using Stacked Multilayer Perceptron\n  Networks", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, 6 pages, 4\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often use a web search engine to find information about events of\ninterest, for example, sport competitions, political elections, festivals and\nentertainment news. In this paper, we study a problem of detecting\nevent-related queries, which is the first step before selecting a suitable\ntime-aware retrieval model. In general, event-related information needs can be\nobserved in query streams through various temporal patterns of user search\nbehavior, e.g., spiky peaks for popular events, and periodicities for\nrepetitive events. However, it is also common that users search for non-popular\nevents, which may not exhibit temporal variations in query streams, e.g., past\nevents recently occurred, historical events triggered by anniversaries or\nsimilar events, and future events anticipated to happen. To address the\nchallenge of detecting dynamic classes of events, we propose a novel deep\nlearning model to classify a given query into a predetermined set of multiple\nevent types. Our proposed model, a Stacked Multilayer Perceptron (S-MLP)\nnetwork, consists of multilayer perceptron used as a basic learning unit. We\nassemble stacked units to further learn complex relationships between neutrons\nin successive layers. To evaluate our proposed model, we conduct experiments\nusing real-world queries and a set of manually created ground truth.\nPreliminary results have shown that our proposed deep learning model\noutperforms the state-of-the-art classification models significantly.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 08:16:38 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 21:53:43 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kanhabua", "Nattiya", ""], ["Ren", "Huamin", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "1606.07230", "submitter": "Ziwei Liu", "authors": "Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Deep Learning Markov Random Field for Semantic Segmentation", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), 2017. Extended version of our previous ICCV 2015 paper\n  (arXiv:1509.02634)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation tasks can be well modeled by Markov Random Field (MRF).\nThis paper addresses semantic segmentation by incorporating high-order\nrelations and mixture of label contexts into MRF. Unlike previous works that\noptimized MRFs using iterative algorithm, we solve MRF by proposing a\nConvolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which\nenables deterministic end-to-end computation in a single forward pass.\nSpecifically, DPN extends a contemporary CNN to model unary terms and\nadditional layers are devised to approximate the mean field (MF) algorithm for\npairwise terms. It has several appealing properties. First, different from the\nrecent works that required many iterations of MF during back-propagation, DPN\nis able to achieve high performance by approximating one iteration of MF.\nSecond, DPN represents various types of pairwise terms, making many existing\nmodels as its special cases. Furthermore, pairwise terms in DPN provide a\nunified framework to encode rich contextual information in high-dimensional\ndata, such as images and videos. Third, DPN makes MF easier to be parallelized\nand speeded up, thus enabling efficient inference. DPN is thoroughly evaluated\non standard semantic image/video segmentation benchmarks, where a single DPN\nmodel yields state-of-the-art segmentation accuracies on PASCAL VOC 2012,\nCityscapes dataset and CamVid dataset.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 08:52:39 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 09:24:18 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Liu", "Ziwei", ""], ["Li", "Xiaoxiao", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1606.07251", "submitter": "Florian Colombo", "authors": "Florian Colombo, Samuel P. Muscinelli, Alexander Seeholzer, Johanni\n  Brea and Wulfram Gerstner", "title": "Algorithmic Composition of Melodies with Deep Recurrent Neural Networks", "comments": "Proceeding of the 1st Conference on Computer Simulation of Musical\n  Creativity, Huddersfield University", "journal-ref": null, "doi": "10.13140/RG.2.1.2436.5683", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A big challenge in algorithmic composition is to devise a model that is both\neasily trainable and able to reproduce the long-range temporal dependencies\ntypical of music. Here we investigate how artificial neural networks can be\ntrained on a large corpus of melodies and turned into automated music composers\nable to generate new melodies coherent with the style they have been trained\non. We employ gated recurrent unit networks that have been shown to be\nparticularly efficient in learning complex sequential activations with\narbitrary long time lags. Our model processes rhythm and melody in parallel\nwhile modeling the relation between these two features. Using such an approach,\nwe were able to generate interesting complete melodies or suggest possible\ncontinuations of a melody fragment that is coherent with the characteristics of\nthe fragment itself.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 09:53:30 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Colombo", "Florian", ""], ["Muscinelli", "Samuel P.", ""], ["Seeholzer", "Alexander", ""], ["Brea", "Johanni", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "1606.07262", "submitter": "Ofer Shir", "authors": "Ofer M. Shir, Jonathan Roslund and Amir Yehudayoff", "title": "On the Theoretical Capacity of Evolution Strategies to Statistically\n  Learn the Landscape Hessian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the theoretical capacity to statistically learn local landscape\ninformation by Evolution Strategies (ESs). Specifically, we investigate the\ncovariance matrix when constructed by ESs operating with the selection operator\nalone. We model continuous generation of candidate solutions about quadratic\nbasins of attraction, with deterministic selection of the decision vectors that\nminimize the objective function values. Our goal is to rigorously show that\naccumulation of winning individuals carries the potential to reveal valuable\ninformation about the search landscape, e.g., as already practically utilized\nby derandomized ES variants. We first show that the statistically-constructed\ncovariance matrix over such winning decision vectors shares the same\neigenvectors with the Hessian matrix about the optimum. We then provide an\nanalytic approximation of this covariance matrix for a non-elitist multi-child\n$(1,\\lambda)$-strategy, which holds for a large population size $\\lambda$.\nFinally, we also numerically corroborate our results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 10:38:49 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Shir", "Ofer M.", ""], ["Roslund", "Jonathan", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1606.07279", "submitter": "Remi Flamary", "authors": "Devis Tuia, R\\'emi Flamary, Nicolas Courty", "title": "Multiclass feature learning for hyperspectral image classification:\n  sparse and hierarchical solutions", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, Volume 105,\n  July 2015, Pages 272-285", "doi": "10.1016/j.isprsjprs.2015.01.006", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we tackle the question of discovering an effective set of\nspatial filters to solve hyperspectral classification problems. Instead of\nfixing a priori the filters and their parameters using expert knowledge, we let\nthe model find them within random draws in the (possibly infinite) space of\npossible filters. We define an active set feature learner that includes in the\nmodel only features that improve the classifier. To this end, we consider a\nfast and linear classifier, multiclass logistic classification, and show that\nwith a good representation (the filters discovered), such a simple classifier\ncan reach at least state of the art performances. We apply the proposed active\nset learner in four hyperspectral image classification problems, including\nagricultural and urban classification at different resolutions, as well as\nmultimodal data. We also propose a hierarchical setting, which allows to\ngenerate more complex banks of features that can better describe the\nnonlinearities present in the data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:05:23 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Tuia", "Devis", ""], ["Flamary", "R\u00e9mi", ""], ["Courty", "Nicolas", ""]]}, {"id": "1606.07283", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Reinder Haakma, Wil M. P. van der Aalst", "title": "Event Abstraction for Process Mining using Supervised Learning\n  Techniques", "comments": "paper to appear in the proceedings of the SAI Intelligent Systems\n  Conference 2016", "journal-ref": "Lecture Notes in Networks and Systems, 15 (2017), 251-269", "doi": "10.1007/978-3-319-56994-9_18", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining techniques focus on extracting insight in processes from event\nlogs. In many cases, events recorded in the event log are too fine-grained,\ncausing process discovery algorithms to discover incomprehensible process\nmodels or process models that are not representative of the event log. We show\nthat when process discovery algorithms are only able to discover an\nunrepresentative process model from a low-level event log, structure in the\nprocess can in some cases still be discovered by first abstracting the event\nlog to a higher level of granularity. This gives rise to the challenge to\nbridge the gap between an original low-level event log and a desired high-level\nperspective on this log, such that a more structured or more comprehensible\nprocess model can be discovered. We show that supervised learning can be\nleveraged for the event abstraction task when annotations with high-level\ninterpretations of the low-level events are available for a subset of the\nsequences (i.e., traces). We present a method to generate feature vector\nrepresentations of events based on XES extensions, and describe an approach to\nabstract events in an event log with Condition Random Fields using these event\nfeatures. Furthermore, we propose a sequence-focused metric to evaluate\nsupervised event abstraction results that fits closely to the tasks of process\ndiscovery and conformance checking. We conclude this paper by demonstrating the\nusefulness of supervised event abstraction for obtaining more structured and/or\nmore comprehensible process models using both real life event data and\nsynthetic event data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:12:45 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["Haakma", "Reinder", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1606.07286", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary (LAGRANGE, OCA), Alain Rakotomamonjy (LITIS), Gilles\n  Gasso (LITIS)", "title": "Importance sampling strategy for non-convex randomized block-coordinate\n  descent", "comments": null, "journal-ref": "IEEE INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN\n  MULTI-SENSOR ADAPTIVE PROCESSING, Dec 2015, Cancun, Mexico. 2015", "doi": "10.1109/CAMSAP.2015.7383796", "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of samples and dimensionality of optimization problems related\nto statistics an machine learning explode, block coordinate descent algorithms\nhave gained popularity since they reduce the original problem to several\nsmaller ones. Coordinates to be optimized are usually selected randomly\naccording to a given probability distribution. We introduce an importance\nsampling strategy that helps randomized coordinate descent algorithms to focus\non blocks that are still far from convergence. The framework applies to\nproblems composed of the sum of two possibly non-convex terms, one being\nseparable and non-smooth. We have compared our algorithm to a full gradient\nproximal approach as well as to a randomized block coordinate algorithm that\nconsiders uniform sampling and cyclic block coordinate descent. Experimental\nevidences show the clear benefit of using an importance sampling strategy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:25:01 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Flamary", "R\u00e9mi", "", "LAGRANGE, OCA"], ["Rakotomamonjy", "Alain", "", "LITIS"], ["Gasso", "Gilles", "", "LITIS"]]}, {"id": "1606.07289", "submitter": "Remi Flamary", "authors": "Devis Tuia, Remi Flamary, Michel Barlaud", "title": "Non-convex regularization in remote sensing", "comments": "11 pages, 11 figures", "journal-ref": "Geoscience and Remote Sensing, IEEE Transactions on, 2016", "doi": "10.1109/TGRS.2016.2585201", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the effect of different regularizers and their\nimplications in high dimensional image classification and sparse linear\nunmixing. Although kernelization or sparse methods are globally accepted\nsolutions for processing data in high dimensions, we present here a study on\nthe impact of the form of regularization used and its parametrization. We\nconsider regularization via traditional squared (2) and sparsity-promoting (1)\nnorms, as well as more unconventional nonconvex regularizers (p and Log Sum\nPenalty). We compare their properties and advantages on several classification\nand linear unmixing tasks and provide advices on the choice of the best\nregularizer for the problem at hand. Finally, we also provide a fully\nfunctional toolbox for the community.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:36:01 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Tuia", "Devis", ""], ["Flamary", "Remi", ""], ["Barlaud", "Michel", ""]]}, {"id": "1606.07298", "submitter": "Wojciech Samek", "authors": "Leila Arras and Franziska Horn and Gr\\'egoire Montavon and\n  Klaus-Robert M\\\"uller and Wojciech Samek", "title": "Explaining Predictions of Non-Linear Classifiers in NLP", "comments": "7 pages, 3 figures, Paper accepted for 1st Workshop on Representation\n  Learning for NLP at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer-wise relevance propagation (LRP) is a recently proposed technique for\nexplaining predictions of complex non-linear classifiers in terms of input\nvariables. In this paper, we apply LRP for the first time to natural language\nprocessing (NLP). More precisely, we use it to explain the predictions of a\nconvolutional neural network (CNN) trained on a topic categorization task. Our\nanalysis highlights which words are relevant for a specific prediction of the\nCNN. We compare our technique to standard sensitivity analysis, both\nqualitatively and quantitatively, using a \"word deleting\" perturbation\nexperiment, a PCA analysis, and various visualizations. All experiments\nvalidate the suitability of LRP for explaining the CNN predictions, which is\nalso in line with results reported in recent image classification studies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:53:31 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Arras", "Leila", ""], ["Horn", "Franziska", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1606.07312", "submitter": "Maximilian Karl", "authors": "Maximilian Karl, Justin Bayer, Patrick van der Smagt", "title": "Unsupervised preprocessing for Tactile Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile information is important for gripping, stable grasp, and in-hand\nmanipulation, yet the complexity of tactile data prevents widespread use of\nsuch sensors. We make use of an unsupervised learning algorithm that transforms\nthe complex tactile data into a compact, latent representation without the need\nto record ground truth reference data. These compact representations can either\nbe used directly in a reinforcement learning based controller or can be used to\ncalibrate the tactile sensor to physical quantities with only a few datapoints.\nWe show the quality of our latent representation by predicting important\nfeatures and with a simple control task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 13:44:28 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Karl", "Maximilian", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1606.07315", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Kartik Gupta, Prateek Jain", "title": "Nearly-optimal Robust Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of Robust Matrix Completion (RMC)\nwhere the goal is to recover a low-rank matrix by observing a small number of\nits entries out of which a few can be arbitrarily corrupted. We propose a\nsimple projected gradient descent method to estimate the low-rank matrix that\nalternately performs a projected gradient descent step and cleans up a few of\nthe corrupted entries using hard-thresholding. Our algorithm solves RMC using\nnearly optimal number of observations as well as nearly optimal number of\ncorruptions. Our result also implies significant improvement over the existing\ntime complexity bounds for the low-rank matrix completion problem. Finally, an\napplication of our result to the robust PCA problem (low-rank+sparse matrix\nseparation) leads to nearly linear time (in matrix dimensions) algorithm for\nthe same; existing state-of-the-art methods require quadratic time. Our\nempirical results corroborate our theoretical results and show that even for\nmoderate sized problems, our method for robust PCA is an an order of magnitude\nfaster than the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 13:57:56 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 20:05:13 GMT"}, {"version": "v3", "created": "Thu, 8 Dec 2016 19:48:40 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Gupta", "Kartik", ""], ["Jain", "Prateek", ""]]}, {"id": "1606.07326", "submitter": "Wei Pan", "authors": "Wei Pan and Hao Dong and Yike Guo", "title": "DropNeuron: Simplifying the Structure of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning using multi-layer neural networks (NNs) architecture manifests\nsuperb power in modern machine learning systems. The trained Deep Neural\nNetworks (DNNs) are typically large. The question we would like to address is\nwhether it is possible to simplify the NN during training process to achieve a\nreasonable performance within an acceptable computational time. We presented a\nnovel approach of optimising a deep neural network through regularisation of\nnet- work architecture. We proposed regularisers which support a simple\nmechanism of dropping neurons during a network training process. The method\nsupports the construction of a simpler deep neural networks with compatible\nperformance with its simplified version. As a proof of concept, we evaluate the\nproposed method with examples including sparse linear regression, deep\nautoencoder and convolutional neural network. The valuations demonstrate\nexcellent performance.\n  The code for this work can be found in\nhttp://www.github.com/panweihit/DropNeuron\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 14:30:36 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 12:50:27 GMT"}, {"version": "v3", "created": "Sun, 3 Jul 2016 09:39:30 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Pan", "Wei", ""], ["Dong", "Hao", ""], ["Guo", "Yike", ""]]}, {"id": "1606.07356", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh", "title": "Analyzing the Behavior of Visual Question Answering Models", "comments": "13 pages, 20 figures; To appear in EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a number of deep-learning based models have been proposed for the\ntask of Visual Question Answering (VQA). The performance of most models is\nclustered around 60-70%. In this paper we propose systematic methods to analyze\nthe behavior of these models as a first step towards recognizing their\nstrengths and weaknesses, and identifying the most fruitful directions for\nprogress. We analyze two models, one each from two major classes of VQA models\n-- with-attention and without-attention and show the similarities and\ndifferences in the behavior of these models. We also analyze the winning entry\nof the VQA Challenge 2016.\n  Our behavior analysis reveals that despite recent progress, today's VQA\nmodels are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump\nto conclusions\" (converge on a predicted answer after 'listening' to just half\nthe question), and are \"stubborn\" (do not change their answers across images).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 16:05:16 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 19:56:22 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1606.07365", "submitter": "Jian Zhang", "authors": "Jian Zhang, Christopher De Sa, Ioannis Mitliagkas, Christopher R\\'e", "title": "Parallel SGD: When does averaging help?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a number of workers running SGD independently on the same pool of\ndata and averaging the models every once in a while -- a common but not well\nunderstood practice. We study model averaging as a variance-reducing mechanism\nand describe two ways in which the frequency of averaging affects convergence.\nFor convex objectives, we show the benefit of frequent averaging depends on the\ngradient variance envelope. For non-convex objectives, we illustrate that this\nbenefit depends on the presence of multiple globally optimal points. We\ncomplement our findings with multicore experiments on both synthetic and real\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 16:23:35 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Zhang", "Jian", ""], ["De Sa", "Christopher", ""], ["Mitliagkas", "Ioannis", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1606.07369", "submitter": "David Dooling", "authors": "David Dooling, Angela Kim, Barbara McAneny, Jennifer Webster", "title": "Personalized Prognostic Models for Oncology: A Machine Learning Approach", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have applied a little-known data transformation to subsets of the\nSurveillance, Epidemiology, and End Results (SEER) publically available data of\nthe National Cancer Institute (NCI) to make it suitable input to standard\nmachine learning classifiers. This transformation properly treats the\nright-censored data in the SEER data and the resulting Random Forest and\nMulti-Layer Perceptron models predict full survival curves. Treating the 6, 12,\nand 60 months points of the resulting survival curves as 3 binary classifiers,\nthe 18 resulting classifiers have AUC values ranging from .765 to .885. Further\nevidence that the models have generalized well from the training data is\nprovided by the extremely high levels of agreement between the random forest\nand neural network models predictions on the 6, 12, and 60 month binary\nclassifiers.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 15:55:22 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Dooling", "David", ""], ["Kim", "Angela", ""], ["McAneny", "Barbara", ""], ["Webster", "Jennifer", ""]]}, {"id": "1606.07374", "submitter": "Kun-Hao Yeh", "authors": "Kun-Hao Yeh, I-Chen Wu, Chu-Hsuan Hsueh, Chia-Chuan Chang, Chao-Chin\n  Liang, Han Chiang", "title": "Multi-Stage Temporal Difference Learning for 2048-like Games", "comments": "The version has been accepted by TCIAIG (The first version was sent\n  on 23, October, 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Szubert and Jaskowski successfully used temporal difference (TD) learning\ntogether with n-tuple networks for playing the game 2048. However, we observed\na phenomenon that the programs based on TD learning still hardly reach large\ntiles. In this paper, we propose multi-stage TD (MS-TD) learning, a kind of\nhierarchical reinforcement learning method, to effectively improve the\nperformance for the rates of reaching large tiles, which are good metrics to\nanalyze the strength of 2048 programs. Our experiments showed significant\nimprovements over the one without using MS-TD learning. Namely, using 3-ply\nexpectimax search, the program with MS-TD learning reached 32768-tiles with a\nrate of 18.31%, while the one with TD learning did not reach any. After further\ntuned, our 2048 program reached 32768-tiles with a rate of 31.75% in 10,000\ngames, and one among these games even reached a 65536-tile, which is the first\never reaching a 65536-tile to our knowledge. In addition, MS-TD learning method\ncan be easily applied to other 2048-like games, such as Threes. Based on MS-TD\nlearning, our experiments for Threes also demonstrated similar performance\nimprovement, where the program with MS-TD learning reached 6144-tiles with a\nrate of 7.83%, while the one with TD learning only reached 0.45%.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 16:58:33 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 18:36:49 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Yeh", "Kun-Hao", ""], ["Wu", "I-Chen", ""], ["Hsueh", "Chu-Hsuan", ""], ["Chang", "Chia-Chuan", ""], ["Liang", "Chao-Chin", ""], ["Chiang", "Han", ""]]}, {"id": "1606.07384", "submitter": "Yu Cheng", "authors": "Yu Cheng, Ilias Diakonikolas, Daniel Kane, Alistair Stewart", "title": "Robust Learning of Fixed-Structure Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning Bayesian networks in a robust model\nwhere an $\\epsilon$-fraction of the samples are adversarially corrupted. In\nthis work, we study the fully observable discrete case where the structure of\nthe network is given. Even in this basic setting, previous learning algorithms\neither run in exponential time or lose dimension-dependent factors in their\nerror guarantees. We provide the first computationally efficient robust\nlearning algorithm for this problem with dimension-independent error\nguarantees. Our algorithm has near-optimal sample complexity, runs in\npolynomial time, and achieves error that scales nearly-linearly with the\nfraction of adversarially corrupted samples. Finally, we show on both synthetic\nand semi-synthetic data that our algorithm performs well in practice.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 17:47:13 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 05:31:52 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Cheng", "Yu", ""], ["Diakonikolas", "Ilias", ""], ["Kane", "Daniel", ""], ["Stewart", "Alistair", ""]]}, {"id": "1606.07442", "submitter": "Tom Charnock", "authors": "Tom Charnock and Adam Moss", "title": "Deep Recurrent Neural Networks for Supernovae Classification", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": "10.3847/2041-8213/aa603d", "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply deep recurrent neural networks, which are capable of learning\ncomplex sequential information, to classify supernovae\\footnote{Code available\nat\n\\href{https://github.com/adammoss/supernovae}{https://github.com/adammoss/supernovae}}.\nThe observational time and filter fluxes are used as inputs to the network, but\nsince the inputs are agnostic additional data such as host galaxy information\ncan also be included. Using the Supernovae Photometric Classification Challenge\n(SPCC) data, we find that deep networks are capable of learning about light\ncurves, however the performance of the network is highly sensitive to the\namount of training data. For a training size of 50\\% of the representational\nSPCC dataset (around $10^4$ supernovae) we obtain a type-Ia vs. non-type-Ia\nclassification accuracy of 94.7\\%, an area under the Receiver Operating\nCharacteristic curve AUC of 0.986 and a SPCC figure-of-merit $F_1=0.64$. When\nusing only the data for the early-epoch challenge defined by the SPCC we\nachieve a classification accuracy of 93.1\\%, AUC of 0.977 and $F_1=0.58$,\nresults almost as good as with the whole light-curve. By employing\nbidirectional neural networks we can acquire impressive classification results\nbetween supernovae types -I,~-II and~-III at an accuracy of 90.4\\% and AUC of\n0.974. We also apply a pre-trained model to obtain classification probabilities\nas a function of time, and show it can give early indications of supernovae\ntype. Our method is competitive with existing algorithms and has applications\nfor future large-scale photometric surveys.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 20:00:02 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 18:57:31 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Charnock", "Tom", ""], ["Moss", "Adam", ""]]}, {"id": "1606.07487", "submitter": "Santiago Ontanon", "authors": "Adam James Summerville, Sam Snodgrass, Michael Mateas, Santiago\n  Onta\\~n\\'on", "title": "The VGLC: The Video Game Level Corpus", "comments": "To appear in proceedings of the 7th Workshop on Procedural Content\n  Generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Levels are a key component of many different video games, and a large body of\nwork has been produced on how to procedurally generate game levels. Recently,\nMachine Learning techniques have been applied to video game level generation\ntowards the purpose of automatically generating levels that have the properties\nof the training corpus. Towards that end we have made available a corpora of\nvideo game levels in an easy to parse format ideal for different machine\nlearning and other game AI research purposes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 21:36:36 GMT"}, {"version": "v2", "created": "Sun, 3 Jul 2016 20:04:55 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Summerville", "Adam James", ""], ["Snodgrass", "Sam", ""], ["Mateas", "Michael", ""], ["Onta\u00f1\u00f3n", "Santiago", ""]]}, {"id": "1606.07493", "submitter": "Arjun Chandrasekaran", "authors": "Harsh Agrawal, Arjun Chandrasekaran, Dhruv Batra, Devi Parikh, Mohit\n  Bansal", "title": "Sort Story: Sorting Jumbled Images and Captions into Stories", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal common sense has applications in AI tasks such as QA, multi-document\nsummarization, and human-AI communication. We propose the task of sequencing --\ngiven a jumbled set of aligned image-caption pairs that belong to a story, the\ntask is to sort them such that the output sequence forms a coherent story. We\npresent multiple approaches, via unary (position) and pairwise (order)\npredictions, and their ensemble-based combinations, achieving strong results on\nthis task. We use both text-based and image-based features, which depict\ncomplementary improvements. Using qualitative examples, we demonstrate that our\nmodels have learnt interesting aspects of temporal common sense.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 21:54:44 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 05:26:43 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 19:56:36 GMT"}, {"version": "v4", "created": "Sat, 24 Sep 2016 00:37:27 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 18:48:13 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Agrawal", "Harsh", ""], ["Chandrasekaran", "Arjun", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Bansal", "Mohit", ""]]}, {"id": "1606.07496", "submitter": "Roberto Camacho Barranco", "authors": "Roberto Camacho Barranco (1), Laura M. Rodriguez (1), Rebecca Urbina\n  (1), and M. Shahriar Hossain (1) ((1) The University of Texas at El Paso)", "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?", "comments": "10 pages, 11 figures, \"for associated results, see\n  http://http://auto-captioning.herokuapp.com/\" \"submitted to DLRS 2016\n  workshop\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While textual reviews have become prominent in many recommendation-based\nsystems, automated frameworks to provide relevant visual cues against text\nreviews where pictures are not available is a new form of task confronted by\ndata mining and machine learning researchers. Suggestions of pictures that are\nrelevant to the content of a review could significantly benefit the users by\nincreasing the effectiveness of a review. We propose a deep learning-based\nframework to automatically: (1) tag the images available in a review dataset,\n(2) generate a caption for each image that does not have one, and (3) enhance\neach review by recommending relevant images that might not be uploaded by the\ncorresponding reviewer. We evaluate the proposed framework using the Yelp\nChallenge Dataset. While a subset of the images in this particular dataset are\ncorrectly captioned, the majority of the pictures do not have any associated\ntext. Moreover, there is no mapping between reviews and images. Each image has\na corresponding business-tag where the picture was taken, though. The overall\ndata setting and unavailability of crucial pieces required for a mapping make\nthe problem of recommending images for reviews a major challenge. Qualitative\nand quantitative evaluations indicate that our proposed framework provides high\nquality enhancements through automatic captioning, tagging, and recommendation\nfor mapping reviews and images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 22:04:08 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Barranco", "Roberto Camacho", "", "The University of Texas at El Paso"], ["Rodriguez", "Laura M.", "", "The University of Texas at El Paso"], ["Urbina", "Rebecca", "", "The University of Texas at El Paso"], ["Hossain", "M. Shahriar", "", "The University of Texas at El Paso"]]}, {"id": "1606.07518", "submitter": "EPTCS", "authors": "Alexandru Baltag (Institute for logic, Language and Computation.\n  University of Amsterdam), Nina Gierasimczuk (Institute for Logic, Language\n  and Computation. University of Amsterdam), Sonja Smets (Institute for Logic,\n  Language and Computation. University of Amsterdam)", "title": "On the Solvability of Inductive Problems: A Study in Epistemic Topology", "comments": "In Proceedings TARK 2015, arXiv:1606.07295", "journal-ref": "EPTCS 215, 2016, pp. 81-98", "doi": "10.4204/EPTCS.215.7", "report-no": null, "categories": "cs.LO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the issues of inductive problem-solving and learning by\ndoxastic agents. We provide topological characterizations of solvability and\nlearnability, and we use them to prove that AGM-style belief revision is\n\"universal\", i.e., that every solvable problem is solvable by AGM conditioning.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 00:30:59 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Baltag", "Alexandru", "", "Institute for logic, Language and Computation.\n  University of Amsterdam"], ["Gierasimczuk", "Nina", "", "Institute for Logic, Language\n  and Computation. University of Amsterdam"], ["Smets", "Sonja", "", "Institute for Logic,\n  Language and Computation. University of Amsterdam"]]}, {"id": "1606.07558", "submitter": "Andrew Cotter", "authors": "Gabriel Goh, Andrew Cotter, Maya Gupta, Michael Friedlander", "title": "Satisfying Real-world Goals with Dataset Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of minimizing misclassification error on a training set is often\njust one of several real-world goals that might be defined on different\ndatasets. For example, one may require a classifier to also make positive\npredictions at some specified rate for some subpopulation (fairness), or to\nachieve a specified empirical recall. Other real-world goals include reducing\nchurn with respect to a previously deployed model, or stabilizing online\ntraining. In this paper we propose handling multiple goals on multiple datasets\nby training with dataset constraints, using the ramp penalty to accurately\nquantify costs, and present an efficient algorithm to approximately optimize\nthe resulting non-convex constrained optimization problem. Experiments on both\nbenchmark and real-world industry datasets demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 03:42:41 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 23:02:56 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Goh", "Gabriel", ""], ["Cotter", "Andrew", ""], ["Gupta", "Maya", ""], ["Friedlander", "Michael", ""]]}, {"id": "1606.07575", "submitter": "Arash Shahriari", "authors": "Arash Shahriari", "title": "Multipartite Ranking-Selection of Low-Dimensional Instances by\n  Supervised Projection to High-Dimensional Space", "comments": "15 pages, 1 figure, 2 tables, 3 algorithms, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning of redundant or irrelevant instances of data is a key to every\nsuccessful solution for pattern recognition. In this paper, we present a novel\nranking-selection framework for low-length but highly correlated instances.\nInstead of working in the low-dimensional instance space, we learn a supervised\nprojection to high-dimensional space spanned by the number of classes in the\ndataset under study. Imposing higher distinctions via exposing the notion of\nlabels to the instances, lets to deploy one versus all ranking for each\nindividual classes and selecting quality instances via adaptive thresholding of\nthe overall scores. To prove the efficiency of our paradigm, we employ it for\nthe purpose of texture understanding which is a hard recognition challenge due\nto high similarity of texture pixels and low dimensionality of their color\nfeatures. Our experiments show considerable improvements in recognition\nperformance over other local descriptors on several publicly available\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 06:15:45 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Shahriari", "Arash", ""]]}, {"id": "1606.07578", "submitter": "Bienvenue Kouwaye", "authors": "Bienvenue Kouway\\`e", "title": "Regression Trees and Random forest based feature selection for malaria\n  risk exposure prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with prediction of anopheles number, the main vector of\nmalaria risk, using environmental and climate variables. The variables\nselection is based on an automatic machine learning method using regression\ntrees, and random forests combined with stratified two levels cross validation.\nThe minimum threshold of variables importance is accessed using the quadratic\ndistance of variables importance while the optimal subset of selected variables\nis used to perform predictions. Finally the results revealed to be\nqualitatively better, at the selection, the prediction , and the CPU time point\nof view than those obtained by GLM-Lasso method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 06:34:17 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Kouway\u00e8", "Bienvenue", ""]]}, {"id": "1606.07636", "submitter": "Matthieu Geist", "authors": "Matthieu Geist and Bilal Piot and Olivier Pietquin", "title": "Is the Bellman residual a bad proxy?", "comments": "Final NIPS 2017 version (title, among other things, changed)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at theoretically and empirically comparing two standard\noptimization criteria for Reinforcement Learning: i) maximization of the mean\nvalue and ii) minimization of the Bellman residual. For that purpose, we place\nourselves in the framework of policy search algorithms, that are usually\ndesigned to maximize the mean value, and derive a method that minimizes the\nresidual $\\|T_* v_\\pi - v_\\pi\\|_{1,\\nu}$ over policies. A theoretical analysis\nshows how good this proxy is to policy optimization, and notably that it is\nbetter than its value-based counterpart. We also propose experiments on\nrandomly generated generic Markov decision processes, specifically designed for\nstudying the influence of the involved concentrability coefficient. They show\nthat the Bellman residual is generally a bad proxy to policy optimization and\nthat directly maximizing the mean value is much better, despite the current\nlack of deep theoretical analysis. This might seem obvious, as directly\naddressing the problem of interest is usually better, but given the prevalence\nof (projected) Bellman residual minimization in value-based reinforcement\nlearning, we believe that this question is worth to be considered.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 10:54:41 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 11:17:04 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 14:17:46 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Geist", "Matthieu", ""], ["Piot", "Bilal", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1606.07659", "submitter": "Florian Strub", "authors": "Florian Strub (CRIStAL, SEQUEL), Romaric Gaudel (CRIStAL, SEQUEL),\n  J\\'er\\'emie Mary (CRIStAL, SEQUEL)", "title": "Hybrid Recommender System based on Autoencoders", "comments": "arXiv admin note: substantial text overlap with arXiv:1603.00806", "journal-ref": "the 1st Workshop on Deep Learning for Recommender Systems, Sep\n  2016, Boston, United States. pp.11 - 16, 2016", "doi": "10.1145/2988450.2988456", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard model for Recommender Systems is the Matrix Completion setting:\ngiven partially known matrix of ratings given by users (rows) to items\n(columns), infer the unknown ratings. In the last decades, few attempts where\ndone to handle that objective with Neural Networks, but recently an\narchitecture based on Autoencoders proved to be a promising approach. In\ncurrent paper, we enhanced that architecture (i) by using a loss function\nadapted to input data with missing values, and (ii) by incorporating side\ninformation. The experiments demonstrate that while side information only\nslightly improve the test error averaged on all users/items, it has more impact\non cold users/items.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 12:37:04 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 15:41:21 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 14:32:51 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Strub", "Florian", "", "CRIStAL, SEQUEL"], ["Gaudel", "Romaric", "", "CRIStAL, SEQUEL"], ["Mary", "J\u00e9r\u00e9mie", "", "CRIStAL, SEQUEL"]]}, {"id": "1606.07707", "submitter": "Richard Oentaryo", "authors": "Richard J. Oentaryo, Ee-Peng Lim, Freddy Chong Tat Chua, Jia-Wei Low,\n  David Lo", "title": "Collective Semi-Supervised Learning for User Profiling in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of user-generated data in social media has incentivized the\ndevelopment of methods to infer the latent attributes of users, which are\ncrucially useful for personalization, advertising and recommendation. However,\nthe current user profiling approaches have limited success, due to the lack of\na principled way to integrate different types of social relationships of a\nuser, and the reliance on scarcely-available labeled data in building a\nprediction model. In this paper, we present a novel solution termed Collective\nSemi-Supervised Learning (CSL), which provides a principled means to integrate\ndifferent types of social relationship and unlabeled data under a unified\ncomputational framework. The joint learning from multiple relationships and\nunlabeled data yields a computationally sound and accurate approach to model\nuser attributes in social media. Extensive experiments using Twitter data have\ndemonstrated the efficacy of our CSL approach in inferring user attributes such\nas account type and marital status. We also show how CSL can be used to\ndetermine important user features, and to make inference on a larger user\npopulation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 14:42:17 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Oentaryo", "Richard J.", ""], ["Lim", "Ee-Peng", ""], ["Chua", "Freddy Chong Tat", ""], ["Low", "Jia-Wei", ""], ["Lo", "David", ""]]}, {"id": "1606.07722", "submitter": "Kai-Chun Hsu", "authors": "Kai-Chun Hsu, Szu-Yu Chou, Yi-Hsuan Yang, Tai-Shih Chi", "title": "Neural Network Based Next-Song Recommendation", "comments": "5 pages, 3 figures, the 1st Workshop on Deep Learning for Recommender\n  Systems (DLRS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the next-item/basket recommendation system, which considers the\nsequential relation between bought items, has drawn attention of researchers.\nThe utilization of sequential patterns has boosted performance on several kinds\nof recommendation tasks. Inspired by natural language processing (NLP)\ntechniques, we propose a novel neural network (NN) based next-song recommender,\nCNN-rec, in this paper. Then, we compare the proposed system with several NN\nbased and classic recommendation systems on the next-song recommendation task.\nVerification results indicate the proposed system outperforms classic systems\nand has comparable performance with the state-of-the-art system.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 15:25:55 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Hsu", "Kai-Chun", ""], ["Chou", "Szu-Yu", ""], ["Yang", "Yi-Hsuan", ""], ["Chi", "Tai-Shih", ""]]}, {"id": "1606.07767", "submitter": "Dimitri Nowicki", "authors": "Artem Chernodub and Dimitri Nowicki", "title": "Sampling-based Gradient Regularization for Capturing Long-Term\n  Dependencies in Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vanishing (and exploding) gradients effect is a common problem for recurrent\nneural networks with nonlinear activation functions which use backpropagation\nmethod for calculation of derivatives. Deep feedforward neural networks with\nmany hidden layers also suffer from this effect. In this paper we propose a\nnovel universal technique that makes the norm of the gradient stay in the\nsuitable range. We construct a way to estimate a contribution of each training\nexample to the norm of the long-term components of the target function s\ngradient. Using this subroutine we can construct mini-batches for the\nstochastic gradient descent (SGD) training that leads to high performance and\naccuracy of the trained network even for very complex tasks. We provide a\nstraightforward mathematical estimation of minibatch s impact on for the\ngradient norm and prove its correctness theoretically. To check our framework\nexperimentally we use some special synthetic benchmarks for testing RNNs on\nability to capture long-term dependencies. Our network can detect links between\nevents in the (temporal) sequence at the range approx. 100 and longer.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 17:31:02 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 21:30:29 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 21:25:26 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Chernodub", "Artem", ""], ["Nowicki", "Dimitri", ""]]}, {"id": "1606.07786", "submitter": "Jonathan Binas", "authors": "Jonathan Binas, Daniel Neil, Giacomo Indiveri, Shih-Chii Liu, Michael\n  Pfeiffer", "title": "Precise neural network computation with imprecise analog devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The operations used for neural network computation map favorably onto simple\nanalog circuits, which outshine their digital counterparts in terms of\ncompactness and efficiency. Nevertheless, such implementations have been\nlargely supplanted by digital designs, partly because of device mismatch\neffects due to material and fabrication imperfections. We propose a framework\nthat exploits the power of deep learning to compensate for this mismatch by\nincorporating the measured device variations as constraints in the neural\nnetwork training process. This eliminates the need for mismatch minimization\nstrategies and allows circuit complexity and power-consumption to be reduced to\na minimum. Our results, based on large-scale simulations as well as a prototype\nVLSI chip implementation indicate a processing efficiency comparable to current\nstate-of-art digital implementations. This method is suitable for future\ntechnology based on nanodevices with large variability, such as memristive\narrays.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 18:32:43 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 06:27:05 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Binas", "Jonathan", ""], ["Neil", "Daniel", ""], ["Indiveri", "Giacomo", ""], ["Liu", "Shih-Chii", ""], ["Pfeiffer", "Michael", ""]]}, {"id": "1606.07792", "submitter": "Heng-Tze Cheng", "authors": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar\n  Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa\n  Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu,\n  Hemal Shah", "title": "Wide & Deep Learning for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models with nonlinear feature transformations are widely\nused for large-scale regression and classification problems with sparse inputs.\nMemorization of feature interactions through a wide set of cross-product\nfeature transformations are effective and interpretable, while generalization\nrequires more feature engineering effort. With less feature engineering, deep\nneural networks can generalize better to unseen feature combinations through\nlow-dimensional dense embeddings learned for the sparse features. However, deep\nneural networks with embeddings can over-generalize and recommend less relevant\nitems when the user-item interactions are sparse and high-rank. In this paper,\nwe present Wide & Deep learning---jointly trained wide linear models and deep\nneural networks---to combine the benefits of memorization and generalization\nfor recommender systems. We productionized and evaluated the system on Google\nPlay, a commercial mobile app store with over one billion active users and over\none million apps. Online experiment results show that Wide & Deep significantly\nincreased app acquisitions compared with wide-only and deep-only models. We\nhave also open-sourced our implementation in TensorFlow.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 19:07:02 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Cheng", "Heng-Tze", ""], ["Koc", "Levent", ""], ["Harmsen", "Jeremiah", ""], ["Shaked", "Tal", ""], ["Chandra", "Tushar", ""], ["Aradhye", "Hrishi", ""], ["Anderson", "Glen", ""], ["Corrado", "Greg", ""], ["Chai", "Wei", ""], ["Ispir", "Mustafa", ""], ["Anil", "Rohan", ""], ["Haque", "Zakaria", ""], ["Hong", "Lichan", ""], ["Jain", "Vihan", ""], ["Liu", "Xiaobing", ""], ["Shah", "Hemal", ""]]}, {"id": "1606.07947", "submitter": "Yoon Kim", "authors": "Yoon Kim, Alexander M. Rush", "title": "Sequence-Level Knowledge Distillation", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) offers a novel alternative formulation of\ntranslation that is potentially simpler than statistical approaches. However to\nreach competitive performance, NMT models need to be exceedingly large. In this\npaper we consider applying knowledge distillation approaches (Bucila et al.,\n2006; Hinton et al., 2015) that have proven successful for reducing the size of\nneural models in other domains to the problem of NMT. We demonstrate that\nstandard knowledge distillation applied to word-level prediction can be\neffective for NMT, and also introduce two novel sequence-level versions of\nknowledge distillation that further improve performance, and somewhat\nsurprisingly, seem to eliminate the need for beam search (even when applied on\nthe original teacher model). Our best student model runs 10 times faster than\nits state-of-the-art teacher with little loss in performance. It is also\nsignificantly better than a baseline model trained without knowledge\ndistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight\npruning on top of knowledge distillation results in a student model that has 13\ntimes fewer parameters than the original teacher model, with a decrease of 0.4\nBLEU.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 18:16:39 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 17:24:18 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 15:02:54 GMT"}, {"version": "v4", "created": "Thu, 22 Sep 2016 01:17:12 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Kim", "Yoon", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1606.07953", "submitter": "Abhyuday Jagannatha", "authors": "Abhyuday Jagannatha, Hong Yu", "title": "Bidirectional Recurrent Neural Networks for Medical Event Detection in\n  Electronic Health Records", "comments": "In proceedings of NAACL HLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence labeling for extraction of medical events and their attributes from\nunstructured text in Electronic Health Record (EHR) notes is a key step towards\nsemantic understanding of EHRs. It has important applications in health\ninformatics including pharmacovigilance and drug surveillance. The state of the\nart supervised machine learning models in this domain are based on Conditional\nRandom Fields (CRFs) with features calculated from fixed context windows. In\nthis application, we explored various recurrent neural network frameworks and\nshow that they significantly outperformed the CRF models.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 19:46:28 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 17:10:38 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Jagannatha", "Abhyuday", ""], ["Yu", "Hong", ""]]}, {"id": "1606.07981", "submitter": "Amir Hosein Zamanian", "authors": "Amir Hosein Zamanian, Abdolreza Ohadi", "title": "Gear fault diagnosis based on Gaussian correlation of vibrations signals\n  and wavelet coefficients", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2011.06.020", "report-no": null, "categories": "cs.IT cs.CE cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The features of non-stationary multi-component signals are often difficult to\nbe extracted for expert systems. In this paper, a new method for feature\nextraction that is based on maximization of local Gaussian correlation function\nof wavelet coefficients and signal is presented. The effect of empirical mode\ndecomposition (EMD) to decompose multi-component signals to intrinsic mode\nfunctions (IMFs), before using of local Gaussian correlation is discussed. The\nexperimental vibration signals from two gearbox systems are used to show the\nefficiency of the presented method. Linear support vector machine (SVM) is\nutilized to classify feature sets extracted with the presented method. The\nobtained results show that the features extracted in this method have excellent\nability to classify faults without any additional feature selection; it is also\nshown that EMD can improve or degrade features according to the utilized\nfeature reduction method.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 00:16:44 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Zamanian", "Amir Hosein", ""], ["Ohadi", "Abdolreza", ""]]}, {"id": "1606.08009", "submitter": "Ashkan Esmaeili", "authors": "Ashkan Esmaeili, Arash Amini, and Farokh Marvasti", "title": "Fast Methods for Recovering Sparse Parameters in Linear Low Rank Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the recovery of a sparse weight vector\n(parameters vector) from a set of noisy linear combinations. However, only\npartial information about the matrix representing the linear combinations is\navailable. Assuming a low-rank structure for the matrix, one natural solution\nwould be to first apply a matrix completion on the data, and then to solve the\nresulting compressed sensing problem. In big data applications such as massive\nMIMO and medical data, the matrix completion step imposes a huge computational\nburden. Here, we propose to reduce the computational cost of the completion\ntask by ignoring the columns corresponding to zero elements in the sparse\nvector. To this end, we employ a technique to initially approximate the support\nof the sparse vector. We further propose to unify the partial matrix completion\nand sparse vector recovery into an augmented four-step problem. Simulation\nresults reveal that the augmented approach achieves the best performance, while\nboth proposed methods outperform the natural two-step technique with\nsubstantially less computational requirements.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 08:27:45 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 14:46:39 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Esmaeili", "Ashkan", ""], ["Amini", "Arash", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1606.08051", "submitter": "Amir Ahooye Atashin", "authors": "Amir Ahooye Atashin, Kamaledin Ghiasi-Shirazi, Ahad Harati", "title": "Training LDCRF model on unsegmented sequences using Connectionist\n  Temporal Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning problems such as speech recognition, gesture\nrecognition, and handwriting recognition are concerned with simultaneous\nsegmentation and labeling of sequence data. Latent-dynamic conditional random\nfield (LDCRF) is a well-known discriminative method that has been successfully\nused for this task. However, LDCRF can only be trained with pre-segmented data\nsequences in which the label of each frame is available apriori. In the realm\nof neural networks, the invention of connectionist temporal classification\n(CTC) made it possible to train recurrent neural networks on unsegmented\nsequences with great success. In this paper, we use CTC to train an LDCRF model\non unsegmented sequences. Experimental results on two gesture recognition tasks\nshow that the proposed method outperforms LDCRFs, hidden Markov models, and\nconditional random fields.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 16:26:19 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 14:23:06 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2016 11:57:06 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Atashin", "Amir Ahooye", ""], ["Ghiasi-Shirazi", "Kamaledin", ""], ["Harati", "Ahad", ""]]}, {"id": "1606.08061", "submitter": "Pascal Vincent", "authors": "Pascal Vincent, Alexandre de Br\\'ebisson, Xavier Bouthillier", "title": "Exact gradient updates in time independent of output size for the\n  spherical loss family", "comments": "Expanded journal version of our NIPS-2015 conference paper\n  arXiv:1412.7091 with full algorithm generalized to the spherical family", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important class of problems involves training deep neural networks with\nsparse prediction targets of very high dimension D. These occur naturally in\ne.g. neural language models or the learning of word-embeddings, often posed as\npredicting the probability of next words among a vocabulary of size D (e.g.\n200,000). Computing the equally large, but typically non-sparse D-dimensional\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\nincurs a prohibitive O(Dd) computational cost for each example, as does\nupdating the $D \\times d$ output weight matrix and computing the gradient\nneeded for backpropagation to previous layers. While efficient handling of\nlarge sparse network inputs is trivial, the case of large sparse targets is\nnot, and has thus so far been sidestepped with approximate alternatives such as\nhierarchical softmax or sampling-based approximations during training. In this\nwork we develop an original algorithmic approach which, for a family of loss\nfunctions that includes squared error and spherical softmax, can compute the\nexact loss, gradient update for the output weights, and gradient for\nbackpropagation, all in $O(d^{2})$ per example instead of $O(Dd)$, remarkably\nwithout ever computing the D-dimensional output. The proposed algorithm yields\na speedup of up to $D/4d$ i.e. two orders of magnitude for typical sizes, for\nthat critical part of the computations that often dominates the training time\nin this kind of network architecture.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 17:57:36 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Vincent", "Pascal", ""], ["de Br\u00e9bisson", "Alexandre", ""], ["Bouthillier", "Xavier", ""]]}, {"id": "1606.08117", "submitter": "Xinxing Xu", "authors": "Yong Kiam Tan, Xinxing Xu and Yong Liu", "title": "Improved Recurrent Neural Networks for Session-based Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) were recently proposed for the session-based\nrecommendation task. The models showed promising improvements over traditional\nrecommendation approaches. In this work, we further study RNN-based models for\nsession-based recommendations. We propose the application of two techniques to\nimprove model performance, namely, data augmentation, and a method to account\nfor shifts in the input data distribution. We also empirically study the use of\ngeneralised distillation, and a novel alternative model that directly predicts\nitem embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate\nrelative improvements of 12.8% and 14.8% over previously reported results on\nthe Recall@20 and Mean Reciprocal Rank@20 metrics respectively.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 03:06:44 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 09:41:10 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Tan", "Yong Kiam", ""], ["Xu", "Xinxing", ""], ["Liu", "Yong", ""]]}, {"id": "1606.08165", "submitter": "Hesham Mostafa", "authors": "Hesham Mostafa", "title": "Supervised learning based on temporal coding in spiking neural networks", "comments": "Extended the discussion and introduction. Clarified the training\n  parameters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent training techniques are remarkably successful in training\nanalog-valued artificial neural networks (ANNs). Such training techniques,\nhowever, do not transfer easily to spiking networks due to the spike generation\nhard non-linearity and the discrete nature of spike communication. We show that\nin a feedforward spiking network that uses a temporal coding scheme where\ninformation is encoded in spike times instead of spike rates, the network\ninput-output relation is differentiable almost everywhere. Moreover, this\nrelation is piece-wise linear after a transformation of variables. Methods for\ntraining ANNs thus carry directly to the training of such spiking networks as\nwe show when training on the permutation invariant MNIST task. In contrast to\nrate-based spiking networks that are often used to approximate the behavior of\nANNs, the networks we present spike much more sparsely and their behavior can\nnot be directly approximated by conventional ANNs. Our results highlight a new\napproach for controlling the behavior of spiking networks with realistic\ntemporal dynamics, opening up the potential for using these networks to process\nspike patterns with complex temporal information.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 08:58:29 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 16:15:20 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Mostafa", "Hesham", ""]]}, {"id": "1606.08282", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi and Marco F. Duarte and Benjamin Marlin", "title": "Out-of-Sample Extension for Dimensionality Reduction of Noisy Time\n  Series", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2735189", "report-no": null, "categories": "stat.ML cs.CG cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an out-of-sample extension framework for a global\nmanifold learning algorithm (Isomap) that uses temporal information in\nout-of-sample points in order to make the embedding more robust to noise and\nartifacts. Given a set of noise-free training data and its embedding, the\nproposed framework extends the embedding for a noisy time series. This is\nachieved by adding a spatio-temporal compactness term to the optimization\nobjective of the embedding. To the best of our knowledge, this is the first\nmethod for out-of-sample extension of manifold embeddings that leverages timing\ninformation available for the extension set. Experimental results demonstrate\nthat our out-of-sample extension algorithm renders a more robust and accurate\nembedding of sequentially ordered image data in the presence of various noise\nand artifacts when compared to other timing-aware embeddings. Additionally, we\nshow that an out-of-sample extension framework based on the proposed algorithm\noutperforms the state of the art in eye-gaze estimation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 14:03:40 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 20:13:17 GMT"}, {"version": "v3", "created": "Sat, 29 Jul 2017 01:37:40 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Duarte", "Marco F.", ""], ["Marlin", "Benjamin", ""]]}, {"id": "1606.08359", "submitter": "Thomas Demeester", "authors": "Thomas Demeester and Tim Rockt\\\"aschel and Sebastian Riedel", "title": "Lifted Rule Injection for Relation Embeddings", "comments": "Camera-ready version for EMNLP 2016 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods based on representation learning currently hold the state-of-the-art\nin many natural language processing and knowledge base inference tasks. Yet, a\nmajor challenge is how to efficiently incorporate commonsense knowledge into\nsuch models. A recent approach regularizes relation and entity representations\nby propositionalization of first-order logic rules. However,\npropositionalization does not scale beyond domains with only few entities and\nrules. In this paper we present a highly efficient method for incorporating\nimplication rules into distributed representations for automated knowledge base\nconstruction. We map entity-tuple embeddings into an approximately Boolean\nspace and encourage a partial ordering over relation embeddings based on\nimplication rules mined from WordNet. Surprisingly, we find that the strong\nrestriction of the entity-tuple embedding space does not hurt the\nexpressiveness of the model and even acts as a regularizer that improves\ngeneralization. By incorporating few commonsense rules, we achieve an increase\nof 2 percentage points mean average precision over a matrix factorization\nbaseline, while observing a negligible increase in runtime.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 16:39:23 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 20:40:25 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Demeester", "Thomas", ""], ["Rockt\u00e4schel", "Tim", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1606.08362", "submitter": "Huy Nguyen", "authors": "Alina Ene, Huy L. Nguyen", "title": "A Reduction for Optimizing Lattice Submodular Functions with Diminishing\n  Returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f: \\mathbb{Z}_+^E \\rightarrow \\mathbb{R}_+$ is DR-submodular if\nit satisfies $f({\\bf x} + \\chi_i) -f ({\\bf x}) \\ge f({\\bf y} + \\chi_i) - f({\\bf\ny})$ for all ${\\bf x}\\le {\\bf y}, i\\in E$. Recently, the problem of maximizing\na DR-submodular function $f: \\mathbb{Z}_+^E \\rightarrow \\mathbb{R}_+$ subject\nto a budget constraint $\\|{\\bf x}\\|_1 \\leq B$ as well as additional constraints\nhas received significant attention \\cite{SKIK14,SY15,MYK15,SY16}.\n  In this note, we give a generic reduction from the DR-submodular setting to\nthe submodular setting. The running time of the reduction and the size of the\nresulting submodular instance depends only \\emph{logarithmically} on $B$. Using\nthis reduction, one can translate the results for unconstrained and constrained\nsubmodular maximization to the DR-submodular setting for many types of\nconstraints in a unified manner.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 16:44:44 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 19:49:59 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1606.08415", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Kevin Gimpel", "title": "Gaussian Error Linear Units (GELUs)", "comments": "Trimmed version of 2016 draft; add exact formula", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Gaussian Error Linear Unit (GELU), a high-performing neural\nnetwork activation function. The GELU activation function is $x\\Phi(x)$, where\n$\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU\nnonlinearity weights inputs by their value, rather than gates inputs by their\nsign as in ReLUs ($x\\mathbf{1}_{x>0}$). We perform an empirical evaluation of\nthe GELU nonlinearity against the ReLU and ELU activations and find performance\nimprovements across all considered computer vision, natural language\nprocessing, and speech tasks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 19:20:40 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 18:32:46 GMT"}, {"version": "v3", "created": "Sun, 11 Nov 2018 07:40:32 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 16:41:42 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Hendrycks", "Dan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1606.08501", "submitter": "Giorgio Gnecco", "authors": "Giorgio Gnecco", "title": "Symmetric and antisymmetric properties of solutions to kernel-based\n  machine learning problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A particularly interesting instance of supervised learning with kernels is\nwhen each training example is associated with two objects, as in pairwise\nclassification (Brunner et al., 2012), and in supervised learning of preference\nrelations (Herbrich et al., 1998). In these cases, one may want to embed\nadditional prior knowledge into the optimization problem associated with the\ntraining of the learning machine, modeled, respectively, by the symmetry of its\noptimal solution with respect to an exchange of order between the two objects,\nand by its antisymmetry. Extending the approach proposed in (Brunner et al.,\n2012) (where the only symmetric case was considered), we show, focusing on\nsupport vector binary classification, how such embedding is possible through\nthe choice of a suitable pairwise kernel, which takes as inputs the individual\nfeature vectors and also the group feature vectors associated with the two\nobjects. We also prove that the symmetry/antisymmetry constraints still hold\nwhen considering the sequence of suboptimal solutions generated by one version\nof the Sequential Minimal Optimization (SMO) algorithm, and we present\nnumerical results supporting the theoretical findings. We conclude discussing\nextensions of the main results to support vector regression, to transductive\nsupport vector machines, and to several kinds of graph kernels, including\ndiffusion kernels.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 22:34:14 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 02:13:31 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Gnecco", "Giorgio", ""]]}, {"id": "1606.08531", "submitter": "Bahare Fatemi", "authors": "Bahare Fatemi, Seyed Mehran Kazemi, David Poole", "title": "A Learning Algorithm for Relational Logistic Regression: Preliminary\n  Results", "comments": "In IJCAI-16 Statistical Relational AI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Relational logistic regression (RLR) is a representation of conditional\nprobability in terms of weighted formulae for modelling multi-relational data.\nIn this paper, we develop a learning algorithm for RLR models. Learning an RLR\nmodel from data consists of two steps: 1- learning the set of formulae to be\nused in the model (a.k.a. structure learning) and learning the weight of each\nformula (a.k.a. parameter learning). For structure learning, we deploy Schmidt\nand Murphy's hierarchical assumption: first we learn a model with simple\nformulae, then more complex formulae are added iteratively only if all their\nsub-formulae have proven effective in previous learned models. For parameter\nlearning, we convert the problem into a non-relational learning problem and use\nan off-the-shelf logistic regression learning algorithm from Weka, an\nopen-source machine learning tool, to learn the weights. We also indicate how\nhidden features about the individuals can be incorporated into RLR to boost the\nlearning performance. We compare our learning algorithm to other structure and\nparameter learning algorithms in the literature, and compare the performance of\nRLR models to standard logistic regression and RDN-Boost on a modified version\nof the MovieLens data-set.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 01:43:38 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Fatemi", "Bahare", ""], ["Kazemi", "Seyed Mehran", ""], ["Poole", "David", ""]]}, {"id": "1606.08538", "submitter": "Bo Tang", "authors": "Bo Tang and Haibo He", "title": "A Local Density-Based Approach for Local Outlier Detection", "comments": "22 pages, 14 figures, submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple but effective density-based outlier detection\napproach with the local kernel density estimation (KDE). A Relative\nDensity-based Outlier Score (RDOS) is introduced to measure the local\noutlierness of objects, in which the density distribution at the location of an\nobject is estimated with a local KDE method based on extended nearest neighbors\nof the object. Instead of using only $k$ nearest neighbors, we further consider\nreverse nearest neighbors and shared nearest neighbors of an object for density\ndistribution estimation. Some theoretical properties of the proposed RDOS\nincluding its expected value and false alarm probability are derived. A\ncomprehensive experimental study on both synthetic and real-life data sets\ndemonstrates that our approach is more effective than state-of-the-art outlier\ndetection methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 02:23:58 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Tang", "Bo", ""], ["He", "Haibo", ""]]}, {"id": "1606.08561", "submitter": "Shantanu Jain", "authors": "Shantanu Jain, Martha White, Predrag Radivojac", "title": "Estimating the class prior and posterior from noisy positives and\n  unlabeled data", "comments": "Fixed a typo in the MSGMM update equations in the appendix. Other\n  minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a classification algorithm for estimating posterior distributions\nfrom positive-unlabeled data, that is robust to noise in the positive labels\nand effective for high-dimensional data. In recent years, several algorithms\nhave been proposed to learn from positive-unlabeled data; however, many of\nthese contributions remain theoretical, performing poorly on real\nhigh-dimensional data that is typically contaminated with noise. We build on\nthis previous work to develop two practical classification algorithms that\nexplicitly model the noise in the positive labels and utilize univariate\ntransforms built on discriminative classifiers. We prove that these univariate\ntransforms preserve the class prior, enabling estimation in the univariate\nspace and avoiding kernel density estimation for high-dimensional data. The\ntheoretical development and both parametric and nonparametric algorithms\nproposed here constitutes an important step towards wide-spread use of robust\nclassification algorithms for positive-unlabeled data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 05:29:25 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 19:25:14 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Jain", "Shantanu", ""], ["White", "Martha", ""], ["Radivojac", "Predrag", ""]]}, {"id": "1606.08571", "submitter": "Yang Lu", "authors": "Tian Han, Yang Lu, Song-Chun Zhu, and Ying Nian Wu", "title": "Alternating Back-Propagation for Generator Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an alternating back-propagation algorithm for learning\nthe generator network model. The model is a non-linear generalization of factor\nanalysis. In this model, the mapping from the continuous latent factors to the\nobserved signal is parametrized by a convolutional neural network. The\nalternating back-propagation algorithm iterates the following two steps: (1)\nInferential back-propagation, which infers the latent factors by Langevin\ndynamics or gradient descent. (2) Learning back-propagation, which updates the\nparameters given the inferred latent factors by gradient descent. The gradient\ncomputations in both steps are powered by back-propagation, and they share most\nof their code in common. We show that the alternating back-propagation\nalgorithm can learn realistic generator models of natural images, video\nsequences, and sounds. Moreover, it can also be used to learn from incomplete\nor indirect training data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 06:46:05 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 15:11:00 GMT"}, {"version": "v3", "created": "Thu, 15 Sep 2016 04:38:01 GMT"}, {"version": "v4", "created": "Tue, 6 Dec 2016 04:04:19 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Han", "Tian", ""], ["Lu", "Yang", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1606.08658", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Dumancic and Hendrik Blockeel", "title": "Clustering-Based Relational Unsupervised Representation Learning with an\n  Explicit Distributed Representation", "comments": "8 pages, 1 figure, 2 tables, StaRAI 2016 submission, final version", "journal-ref": null, "doi": "10.24963/ijcai.2017/226", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of unsupervised representation learning is to extract a new\nrepresentation of data, such that solving many different tasks becomes easier.\nExisting methods typically focus on vectorized data and offer little support\nfor relational data, which additionally describe relationships among instances.\nIn this work we introduce an approach for relational unsupervised\nrepresentation learning. Viewing a relational dataset as a hypergraph, new\nfeatures are obtained by clustering vertices and hyperedges. To find a\nrepresentation suited for many relational learning tasks, a wide range of\nsimilarities between relational objects is considered, e.g. feature and\nstructural similarities. We experimentally evaluate the proposed approach and\nshow that models learned on such latent representations perform better, have\nlower complexity, and outperform the existing approaches on classification\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 11:37:45 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 07:35:46 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 09:21:40 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Dumancic", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1606.08660", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Dumancic and Wannes Meert and Hendrik Blockeel", "title": "Theory reconstruction: a representation learning view on predicate\n  invention", "comments": "3 pages, StaRAI 2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With this positional paper we present a representation learning view on\npredicate invention. The intention of this proposal is to bridge the relational\nand deep learning communities on the problem of predicate invention. We propose\na theory reconstruction approach, a formalism that extends autoencoder approach\nto representation learning to the relational settings. Our intention is to\nstart a discussion to define a unifying framework for predicate invention and\ntheory revision.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 11:41:03 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 11:29:35 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Dumancic", "Sebastijan", ""], ["Meert", "Wannes", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1606.08698", "submitter": "Guillem Collell", "authors": "Guillem Collell, Drazen Prelec, Kaustubh Patil", "title": "Reviving Threshold-Moving: a Simple Plug-in Bagging Ensemble for Binary\n  and Multiclass Imbalanced Data", "comments": "Typo in the proof fixed. TP/(P+N)=P(y=1) replaced by P/(P+N)=P(y=1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance presents a major hurdle in the application of data mining\nmethods. A common practice to deal with it is to create ensembles of\nclassifiers that learn from resampled balanced data. For example, bagged\ndecision trees combined with random undersampling (RUS) or the synthetic\nminority oversampling technique (SMOTE). However, most of the resampling\nmethods entail asymmetric changes to the examples of different classes, which\nin turn can introduce its own biases in the model. Furthermore, those methods\nrequire a performance measure to be specified a priori before learning. An\nalternative is to use a so-called threshold-moving method that a posteriori\nchanges the decision threshold of a model to counteract the imbalance, thus has\na potential to adapt to the performance measure of interest. Surprisingly,\nlittle attention has been paid to the potential of combining bagging ensemble\nwith threshold-moving. In this paper, we present probability thresholding\nbagging (PT-bagging), a versatile plug-in method that fills this gap. Contrary\nto usual rebalancing practice, our method preserves the natural class\ndistribution of the data resulting in well calibrated posterior probabilities.\nWe also extend the proposed method to handle multiclass data. The method is\nvalidated on binary and multiclass benchmark data sets. We perform analyses\nthat provide insights into the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 13:49:30 GMT"}, {"version": "v2", "created": "Sun, 3 Jul 2016 18:53:14 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 08:09:37 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Collell", "Guillem", ""], ["Prelec", "Drazen", ""], ["Patil", "Kaustubh", ""]]}, {"id": "1606.08777", "submitter": "Gemma Boleda", "authors": "Gemma Boleda and Sebastian Pad\\'o and Marco Baroni", "title": "\"Show me the cup\": Reference with Continuous Representations", "comments": null, "journal-ref": "In: Gelbukh A. (eds) Computational Linguistics and Intelligent\n  Text Processing. CICLing 2017. Lecture Notes in Computer Science, vol 10761.\n  Springer, Cham", "doi": "10.1007/978-3-319-77113-7_17", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most basic functions of language is to refer to objects in a\nshared scene. Modeling reference with continuous representations is challenging\nbecause it requires individuation, i.e., tracking and distinguishing an\narbitrary number of referents. We introduce a neural network model that, given\na definite description and a set of objects represented by natural images,\npoints to the intended object if the expression has a unique referent, or\nindicates a failure, if it does not. The model, directly trained on reference\nacts, is competitive with a pipeline manually engineered to perform the same\ntask, both when referents are purely visual, and when they are characterized by\na combination of visual and linguistic properties.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 16:31:50 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Boleda", "Gemma", ""], ["Pad\u00f3", "Sebastian", ""], ["Baroni", "Marco", ""]]}, {"id": "1606.08808", "submitter": "Miao Cheng", "authors": "Miao Cheng, Ah Chung Tsoi", "title": "Adaptive Training of Random Mapping for Data Quantization", "comments": "6 pages, 5 figures, 15.8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quantization learns encoding results of data with certain requirements,\nand provides a broad perspective of many real-world applications to data\nhandling. Nevertheless, the results of encoder is usually limited to\nmultivariate inputs with the random mapping, and side information of binary\ncodes are hardly to mostly depict the original data patterns as possible. In\nthe literature, cosine based random quantization has attracted much attentions\ndue to its intrinsic bounded results. Nevertheless, it usually suffers from the\nuncertain outputs, and information of original data fails to be fully preserved\nin the reduced codes. In this work, a novel binary embedding method, termed\nadaptive training quantization (ATQ), is proposed to learn the ideal transform\nof random encoder, where the limitation of cosine random mapping is tackled. As\nan adaptive learning idea, the reduced mapping is adaptively calculated with\nidea of data group, while the bias of random transform is to be improved to\nhold most matching information. Experimental results show that the proposed\nmethod is able to obtain outstanding performance compared with other random\nquantization methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 18:15:32 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 15:24:26 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Cheng", "Miao", ""], ["Tsoi", "Ah Chung", ""]]}, {"id": "1606.08813", "submitter": "Seth Flaxman", "authors": "Bryce Goodman and Seth Flaxman", "title": "European Union regulations on algorithmic decision-making and a \"right\n  to explanation\"", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": "AI Magazine, Vol 38, No 3, 2017", "doi": "10.1609/aimag.v38i3.2741", "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We summarize the potential impact that the European Union's new General Data\nProtection Regulation will have on the routine use of machine learning\nalgorithms. Slated to take effect as law across the EU in 2018, it will\nrestrict automated individual decision-making (that is, algorithms that make\ndecisions based on user-level predictors) which \"significantly affect\" users.\nThe law will also effectively create a \"right to explanation,\" whereby a user\ncan ask for an explanation of an algorithmic decision that was made about them.\nWe argue that while this law will pose large challenges for industry, it\nhighlights opportunities for computer scientists to take the lead in designing\nalgorithms and evaluation frameworks which avoid discrimination and enable\nexplanation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 18:20:06 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 15:42:30 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2016 12:30:13 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Goodman", "Bryce", ""], ["Flaxman", "Seth", ""]]}, {"id": "1606.08819", "submitter": "Moshe Salhov", "authors": "Moshe Salhov, Ofir Lindenbaum, Yariv Aizenbud, Avi Silberschatz, Yoel\n  Shkolnisky, Amir Averbuch", "title": "Multi-View Kernel Consensus For Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input data features set for many data driven tasks is high-dimensional\nwhile the intrinsic dimension of the data is low. Data analysis methods aim to\nuncover the underlying low dimensional structure imposed by the low dimensional\nhidden parameters by utilizing distance metrics that consider the set of\nattributes as a single monolithic set. However, the transformation of the low\ndimensional phenomena into the measured high dimensional observations might\ndistort the distance metric, This distortion can effect the desired estimated\nlow dimensional geometric structure. In this paper, we suggest to utilize the\nredundancy in the attribute domain by partitioning the attributes into multiple\nsubsets we call views. The proposed methods utilize the agreement also called\nconsensus between different views to extract valuable geometric information\nthat unifies multiple views about the intrinsic relationships among several\ndifferent observations. This unification enhances the information that a single\nview or a simple concatenations of views provides.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 18:32:43 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 11:11:43 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Salhov", "Moshe", ""], ["Lindenbaum", "Ofir", ""], ["Aizenbud", "Yariv", ""], ["Silberschatz", "Avi", ""], ["Shkolnisky", "Yoel", ""], ["Averbuch", "Amir", ""]]}, {"id": "1606.08842", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Nihar B. Shah and Kannan Ramchandran and Martin J.\n  Wainwright", "title": "Active Ranking from Pairwise Comparisons and when Parametric Assumptions\n  Don't Help", "comments": "improved log factor in main result; added discussion on comparison\n  probabilities close to zero; added numerical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sequential or active ranking of a set of n items based on noisy\npairwise comparisons. Items are ranked according to the probability that a\ngiven item beats a randomly chosen item, and ranking refers to partitioning the\nitems into sets of pre-specified sizes according to their scores. This notion\nof ranking includes as special cases the identification of the top-k items and\nthe total ordering of the items. We first analyze a sequential ranking\nalgorithm that counts the number of comparisons won, and uses these counts to\ndecide whether to stop, or to compare another pair of items, chosen based on\nconfidence intervals specified by the data collected up to that point. We prove\nthat this algorithm succeeds in recovering the ranking using a number of\ncomparisons that is optimal up to logarithmic factors. This guarantee does not\nrequire any structural properties of the underlying pairwise probability\nmatrix, unlike a significant body of past work on pairwise ranking based on\nparametric models such as the Thurstone or Bradley-Terry-Luce models. It has\nbeen a long-standing open question as to whether or not imposing these\nparametric assumptions allows for improved ranking algorithms. For stochastic\ncomparison models, in which the pairwise probabilities are bounded away from\nzero, our second contribution is to resolve this issue by proving a lower bound\nfor parametric models. This shows, perhaps surprisingly, that these popular\nparametric modeling choices offer at most logarithmic gains for stochastic\ncomparisons.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 19:59:52 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 15:55:44 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Heckel", "Reinhard", ""], ["Shah", "Nihar B.", ""], ["Ramchandran", "Kannan", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1606.08866", "submitter": "Terence Parr", "authors": "Terence Parr and Jurgin Vinju", "title": "Technical Report: Towards a Universal Code Formatter through Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many declarative frameworks that allow us to implement code\nformatters relatively easily for any specific language, but constructing them\nis cumbersome. The first problem is that \"everybody\" wants to format their code\ndifferently, leading to either many formatter variants or a ridiculous number\nof configuration options. Second, the size of each implementation scales with a\nlanguage's grammar size, leading to hundreds of rules.\n  In this paper, we solve the formatter construction problem using a novel\napproach, one that automatically derives formatters for any given language\nwithout intervention from a language expert. We introduce a code formatter\ncalled CodeBuff that uses machine learning to abstract formatting rules from a\nrepresentative corpus, using a carefully designed feature set. Our experiments\non Java, SQL, and ANTLR grammars show that CodeBuff is efficient, has excellent\naccuracy, and is grammar invariant for a given language. It also generalizes to\na 4th language tested during manuscript preparation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 20:04:07 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Parr", "Terence", ""], ["Vinju", "Jurgin", ""]]}, {"id": "1606.08883", "submitter": "Lili Su", "authors": "Lili Su, Nitin H. Vaidya", "title": "Defending Non-Bayesian Learning against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of non-Bayesian learning over multi-agent\nnetworks, where agents repeatedly collect partially informative observations\nabout an unknown state of the world, and try to collaboratively learn the true\nstate. We focus on the impact of the adversarial agents on the performance of\nconsensus-based non-Bayesian learning, where non-faulty agents combine local\nlearning updates with consensus primitives. In particular, we consider the\nscenario where an unknown subset of agents suffer Byzantine faults -- agents\nsuffering Byzantine faults behave arbitrarily. Two different learning rules are\nproposed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 20:50:08 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Su", "Lili", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1606.08920", "submitter": "Iosif Pinelis", "authors": "Aryeh Kontorovich and Iosif Pinelis", "title": "Exact Lower Bounds for the Agnostic Probably-Approximately-Correct (PAC)\n  Machine Learning Model", "comments": "Version 2: modified presentation in accordance with referees'\n  comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an exact non-asymptotic lower bound on the minimax expected excess\nrisk (EER) in the agnostic probably-ap\\-proximately-correct (PAC) machine\nlearning classification model and identify minimax learning algorithms as\ncertain maximally symmetric and minimally randomized \"voting\" procedures. Based\non this result, an exact asymptotic lower bound on the minimax EER is provided.\nThis bound is of the simple form $c_\\infty/\\sqrt{\\nu}$ as $\\nu\\to\\infty$, where\n$c_\\infty=0.16997\\dots$ is a universal constant, $\\nu=m/d$, $m$ is the size of\nthe training sample, and $d$ is the Vapnik--Chervonenkis dimension of the\nhypothesis class. It is shown that the differences between these asymptotic and\nnon-asymptotic bounds, as well as the differences between these two bounds and\nthe maximum EER of any learning algorithms that minimize the empirical risk,\nare asymptotically negligible, and all these differences are due to ties in the\nmentioned \"voting\" procedures. A few easy to compute non-asymptotic lower\nbounds on the minimax EER are also obtained, which are shown to be close to the\nexact asymptotic lower bound $c_\\infty/\\sqrt{\\nu}$ even for rather small values\nof the ratio $\\nu=m/d$. As an application of these results, we substantially\nimprove existing lower bounds on the tail probability of the excess risk. Among\nthe tools used are Bayes estimation and apparently new identities and\ninequalities for binomial distributions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 00:19:55 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 03:45:48 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Kontorovich", "Aryeh", ""], ["Pinelis", "Iosif", ""]]}, {"id": "1606.08928", "submitter": "Annamalai Narayanan", "authors": "Annamalai Narayanan, Mahinthan Chandramohan, Lihui Chen, Yang Liu and\n  Santhoshkumar Saminathan", "title": "subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs\n  from Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present subgraph2vec, a novel approach for learning latent\nrepresentations of rooted subgraphs from large graphs inspired by recent\nadvancements in Deep Learning and Graph Kernels. These latent representations\nencode semantic substructure dependencies in a continuous vector space, which\nis easily exploited by statistical models for tasks such as graph\nclassification, clustering, link prediction and community detection.\nsubgraph2vec leverages on local information obtained from neighbourhoods of\nnodes to learn their latent representations in an unsupervised fashion. We\ndemonstrate that subgraph vectors learnt by our approach could be used in\nconjunction with classifiers such as CNNs, SVMs and relational data clustering\nalgorithms to achieve significantly superior accuracies. Also, we show that the\nsubgraph vectors could be used for building a deep learning variant of\nWeisfeiler-Lehman graph kernel. Our experiments on several benchmark and\nlarge-scale real-world datasets reveal that subgraph2vec achieves significant\nimprovements in accuracies over existing graph kernels on both supervised and\nunsupervised learning tasks. Specifically, on two realworld program analysis\ntasks, namely, code clone and malware detection, subgraph2vec outperforms\nstate-of-the-art kernels by more than 17% and 4%, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 01:05:36 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Narayanan", "Annamalai", ""], ["Chandramohan", "Mahinthan", ""], ["Chen", "Lihui", ""], ["Liu", "Yang", ""], ["Saminathan", "Santhoshkumar", ""]]}, {"id": "1606.08963", "submitter": "Nemanja Djuric", "authors": "Nemanja Djuric, Mihajlo Grbovic, Vladan Radosavljevic, Narayan\n  Bhamidipati, Slobodan Vucetic", "title": "Non-linear Label Ranking for Large-scale Prediction of Long-Term User\n  Interests", "comments": "28th AAAI Conference on Artificial Intelligence (AAAI-14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of personalization of online services from the\nviewpoint of ad targeting, where we seek to find the best ad categories to be\nshown to each user, resulting in improved user experience and increased\nadvertisers' revenue. We propose to address this problem as a task of ranking\nthe ad categories depending on a user's preference, and introduce a novel label\nranking approach capable of efficiently learning non-linear, highly accurate\nmodels in large-scale settings. Experiments on a real-world advertising data\nset with more than 3.2 million users show that the proposed algorithm\noutperforms the existing solutions in terms of both rank loss and top-K\nretrieval performance, strongly suggesting the benefit of using the proposed\nmodel on large-scale ranking problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 06:00:35 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Djuric", "Nemanja", ""], ["Grbovic", "Mihajlo", ""], ["Radosavljevic", "Vladan", ""], ["Bhamidipati", "Narayan", ""], ["Vucetic", "Slobodan", ""]]}, {"id": "1606.09022", "submitter": "Eftychios Protopapadakis", "authors": "Eftychios Protopapadakis", "title": "Decision making via semi-supervised machine learning techniques", "comments": "arXiv admin note: text overlap with arXiv:math/0604233,\n  arXiv:1208.2128, arXiv:1406.5298 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) is a class of supervised learning tasks and\ntechniques that also exploits the unlabeled data for training. SSL\nsignificantly reduces labeling related costs and is able to handle large data\nsets. The primary objective is the extraction of robust inference rules.\nDecision support systems (DSSs) who utilize SSL have significant advantages.\nOnly a small amount of labelled data is required for the initialization. Then,\nnew (unlabeled) data can be utilized and improve system's performance. Thus,\nthe DSS is continuously adopted to new conditions, with minimum effort.\nTechniques which are cost effective and easily adopted to dynamic systems, can\nbe beneficial for many practical applications. Such applications fields are:\n(a) industrial assembly lines monitoring, (b) sea border surveillance, (c)\nelders' falls detection, (d) transportation tunnels inspection, (e) concrete\nfoundation piles defect recognition, (f) commercial sector companies financial\nassessment and (g) image advanced filtering for cultural heritage applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 09:47:29 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Protopapadakis", "Eftychios", ""]]}, {"id": "1606.09058", "submitter": "Dimitrios Alikaniotis", "authors": "Dimitrios Alikaniotis and John N. Williams", "title": "A Distributional Semantics Approach to Implicit Language Learning", "comments": "5 pages, 7 figures, NetWords 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we show that distributional information is particularly\nimportant when considering concept availability under implicit language\nlearning conditions. Based on results from different behavioural experiments we\nargue that the implicit learnability of semantic regularities depends on the\ndegree to which the relevant concept is reflected in language use. In our\nsimulations, we train a Vector-Space model on either an English or a Chinese\ncorpus and then feed the resulting representations to a feed-forward neural\nnetwork. The task of the neural network was to find a mapping between the word\nrepresentations and the novel words. Using datasets from four behavioural\nexperiments, which used different semantic manipulations, we were able to\nobtain learning patterns very similar to those obtained by humans.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 12:08:51 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Alikaniotis", "Dimitrios", ""], ["Williams", "John N.", ""]]}, {"id": "1606.09152", "submitter": "Olivier Sigaud", "authors": "Arnaud de Froissard de Broissia and Olivier Sigaud", "title": "Actor-critic versus direct policy search: a comparison based on sample\n  complexity", "comments": "Proceedings JFPDA (Journees Francaises Planification Decision\n  Apprentissage)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample efficiency is a critical property when optimizing policy parameters\nfor the controller of a robot. In this paper, we evaluate two state-of-the-art\npolicy optimization algorithms. One is a recent deep reinforcement learning\nmethod based on an actor-critic algorithm, Deep Deterministic Policy Gradient\n(DDPG), that has been shown to perform well on various control benchmarks. The\nother one is a direct policy search method, Covariance Matrix Adaptation\nEvolution Strategy (CMA-ES), a black-box optimization method that is widely\nused for robot learning. The algorithms are evaluated on a continuous version\nof the mountain car benchmark problem, so as to compare their sample\ncomplexity. From a preliminary analysis, we expect DDPG to be more sample\nefficient than CMA-ES, which is confirmed by our experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 15:22:13 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 11:07:23 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["de Broissia", "Arnaud de Froissard", ""], ["Sigaud", "Olivier", ""]]}, {"id": "1606.09184", "submitter": "Peter Schulam", "authors": "Peter Schulam and Raman Arora", "title": "Disease Trajectory Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical researchers are coming to appreciate that many diseases are in fact\ncomplex, heterogeneous syndromes composed of subpopulations that express\ndifferent variants of a related complication. Time series data extracted from\nindividual electronic health records (EHR) offer an exciting new way to study\nsubtle differences in the way these diseases progress over time. In this paper,\nwe focus on answering two questions that can be asked using these databases of\ntime series. First, we want to understand whether there are individuals with\nsimilar disease trajectories and whether there are a small number of degrees of\nfreedom that account for differences in trajectories across the population.\nSecond, we want to understand how important clinical outcomes are associated\nwith disease trajectories. To answer these questions, we propose the Disease\nTrajectory Map (DTM), a novel probabilistic model that learns low-dimensional\nrepresentations of sparse and irregularly sampled time series. We propose a\nstochastic variational inference algorithm for learning the DTM that allows the\nmodel to scale to large modern medical datasets. To demonstrate the DTM, we\nanalyze data collected on patients with the complex autoimmune disease,\nscleroderma. We find that DTM learns meaningful representations of disease\ntrajectories and that the representations are significantly associated with\nimportant clinical outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 17:06:45 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Schulam", "Peter", ""], ["Arora", "Raman", ""]]}, {"id": "1606.09190", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien, Cl\\'ement Dombry and Adrien Faivre", "title": "A Semi-Definite Programming approach to low dimensional embedding for\n  unsupervised clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a variant of the method of Gu\\'edon and Verhynin for\nestimating the cluster matrix in the Mixture of Gaussians framework via\nSemi-Definite Programming. A clustering oriented embedding is deduced from this\nestimate. The procedure is suitable for very high dimensional data because it\nis based on pairwise distances only. Theoretical garantees are provided and an\neigenvalue optimisation approach is proposed for computing the embedding. The\nperformance of the method is illustrated via Monte Carlo experiements and\ncomparisons with other embeddings from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 17:20:39 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Dombry", "Cl\u00e9ment", ""], ["Faivre", "Adrien", ""]]}, {"id": "1606.09197", "submitter": "Riad Akrour", "authors": "Riad Akrour, Abbas Abdolmaleki, Hany Abdulsamad, Jan Peters and\n  Gerhard Neumann", "title": "Model-Free Trajectory-based Policy Optimization with Monotonic\n  Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the recent trajectory optimization algorithms alternate between\nlinear approximation of the system dynamics around the mean trajectory and\nconservative policy update. One way of constraining the policy change is by\nbounding the Kullback-Leibler (KL) divergence between successive policies.\nThese approaches already demonstrated great experimental success in challenging\nproblems such as end-to-end control of physical systems. However, the linear\napproximation of the system dynamics can introduce a bias in the policy update\nand prevent convergence to the optimal policy. In this article, we propose a\nnew model-free trajectory-based policy optimization algorithm with guaranteed\nmonotonic improvement. The algorithm backpropagates a local, quadratic and\ntime-dependent \\qfunc~learned from trajectory data instead of a model of the\nsystem dynamics. Our policy update ensures exact KL-constraint satisfaction\nwithout simplifying assumptions on the system dynamics. We experimentally\ndemonstrate on highly non-linear control tasks the improvement in performance\nof our algorithm in comparison to approaches linearizing the system dynamics.\nIn order to show the monotonic improvement of our algorithm, we additionally\nconduct a theoretical analysis of our policy update scheme to derive a lower\nbound of the change in policy return between successive iterations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 17:39:09 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 08:35:57 GMT"}, {"version": "v3", "created": "Thu, 29 Jun 2017 09:07:37 GMT"}, {"version": "v4", "created": "Mon, 2 Jul 2018 12:40:05 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Akrour", "Riad", ""], ["Abdolmaleki", "Abbas", ""], ["Abdulsamad", "Hany", ""], ["Peters", "Jan", ""], ["Neumann", "Gerhard", ""]]}, {"id": "1606.09202", "submitter": "Nicolas Le Roux", "authors": "Nicolas Le Roux", "title": "Tighter bounds lead to improved classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to supervised classification involves the minimization\nof a log-loss as an upper bound to the classification error. While this is a\ntight bound early on in the optimization, it overemphasizes the influence of\nincorrectly classified examples far from the decision boundary. Updating the\nupper bound during the optimization leads to improved classification rates\nwhile transforming the learning into a sequence of minimization problems. In\naddition, in the context where the classifier is part of a larger system, this\nmodification makes it possible to link the performance of the classifier to\nthat of the whole system, allowing the seamless introduction of external\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 18:01:15 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 19:54:19 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Roux", "Nicolas Le", ""]]}, {"id": "1606.09239", "submitter": "Hao Zhang", "authors": "Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan, Zhicheng Yan,\n  Eric P. Xing", "title": "Learning Concept Taxonomies from Multi-modal Data", "comments": "To appear in ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of automatically building hypernym taxonomies from\ntextual and visual data. Previous works in taxonomy induction generally ignore\nthe increasingly prominent visual data, which encode important perceptual\nsemantics. Instead, we propose a probabilistic model for taxonomy induction by\njointly leveraging text and images. To avoid hand-crafted feature engineering,\nwe design end-to-end features based on distributed representations of images\nand words. The model is discriminatively trained given a small set of existing\nontologies and is capable of building full taxonomies from scratch for a\ncollection of unseen conceptual label items with associated images. We evaluate\nour model and features on the WordNet hierarchies, where our system outperforms\nprevious approaches by a large gap.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 19:52:53 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Zhang", "Hao", ""], ["Hu", "Zhiting", ""], ["Deng", "Yuntian", ""], ["Sachan", "Mrinmaya", ""], ["Yan", "Zhicheng", ""], ["Xing", "Eric P.", ""]]}, {"id": "1606.09282", "submitter": "Zhizhong Li", "authors": "Zhizhong Li, Derek Hoiem", "title": "Learning without Forgetting", "comments": "Conference version appears in ECCV 2016; updated with journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building a unified vision system or gradually adding new capabilities to\na system, the usual assumption is that training data for all tasks is always\navailable. However, as the number of tasks grows, storing and retraining on\nsuch data becomes infeasible. A new problem arises where we add new\ncapabilities to a Convolutional Neural Network (CNN), but the training data for\nits existing capabilities are unavailable. We propose our Learning without\nForgetting method, which uses only new task data to train the network while\npreserving the original capabilities. Our method performs favorably compared to\ncommonly used feature extraction and fine-tuning adaption techniques and\nperforms similarly to multitask learning that uses original task data we assume\nunavailable. A more surprising observation is that Learning without Forgetting\nmay be able to replace fine-tuning with similar old and new task datasets for\nimproved new task performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 20:54:04 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2016 22:12:43 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 22:32:30 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Li", "Zhizhong", ""], ["Hoiem", "Derek", ""]]}, {"id": "1606.09333", "submitter": "Yossi Arjevani", "authors": "Yossi Arjevani and Ohad Shamir", "title": "Dimension-Free Iteration Complexity of Finite Sum Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many canonical machine learning problems boil down to a convex optimization\nproblem with a finite sum structure. However, whereas much progress has been\nmade in developing faster algorithms for this setting, the inherent limitations\nof these problems are not satisfactorily addressed by existing lower bounds.\nIndeed, current bounds focus on first-order optimization algorithms, and only\napply in the often unrealistic regime where the number of iterations is less\nthan $\\mathcal{O}(d/n)$ (where $d$ is the dimension and $n$ is the number of\nsamples). In this work, we extend the framework of (Arjevani et al., 2015) to\nprovide new lower bounds, which are dimension-free, and go beyond the\nassumptions of current bounds, thereby covering standard finite sum\noptimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as well as\nstochastic coordinate-descent methods, such as SDCA and accelerated proximal\nSDCA.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 03:10:54 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Arjevani", "Yossi", ""], ["Shamir", "Ohad", ""]]}, {"id": "1606.09375", "submitter": "Micha\\\"el Defferrard", "authors": "Micha\\\"el Defferrard, Xavier Bresson, Pierre Vandergheynst", "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral\n  Filtering", "comments": "NIPS 2016 final revision", "journal-ref": "Advances in Neural Information Processing Systems 29 (2016)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 07:42:13 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 15:24:49 GMT"}, {"version": "v3", "created": "Sun, 5 Feb 2017 17:04:39 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Defferrard", "Micha\u00ebl", ""], ["Bresson", "Xavier", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1606.09383", "submitter": "Willem Eerland", "authors": "Willem Eerland, Coen de Visser, Erik-Jan van Kampen", "title": "On Approximate Dynamic Programming with Multivariate Splines for\n  Adaptive Control", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a SDP framework based on the RLSTD algorithm and multivariate\nsimplex B-splines. We introduce a local forget factor capable of preserving the\ncontinuity of the simplex splines. This local forget factor is integrated with\nthe RLSTD algorithm, resulting in a modified RLSTD algorithm that is capable of\ntracking time-varying systems. We present the results of two numerical\nexperiments, one validating SDP and comparing it with NDP and another to show\nthe advantages of the modified RLSTD algorithm over the original. While SDP\nrequires more computations per time-step, the experiment shows that for the\nsame amount of function approximator parameters, there is an increase in\nperformance in terms of stability and learning rate compared to NDP. The second\nexperiment shows that SDP in combination with the modified RLSTD algorithm\nallows for faster recovery compared to the original RLSTD algorithm when system\nparameters are altered, paving the way for an adaptive high-performance\nnon-linear control method.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 08:12:21 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Eerland", "Willem", ""], ["de Visser", "Coen", ""], ["van Kampen", "Erik-Jan", ""]]}, {"id": "1606.09388", "submitter": "Emilie Kaufmann", "authors": "Alexander Luedtke, Emilie Kaufmann (CRIStAL), Antoine Chambaz (MAP5 -\n  UMR 8145)", "title": "Asymptotically Optimal Algorithms for Budgeted Multiple Play Bandits", "comments": null, "journal-ref": "Machine Learning Journal, Springer, In press", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalization of the multi-armed bandit problem with multiple\nplays where there is a cost associated with pulling each arm and the agent has\na budget at each time that dictates how much she can expect to spend. We derive\nan asymptotic regret lower bound for any uniformly efficient algorithm in our\nsetting. We then study a variant of Thompson sampling for Bernoulli rewards and\na variant of KL-UCB for both single-parameter exponential families and bounded,\nfinitely supported rewards. We show these algorithms are asymptotically\noptimal, both in rateand leading problem-dependent constants, including in the\nthick margin setting where multiple arms fall on the decision boundary.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 08:30:57 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 07:38:48 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 12:20:09 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Luedtke", "Alexander", "", "CRIStAL"], ["Kaufmann", "Emilie", "", "CRIStAL"], ["Chambaz", "Antoine", "", "MAP5 -\n  UMR 8145"]]}, {"id": "1606.09458", "submitter": "Maryam Sabzevari", "authors": "Maryam Sabzevari, Gonzalo Mart\\'inez-Mu\\~noz, Alberto Su\\'arez", "title": "Vote-boosting ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vote-boosting is a sequential ensemble learning method in which the\nindividual classifiers are built on different weighted versions of the training\ndata. To build a new classifier, the weight of each training instance is\ndetermined in terms of the degree of disagreement among the current ensemble\npredictions for that instance. For low class-label noise levels, especially\nwhen simple base learners are used, emphasis should be made on instances for\nwhich the disagreement rate is high. When more flexible classifiers are used\nand as the noise level increases, the emphasis on these uncertain instances\nshould be reduced. In fact, at sufficiently high levels of class-label noise,\nthe focus should be on instances on which the ensemble classifiers agree. The\noptimal type of emphasis can be automatically determined using\ncross-validation. An extensive empirical analysis using the beta distribution\nas emphasis function illustrates that vote-boosting is an effective method to\ngenerate ensembles that are both accurate and robust.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 12:24:04 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 12:31:01 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Sabzevari", "Maryam", ""], ["Mart\u00ednez-Mu\u00f1oz", "Gonzalo", ""], ["Su\u00e1rez", "Alberto", ""]]}, {"id": "1606.09517", "submitter": "Ryan Turner", "authors": "Ryan Turner", "title": "A Model Explanation System: Latest Updates and Extensions", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general model explanation system (MES) for \"explaining\" the\noutput of black box classifiers. This paper describes extensions to Turner\n(2015), which is referred to frequently in the text. We use the motivating\nexample of a classifier trained to detect fraud in a credit card transaction\nhistory. The key aspect is that we provide explanations applicable to a single\nprediction, rather than provide an interpretable set of parameters. We focus on\nexplaining positive predictions (alerts). However, the presented methodology is\nsymmetrically applicable to negative predictions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 14:44:26 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Turner", "Ryan", ""]]}, {"id": "1606.09581", "submitter": "Sahil Sharma", "authors": "Sahil Sharma, Vinod Sharma and Atul Sharma", "title": "Performance Based Evaluation of Various Machine Learning Classification\n  Techniques for Chronic Kidney Disease Diagnosis", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": "International Journal of Modern Computer Science, Vol.4, Issue3,\n  June 2016, pp.11-16", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Areas where Artificial Intelligence (AI) & related fields are finding their\napplications are increasing day by day, moving from core areas of computer\nscience they are finding their applications in various other domains.In recent\ntimes Machine Learning i.e. a sub-domain of AI has been widely used in order to\nassist medical experts and doctors in the prediction, diagnosis and prognosis\nof various diseases and other medical disorders. In this manuscript the authors\napplied various machine learning algorithms to a problem in the domain of\nmedical diagnosis and analyzed their efficiency in predicting the results. The\nproblem selected for the study is the diagnosis of the Chronic Kidney\nDisease.The dataset used for the study consists of 400 instances and 24\nattributes. The authors evaluated 12 classification techniques by applying them\nto the Chronic Kidney Disease data. In order to calculate efficiency, results\nof the prediction by candidate methods were compared with the actual medical\nresults of the subject.The various metrics used for performance evaluation are\npredictive accuracy, precision, sensitivity and specificity. The results\nindicate that decision-tree performed best with nearly the accuracy of 98.6%,\nsensitivity of 0.9720, precision of 1 and specificity of 1.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 07:00:07 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 08:14:43 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Sharma", "Sahil", ""], ["Sharma", "Vinod", ""], ["Sharma", "Atul", ""]]}, {"id": "1606.09632", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright", "title": "A Permutation-based Model for Crowd Labeling: Optimal Estimation and\n  Robustness", "comments": "in IEEE Transactions on Information Theory (online), 2020", "journal-ref": null, "doi": "10.1109/TIT.2020.3045613", "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of aggregating and denoising crowd-labeled data has gained increased\nsignificance with the advent of crowdsourcing platforms and massive datasets.\nWe propose a permutation-based model for crowd labeled data that is a\nsignificant generalization of the classical Dawid-Skene model, and introduce a\nnew error metric by which to compare different estimators. We derive global\nminimax rates for the permutation-based model that are sharp up to logarithmic\nfactors, and match the minimax lower bounds derived under the simpler\nDawid-Skene model. We then design two computationally-efficient estimators: the\nWAN estimator for the setting where the ordering of workers in terms of their\nabilities is approximately known, and the OBI-WAN estimator where that is not\nknown. For each of these estimators, we provide non-asymptotic bounds on their\nperformance. We conduct synthetic simulations and experiments on real-world\ncrowdsourcing data, and the experimental results corroborate our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 19:40:56 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 04:58:30 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 18:18:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""]]}]